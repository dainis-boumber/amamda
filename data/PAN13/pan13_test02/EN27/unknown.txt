We saw in Chapter 11 that the Field Values Table (or Tables, plural) will always be in memory at run time, at least to a first approximation. And we saw in Chapter 12 how to use file factoring to reduce the space requirements for the Record Reconstruction Table(s) as well; basically, what we do is decompose the original file—at least conceptually—so that we wind up with one “large” Record Reconstruction Table and several “small” ones. And the small Record Reconstruction Tables too will always be in memory at run time (again to a first approximation). So we’re left with the large Record Reconstruction Table on disk. That large table can’t be compressed any further, more or less by definition; in other words, if we regard its contents just as simple bit strings, then those bit strings are essentially random sequences of zeros and ones.1 The techniques discussed in this chapter and the next are specifically aimed at getting the best possible performance out of that large table, despite the fact that it’s necessarily disk-resident. 
Before I go any further, I should make it clear that, although I call it “large,” the table we’re dealing with—in fact, the entire TR data representation, including the Field Values Table(s) and all of the corresponding Record Reconstruction Tables—is still likely to be far smaller than a conventional direct-image representation. Actual experiments have shown that a reduction of five to one is quite typical (if anything, that estimate is probably on the low side). And that’s just for the raw data; when indexes and other auxiliary structures are taken into account, the direct-image space requirement can increase by another five-to-one ratio, possibly even more.2 However, when I need to appeal to such matters later in this chapter, I’ll stick, conservatively, to the five-to-one figure.

Anyway, to remind you from Chapter 11, the problem with the large table is that the zigzags in that table, even if their starting points are physically contiguous (as indeed they are, because the table is stored column-wise), quickly splay out to essentially random positions “all over the disk,” with the consequence that we might have to do a separate seek for every point after the starting point every time we chase such a zigzag. File banding, or just banding for short, is a technique for addressing this problem. Here in outline is how it works: 
■■Starting with a given user-level relation and hence a corresponding file, we sort that file into order based on values of some characteristic field (or field combination; for simplicity, I’ll assume throughout what follows that we’re always dealing with a single characteristic field, barring explicit statements to the contrary). 
■■Next, we decompose that sorted file horizontally3 into two or more subfiles of approximately equal size. Each subfile is smaller than the original file in the sense that it has fewer records (usually far fewer) than the original file did. Note: The official term for “horizontal subfiles” is bands, and I’ll favor this latter term in subsequent sections. You can think of those bands or horizontal subfiles as partitions, if you like; note, however, that specifics of the partitioning in question are determined primarily by physical space requirements and only secondarily by values of the characteristic field. We’ll see some implications of this state of affairs in the next section (in particular, we’ll see that a given characteristic field value might appear in more than one band, something that couldn’t happen if the partitioning were done purely on the basis of values of that field). 
■■We then represent each of those subfiles or bands by its own Field Values Table and its own Record Reconstruction Table. Because the bands are smaller than the original file, those Field Values and Record Reconstruction Tables too are smaller than their counterparts would have been for the original file. In fact, we choose the band size such that any given band can fit into memory in its entirety at run time, and we lay the bands out on the disk in such a way as to allow any given band to be streamed into memory as and when it’s needed. (When I say the entire band fits into memory, what I’m mainly talking about is the Record Reconstruction Table for the given band, of course. If the Record Reconstruction Table for a given band can be entirely contained in memory, then all of the zigzags within that table will also be entirely contained in memory a fortiori, and—insofar as that particular table is concerned, at least—the splay problem thus won’t arise.) 
Note: Please understand that the foregoing account is deliberately somewhat simplified; I’ll come back and explain later (in Section 13.4) how banding is really done. However, the foregoing explanation is accurate enough to serve as a basis for discussions prior to that section. 
The structure of the chapter is as follows. Following this introductory section, I’ll explain the basic idea of banding by means of a simple example in Section 13.2; then I’ll elaborate on and generalize from that example in Section 13.3, and introduce the important idea of controlled redundancy. As already mentioned, in Section 13.4 I’ll build on the ideas of previous sections to show how banding is really done. Finally, in Section 13.5, I’ll discuss the concept of controlled redundancy in more detail.

In the interests of “user-friendliness,” I’ll continue to work with the familiar parts example (or an extended version of that example, rather). Assume again—as in Chapter 12, Section 12.5—that we’ve factored the parts file into large and small files that look like this:

Sample values for these files are shown in Fig. 13.1 (an extended version of Fig. 12.2 from Chapter 12). Note: Of course, it’s the large file we’re interested in here, not the small one. In the figure, of course, that file is hardly very “large” (obviously, since it has just nine records); however, don’t lose sight of the fact that if we’re really supposed to be building on the example from Chapter 12, then the file is really supposed to have some ten million records. What’s more, fields in that file—with the obvious exception of the introduced artificial identifier CC#—are supposed to be of high cardinality, meaning each such field has around ten million distinct values as well; in fact, the data in the large file isn’t supposed to display any “statistical clumpiness” at all.