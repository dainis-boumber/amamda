This chapter continues our examination of the core constructs of the TR model (principally the Field Values and Record Reconstruction Tables). However, the chapter is rather more of a potpourri than the previous one. Its structure is as follows. Following this short introductory section, Section 5.2 offers some general observations regarding performance. Section 5.3 then briefly surveys the TR operators, and Sections 5.4 and 5.5 take another look at how the Record Reconstruction Table is built and how record reconstruction is done. Sections 5.6 and 5.7 describe some alternative perspectives on certain of the TR constructs introduced in Chapter 4. Finally, Section 5.6 takes a look at some alternative ways of implementing some of the TR structures and algorithms also first described in that previous chapter.
It seems to me undeniable that the mechanisms described in the previous chapter for representing and reconstructing records and files are vastly different from those found in conventional DBMSs, and I presume you agree with this assessment. At the same time, however, they certainly look pretty complicated ... How does all of that complexity square with the claims I made in Chapter 1 regarding good performance? Let me remind you of some of the things I said there:
Well, let me say a little more now regarding query performance specifically (I haven’t really discussed updates yet, so I’ll have to come back to the question of update performance later—actually in the next chapter). Now, any given query involves two logically distinct processes: 
a) Finding the data that’s required, and then 
b) Retrieving that data. 
TR is designed to exploit this fact. Precisely because it separates field value information and linkage information, it can treat these two processes more or less independently. To find the data, it uses the Field Values Table; to retrieve it, it uses the Record Reconstruction Table. (These characterizations aren’t 100 percent accurate, but they’re good to a first approximation—good enough for present purposes, at any rate.) And the Field Values Table in particular is designed to make the finding of data very efficient (for example, via binary search), as we saw in Chapter 4. Of course, it’s true that subsequent retrieval of that data then involves the record reconstruction process, and this latter process in turn involves a lot of pointer chasing, but:
■■Even in a disk-based implementation, the system will do its best to ensure that pertinent portions of both the Field Values Table and the Record Reconstruction Table are kept in main memory at run time, as we’ll see in Part III. Assuming this goal is met, the reconstruction will be done at main-memory speeds. 
■■The “frills” to be discussed in Chapters 7 9 (as well as others that are beyond the scope of this book) have the effect, among other things, of dramatically improving the performance of various aspects of the reconstruction process. 
■■Most important of all: Almost always, finding the data that’s wanted is a much bigger issue than returning that data to the user is. In a sense, the design of the TR internal structures is biased in favor of the first of these issues at the expense of the second. Observe the implication: The more complex the query, the better TR will perform—in comparison with traditional approaches, that is. (Of course, I don’t mean to suggest by these remarks that record reconstruction is slow or inefficient—it isn’t—nor that TR performs well on complex queries but not on simple ones. I just want to stress the relative importance of finding the data in the first place, that’s all.)
I’d like to say more on this question of query performance. In 1969, in his very first paper on the relational model [5], Codd had this to say:
Once aware that a certain relation exists, the user will expect to be able to exploit that relation using any combination of its attributes as “knowns” and the remaining attributes as “unknowns,” because the information (like Everest) is there. This is a system feature (missing from many current information systems) which we shall call (logically) symmetric exploitation of relations. Naturally, symmetry in performance is not to be expected. 
—E. F. Codd
Note: I’ve reworded Codd’s remarks just slightly here. In particular, the final sentence (the caveat concerning performance) didn’t appear in the original 1969 paper [5] but was added in the expanded 1970 version [6]. 
Anyway, the point I want to make is that the TR approach gives us symmetry in performance, too—or, at least, it comes much closer to doing so than previous approaches ever did. This is because, as we saw in Chapter 4, the separation of field values from linkage information effectively allows the data to be physically stored in several different sort orders simultaneously. When Codd said “symmetry in performance is not to be expected,” he was tacitly assuming a direct-image style of implementation, one involving auxiliary structures like those described in Chapter 2. However, as I said in that chapter: 
[Auxiliary structures such as pointer chains and] indexes can be used to impose different orderings on a given file and thus (in a sense) “level the playing field” with respect to different processing sequences; all of those sequences are equally good from a logical point of view. But they certainly aren’t equally good from a performance point of view. For example, even if there’s a city index, processing suppliers in city name sequence will involve (in effect) random accesses to storage, precisely because the supplier records aren’t physically stored in city name sequence but are scattered all over the disk. 
—from Chapter 2
As we’ve seen, however, these remarks simply don’t apply to the TR data representation. 
And now I can address another issue that might possibly have been bothering you. We’ve seen that the TR model relies heavily on pointers. Now, the CODASYL “network model” [14,25] also relies heavily on pointers—as the “object model” [3,4,28,29] and “hierarchic model” [25,56] both do also, as a matter of fact—and I and many other writers have criticized it vigorously in the past on exactly that score (see, for example, references [10], [21], and [37]). So am I arguing out of both sides of my mouth here? How can TR pointers be good while CODASYL pointers are bad?