Knowledge is essential for intelligent behaviour. Without knowledge, an intelligent agent cannot make informed decisions, and instead must rely on using some form of searching type behaviour involving exploration and/or communication in order to gain the missing knowledge. Humans rely on knowledge every moment of their life – knowledge of how to communicate with other humans, knowledge of where they and other people live and work, knowledge of where things are, knowledge of how to behave in different situations, knowledge of how to perform different tasks and so on. Without the ability to store and process knowledge, the cognitive abilities of a human is seriously curtailed. An illness such as Alzheimer’s, for example, can be debilitating when memory loss occurs such as the difficulty in remembering recently learned facts. We all know (or think we know) what we mean when we use the term ‘knowledge’. But what exactly is knowledge? Bertrand Russell (1926) acknowledged the difficult question of how to define the meaning of ‘knowledge’: “It is perhaps unwise to begin with a definition of the subject, since, as elsewhere in philosophical discussions, definitions are controversial, and will necessarily differ for different schools”. 

A definition of knowledge is the subject of ongoing philosophical debate and presently there are many competing theories with no single definition universally agreed upon. Consequently, treatment of knowledge from an Artificial Intelligence perspective has often consciously avoided the definition of what knowledge is. However, this avoidance of providing a definition of knowledge upfront results in a lack of preciseness in the literature and research. The following argument will illustrate why. A knowledge-based system is a term used in Artificial Intelligence to refer to a system that processes knowledge in some manner. We can make the analogy of a knowledge-based system as being a repository of knowledge, whereas a database is a repository of data. However, in this definition, we have neglected to define the meaning of the term ‘knowledge’ and how it is different to data. For example, we can ask ourselves the following question – “What constitutes a knowledge-based system, and how does it differ from a database system?” This is a difficult question that cannot readily be answered in a straightforward way. A common approach taken in the literature is that a knowledge-based system can perform reasoning using some form of inferencing (whether rule-based, frame-based; see below). Modern database systems, however, now employ most of these standard ‘knowledge-based’ techniques and more. Clearly, the addition of inferencing capabilities alone is not sufficient to define what a knowledge-based system is. However, a great deal of A.I. literature makes such an assumption. By avoiding a definition of knowledge, the problem becomes that it is no longer clear that what we are building really is in fact ‘knowledge-based’. In Chapter 1, it was stated that early A.I. systems in the 1970s and 1980s suffered from a lack of evaluation – there was a rush to build new systems, but often very little evaluation was undertaken of how well the systems worked. Without a working definition of knowledge, the same problem occurs now with current knowledge-based systems – how can we evaluate how effective our knowledge-base system might be if we do not have a definition of what it should be (or even achieve or do)? We can, however, avoid the philosophical pitfalls, and rather than attempting to define knowledge, and making a claim that this definition is the “right” one, instead we can propose design principles for our knowledge-based system. Hence, we can decide what principles we wish our knowledge-based system to adhere to, and we, as designers, are free to change them as we see fit based on knowledge we gain during the design process. Also, we are no longer standing on shaky ground in the sense that we do not have to provide one particular definition of knowledge which is open to philosophical debate, although we are still open to criticism about whether our principles are worthwhile from an engineering perspective (i.e. whether they produce “good” programs, or aren’t as good as other approaches). But evaluation becomes much simpler – all we need to do is evaluate whether our design principles are met. 

The following are some design principles for knowledge-based systems. 

The argument for this design principle is that if we design from an embodied, situated agent perspective, then all knowledge cannot exist independently of the agents. That is, knowledge cannot exist by itself – it can only be found in the ‘minds’ of the agents that are embodied and situated in an environment. We also wish to define and use the term ‘knowledge’ in a way similar to the way the term is used in natural language. The root of the word ‘knowledge’ comes from the verb “to know”. From a natural language perspective, ‘knowing’ and ‘knowledge’ are related. A rock, for example, does not ‘know’ anything. But a dog can ‘know’ where it has buried a bone; and it makes sense to say in natural language that the dog has ‘knowledge’ of where the bone is buried. The dog, in this case, is the agent, and the rock is an object in the environment. In other words, knowing behaviour is associated with an agent who has knowledge. A knowledge-based system can then be thought of as an agent whose role is to convey the knowledge that it contains to the users of the system. The interaction between the user agent and the system agent can be characterised by the actions that determine each agent’s behaviour, and whether the user agent perceives the system agent to be acting in a knowledgeable way. This leads to the next design principle. 

The following design principles are based on properties of ‘good’ knowledge-base systems proposed by Russell and Norvig (2002): 

A behavioural approach to knowledge places the emphasis not on building a specific independent system, but on building agents that exhibit behaviour that demonstrates they have knowledge of their environment and of other agents. In this approach, the act of ‘knowing’ occurs when an agent has information that might potentially aid the performance of an action taken by itself or by another agent. Further, an agent can be considered to have ‘knowledge’ if it knows what the likely outcomes will be of an action it may perform, or of an action another agent is performing, or what is likely to happen to an object in the environment.