Abstract

We consider the sparse inverse covariance regularization problem or graphical lasso with regular-
ization parameter l
. Suppose the sample covariance graph formed by thresholding the entries of
the sample covariance matrix at l
is decomposed into connected components. We show that the
vertex-partition induced by the connected components of the thresholded sample covariance graph
(at l ) is exactly equal to that induced by the connected components of the estimated concentration
graph, obtained by solving the graphical lasso problem for the same l
. This characterizes a very
interesting property of a path of graphical lasso solutions. Furthermore, this simple rule, when used
as a wrapper around existing algorithms for the graphical lasso, leads to enormous performance
gains. For a range of values of l
, our proposal splits a large graphical lasso problem into smaller
tractable problems, making it possible to solve an otherwise infeasible large-scale problem. We
illustrate the graceful scalability of our proposal via synthetic and real-life microarray examples.
Keywords: sparse inverse covariance selection, sparsity, graphical lasso, Gaussian graphical mod-
els, graph connected components, concentration graph, large scale covariance estimation

i.i.d

1. Introduction
Consider a data matrix Xnp comprising of n sample realizations from a p dimensional Gaus-
sian distribution with zero mean and positive denite covariance matrix S pp (unknown), that is,
 MVN(0,S ), i = 1, . . . , n. The task is to estimate the unknown S based on the n samples. 1
xi
regularized Sparse Inverse Covariance Selection also known as graphical lasso (Friedman et al.,
2007; Banerjee et al., 2008; Yuan and Lin, 2007) estimates the covariance matrix S
, under the as-
sumption that the inverse covariance matrix, that is, S 1 is sparse. This is achieved by minimizing
the regularized negative log-likelihood function:

minimize

Q (cid:23)0

 logdet(Q ) + tr(SQ ) + l

i, j |Q

i j|,

(1)

where S is the sample covariance matrix. Problem (1) is a convex optimization problem in the
(l ) denote the solution to (1). We note that (1)
variable Q
can also be used in a more non-parametric fashion for any positive semi-denite input matrix S, not
necessarily a sample covariance matrix of a MVN sample as described above.

(Boyd and Vandenberghe, 2004). Let bQ

. Also in the Department of Health, Research and Policy

c(cid:13)2012 Rahul Mazumder and Trevor Hastie.

(cid:229)
MAZUMDER AND HASTIE

A related criterion to (1) is one where the diagonals are not penalizedby substituting S 
S + l Ipp in the unpenalized problem we get (1). In this paper we concentrate on problem (1).
Developing efcient large-scale algorithms for (1) is an active area of research across the elds
of Convex Optimization, Machine Learning and Statistics. Many algorithms have been proposed
for this task (Friedman et al., 2007; Banerjee et al., 2008; Lu, 2009, 2010; Scheinberg et al., 2010;
Yuan, 2009, for example). However, it appears that certain special properties of the solution to
(1) have been largely ignored. This paper is about one such (surprising) propertynamely estab-
lishing an equivalence between the vertex-partition induced by the connected components of the
(l ) and the thresholded sample covariance matrix S. This paper is not about
a specic algorithm for the problem (1)it focuses on the aforementioned observation that leads
to a novel thresholding/screening procedure based on S. This provides interesting insight into the
}l 0 obtained by solving (1), over a path of l values. The behavior of
}l 0 can be completely
understood by simple screening rules on S. This can be done without even attempting to solve (1)
arguably a very challenging convex optimization problem. Furthermore, this thresholding rule can
be used as a wrapper to enormously boost the performance of existing algorithms, as seen in our
experiments. This strategy becomes extremely effective in solving large problems over a range of
values of l sufciently restricted to ensure sparsity and the separation into connected components.
Of course, for sufciently small values of l
there will be no separation into components, and hence
no computational savings.

non-zero pattern of bQ
path of solutions {bQ
the connected-components obtained from the non-zero patterns of {bQ

(l )

(l )

At this point we introduce some notation and terminology, which we will use throughout the

paper.

1.1 Notations and Preliminaries

For a matrix Z, its (i, j)th entry is denoted by Zi j.

We also introduce some graph theory notations and denitions (Bollobas, 1998) sufcient for
this exposition. A nite undirected graph G on p vertices is given by the ordered tuple G = (V , E),
where V is the set of nodes and E the collection of (undirected) edges. The edge-set is equiv-
alently represented via a (symmetric) 0-1 matrix1 (also known as the adjacency matrix) with p
rows/columns. We use the convention that a node is not connected to itself, so the diagonals of the
adjacency matrix are all zeros. Let |V | and |E| denote the number of nodes and edges respectively.
We say two nodes u, v  V are connected if there is a path between them. A maximal connected
subgraph2 is a connected component of the graph G. Connectedness is an equivalence relation that
decomposes a graph G into its connected components {(V, E )}1Kwith G = K
=1(V, E ),
where K denotes the number of connected components. This decomposition partitions the vertices
V of G into {V}1K. Note that the labeling of the components is unique upto permutations
on {1, . . . , K}. Throughout this paper we will often refer to this partition as the vertex-partition
If the size of a component is one, that is, |V| =
induced by the components of the graph G.

1, we say that the node is isolated. Suppose a graph bG dened on the set of vertices V admits
=1(bV,bE ). We say the vertex-
the following decomposition into connected components: bG = bK

1. 0 denotes absence of an edge and 1 denotes its presence.
2. G = (V , E) is a subgraph of G if V   V and E  E.

782

E (l )

i j =( 1 ifbQ

(l )
i j

6= 0, i 6= j;

0 otherwise.

(2)

EXACT THRESHOLDING FOR GRAPHICAL LASSO

partitions induced by the connected components of G and bG are equal if bK = K and there is a
permutation p on {1, . . . , K} such that bVp () = V for all   {1, . . . , K}.

The paper is organized as follows. Section 2 describes the covariance graph thresholding idea
along with theoretical justication and related work, followed by complexity analysis of the algo-
rithmic framework in Section 3. Numerical experiments appear in Section 4, concluding remarks in
Section 5 and the proofs are gathered in the Appendix A.

2. Methodology: Exact Thresholding of the Covariance Graph

(l ) to (1) gives rise to the symmetric edge matrix/skeleton

The sparsity pattern of the solution bQ

 {0, 1}pp dened by:

The above denes a symmetric graph G (l ) = (V , E (l )), namely the estimated concentration graph
(Cox and Wermuth, 1996; Lauritzen, 1996) dened on the nodes V = {1, . . . , p} with edges E (l ).

Suppose the graph G (l ) admits a decomposition into k (l ) connected components:

G (l ) = 

k (l )
=1 G (l )



,

(3)



, E (l )

 = (bV (l )

 ) are the components of the graph G (l ). Note that k (l )  {1, . . . , p}, with

where G (l )
k (l ) = p (large l ) implying that all nodes are isolated and for small enough values of l
only one component, that is, k (l ) = 1.
on the entries of the sample covariance matrix S and obtain a graph edge skeleton E(l )  {0, 1}pp

We now describe the simple screening/thresholding rule. Given l

, we perform a thresholding

, there is

dened by:

E(l )

i j =(cid:26) 1 if |Si j| > l

0 otherwise.

, i 6= j;

(4)

The symmetric matrix E(l ) denes a symmetric graph on the nodes V = {1, . . . , p} given by G(l ) =
(V , E(l )). We refer to this as the thresholded sample covariance graph. Similar to the decomposi-
tion in (3), the graph G(l ) also admits a decomposition into connected components:

G(l ) = 

=1G(l )
k(l )



,

(5)

where G(l )

 = (V (l )



, E(l )

 ) are the components of the graph G(l ).

Note that the components of G (l ) require knowledge ofbQ

(l )the solution to (1). Construction
of G(l ) and its components require operating on San operation that can be performed completely
independent of the optimization problem (1), which is arguably more expensive (See Section 3). The
surprising message we describe in this paper is that the vertex-partition of the connected components
of (5) is exactly equal to that of (3).

This observation has the following consequences:

1. We obtain a very interesting property of the path of solutions {bQ

}l 0the behavior of the
connected components of the estimated concentration graph can be completely understood by
simple screening rules on S.

(l )

783

MAZUMDER AND HASTIE

2. The cost of computing the connected components of the thresholded sample covariance graph
(5) is orders of magnitude smaller than the cost of tting graphical models (1). Furthermore,
the computations pertaining to the covariance graph can be done off-line and are amenable to
parallel computation (See Section 3).

3. The optimization problem (1) completely separates into k(l ) separate optimization
sub-problems of the form (1). The sub-problems have size equal to the number of nodes
in each component pi := |Vi|, i = 1, . . . , k(l ). Hence for certain values of l
, solving prob-
lem (1) becomes feasible although it may be impossible to operate on the p p dimensional
(global) variable Q on a single machine.

4. Suppose that for l 0, there are k(l 0) components and the graphical model computations are
distributed.3 Since the vertex-partitions induced via (3) and (5) are nested with increasing l
(see Theorem 2), it sufces to operate independently on these separate machines to obtain the

path of solutions {bQ

(l )

for all l  l 0.

}l

5. Consider a distributed computing architecture, where every machine allows operating on a
graphical lasso problem (1) of maximal size pmax. Then with relatively small effort we can
nd the smallest value of l = l pmax, such that there are no connected components of size
larger than pmax. Problem (1) thus splits up independently into manageable problems across
the different machines. When this structure is not exploited the global problem (1) remains
intractable.

The following theorem establishes the main technical contribution of this paperthe equivalence
of the vertex-partitions induced by the connected components of the thresholded sample covariance
graph and the estimated concentration graph.
Theorem 1 For any l > 0, the components of the estimated concentration graph G (l ), as dened
in (2) and (3) induce exactly the same vertex-partition as that of the thresholded sample covari-
ance graph G(l ), dened in (4) and (5). That is k (l ) = k(l ) and there exists a permutation p on
{1, . . . , k(l )} such that:

(6)

Proof The proof of the theorem appears in Appendix A.1.

i = V (l )

p (i), i = 1, . . . , k(l ).

bV (l )

Since the decomposition of a symmetric graph into its connected components depends upon the
ordering/ labeling of the components, the permutation p appears in Theorem 1.

Remark 1 Note that the edge-structures within each block need not be preserved. Under a match-
ing reordering of the labels of the components of G (l ) and G(l ):

 = V (l )



the edge-sets E (l )

 and E(l )

 are not necessarily equal.

for every xed  such that bV (l )

3. Distributing these operations depend upon the number of processors available, their capacities, communication lag,
the number of components and the maximal size of the blocks across all machines. These of-course depend upon the
computing environment. In the context of the present problem, it is often desirable to club smaller components into
a single machine.

784

EXACT THRESHOLDING FOR GRAPHICAL LASSO

Theorem 1 leads to a special property of the path-of-solutions to (1), that is, the vertex-partition
. This is the content of

induced by the connected components of G (l ) are nested with increasing l
the following theorem.

Theorem 2 Consider two values of the regularization parameter such that l > l
 > 0, with corre-
sponding concentration graphs G (l ) and G (l
) as in (2) and connected components (3). Then the
vertex-partition induced by the components of G (l ) are nested within the partition induced by the
components of G (l
 }1k (l ) forms a ner

) and the vertex-partition {bV (l )

Proof The proof of this theorem appears in the Appendix A.2.

). Formally, k (l )  k (l
}1k (l

).

)

resolution of {bV (l



Remark 2 It is worth noting that Theorem 2 addresses the nesting of the edges across connected
components and not within a component. In general, the edge-set E (l ) of the estimated concentra-
tion graph need not be nested as a function of l :
for l > l

).

, in general, E (l ) 6 E (l

See Friedman et al. (2007, Figure 3), for numerical examples demonstrating the non-monotonicity
of the edge-set across l

, as described in Remark 2.

2.1 Node-Thresholding

If l  max j6=i|Si j|, then the
A simple consequence of Theorem 1 is that of node-thresholding.
ith node will be isolated from the other nodes, the off-diagonal entries of the ith row/column are
(l )
all zero, that is, max j6=i|bQ
i j | = 0. Furthermore, the ith diagonal entries of the estimated covari-
ance and precision matrices are given by (Sii + l ) and
, respectively. Hence, as soon as
l  maxi=1,...,p{max j6=i|Si j|}, the estimated covariance and precision matrices obtained from (1)

1
Sii+l

are both diagonal.

2.2 Related Work

Witten et al. (2011) independently discovered block screening as described in this paper. At the
time of our writing, an earlier version of their paper was available (Witten and Friedman, 2011); it
proposed a scheme to detect isolated nodes for problem (1) via a simple screening of the entries of
S, but no block screening. Earlier, Banerjee et al. (2008, Theorem 4) made the same observation
about isolated nodes. The revised manuscript (Witten et al., 2011) that includes block screening
became available shortly after our paper was submitted for publication.

Zhou et al. (2011) use a thresholding strategy followed by re-tting for estimating Gaussian
graphical models. Their approach is based on the node-wise lasso-regression procedure of Mein-
shausen and Buhlmann (2006). A hard thresholding is performed on the 1-penalized regression
coefcient estimates at every node to obtain the graph structure. A restricted MLE for the concen-
tration matrix is obtained for the graph. The proposal in our paper differs since we are interested in
solving the GLASSO problem (1).

785

MAZUMDER AND HASTIE

3. Computational Complexity

The overall complexity of our proposal depends upon (a) the graph partition stage and (b) solving
(sub)problems of the form (1). In addition to these, there is an unavoidable complexity associated
with handling and/or forming S.

The cost of computing the connected components of the thresholded covariance graph is fairly
negligible when compared to solving a similar sized graphical lasso problem (1)see also our sim-
ulation studies in Section 4. In case we observe samples xi  
p, i = 1, . . . , n the cost for creating the
sample covariance matrix S is O(n p2). Thresholding the sample covariance matrix costs O(p2).
Obtaining the connected components of the thresholded covariance graph costs O(|E(l )| + p) (Tar-
jan, 1972). Since we are interested in a region where the thresholded covariance graph is sparse
enough to be broken into smaller connected components|E(l )|  p2. Note that all computations
pertaining to the construction of the connected components and the task of computing S can be
computed off-line. Furthermore the computations are parallelizable. Gazit (1991, for example)
describes parallel algorithms for computing connected components of a graphthey have a time

complexity O(log(p)) and require O((|E(l )| + p)/log(p)) processors with space O(p +|E(l )|).

There are a wide variety of algorithms for the task of solving (1). While an exhaustive review
of the computational complexities of the different algorithms is beyond the scope of this paper, we
provide a brief summary for a few algorithms below.

Banerjee et al. (2008) proposed a smooth accelerated gradient based method (Nesterov, 2005)
) to obtain an e accurate solutionthe per iteration cost being O(p3). They

with complexity O( p4.5
also proposed a block coordinate method which has a complexity of O(p4).

The complexity of the GLASSO algorithm (Friedman et al., 2007) which uses a row-by-row
block coordinate method is roughly O(p3) for reasonably sparse-problems with p nodes. For denser
problems the cost can be as large as O(p4).

The algorithm SMACS proposed in Lu (2010) has a per iteration complexity of O(p3) and an

overall complexity of O( p4

e ) to obtain an e > 0 accurate solution.

It appears that most existing algorithms for (1), have a complexity of at least O(p3) to O(p4)
or possibly larger, depending upon the algorithm used and the desired accuracy of the solution
making computations for (1) almost impractical for values of p much larger than 2000.

It is quite clear that the role played by covariance thresholding is indeed crucial in this context.
Assume that we use a solver of complexity O(pJ) with J  {3, 4}, along with our screening proce-
dure. Suppose for a given l
, the thresholded sample covariance graph has k(l ) componentsthe
total cost of solving these smaller problems is then (cid:229)
|J)  O(pJ), with J  {3, 4}. This
difference in practice can be enormoussee Section 4 for numerical examples. This is what makes
large scale graphical lasso problems solvable!

i=1 O(|V (l )

k(l )

i

4. Numerical Examples

In this section we show via numerical experiments that the screening property helps in obtaining
many fold speed-ups when compared to an algorithm that does not exploit it. Section 4.1 considers
synthetic examples and Section 4.2 discusses real-life microarray data-examples.

786

e
EXACT THRESHOLDING FOR GRAPHICAL LASSO

4.1 Synthetic Examples

Experiments are performed with two publicly available algorithm implementations for the problem
(1):

GLASSO: The algorithm of Friedman et al. (2007). We used the MATLAB wrapper available at
http://www-stat.stanford.edu/tibs/glasso/index.html to the Fortran code. The
specic criterion for convergence (lack of progress of the diagonal entries) was set to 105
and the maximal number of iterations was set to 1000.

SMACS: denotes the algorithm of Lu (2010). We used the MATLAB implementation
smooth_covsel available at http://people.math.sfu.ca/zhaosong/Codes/SMOOTH_
COVSEL/. The criterion for convergence (based on duality gap) was set to 105 and the max-
imal number of iterations was set to 1000.

We will like to note that the convergence criteria of the two algorithms GLASSO and SMACS are not
the same. For obtaining the connected components of a symmetric adjacency matrix we used the
MATLAB function graphconncomp. All of our computations are done in MATLAB 7.11.0 on a 3.3
GhZ Intel Xeon processor.

The simulation examples are created as follows. We generated a block diagonal matrix given by
S = blkdiag( S1, . . . , SK), where each block S = 1ppa matrix of all ones and (cid:229)
 p = p. In the
examples we took all ps to be equal to p1 (say). Noise of the form s
UU (U is a p p matrix with
i.i.d. standard Gaussian entries) is added to S such that 1.25 times the largest (in absolute value) off
block-diagonal (as in the block structure of S) entry of s
UU equals the smallest absolute non-zero
entry in S, that is, one. The sample covariance matrix is S = S + s
We consider a number of examples for varying K and p1 values, as shown in Table 1. Sizes were
chosen such that it is at-least conceivable to solve (1) on the full dimensional problem, without
screening. In all the examples shown in Table 1, we set l
I := (l max + l min)/2, where for all values
in the interval [l min,l max] the thresh-holded version of the sample covariance matrix has exactly
of l
K connected components. We also took a larger value of l
II := l max, which gave sparser
estimates of the precision matrix but the number of connected components were the same.

, that is, l

UU.

The computations across different connected blocks could be distributed into as many machines.
This would lead to almost a K fold improvement in timings, however in Table 1 we report the timings
by operating serially across the blocks. The serial loop across the different blocks are implemented
in MATLAB.

Table 1 shows the rather remarkable improvements obtained by using our proposed covariance
thresholding strategy as compared to operating on the whole matrix. Timing comparisons between
GLASSO and SMACS are not fair, since GLASSO is written in Fortran and SMACS in MATLAB.
However, we note that our experiments are meant to demonstrate how the thresholding helps in
improving the overall computational time over the baseline method of not exploiting screening.
Clearly our proposed strategy makes solving larger problems (1), not only feasible but with quite
attractive computational time. The time taken by the graph-partitioning step in splitting the thresh-
olded covariance graph into its connected components is negligible as compared to the timings for
the optimization problem.

787

MAZUMDER AND HASTIE

Algorithm Timings (sec)
with
screen
11.1
12.31

without
screen
25.97
137.45

p1 / p

Algorithm

K

2

200 / 400

2

500 /1000

5

300 /1500

5

500 /2500

8

300 /2400

I

II

I

II

I

II

I

II

I

II

GLASSO
SMACS

GLASSO
SMACS
GLASSO
SMACS

GLASSO
SMACS
GLASSO
SMACS

GLASSO
SMACS
GLASSO
SMACS

GLASSO
SMACS
GLASSO
SMACS

GLASSO
SMACS

1.687
10.01
305.24

175

29.8
272.6
210.86
63.22

10.47
219.72
1386.9

493

17.79
354.81
692.25
185.75

9.07
153.55

Ratio
Speedup

factor
2.33
11.16

2.83
4.20
2.40
12.21

4.08
4.57
6.82
95.88

28.04
27.58

-
-

Time (sec)

graph
partition

0.04

0.066

0.247

0.35

0.18

0.123

0.71

0.018

0.713

0.023

4.783
42.08
735.39
2138*

121.8
1247.1
1439
6062*

293.63
6061.6

-
-

963.92

54.18

-
-
-

-
-
-

842.7

-

92.91

-

Table 1: Table showing (a) the times in seconds with screening, (b) without screening, that is, on
the whole matrix and (c) the ratio (b)/(a)Speedup factor for algorithms GLASSO and
SMACS. Algorithms with screening are operated seriallythe times reect the total time
summed across all blocks. The column graph partition lists the time for computing the
connected components of the thresholded sample covariance graph. Since l
I, the
former gives sparser models.
* denotes the algorithm did not converge within 1000
iterations. - refers to cases where the respective algorithms failed to converge within 2
hours.

II > l

4.2 Micro-array Data Examples

The graphical lasso is often used in learning connectivity networks in gene-microarray data (Fried-
man et al., 2007, see for example). Since in most real examples the number of genes p is around
tens of thousands, obtaining an inverse covariance matrix by solving (1) is computationally imprac-
tical. The covariance thresholding method we propose easily applies to these problemsand as we

788

l
l
l
l
l
l
l
l
l
l
l
EXACT THRESHOLDING FOR GRAPHICAL LASSO

see gracefully delivers solutions over a large range of the parameter l
. We study three different
micro-array examples and observe that as one varies l
from large to small values, the thresholded
covariance graph splits into a number of non-trivial connected components of varying sizes. We
continue till a small/moderate value of l when the maximal size of a connected component gets
larger than a predened machine-capacity or the computational budget for a single graphical lasso
problem. Note that in relevant micro-array applications, since p  n (n, the number of samples is
at most a few hundred) heavy regularization is required to control the variance of the covariance
estimatesso it does seem reasonable to restrict to solutions of (1) for large values of l

.

Following are the data-sets we used for our experiments:

(A) This data-set appears in Alon et al. (1999) and has been analyzed by Rothman et al. (2008,
for example). In this experiment, tissue samples were analyzed using an Affymetrix Oligonu-
cleotide array. The data were processed, ltered and reduced to a subset of p = 2000 gene
expression values. The number of Colon Adenocarcinoma tissue samples is n = 62.

(B) This is an early example of an expression array, obtained from the Patrick Brown Laboratory
at Stanford University. There are n = 385 patient samples of tissue from various regions of
the body (some from tumors, some not), with gene-expression measurements for p = 4718
genes.

(C) The third example is the by now famous NKI data set that produced the 70-gene prognostic
signature for breast cancer (Van-De-Vijver et al., 2002). Here there are n = 295 samples and
p = 24481 genes.

Among the above, both (B) and (C) have few missing valueswhich we imputed by the respective
global means of the observed expression values. For each of the three data-sets, we took S to be the
corresponding sample correlation matrix. The exact thresholding methodolgy could have also been
applied to the sample covariance matrix. Since it is a common practice to standardize the genes,
we operate on the sample correlation matrix.

Figure 1 shows how the component sizes of the thresholded covariance graph change across l

.
We describe the strategy we used to arrive at the gure. Note that the connected components change
only at the absolute values of the entries of S. From the sorted absolute values of the off-diagonal
entries of S, we obtained the smallest value of l
, say l
min, for which the size of the maximal
connected component was 1500. For a grid of values of l
min, we computed the connected
components of the thresholded sample-covariance matrix and obtained the size-distribution of the
various connected components. Figure 1 shows how these components change over a range of values
of l
for the three examples (A), (B) and (C). The number of connected components of a particular
size is denoted by a color-scheme, described by the color-bar in the gures. With increasing l : the
larger connected components gradually disappear as they decompose into smaller components; the
sizes of the connected components decrease and the frequency of the smaller components increase.

till l

Since these are all correlation matrices, for l  1 all the nodes in the graph become isolated. The
range of l values for which the maximal size of the components is smaller than 1500 differ across
the three examples. For (C) there is a greater variety in the sizes of the components as compared
to (A) and (B). Note that by Theorem 1, the pattern of the components appearing in Figure 1 are
exactly the same as the components appearing in the solution of (1) for that l

.

789



0.98

(C) p=24281



1
110
1050
100800
15008000
>8000

MAZUMDER AND HASTIE

(B) p=4718

1
110
3080
200800
8001500
>1500

1

0.5

0
3
log10(Size of Components)

1.5

2.5

2

0.9

0.85

0.8

0.75

0.7

0.65

0.6



0.95

0.94

0.93

0.92

0.91

0.9

0.89

0.88

0.87

0.86



(A) p=2000



1
15
1020
20100
200800
>800

1

2

0.5

0
3
log10(Size of Components)

1.5

2.5

0.96

0.94

0.92

0.9

0.88

0.86

0.84

0.82

0.8

0.78



1

0.5

0
3
log10(Size of Components)

1.5

2.5

2

Figure 1: Figure showing the size distribution (in the log-scale) of connected components arising
from the thresholded sample covariance graph for examples (A)-(C). For every value of l
(vertical axis), the horizontal slice denotes the sizes of the different components appearing
in the thresholded covariance graph. The colors represent the number of components in
the graph having that specic size. For every gure, the range of l values is chosen such
that the maximal size of the connected components do not exceed 1500.

For examples (B) and (C) we found that the full problem sizes are beyond the scope of GLASSO
and SMACSthe screening rule is apparently the only way to obtain solutions for a reasonable range
of l -values as shown in Figure 1.

5. Conclusions

In this paper we present a novel property characterizing the family of solutions to the graphical lasso
problem (1), as a function of the regularization parameter l
. The property is fairly surprisingthe
vertex partition induced by the connected components of the non-zero pattern of the estimated
concentration matrix (at l ) and the thresholded sample covariance matrix S (at l ) are exactly equal.
This property seems to have been unobserved in the literature. Our observation not only provides
interesting insights into the properties of the graphical lasso solution-path but also opens the door
to solving large-scale graphical lasso problems, which are otherwise intractable. This simple rule
when used as a wrapper around existing algorithms leads to enormous performance boostson
occasions by a factor of thousands!

Acknowledgments

This work is supported by NSF-DMS grant 1007719.

790

l
EXACT THRESHOLDING FOR GRAPHICAL LASSO

Appendix A. Proofs

Here we provide proofs of Theorems 1 and 2.

A.1 Proof of Theorem 1

(we suppress the superscript l

for notational convenience) solves problem (1),

then standard KKT conditions of optimality (Boyd and Vandenberghe, 2004) give:

Proof Suppose bQ

|Si j cWi j|  l
cWi j = Si j + l

bQ
bQ

i j = 0;

and

i j > 0; cWi j = Si j  l bQ

i j < 0;

(7)
(8)

wherecW = (bQ )1. The diagonal entries satisfycWii = Sii + l
Using (4) and (5), there exists an ordering of the vertices {1, . . . , p} of the graph such that E(l ) is

block-diagonal. For notational convenience, we will assume that the matrix is already in that order.
Under this ordering of the vertices, the edge-matrix of the thresholded covariance graph is of the
form:

, for i = 1, . . . , p.

E(l ) =

E(l )
1
0
...
0



2

0
E(l )
...



0
...
0

0

...
E(l )
k(l )



(9)

,  = 1, . . . , k(l ).


bQ

where the different components represent blocks of indices given by: V (l )



0

We will construct a matrixcW having the same structure as (9) which is a solution to (1). Note
that ifcW is block diagonal then so is its inverse. LetcW and its inversebQ be given by:



, bQ =
Dene the block diagonal matricescW or equivalentlybQ

cW1
0 cW2

 via the following sub-problems

) + l

i j | (Q

)i j |}

(10)

0 cWk(l )
{ logdet(Q

0

...
k(l )

cW =

) + tr(SQ


0
...
0

 = argmin

bQ

...


0

...


0
...

...


1
0
...
0

bQ

bQ

0

2

...
0



for  = 1, . . . , k(l ), where S is a sub-block of S, with row/column indices from V (l )
same notation is used for Q

satises the KKT conditions(7) and (8).
Note that by construction of the thresholded sample covariance graph,

. Denote the inverses of the block-precision matrices by {bQ
and j  V (l )

 with  6= , then |Si j|  l

We will show that the abovebQ
and j  V (l )
if i  V (l )
Hence, for i  V (l )

  V (l )
}1 =cW.
i j =cWi j = 0 satises the KKT

 with  6= ; the choice bQ

conditions (7)

. The

.







for all the off-diagonal entries in the block-matrix (9).

|Si j cWi j|  l

791

Q
(cid:229)
MAZUMDER AND HASTIE

problem (1).

 satises the KKT conditions
solves

The above argument shows that the connected components obtained from the estimated preci-

By construction (10) it is easy to see that for every , the matrixbQ
(7) and (8) corresponding to the th block of the p  p dimensional problem. Hence bQ
sion graph G (l ) leads to a partition of the vertices {bV (l )
there is a   {1, . . . ,k (l )} such that bV (l )
  V (l )
Conversely, if bQ
for i  bV (l )
 with  6= ; |Si j cWi j|  l
. This
proves that the connected components of G(l ) leads to a partition of the vertices, which is ner than
the vertex-partition induced by the components of G (l ). In particular this implies that k(l )  k (l ).
Combining the above two we conclude k(l ) = k (l ) and also the equality (6). The permutation
in the theorem appears since the labeling of the connected components is not unique.

 }1k (l ) such that for every {1, . . . , k(l )},
. In particular k(l )  k (l ).

. SincecWi j = 0, we have |Si j|  l

admits the decomposition as in the statement of the theorem, then it follows

from (7) that:





and j  bV (l )

A.2 Proof of Theorem 2

Proof This proof is a direct consequence of Theorem 1, which establishes that the vertex-partitions
induced by the the connected components of the estimated precision graph and the thresholded
sample covariance graph are equal.

Observe that, by construction, the connected components of the thresholded sample covariance
graph, that is, G(l ) are nested within the connected components of G(l
). In particular, the vertex-
partition induced by the components of the thresholded sample covariance graph at l
, is contained
inside the vertex-partition induced by the components of the thresholded sample covariance graph
at l
. Now, using Theorem 1 we conclude that the vertex-partition induced by the components of
the estimated precision graph at l
 }1k (l ) is contained inside the vertex-partition
induced by the components of the estimated precision graph at l
). The
proof is thus complete.

, given by {bV (l )

}1k (l

)

, given by {bV (l



