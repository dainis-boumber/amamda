A tutorial introduction to Bayesian models of

cognitive development

Amy Perfors

School of Psychology, University of Adelaide

Joshua B. Tenenbaum

Brain and Cognitive Sciences, Massachusetts Institute of Technology

Thomas L. Griths

Fei Xu

Department of Psychology, University of California, Berkeley

1

Report Documentation Page

Form Approved

OMB No. 0704-0188

Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and
maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information,
including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington
VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it
does not display a currently valid OMB control number.

1. REPORT DATE
2011

2. REPORT TYPE

4. TITLE AND SUBTITLE
A tutorial introduction to Bayesian models of cognitive development

6. AUTHOR(S)

7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)
Massachusetts Institute of Technology,Brain and Cognitive
Sciences,Cambridge,MA,02139

3. DATES COVERED
00-00-2011 to 00-00-2011

5a. CONTRACT NUMBER
5b. GRANT NUMBER
5c. PROGRAM ELEMENT NUMBER
5d. PROJECT NUMBER
5e. TASK NUMBER
5f. WORK UNIT NUMBER

8. PERFORMING ORGANIZATION
REPORT NUMBER

9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES)

10. SPONSOR/MONITORS ACRONYM(S)

11. SPONSOR/MONITORS REPORT
NUMBER(S)

12. DISTRIBUTION/AVAILABILITY STATEMENT
Approved for public release; distribution unlimited

13. SUPPLEMENTARY NOTES
in press

14. ABSTRACT
We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive
development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of
the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and
why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian
inference, but also include information about additional resources for those interested in the cognitive
science applications, mathematical foundations or machine learning details in more depth. In addition, we
discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive
science.

15. SUBJECT TERMS
16. SECURITY CLASSIFICATION OF:

a. REPORT

unclassified

b. ABSTRACT

unclassified

c. THIS PAGE

unclassified

17. LIMITATION OF

ABSTRACT
Same as

Report (SAR)

18. NUMBER
OF PAGES

19a. NAME OF
RESPONSIBLE PERSON

61

Standard Form 298 (Rev. 8-98)
Prescribed by ANSI Std Z39-18

Abstract

We present an introduction to Bayesian inference as it is used in probabilistic

models of cognitive development. Our goal is to provide an intuitive and accessible

guide to the what, the how, and the why of the Bayesian approach: what sorts

of problems and data the framework is most relevant for, and how and why it

may be useful for developmentalists. We emphasize a qualitative understanding

of Bayesian inference, but also include information about additional resources for

those interested in the cognitive science applications, mathematical foundations,

or machine learning details in more depth. In addition, we discuss some important

interpretation issues that often arise when evaluating Bayesian models in cognitive

science.

Keywords: Bayesian models; cognitive development

2

1

Introduction

One of the central questions of cognitive development is how we learn so much from such

apparently limited evidence. In learning about causal relations, reasoning about object

categories or their properties, acquiring language, or constructing intuitive theories,

children routinely draw inferences that go beyond the data they observe. Probabilistic

models provide a general-purpose computational framework for exploring how a learner

might make these inductive leaps, explaining them as forms of Bayesian inference.

This paper presents a tutorial overview of the Bayesian framework for studying

cognitive development. Our goal is to provide an intuitive and accessible guide to

the what, the how, and the why of the Bayesian approach: what sorts of problems

and data the framework is most relevant for, and how and why it may be useful for

developmentalists. We consider three general inductive problems that learners face,

each grounded in specic developmental challenges:

1. Inductive generalization from examples, with a focus on learning the referents of

words for object categories.

2. Acquiring inductive constraints, tuning and shaping prior knowledge from expe-

rience, with a focus on learning to learn categories.

3. Learning inductive frameworks, constructing or selecting appropriate hypothesis

spaces for inductive generalization, with applications to acquiring intuitive theo-

ries of mind and inferring hierarchical phrase structure in language.

We also discuss several general issues as they bear on the use of Bayesian models:

assumptions about optimality, biological plausibility, and what idealized models can tell

us about actual human minds. The paper ends with an appendix containing a glossary

and a collection of useful resources for those interested in learning more.

3

2 Bayesian Basics:

Inductive generalization from

examples

The most basic question the Bayesian framework addresses is how to update beliefs and

make inferences in light of observed data. In the spirit of Marrs (1982) computational-

level of analysis, it begins with understanding the logic of the inference made when

generalizing from examples, rather than the algorithmic steps or specic cognitive pro-

cesses involved. A central assumption is that degrees of belief can be represented as

probabilities: that our conviction in some hypothesis h can be expressed as a real num-

ber ranging from 0 to 1, where 0 means something like h is completely false and 1

that h is completely true. The framework also assumes that learners represent prob-

ability distributions and that they use these probabilities to represent uncertainty in

inference. These assumptions turn the mathematics of probability theory into an engine

of inference, a means of weighing each of a set of mutually exclusive and exhaustive
hypotheses H to determine which best explain the observed data. Probability theory

tells us how to compute the degree of belief in some hypothesis hi, given some data d.

Computing degrees of belief as probabilities depends on two components. One,

called the prior probability and denoted P (hi), captures how much we believe in hi
prior to observing the data d. The other, called the likelihood and denoted P (d|hi),

captures the probability with which we would expect to observe the data d if hi were

true. These combine to yield the posterior probability of hi, given via Bayes Rule:

P (hi|d) =

(cid:80)

P (d|hi)P (hi)
hjH P (d|hj)P (hj)

.

(1)

As we will see, the product of priors and likelihoods often has an intuitive interpretation.

It balances between a sense of plausibility based on background knowledge on one hand

and the data-driven sense of a suspicious coincidence on the other.

In the spirit

of Ockhams Razor, it expresses the tradeo between the intrinsic complexity of an

explanation and how well it ts the observed data.

4

The denominator in Equation 1 provides a normalizing term which is the sum of

the probability of each of the possible hypotheses under consideration; this ensures that

Bayes Rule will reect the proportion of all of the probability that is assigned to any

single hypothesis hi, and (relatedly) that the posterior probabilities of all hypotheses

sum to one. This captures what we might call the law of conservation of belief: a

rational learner has a xed mass of belief to allocate over dierent hypotheses, and the

act of observing data just pushes this mass around to dierent regions of the hypothesis

space.

If the data lead us to strongly believe one hypothesis, we must decrease our

degree of belief in all other hypotheses. By contrast, if the data strongly disfavor all

but one hypothesis, then (to paraphrase Sherlock Holmes) whichever remains, however

implausible a priori, is very likely to be the truth.

To illustrate how Bayes Rule works in practice, let us consider a simple application

with three hypotheses. Imagine you see your friend Sally coughing. What could explain

this? One possibility (call it hcold) is that Sally has a cold; another (call it hcancer is

that she has lung cancer; and yet another (call it hheartburn) is that she has heartburn.

Intuitively, in most contexts, hcold seems by far the most probable, and may even be the

only one that comes to mind consciously. Why? The likelihood favors hcold and hcancer

over hheartburn, since colds and lung cancer cause coughing, while heartburn does not.

The prior, however, favors hcold and hheartburn over hcancer: lung cancer is thankfully rare,

while colds and heartburn are common. Thus the posterior probability  the product

of these two terms  is high only for hcold.

The intuitions here should be fairly clear, but to illustrate precisely how Bayes

Rule can be used to back them up, it can be helpful to assign numbers.1 Let us set

the priors as follows: P (hcold) = 0.5, P (hheartburn) = 0.4, and P (hcancer) = 0.1. This

captures the intuition that colds are slightly more common than heartburn, but both are

1Note that we have assumed that these are the only possible hypotheses, and that exactly one
applies. That is why the priors are much higher than the base rates of these diseases.
In a real
setting, there would be many more diseases under consideration, and each would have much lower
prior probability. They would also not be mutually exclusive. Adding such details would make the
math more complex but not change anything else, so for clarity of exposition we consider only the
simplied version.

5

signicantly more common than cancer. We can set our likelihoods to be the following:
P (d|hcold) = 0.8, P (d|hcancer) = 0.9, and P (d|hheartburn) = 0.1. This captures the

intuition that both colds and cancer tend to lead to coughing, and heartburn generally

does not. Plugging this into Bayes Rule gives:

P (hcold|d) =

P (d|hcold)P (hcold) + P (d|hcancer)P (hcancer) + P (d|hheartburn)P (hheartburn)

P (d|hcold)P (hcold)

=

=

(0.8)(0.5)

(0.8)(0.5) + (0.9)(0.1) + (0.1)(0.4)

0.4

0.4 + 0.09 + 0.04

= 0.7547.

Thus, the probability that Sally is coughing because she has a cold is much higher than

the probability of either of the other two hypotheses we considered. Of course, these

inferences could change with dierent data or in a dierent context. For instance, if the

data also included coughing up blood, chest pain, and shortness of breath, you might

start to consider lung cancer as a real possibility: the likelihood now explains that data

better than a cold would, which begins to balance the low prior probability of cancer

in the rst place. On the other hand, if you had other information about Sally  e.g.,

that she had been smoking two packs of cigarettes per day for 40 years  then it might

raise the prior probability of lung cancer in her case. Bayes Rule will respond to these

changes in the likelihood or the prior in a way that accords with our intuitive reasoning.

The Bayesian framework is generative, meaning that observed data are assumed

to be generated by some underlying process or mechanism responsible for creating the

data. In the example above, data (symptoms) are generated by an underlying illness.

More cognitively, words in a language may be generated by a grammar of some sort, in

combination with social and pragmatic factors. In a physical system, observed events

may be generated by some underlying network of causal relations. The job of the learner

is to evaluate dierent hypotheses about the underlying nature of the generative process,

and to make predictions based on the most likely ones. A probabilistic model is simply

a specication of the generative processes at work, identifying the steps (and associated

6

probabilities) involved in generating data. Both priors and likelihoods are typically

describable in generative terms.

To illustrate how the nature of the generative process can aect a learners inference,

consider another example, also involving illness. Suppose you observe that 80% of the

people around you are coughing. Is this a sign that a new virus is going around? Your

inference will depend on how those data were generated  in this case, whether it is a

random sample (composed, say, of people that you saw on public transport) or a non-

random one (composed of people you see sitting in the waiting room at the doctors

oce). The data are the same  80% of people are coughing  regardless of how it was

generated, but the inferences are very dierent: you are more likely to conclude that

a new virus is going around if you see 80% of people on the bus coughing. A doctors

oce full of coughing people means little about whether a new virus is going around,

since doctors oces are never full of healthy people.

How can the logic of Bayesian inference, illustrated here with these medical exam-

ples, apply to problems like word and concept learning, the acquisition of language,

or learning about causality or intuitive theories? In these cases, there is often a huge

space of hypotheses (possibly an innite one). It may not be clear how the models in

question should be interpreted generatively, since they seem to delineate sets (e.g., the

set of instances in a concept, the set of grammatical sentences, or the set of phenomena

explained by a theory). Here we illustrate how Bayesian inference works more generally

in the context of a simple schematic example. We will build on this example throughout

the paper, and see how it applies and reects problems of cognitive interest.

Our simple example, shown graphically in Figure 1, uses dots to represent individual

data points (e.g., words or events) generated independently from some unknown process

(e.g., a language or a causal network) that we depict in terms of a region or subset of

space: the process generates data points randomly within its region, never outside.

Just as each of the hypotheses in the medical example above (i.e., cold, heartburn,

or cancer) are associated with dierent data (i.e., symptoms), each hypothesis here

7

(i)

(ii)

Figure 1: (i) Example data and hypothesis. Graphical representation of data and one
possible hypothesis about how those data were generated. There are three hypotheses
here, each corresponding to a single rectangle. The black data points can only be
generated by the solid or the dashed rectangle. A new data point in position a might
be generated if the dashed rectangle is correct, but not the solid or dotted one. (ii) Some
hypotheses in the hypothesis space for this example. Hypotheses consist of rectangles;
some are well-supported by the data and some are not.

encodes a dierent idea about which subset of space the data are drawn from. Figure

1(i) depicts three possible hypotheses, each consisting of a single rectangle in the space:

hsolid corresponds to the solid line, hdashed to the dashed line, and hdotted to the dotted

line. Before seeing data, a learner might have certain beliefs about which hypothesis is

most likely; perhaps they believe that all are equally likely, or they have a bias to prefer

smaller or larger rectangles. These prior beliefs, whatever they are, would be captured

in the prior probability of each hypothesis: P (hsolid), P (hdashed), and P (hdotted). The

dierent hypotheses also yield dierent predictions about what data one would expect

to see; in Figure 1(i), the data are consistent with hsolid and hdashed, but not hdotted,

since some of the points are not within the dotted rectangle. This would be reected in
their likelihoods; P (d|hsolid) and P (d|hdashed) would both be non-zero, but P (d|hdotted)

would be zero. Bayesian inference can also yield predictions about about unobserved

data. For instance, one would only observe new data at position a if hdashed is correct,
since P (a|hsolid) = 0, but P (a|hdashed) > 0. In this sense, inferring the hypotheses most

likely to have generated the observed data guides the learner in generalizing beyond

the data to new situations.

The hypothesis space H can be thought of as the set of all possible hypotheses,

8

dened by the structure of the problem that the learner can entertain. Figure 1(ii)

shows a possible hypothesis space for our example, consisting of all possible rectangles

in this space. Note that this hypothesis space is innite in size, although just a few

representative hypotheses are shown.

The hypothesis space is dened by the nature of the learning problem, and thus

provided to the learner a priori. For instance, in our example, the hypothesis space

would be constrained by the range of possible values for the lower corner (x and y),

length (l), and width (w) of rectangular regions. Such constraints need not be very

strong or very limiting: for instance, one might simply specify that the range of possible

values for x, y, l, and w lies between 0 and some extremely large number like 109, or

be drawn from a probability distribution with a very long tail. In this sense, the prior

probability of a hypothesis P (hi) is also given by a probabilistic generative process 

a process operating one level up from the process indexed by each hypothesis that

generates the observed data points. We will see below how these hypothesis spaces and

priors need not be built in, but can be constructed or modied from experience.

In our example the hypothesis space has a very simple structure, but because a

Bayesian model can be dened for any well-specied generative framework, inference

can operate over any representation that can be specied by a generative process. This

includes, among other possibilities, probability distributions in a space (appropriate

for phonemes as clusters in phonetic space); directed graphical models (appropriate

for causal reasoning); abstract structures including taxonomies (appropriate for some

aspects of conceptual structure); objects as sets of features (appropriate for catego-

rization and object understanding); word frequency counts (convenient for some types

of semantic representation); grammars (appropriate for syntax); argument structure

frames (appropriate for verb knowledge); Markov models (appropriate for action plan-

ning or part-of-speech tagging); and even logical rules (appropriate for some aspects of

conceptual knowledge). The appendix contains a detailed list of papers that use these

and other representations.

9

The representational exibility of Bayesian models allows us to move beyond some

of the traditional dichotomies that have shaped decades of research in cognitive de-

velopment: structured knowledge vs. probabilistic learning (but not both), or innate

structured knowledge vs.

learned unstructured knowledge (but not the possibility of

knowledge that is both learned and structured). As a result of this exibility, tradi-

tional critiques of connectionism that focus on their inability to adequately capture

compositionality and systematicity (e.g., Fodor & Pylyshyn, 1988) do not apply to

Bayesian models. In fact, there are several recent examples of Bayesian models that

embrace language-like or compositional representations in domains ranging from causal

induction (Griths & Tenenbaum, 2009) to grammar learning (Perfors, Tenenbaum,

& Regier, submitted) to theory acquisition (Kemp, Tenenbaum, Niyogi, & Griths,

2010).

2.1 A case study: learning names for object categories

To illustrate more concretely how this basic Bayesian analysis of inductive generalization

applies in cognitive development, consider the task a child faces in learning names for

object categories. This is a classic instance of the problem of induction in cognitive

development, as many authors have observed. Even an apparently simple word like

dog can refer to a potentially innite number of hypotheses, including all dogs, all

Labradors, all mammals, all animals, all pets, all four-legged creatures, all dogs except

Chihuahuas, all things with fur, all running things, etc. Despite the sheer number of

possible extensions of the word, young children are surprisingly adept at acquiring the

meanings of words  even when there are only a few examples, and even when there is

no systematic negative evidence (Markman, 1989; Bloom, 2000).

How do children learn word meanings so well, so quickly? One suggestion is that in-

fants are born equipped with strong prior knowledge about what sort of word meanings

are natural (Carey, 1978; Markman, 1989), which constrains the possible hypotheses

considered. For instance, even if a child is able to rule out part-objects as possible

10

Figure 2: Schematic view of hypotheses about possible extensions considered by the
learner in Xu & Tenenbaum (2007); because the taxonomy is hierarchical, the hypothe-
ses are nested within each other. Figure reproduced from Xu & Tenenbaum (2007).

extensions, she cannot know what level of a taxonomy the word applies: whether dog

actually refers to dogs, mammals, Labradors, canines, or living beings. One solution

would be to add another constraint  the presumption that count nouns map preferen-

tially to the basic level in a taxonomy (Rosch, Mervis, Gray, Johnson, & Boyes-Braem,

1976). This preference would allow children to learn names for basic-level categories,

but would be counterproductive for every other kind of word.

Xu and Tenenbaum (2007b) present a Bayesian model of word learning that oers

a precise account of how learners could make meaningful generalizations from one or a

few examples of a novel word. This problem can be schematically depicted as in Figure

2: for concepts that are organized in a hierarchical taxonomy, labelled examples are

consistent with multiple dierent extensions. For instance, a single label Labrador

could pick out only Labradors, but it could also pick out dogs, animals, or living things.

This problem is faced by a child who, shown one or many objects with a given label,

must decide which hypothesis about possible extensions of the label is best. Intuitively,

11

(i)

(ii)

Figure 3: Learning object words.
(i) Hypothesis space that is conceptually similar
to that in Figure 2, now depicted as a two-dimensional dot diagram; hypotheses with
higher probability are darker rectangles. With one data point, many hypotheses have
some support. (ii) With three examples, the most restrictive hypothesis is much more
strongly favored.

we would expect that when given one object, a reasonable learner should not strongly

prefer any of the hypotheses that include it, though the more restricted ones might be

slightly favored. If the learner were shown three examples, we would expect the most

closely-tting hypothesis to be much more strongly preferred. For instance, given one

Labrador as an example of a fep, it is unclear whether fep refers to Labradors,

dogs, mammals, or animals. But if given three Labradors as the rst three examples of

fep, it would be quite surprising  a highly suspicious coincidence  if fep in fact

referred to a much more general class such as all dogs.

The same problem is depicted more abstractly in the dot diagram in Figure 3. Su-

perordinate hypotheses (e.g., animal) are represented as larger rectangles. Sometimes

they fully enclose smaller rectangles (corresponding to more subordinate hypotheses),

just as the extension of animals includes all Labradors. Sometimes they can also cross-

cut each other, just as the extension of pets includes many (but not all) Labradors.

The smaller rectangles represent hypotheses with smaller extensions, and we can use

this to understand how Bayesian reasoning captures the notion of a suspicious coin-

cidence, explaining the tendency to increasingly favor the smallest hypothesis that is

consistent with the data as the number of data points increases.

This ability emerges due to the likelihood p(d|h), the probability of observing the

12

data d assuming hypothesis h is true. In general, more restrictive hypotheses, corre-

sponding to smaller regions in the data space, receive more likelihood for a given piece

of data.

If a small hypothesis is the correct extension of a word, then it is not too

surprising that the examples occur where they do; a larger hypothesis could be consis-

tent with the same data points, but explains less well exactly why the data fall where

they do. The more data points we observe falling in the same small region, the more

of a suspicious coincidence it would be if in fact the words extension corresponded to

a much larger region.

More formally, if we assume that data are sampled uniformly at random from all

cases consistent with the concept, then the probability of any single data point d con-

sistent with h is inversely proportional to the size of the region h picks out  call this

the size of h. This is why when there is one data point, as in Figure 3(i), there is a

slight preference for the most restrictive (smallest) hypothesis; however, the preference

is only slight, because it could still easily have been generated by any of the hypothe-

ses that include it. But if multiple data points are generated independently from the

concept, as in Figure 3(ii), the likelihood of h with n consistent examples is inversely

proportional to the size of h, raised to the nth power. Thus the preference for smaller

consistent hypotheses over larger hypotheses increases exponentially with the number

of examples, and the most restrictive consistent hypothesis is strongly favored. This

assumption is often referred to as the size principle (Tenenbaum & Griths, 2001).

The math behind the size principle is best understood concretely if we think about

the hypotheses as discrete subsets of possible objects we might observe, such as bags

of colored marbles, rather than as continuous regions such as rectangular regions in a

two-dimensional space. Suppose bag A contains two marbles (a red and a green) and

bag B contains three (a red, a green, and a yellow). The probability of pulling the red

marble out of bag A is 1

2 = 0.5, since there are two possible marbles to choose from.

The probability of pulling the red marble out of bag B is 1

3 = 0.33 for similar reasons.

Thus, if you know only that a red marble has been pulled out of a bag (but not which

13

bag it is), you might have a weak bias to think that it was pulled out of bag A, which

is 0.5

0.33 = 1.67 times as likely as bag B.

Now suppose that someone draws out the following series of marbles, shaking the

bag fully between each draw: red, green, red, green. At this point most people would

be more certain that the bag is A. The size principle explains why. If the probability

of pulling one red (or green) marble from bag A is 1

2, the probability of pulling that

specic series of marbles is 1

2 = 1

24 = 1

16 = 0.0625, since each draw is

2  1

2  1

2  1

independent. By contrast, the probability of drawing those marbles from bag B is

3  1

3  1

3  1

1

3 = 1

34 = 1

91 = 0.0109. This means that bag A is now 0.0625

0.0109 = 5.73 times

as likely as B. In essence, the slight preference for the smaller bag is magnied over

many draws, since it becomes an increasingly unlikely coincidence for only red or green

marbles to be drawn if there is also a yellow one in there. This can be magnied if

the number of observations increases still further (e.g., consider observing a sequence

of red, green, red, green, green, green, red, green, red, red, green) or the relative size

of the bags changes (e.g., suppose the observations are still red, green, red, green, but

that the larger bag contains six marbles, each of a dierent color, rather than three).

In either case bag A is now preferred to bag B by over a factor of 80, and there is little

doubt that the marbles were drawn from bag A. In a similar way, a small hypothesis

makes more precise predictions; thus, if the data are consistent with those predictions,

then the smaller hypothesis is favored.

The size principle explains how it is possible to make strong inferences based on very

few examples. It also captures the notion of a suspicious coincidence: as the number

of examples increases, hypotheses that make specic predictions  those with more

explanatory power  tend to be favored over those that are more vague. This provides

a natural solution to the no negative evidence problem: deciding among hypotheses

given positive-only examples. As the size of the data set approaches innity, a Bayesian

learner rejects larger or more overgeneral hypotheses in favor of more precise ones. With

limited amounts of data, the Bayesian approach can make more subtle predictions, as

14

the graded size-based likelihood trades o against the preference for simplicity in the

prior. The likelihood in Bayesian learning can thus be seen as a principled quantitative

measure of the weight of implicit negative evidence  one that explains both how and

when overgeneralization should occur.

The results of Xu and Tenenbaum (2007b) reect this idea. Adults and 3- and

4-year-old children were presented with 45 objects distributed across three dierent

superordinate categories (animals, vegetables, and vehicles), including many basic-level

and subordinate-level categories within those. Subjects were then shown either one or

three labelled examples of a novel word such as fep, and were asked to pick out the

other feps from the set of objects. Both children and adults responded dierently

depending on how many examples they were given. Just as in Figure 3, with one

example, people and the model both showed graded generalization from subordinate

to superordinate matches. By contrast, when given three examples, generalizations

became much sharper and were usually limited to the most restrictive level.

This also illustrates how assumptions about the nature of the generative process

aect the types of inferences that can be made. We have seen that people tend show

restricted generalizations on the basis of three examples; however, this only if they

think the experimenter was choosing those examples sensibly (i.e., as examples of the

concept). If people think the data were generated in some other way  for instance, an-

other learner was asking about those particular pictures  then their inferences change

(Xu & Tenenbaum, 2007a). In this case, the lack of non-Labradors no longer reects

something the experimenter can control; though it is a coincidence, it is not a suspi-

cious one. The data are the same, but the inference changes as the generative process

underlying the data changes. In other words, the size principle applies in just those

cases where the generative process is such that data are generated from the concept

(or, more generally, hypothesis) itself.

So far we have illustrated how Bayesian inference can capture generalization from

just a few examples, the simultaneous learning of overlapping extensions, and the use

15

of implicit negative evidence. All of these are important, but it is also true that we

have built in a great deal, including a restricted and well-specied hypothesis space.

Very often, human learners must not make reasonable specic generalizations within a

set hypothesis space, they also much be able to make generalizations about what sort

of generalizations are reasonable. We see an example of this in the next section.

3 Acquiring inductive constraints

One of the implications of classic problems of induction is the need for generalizations

about generalizations, or inductive constraints, of some sort. The core problem is how

induction is justied based on a nite sample of any kind of data, and the inevitable

conclusion is that there must be some kind of constraint that enables learning to occur.

Nearly every domain studied by cognitive science yields evidence that children rely

on higher-level inductive constraints. Children learning words prefer to apply them

to whole objects rather than parts (Markman, 1990). Babies believe that agents are

distinct from objects in that they can move without contact (Spelke, Phillips, & Wood-

ward, 1995) and act in certain ways in response to goals (Woodward, 1998; Gergely &

Csibra, 2003). Confronted with evidence that childrens behavior is restricted in pre-

dictable ways, the natural response is to hypothesize the existence of innate constraints,

including the whole object constraint (Markman, 1990) core systems of object repre-

sentation, psychology, physics, and biology (Carey & Spelke, 1996; Spelke & Kinzler,

2007; Carey, 2009), and so on. Given that they appear so early in development, it

seems sensible to postulate that these constraints are innate rather than learned.

However, it may be possible for inductive constraints to be learned, at least in some

cases. For instance, consider the problem of learning that some features matter for

categorizing new objects while others should be ignored (e.g., Nosofsky, 1986). Acquir-

ing higher-level abstract knowledge would enable one to make correct generalizations

about an object from a completely novel category, even after seeing only one example.

A wealth of research indicates that people are capable of acquiring this sort of knowl-

16

edge, both rapidly in the lab (Nosofsky, 1986; Perfors & Tenenbaum, 2009) and over

the course of development (Landau, Smith, & Jones, 1988; L. Smith, Jones, Landau,

Gershko-Stowe, & Samuelson, 2002). Children also acquire other sorts of inductive

constraints over the course of development, including the realization that categories

may be organized taxonomically (Rosch, 1978), that some verbs occur in alternating

patterns and others dont (e.g., Pinker, 1989) or that comparative orderings should be

transitive (Shultz & Vogel, 2004).

How can an inductive constraint be learned, and how might a Bayesian framework

explain this? Is it possible to acquire an inductive constraint faster than the specic

hypotheses it is meant to constrain? If not, how can we explain peoples learning in

some situations? If so, what principles explain this acquisition?

A familiar example of the learning of inductive constraints was provided by Goodman

(1955). Suppose we have many bags of colored marbles and discover by drawing samples

that some bags seem to have black marbles, others have white marbles, and still others

have red or green marbles. Every bag is uniform in color; no bag contains marbles of

more than one color. If we draw a single marble from a new bag in this population

and observe a color never seen before  say, purple  it seems reasonable to expect that

other draws from this same bag will also be purple. Before we started drawing from

any of these bags, we had much less reason to expect that such a generalization would

hold. The assumption that color is uniform within bags is a learned overhypothesis,

an acquired inductive constraint. The ability to infer such a constraint is not in itself

a solution to the ultimate challenges of induction; it rests on other, arguably deeper

assumptions  that the new bag is like the previous bags we have seen in relevant

ways. Yet it is still a very useful piece of abstract knowledge that guides subsequent

generalizations and can itself be induced from experience.

We can illustrate a similar idea in the rectangle world by imagining a learner who

is shown the schematic data in Figure 4(i). Having seen point a only, the learner has

no way to decide whether b or c is more likely to be in the same category or region as

17

(i)

(ii)

Figure 4: Learning higher-order information. (i) Given point a, one cannot identify
whether b or c is more likely.
(ii) Given additional data, a model that could learn
higher-order information about hypotheses might favor regions that tend to be long,
thin rectangles oriented along the y axis (i.e., regions for which the length l tends to
be short, the width w tends to be long, and the location (x and y coordinates) can
be nearly anywhere). If this is the case, points a and b are probably within the same
region, but a and c are not.

a. However, if the learner has also seen the data in Figure 4(ii), they might infer both

rst-order and second-order knowledge about the data set. First-order learning refers

to the realization that the specic rectangular regions constitute the best explanation

for the data points seen so far; second-order (overhypothesis) learning would involve

realizing that the regions tend to be long, thin, and oriented along the y-axis. Just

as learning the how categories are organized helps children generalize from new items,

this type of higher-order inference helps with the interpretation of novel data, leading

to the realization that point b is probably in the same region as a but point c is not,

even though b and c are equidistant from a.

A certain kind of Bayesian model, known as a hierarchical Bayesian model (HBM),

can learn overhypotheses by not only choosing among specic hypotheses, but by also

making higher-order generalizations about those hypotheses. As weve already seen, in

a non-hierarchical model, the modeler sets the range of the parameters that dene the

hypotheses. In a hierarchical model, the modeler instead species hyperparameters 

parameters dening the parameters  and the model learns the range of the parameters

themselves. So rather than being given that the range of each of the the lower corner

18

(x, y), length l, and width w values lies between 0 and 109, a hierarchical model might

instead learn the typical range of each (e.g., that l tends to be short while w tends to

be long, as depicted in Figure 4(ii)) while the modeler species the range of the ranges.

The idea of wiring in abstract knowledge at higher levels of hierarchical Bayesian

models may seem reminiscent of nativist approaches, but several key features t well

with empiricist intuitions about learning. The top level of knowledge in an HBM is

prespecied, but every level beneath that can be learned. As one moves up the hierarchy,

knowledge becomes increasingly abstract and imposes increasingly weak constraints on

the learners specic beliefs at the lower levels. Thus, a version of the model that

learns at higher levels builds in weaker constraints than a version that learns only at

lower levels. By adding further levels of abstraction to an HBM while keeping pre-

specied parameters to a minimum, at the highest levels of the model, we can come

increasingly close to the classical empiricist proposal for the bottom-up, data-driven

origins of abstract knowledge.

Although the precise mathematical details of any HBM are too complex to go into

detail here, we can give a simplied example designed to motivate how it might be

possible to learn on multiple levels simultaneously.

Imagine you are faced with the

marble example described earlier. We can capture this problem by saying that for each

bag b, you have to learn the distribution of colors in the bag: call this distribution b.

At the same time, you want to make two inferences about the sets of bags as a whole:

how uniform colors tend to be within bags (call this ) and what sorts of colors exist

overall (call this ). Here,  and  are the hyperparameters of each of the b values,

since how likely any particular bag is will depend on the higher-level assumptions about

bags: if you think, for instance, that colors tend to be uniform within bags, then a bag

with lots of marbles of dierent colors in it will be low probability relative to a bag

with only one. We can use this fact to learn on the higher level as well. A Bayesian

model that sees three bags, all uniform in color, will search for the setting of , , and

 that make the observed data most probable; this will correspond to  values that

19

tend to favor uniformity within bags, and  and  values that capture the observed

color distributions. The Bayesian model learns these things simultaneously in the sense

that it seeks to maximize the joint probability of all of the parameters, not just the

lower-level ones.

This example is a simplied description of one of the existing hierarchical Bayesian

models for category learning (Kemp, Perfors, & Tenenbaum, 2007); there are sev-

eral other HBMs for the same underlying problem (Navarro, 2006; Griths, Sanborn,

Canini, & Navarro, 2008; Heller, Sanborn, & Chater, 2009). Though they dier in

many particulars, what all of these models have in common is that they can perform

inference on multiple levels of abstraction. When presented with data consisting of

specic objects and features, these models are capable of making generalizations about

the specic objects as well as the appropriate generalizations about categorization in

general. For instance, children in an experiment by L. Smith et al. (2002) were pre-

sented with four novel concepts and labels and acquired a bias to assume not only that

individual categories like chairs tend to be organized by shape, but also that categories

of solid artifacts in general are as well. A hierarchical Bayesian model can make the

same generalization on the basis of the same data (Kemp, Perfors, & Tenenbaum, 2007).

A surprising eect of learning in hierarchical models is that, quite often, the higher-

order abstractions are acquired before all of the specic lower-level details:

just as

children acquire some categorization biases even before they have learned all categories,

the model might infer parameter values such that l tends to be short and w tends to be

long, signicantly before the size and location of each rectangular region is learned with

precision. This eect, which we might call the blessing of abstraction2, is somewhat

counterintuitive. Why are higher-order generalizations like this sometimes easier for a

Bayesian learner to acquire?

One reason is that the higher-level hypothesis space is often smaller than the lower-

level ones. As a result, the model has to choose between fewer options at the higher level,

which may require less evidence. In our rectangle example, the higher-level knowledge

2We thank Noah Goodman for this coinage.

20

may consist of only three options: l and w are approximately equal, l is smaller than

w, or w is smaller than l. Even if a learner doesnt know whether l is 10 units or 11

units long and w is 20 or 22, it might be fairly obvious that l is smaller than w.

More generally, the higher-level inference concerns the lower-level hypothesis space

(and is therefore based on the data set as a whole), whereas the lower-level inference is

only relevant for specic data points. A single data point is informative only about the

precise size and location of a single region. However, it  and every other single data

point  is informative about all of the higher-level hypotheses. There is, in eect, more

evidence available to the higher levels than the lower ones, and they can therefore be

learned quite quickly.

Is there empirical evidence that people acquire higher-level abstract knowledge at

the same time as, or before, lower-level specic knowledge? Adult laboratory category

learning studies indicate that generalizations on the basis of abstract knowledge occurs

at least as rapidly as lower-level generalizations (Perfors & Tenenbaum, 2009). There is

also some indication that children show an abstract to concrete shift in both biological

knowledge (Simons & Keil, 1995) and categorization, tending to dierentiate global,

super-ordinate categories before basic level kinds (Mandler & McDonough, 1993). Even

infants have been shown to have the capacity to form overhypotheses given a small

amount of data, providing initial evidence that the mechanisms needed for rapidly

acquired inductive constraints exist early in development (Dewar & Xu, in press).

There is also a great deal of research that demonstrates the existence of abstract

knowledge before any concrete knowledge has been acquired. For instance, the core

knowledge research program suggests that before children learn about many specic

physical objects or mental states, they have abstract knowledge about physical objects

and intentional agents in general (e.g., Spelke & Kinzler, 2007). The core knowledge

view suggests that the presence of this abstract knowledge so early in development, and

before the existence of specic knowledge, implies that the abstract knowledge must be

innate in some meaningful sense. More broadly, the basic motivation for positing innate

21

constraints on cognitive development is that without these constraints, children would

be unable to infer the specic knowledge that they seem to acquire from the limited data

available to them. What is critical to the argument is that some constraints are present

prior to learning some of the specic data, not that those constraints must be innate.

Approaches to cognitive development that emphasize learning from data typically view

the course of development as a progressive layering of increasingly abstract knowledge

on top of more concrete representations; under such a view, learned abstract knowledge

would tend to come in after more specic concrete knowledge is learned, so the former

could not usefully constrain the latter.

This view is sensible in the absence of explanations that can capture how abstract

constraints could be learned together with (or before) the more specic knowledge they

are needed to constrain. However, the hierarchical Bayesian framework provides such

an explanation (or, at minimum, evidence that such a thing is possible). A model

with the capability of acquiring abstract knowledge of a certain form3 can identify

what abstract knowledge is best supported by the data by learning which values of

hyper-parameters (like  and ) are the most probable given the data seen so far. If

an abstract generalization like this can be acquired very early and can function as a

constraint on later acquisition of specic data, it may function eectively as if it were an

innate domain-specic constraint, even if it is in fact not innate and instead is acquired

by domain-general induction from data.

In sum, then, hierarchical Bayesian models oer a valuable tool for exploring ques-

tions of innateness due to the ability to limit built-in knowledge to increasingly abstract

levels and thereby learn inductive constraints at other levels. As we will see in the next

section, the Bayesian framework is also a useful way of approaching these questions for

another reason  their ability to evaluate the rational tradeo between the simplicity

of a hypothesis and its goodness-of-t to the evidence in the world. Because of this,

Bayesian learners can make inferences that otherwise appear to go beyond the amount

3See Section for a more thorough discussion of how this degree of supervision is consistent with a

non-innatist perspective, and is in fact impossible to avoid in any model of learning.

22

of evidence available.

4 Developing inductive frameworks

The hierarchical Bayesian models described above explain the origins of inductive biases

and constraints by tuning priors in response to data observed from multiple settings

or contexts. But the acquisition of abstract knowledge often appears more discrete or

qualitative  more like constructing an appropriate hypothesis space, or selecting an ap-

propriate hypothesis space from a higher level hypothesis space of hypothesis spaces.

Consider the theory theory view of cognitive development. Childrens knowledge

about the world is organized into intuitive theories with a structure and function anal-

ogous to scientic theories (Carey, 1985; Gopnik & Meltzo, 1997; Karmilo-Smith,

1988; Keil, 1989). The theory serves as an abstract framework that guides inductive

generalization at more concrete levels of knowledge, by generating a space of hypothe-

ses. Intuitive theories have been posited to underlie real-world categorization (Murphy

& Medin, 1985), causal induction (Waldmann, 1996; Griths & Tenenbaum, 2009),

biological reasoning (Atran, 1995; Inagaki & Hatano, 2002; Medin & Atran, 1999),

physical reasoning (McCloskey, 1983) and social interaction (Nichols & Stich, 2003;

Wellman, 1990). For instance, an intuitive theory of mind generates hypotheses about

how a specic agents behavior might be explained in particular situations  candidate

