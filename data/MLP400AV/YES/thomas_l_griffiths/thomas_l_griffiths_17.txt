Abstract

People routinely make sophisticated causal inferences unconsciously, ef-
fortlessly, and from very little data  often from just one or a few ob-
servations. We argue that these inferences can be explained as Bayesian
computations over a hypothesis space of causal graphical models, shaped
by strong top-down prior knowledge in the form of intuitive theories. We
present two case studies of our approach, including quantitative mod-
els of human causal judgments and brief comparisons with traditional
bottom-up models of inference.

1 Introduction

People are remarkably good at inferring the causal structure of a system from observations
of its behavior. Like any inductive task, causal inference is an ill-posed problem: the data
we see typically underdetermine the true causal structure. This problem is worse than
the usual statisticians dilemma that correlation does not imply causation. Many cases of
everyday causal inference follow from just one or a few observations, where there isnt even
enough data to reliably infer correlations! This fact notwithstanding, most conventional
accounts of causal inference attempt to generate hypotheses in a bottom-up fashion based
on empirical correlations. These include associationist models [12], as well as more recent
rational models that embody an explicit concept of causation [1,3], and most algorithms
for learning causal Bayes nets [10,14,7].

Here we argue for an alternative top-down approach, within the causal Bayes net frame-
work. In contrast to standard bottom-up approaches to structure learning [10,14,7], which
aim to optimize or integrate over all possible causal models (structures and parameters),
we propose that people consider only a relatively constrained set of hypotheses determined
by their prior knowledge of how the world works. The allowed causal hypotheses not only
form a small set of all possible causal graphs, but also instantiate specic causal mecha-
nisms with constrained conditional probability tables, rather than much more general con-
ditional dependence and independence relations.

The prior knowledge that generates this hypothesis space of possible causal models can be
thought of as an intuitive theory, analogous to the scientic theories of classical mechan-
ics or electrodynamics that generate constrained spaces of possible causal models in their
domains. Following the suggestions of recent work in cognitive development (reviewed
in [4]), we take the existence of strong intuitive theories to be the foundation for human
causal inference. However, our view contrasts with some recent suggestions [4,11] that


an intuitive theory may be represented as a causal Bayes net model. Rather, we consider
a theory to be the underlying principles that generate the range of causal network models
potentially applicable in a given domain  the abstractions that allow a learner to construct
and reason with appropriate causal network hypotheses about novel systems in the presence
of minimal perceptual input.

Given the hypothesis space generated by an intuitive theory, causal inference then follows
the standard Bayesian paradigm: weighing each hypothesis according to its posterior prob-
ability and averaging their predictions about the system according to those weights. The
combination of Bayesian causal inference with strong top-down knowledge is quite power-
ful, allowing us to explain peoples very rapid inferences about model complexity in both
static and temporally extended domains. Here we present two case studies of our approach,
including quantitative models of human causal judgments and brief comparisons with more
bottom-up accounts.

2 Inferring hidden causal powers

We begin with a paradigm introduced by Gopnik and Sobel for studying causal infer-
ence in children [5]. Subjects are shown a number of blocks, along with a machine 
the blicket detector. The blicket detector activates  lights up and makes noise  when-
ever a blicket is placed on it. Some of the blocks are blickets, others are not, but their
outward appearance is no guide. Subjects observe a series of trials, on each of which one
or more blocks are placed on the detector and the detector activates or not. They are then
asked which blocks have the hidden causal power to activate the machine.

and

Gopnik and Sobel have demonstrated various conditions under which children successfully
infer the causal status of blocks from just one or a few observations. Of particular interest
is their backwards blocking condition [13]: on trial 1 (the 1-2 trial), children observe
two blocks (
) placed on the detector and the detector activates. Most children

is placed on
now say that both
the detector alone and the detector activates. Now all children say that
is a blicket, and
is not a blicket. Intuitively, this is a kind of explaining away: seeing
most say that
is sufcient to activate the detector alone explains away the previously observed
that
association of

are blickets. On trial 2 (the 1 alone trial),

with detector activation.


and





and

#

#

and







but no edge

are on the detector;

	
represents the hypothesis that

Gopnik et al. [6] suggest that childrens causal reasoning here may be thought of in terms
, that is
of learning the structure of a causal Bayes net. Figure 1a shows a Bayes net,
represent whether
consistent with childrens judgments after trial 2. Variables
represents whether the detector activates; the
blocks
but
existence of an edge


has the power to turn on the detector. We encode the
not
is a blicket  that


, where
if block 1 is on the detector
two observations 



and block 2, and
), likewise for
(else
if the detector is active (else
"
, standard Bayes net learning algorithms
Given only the data 
$$
%
have no way to converge on subjectss choice
. The data are not sufcient to compute
()
the conditional independence relations required by constraint-based methods [9,13], 1 nor
to strongly inuence the Bayesian structural score using arbitrary conditional probability
tables [7]. Standard psychological models of causal strength judgment [12,3], equivalent
to maximum-likelihood parameter estimates for the family of Bayes nets in Figure 1a [15],
either predict no explaining away here or make no prediction due to insufcient data.

but not

as vectors

$%&'

 

).

1Gopnik et al. [6] argue that constraint-based learning could be applied here, if we supplement the
observed data with large numbers of ctional observations. However, this account does not explain
why subjects make the inferences that they do from the very limited data actually observed, nor why
they are justied in doing so. Nor does it generalize to the three experiments we present here.































!


!



!





is a blicket but

Alternatively, reasoning on this task could be explained in terms of a simple logical deduc-
tion. We require as a premise the activation law: a blicket detector activates if and only if
,
one or more blickets are placed on it. Based on the activation law and the data 
we can deduce that
remains undetermined. If we further assume a
form of Occams razor, positing the minimal number of hidden causal powers, then we can
infer that
is not a blicket, as most children do. Other cases studied by Gopnik et al. can
be explained similarly. However, this deductive model cannot explain many plausible but
nondemonstrative causal inferences that people make, or peoples degrees of condence in
their judgments, or their ability to infer probabilistic causal relationships from noisy data
[3,12,15]. It also leaves mysterious the origin and form of Occams razor. In sum, neither
deductive logic nor standard Bayes net learning provides a satisfying account of peoples
rapid causal inferences. We now show how a Bayesian structural inference based on strong
top-down knowledge can explain the blicket detector judgments, as well as several proba-
bilistic variants that clearly exceed the capacity of deductive accounts.

Most generally, the top-down knowledge takes the form of a causal theory with at least two
components: an ontology of object, attribute and event types, and a set of causal principles
relating these elements. Here we treat theories only informally; we are currently developing
a formal treatment using the tools of probabilistic relational logic (e.g., [9]). In the basic
blicket detector domain, we have two kinds of objects, blocks and machines; two relevant
attributes, being a blicket and being a blicket detector; and two kinds of events, a block
being placed on a machine and a machine activating. The causal principle relating these
events and attributes is just the activation law introduced above. Instead of serving as a
premise for deductive inference, the causal law now generates a hypothesis space of causal
Bayes nets for statistical inference. This space is quite restricted: with two objects and one
detector, there are only 4 consistent hypotheses
(Figure 1a). The
are also determined by the theory. Based
conditional probabilities for each hypothesis

if
;
on the activation law,
otherwise it equals 0.

	
and


, or

&

and



	

	





















	



(


to the set of hypotheses consistent with

Causal inference then follows by Bayesian updating of probabilities over
in light of
the observed data
. We assume independent observations so that the total likelihood
, the individual-trial
factors into separate terms for individual trials. For all hypotheses in
likelihoods also factor into
, and we can ignore the
	




assuming that block positions are independent of the
last two terms



is 1 for any hypothesis consistent
causal structure. The remaining term

with the data and 0 otherwise, because of the deterministic activation law. The posterior
for any data set
is then simply the restriction and renormalization of the prior





Backwards blocking proceeds as follows. After the 1-2 trial (
be a blicket: the consistent hypotheses are

), only
(
remain consistent. The prior over causal structures

written as

probability
lows (all others are zero):

), at least one block must
. After the 1 alone trial
and
can be
  , assuming that each block has some independent

of being a blicket. The nonzero posterior probabilities are then given as fol-
,




. 2

!#"
!%&
!$
!%'&

. Finally, the
!#"

probability that block
may be computed by averaging the
&!("
predictions of all consistent hypotheses weighted their posterior probabilities:


.

.

2More generally, we could allow for some noise in the detector, by letting the likelihood
:<;=>:	?(=A@CB'DFE be probabilistic rather than deterministic. For simplicity we consider only the noise-

less case here; a low level of noise would give similar results.


, and

-,

2/
.



	

)
!$

is a blicket
!$


!$




.

'


&*!#"

&


465879




1



!("

, and

,



.







!#"

!$


	



	%

&





,

!$

0/

.





3/

)




















































































































!
%








!
%









!
%
!
%


+



























#






; then, after the 1 alone trial,





-,
#

. Setting


is probably not (


In comparing with human judgments in the backwards blocking paradigm, the relevant
, the baseline judgments before either block is placed on
probabilities are

the detector;
, judgments after the 1-2 trial; and
,

#

judgments after the 1 alone trial. These probabilities depend only on the prior proba-
bility of blickets,
qualitatively matches childrens backwards block-
ing behavior: after the 1-2 trial, both blocks are more likely than not to be blickets
(
is denitely a blicket while

). Thus there is no need to posit a special

Occams razor just to explain why
becomes like less likely to be a blicket after the 1
alone trial  this adjustment follows naturally as a rational statistical inference. However,
we do have to assume that blickets are somewhat rare (
). Following the 1 alone
), because the unambiguous
trial the probability of
second trial explains away all the evidence for
,


block 2 would remain likely to be a blicket even after the 1 alone trial.
In order to test whether human causal reasoning actually embodies this Bayesian form of
Occams razor, or instead a more qualitative rule such as the classical version, Entities
should not be multiplied beyond necessity, we conducted three new blicket-detector ex-
periments on both adults and 4-year-old children (in collaboration with Sobel & Gopnik).
The rst two experiments were just like the original backwards blocking studies, except
that we manipulated subjects estimates of
by introducing a pretraining phase. Subjects
rst saw 12 objects placed on the detector, of which either 2, in the rare condition,
or 10, in the common condition, activated the detector. We hypothesized that this ma-
nipulation would lead subjects to set their subjective prior for blickets to either

or
, and thus, if guided by the Bayesian Occams razor, to show strong or weak
blocking respectively.



being a blicket returns to baseline (

from the rst trial. Thus for






	










We gave adult subjects a different cover story, involving super pencils and a superlead
detector, but here we translate the results into blicket detector terms. Following the rare
were picked at random from the same
or common training, two new objects
pile and subjects were asked three times to judge the probability that each one could activate
the detector: rst, before seeing it on the detector, as a baseline; second, after a 1-2 trial;
third, after a 1 alone trial. Probabilities were judged on a 1-7 scale and then rescaled to
the range 0-1.

and





and

The mean adult probability judgments and the model predictions are shown in Figures 2a
(rare) and 2b (common). Wherever two objects have the same pattern of observed contin-
at baseline and after the 1-2 trial), subjects mean judgments
gencies (e.g.,
were found not to be signicantly different and were averaged together for this analysis. In
to match subjects baseline judgments; the best-tting val-
tting the model, we adjusted
ues were very close to the true base rates. More interestingly, subjects judgments tracked
the Bayesian model over both trials and conditions. Following the 1-2 trial, mean ratings
of both objects increased above baseline, but more so in the rare condition where the activa-
tion of the detector was more surprising. Following the 1 alone trial, all subjects in both
conditions were 100% sure that
had the power to activate the detector, and the mean
rating of
returned to baseline: low in the rare condition, but high in the common con-
dition. Four-year-old children made yes/no judgments that were qualitatively similar,
across both rare and common conditions [13].



Human causal inference thus appears to follow rational statistical principles, obeying the
Bayesian version of Occams razor rather than the classical logical version. However, an
alternative explanation of our data is that subjects are simply employing a combination of
logical reasoning and simple heuristics. Following the 1 alone trial, people could log-
ically deduce that they have no information about the status of
and then fall back on
the base rate of blickets as a default, without the need for any genuinely Bayesian com-
putations. To rule out this possibility, our third study tested causal explaining way in the






,






,












,

































,

 ) hypotheses

absence of unambiguous data that could be used to support deductive reasoning. Subjects
again saw the rare pretraining, but now the critical trials involved three objects,
,
. After judging the baseline probability that each object could activate the detector,

subjects saw two trials: a 1-2 trial, followed by a 1-3 trial, in which objects
and
activated the detector together. The Bayesian hypothesis space is analogous to Figure
1a, but now includes eight (
representing all possible assignments of
causal powers to the three objects. As before, the prior over causal structures
can

 , the likelihood
be written as
reduces
6





(under the activation law) and 0 otherwise, and
to 1 for any hypothesis consistent with
may be computed by summing the
is a blicket
the probability that block
.


posterior probabilities of all consistent hypotheses, e.g.,
.
.


Figure 2c shows that the Bayesian models predictions and subjects mean judgments match
well except for a slight overshoot in the model. Following the 1-3 trial, people judge that
probably activates the detector, but now with less than 100% condence. Correspond-

ingly, the probability that
activates the detector increases, to a level above baseline but below 0.5. All of these pre-
dicted effects are statistically signicant (

activates the detector decreases, and the probability that

, one-tailed paired t-tests).


)




0/

1






'










!

These results provide strong support for our claim that rapid human inferences about causal
structure can be explained as theory-guided Bayesian computations. Particularly striking
is the contrast between the effects of the 1 alone trial and the 1-3 trial. In the former
case, subjects observe unambiguously that
falls completely to baseline; in the latter, they observe only a suspicious coincidence and
so explaining away is not complete. A logical deductive mechanism might generate the
all-or-none explaining-away observed in the former case, while a bottom-up associative
learning mechanism might generate the incomplete effect seen in the latter case, but only
our top-down Bayesian approach naturally explains the full spectrum of one-shot causal
inferences, from uncertainty to certainty.

is a cause and their judgment about

3 Causal inference in perception

Our second case study argues for the importance of causal theories in a very different
domain: perceiving the mechanics of collisions and vibrations. Michottes [8] studies of
causal perception showed that a moving ball coming to rest next to a stationary ball would
be perceived as the cause of the latters subsequent motion only if there was essentially no
gap in space or time between the end of the rst balls motion and the beginning of the sec-
ond balls. The standard explanation is that people have automatic perceptual mechanisms
for detecting certain kinds of physical causal relations, such as transfer of force, and these
mechanisms are driven by simple bottom-up cues such as spatial and temporal proximity.

Figure 3a shows data from an experiment described in [2] which might appear to support
this view. Subjects viewed a computer screen depicting a long horizontal beam. At one end
of the beam was a trap door, closed at the beginning of each trial. On each trial, a heavy
, the trap door
block was dropped onto the beam at some position
opened and a ball ew out. Subjects were told that the block dropping on the beam might
have jarred loose a latch that opens the door, and they were asked to judge (on a
scale)
how likely it was that the block dropping was the cause of the door opening. The distance
separating these two events were varied across trials. Figure 3a shows that

, and after some time 

 	

increases, the judged probability of a causal link decreases.

and time 

as either

or 

Anderson [1] proposed that this judgment could be formalized as a Bayesian inference with
two alternative hypotheses:
, that no causal link exists.
He suggested that the likelihood
should be product of decreasing exponentials
in space and time,
would pre-

, while

, that a causal link exists, and













































+


,

















!





















































increases.

. The door state


-
, the door opens if and only if the noise amplitude

and
sumably be constant. This model has three free parameters  the decay constants
, and the prior probability
 plus multiplicative and additive scaling parameters to
bring the model ouptuts onto the same range as the data. Figure 3c shows that this model
can be adjusted to t the broad outlines of the data, but it misses the crossover interac-
tion: in the data, but not the model, the typical advantage of small distances
over large
distances disappears and even reverses as 
This crossover may reect the presence of a much more sophisticated theory of force trans-
fer than is captured by the spatiotemporal decay model. Figure 1b shows a causal graphical
structure representing a simplied physical model of this situation. The graph is a dynamic
Bayes net (DBN), enabling inferences about the systems behavior over time. There are
four basic event types, each indexed by time
can be either open
(
), and once open it stays open. There is an intrinsic source
) or closed (

-
"
in the door mechanism, which we take to be i.i.d., zero-mean gaussian. At
of noise

each time step
exceeds some
threshold (which we take to be 1 without loss of generality). The block hits the beam at
position
), setting up a vibration in the door mechanism with en-
!

. We assume this energy decreases according to an inverse power law with the
ergy
-
distance between the block and the door,
,
absorbing it into the parameter
below.) For simplicity, we assume that energy propagates
instantaneously from the block to the door (plausible given the speed of sound relative
to the distances and times used here), and that there is no vibrational damping over time
). Anderson [2] also sketches an account along these lines, although he
(
-
provides no formal model.
At time
depends strictly on the variance of the noise
 the bigger the variance, the sooner the
door should pop open. At issue is whether there exists a causal link between the vibration
 which causes the door to open. More

 . (We can always set

 caused by the block dropping  and the noise

, the door pops open; we denote this event as

. The likelihood of

(and time

-





!


!


!




(causal link) and

of the vibrational energy

precisely, we propose that causal inference is based on the probabilities
under the two hypotheses
some low intrinsic level

(no causal link). The noise variance has
 is increased by some fraction
.
6


	-

) analytically or through simulation.
We can then solve for the likelihoods
,
,
, leaving three free parameters,
We take the limit as the intrinsic noise level
, plus multiplicative and additive scaling parameters, just as in the spatiotemporal
and
decay model. Figure 3b plots the (scaled) posterior probabilities
for the best
tting parameter values. In contrast to the spatiotemporal decay model, the DBN model
captures the crossover interaction between space and time.

, which under
. That is,


 but not

-






















and

This difference between the two models is fundamental, not just an accident of the param-
eter values chosen. The spatiotemporal decay model can never produce a crossover effect
. A crossover of some form is generic in
due to its functional form  separable in 
the DBN model, because its predictions essentially follow an exponential decay function
on  with a decay rate that is a nonlinear function of
. Other mathematical models with
a nonseparable form could surely be devised to t this data as well. The strength of our
model lies in its combination of rational statistical inference and realistic physical motiva-
tion. These results suggest that whatever schema of force transfer is in peoples brains, it
must embody a more complex interaction between spatial and temporal factors than is as-
sumed in traditional bottom-up models of causal inference, and its functional form may be
a rational consequence of a rich but implicit physical theory that underlies peoples instan-
taneous percepts of causality. It is an interesting open question whether human observers
can use this knowledge only by carrying out an online simulation in parallel with their
observations, or can access it in a compiled form to interpret bottom-up spatiotemporal
cues without the need to conduct any explicit internal simulations.



















!












!































































!



















!





















4 Conclusion

In two case studies, we have explored how people make rapid inferences about the causal
texture of their environment. We have argued that these inferences can be explained best as
Bayesian computations, working over hypothesis spaces strongly constrained by top-down
causal theories. This framework allowed us to construct quantitative models of causal
judgment  the most accurate models to date in both domains, and in the blicket detec-
tor domain, the only quantitatively predictive model to date. Our models make a number
of substantive and mechanistic assumptions about aspects of the environment that are not
directly accessible to human observers. From a scientic standpoint this might seem unde-
sirable; we would like to work towards models that require the fewest number of a priori
assumptions. Yet we feel there is no escaping the need for powerful top-down constraints
on causal inference, in the form of intuitive theories. In ongoing work, we are beginning
to study the origins of these theories themselves. We expect that Bayesian learning mecha-
nisms similar to those considered here will also be useful in understanding how we acquire
the ingredients of theories: abstract causal principles and ontological types.

