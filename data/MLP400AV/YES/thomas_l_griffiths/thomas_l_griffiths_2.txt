Abstract

As the availability and importance of relational datasuch as the friendships sum-
marized on a social networking websiteincreases, it becomes increasingly im-
portant to have good models for such data. The kinds of latent structure that have
been considered for use in predicting links in such networks have been relatively
limited. In particular, the machine learning community has focused on latent class
models, adapting Bayesian nonparametric methods to jointly infer how many la-
tent classes there are while learning which entities belong to each class. We pursue
a similar approach with a richer kind of latent variablelatent featuresusing a
Bayesian nonparametric approach to simultaneously infer the number of features
at the same time we learn which entities have each feature. Our model combines
these inferred features with known covariates in order to perform link prediction.
We demonstrate that the greater expressiveness of this approach allows us to im-
prove performance on three datasets.

1 Introduction

Statistical analysis of social networks and other relational data has been an active area of research for
over seventy years and is becoming an increasingly important problem as the scope and availability
of social network datasets increase [1]. In these problems, we observe the interactions between a set
of entities and we wish to extract informative representations that are useful for making predictions
about the entities and their relationships. One basic challenge is link prediction, where we observe
the relationships (or links) between some pairs of entities in a network (or graph) and we try
to predict unobserved links. For example, in a social network, we might only know some subset of
people are friends and some are not, and seek to predict which other people are likely to get along.
Our goal is to improve the expressiveness and performance of generative models based on extracting
latent structure representing the properties of individual entities from the observed data, so we will
focus on these kinds of models. This rules out approaches like the popular p model that uses global
quantities of the graph, such as how many edges or triangles are present [2, 3]. Of the approaches
that do link prediction based on attributes of the individual entities, these can largely be classied
into class-based and feature-based approaches. There are many models that can be placed under
these approaches, so we will focus on the models that are most comparable to our approach.

1

Most generative models using a class-based representation are based on the stochastic blockmodel,
introduced in [4] and further developed in [5]. In the most basic form of the model, we assume there
are a nite number of classes that entities can belong to and that these classes entirely determine the
structure of the graph, with the probability of a link existing between two entities depending only
on the classes of those entities. In general, these classes are unobserved, and inference reduces to
assigning entities to classes and inferring the class interactions. One of the important issues that arise
in working with this model is determining how many latent classes there are for a given problem.
The Innite Relational Model (IRM) [6] used methods from nonparametric Bayesian statistics to
tackle this problem, allowing the number of classes to be determined at inference time. The Innite
Hidden Relational Model [7] further elaborated on this model and the Mixed Membership Stochastic
Blockmodel (MMSB) [8] extended it to allow entities to have mixed memberships.
All these class-based models share a basic limitation in the kinds of relational structure they natu-
rally capture. For example, in a social network, we might nd a class which contains male high
school athletes and another which contains male high school musicians. We might believe these
two classes will behave similarly, but with a class-based model, our options are to either merge the
classes or duplicate our knowledge about common aspects of them. In a similar vein, with a limited
amount of data, it might be reasonable to combine these into a single class male high school stu-
dents, but with more data we would want to split this group into athletes and musicians. For every
new attribute like this that we add, the number of classes would potentially double, quickly leading
to an overabundance of classes. In addition, if someone is both an athlete and a musician, we would
either have to add another class for that or use a mixed membership model, which would say that
the more a student is an athlete, the less he is a musician.
An alternative approach that addresses this problem is to use features to describe the entities. There
could be a separate feature for high school student, male, athlete, and musician and the
presence or absence of each of these features is what denes each person and determines their
relationships. One class of latent-feature models for social networks has been developed by [9, 10,
11], who proposed real-valued vectors as latent representations of the entities in the network where
depending on the model, either the distance, inner product, or weighted combination of the vectors
corresponding to two entities affects the likelihood of there being a link between them. However,
extending our high school student example, we might hope that instead of having arbitrary real-
valued features (which are still useful for visualization), we would infer binary features where each
feature could correspond to an attribute like male or athlete. Continuing our earlier example, if
we had a limited amount of data, we might not pick up on a feature like athlete. However, as we
observe more interactions, this could emerge as a clear feature. Instead of doubling the numbers of
classes in our model, we simply add an additional feature. Determining the number of features will
therefore be of extreme importance.
In this paper, we present the nonparametric latent feature relational model, a Bayesian nonpara-
metric model in which each entity has binary-valued latent features that inuences its relations. In
addition, the relations depend on a set of known covariates. This model allows us to simultaneously
infer how many latent features there are while at the same time inferring what features each entity
has and how those features inuence the observations. This model is strictly more expressive than
the stochastic blockmodel. In Section 2, we describe a simplied version of our model and then
the full model. In Section 3, we discuss how to perform inference. In Section 4, we illustrate the
properties of our model using synthetic data and then show that the greater expressiveness of the
latent feature representation results in improved link prediction on three real datasets. Finally, we
conclude in Section 5.

2 The nonparametric latent feature relational model

Assume we observe the directed relational links between a set of N entities. Let Y be the N  N
binary matrix that contains these links. That is, let yij  Y (i, j) = 1 if we observe a link from
entity i to entity j in that relation and yij = 0 if we observe that there is not a link. Unobserved
links are left unlled. Our goal will be to learn a model from the observed links such that we can
predict the values of the unlled entries.

2

2.1 Basic model

Y

i,j



>
j



0@X

k,k0

1A

In our basic model, each entity is described by a set of binary features. We are not given these
features a priori and will attempt to infer them. We assume that the probability of having a link
from one entity to another is entirely determined by the combined effect of all pairwise feature
If there are K features, then let Z be the N  K binary matrix where each row
interactions.
corresponds to an entity and each column corresponds to a feature such that zik  Z(i, k) = 1 if the
ith entity has feature k and zik = 0 otherwise. and let Zi denote the feature vector corresponding to
entity i. Let W be a K  K real-valued weight matrix where wkk0  W (k, k0) is the weight that
affects the probability of there being a link from entity i to entity j if both entity i has feature k and
entity j has feature k0.
We assume that links are independent conditioned on Z and W , and that only the features of entities
i and j inuence the probability of a link between those entities. This denes the likelihood

Pr(Y |Z, W ) =

Pr(yij|Zi, Zj, W )

(1)

where the product ranges over all pairs of entities. Given the feature matrix Z and weight matrix W ,
the probability that there is a link from entity i to entity j is

Pr(yij = 1|Z, W ) = 

ZiW Z

= 

zikzjk0 wkk0

(2)

1

where () is a function that transforms values on (,) to (0, 1) such as the sigmoid function
(x) =
1+exp(x) or the probit function (x) = (x). An important aspect of this model is that
all-zero columns of Z do not affect the likelihood. We will take advantage of this in Section 2.2.
This model is very exible. With a single feature per entity, it is equivalent to a stochastic block-
model. However, since entities can have more than a single feature, the model is more expressive. In
the high school student example, each feature can correspond to an attribute like male, musician,
and athlete. If we were looking at the relation friend of (not necessarily symmetric!), then the
weight at the (athlete, musician) entry of W would correspond to the weight that an athlete would be
a friend of a musician. A positive weight would correspond to an increased probability, a negative
weight a decreased probability, and a zero weight would indicate that there is no correlation between
those two features and the observed relation. The more positively correlated features people have,
the more likely they are to be friends. Another advantage of this representation is that if our data
contained observations of students in two distant locations, we could have a geographic feature for
the different locations. While other features such as athlete or musician might indicate that one
person could be a friend of another, the geographic features could have extremely negative weights
so that people who live far from each other are less likely to be friends. However, the parameters
for the non-geographic features would still be tied for all people, allowing us to make stronger in-
ferences about how they inuence the relations. Class-based models would need an abundance of
classes to capture these effects and would not have the same kind of parameter sharing.
Given the full set of observations Y , we wish to infer the posterior distribution of the feature matrix
Z and the weights W . We do this using Bayes theorem, p(Z, W|Y )  p(Y |Z, W )p(Z)p(W ),
where we have placed an independent prior on Z and W . Without any prior knowledge about the
features or their weights, a natural prior for W involves placing an independent N(0, 2
w) prior on
each wij. However, placing a prior on Z is more challenging. If we knew how many features there
were, we could place an arbitrary parametric prior on Z. However, we wish to have a exible prior
that allows us to simultaneously infer the number of features at the same time we infer all the entries
in Z. The Indian Buffet Process is such a prior.

2.2 The Indian Buffet Process and the basic generative model

As mentioned in the previous section, any features which are all-zero do not affect the likelihood.
That means that even if we added an innite number of all-zero features, the likelihood would remain
the same. The Indian Buffet Process (IBP) [12] is a prior on innite binary matrices such that with
probability one, a feature matrix drawn from it for a nite number of entities will only have a nite
number of non-zero features. Moreover, any feature matrix, no matter how many non-zero features

3

it contains, has positive probability under the IBP prior. It is therefore a useful nonparametric prior
to place on our latent feature matrix Z.
The generative process to sample matrices from the IBP can be described through a culinary
metaphor that gave the IBP its name. In this metaphor, each row of Z corresponds to a diner at an
Indian buffet and each column corresponds to a dish at the innitely long buffet. If a customer takes
a particular dish, then the entry that corresponds to the customers row and the dishs column is a one
and the entry is zero otherwise. The culinary metaphor describes how people choose the dishes. In
the IBP, the rst customer chooses a Poisson() number of dishes to sample, where  is a parameter
of the IBP. The ith customer tries each previously sampled dish with probability proportional to the
number of people that have already tried the dish and then samples a Poisson(/i) number of new
dishes. This process is exchangeable, which means that the order in which the customers enter the
restaurant does not affect the conguration of the dishes that people try (up to permutations of the
dishes as described in [12]). This insight leads to a straightforward Gibbs sampler to do posterior
inference that we describe in Section 3.
Using an IBP prior on Z, our basic generative latent feature relational model is:

Z  IBP()
wkk0  N (0, 2
w)

yij  (cid:0)ZiW Z>

j

(cid:1)

for all k, k0 for which features k and k0 are non-zero
for each observation.

2.3 Full nonparametric latent feature relational model

We have described the basic nonparametric latent feature relational model. We now combine it
with ideas from the social network community to get our full model. First, we note that there are
many instances of logit models used in statistical network analysis that make use of covariates in
link prediction [2]. Here we will focus on a subset of ideas discussed in [10]. Let Xij be a vector
that inuences the relation yij, let Xp,i be a vector of known attributes of entity i when it is the
parent of a link, and let Xc,i be a vector of known attributes of entity i when it is a child of a link.
For example, in Section 4.2, when Y represents relationships amongst countries, Xij is a scalar
representing the geographic similarity between countries (Xij = exp(d(i, j))) since this could
inuence the relationships and Xp,i = Xc,i is a set of known features associated with each country
(Xp,i and Xc,i would be distinct if we had covariates specic to each countrys roles). We then let
c be a normally distributed scalar and , p, c, a, and b be normally distributed vectors in our full
model in which

Pr(yij = 1|Z, W, X, , a, b, c) = 

ZiW Z

>
j + 

>

Xij + (

>
p Xp,i + ai) + (

>
c Xc,j + bj) + c

.

(3)





If we do not have information about one or all of X, Xp, and Xc, we drop the corresponding term(s).
In this model, c is a global offset that affects the default likelihood of a relation and ai and bj are
entity and role specic offsets.
So far, we have only considered the case of observing a single relation. It is not uncommon to
observe multiple relations for the same set of entities. For example, in addition to the friend of
relation, we might also observe the admires and collaborates with relations. We still believe that
each entity has a single set of features that determines all its relations, but these features will not
affect each relation in the same way. If we are given m relations, label them Y 1, Y 2, . . . , Y m. We
will use the same features for each relation, but we will use an independent weight matrix W i for
each relation Y i. In addition, covariates might be relation specic or common across all relations.
Regardless, they will interact in different ways in each relation. Our full model is now

Pr(Y 1, . . . , Y m|Z,{W i, X i, i, ai, bi, ci}m

i=1) =

Pr(Y i|Z, W i, X i, i, ai, bi, ci).

i=1

2.4 Variations of the nonparametric latent feature relational model

The model that we have dened is for directed graphs in which the matrix Y i is not assumed to be
symmetric. For undirected graphs, we would like to dene a symmetric model. This is easy to do by
restricting W i to be symmetric. If we further believe that the features we learn should not interact,
we can assume that W i is diagonal.

4

mY

2.5 Related nonparametric latent feature models

There are two models related to our nonparametric latent feature relational model that both use the
IBP as a prior on binary latent feature matrices. The most closely related model is the Binary Matrix
Factorization (BMF) model of [13]. The BMF is a general model with several concrete variants,
the most relevant of which was used to predict unobserved entries of binary matrices for image
reconstruction and collaborative ltering. If Y is the observed part of a binary matrix, then in this
variant, we assume that Y |U, V, W  (U W V >) where () is the logistic function, U and V are
independent binary matrices drawn from the IBP, and the entries in W are independent draws from a
normal distribution. If Y is an N  N matrix where we assume the rows and columns have the same
features (i.e., U = V ), then this special case of their model is equivalent to our basic (covariate-free)
model. While [13] were interested in a more general formalization that is applicable to other tasks,
we have specialized and extended this model for the task of link prediction. The other related model
is the ADCLUS model [14]. This model assumes we are given a symmetric matrix of nonnegative
similarities Y and that Y = ZW Z> +  where Z is drawn from the IBP, W is a diagonal matrix
with entries independently drawn from a Gamma distribution, and  is independent Gaussian noise.
This model does not allow for arbitrary feature interactions nor does it allow for negative feature
correlations.

3 Inference

Exact inference in our nonparametric latent feature relational model is intractable [12]. However,
the IBP prior lends itself nicely to approximate inference via Markov Chain Monte Carlo [15]. We
rst describe inference in the single relation, basic model, later extending it to the full model. In our
basic model, we must do posterior inference on Z and W . Since with probability one, any sample
of Z will have a nite number of non-zero entries, we can store just the non-zero columns of each
sample of the innite binary matrix Z. Since we do not have a conjugate prior on W , we must also
sample the corresponding entries of W . Our sampler is as follows:

Given W , resample Z We do this by resampling each row Zi in succession. When sampling
entries in the ith row, we use the fact that the IBP is exchangeable to assume that the ith customer in
the IBP was the last one to enter the buffet. Therefore, when resampling zik for non-zero columns
k, if mk is the number of non-zero entries in column k excluding row i, then
Pr(zik = 1|Zik, W, Y )  mk Pr(Y |zik = 1, Zik, W ).

We must also sample zik for each of the innitely many all-zero columns to add features to the
representation. Here, we use the fact that in the IBP, the prior distribution on the number of new
features for the last customer is Poisson(/N). As described in [12], we must then weight this
by the likelihood term for having that many new features, computing this for 0, 1, . . . .kmax new
features for some maximum number of new features kmax and sampling the number of new features
from this normalized distribution. The main difculty arises because we have not sampled the values
of W for the all-zero columns and we do not have a conjugate prior on W , so we cannot compute
the likelihood term exactly. We can adopt one of the non-conjugate sampling approaches from the
Dirichlet process [16] to this task or use the suggestion in [13] to include a Metropolis-Hastings step
to propose and either accept or reject some number of new columns and the corresponding weights.
We chose to use a stochastic Monte Carlo approximation of the likelihood. Once the number of new
features is sampled, we must sample the new values in W as described below.

Given Z, resample W We sequentially resample each of the weights in W that correspond to
non-zero features and drop all weights that correspond to all-zero features. Since we do not have
a conjugate prior on W , we cannot directly sample W from its posterior. If () is the probit, we
adapt the auxiliary sampling trick from [17] to have a Gibbs sampler for the entries of W . If () is
the logistic function, no such trick exists and we resort to using a Metropolis-Hastings step for each
weight in which we propose a new weight from a normal distribution centered around the old one.

Hyperparameters We can also place conjugate priors on the hyperparameters  and w and per-
form posterior inference on them. We use the approach from [18] for sampling of .

5

(a)

(b)

(c)

(d)

(e)

Figure 1: Features and corresponding observations for synthetic data. In (a), we show features that
could be explained by a latent-class model that then produces the observation matrix in (b). White
indicates one values, black indicates zero values, and gray indicates held out values. In (c), we show
the feature matrix of our other synthetic dataset along with the corresponding observations in (d).
(e) shows the feature matrix of a randomly chosen sample from our Gibbs sampler.

Multiple relations
each i as above. However, when we resample Z, we must compute

In the case of multiple relations, we can sample Wi given Z independently for

Pr(zik = 1|Zik,{W, Y }m

i=1)  mk

Pr(Y i|zik = 1, Zik, W i).

mY

i=1

In the full model, we must also update {i, i

Full model
i=1. By conditioning on
these, the update equations for Z and W i take the same form, but with Equation (3) used for the
c, ai, bi, ci) are
likelihood. When we condition on Z and W i, the posterior updates for (i, i
independent and can be derived from the updates in [10].

c, ai, bi, ci}m

p, i

p, i

Implementation details Despite the ease of writing down the sampler, samplers for the IBP often
mix slowly due to the extremely large state space full of local optima. Even if we limited Z to have
K columns, there are 2N K potential feature matrices. In an effort to explore the space better, we can
augment the Gibbs sampler for Z by introducing split-merge style moves as described in [13] as well
as perform annealing or tempering to smooth out the likelihood. However, we found that the most
signicant improvement came from using a good initialization. A key insight that was mentioned
in Section 2.1 is that the stochastic blockmodel is a special case of our model in which each entity
only has a single feature. Stochastic blockmodels have been shown to perform well for statistical
network analysis, so they seem like a reasonable way to initialize the feature matrix. In the results
section, we compare the performance of a random initialization to one in which Z is initialized with
a matrix learned by the Innite Relational Model (IRM). To get our initialization point, we ran the
Gibbs sampler for the IRM for only 15 iterations and used the resulting class assignments to seed Z.

4 Results

We rst qualitatively analyze the strengths and weaknesses of our model on synthetic data, estab-
lishing what we can and cannot expect from it. We then compare our model against two class-based
generative models, the Innite Relational Model (IRM) [6] and the Mixed Membership Stochastic
Blockmodel (MMSB) [8], on two datasets from the original IRM paper and a NIPS coauthorship
dataset, establishing that our model does better than the best of those models on those datasets.

4.1 Synthetic data

We rst focus on the qualitative performance of our model. We applied the basic model to two very
simple synthetic datasets generated from known features. These datasets were simple enough that
the basic model could attain 100% accuracy on held-out data, but were different enough to address
the qualitative characteristics of the latent features inferred. In one dataset, the features were the
class-based features seen in Figure 1(a) and in the other, we used the features in Figure 1(c). The
observations derived from these features can be seen in Figure 1(b) and Figure 1(d), respectively.

6

On both datasets, we initialized Z and W randomly. With the very simple, class-based model, 50%
of the sampled feature matrices were identical to the generating feature matrix with another 25%
differing by a single bit. However, on the other dataset, only 25% of the samples were at most a
single bit different than the true matrix. It is not the case that the other 75% of the samples were bad
samples, though. A randomly chosen sample of Z is shown in Figure 1(e). Though this matrix is
different from the true generating features, with the appropriate weight matrix it predicts just as well
as the true feature matrix. These tests show that while our latent feature approach is able to learn
features that explain the data well, due to subtle interactions between sets of features and weights,
the features themselves will not in general correspond to interpretable features. However, we can
expect the inferred features to do a good job explaining the data. This also indicates that there are
many local optima in the feature space, further motivating the need for good initialization.

4.2 Multi-relational datasets

In the original IRM paper, the IRM was applied to several datasets [6]. These include a dataset
containing 54 relations of 14 countries (such as exports to and protests) along with 90 given
features of the countries [19] and a dataset containing 26 kinship relationships of 104 people in the
Alyawarra tribe in Central Australia [20]. See [6, 19, 20] for more details on the datasets.
Our goal in applying the latent feature relational model to these datasets was to demonstrate the
effectiveness of our algorithm when compared to two established class-based algorithms, the IRM
and the MMSB, and to demonstrate the effectiveness of our full algorithm. For the Alyawarra
dataset, we had no known covariates. For the countries dataset, Xp = Xc was the set of known
features of the countries and X was the country distance similarity matrix described in Section 2.3.
As mentioned in the synthetic data section, the inferred features do not necessarily have any inter-
pretable meaning, so we restrict ourselves to a quantitative comparison. For each dataset, we held
out 20% of the data during training and we report the AUC, the area under the ROC (Receiver Oper-
ating Characteristic) curve, for the held-out data [21]. We report results for inferring a global set of
features for all relations as described in Section 2.3 which we refer to as global as well as results
when a different set of features is independently learned for each relation and then the AUCs of all
relations are averaged together, which we refer to as single. In addition, we tried initializing our
sampler for the latent feature relational model with either a random feature matrix (LFRM rand)
or class-based features from the IRM (LFRM w/ IRM). We ran our sampler for 1000 iterations for
each conguration using a logistic squashing function (though results using the probit are similar),
throwing out the rst 200 samples as burn-in. Each method was given ve random restarts.

Table 1: AUC on the countries and kinship datasets. Bold identies the best performance.

Countries single
LFRM w/ IRM 0.8521  0.0035
0.8529  0.0037
LFRM rand
0.8423  0.0034
0.8212  0.0032

IRM
MMSB

Countries global Alyawarra single Alyawarra global
0.8772  0.0075
0.9183  0.0108
0.7127  0.030
0.7067  0.0534
0.8500  0.0033
0.8943  0.0300
0.8643  0.0077
0.9143  0.0097

0.9346  0.0013
0.9443  0.0018
0.9310  0.0023
0.9005  0.0022

Results of these tests are in Table 1. As can be seen, the LFRM with class-based initialization out-
performs both the IRM and MMSB. On the individual relations (single), the LFRM with random
initialization also does well, beating the IRM initialization on both datasets. However, the random
initialization does poorly at inferring the global features due to the coupling of features and the
weights for each of the relations. This highlights the importance of proper initialization. To demon-
strate that the covariates are helping, but that even without them, our model does well, we ran the
global LFRM with class-based initialization without covariates on the countries dataset and the AUC
dropped to 0.8713  0.0105, which is still the best performance.
On the countries data, the latent feature model inferred on average 5-7 features when seeded with
the IRM and 8-9 with a random initialization. On the kinship data, it inferred 9-11 features when
seeded with the IRM and 13-19 when seeded randomly.

7

(a) True relations

(b) Feature predictions

(c) IRM predictions

(d) MMSB predictions

Figure 2: Predictions for all algorithms on the NIPS coauthorship dataset. In (a), a white entry
means two people wrote a paper together. In (b-d), the lighter an entry, the more likely that algorithm
predicted the corresponding people would interact.

4.3 Predicting NIPS coauthorship

As our nal example, highlighting the expressiveness of the latent feature relational model, we used
the coauthorship data from the NIPS dataset compiled in [22]. This dataset contains a list of all
papers and authors from NIPS 1-17. We took the 234 authors who had published with the most
other people and looked at their coauthorship information. The symmetric coauthor graph can be
seen in Figure 2(a). We again learned models for the latent feature relational model, the IRM and the
MMSB training on 80% of the data and using the remaining 20% as a test set. For the latent feature
model, since the coauthorship relationship is symmetric, we learned a full, symmetric weight matrix
W as described in Section 2.4. We did not use any covariates. A visualization of the predictions for
each of these algorithms can be seen in Figure 2(b-d). Figure 2 really drives home the difference
in expressiveness. Stochastic blockmodels are required to group authors into classes, and assumes
that all members of classes interact similarly. For visualization, we have ordered the authors by
the groups the IRM found. These groups can clearly be seen in Figure 2(c). The MMSB, by
allowing partial membership is not as restrictive. However, on this dataset, the IRM outperformed
it. The latent feature relational model is the most expressive of the models and is able to much more
faithfully reproduce the coauthorship network.
The latent feature relational model also quantitatively outperformed the IRM and MMSB. We again
ran our sampler for 1000 samples initializing with either a random feature matrix or a class-based
feature matrix from the IRM and reported the AUC on the held-out data. Using ve restarts for each
method, the LFRM w/ IRM performed best with an AUC of 0.9509, the LFRM rand was next with
0.9466 and much lower were the IRM at 0.8906 and the MMSB at 0.8705 (all at most 0.013). On
average, the latent feature relational model inferred 20-22 features when initialized with the IRM
and 38-44 features when initialized randomly.

5 Conclusion

We have introduced the nonparametric latent feature relational model, an expressive nonparametric
model for inferring latent binary features in relational entities. This model combines approaches
from the statistical network analysis community, which have emphasized feature-based methods for
analyzing network data, with ideas from Bayesian nonparametrics in order to simultaneously infer
the number of latent binary features at the same time we infer the features of each entity and how
those features interact. Existing class-based approaches infer latent structure that is a special case
of what can be inferred by this model. As a consequence, our model is strictly more expressive
than these approaches, and can use the solutions produced by these approaches for initialization.
We showed empirically that the nonparametric latent feature model performs well at link prediction
on several different datasets, including datasets that were originally used to argue for class-based
approaches. The success of this model can be traced to its richer representations, which make it able
to capture subtle patterns of interaction much better than class-based models.

Acknowledgments KTM was supported by the U.S. Department of Energy contract DE-AC52-
07NA27344 through Lawrence Livermore National Laboratory. TLG was supported by grant number FA9550-
07-1-0351 from the Air Force Ofce of Scientic Research.

8

