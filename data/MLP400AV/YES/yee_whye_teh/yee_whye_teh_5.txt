Abstract

In this paper we propose a new framework
for learning from large scale datasets based
on iterative learning from small mini-batches.
By adding the right amount of noise to a
standard stochastic gradient optimization al-
gorithm we show that the iterates will con-
verge to samples from the true posterior dis-
tribution as we anneal the stepsize. This
seamless transition between optimization and
Bayesian posterior sampling provides an in-
built protection against overtting. We also
propose a practical method for Monte Carlo
estimates of posterior statistics which moni-
tors a sampling threshold and collects sam-
ples after it has been surpassed. We apply
the method to three models: a mixture of
Gaussians, logistic regression and ICA with
natural gradients.

1. Introduction

In recent years there has been an increasing amount
of very large scale machine learning datasets, ranging
from internet trac and network data, computer vi-
sion, natural language processing, to bioinformatics.
More and more advances in machine learning are now
driven by these large scale data, which oers the op-
portunity to learn large and complex models for solv-
ing many useful applied problems. Recent successes
in large scale machine learning have mostly been opti-
mization based approaches. While there are sophisti-
cated algorithms designed specically for certain types
of models, one of the most successful class of algo-
rithms are stochastic optimization, or Robbins-Monro,
algorithms. These algorithms process small (mini-

Appearing in Proceedings of the 28 th International Con-
ference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).

)batches of data at each iteration, updating model
parameters by taking small gradient steps in a cost
function. Often these algorithms are run in an on-
line setting, where the data batches are discarded af-
ter processing and only one pass through the data is
performed, reducing memory requirements drastically.
One class of methods left-behind by the recent ad-
vances in large scale machine learning are the Bayesian
methods. This has partially to do with the negative
results in Bayesian online parameter estimation (An-
drieu et al., 1999), but also the fact that each iteration
of typical Markov chain Monte Carlo (MCMC) algo-
rithms requires computations over the whole dataset.
Nevertheless, Bayesian methods are appealing in their
ability to capture uncertainty in learned parameters
and avoid overtting. Arguably with large datasets
there will be little overtting. Alternatively, as we
have access to larger datasets and more computational
resources, we become interested in building more com-
plex models, so that there will always be a need to
quantify the amount of parameter uncertainty.
In this paper, we propose a method for Bayesian learn-
ing from large scale datasets. Our method combines
Robbins-Monro type algorithms which stochastically
optimize a likelihood, with Langevin dynamics which
injects noise into the parameter updates in such a way
that the trajectory of the parameters will converge
to the full posterior distribution rather than just the
maximum a posteriori mode. The resulting algorithm
starts o being similar to stochastic optimization, then
automatically transitions to one that simulates sam-
ples from the posterior using Langevin dynamics.
In Section 2 we introduce the two ingredients of our
method:
stochastic optimization and Langevin dy-
namics. Section 3 describes our algorithm and how
it converges to the posterior distribution. Section 4
describes a practical method of estimating when our
algorithm will transition from stochastic optimization
to Langevin dynamics. Section 5 demonstrates our al-

Stochastic Gradient Langevin Dynamics

gorithm on a few models and Section 6 concludes.

2. Preliminaries

N



i=1 p(xi|).

Let  denote a parameter vector, with p() a prior
distribution, and p(x|) the probability of data item
x given our model parameterized by . The posterior
distribution of a set of N data items X = {xi}N
is: p(|X)  p()
i=1
In the optimization
literature the prior regularizes the parameters while
the likelihood terms constitute the cost function to
be optimized, and the task is to nd the maximum
. A popular class
a posteriori (MAP) parameters 
of methods called stochastic optimization (Robbins &
Monro, 1951) operates as follows. At each iteration t,
a subset of n data items Xt = {xt1, . . . , xtn} is given,
and the parameters are updated as follows:

t = t
2

 log p(t) + N

n

 log p(xti|t)

(1)

(

)

n

i=1

where t is a sequence of step sizes. The general
idea is that the gradient computed on the subset is
used to approximate the true gradient over the whole
dataset. Over multiple iterations the whole dataset
is used and the noise in the gradient caused by using
subsets rather than the whole dataset averages out.
For large datasets where the subset gradient approx-
imation is accurate enough, this can give signicant
computational savings over using the whole dataset to
compute gradients at each iteration.
To ensure convergence to a local maximum, in addition
to other technical assumptions, a major requirement
is for the step sizes to satisfy the property
t < 
2

t = 





(2)

t=1

t=1

Intuitively, the rst constraint ensures that parameters
will reach the high probability regions no matter how
far away it was initialized to, while the second ensures
that the parameters will converge to the mode instead
of just bouncing around it. Typically, step sizes t =
a(b + t) are decayed polynomially with   (0.5, 1].
The issue with ML or MAP estimation, as stochas-
tic optimization aims to do, is that they do not cap-
ture parameter uncertainty and can potentially overt
data. The typical way in which Bayesian approaches
capture parameter uncertainty is via Markov chain
Monte Carlo (MCMC) techniques (Robert & Casella,
2004). In this paper we will consider a class of MCMC
techniques called Langevin dynamics (Neal, 2010). As
before, these take gradient steps, but also injects Gaus-
sian noise into the parameter updates so that they do

not collapse to just the MAP solution:

(

t = 
2
t  N(0, )

N

i=1

 log p(t) +

 log p(xi|t)

)

+ t

(3)

The gradient step sizes and the variances of the in-
jected noise are balanced so that the variance of the
samples matches that of the posterior. Langevin dy-
namics is motivated and originally derived as a dis-
cretization of a stochastic dierential equation whose
equilibrium distribution is the posterior distribution.
To correct for discretization error, one can take (3)
to just be a proposal distribution and correct using
Metropolis-Hastings.
Interestingly, as we decrease 
the discretization error decreases as well so that the re-
jection rate approaches zero. However typical MCMC
practice is to allow an initial adaptation phase where
the step sizes are adjusted, followed by xing the step
sizes to ensure a stationary Markov chain thereafter.
More sophisticated techniques use Hamiltonian dy-
namics with momentum variables to allow parameters
to move over larger distances without the inecient
random walk behaviour of Langevin dynamics (Neal,
2010). However, to the extent of our knowledge all
MCMC methods proposed thus far require computa-
tions over the whole dataset at every iteration, result-
ing in very high computational costs for large datasets.

3. Stochastic Gradient Langevin

Dynamics

Given the similarities between stochastic gradient al-
gorithms (1) and Langevin dynamics (3), it is nat-
ural to consider combining ideas from the two ap-
proaches. This allows ecient use of large datasets
while allowing for parameter uncertainty to be cap-
tured in a Bayesian manner. The approach is straight-
forward: use Robbins-Monro stochastic gradients, add
an amount of Gaussian noise balanced with the step
size used, and allow step sizes to go to zero. The pro-
posed update is simply:

 log p(t) + N

 log p(xti|t)

(

t = t
2
t  N(0, t)

n

n

i=1

)

+ t

(4)

where the step sizes decrease towards zero at rates sat-
isfying (2). This allows averaging out of the stochastic-
ity in the gradients, as well as MH rejection rates that
go to zero asymptotically, so that we can simply ignore
the MH acceptance steps, which require evaluation of
probabilities over the whole dataset, all together.

Stochastic Gradient Langevin Dynamics

In the rest of this section we will give an intuitive argu-
ment for why t will approach samples from the pos-
terior distribution as t  .
In particular, we will
show that for large t, the updates (4) will approach
Langevin dynamics (3), which converges to the poste-
N
rior distribution. Let

 log p(xi|)

(5)

g() =  log p() +
n

i=1

n

i=1

be the true gradient of the log probability at  and

ht() =  log p() + N

 log p(xti|)  g() (6)

The stochastic gradient is then g()+ht(), with ht()
a zero mean random variable (due to the stochasticity
of the data items chosen at step t) with nite variance
V (), and (4) is,

t = t
2

(g(t) + ht(t)) + t,

t  N(0, t)

(7)

There are two sources of stochasticity in (7): the in-
jected Gaussian noise with variance t, and the noise in
the stochastic gradient, which has variance ( t
2 )2V (t).
The rst observation is that for large t, t  0, and
the injected noise will dominate the stochastic gradient
noise, so that (7) will be eectively Langevin dynam-
ics (3). The second observation is that as t  0,
the discretization error of Langevin dynamics will be
negligible so that the MH rejection probability will ap-
proach 0 and we may simply ignore this step.
In other words, (4), (7) eectively dene a non-
stationary Markov chain such that the tth step tran-
sition operator, for all large t, will have as its equilib-
rium distribution the posterior over . The next ques-
tion we address is whether the sequence of parameters
1, 2, . . . will converge to the posterior distribution.
Because the Markov chain is not stationary and the
step sizes reduce to 0, it is not immediately clear that
this is the case. To see that this is indeed true, we
will show that a subsequence t1, t2, . . . will converge
to the posterior as intended so the whole sequence will
also converge.

First x an 0 such that 0 < 0  1. Since {t} satisfy
the step size property (2), we can nd a subsequence
t=ts+1 t  0 as s  .
t1 < t2 <  such that
Since the injected noise at each step is independent, for
t=ts+1 t2,

0). We now
between steps ts and ts+1 will be O(
show that the total noise due to the stochasticity of
the gradients among these steps will be dominated by
the total injected noise. Since 0  1, we may take

large enough s the total injected noise, 

ts+1

ts+1

2  1 for t between ts and ts+1. Making
t  ts
the assumption that the gradient g() vary smoothly
(e.g. they are Lipschitz continuous in the models in
ts+1
Section 5), the total stochastic gradient is:

(g(t) + ht(t))

(8)

t
2

t=ts+1

= 0

2 g(ts) + O(0) +

t
2 ht(t)

ts+1

t=ts+1

Since the parameters did not vary much between ts
and ts+1, the stochasticity in ht(t) will be dominated
by the randomness in the choice of the mini-batches.
Assuming that these are chosen randomly and inde-
pendently, ht(t) for each t will be basically iid (if
mini-batches were chosen by random partitioning of
the whole dataset, ht(t) will be negatively correlated
instead and will not change the results here). Thus
the variance of





ts+1
t=ts+1

2 ht(t) is O(
t

2
4 ) and
t

t

(

)

ts+1
t=ts+1

2
t
4

= 0
= 0

2 g(ts) + O(0) + O
2 g(ts) + O(0)



The last equation says that the total stochastic gra-
dient step is approximately the exact gradient step at
ts with a step size of 0, with a deviation dominated
by O(0). Since this is in turn dominated by the total
injected noise which is O(
0), this means that the se-
quence t1, t2 , . . . will approach a sequence generated
by Langevin dynamics with a xed step size 0, so it
will converge to the posterior distribution. Note also
that it will have innite eective sample size.
The implication of this argument is that we can use
stochastic gradient Langevin dynamics as an any-
time and general-purpose algorithm.
In the initial
phase the stochastic gradient noise will dominate and
the algorithm will imitate an ecient stochastic gra-
dient ascent algorithm. In the later phase the injected
noise will dominate, so the algorithm will imitate a
Langevin dynamics MH algorithm, and the algorithm
will transition smoothly between the two. However
a disadvantage is that to guarantee the algorithm to
work it is important for the step sizes to decrease to
zero, so that the mixing rate of the algorithm will
slow down with increasing number of iterations. To
address this, we can keep the step size constant once
it has decreased below a critical level where the MH
rejection rate is considered negligible, or use this al-
gorithm for burn-in, but switch to a dierent MCMC
algorithm that makes more ecient use of the whole
dataset later. These alternatives can perform better

Stochastic Gradient Langevin Dynamics

but will require further hand-tuning and are beyond
the scope of this paper. The point of this paper is
to demonstrate a practical algorithm that can achieve
proper Bayesian learning using only mini-batch data.

4. Posterior Sampling

In this section we consider the use of our stochastic
gradient Langevin dynamics algorithm as one which
produces samples from the posterior distribution. We
rst derive an estimate of when the algorithm will
transition from stochastic optimization to Langevin
dynamics. The idea is that we should only start col-
lecting samples after it has entered its posterior sam-
pling phase, which will not happen until after it be-
comes Langevin dynamics. Then we discuss how the
algorithm scales with the dataset size N and give a
rough estimate of the number of iterations required for
the algorithm to traverse the whole posterior. Finally
we discuss how the obtained samples can be used to
form Monte Carlo estimates of posterior expectations.

4.1. Transition into Langevin dynamics phase

We rst generalize our method to allow for precon-
ditioning, which can lead to signicant speed ups by
better adapting the step sizes to the local structure of
the posterior (Roberts & Stramer, 2002; Girolami &
Calderhead, 2011). For instance, certain dimensions
may have a vastly larger curvature leading to much
bigger gradients. In this case a symmetric precondi-
tioning matrix M can transform all dimensions to the
same scale. The preconditioned stochastic gradient
Langevin dynamics is simply,

t = t

2 M

g(t) + ht(t)

+ t,

t  N(0, tM)

(

)

As noted previously, whether the algorithm is in the
stochastic optimization phase or Langevin dynamics
phase depends on the variance of the injected noise,
which is simply tM, versus that of the stochastic gra-
dient. Since the stochastic gradient is a sum over the
current mini-batch, if its size n is large enough the
central limit theorem will kick in and the variations
ht(t) around the true gradient g(t) will become nor-
mally distributed. Its covariance matrix can then be
estimated from the empirical covariance:
V (t)  V [ht(t)]  N 2
n2

(sti  st)(sti  st) (9)

n

i=1

where sti =  log p(xti|t) + 1
of data item i at iteration t and st = 1
n
the empirical mean. Note that V (t) = N 2

 log p(t) is the score
i=1 sti is
n Vs, where

N

n



Vs is the empirical covariance of the scores {sti}, so
scales as N 2
n . From this we see that the variance of
the stochastic gradient step is 2
t N 2
4n M VsM, so that to
get the injected noise to dominate in all directions, we
need the condition

tN 2
4n

max(M

1
2 VsM

1

2 ) =   1

(10)

where max(A) is the largest eigenvalue of A. In other
words, if we choose a stepsize such that the sample
threshold   1, the algorithm will be in its Langevin
dynamics phase and will be sampling approximately
from the posterior.
We can now relate the step size at the sampling thresh-
old to the posterior variance via the Fisher informa-
tion, which is related to Vs as IF  N Vs, and to the
posterior variance   I
1
F . Using these relationships
as well as (10), we see that the step size at the sam-
pling threshold is t  4n
N min(). Since Langevin
dynamics explores the posterior via a random walk,
using this step size implies that we need on the order
of N/n steps to traverse the posterior, i.e. we process
the whole dataset. So we see this method is not a
silver bullet. However, the advantage of the method
is its convenience: stochastic optimization smoothly
and automatically transitions into posterior sampling
without changing the update equation. Even without
measuring the sampling threshold one will enjoy the
benet of protection against overtting and the ability
to perform Bayesian learning. Measuring the sampling
threshold will only be important if one needs to faith-
fully represent the posterior distribution with a nite
collection of samples.

T



4.2. Estimating Posterior Expectations
Since 1, 2, . . . converges to the posterior distribution,
we can estimate the posterior expectation E[f()] of
some function f() by simply taking the sample av-
erage 1
t=1 f(t) (as typically in MCMC, we may
T
remove the initial burn-in phase, say estimated using
the sampling threshold). Since f(t) is an asymptoti-
cally unbiased estimator for E[f()], this sample aver-
age will be consistent. Observe however that because
the step size decreases, the mixing rate of the Markov
chain decreases as well, and the simple sample aver-
age will over-emphasize the tail end of the sequence
where there is higher correlation among the samples,
resulting in higher variance in the estimator. Instead
we propose to use the step sizes to weight the samples:





T

E[f()] 

t=1 tf(t)

T
t=1 t

(11)

Stochastic Gradient Langevin Dynamics

Figure 1. True and estimated posterior distribution.

Figure 2. Left: variances of stochastic gradient noise and
injected noise. Right: rejection probability versus step size.
We report the average rejection probability per iteration in
each sweep through the dataset.


t=1 t = , this estimator will be consistent
Since
as well. The intuition is that the rate at which the
Markov chain mixes is proportional to the step size, so
that we expect the eective sample size of {1, . . . , T}
to be proportional to
t=1 t, and that each t will
contribute an eective sample size proportional to t.



T

5. Experiments

5.1. Simple Demonstration

We rst demonstrate the workings of our stochastic
gradient Langevin algorithm on a simple example in-
volving only two parameters. To make the posterior
multimodal and a little more interesting, we use a mix-
ture of Gaussians with tied means:

x) + 1

1) ;
2 N(1, 2

1  N(0, 2
xi  1
1 = 10, 2

2  N(0, 2
2)
2 N(1 + 2, 2
x)
where 2
x = 2. 100 data points
are drawn from the model with 1 = 0 and 2 = 1.
There is a mode at this parameter setting, but also a
secondary mode at 1 = 1, 2 = 1, with strong neg-
ative correlation between the parameters. We ran the
stochastic gradient Langevin algorithm with a batch-

2 = 1 and 2

Figure 3. Average log joint probability per data item (left)
and accuracy on test set (right) as functions of the num-
ber of sweeps through the whole dataset. Red dashed line
represents accuracy after 10 iterations. Results are aver-
aged over 50 runs; blue dotted lines indicate 1 standard
deviation.

size of 1 and using 10000 sweeps through the whole
dataset. The step sizes are t = a(b + t) where
 = .55 and a and b are set such that t decreases
from .01 to .0001 over the duration of the run. We see
from Figure 1 that the estimated posterior distribu-
tion is very accurate. In Figure 2 we see that there are
indeed two phases to the stochastic gradient Langevin
algorithm: a rst phase where the stochastic gradient
noise dominates the injected noise, and a second phase
where the converse occurs. To explore the scaling of
the rejection rate as a function of step sizes, we reran
the experiment with step sizes exponentially decreas-
ing from 102 to 108. In the original experiment the
dynamic range of the step sizes is not wide enough for
visual inspection. Figure 2(right) shows the rejection
probability decreasing to zero as step size decreases.

5.2. Logistic Regression

We applied our stochastic gradient Langevin algorithm
to a Bayesian logistic regression model. The probabil-
ity of the ith output yi  {1, +1} given the corre-
sponding input vector xi is modelled as:

p(yi|xi) = (yi



xi)

(12)

1+exp(z).
where  are the parameters, and (z) =
The bias parameter is absorbed into  by including 1
as an entry in xi. We use a Laplace prior for  with a
scale of 1. The gradient of the log likelihood is:

1






log p(yi|xi) = (yi

xi)yixi

(13)
while the gradient of the prior is simply sign(),
which is applied elementwise.
We applied our inference algorithm to the a9a dataset
derived by (Lin et al., 2008) from the UCI adult
dataset. It consists of 32561 observations and 123 fea-
tures, and we used batch sizes of 10. Results from 50

1012321012310123210123100102104106106104102100iterationnoise variance  1 noise2 noiseinjected noise108106104102103102101100step sizeaverage rejection rate024681076543210Number of iterations through whole datasetLog joint probability per datum02684-6-4-5-310-2-10-700.511.520.650.70.750.80.85Number of iterations through whole datasetAccuracy on test data  Accuracy after 10 iterationsAccuracy00.51.5210.70.80.750.850.65Stochastic Gradient Langevin Dynamics

runs are shown in Figure 3, with the model trained
on a random 80% of the dataset and tested on the
other 20% in each run. We see that both the joint
probability and the accuracy increase rapidly, with the
joint probability converging after at most 10 iterations,
while the accuracy converging after less than 1 itera-
tion through the dataset, demonstrating the eciency
of the stochastic gradient Langevin dynamics.

5.3. Independent Components Analysis

In the following we will briey review a popular ICA
algorithm based on stochastic (natural) gradient opti-
mization (Amari et al., 1996). We start from a proba-
bilistic model that assumes independent, heavy tailed
marginal distributions,

[

]

i

ij

i x)

pi(wT

p(x, W ) = | det(W )|

N (Wij; 0, )
(14)
where we have used a Gaussian prior over the weights.
It has been found that the eciency of gradient descent
can be signicantly improved if we use a natural gradi-
ent. This is implemented by post-multiplication of the
gradient with the term W T W (Amari et al., 1996). If
we choose pi(yi) =
i x, we get

2 yi) with yi = wT

4 cosh2( 1

1

.= W log[p(X, W )] W T W =

)

tanh(

1
2

yn)yT
n

W  W W T W (15)

(
NI  N
DW

n=1

The term W T W acts like a preconditioning matrix (see
section 4.1), Mij,kl = ik(W T W )jl which is symmetric
under the exchange (i  k, j  l). It can be shown
1 = (cid:14)(W T W )1,
that the inverse of M is given by M

and the matrix square root as
W T W with





M = (cid:14)
2 U T if W T W = UU T .

W T W = U 1

The update equation for Langevin dynamics thus be-
comes,

Wt+1 = Wt +

1

2 tDW + (cid:17)t

W T W

(16)



where every element of (cid:17)t is normally distributed with
variance t: ij,t  N [0, t]. Our stochastic version
simply approximates the part of the gradient that
sums over data-cases with a sum over a small mini-
batch of size n and multiplies the result with N/n to
bring it back to the correct scale. We also anneal the
stepsizes according to t  a(b + t).
To assess how well our stochastic Langevin approach
compares against a standard Bayesian method we im-
plemented the corrected Langevin MCMC sampler.

[

[

]

]

, as in Eqn.16.
This sampler, proposes a new state W
Note however that we sum over all data-cases and that
we do not anneal the stepsize. Secondly, we need to
accept or reject the proposed step based on all the
data-cases in order to guarantee detailed balance. The
proposal distribution is given by (suppressing depen-
dence on t),

q(W  W) = N

W

; W +

1

2 DW ; M

(17)

where the quadratic function in the exponent is con-
veniently computed as,

1
2

tr[(W  1

2 DW )T ]
2 DW )(W T W )1(W  1
(18)
  W and the normalization constant
with W = W
requires the quantity det M = det(W T W )D. The ac-
cept/reject step is then given by the usual Metropolis
Hastings rule:

p(accept) = min

1,

)q(W

  W )
p(W
p(W )q(W  W )

(19)

Finally, to compute the sampling threshold of Eqn.10,
we can use
[(
2V(s)M
1
N

2 =
 log p(W ) +  log p(xi|W )

(W T W ) 1

)

(20)

covn

]

M

1

1

2

with covn the sample covariance over the mini-batch
of n data-cases.
To show the utility of our Bayesian variant of ICA we
dene the following instability metric for indepen-
dent components:
Ii =

var(Wij)var(xj)



(21)

j

where var(Wij) is computed over the posterior sam-
ples and var(xj) is computed over the data-cases.
The reason that we scale the variance of the weight
entry Wij with the variance of xj is that the vari-
ance of the sources yi =
j Wijxj is approximately
equal for all i because they are t to the distribution
pi(yi) =



1

4 cosh2( 1

2 yi).

5.3.1. Artificial Data

In the rst experiment we generated 1000 data-cases
IID in six channels. Three channels had high kurtosis
distributions while three others where normally dis-
tributed. We ran stochastic Langevin dynamics with

Stochastic Gradient Langevin Dynamics

Figure 4. Left two (cid:12)gures: Amari distance over time for stochastic Langevin dynamics and corrected Langevin dynamics.
Thick line represents the online average. First few hundred iterations were removed to show the scale of the (cid:13)uctuations.
Right two (cid:12)gures: Instability index for the 6 independent components computed in section 5.3.1 for stochastic Langevin
dynamics and corrected Langevin dynamics.

Figure 5. Posterior density estimates for arti(cid:12)cial dataset for stochastic Langevin and corrected Langevin dynamics mea-
sured across the W11 (cid:0) W12 and W1;1 (cid:0) W2;1 axes.



a batch-size of 100 for a total of 500,000 iterations
0.55.
and a polynomial annealing schedule t = 4
N t
After around 10,000 iterations the sampling threshold
at  = 0.1 was met. At that point we recorded the
mixing distance as D0 = t and collected samples
t t from the last sample time
only when the sum
exceeded D0 (in other words, as t decreases we col-
lect fewer samples per unit time). We note that simply
collecting all samples had no noticeable impact on the
nal results. The last estimate of W was used to ini-
tialize corrected Langevin dynamics (this was done to
force the samplers into the same local maximum) after
which we also collected 500, 000 samples. For corrected
Langevin we used a constant stepsize of  = 0.1
N .
The two left gures of Figure 4 show the Amari dis-
tance (Amari et al., 1996) over time for stochastic
and corrected Langevin dynamics respectively. The
right two gures show the sorted values of our pro-
posed instability index. Figures 5 show two dimen-
sional marginal density estimates of the posterior dis-
tribution of W . ICA cannot determine the Gaussian
components and this fact is veried by looking at the
posterior distribution. In fact, the stochastic Langevin
algorithm has mixed over a number of modes that pre-
sumably correspond to dierent linear combinations of
the Gaussian components. To a lesser degree the cor-
rected Langevin has also explored two modes. Due
to the complicated structure of the posterior distri-

bution the stability index varies strongly between the
two sampling algorithms for the Gaussian components
(and in fact also varies across dierent runs). We veri-
ed that the last three components correspond to sta-
ble, high kurtosis components.

5.3.2. MEG Data

downloaded

dataset

the MEG

We
from
http://www.cis.hut.(cid:12)/projects/ica/eegmeg/MEG data.html.
There are 122 channels and 17730 time-points, from
which we extracted the rst 10 channels for our
experiment. To initialize the sampling algorithms,
we rst ran fastICA (Hyvarinen, 1999) to nd an
initial estimate of the de-mixing matrix W . We
then ran stochastic Langevin and corrected Langevin
dynamics to sample from the posterior. The settings
were very similar to the previous experiment with a
0.55 for stochastic Langevin and
schedule of t = 0.1
N t
a constant stepsize of 1/N for corrected Langevin.
We obtained 500, 000 samples for stochastic Langevin
in 800 seconds and 100,000 samples for corrected
Langevin in 9000 seconds. We visually veried
that the two dimensional marginal distributions of
stochastic Langevin and corrected Langevin dynamics
were very similar. The instability values are shown in
gure 6. Due to the absence of Gaussian components
we see that the stability indices are very similar across
the two sampling algorithms.
It was veried that

2468x 104246810iterationAmari distanceAmari Distance Stoc. Lan.00.511.52x 104246810iterationAmari distanceAmari Distance Corr. Lan.123456050100150200Sorted Component IDInstability MetricInstability Metric Stoc. Lan.123456020406080100Sorted Component IDInstability MetricInstability Metric Corr. Lan.W(1,1)W(1,2)PDF W(1,1) vs W(1,2) Stoc. Lan.5056420246W(1,1)W(1,2)PDF W(1,1) vs W(1,2) Corr. Lan.42024642024W(1,1)W(2,1)PDF W(1,1) vs W(2,1) Stoc. Lan.505505W(1,1)W(2,1)PDF W(1,1) vs W(2,1) Corr. Lan.42024012345Stochastic Gradient Langevin Dynamics

We believe that this work represents only a tentative
rst step to further work on ecient MCMC sampling
based on stochastic gradients. Interesting directions of
research include stronger theory providing a solid proof
of convergence, deriving a MH rejection step based
on mini-batch data, extending the algorithm to the
online estimation of dynamical systems, and deriving
algorithms based on more sophisticated Hamiltonian
Monte Carlo approaches which do not suer from ran-
dom walk behaviour.

Acknowledgements

This material is based upon work supported by the Na-
tional Science Foundation under Grant No.
0447903,
1018433 (MW) and the Gatsby Charitable Foundation
(YWT).

