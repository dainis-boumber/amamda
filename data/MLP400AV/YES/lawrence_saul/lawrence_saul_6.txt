An Introduction to Locally Linear Embedding

Lawrence K. Saul

AT&T Labs  Research

180 Park Ave, Florham Park, NJ 07932 USA

lsaul@research.att.com

Sam T. Roweis

Gatsby Computational Neuroscience Unit, UCL

17 Queen Square, London WC1N 3AR, UK

roweis@gatsby.ucl.ac.uk

Abstract

Many problems in information processing involve some form of dimension-
ality reduction. Here we describe locally linear embedding (LLE), an unsu-
pervised learning algorithm that computes low dimensional, neighborhood
preserving embeddings of high dimensional data. LLE attempts to discover
nonlinear structure in high dimensional data by exploiting the local symme-
tries of linear reconstructions. Notably, LLE maps its inputs into a single
global coordinate system of lower dimensionality, and its optimizations
though capable of generating highly nonlinear embeddingsdo not involve
local minima. We illustrate the method on images of lips used in audiovisual
speech synthesis.

1

Introduction

Many problems in statistical pattern recognition begin with the preprocessing of
multidimensional signals, such as images of faces or spectrograms of speech.
Often, the goal of preprocessing is some form of dimensionality reduction: to com-
press the signals in size and to discover compact representations of their variability.

Two popular forms of dimensionality reduction are the methods of principal com-
ponent analysis (PCA) [1] and multidimensional scaling (MDS) [2]. Both PCA and
MDS are eigenvector methods designed to model linear variabilities in high dimen-
sional data. In PCA, one computes the linear projections of greatest variance from

1

the top eigenvectors of the data covariance matrix. In classical (or metric) MDS,
one computes the low dimensional embedding that best preserves pairwise dis-
tances between data points. If these distances correspond to Euclidean distances,
the results of metric MDS are equivalent to PCA. Both methods are simple to
implement, and their optimizations do not involve local minima. These virtues ac-
count for the widespread use of PCA and MDS, despite their inherent limitations
as linear methods.

Recently, we introduced an eigenvector methodcalled locally linear embedding
(LLE)for the problem of nonlinear dimensionality reduction[4]. This problem
is illustrated by the nonlinear manifold in Figure 1. In this example, the dimen-
sionality reduction by LLE succeeds in identifying the underlying structure of the
manifold, while projections of the data by PCA or metric MDS map faraway data
points to nearby points in the plane. Like PCA and MDS, our algorithm is sim-
ple to implement, and its optimizations do not involve local minima. At the same
time, however, it is capable of generating highly nonlinear embeddings. Note that
mixture models for local dimensionality reduction[5, 6], which cluster the data and
perform PCA within each cluster, do not address the problem considered here
namely, how to map high dimensional data into a single global coordinate system
of lower dimensionality.

In this paper, we review the LLE algorithm in its most basic form and illustrate a
potential application to audiovisual speech synthesis[3].

(A)

(B)

(C)

3

2

1

0

-1
-1

3

2

1

0

0

1 0

5

-1
-1

0

1 0

5

2

1

0

-1

-2
-2

-1

0

1

2

Figure 1: The problem of nonlinear dimensionality reduction, as illustrated for
three dimensional data (B) sampled from a two dimensional manifold (A). An un-
supervised learning algorithm must discover the global internal coordinates of the
manifold without signals that explicitly indicate how the data should be embed-
ded in two dimensions. The shading in (C) illustrates the neighborhood-preserving
mapping discovered by LLE.

2

2 Algorithm

if

The LLE algorithm, summarized in Fig. 2, is based on simple geometric intuitions.
,
sampled from some smooth underlying manifold. Provided there is sufcient data
(such that the manifold is well-sampled), we expect each data point and its neigh-
bors to lie on or close to a locally linear patch of the manifold.

 , each of dimensionality

real-valued vectors

We can characterize the local geometry of these patches by linear coefcients that
reconstruct each data point from its neighbors. In the simplest formulation of LLE,
nearest neighbors per data point, as measured by Euclidean dis-
tance. (Alternatively, one can identify neighbors by choosing all points within a
ball of xed radius, or by using more sophisticated rules based on local metrics.)
Reconstruction errors are then measured by the cost function:

Suppose the data consist of
one identies

	





 summarize the contribution of the th data point to
structions. The weights

 , we minimize the cost func-
the th reconstruction. To compute the weights

 does not belong to this set; second,

from its neighbors, enforcing


. The reason for the
that the rows of the weight matrix sum to one:
 subject
sum-to-one constraint will become clear shortly. The optimal weights


to these constraints are found by solving a least squares problem, as discussed in
Appendix A.

which adds up the squared distances between all the data points and their recon-

Note that the constrained weights that minimize these reconstruction errors obey an
important symmetry: for any particular data point, they are invariant to rotations,
rescalings, and translations of that data point and its neighbors. The invariance to
rotations and rescalings follows immediately from the form of eq. (1); the invari-
ance to translations is enforced by the sum-to-one constraint on the rows of the
weight matrix. A consequence of this symmetry is that the reconstruction weights
characterize intrinsic geometric properties of each neighborhood, as opposed to
properties that depend on a particular frame of reference.

tion subject to two constraints: rst, that each data point

is reconstructed only

(1)

!#"
design, the reconstruction weights


Suppose the data lie on or near a smooth nonlinear manifold of dimensionality
. To a good approximation, then, there exists a linear mappingconsisting
of a translation, rotation, and rescalingthat maps the high dimensional coordi-
nates of each neighborhood to global internal coordinates on the manifold. By

 reect intrinsic geometric properties of the

3


































data that are invariant to exactly such transformations. We therefore expect their
characterization of local geometry in the original data space to be equally valid for
that reconstruct
dimensions should also reconstruct its embedded manifold

local patches on the manifold. In particular, the same weights

the th data point in
coordinates in! dimensions.

(Informally, imagine taking a pair of scissors, cutting out locally linear patches
of the underlying manifold, and placing them in the low dimensional embedding
space. Assume further that this operation is done in a way that preserves the angles
formed by each data point to its nearest neighbors. In this case, the transplantation
of each patch involves no more than a translation, rotation, and rescaling of its
data, exactly the operations to which the weights are invariant. Thus, when the
patch arrives at its low dimensional destination, we expect the same weights to
reconstruct each data point from its neighbors.)

cost function:

(2)

constraints that make the problem well-posed, it can be minimized by solving a

an ordered set of orthogonal coordinates centered on the origin. Details of this
eigenvector problem are discussed in Appendix B.

This cost functionlike the previous oneis based on locally linear reconstruction

LLE constructs a neighborhood preserving mapping based on the above idea. In the
is mapped to a

nal step of the algorithm, each high dimensional observation

low dimensional vector
 representing global internal coordinates on the manifold.
This is done by choosing! -dimensional coordinates
 to minimize the embedding
)(
&
'
 while optimizing the coordinates
errors, but here we x the weights

 . The
embedding cost in Eq. (2) denes a quadratic form in the vectors
 . Subject to
eigenvector problem, whose bottom! non-zero eigenvectors provide
sparse+*,
bedding coordinates are computed by an-*.
one free parameter: the number of neighbors per data point,

Note that while the reconstruction weights for each data point are computed from
its local neighborhoodindependent of the weights for other data pointsthe em-
eigensolver, a global operation that
couples all data points in connected components of the graph dened by the weight
matrix. The different dimensions in the embedding space can be computed succes-
sively; this is done simply by computing the bottom eigenvectors from eq. (2) one
at a time. But the computation is always coupled across data points. This is how
the algorithm leverages overlapping local information to discover global structure.

Implementation of the algorithm is fairly straightforward, as the algorithm has only
. Once neighbors

4



$
$
%

$






$







$






$
$
LLE ALGORITHM

1. Compute the neighbors of each data point,

 .

its neighbors, minimizing the cost in eq. (1) by constrained linear ts.

that best reconstruct each data point

the quadratic form in eq. (2) by its bottom nonzero eigenvectors.

Figure 2: Summary of the LLE algorithm, mapping high dimensional data points,

methods in linear algebra. The algorithm involves a single pass through the three
steps in Fig. 2 and nds global minima of the reconstruction and embedding costs
in Eqs. (1) and (2). As discussed in Appendix A, in the unusual case where the

2. Compute the weights

 from
 , minimizing
3. Compute the vectors
$/ best reconstructed by the weights

 , to low dimensional embedding vectors,
$0 .
 and coordinates$
are chosen, the optimal weights

 are computed by standard
neighbors outnumber the input dimensionality
, the least squares problem
213
The algorithm, as described in Fig. 2, takes as input the
 . In many settings, however, the user may not have access to data of this

tors,
form, but only to measurements of dissimilarity or pairwise distance between dif-
ferent data points. A simple variation of LLE, described in Appendix C, can be
applied to input of this form. In this way, matrices of pairwise distances can be
analyzed by LLE just as easily as MDS[2]; in fact only a small fraction of all pos-
sible pairwise distances (representing distances between neighboring points and
their respective neighbors) are required for running LLE.

for nding the weights does not have a unique solution, and a regularization term
for example, one that penalizes the squared magnitudes of the weightsmust be
added to the reconstruction cost.

high dimensional vec-

3 Examples

The embeddings discovered by LLE are easiest to visualize for intrinsically two

data points sampled off the S-shaped manifold. The resulting embedding shows

5467
dimensional manifolds. In Fig. 1, for example, the input to LLE consisted
8 :9 neighbors per data point, successfully unraveled
how the algorithm, using

the underlying two dimensional structure.

5













alize, the manifold of translated faces is highly nonlinear in the high dimensional

By contrast, the top portion shows the rst two components discovered by PCA. It
is clear that the manifold structure in this example is much better modeled by LLE.

Fig. 3 shows another two dimensional manifold, this one living in a much higher
dimensional space. Here, we generated examplesshown in the middle panel of
the gureby translating the image of a single face across a larger background
of random noise. The noise was uncorrelated from one example to the next. The
only consistent structure in the resulting images thus described a two-dimensional
manifold parameterized by the faces center of mass. The input to LLE consisted
face su-

<;74=  grayscale images, with each image containing a97>
of
background of noise. Note that while simple to visu-
perimposed on a?
@67A; ) vector space of pixel coordinates. The bottom portion of Fig. 3 shows
(
CB neighbors per data point.
the rst two components discovered by LLE, with
>7> color (RGB) images of lips at EA>
heads[3]. Our database contained
>FB
59AG69= :4 ) is useful for
resolution. Dimensionality reduction of these images (
H :4 ).
rst two components discovered, respectively, by PCA and LLE (with

If the lip images described a nearly linear manifold, these two methods would
yield similar results; thus, the signicant differences in these embeddings reveal
the presence of nonlinear structure. Note that while the linear projection by PCA
has a somewhat uniform distribution about its mean, the locally linear embedding
has a distinctly spiny structure, with the tips of the spines corresponding to extremal
congurations of the lips.

Finally, in addition to these examples, for which the true manifold structure was
known, we also applied LLE to images of lips used in the animation of talking

faster and more efcient animation. The top and bottom panels of Fig. 4 show the

96

D>

4 Discussion

It is worth noting that many popular learning algorithms for nonlinear dimension-
ality reduction do not share the favorable properties of LLE. Iterative hill-climbing
methods for autoencoder neural networks[7, 8], self-organizing maps[9], and latent
variable models[10] do not have the same guarantees of global optimality or con-
vergence; they also tend to involve many more free parameters, such as learning
rates, convergence criteria, and architectural specications.

The different steps of LLE have the following complexities. In Step 1, computing

nearest neighbors scales (in the worst case) asI
dimensionality,

, and quadratically in the number of data points,

, or linearly in the input

. For many

6

*
;
*
?
?
*




Figure 3: The results of PCA (top) and LLE (bottom), applied to images of a single
face translated across a two-dimensional background of noise. Note how LLE
maps the images with corner faces to the corners of its two dimensional embedding,
while PCA fails to preserve the neighborhood structure of nearby images.

7

Figure 4: Images of lips mapped into the embedding space described by the rst
two coordinates of PCA (top) and LLE (bottom). Representative lips are shown
next to circled points in different parts of each space. The differences between the
two embeddings indicate the presence of nonlinear structure in the data.

8

set of linear equations for each data point. In Step 3, computing

data distributions, however  and especially for data distributed on a thin subman-
ifold of the observation space  constructions such as K-D trees can be used to
time[13]. In Step 2, computing the recon-

8JLK7M	
compute the neighbors inI
struction weights scales asI
ON
to solve a
the bottom eigenvectors scales asI
dimensions,! , and quadratically in the number of data points,
quadratic in

. Methods for
sparse eigenproblems[14], however, can be used to reduce the complexity to sub-
. Note that as more dimensions are added to the embedding space,
the existing ones do not change, so that LLE does not have to be rerun to compute
higher dimensional embeddings. The storage requirements of LLE are limited by
the weight matrix which is size N by K.

; this is the number of operations required
, linearly in the number of embedding

LLE illustrates a general principle of manifold learning, elucidated by Tenenbaum
et al[11], that overlapping local neighborhoodscollectively analyzedcan pro-
vide information about global geometry. Many virtues of LLE are shared by the
Isomap algorithm[11], which has been successfully applied to similar problems
in nonlinear dimensionality reduction. Isomap is an extension of MDS in which
embeddings are optimized to preserve geodesic distances between pairs of data
points; these distances are estimated by computing shortest paths through large
sublattices of data. A virtue of LLE is that it avoids the need to solve large dy-
namic programming problems. LLE also tends to accumulate very sparse matrices,
whose structure can be exploited for savings in time and space.

LLE is likely to be even more useful in combination with other methods in data
analysis and statistical learning. An interesting and important question is how to
learn a parametric mapping between the observation and embedding spaces, given

A Constrained Least Squares Problem

amples for statistical models of supervised learning. The ability to learn such map-
pings should make LLE broadly useful in many areas of information processing.

RQ pairs as labeled ex-
the results of LLE. One possible approach is to useP
can be computed in closed form. Consider a particular data point
S with
neighbors
X7Z)YX
YX

The constrained weights that best reconstruct each data point from its neighbors
nearest
that sum to one. We can write the

 and reconstruction weightsU
W

reconstruction error as:

(3)

9






*


!







$
T

V





S



U


T









U



S


T








U

U

(4)

Z)YX
+
]  . In terms of the inverse local covariance matrix, the

, and then to rescale

(5)

The solution, as written in eq. (5), appears to require an explicit inversion of the
local covariance matrix. In practice, a more efcient way to minimize the error is

the weights so that they sum to one (which yields the same result). By construction,
the local covariance matrix in eq. (4) is symmetric and semipositive denite. If
the covariance matrix is singular or nearly singularas arises, for example, when
), or when the data points
are not in general positionit can be conditioned (before solving the system) by
adding a small multiple of the identity matrix,

optimal weights are given by:

[\
the constraint that
Z#^`_
aX
^`_
cbed
bfd
Z)aX
simply to solve the linear system of equations,
there are more neighbors than input dimensions (21g
YX
Z)aX,hiZ)aXkjml)n
porq
is small compared to the trace ofZ
wheren
The embedding vectors
 :sutwv
xed weights


B Eigenvector Problem

Note that the cost denes a quadratic form,

. This amounts to penalizing large
weights that exploit correlations beyond some level of precision in the data sam-
pling process.

(6)

where in the rst identity, we have exploited the fact that the weights sum to one,
and in the second identity, we have introduced the local covariance matrix,

This error can be minimized in closed form, using a Lagrange multiplier to enforce

$0 are found by minimizing the cost function, eq. (2), for
y


2F


)(

/

(7)

10

{z


S


T


S


T
X

(

U

U



X
Z
(

U
X




x
%

$







$



$





%

$






$

[

$



:

(9)

(8)

X}

is 1 if

requiring the coordinates to be centered on the origin:

This optimization is performed subject to constraints that make the problem well

involving inner products of the embedding vectors and the*| matrixz
 and 0 otherwise.
whereq
posed. It is clear that the coordinates
 can be translated by a constant displace-
ment without affecting the cost,%
. We remove this degree of freedom by

$#~

Also, to avoid degenerate solutions, we constrain the embedding vectors to have
unit covariance, with outer products that satisfy

(10)

identity matrix. Note that there is no loss in generality in
to be diagonal and of order unity, since the cost
function in eq. (2) is invariant to rotations and homogeneous rescalings. The further
constraint that the covariance is equal to the identity matrix expresses an assump-
tion that reconstruction errors for different coordinates in the embedding space
should be measured on the same scale.

The optimal embeddingup to a global rotation of the embedding spaceis found
; this is a version of
the Rayleitz-Ritz theorem[12]. The bottom eigenvector of this matrix, which we
discard, is the unit vector with all equal components; it represents a free translation
mode of eigenvalue zero. Discarding this eigenvector enforces the constraint that
the embeddings have zero mean, since the components of other eigenvectors must

where
is the!
constraining the covariance of
eigenvectors of the matrix,z
by computing the bottom!
sum to zero, by virtue of orthogonality. The remaining! eigenvectors form the
! embedding coordinates found by LLE.
eigenvectors of the matrixz
Note that the bottom!
eigenvalues) can be found without performing a full matrix
ing to its smallest!
diagonalization[14]. Moreover, the matrixz




8	
+

can be stored and manipulated as the

(that is, those correspond-

sparse symmetric matrix

(11)

11

z



q












j

X


X







$

$



$




(





$




*
!
$
j
j
j
z



performed as

z-

(12)

, both of
never needs to be explicitly cre-

.

C LLE from Pairwise Distances

. In particular, left
(the subroutine required by most sparse eigensolvers) can be

giving substantial computational savings for large values of
multiplication byz

requiring just one multiplication by

and one multiplication by

which are extremely sparse. Thus, the matrixz
ated or stored; it is sufcient to store and multiply the matrix

need to compute the local covariance matrixZ)aX between its nearest neighbors,
aX denotes the squared distance between the th and th neighbors,
where
aX . In terms of this local covariance matrix, the
 and

`

as dened by eq. (4) in appendix A. This can be done by exploiting the usual
relation between pairwise distances and dot products that forms the basis of metric
MDS[2]. Thus, for a particular data point, we set:

LLE can be applied to user input in the form of pairwise distances. In this case,
nearest neighbors are identied by the smallest non-zero elements of each row in
the distance matrix. To derive the reconstruction weights for each data point, we

reconstruction weights for each data point are given by eq. (5). The rest of the
algorithm proceeds as usual.

Note that this variant of LLE requires signicantly less user input than the com-
plete matrix of pairwise distances.
Instead, for each data point, the user needs
only to specify its nearest neighbors and the submatrix of pairwise distances be-
tween those neighbors. Is it possible to recover manifold structure from even less
user inputsay, just the pairwise distances between each data point and its near-
est neighbors? A simple counterexample shows that this is not possible. Consider
the square lattice of three dimensional data points whose integer coordinates sum to

zero. Imagine that points with evenS -coordinates are colored black, and that points
with oddS -coordinates are colored red. The two point embedding that maps all

black points to the origin and all red points to one unit away preserves the distance
between each point and its four nearest neighbors. Nevertheless, this embedding
completely fails to preserve the underlying structure of the original manifold.

Z)aX

aX

aX



(13)

12










~








~


9



j

X










Acknowledgements

The authors thank E. Cosatto, H.P. Graf, and Y. LeCun (AT&T Labs) and B. Frey
(U. Toronto) for providing data for these experiments. S. Roweis acknowledges
the support of the Gatsby Charitable Foundation, the National Science Foundation,
and the National Sciences and Engineering Research Council of Canada.

