ABSTRACT

We develop a framework for large margin classication by
Gaussian mixture models (GMMs). Large margin GMMs
have many parallels to support vector machines (SVMs), but
with classes modeled by ellipsoids instead of half-spaces.
Model parameters are trained discriminatively to maximize
the margin of correct classication, as measured in terms of
Mahalanobis distances. The required optimization is convex
over the models parameter space of positive semidenite ma-
trices and can be performed efciently. Large margin GMMs
are naturally suited to large problems in multiway classica-
tion; we apply them to phonetic classication and recogni-
tion on the TIMIT database. On both tasks, we obtain signi-
cant improvement over baseline systems trained by maximum
likelihood estimation. For the problem of phonetic classi-
cation, our results are competitive with other state-of-the-art
classiers, such as hidden conditional random elds.

1. INTRODUCTION

Much of the acoustic-phonetic modeling in automatic speech
recognition (ASR) is handled by Gaussian mixture models
(GMMs) [1]. It is widely recognized that maximum likeli-
hood (ML) estimation of GMMs does not directly optimize
the performance of these models as classiers. It is therefore
of interest to develop alternative learning paradigms that op-
timize discriminative measures of performance [2, 3, 4].

Support vector machines (SVMs) currently provide state-
of-the-art performance for many problems in pattern recog-
nition [5]. The simplest setting for SVMs is binary classi-
cation. If the positively and negatively labeled examples are
linearly separable, SVMs compute the linear decision bound-
ary that maximizes the margin of correct classication
that is, the distance of the closest example(s) to the separating
hyperplane. If the labeled examples are not linearly separa-
ble, the kernel trick can be used to map the examples into a
nonlinear feature space and to compute the maximum margin
hyperplane in this space. Alternately, or in conjunction with
the kernel trick, the optimization for SVMs can be relaxed to

permit margin violations (i.e., incorrectly labeled examples)
in the training data.

For various reasons, it can be challenging to apply SVMs
to large problems in multiway classication. First, to ap-
ply the kernel trick (which is required for nonlinear decision
boundaries), one must construct a large kernel matrix with
as many rows and columns as training examples. Second,
the training complexity increases with the number of classes,
depending to some extent on the way that binary SVMs are
generalized to multiway classication.

In this paper, we develop a framework for large margin
classication by GMMs. As in SVMs, our approach is based
on the idea of margin maximization. Intuitively, we show how
to train large margin GMMs that maximize the Mahanalo-
bis distance of labeled examples from the decision boundaries
that dene competing classes. As in SVMs, the parameters
of large margin GMMs are trained by a convex optimization
that focuses on examples near the decision boundaries. After
developing the basic approach in section 2, we discuss exten-
sions for segmental training and outlier handling in section 3
and report experimental results on phonetic classication and
recognition in section 4.

Our approach has certain advantages over SVMs for large
problems in multiway classication. For example, the classes
in large margin GMMs are modeled by ellipsoidswhich in-
duce nonlinear decision boundaries in the input spaceas op-
posed to the half-spaces and hyperplanes in SVMs. Because
the kernel trick is not necessary to induce nonlinear decision
boundaries, large margin GMMs are more readily trained on
very large and difcult data sets, as arise in ASR.

2. LARGE MARGIN MIXTURE MODELS

We begin by describing large margin GMMs in the simple
setting where each class is modeled by a single ellipsoid. We
then extend this framework to the case where each class is
modeled by one or more ellipsoids. Finally, we relate our
framework to other discriminative paradigms that have been
proposed for training GMMs.

c c
c cc + c

(2)

(cid:21)
(cid:20) x

.

(cid:21)

2.1. Large margin classication

The simplest large margin GMM represents each class of la-
beled examples by a single ellipsoid. Each ellipsoid is pa-
rameterized by a vector centroid   <d and a positive
semidenite orientation matrix   <dd. These param-
eters are analogous to the means and inverse covariance ma-
trices of multivariate Gaussians, but they are not estimated in
the same way. In addition, a nonnegative scalar offset   0
for each class is used in the scoring procedure.

Let (c,c,c) denote the centroid, orientation matrix,
and scalar offset representing examples in class c. We label
an example x  <d by whichever ellipsoid has the smallest
Mahanalobis distance (plus offset) to its centroid:

(cid:8)(xc)Tc(xc) + c

(cid:9) .

(1)

y = argmin

c

The goal of learning is to estimate the parameters (c,c,c)
for each class of labeled examples that optimize the perfor-
mance of this decision rule.

It is useful to collect the ellipsoid parameters of each class

in a single enlarged positive semidenite matrix:

c =

T

(cid:20) c
(cid:8)zTc z(cid:9) where

c c

T

We can then rewrite the decision rule in eq. (1) as simply:

c

.

1

z =

y = argmin

(3)
Here, z  <d+1 is the vector created by appending a unit
element to x  <d.
In this transformed representation,
the goal of learning is simply to estimate the single matrix
c  <(d+1)(d+1) for each class of labeled examples.
We now consider the problem of learning in more detail.
Let {(xn, yn)}N
n=1 denote a set of N labeled examples drawn
from C classes, where xn  <d and yn  {1, 2, . . . , C}.
In large margin GMMs, we seek matrices c such that all
the examples in the training set are correctly classied by a
large margini.e., situated far from the decision boundaries
that dene competing classes. For the nth example with class
label yn, this condition can be written as:
n czn  1 + zT
zT

(4)
Eq. (4) states that for each competing class c 6= yn, the Maha-
lanobis distance (plus offset) to the cth centroid exceeds the
Mahalanobis distance (plus offset) to the target centroid by a
margin of at least one unit.

c 6= yn,

n ynzn.

We adopt a convex loss function for training large margin
GMMs. Analogous to SVMs, the loss function has two terms,
one that penalizes margin violations of eq. (4) and one that
regularizes the matrices c. Letting [f]+ =max(0,f) denote
the so-called hinge function, we can write the loss function
for large margin GMMs as:
L = 

(cid:2)1+zT

n (yn c)zn

++X
(cid:3)

trace(c). (5)

X

X

c6=yn

n

c

The second term regularizes the orientation matrices c
which appear in the d  d upper left blocks of c. In real-
izable settings, where all the examples can be correctly clas-
sied, the second term favors the minimum trace Maha-
lanobis metrics consistent with the unit margin constraints in
eq. (4). The relative weight of the two terms is controlled by
a hyperparameter  >0 set by cross-validation.

The loss function in eq. (5) is a piecewise linear, convex
function of the matrices c, which are further constrained to
be positive semidenite. Its optimization can thus be formu-
lated as a problem in semidenite programming [6]. Such
problems can be generically solved by interior point algo-
rithms with polynomial time guarantees (though we imple-
mented a special-purpose solver using gradient-based meth-
ods for the results in this paper). Most importantly, eq. (5)
has the desirable property that its optimization is not plagued
by spurious local minima.

2.2. Mixture models

We now extend the previous model to represent each class by
multiple ellipsoids. This is analogous to modeling each class
by its own GMM, as opposed to a single Gaussian. Let cm
denote the matrix for the mth ellipsoid in class c. The most
straightforward extension is to imagine that each example xn
has not only a class label yn, but also a mixture component
label mn. The latter labels are not provided a priori, but we
can generate proxy labels by tting a GMM to the exam-
ples in each class by ML estimation, then for each example,
computing the mixture component with the highest posterior
probability under this GMM. Given joint labels (yn, mn), we
rewrite the large margin criterion in eq. (4) as:
n cmzn  1 + zT

ezT

n ynmnzn.

(6)

m

c 6= yn,  logX
follows from the fact that  logP
n ynmnzn + logP

Eq. (6) states that for each competing class c 6= yn, the match
to any centroid in class c is worse than the match to the target
centroid by a margin of at least one unit. This interpretation

m eam  minm am.

m ezT

classes and mixture components: P

The loss function for mixture models is a simple exten-
sion of eq. (5). We replace the hinge loss in the rst term
by [1 + zT
n cmzn]+, which penal-
izes violations of the margin inequalities in eq. (6). The regu-
larizer in the second term changes only to sum over different
cm trace(cm). Due to
the softmin operation over mixture components, the result-
ing loss function is no longer piecewise linear in the matri-
ces cm; however, it is easy to verify that it remains convex.
Thus, even the optimization of this more general loss function
for large margin GMMs is quite tractable.

2.3. Relation to previous work

Our framework differs in important aspects from previous
frameworks for discriminative training of GMMs [2]. Sup-

pose the class-conditional densities p(x|y) are modeled by
GMMs. One common approach to discriminative training es-
P
timates the means, covariance matrices, and mixture weights
of these models that maximize the conditional log-likelihood
n log p(yn|xn). Such models generally outperform GMMs
P
that are estimated by maximizing the joint log-likelihood
n log p(xn, yn). In contrast to our framework, however, the
optimization of GMM parameters in this way is not convex.
Moreover, as a loss function, the conditional log-likelihood
does not focus on examples near the decision boundaries nor
incorporate the idea of a large margin.

The main difference between large margin GMMs and
SVMs is that classes are modeled by ellipsoids, rather than
half-spaces, and that the kernel trick (which involves main-
taining a large kernel matrix) is not required for nonlinear de-
cision boundaries. Of course, one can generate quadratic de-
cision boundaries in SVMs by choosing a polynomial kernel
of degree two, or by expanding the vectors xn to include
pairwise products of their original elements. Large margin
GMMs differ from such SVMs by restricting their quadratic
forms to be positive semidenite and by imagining the sup-
port of each class as some bounded region in input space.
In addition, such SVMs cannot represent the large margin
GMMs in section 2.2, with multiple ellipsoids per class.

3. EXTENSIONS

Two further extensions of large margin GMMs are impor-
tant for problems in ASR: handling of outliers, and segmental
training. We describe each extension in isolation, assuming
for simplicity that each class is modeled by a single ellip-
soid, as in section 2.1. The generalization to the large margin
GMMs described in section 2.2 is straightforward, as is the
handling of outliers in combination with segmental training.

3.1. Handling of outliers

Many discriminative learning algorithms are sensitive to out-
liers. Margin-based loss functions,
in particular, do not
closely track the classication error rate when the training
data has many outliers. We adopt a simple strategy to detect
outliers and reduce their malicious effect on learning.

Outliers are detected using ML estimates of the mean and
covariance matrix of each class. These estimates are used to
initialize matrices ML
of the form in eq. (2). Then, for each
example xn, we compute the accumulated hinge loss incurred
by violations of the large margin constraints in eq. (4):

c

n = X

hML

(cid:2)1+zT

c6=yn

n (ML
yn

 ML

c

)zn

(7)

(cid:3)

+

n  0 measures the decrease in the loss func-
Note that hML
tion when an initially misclassied example xn is corrected
during the course of learning. We associate outliers with large
values of hML
n .

Outliers distort the learning process by diverting its fo-
cus away from misclassied examples that could otherwise be
easily corrected. In particular, correcting one badly misclas-
sied outlier decreases the cost function proposed in eq. (5)
more than correcting multiple examples that lie just barely
on the wrong side of a decision boundary. To x this sit-
uation, we reweight the hinge loss terms in eq. (5) involv-
ing example xn by a multiplicative factor of min(1, 1/hML
n ).
This reweighting equalizes the losses incurred by all initially
misclassied examples, thus reducing the malicious effect of
outliers. We compute the weighting factors once from the ML
estimates and hold them xed during discriminative training.
In practice, this scheme appears to work very satisfactorily.

3.2. Segmental training

The margin constraints in eq. (4) apply to individually labeled
examples. We can also relax them to apply, collectively, to
multiple examples known to share the same class label. This
is useful for ASR, where we can train on variable-length seg-
ments, consisting of multiple consecutive analysis frames,
all of which belong to the same phoneme. Specically, let p
index the  frames in the nth phonetic segment {xnp}
p=1. For
segmental training, we rewrite the constraints in eq. (4) as:
c 6= yn,

npcznp  1 +
zT

zT
npynznp, (8)

X

X

1


p

1


p

where the scores on both sides have been normalized by the
segment length. The segment-based constraint in eq. (8) is
especially well motivated if a segment-based decision rule is
used for classication (e.g., y = argminc
p czp) as
opposed to the frame-based rule in eq. (1).

P

p zT

4. EXPERIMENTAL RESULTS

We applied large margin GMMs to well-benchmarked prob-
lems in phonetic classication and recognition on the TIMIT
database [7, 8, 9, 4]. We used the standard partition of train-
ing and test data and the same development set as in earlier
work [9, 4]. All sa sentences were excluded. We mapped
the 61 phonetic labels in TIMIT to 48 classes and trained ML
and large margin GMMs for each classes. Results were eval-
uated by mapping these 48 classes to 39 phones to remove
further confusions, as in previous benchmarks. Our front
end computed mel-frequency cepstral coefcients (MFCCs)
with 25 ms windows at a 10 ms frame rate. We retained the
rst 13 MFCC coefcients of each frame, along with their
rst and second time derivatives. GMMs modeled these 39-
dimensional feature vectors after they were whitened by PCA.

4.1. Phonetic classication

Phonetic classication is an articial but instructive prob-
lem in ASR. One assumes in this case that the speech has

# of mixture
components

1
2
4
8
16

classication

recognition

baseline margin
baseline margin
40.1% 34.7%
32.1% 24.3%
36.5% 33.5%
30.1% 23.4%
34.7% 32.7%
27.8% 22.3%
25.9% 21.1% 32.7% 31.1%
31.7% 30.1%
26.0% 21.4%

Table 1. Error rates for phonetic classication and recognition on
the TIMIT database. Large margin GMMs are compared to baseline
GMMs trained by ML estimation. See text for details.

been correctly segmented into phonetic units, but that the
phonetic class label of each segment is unknown. The in-
put to the classier is the segment of consecutive analysis
frames that spans precisely one phoneme. We trained large
margin GMMs using the segment-based margin criteria in
section 3.2 and compared them to baseline (full covariance)
GMMs trained by ML estimation. The baseline GMMs were
also used to determine the proxy labels for mixture com-
ponents in eq. (6) and to detect and reweight outliers, using
eq. (7). We used the development data set to choose the hyper-
parameter  > 0 in eq. (5), to tune a unigram language model,
and to perform early stopping of the optimization procedure.
The training time on 1.1M frames (roughly, 140K segments)
ranged from 2-9 hours depending on the model size.

Table 1 shows the percentage of incorrectly classied pho-
netic segments on the TIMIT test set. Large margin GMMs
consistently and signicantly outperform baseline GMMs
with equal numbers of mixture components. The best large
margin GMM also yields a slightly lower classication error
rate than state-of-the-art results (21.7%) obtained by hidden
conditional random elds [4].

4.2. Phonetic Recognition

The same baseline and large margin GMMs were used to
build phonetic recognizers.
The recognizers were rst-
order hidden Markov models (HMMs) with one context-
independent state per phonetic class. Baseline or large mar-
gin GMMs were used in these HMMs to compute the log
probabilities (or scores) of observed frames. In all HMMs,
we used the development set to optimize the weighting of
log transition probabilities. Table 1 compares the phone er-
ror rates of these HMMs, obtained by aligning the results
of Viterbi decoding with the ground-truth phonetic transcrip-
tions [7]. Again, the large margin GMMs lead to consis-
tently lower error rates, here computed as the sum of sub-
stitution, deletion, and insertion error rates. We also ob-
serve, as in previous work [3], that discriminatively-trained
context-independent phone models achieve lower error rates
than context-dependent models trained by ML estimation.

5. CONCLUSION

We have shown how to learn GMMs for multiway classi-
cation based on similar principles as large margin classica-
tion in SVMs. Classes are represented by ellipsoids whose
location, shape, and size are discriminatively trained to maxi-
mize the margin of correct classication, as measured in terms
of Mahalanobis distances. The required optimization is con-
vex over the models parameter space of positive semide-
nite matrices. Applied to problems in phonetic classication
and recognition, we showed that large margin GMMs led to
signicant improvement over baseline GMMs.
In ongoing
work, we are investigating phonetic recognizers with context-
dependent phone models, which are known to reduce phone
error rates [7, 8]. We are also studying schemes for integrat-
ing the large margin training of GMMs with sequence models
such as HMMs and/or conditional random elds [4].

Acknowledgments
This work was supported by NSF award 0238323. We thank
A. Gunawardana (Microsoft Research) and K. Crammer (Uni-
versity of Pennsylvania) for many useful discussions and
helpful correspondence.

