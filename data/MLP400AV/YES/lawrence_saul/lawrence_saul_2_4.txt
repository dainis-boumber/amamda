Abstract
In this paper, we propose a model for representing and predicting distances in large-scale networks by matrix
factorization. The model is useful for network distance sensitive applications, such as content distribution
networks, topology-aware overlays, and server selections. Our approach overcomes several limitations of
previous coordinates-based mechanisms, which cannot model sub-optimal routing or asymmetric routing
policies. We describe two algorithms -- singular value decomposition (SVD) and nonnegative matrix
factorization (NMF) -- for representing a matrix of network distances as the product of two smaller matrices.
With such a representation, we build a scalable system -- Internet Distance Estimation Service (IDES) -- that
predicts large numbers of network distances from limited numbers of measurements. Extensive simulations
on real-world data sets show that IDES leads to more accurate, efficient and robust predictions of latencies in
large-scale networks than previous approaches.

Comments
Postprint version. Copyright ACM, 2004. This is the author's version of the work. It is posted here by
permission of ACM for your personal use. Not for redistribution. The definitive version was published in
Proceedings of the 4th ACM SIGCOMM Conference on Internet Measurement (IMC 2004), pages 278-287.
Publisher URL: http://doi.acm.org/10.1145/1028788.1028827

This conference paper is available at ScholarlyCommons: http://repository.upenn.edu/cis_papers/169

Modeling Distances in Large-Scale Networks

by Matrix Factorization

Yun Mao and Lawrence K. Saul

Department of Computer and Information Science

University of Pennsylvania
fmaoy,lsaulg@cis.upenn.edu

Abstract

In this paper, we propose a model for representing
and predicting distances in large-scale networks by
matrix factorization. The model is useful for net-
work distance sensitive applications, such as con-
tent distribution networks, topology-aware overlays,
and server selections. Our approach overcomes sev-
eral limitations of previous coordinates-based mech-
anisms, which cannot model sub-optimal routing or
asymmetric routing policies. We describe two algo-
rithms | singular value decomposition (SVD) and
nonnegative matrix factorization (NMF)|for repre-
senting a matrix of network distances as the product
of two smaller matrices. With such a representation,
we build a scalable system|Internet Distance Esti-
mation Service (IDES)|that predicts large numbers
of network distances from limited numbers of mea-
surements. Extensive simulations on real-world data
sets show that IDES leads to more accurate, e(cid:14)cient
and robust predictions of latencies in large-scale net-
works than previous approaches.

1

Introduction

Wide-area distributed applications have evolved con-
siderably beyond the traditional client-server model,
in which a client only communicates with a single
server. In content distribution networks(CDN), peer-
to-peer distributed hash tables(DHT) [16, 17, 18, 22],
and overlay routing [2], nodes often have the (cid:13)exibil-
ity to choose their communication peers. This (cid:13)exi-
bility can greatly improve performance if relevant net-
work distances are known. For example, in a CDN,
an optimized client can download Web objects from
the particular mirror site to which it has the highest
bandwidth. Likewise, in DHT construction, a peer
can route lookup requests to the peer (among those

that are closer to the target in the virtual overlay
network) with the lowest latency in the IP underlay
network.

Unfortunately, knowledge of network distances is
not available without cost. On-demand network mea-
surements are expensive and time-consuming, espe-
cially when the number of possible communication
peers is large. Thus, a highly promising approach is
to construct a model that can predict unknown net-
work distances from a set of partially observed mea-
surements [4, 6, 7, 12, 13, 20].

Many previously proposed models are based on the
embedding of host positions in a low dimensional
space, with network distances estimated by Euclidean
distances. Such models, however, share certain lim-
itations.
In particular, they cannot represent net-
works with complex routing policies, such as sub-
optimal routing1 or asymmetric routing, since Eu-
clidean distances satisfy the triangle inequality and
are inherently symmetric. On the Internet, routing
schemes of this nature are quite common [3, 10, 15],
and models that do not take them into account yield
inaccurate predictions of network distances.

In this paper, we propose a model based on ma-
trix factorization for representing and predicting dis-
tances in large-scale networks. The essential idea is
to approximate a large matrix whose elements repre-
sent pairwise distances by the product of two smaller
matrices. Such a model can be viewed as a form
of dimensionality reduction. Models based on ma-
trix factorization do not su(cid:11)er from the limitations
of previous work:
in particular, they can represent
distances that violate the triangle inequality, as well
as asymmetric distances. Two algorithms|singular

1With sub-optimal routing policies, the network distance
between two end hosts does not necessarily represent the short-
est path in the network. Such routing policies exist widely in
the Internet for various technical, political and economic rea-
sons.

1

value decomposition (SVD) and nonnegative matrix
factorization (NMF){are presented for learning mod-
els of this form. We evaluate the advantages and dis-
advantages of each algorithm for learning compact
models of network distances.

The rest of the paper is organized as follows. Sec-
tion 2 reviews previous work based on the low di-
mensional embedding of host positions in Euclidean
space. Section 3 presents the model for matrix factor-
ization of network distances. The SVD and NMF al-
gorithms for learning these models from network mea-
surements are presented and evaluated in section 4.
Section 5 proposes an architecture to estimate dis-
tances required by an arbitrary host from low dimen-
sional reconstructions. The architecture is evaluated
in section 6. Finally, section 7 summarizes the paper.

2 Network Embeddings

One way to predict network distance between arbi-
trary Internet end hosts is to assign each host a \po-
sition" in a (cid:12)nite-dimensional vector space. This can
be done at the cost of a limited number of network
measurements to a set of well-positioned infrastruc-
ture nodes2, or other peer nodes. In such a model,
a pair of hosts can estimate the network distance
between them by applying a distance function to
their positions, without direct network measurement.
Most previous work on these models has represented
the host positions by coordinates in Euclidean space
and adopted Euclidean distance as the distance func-
tion.

We de(cid:12)ne the problem formally as follows. Sup-
pose there are N hosts H = fH1;H2;(cid:1)(cid:1)(cid:1) ;HNg in
the network. The pairwise network distance matrix
is a N(cid:2)N matrix D, such that Dij (cid:21) 0 is the network
distance from Hi to Hj .
A network embedding is a mapping H : H ! Rd
such that

Dij (cid:25) ^Dij = kH(Hi)(cid:0)H(Hj)k;8i; j = 1; : : : ; N (1)
where ^Dij is the estimated network distance from Hi
to Hj and H(Hi) is the position coordinate of Hi as a
d-dimensional real vector. We simplify the coordinate
notation from H(Hi) to ~Hi = (Hi1; Hi2;(cid:1)(cid:1)(cid:1) ; Hid).
The network distance between two hosts Hi and Hj
is estimated by the Euclidean distance of their coor-

2referred as landmark nodes in this paper. They are also

called beacon nodes.

dinates:

^Dij = k ~Hi (cid:0) ~Hjk =  d
Xk=1

1
2

(Hik (cid:0) Hjk)2!

(2)

The main problem in constructing a network em-
bedding is to compute the position vectors ~Hi for all
hosts Hi from a partially observed distance matrix D.
A number of learning algorithms have been proposed
to solve this problem, which we describe in the next
section.

2.1 Previous work

The (cid:12)rst work in the network embedding area was
done by Ng and Zhang [13], whose Global Network
Positioning (GNP) System embedded network hosts
in a low-dimensional Euclidean space. Many algo-
rithms were subsequently proposed to calculate the
coordinates of network hosts. GNP uses a Simplex
Downhill method to minimize the sum of relative er-
rors:

(3)

total err =Xi Xj

jDij (cid:0) ^Dijj

Dij

The drawback of GNP is that the Simplex Downhill
method converges slowly, and the (cid:12)nal results depend
on the initial values of the search. PIC [4] applies the
same algorithm to the sum of squared relative errors
and studies security-related issues.

Cox, Dabek et. al. proposed the Vivaldi algo-
rithm [5, 6] based on an analogy to a network of
physical springs.
In this approach, the problem of
minimizing the sum of errors is related to the prob-
lem of minimizing the potential energy of a spring
system. Vivaldi has two main advantages: it is a dis-
tributed algorithm, and it does not require landmark
nodes.

Lim et. al. [12] and Tang et. al. [20] independently
proposed models based on Lipschitz embeddings and
Principal Component Analysis (PCA). These models
begin by embedding the hosts in an N -dimensional
space, where the coordinates of the host Hi are given
by its distances (Di1;(cid:1)(cid:1)(cid:1) ; DiN ) to N landmark nodes.
This so-called Lipschitz embedding has the property
that hosts with similar distances to other hosts are
located nearby in the N -dimensional space. To re-
duce the dimensionality, the host positions in this
N -dimensional space are then projected into the d-
dimensional subspace of maximum variance by PCA.
A linear normalization is used to further calibrate the
results, yielding the (cid:12)nal host positions ~Hi 2 Rd.

2

H(cid:13)1(cid:13)

1(cid:13)

H(cid:13)3(cid:13)

1(cid:13)

1(cid:13)

H(cid:13)2(cid:13)

H(cid:13)4(cid:13)

1(cid:13)

((cid:13)-(cid:13)0.5,0.5)(cid:13)

(0.5,0.5)(cid:13)

H(cid:13)1(cid:13)

H(cid:13)3(cid:13)

(0,0)(cid:13)

H(cid:13)2(cid:13)

H(cid:13)4(cid:13)

((cid:13)-(cid:13)0.5,(cid:13)-(cid:13)0.5)(cid:13)

(0.5,(cid:13)-(cid:13)0.5)(cid:13)

Network Topology(cid:13)

One Possible 2(cid:13)-(cid:13)D Embedding(cid:13)

Figure 1: Four hosts H1 (cid:0) H4 in a simple network
topology

2.2 Limitations

Euclidean distances are inherently symmetric; they
also satisfy the triangle inequality. Thus, in any net-
work embedding,

^Dij = ^Dji

8i; j
^Dij + ^Djk (cid:21) ^Dik 8i; j; k

These two properties are inconsistent with observed
network distances. On the Internet, studies indi-
cate that as many as 40% of node pairs of real-world
data sets have a shorter path through an alternate
node[3, 20]. Another study shows that asymmetric
routing is quite common [15]; even for the same link,
the upstream and downstream capacities may be very
di(cid:11)erent [10].

In addition to these limitations, low-dimensional
embeddings of host positions cannot always model
distances in networks where there are pairs of nodes
that do not have a direct path between them, even if
the distances are symmetric and satisfy triangle in-
equality. Figure 1 illustrates a simple network topol-
ogy in which four hosts in di(cid:11)erent autonomous sys-
tems are connected with unit distance to their neigh-
bors. An intuitive two-dimensional embedding is also
shown. In the given embedding, the estimated dis-
tances are ^D14 = ^D23 = p2, but the real distances
are D14 = D23 = 2. It is provable that there exists
no Euclidean space embedding (of any dimensional-
ity) that can exactly reconstruct the distances in this
network. Similar cases arise in networks with tree-
like topologies.

3 Distance Matrix Factoriza-

tion

The limitations of previous models lead us to consider
a di(cid:11)erent framework for compactly representing net-
work distances. Suppose that two nearby hosts have
similar distances to all the other hosts in the network.
In this case, their corresponding rows in the distance
matrix will be nearly identical. More generally, there
may be many rows in the distance matrix that are
equal or nearly equal to linear combinations of other
rows. Recall from linear algebra that an N (cid:2) N ma-
trix whose rows are not linearly independent has rank
strictly less than N and can be expressed as the prod-
uct of two smaller matrices. With this in mind, we
seek an approximate factorization of the distance ma-
trix, given by:

D (cid:25) XY T ;

where X and Y are N(cid:2)d matrices with d (cid:28) N . From
such a model, we can estimate the network distance
from Hi to Hj by ^Dij = ~Xi (cid:1) ~Yj , where ~Xi is the ith
row vector of the matrix X and ~Yj is the jth row
vector of the matrix Y .

More formally, for a network with distance matrix
Dij, we de(cid:12)ne a distance matrix factorization as two
mappings

X : H ! Rd;
: H ! Rd;

Y

and an approximate distance function computed by

^Dij = X(Hi) (cid:1) Y (Hj ):

As shorthand, we denote X(Hi) as ~Xi and Y (Hi) as
~Yi, so that we can write the above distance compu-
tation as:

^Dij = ~Xi (cid:1) ~Yj =

d

Xk=1

XikYjk:

(4)

Note that in contrast to the model in section 2,
which maps each host to one position vector, our
model associates two vectors with each host. We call
~Xi the outgoing vector and ~Yi the incoming vector for
Hi. The estimated distance from Hi to Hj is simply
the dot product between the outgoing vector of Hi
and the incoming vector of Hj.
Applying this model of network distances in dis-
tributed applications is straightforward. For exam-
ple, consider the problem of mirror selection. To

3

locate the closest server among several mirror can-
didates, a client can retrieve the outgoing vectors of
the mirrors from a directory server, calculate the dot
product of these outgoing vectors with its own in-
coming vector, and choose the mirror that yields the
smallest estimate of network distance (i.e., the small-
est dot product).

Our model for representing network distances by
matrix factorization overcomes certain limitations of
models based on low dimensional embeddings.
In
particular, it does not require that network distances
are symmetric because in general ^Dij = ~Xi (cid:1) ~Yj 6=
~Xj (cid:1) ~Yi = ^Dji. Distances computed in this way also
are not constrained to satisfy the triangle inequal-
ity. The main assumption of our model is that many
rows in the distance matrix are linearly dependent, or
nearly so. This is likely to occur whenever there are
clusters of nearby nodes in the network which have
similar distances to distant nodes. In this case, the
distance matrix D will be well approximated by the
product of two smaller matrices.

4 Distance Reconstruction

In this section we investigate how to estimate outgo-
ing and incoming vectors ~Xi and ~Yi for each host Hi
from the distance matrix D. We also examine the ac-
curacy of models that approximate the true distance
matrix by the product of two smaller matrices in this
way.

The distance matrix D can be viewed3 as storing
N row-vectors in N -dimensional space. Factoring
this matrix D (cid:25) XY T is essentially a problem in
linear dimensionality reduction, where Y stores d ba-
sis vectors and X stores the linear coe(cid:14)cients that
best reconstruct each row vector of D. We present
two algorithms for matrix factorization that solve this
problem in linear dimensionality reduction.

4.1 Singular value decomposition
An N (cid:2) N distance matrix D can be factored into
three matrices by its singular value decomposition
(SVD), of the form:

D = U SV T ;

3Note that D does not have to be a square matrix of pair-
wise distances. It can be the distance matrix from one set of N
hosts H to another set of N 0 hosts H0, which may or may not
overlap with each other. In this case, X 2 RN(cid:2)d contains the
outgoing vectors for H and Y 2 Rd(cid:2)N 0
contains the incoming
vectors for H0. For simplicity, though, we consider the case
N = N0 in what follows.

where U and V are N (cid:2) N orthogonal matrices and
S is an N (cid:2) N diagonal matrix with nonnegative ele-
ments (arranged in decreasing order). Let A = U S
ii = pSii. It is easy to see
and B = S
that ABT = U S
2 V T = D. Thus
2 )T = U S
SVD yields an exact factorization D = AB T , where
the matrices A and B are the same size as D.

2 V , where S

2 (V S

1
2

1

1

2 S

1

1

1

1
2

We can also use SVD, however, to obtain an ap-
proximate factorization of the distance matrix into
two smaller matrices.
In particular, suppose that
only a few of the diagonal elements of the matrix
S are appreciable in magnitude. De(cid:12)ne the N (cid:2) d
matrices:

Xij = UijpSjj ;
Yij = VijpSjj ;

(5)

(6)

where i = 1 : : : N and j = 1 : : : d. The product XY T
is a low-rank approximation to the distance matrix D;
if the distance matrix is itself of rank d or less, as in-
dicated by Sjj = 0 for j > d, then the approximation
will in fact be exact. The low-rank approximation
obtained from SVD can be viewed as minimizing the
squared error function

(Dij (cid:0) ~Xi (cid:1) ~Yj)2

Xi Xj

(7)

with respect to Xi 2 Rd and Yj 2 Rd. Eqs.
(5)
and (6) compute the global minimum of this error
function.

Matrix factorization by SVD is related to princi-
pal component analysis (PCA) [9] on the row vectors.
Principal components of the row vectors are obtained
from the orthogonal eigenvectors of their correlation
matrix; each row vector can be expressed as a linear
combination of these eigenvectors. The diagonal val-
ues of S measure the signi(cid:12)cance of the contribution
from each principal component. In previous work on
embedding of host positions by PCA, such as ICS [12]
and Virtual Landmark [20], the (cid:12)rst d rows of the ma-
trix U were used as coordinates for the hosts, while
discarding the information in the matrices S and V .
By contrast, our approach uses U , S and V to com-
pute outgoing and incoming vectors for each host.

We use the topology in Figure 1 as an example to
show how the algorithm works. The distance matrix
is

D =

2
664

0
1
1
2

1
0
2
1

1
2
0
1

2
1
1
0

3
775

4

We obtain the SVD result as

U =

2

6664

0

(cid:0)0:5
(cid:0)0:5 (cid:0)
(cid:0)0:5
(cid:0)0:5

1

1

p2
p2
0

1

p2
0
0

(cid:0)

1

p2

0:5
(cid:0)0:5
(cid:0)0:5
0:5

3

7775

; S =

2
664

4
0
0
0

0
2
0
0

0
0
2
0

0
0
0
0

3
775

0

(cid:0)

V =

2

6664

(cid:0)0:5
(cid:0)0:5
(cid:0)0:5 (cid:0)
(cid:0)0:5

1

1

p2
p2
0

1

p2
0
0

1

p2

(cid:0)0:5
0:5
0:5
(cid:0)0:5

3

7775

Note that S44 = 0. Therefore, an exact d = 3 fac-
torization exists with:

X =2
664

3
(cid:0)1
0
1
(cid:0)1 (cid:0)1
0
775
1
(cid:0)1
0
0 (cid:0)1
(cid:0)1

; Y =2
664

3
0 (cid:0)1
(cid:0)1
(cid:0)1
1
0
775
0
(cid:0)1 (cid:0)1
0
(cid:0)1
1

One can verify in this case that the reconstructed
distance matrix XY T is equal to the original distance
matrix D.

4.2 Non-negative matrix factorization

Non-negative matrix factorization (NMF) [11] is an-
other form of linear dimensionality reduction that can
be applied to the distance matrix Dij. The goal of
NMF is to minimize the same error function as in
Eq. (7), but subject to the constraint that X and Y
are non-negative matrices. In contrast to SVD, NMF
guarantees that the approximately reconstructed dis-
tances are nonnegative: ^Dij (cid:21) 0. The error func-
tion for NMF can be minimized by an iterative algo-
rithm. Compared to gradient descent and the Sim-
plex Downhill method, however, the algorithm for
NMF converges much faster and does not involve any
heuristics, such as choosing a step size. The only
constraint on the algorithm is that the true network
distances must themselves be nonnegative, Dij (cid:21) 0;
this is generally true and holds for all the examples we
consider. The algorithm takes as input initial (ran-
dom) matrices X and Y and updates them in an al-
ternating fashion. The update rules for each iteration
are:

Xia   Xia

Yja   Yja

(DY )ia

(XY T Y )ia
(X T D)aj

(X T XY T )aj

Eq. (7). Our experience shows that two hundred it-
erations su(cid:14)ce to converge to a local minimum.

One major advantage of NMF over SVD is that it
is straightforward to modify NMF to handle missing
entries in the distance matrix D. For various reasons,
a small number of elements in D may be unavailable.
SVD can proceed with missing values if we eliminate
the rows and columns in D that contain them, but
doing so will leave the corresponding host positions
unknown.

NMF can cope with missing values if we slightly
change the update rules. Suppose M is a binary
matrix where Mij = 1 indicates Dij is known and
Mij = 0 indicates Dij is missing. The modi(cid:12)ed up-
date rules are:

Xia   Xia Pk DikMikYka
Pk(XY T )ikMikYka
Yja   Yja Pk(X T )akDkjMkj
Pk(X T )ak(XY T )kj Mkj
error function, Pij MijjDij (cid:0) ~Xi (cid:1) ~Yjj2.

4.3 Evaluation

These update rules converge to local minima of the

(8)

(9)

We evaluated the accuracy of network distance ma-
trices modeled by SVD and NMF and compared the
results to those of PCA from the Lipschitz embed-
dings used by Virtual Landmark [20] and ICS [12].
We did not evaluate the Simplex Downhill algorithm
used in GNP because while its accuracy is not ob-
viously better than Lipschitz embedding, it is much
more expensive, requiring hours of computation on
large data sets [20]. Accuracies were evaluated by
the modi(cid:12)ed relative error,

relative error = jDij (cid:0) ^Dijj
min(Dij ; ^Dij)

(10)

where the min-operation in the denominator serves
to increase the penalty for underestimated network
distances.

4.3.1 Data sets

We used the following (cid:12)ve real-world data sets in sim-
ulation. Parts of the data sets were (cid:12)ltered out to
eliminate missing elements in the distance matrices
(since none of the algorithms except NMF can cope
with missing data).

It is known that these update rules converge mono-
tonically to stationary points of the error function,

The network distances in the data sets are round-
trip time (RTT) between pairs of Internet hosts. RTT

5

is symmetric between two end hosts, but it does vio-
late the triangle inequality and also give rise to other
e(cid:11)ects (described in Section 2.2) that are poorly mod-
eled by network embeddings in Euclidean space.

(cid:15) NLANR: The NLANR Active Measurement
Project [1] collects a variety of measurements
between all pairs of participating nodes. The
nodes are mainly at NSF supported HPC sites,
with about 10% outside the US. The data set we
used was collected on January 30, 2003, consist-
ing of measurements of a 110(cid:2) 110 clique. Each
host was pinged once per minute, and network
distance was taken as the minimum of the ping
times over the day.

(cid:15) GNP and AGNP: The GNP project measured
minimum round trip time between 19 active sites
in May 2001. About half of the hosts are in
North America; the rest are distributed globally.
We used GNP to construct a symmetric 19 (cid:2) 19
data set and AGNP to construct an asymmetric
869 (cid:2) 19 dataset.

(cid:15) P2PSim:

The P2Psim project

[14] mea-
sured a distance matrix of RTTs among about
2000 Internet DNS servers based on the King
method [8]. The DNS servers were obtained from
an Internet-scale Gnutella network trace.

(cid:15) PL-RTT: Obtained from PlanetLab pairwise
ping project [19]. We chose the minimum RTT
measured at 3/23/2004 0:00 EST. A 169 (cid:2) 169
full distance matrix was obtained by (cid:12)ltering out
missing values.

4.3.2 Simulated Results

Figure 2 illustrates the cumulative density function
(CDF) of relative errors of RTT reconstructed by
SVD when d = 10, on 5 RTT data sets. The best re-
sult is over GNP data set: more than 90% distances
are reconstructed within 9% relative error. This is
not too surprising because the GNP data set only
contains 19 nodes. However, SVD also works well
over NLANR, which has more than 100 nodes: about
90% fraction of distances are reconstructed within
15% relative error. Over P2PSim and PL-RTT data
sets, SVD achieves similar accuracy results: 90 per-
centile relative error is 50%. We ran the same tests
on NMF and observed similar results. Therefore,
we chose NLANR and P2PSim as two representative
data sets for the remaining simulations.

GNP

NLANR

AGNP

P2PSim

PLRTT

1

0.8

0.6

0.4

0.2

NLANR
GNP
AGNP
PLRTT
P2PSim

y
t
i
l
i

b
a
b
o
r
p



e
v
i
t

l

a
u
m
u
c

0
0

0.2

0.6
0.4
relative error

0.8

1

Figure 2: Cumulative distribution of relative error by
SVD over various data sets, d = 10

Figure 3 compares the reconstruction accuracy of
three algorithms: matrix factorization by SVD and
NMF, and PCA applied to the Lipschitz embedding.
The algorithms were simulated over NLANR and
P2PSim data sets.
It is shown that NMF has al-
most exactly the same median relative errors as SVD
on both data sets when the dimension d < 10. Both
NMF and SVD yield much more accurate results than
Lipschitz: the median relative error of SVD and NMF
is more than 5 times smaller than Lipschitz when
d = 10. SVD is slightly better than NMF when d is
large. The reason for this may be that the algorithm
for NMF is only guaranteed to converge to local min-
ima. Considering that the hosts in the data sets come
from all over the Internet, the results show that ma-
trix factorization is a scalable approach to modeling
distances in large-scale networks. In terms of main-
taining a low-dimensional representation, d (cid:25) 10 ap-
pears to be a good tradeo(cid:11) between complexity and
accuracy for both SVD and NMF.

5 Distance Prediction

The simulation results from the previous section
demonstrate that pairwise distances in large-scale
networks are well modeled by matrix factorization.
In this section we present the Internet Distance Esti-
mation Service (IDES) | a scalable and robust ser-
vice based on matrix factorization to estimate net-
work distances between arbitrary Internet hosts.

6

r
o
r
r
e
e
v
i
t



l

a
e
r

n
a
d
e
m

i

0.5

0.4

0.3

0.2

0.1

0
0

Lipschitz+PCA
SVD
NMF

Lipschitz+PCA
SVD
NMF

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r
e

e
v
i
t
a
e
r

n
a
d
e
m

i

l

20

40

dimension

60

80

0
0

20

40
60
dimension

80

100

(a) Comparison over NLANR data set

(b) Comparison over P2PSim data set

Figure 3: Reconstruction error comparison of SVD, NMF and Lipschitz over NLANR and P2PSim data set

5.1 Basic architecture

We classify Internet hosts into two categories: land-
mark nodes and ordinary hosts. Landmark nodes
are a set of well-positioned distributed hosts. The
network distances between each of them is available
to the information server of IDES. We assume that
landmarks can measure network distances to others
and report the results to the information server. The
information server can also measure the pairwise dis-
tances via indirect methods without landmark sup-
port, e.g. by the King method [8] if the metric is
RTT. An ordinary host is an arbitrary end node in
the Internet, which is identi(cid:12)ed by a valid IP address.
Suppose there are m landmark nodes. The (cid:12)rst
step of IDES is to gather the m(cid:2) m pairwise distance
matrix D on the information server. Then, we can
apply either SVD or NMF algorithm over D to obtain
landmark outgoing and incoming vectors ~Xi and ~Yi
in d dimensions, d < m, for each host Hi. As before,
we use X and Y to denote the d (cid:2) m matrices with
~Xi and ~Yi as row vectors. Note that NMF can be
used even when D contains missing elements.

Now suppose an ordinary host Hnew wants to
gather distance information over the network. The
(cid:12)rst step is to calculate its outgoing vector ~Xnew and
incoming vector ~Ynew. To this end, it measures the
network distances to and from the landmark nodes.
We denote Dout
as the distance to landmark i, and
Din
i as the distance from landmark i to the host. Ide-
ally, we would like the outgoing and incoming vectors

i

to satisfy Dout
solution with the least squares error is given by:

i = ~Xnew (cid:1) ~Yi and Din

i = ~Xi (cid:1) ~Ynew. The

m

~Xnew = arg min

~U 2Rd

~Ynew = arg min

~U 2Rd

m

Xi=1
Xi=1

(Dout

i (cid:0) ~U (cid:1) ~Yi)2

(Din

i (cid:0) ~Xi (cid:1) ~U )2

(11)

(12)

The global minima of these error functions, computed
by simple matrix operations, have the closed form:

~Xnew = (DoutY )(Y T Y )(cid:0)1
~Ynew = (DinX)(X T X)(cid:0)1

(13)

(14)

Eqs. (13{14) assume that the optimizations are un-
constrained. Alternatively, one can impose nonnega-
tivity constraints on ~Xnew and ~Ynew; this will guaran-
tee that the predicted distances are themselves non-
negative (assuming that the landmark distance ma-
trix was also modeled by NMF). The least squared
error problems in Eqs. (11{12) can be solved with
nonnegativity constraints, but the solution is some-
what more complicated. Our simulation results did
not reveal any signi(cid:12)cant di(cid:11)erence between the pre-
diction accuracies of least squares solutions with and
without non-negativity constraints; thus, in what fol-
lows, we focus on the simpler unconstrained solutions
in Eqs. (13{14).

We give a simple example of this procedure in Fig-
ure 4. The network is an enlarged version of the net-
work in Figure 1, with the four original nodes serv-
ing as landmarks and two new nodes introduced as

7

H(cid:13)1(cid:13)

L(cid:13)1(cid:13)

0.5(cid:13)

1(cid:13)

1(cid:13)

L(cid:13)2(cid:13)

L(cid:13)3(cid:13)

1(cid:13)

1(cid:13)

L(cid:13)4(cid:13)

0.5(cid:13)

H(cid:13)2(cid:13)

Figure 4: Four landmark nodes L1 (cid:0) L4 and two or-
dinary hosts H1, H2 interconnected by a simple net-
work topology

ordinary hosts. The (cid:12)rst step is to measure inter-
landmark distances and calculate landmark incom-
ing and outgoing vectors. We used SVD to factor
the landmark distance matrix in this example. The
result is the same as the example in section 4:

X =2
664

3
(cid:0)1
0
1
(cid:0)1 (cid:0)1
0
775
1
(cid:0)1
0
0 (cid:0)1
(cid:0)1

; Y =2
664

3
0 (cid:0)1
(cid:0)1
(cid:0)1
1
0
775
0
(cid:0)1 (cid:0)1
0
(cid:0)1
1

Note that SVD can be substituted by NMF and the
following steps are identical.

Second, we measure the distance vectors for the
ordinary hosts: Dout = Din = [0:5 1:5 1:5 2:5] for
ordinary host H1. According to Eqs. (13 { 14),
~XH1 = [(cid:0)1:5 0 1], ~YH1 = [(cid:0)1:5 0 (cid:0) 1]. Similarly, we
obtain the distance vector of H2 as [2:5 1:5 1:5 0:5],
and calculate its outgoing and incoming vectors:
~XH2 = [(cid:0)1:5 0 (cid:0)1], ~YH2 = [(cid:0)1:5 0 1]. One can verify
that distances between ordinary hosts and landmarks
are exactly preserved. The distance between two or-
dinary hosts is not measured, but can be estimated as
~XH1 (cid:1) ~YH2 = ~XH2 (cid:1) ~YH1 = 3:25, while the real network
distance is 3.

5.2 Optimization

The basic architecture requires an ordinary host to
measure network distances to all landmarks, which
limits the scalability of IDES. Furthermore, if some
of the landmark nodes experience transient failures
or a network partition, an ordinary host may not be
able to retrieve the measurements it needs to solve
Eqs. (13{14).

To improve the scalability and robustness of IDES,
we propose a relaxation to the basic architecture: an
ordinary host Hnew only has to measure distances to
a set of k nodes with pre-computed outgoing and in-
coming vectors. The k nodes can be landmark nodes,

8

or other ordinary hosts that have already computed
their vectors. Suppose the outgoing vectors of those
k nodes are ~X1; ~X2;(cid:1)(cid:1)(cid:1) ; ~Xk and the incoming vectors
are ~Y1; ~Y2;(cid:1)(cid:1)(cid:1) ; ~Yk. We measure Dout
i as the
distance from and to the ith node, for all i = 1;(cid:1)(cid:1)(cid:1) ; k.
Calculating the new vectors ~Xnew and ~Ynew for Hnew
is done by solving the least squares problems:

and Din

i

~Xnew = arg min

~U 2Rd

~Ynew = arg min

~U 2Rd

k

k

Xi=1
Xi=1

(Dout

i (cid:0) ~U (cid:1) ~Yi)2

(15)

(Din

i (cid:0) ~Xi (cid:1) ~U )2

(16)

The solution is exactly the same form as described
in Eq. (13) and Eq. (14). The constraint k (cid:21) d is
necessary (and usually su(cid:14)cient) to ensure that the
problem is not singular. In general, larger values of k
lead to better prediction results, as they incorporate
more measurements of network distances involving
Hnew into the calculation of the vectors ~Xnew and
~Ynew.
We use the topology in Figure 4 again to demon-
strate how the system works. As in the basic archi-
tecture, the (cid:12)rst step is to measure inter-landmark
distances and calculate landmark outgoing and in-
coming vectors. Secondly, the ordinary host H1 mea-
sures the distances to L1, L2 and L3 as [0.5 1.5 1.5].
By Eq. (13) and Eq. (14), the vectors are ~XH1 =[-1.5
0 1], ~YH1 =[-1.5 0 -1]. Note that we did not measure
the distance between H1 and L4, but it can be esti-
mated as ~XH1(cid:1) ~YL4 =[-1.5 0 1](cid:1)[-1 0 1]= 2:5, which is in
fact the true distance. Finally, the ordinary host H2
measures the distances to L2, L4 and H1 as [1.5 0.5
3]. Because all of them already have pre-computed
vectors, H2 can compute its own vectors by Eq. (13)
and Eq. (14). The results are ~XH2 =[-1.4 0.1 -0.9],
~YH2 =[-1.4 -0.1 0.9]. The distances between ordinary
host H2 and L1/L3 are not measured directly, but
can be estimated as ~XH2 (cid:1) ~YL1 =[-1.4 0.1 -0.9](cid:1)[-1 0
-1]= 2:3 and ~XH2 (cid:1) ~YL3 =[-1.4 0.1 -0.9](cid:1)[-1 -1 0]= 1:3.
This example illustrates that even without mea-
surement to all landmarks, the estimated distances
can still be accurate. In this example, most of the
pairwise distances are exactly preserved; the maxi-
mum relative error is 15% when predicting the dis-
tance between H2 and L2. In the example, the load is
well distributed among landmarks. As shown in Fig-
ure 5, distances to L2 are only measured twice dur-
ing this estimation procedure. Such a scheme allows
IDES to scale to a large number of ordinary hosts and
landmarks. It is also robust against partial landmark

1.5/1.5(cid:13)

H(cid:13)1(cid:13)

L(cid:13)1(cid:13)

0.5/0.5(cid:13)

1.5/1.5(cid:13)

2.5/2.5(cid:13)

L(cid:13)4(cid:13)

L(cid:13)2(cid:13)

L(cid:13)3(cid:13)

L(cid:13)2(cid:13)

2.5/2.3(cid:13)

1.5/1.5(cid:13)

H(cid:13)1(cid:13)

L(cid:13)1(cid:13)

L(cid:13)4(cid:13)

H(cid:13)2(cid:13)

0.5/0.5(cid:13)

1.5/1.3(cid:13)

3/3(cid:13)

L(cid:13)3(cid:13)

Figure 5: Learning outgoing and incoming vectors
for two ordinary hosts. Solid lines indicate that real
network measurement is conducted. Each edge is an-
notated with (real network distance / estimated dis-
tance).

failures.

6 Evaluation

In this section we evaluate IDES, using SVD and
NMF algorithms to learn models of network dis-
tances, and compare them to the GNP [13] and
ICS [12] systems.

The experiments were performed on a Dell Dimen-
sion 4600 with Pentium 4 3.2GHz CPU, 2GB RAM.
The GNP implementation was obtained from the of-
(cid:12)cial GNP software release written in C. We imple-
mented IDES and ICS in MatLab 6.0.
We identify four evaluation criteria:

(cid:15) E(cid:14)ciency

We measure e(cid:14)ciency by the total running time
required by a system to build its model of net-
work distances between all landmark nodes and
ordinary hosts.

(cid:15) Accuracy

The prediction error between Dij and ^Dij should
be small. We use the modi(cid:12)ed relative error
function in Eq. (10) to evaluate accuracy, which
is also used in GNP and Vivaldi. Note that pre-
dicted distances are computed between ordinary

hosts that have not conducted any network mea-
surements of their distance. Predicted distance
errors are di(cid:11)erent than reconstructed distance
errors (where actual network measurements are
conducted).

(cid:15) Scalability

The storage requirements are O(d) for models
based on network embeddings (with one position
vector for each host) and matrix factorizations
(with one incoming and outgoing vector for each
host).
In large-scale networks, the number of
hosts N is very large. The condition d (cid:28) N
allows the model to scale, assuming that rea-
sonable accuracy of predicted distances is main-
tained. Also, to support multiple hosts concur-
rently,
it is desirable to distribute the load|
for instance, by only requiring distance measure-
ments to partial sets of landmarks.

(cid:15) Robustness

A robust system should be resilient against host
failures and temporary network partitioning. In
particular, partial
landmark nodes
should not prevent the system from building
models of network distances.

failure of

6.1 E(cid:14)ciency and accuracy

We use three data sets for evaluating accuracy and
e(cid:14)ciency.

(cid:15) GNP: 15 out of 19 nodes in the symmetric data
set were selected as landmarks. The rest of the
4 nodes and the 869 nodes in the AGNP data
set were selected as ordinary hosts. Prediction
accuracy was evaluated on 869(cid:2) 4 pairs of hosts.
(cid:15) NLANR: 20 out of 110 nodes were selected ran-
domly as landmarks. The remaining 90 nodes
were treated as ordinary hosts. The prediction
accuracy was evaluated on 90(cid:2) 90 pairs of hosts.
(cid:15) P2PSim: 20 out of 1143 nodes were selected ran-
domly as landmarks. The remaining 1123 nodes
were treated as ordinary hosts. The prediction
accuracy was evaluated on 1123 (cid:2) 1123 pairs of
hosts.

Although deliberate placement of landmarks may
yield more accurate results, we chose the landmarks
randomly since in general they may be placed any-
where on the Internet. A previous study also shows
that random landmark selection is fairly e(cid:11)ective if

9

data set

IDES/SVD IDES/NMF

GNP

NLANR
P2PSim

0.10s
0.01s
0.16s

0.12s
0.02s
0.17s

ICS
0.02s
0.01s
0.03s

GNP

1min 19s
4min 44s
2min 30s

1

0.8

y
t
i
l
i

GNP

IDES/NMF

IDES/SVD

ICS

IDES/SVD
IDES/NMF
ICS
GNP

Table 1: E(cid:14)ciency comparison on IDES, ICS and
GNP over four data sets

more than 20 landmarks are employed [21]. To ensure
fair comparisons, we used the same set of landmarks
for all four algorithms. We also repeated the simu-
lation several times, and no signi(cid:12)cant di(cid:11)erences in
results were observed from one run to the next.

Table 1 illustrates the running time comparison
between IDES, ICS and GNP. GNP is much more
ine(cid:14)cient than the IDES and ICS. This is because
GNP uses Simplex Downhill method, which con-
verges slowly to local minima. Both IDES and ICS
have running time less than 1 second, even when the
data sets contain thousands of nodes. It is possible
to reduce the running time of GNP by sacrifying the
accuracy, but the parameters are hard to tune, which
is another drawback of Simplex Downhill method.

Figure 6 plots the CDF of prediction errors for
IDES using SVD, IDES using NMF, ICS and GNP
over the three data sets respectively. In Figure 6(a),
the GNP system is the most accurate system for the
GNP data set.
IDES using SVD and NMF are as
accurate as GNP for 70% of the predicted distances.
The GNP data set is somewhat atypical, however, in
that the predicted distance matrix has many more
columns (869) than rows (4). Figure 6(b) and 6(c)
depict the CDF of prediction errors over NLANR and
P2PSim data sets, which are more typical. In both
cases, IDES has the best prediction accuracy. On
the NLANR data set, IDES yields better results than
GNP and ICS: the median relative error of IDES us-
ing SVD is only 0.03. Its 90 percentile relative error
is about 0.23. The accuracy is worse for all three sys-
tems in P2PSim data set than in NLANR data set.
However, IDES (with either SVD or NMF) is still the
most accurate system among the three. The better
prediction results on the NLANR data set may be due
to the fact that 90% of the hosts in NLANR are in
North America and the network distances, computed
from minimum RTT over a day, are not a(cid:11)ected much
by queueing delays and route congestion. These prop-
erties make the data set more uniform, and therefore,
more easily modeled by a low dimensional represen-
tation.

10

0.6

b
a
b
o
r
p

e
v
i
t
a
u
m
u
c

l

0.4

0.2

0
0

0.2

0.4
0.6
relative error

0.8

1

(a) CDF of relative error over GNP data set,
15 landmarks

IDES/NMF

IDES/SVD

GNP

ICS

1

0.8

0.6

0.4

0.2

IDES/SVD
IDES/NMF
ICS
GNP

y
t
i
l
i

b
a
b
o
r
p
e
v
i
t



l

a
u
m
u
c

0
0

0.2

0.4
0.6
relative error

0.8

1

(b) CDF of relative error over NLANR data
set, 20 landmarks

y
t
i
l
i

b
a
b
o
r
p
e
v
i
t



l

a
u
m
u
c

1

0.8

0.6

0.4

0.2

0
0

IDES/NMF

IDES/SVD

GNP

ICS

IDES/SVD
IDES/NMF
ICS
GNP

0.2

0.4
0.6
relative error

0.8

1

(c) CDF of relative error over P2PSim data
set, 20 landmarks

Figure 6: Accuracy comparison on IDES using SVD
and NMF, ICS, and GNP, d = 8

r
o
r
r
e

e
v
i
t
a
e
r

n
a
d
e
m

i

l

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0
0

20 landmarks, d=8
50 landmarks, d=8

20 landmarks, d=10
50 landmarks, d=10

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

r
o
r
r
e

e
v
i
t
a
e
r

n
a
d
e
m

i

l

0.2
0.8
fraction of unobserved landmarks

0.4

0.6

1

0.1
0

0.2

fraction of unobserved landmarks

0.4

0.6

0.8

(a) over NLANR data set

(b) over P2PSim data set

Figure 7: the correlation between accuracy and landmark failures on IDES using SVD algorithm.

6.2 Scalability and robustness

In the previous subsection, we showed that IDES can
accurately model the network distances in low dimen-
sions d (cid:20) 10, which is fundamental to make the sys-
tem scale to large-scale networks. In this subsection,
we study the impact of partially observed landmarks
on the accuracy of IDES. Measuring the distances to
only a subset of landmark nodes reduces the overall
load and allows the system to support more ordinary
hosts concurrently. It also makes the system robust
to partial landmark failures.

We simulated partially observed landmark scenar-
ios in IDES using SVD to model partial distance ma-
trices from the NLANR and P2PSim data sets. For
each data set, we experimented with two settings: 20
random landmarks and 50 random landmarks. The
simulation results are shown in Figure 7. The x-axis
indicates the fraction of unobserved landmarks. The
unobserved landmarks for each ordinary host were in-
dependently generated at random. When the number
of landmarks is less than twice the model dimension-
ality d, the accuracy appears sensitive to the fraction
of unobserved landmarks. However, as the number of
landmarks increases, the system tolerates more fail-
ure: for example, not observing 40% of the landmarks
has little impact on the system accuracy when 50
landmarks are used in the test.

between arbitrary Internet hosts. Our model imposes
fewer constraints on network distances than models
based on low dimensional embeddings;
in particu-
lar, it can represent distances that violate the tri-
angle inequality, as well as asymmetric network dis-
tances. Such a model is more suitable for modeling
the topology and complex routing policies on the In-
ternet. Based on this model, we proposed the IDES
system and two learning algorithms, SVD and NMF,
for factoring matrices of network distances between
arbitrary Internet hosts. Simulations on real world
data sets have shown that IDES is computationally
e(cid:14)cient, scalable to large-scale networks, more accu-
rate than previous models, and resilient to temporary
landmark failures.

8 Acknowledgments

We are grateful to Jonathan M. Smith (UPenn)
for helpful comments on the manuscript, and Frank
Dabek (MIT) for sharing the P2PSim data set. This
material is based upon work supported by the Na-
tional Science Foundation under Grant No. 0238323
and DARPA under contract F30602-99-1-0512.

