Abstract

We study the problem of parameter estimation in continuous density hidden
Markov models (CD-HMMs) for automatic speech recognition (ASR). As in sup-
port vector machines, we propose a learning algorithm based on the goal of mar-
gin maximization. Unlike earlier work on max-margin Markov networks, our
approach is specically geared to the modeling of real-valued observations (such
as acoustic feature vectors) using Gaussian mixture models. Unlike previous dis-
criminative frameworks for ASR, such as maximum mutual information and min-
imum classication error, our framework leads to a convex optimization, without
any spurious local minima. The objective function for large margin training of
CD-HMMs is dened over a parameter space of positive semidenite matrices.
Its optimization can be performed efciently with simple gradient-based meth-
ods that scale well to large problems. We obtain competitive results for phonetic
recognition on the TIMIT speech corpus.

1 Introduction

As a result of many years of widespread use, continuous density hidden Markov models (CD-
HMMs) are very well matched to current front and back ends for automatic speech recognition
(ASR) [21]. Typical front ends compute real-valued feature vectors from the short-time power spec-
tra of speech signals. The distributions of these acoustic feature vectors are modeled by Gaussian
mixture models (GMMs), which in turn appear as observation models in CD-HMMs. Viterbi de-
coding is used to solve the problem of sequential classication in ASRnamely, the mapping of
sequences of acoustic feature vectors to sequences of phonemes and/or words, which are modeled
by state transitions in CD-HMMs.

The simplest method for parameter estimation in CD-HMMs is the Expectation-Maximization (EM)
algorithm. The EM algorithm is based on maximizing the joint likelihood of observed feature vectors
and label sequences. It is widely used due to its simplicity and scalability to large data sets, which
are common in ASR. A weakness of this approach, however, is that the model parameters of CD-
HMMs are not optimized for sequential classication: in general, maximizing the joint likelihood
does not minimize the phoneme or word error rates, which are more relevant metrics for ASR.

Noting this weakness, many researchers in ASR have studied alternative frameworks for parame-
ter estimation based on conditional maximum likelihood [11], minimum classication error [4] and
maximum mutual information [20]. The learning algorithms in these frameworks optimize discrim-
inative criteria that more closely track actual error rates, as opposed to the EM algorithm for maxi-
mum likelihood estimation. These algorithms do not enjoy the simple update rules and relatively fast
convergence of EM, but carefully and skillfully implemented, they lead to lower error rates [13, 20].

Recently, in a new approach to discriminative acoustic modeling, we proposed the use of large
margin GMMs for multiway classication [15]. Inspired by support vector machines (SVMs), the
learning algorithm in large margin GMMs is designed to maximize the distance between labeled ex-
amples and the decision boundaries that separate different classes [19]. Under mild assumptions, the
required optimization is convex, without any spurious local minima. In contrast to SVMs, however,
large margin GMMs are very naturally suited to problems in multiway (as opposed to binary) clas-
sication; also, they do not require the kernel trick for nonlinear decision boundaries. We showed
how to train large margin GMMs as segment-based phonetic classiers, yielding signicantly lower
error rates than maximum likelihood GMMs [15]. The integrated large margin training of GMMs
and transition probabilities in CD-HMMs, however, was left as an open problem.

We address that problem in this paper, showing how to train large margin CD-HMMs in the more
general setting of sequential (as opposed to multiway) classication. In this setting, the GMMs
appear as acoustic models whose likelihoods are integrated over time by Viterbi decoding. Experi-
mentally, we nd that large margin training of HMMs for sequential classication leads to signicant
improvement beyond the frame-based and segment-based discriminative training in [15].

Our framework for large margin training of CD-HMMs builds on ideas from many previous studies
in machine learning and ASR. It has similar motivation as recent frameworks for sequential classi-
cation in the machine learning community [1, 6, 17], but differs in its focus on the real-valued acous-
tic feature representations used in ASR. It has similar motivation as other discriminative paradigms
in ASR [3, 4, 5, 11, 13, 20], but differs in its goal of margin maximization and its formulation
of the learning problem as a convex optimization over positive semidenite matrices. The recent
margin-based approach of [10] is closest in terms of its goals, but entirely different in its mechanics;
moreover, its learning is limited to the mean parameters in GMMs.

2 Large margin GMMs for multiway classication

Before developing large margin HMMs for ASR, we briey review large margin GMMs for multi-
way classication [15]. The problem of multiway classication is to map inputs x  d to labels
y  {1, 2, . . . , C}, where C is the number of classes. Large margin GMMs are trained from a set of
labeled examples {(xn, yn)}N
n=1. They have many parallels to SVMs, including the goal of margin
maximization and the use of a convex surrogate to the zero-one loss [19]. Unlike SVMs, where
classes are modeled by half-spaces, in large margin GMMs the classes are modeled by collections
of ellipsoids. For this reason, they are more naturally suited to problems in multiway as opposed to
binary classication. Sections 2.12.3 review the basic framework for large margin GMMs: rst,
the simplest setting in which each class is modeled by a single ellipsoid; second, the formulation
of the learning problem as a convex optimization; third, the general setting in which each class is
modeled by two or more ellipsoids. Section 2.4 presents results on handwritten digit recognition.

2.1 Parameterization of the decision rule

The simplest large margin GMMs model each class by a single ellipsoid in the input space. The
ellipsoid for class c is parameterized by a centroid vector c  d and a positive semidenite
matrix c  dd that determines its orientation. Also associated with each class is a nonnegative
scalar offset c  0. The decision rule labels an example x  d by the class whose centroid yields
the smallest Mahalanobis distance:

y = argmin

c

(cid:8)(xc)Tc(xc) + c(cid:9) .

(1)

The decision rule in eq. (1) is merely an alternative way of parameterizing the maximum a posterior
(MAP) label in traditional GMMs with mean vectors c, covariance matrices 1
c , and prior class
probabilities pc, given by y = argminc { pc N (c, 1
The argument on the right hand side of the decision rule in eq. (1) is nonlinear in the ellipsoid
parameters c and c. As shown in [15], however, a useful reparameterization yields a simpler
expression. For each class c, the reparameterization collects the parameters {c, c, c} in a single
enlarged matrix c  (d+1)(d+1):

c ) }.

c = (cid:20) c

T
c

c

c c

cc + c (cid:21) .

T
c

(2)

Note that c is positive semidenite. Furthermore, if c is strictly positive denite, the parameters
{c, c, c} can be uniquely recovered from c. With this reparameterization, the decision rule in
eq. (1) simplies to:

y = argmin

c

(cid:8)zTc z(cid:9) where

z = (cid:20) x

1 (cid:21).

(3)

The argument on the right hand side of the decision rule in eq. (3) is linear in the parameters c. In
what follows, we will adopt the representation in eq. (3), implicitly constructing the augmented
vector z for each input vector x. Note that eq. (3) still yields nonlinear (piecewise quadratic) deci-
sion boundaries in the vector z.

2.2 Margin maximization

Analogous to learning in SVMs, we nd the parameters {c} that minimize the empirical risk on
the training datai.e., parameters that not only classify the training data correctly, but also place
the decision boundaries as far away as possible. The margin of a labeled example is dened as its
distance to the nearest decision boundary. If possible, each labeled example is constrained to lie at
least one unit distance away from the decision boundary to each competing class:

c 6= yn, zT

n (c  yn) zn  1.

(4)

Fig. 1 illustrates this idea. Note that in the realizable setting where these constraints can be
simultaneously satised, they do not uniquely determine the parameters {c}, which can be scaled
to yield arbitrarily large margins. Therefore, as in SVMs, we propose a convex optimization that
selects the smallest parameters that satisfy the large margin constraints in eq. (4). In this case, the
optimization is an instance of semidenite programming [18]:

min Pc trace(c)

s.t.

1 + zT
c  0,

n (yn  c)zn  0,
c = 1, 2, . . . , C

c 6= yn, n = 1, 2, . . . , N

(5)

Note that the trace of the matrix c appears in the above objective function, as opposed to the trace
of the matrix c, as dened in eq. (2); minimizing the former imposes the scale regularization only
on the inverse covariance matrices of the GMM, while the latter would improperly regularize the
mean vectors as well. The constraints c  0 restrict the matrices to be positive semidenite.

The objective function must be modied for training data that lead to infeasible constraints in eq. (5).
As in SVMs, we introduce nonnegative slack variables nc to monitor the amount by which the
margin constraints in eq. (4) are violated [15]. The objective function in this setting balances the
margin violations versus the scale regularization:

min Pnc nc + Pc trace(c)

n (yn  c)zn  nc,

s.t.

1 + zT
nc  0,
c  0,

c 6= yn, n = 1, 2, . . . , N
c = 1, 2, . . . , C

(6)

where the balancing hyperparameter  > 0 is set by some form of cross-validation. This optimization
is also an instance of semidenite programming.

2.3 Softmax margin maximization for multiple mixture components

Lastly we review the extension to mixture modeling where each class is represented by multiple
ellipsoids [15]. Let cm denote the matrix for the mth ellipsoid (or mixture component) in class
c. We imagine that each example xn has not only a class label yn, but also a mixture component
label mn. Such labels are not provided a priori in the training data, but we can generate proxy
labels by tting GMMs to the examples in each class by maximum likelihood estimation, then for
each example, computing the mixture component with the highest posterior probability.

In the setting where each class is represented by multiple ellipsoids, the goal of learning is to ensure
that each example is closer to its target ellipsoid than the ellipsoids from all other classes. Speci-
cally, for a labeled example (xn, yn, mn), the constraint in eq. (4) is replaced by the M constraints:

c 6= yn, m, zT

n (cm  ynmn )zn  1,

(7)

d

e

c

i

s

i

o

n

b

o

u

n

d

a

r

y

a
a

r
r

g
g

i
i

n
n

m
m

Figure 1: Decision boundary in a large margin GMM:
labeled examples lie at least one unit of distance away.

mixture

1
2
4
8

EM margin
4.2% 1.4%
3.4% 1.4%
3.0% 1.2%
3.3% 1.5%

Table 1: Test error rates on MNIST
digit recognition: maximum likeli-
hood versus large margin GMMs.

where M is the number of mixture components (assumed, for simplicity, to be the same for each
class). We fold these multiple constraints into a single one by appealing to the softmax inequal-

ity: minm am   logPm eam . Specically, using the inequality to derive a lower bound on

cm zn, we replace the M constraints in eq. (7) by the stricter constraint:

minm zT
n

c 6= yn,  logXm

ezT

n

cmzn  zT
n

ynmn zn  1.

(8)

We will use a similar technique in section 3 to handle the exponentially many constraints that arise
in sequential classication. Note that the inequality in eq. (8) implies the inequality of eq. (7) but not
vice versa. Also, though nonlinear in the matrices {cm}, the constraint in eq. (8) is still convex.

The objective function in eq. (6) extends straightforwardly to this setting. It balances a regularizing
term that sums over ellipsoids versus a penalty term that sums over slack variables, one for each
constraint in eq. (8). The optimization is given by:

min Pnc nc + Pcm trace(cm)

s.t.

cmzn  nc,

(9)

ynmn zn + logPm ezT

c 6= yn, n = 1, 2, . . . , N

n

1 + zT
n
nc  0,
cm  0,

c = 1, 2, . . . , C, m = 1, 2, . . . , M

This optimization is not an instance of semidenite programming, but it is convex. We discuss how
to perform the optimization efciently for large data sets in appendix A.

2.4 Handwritten digit recognition

We trained large margin GMMs for multiway classication of MNIST handwritten digits [8]. The
MNIST data set has 60000 training examples and 10000 test examples. Table 1 shows that the large
margin GMMs yielded signicantly lower test error rates than GMMs trained by maximum likeli-
hood estimation. Our best results are comparable to the best SVM results (1.0-1.4%) on deskewed
images [8] that do not make use of prior knowledge. For our best model, with four mixture compo-
nents per digit class, the core training optimization over all training examples took ve minutes on a
PC. (Multiple runs of this optimization on smaller validation sets, however, were also required to set
two hyperparameters: the regularizer for model complexity, and the termination criterion for early
stopping.)

3 Large margin HMMs for sequential classication

In this section, we extend the framework in the previous section from multiway classication to
sequential classication. Particularly, we have in mind the application to ASR, where GMMs are
used to parameterize the emission densities of CD-HMMs. Strictly speaking, the GMMs in our
framework cannot be interpreted as emission densities because their parameters are not constrained
to represent normalized distributions. Such an interpretation, however, is not necessary for their use
as discriminative models. In sequential classication by CD-HMMs, the goal is to infer the correct
hidden state sequence y = [y1, y2, . . . , yT ] given the observation sequence X = [x1, x2, . . . , xT ].
In the application to ASR, the hidden states correspond to phoneme labels, and the observations are

acoustic feature vectors. Note that if an observation sequence has length T and each label can belong
to C classes, then the number of incorrect state sequences grows as O(C T ). This combinatorial
explosion presents the main challenge for large margin methods in sequential classication: how to
separate the correct hidden state sequence from the exponentially large number of incorrect ones.

The section is organized as follows. Section 3.1 explains the way that margins are computed for se-
quential classication. Section 3.2 describes our algorithm for large margin training of CD-HMMs.
Details are given only for the simple case where the observations in each hidden state are modeled
by a single ellipsoid. The extension to multiple mixture components closely follows the approach
in section 2.3 and can be found in [14, 16]. Margin-based learning of transition probabilities is
likewise straightforward but omitted for brevity. Both these extensions were implemented, however,
for the experiments on phonetic recognition in section 3.3.

3.1 Margin constraints for sequential classication

We start by dening a discriminant function over state (label) sequences of the CD-HMM. Let
a(i, j) denote the transition probabilities of the CD-HMM, and let s denote the ellipsoid pa-
rameters of state s. The discriminant function D(X, s) computes the score of the state sequence
s = [s1, s2, . . . , sT ] on an observation sequence X = [x1, x2, . . . , xT ] as:

D(X, s) = Xt

log a(st1, st) 

T

Xt=1

zT
t

st zt.

(10)

This score has the same form as the log-probability log P (X, s) in a CD-HMM with Gaussian emis-
sion densities. The rst term accumulates the log-transition probabilities along the state sequence,
while the second term accumulates acoustic scores computed as the Mahalanobis distances to
each states centroid. In the setting where each state is modeled by multiple mixture components,
the acoustic scores from individual Mahalanobis distances are replaced with softmax distances of

stmzt , as described in section 2.3 and [14, 16].

the form logPM

m=1 ezT

t

We introduce margin constraints in terms of the above discriminant function. Let H(s, y) denote
the Hamming distance (i.e., the number of mismatched labels) between an arbitrary state sequence s
and the target state sequence y. Earlier, in section 2 on multiway classication, we constrained each
labeled example to lie at least one unit distance from the decision boundary to each competing class;
see eq. (4). Here, by extension, we constrain the score of each target sequence to exceed that of each
competing sequence by an amount equal to or greater than the Hamming distance:

s 6= y, D(X, y)  D(X, s)  H(s, y)

(11)

Intuitively, eq. (11) requires that the (log-likelihood) gap between the score of an incorrect se-
quence s and the target sequence y should grow in proportion to the number of individual label
errors. The appropriateness of such proportional constraints for sequential classication was rst
noted by [17].

3.2 Softmax margin maximization for sequential classication

The challenge of large margin sequence classication lies in the exponentially large number of
constraints, one for each incorrect sequence s, embodied by eq. (11). We will use the same softmax
inequality, previously introduced in section 2.3, to fold these multiple constraints into one, thus
considerably simplifying the optimization required for parameter estimation. We rst rewrite the
constraint in eq. (11) as:

D(X, y) + max
s6=y

{H(s, y) + D(X, s)}  0

(12)

We obtain a more manageable constraint by substituting a softmax upper bound for the max term
and requiring that the inequality still hold:

D(X, y) + logXs6=y

eH(s,y)+D(X,s)  0

(13)

Note that eq. (13) implies eq. (12) but not vice versa. As in the setting for multiway classication,
the objective function for sequential classication balances two terms: one regularizing the scale of

the GMM parameters, the other penalizing margin violations. Denoting the training sequences by
{X n, yn}N
n=1 and the slack variables (one for each training sequence) by n  0, we obtain the
following convex optimization:

min Pn n + Pcm trace(cm)
s.t. D(X n, yn) + logPs6=yn

n  0, n = 1, 2, . . . , N
cm  0,

c = 1, 2, . . . , C, m = 1, 2, . . . , M

eH(s,yn)+D(X n,s)  n,

(14)

It is worth emphasizing several crucial differences between this optimization and previous ones [4,
11, 20] for discriminative training of CD-HMMs for ASR. First, the softmax large margin constraint
in eq. (13) is a differentiable function of the model parameters, as opposed to the hard maximum
in eq. (12) and the number of classication errors in the MCE training criteria [4]. The constraint and
its gradients with respect to GMM parameters cm and transition parameters a(, ) can be computed
efciently using dynamic programming, by a variant of the standard forward-backward procedure in
HMMs [14]. Second, due to the reparameterization in eq. (2), the discriminant function D(X n, yn)
and the softmax function are convex in the model parameters. Therefore, the optimization eq. (14)
can be cast as convex optimization, avoiding spurious local minima [14]. Third, the optimization not
only increases the log-likelihood gap between correct and incorrect state sequences, but also drives
the gap to grow in proportion to the number of individually incorrect labels (which we believe leads
to more robust generalization). Finally, compared to the large margin framework in [17], the softmax
handling of exponentially large number of margin constraints makes it possible to train on larger data
sets. We discuss how to perform the optimization efciently in appendix A.

3.3 Phoneme recognition

We used the TIMIT speech corpus [7, 9, 12] to perform experiments in phonetic recognition. We
followed standard practices in preparing the training, development, and test data. Our signal pro-
cessing front-end computed 39-dimensional acoustic feature vectors from 13 mel-frequency cepstral
coefcients and their rst and second temporal derivatives. In total, the training utterances gave rise
to roughly 1.2 million frames, all of which were used in training.

We trained baseline maximum likelihood recognizers and two different types of large margin recog-
nizers. The large margin recognizers in the rst group were low-cost discriminative CD-HMMs
whose GMMs were merely trained for frame-based classication. In particular, these GMMs were
estimated by solving the optimization in eq. (8), then substituted into rst-order CD-HMMs for se-
quence decoding. The large margin recognizers in the second group were fully trained for sequential
classication. In particular, their CD-HMMs were estimated by solving the optimization in eq. (14),
generalized to multiple mixture components and adaptive transition parameters [14, 16]. In all the
recognizers, the acoustic feature vectors were labeled by 48 phonetic classes, each represented by
one state in a rst-order CD-HMM.

For each recognizer, we compared the phonetic state sequences obtained by Viterbi decoding to the
ground-truth phonetic transcriptions provided by the TIMIT corpus. For the purpose of comput-
ing error rates, we followed standard conventions in mapping the 48 phonetic state labels down to
39 broader phone categories. We computed two different types of phone error rates, one based on
Hamming distance, the other based on edit distance. The former was computed simply from the
percentage of mismatches at the level of individual frames. The latter was computed by aligning the
Viterbi and ground truth transcriptions using dynamic programming [9] and summing the substitu-
tion, deletion, and insertion error rates from the alignment process. The frame-based phone error
rate computed from Hamming distances is more closely tracked by our objective function for large
margin training, while the string-based phone error rate computed from edit distances provides a
more relevant metric for ASR.

Tables 2 and 3 show the results of our experiments. For both types of error rates, and across all
model sizes, the best performance was consistently obtained by large margin CD-HMMs trained
for sequential classication. Moreover, among the two different types of large margin recognizers,
utterance-based training generally yielded signicant improvement over frame-based training.

Discriminative learning of CD-HMMs is an active research area in ASR. Two types of algorithms
have been widely used: maximum mutual information (MMI) [20] and minimum classication er-

mixture
(per state)

1
2
4
8

(EM)
45%
45%
42%
41%

baseline margin
(frame)

margin

(utterance)

mixture
(per state)

37%
36%
35%
34%

30%
29%
28%
27%

1
2
4
8

baseline margin
(frame)
36.3%
33.5%
32.6%
31.0%

(EM)
40.1%
36.5%
34.7%
32.7%

margin

(utterance)

31.2%
30.8%
29.8%
28.2%

Table 2: Frame-based phone error rates, from
Hamming distance, of different recognizers.
See text for details.

Table 3: String-based phone error rates, from
edit distance, of different recognizers. See text
for details.

ror [4]. In [16], we compare the large margin training proposed in this paper to both MMI and MCE
systems for phoneme recognition trained on the exact same acoustic features. There we nd that the
large margin approach leads to lower error rates, owing perhaps to the absence of local minima in
the objective function and/or the use of margin constraints based on Hamming distances.

4 Discussion

Discriminative learning of sequential models is an active area of research in both ASR [10, 13, 20]
and machine learning [1, 6, 17]. This paper makes contributions to lines of work in both commu-
nities. First, in distinction to previous work in ASR, we have proposed a convex, margin-based
cost function that penalizes incorrect decodings in proportion to their Hamming distance from the
desired transcription. The use of the Hamming distance in this context is a crucial insight from the
work of [17] in the machine learning community, and it differs profoundly from merely penalizing
the log-likelihood gap between incorrect and correct transcriptions, as commonly done in ASR.

Second, in distinction to previous work in machine learning, we have proposed a framework for se-
quential classication that naturally integrates with the infrastructure of modern speech recognizers.
Using the softmax function, we have also proposed a novel way to monitor the exponentially many
margin constraints that arise in sequential classication. For real-valued observation sequences, we
have shown how to train large margin HMMs via convex optimizations over their parameter space of
positive semidenite matrices. Finally, we have demonstrated that these learning algorithms lead to
improved sequential classication on data sets with over one million training examples (i.e., phonet-
ically labeled frames of speech). In ongoing work, we are applying our approach to large vocabulary
ASR and other tasks such as speaker identication and visual object recognition.

A Solver

The optimizations in eqs. (5), (6), (9) and (14) are convex: specically, in terms of the matrices
that parameterize large margin GMMs and HMMs, the objective functions are linear, while the con-
straints dene convex sets. Despite being convex, however, these optimizations cannot be managed
by off-the-shelf numerical optimization solvers or generic interior point methods for problems as
large as the ones in this paper. We devised our own special-purpose solver for these purposes.

For simplicity, we describe our solver for the optimization of eq. (6), noting that it is easily extended
to eqs. (9) and (14). To begin, we eliminate the slack variables and rewrite the objective function in
terms of the hinge loss function: hinge(z) = max(0, z). This yields the objective function:

L = Xn,c6=yn

hinge(cid:0)1 + zT

n (yn c)zn(cid:1) + Xc

trace(c),

(15)

which is convex in terms of the positive semidenite matrices c. We minimize L using a projected
subgradient method [2], taking steps along the subgradient of L, then projecting the matrices {c}
back onto the set of positive semidenite matrices after each update. This method is guaranteed to
converge to the global minimum, though it typically converges very slowly. For faster convergence,
we precede this method with an unconstrained conjugate gradient optimization in the square-root
matrices {c}, where c = cT
c . The latter optimization is not convex, but in practice it rapidly
converges to an excellent starting point for the projected subgradient method.

Acknowledgment

This work was supported by the National Science Foundation under grant Number 0238323. We
thank F. Pereira, K. Crammer, and S. Roweis for useful discussions and correspondence. Part of this
work was conducted while both authors were afliated with the University of Pennsylvania.

