Abstract

An approach to semi-supervised learning is pro-
posed that is based on a Gaussian random eld
model. Labeled and unlabeled data are rep-
resented as vertices in a weighted graph, with
edge weights encoding the similarity between in-
stances. The learning problem is then formulated
in terms of a Gaussian random eld on this graph,
where the mean of the eld is characterized in
terms of harmonic functions, and is efciently
obtained using matrix methods or belief propa-
gation. The resulting learning algorithms have
intimate connections with random walks, elec-
tric networks, and spectral graph theory. We dis-
cuss methods to incorporate class priors and the
predictions of classiers obtained by supervised
learning. We also propose a method of parameter
learning by entropy minimization, and show the
algorithms ability to perform feature selection.
Promising experimental results are presented for
synthetic data, digit classication, and text clas-
sication tasks.

1. Introduction

In many traditional approaches to machine learning, a tar-
get function is estimated using labeled data, which can be
thought of as examples given by a teacher to a student.
Labeled examples are often, however, very time consum-
ing and expensive to obtain, as they require the efforts of
human annotators, who must often be quite skilled. For in-
stance, obtaining a single labeled example for protein shape
classication, which is one of the grand challenges of bio-
logical and computational science, requires months of ex-
pensive analysis by expert crystallographers. The problem
of effectively combining unlabeled data with labeled data
is therefore of central importance in machine learning.

The semi-supervised learning problem has attracted an in-
creasing amount of interest recently, and several novel ap-
proaches have been proposed; we refer to (Seeger, 2001)
for an overview. Among these methods is a promising fam-
ily of techniques that exploit the manifold structure of the
data; such methods are generally based upon an assumption
that similar unlabeled examples should be given the same
classication. In this paper we introduce a new approach
to semi-supervised learning that is based on a random eld
model dened on a weighted graph over the unlabeled and
labeled data, where the weights are given in terms of a sim-
ilarity function between instances.

Unlike other recent work based on energy minimization
and random elds in machine learning (Blum & Chawla,
2001) and image processing (Boykov et al., 2001), we
adopt Gaussian elds over a continuous state space rather
than random elds over the discrete label set. This re-
laxation to a continuous rather than discrete sample space
results in many attractive properties. In particular, the most
probable conguration of the eld is unique, is character-
ized in terms of harmonic functions, and has a closed form
solution that can be computed using matrix methods or
loopy belief propagation (Weiss et al., 2001). In contrast,
for multi-label discrete random elds, computing the low-
est energy conguration is typically NP-hard, and approxi-
mation algorithms or other heuristics must be used (Boykov
et al., 2001). The resulting classication algorithms for
Gaussian elds can be viewed as a form of nearest neigh-
bor approach, where the nearest labeled examples are com-
puted in terms of a random walk on the graph. The learning
methods introduced here have intimate connections with
random walks, electric networks, and spectral graph the-
ory, in particular heat kernels and normalized cuts.

In our basic approach the solution is solely based on the
structure of the data manifold, which is derived from data
features. In practice, however, this derived manifold struc-
ture may be insufcient for accurate classication. We

Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003), Washington DC, 2003.


Figure 1).

to take val-

is the combinatorial

Our strategy is to rst compute a real-valued function

, we form
is an inverse
is the partition function
, which normalizes over

It is not difcult to show that the minimum energy function
is harmonic; namely, it satises

weightings are possible, of course, and may be more appro-
is discrete or symbolic. For our purposes the
fully species the data manifold structure (see

Intuitively, we want unlabeled points that are nearby in the
graph to have similar labels. This motivates the choice of
the quadratic energy function

priate when
matrixE
Ta
_#`
on0 with certain nice properties, and to
then assign labels based on_
. We constrain_
ues_
K on the labeled databD!e.f@ .
bcd^
bc!
JLKNM
5c
I!
b
ji

Kh
To assign a probability distribution on functions_
, wherev
I!1monpqRrst
the Gaussian eldkl
temperature parameter, andwxl
v5c
{	OPRQ
]
wxly!ez>{}|
all functions constrained to_
 on the labeled data.
arg min{}|
{	
5c
, and is equal to_
!+ on unlabeled data points;
. Here
on the labeled data points7
Laplacian, given in matrix form as
!
E where<!
diag]
is the diagonal matrix with entries]
!
KM is the weight matrix.
andE!
The harmonic property means that the value of_
unlabeled data point is the average of_
JKM
b: fori!$.f@o$
ji
I!
KM
_ with respect to the graph. Expressed slightly differently,
, where^!C
!
principle of harmonic functions (Doyle & Snell, 1984),_
unique and is either a constant or it satises+c
i
.
fori';
matrix operations, we split the weight matrixE
) into 4 blocks after the th row and column:
ilarly


E!
>&
,
Letting_
A where_
 denotes the values on the un-
labeled data points, the harmonic solution
!?+ subject
to_
 is given by
,
U,R
!
E,
E>R
!D>

To compute the harmonic solution explicitly in terms of
(and sim-

which is consistent with our prior notion of smoothness of

. Because of the maximum
is

at each
at neighboring

(2)

KNM

(3)

(4)

(5)

points:

.

Figure1.The random elds used in this work are constructed on
labeled and unlabeled examples. We form a graph with weighted
edges between instances (in this case scanned digits), with labeled
data items appearing as special boundarypoints, and unlabeled
points as interiorpoints. We consider Gaussian random elds
on this graph.

show how the extra evidence of class priors can help classi-
cation in Section 4. Alternatively, we may combine exter-
nal classiers using vertex weights or assignment costs,
as described in Section 5. Encouraging experimental re-
sults for synthetic data, digit classication, and text clas-
sication tasks are presented in Section 7. One difculty
with the random eld approach is that the right choice of
graph is often not entirely clear, and it may be desirable to
learn it from data. In Section 6 we propose a method for
learning these weights by entropy minimization, and show
the algorithms ability to perform feature selection to better
characterize the data manifold.

2. Basic Framework

.

correspond-

We suppose there are labeled points	

 ,
and unlabeled points ; typically
Let "!#%$& be the total number of data points. To be-
gin, we assume the labels are binary:('*),+-./ . Consider
a connected graph01!132456 with nodes2
ing to the  data points, with nodes7"!8)9.9%:/ corre-
sponding to the labeled points with labels

 , and
nodes;<!=)>$?.9@%$A/ corresponding to the unla-
beled points. Our task is to assign labels to nodes;
assume an CBD
symmetric weight matrixE
of the graph is given. For example, when<'GFIH
SUT
WYX
Z

JLKNM
!&OPRQ
K represented
is the] -th component of instance
where
, and[

as a vector
'^F

are length scale
hyperparameters for each dimension. Thus, nearby points
in Euclidean space are assigned large edge weight. Other

. We
on the edges
, the

weight matrix can be

(1)


H
V

K
W
T

M
W
[
Z
W
\
K
W
K
H
[
H
2
F
_
_
.
g
V
M

_
T
_
Z
_
u
p
~
X
T
_
_
_
!
~
X
_


_

T
K

K
M
J
J
_
.
]
M
V
_
_
_

E
_
E
E
E
E
!

_

_
_

!
_
_
T


_
T


_

3.5

3

2.5

2

1.5

1

0.5

0
0

4

3

2

1

0
2

1

2

3

0

2

2

2

0

Figure2.Demonstration of harmonic energy minimization on two
synthetic datasets. Large symbols indicate labeled data, other
points are unlabeled.

In this paper we focus on the above harmonic function as a
basis for semi-supervised classication. However, we em-
phasize that the Gaussian random eld model from which
this function is derived provides the learning framework
with a consistent probabilistic semantics.

In the following, we refer to the procedure described above
as harmonic energy minimization, to underscore the har-
monic property (3) as well as the objective function being
minimized. Figure 2 demonstrates the use of harmonic en-
ergy minimization on two synthetic datasets. The left gure

shows that the data has three bands, with4! ,
and[
g ,"!

!#. ,
gfg ; the right gure shows two spirals, with
. , and[
 . Here we see harmonic

energy minimization clearly follows the structure of data,
while obviously methods such as kNN would fail to do so.

!1+-

+-

3. Interpretation and Connections

As outlined briey in this section, the basic framework pre-
sented in the previous section can be viewed in several fun-
damentally different ways, and these different viewpoints
provide a rich and complementary set of techniques for rea-
soning about this approach to the semi-supervised learning
problem.

3.1. Random Walks and Electric Networks

b

. Starting

is the probability that

label 1. Here the labeled data is viewed as an absorbing
boundary for the random walk.

Imagine a particle walking along the graph0
from an unlabeled nodeb , it moves to a nodei with proba-
KNM after one step. The walk continues until the par-
bility
ticle hits a labeled node. Then_
the particle, starting from nodeb , hits a labeled node with
ences. First, we x the value of_ on the labeled points,

This view of the harmonic solution indicates that it is
closely related to the random walk approach of Szummer
and Jaakkola (2001), however there are two major differ-

and second, our solution is an equilibrium state, expressed
in terms of a hitting time, while in (Szummer & Jaakkola,

3.2. Graph Kernels

We will return to this point when discussing heat kernels.

is the voltage in the resulting electric network on each of

is
the solution to the heat equation on the graph with initial

be chosen using an auxiliary technique, for example cross-
validation.

and Lafferty (2002) propose this as an appropriate kernel
for machine learning with categorical data. When used in a
kernel method such as a support vector machine, the kernel

An electrical network interpretation is given in (Doyle &
to be resistors with
to a positive

harmonic property here follows from Kirchoffs and Ohms
laws, and the maximum principle then shows that this is
precisely the same solution obtained in (5).

2001) the walk crucially depends on the time parameter	 .
Snell, 1984). Imagine the edges of0
. We connect nodes labeled.
conductanceE
to ground. Then_
voltage source, and points labeled+
the unlabeled nodes. Furthermore_
 minimizes the energy
for the given_
dissipation of the electric network0
 . The
The solution_ can be viewed from the viewpoint of spec-
tral graph theory. The heat kernel with time parameter
on the graph0
. Here
YbY
i

is dened as
!
9
conditions being a point source atb at time	!+ . Kondor
classier 

K
bY
ji
!
i} can be viewed as a
solution to the heat equation with initial heat sources
on the labeled data. The time parameter	 must, however,
dent of	 , the diffusion time. Let
, be the lower right
. Since
submatrix of
!
B(
>
>
>
Laplacian restricted to the unlabeled nodes in0
 . Then
!
9
describes heat diffusion on the unlabeled subgraph with
Greens function
>
!? , which can be expressed in terms of
Laplacian,
the integral over time of the heat kernel

:



3
*!
]	4!
]	I!
>
>
V!
or_
i

#"%
! xE,
ji
I!
a kernel classier with the kernel
Laplacian.) From (6) we also see that the spectrum of
is the spectrum of
/ , where)%$
> . This indicates
)$

Expression (7) shows that this approach can be viewed as
and a specic form of
kernel machine. (See also (Chung & Yau, 2000), where a
normalized Laplacian is used instead of the combinatorial
is

a connection to the work of Chapelle et al. (2002), who ma-
nipulate the eigenvalues of the Laplacian to create various

Dirichlet boundary conditions on the labeled nodes. The
is the inverse operator of the restricted

Our algorithm uses a different approach which is indepen-

The harmonic solution (5) can then be written as

the heat kernel on this submatrix:

, it is the
. Consider

(6)

(7)





!

!

_


K

K



K

K
T
E

















T
E



_

_


V
K
X


K
J
K
!



K
K
/
KM

KM

(8)

not.

_

I!

b:

b

corresponding

i
Z

kernels. A related approach is given by Belkin and Niyogi

3.3. Spectral Clustering and Graph Mincuts

The normalized cut approach of Shi and Malik (2000) has
as its objective function the minimization of the Raleigh

smallest eigenvector of the generalized eigenvalue problem
. Yu and Shi (2001) add a grouping bias to
the normalized cut to specify which points should be in
the same group. Since labeled data can be encoded into
such pairwise grouping constraints, this technique can be
applied to semi-supervised learning as well.
In general,
is close to block diagonal, it can be shown that
data points are tightly clustered in the eigenspace spanned
(Ng et al., 2001a; Meila
& Shi, 2001), leading to various spectral clustering algo-
rithms.

(2002), who propose to regularize functions on0 by select-
ing the topk normalized eigenvectors of
to the smallest eigenvalues, thus obtaining the best t to_
in the least squares sense. We remark that our_ ts the
labeled data exactly, while the orderk approximation may
quotient
subject to the constraint_ . The solution is the second
whenE
by the rst few eigenvectors of
for this work is also a weighted graph0
minimum	 -cut, where negative labeled data is connected
(with large weight) to a special source node , and positive
labeled data is connected to a special sink node	 . A mini-
mum
-cut, which is not necessarily unique, minimizes the
 objective function5
Kh
b
!
i}
and corresponds to a function_#`
.9Y$.f/ ; the
.f
$./ , but the eld is pinned on
over the label space)

Perhaps the most interesting and substantial connection to
the methods we propose here is the graph mincut approach
proposed by Blum and Chawla (2001). The starting point
, but the semi-
supervised learning problem is cast as one of nding a

the labeled entries. Because of this constraint, approxima-
tion methods based on rapidly mixing Markov chains that
apply to the ferromagnetic Ising model unfortunately can-
not be used. Moreover, multi-label extensions are generally
NP-hard in this framework. In contrast, the harmonic so-
lution can be computed efciently using matrix methods,
even in the multi-label case, and inference for the Gaussian
random eld can be efciently and accurately carried out
using loopy belief propagation (Weiss et al., 2001).

solutions can be obtained using linear programming. The
corresponding random eld model is a traditional eld

JLKM

b	

4. Incorporating Class Prior Knowledge

to labels, the obvious decision rule is to

tends to produce severely unbalanced classication.

wise. We call this rule the harmonic threshold (abbreviated
thresh below). In terms of the random walk interpreta-

more likely to reach a positively labeled point before a neg-
atively labeled point. This decision rule works well when
the classes are well separated. However in real datasets,
as is

, which species
the data manifold, is often poorly estimated in practice and
does not reect the classication goal. In other words, we
should not fully trust the graph structure. The class priors
are a valuable piece of complementary information. Lets

To go from_
if_
assign label 1 to nodeb
Z , and label 0 other-
tion, if_
b	
Z , then starting atb , the random walk is
classes are often not ideally separated, and using_
The problem stems from the fact thatE
assume the desirable proportions for classes 1 and 0 are


, respectively, where these values are either given
and.
the mass of class 1 to be
b , and the mass of class 0
	.
to be
b	 . Class mass normalization scales these
masses so that an unlabeled pointb
b
?	.


.
b

by an oracle or estimated from labeled data. We adopt a
simple procedure called class mass normalization (CMN)
to adjust the class distributions to match the priors. Dene

This method extends naturally to the general multi-label
case.

b:
b

is classied as class 1

(9)

iff

5. Incorporating External Classiers

Often we have an external classier at hand, which is con-
structed on labeled data alone. In this section we suggest
how this can be combined with harmonic energy minimiza-

in
the original graph, we attach a dongle node which is a la-

tion. Assume the external classier produces labels% on
 . We
the unlabeled data;
+-.
 can be 0/1 or soft labels in
combine
 with harmonic energy minimization by a sim-
ple modication of the graph. For each unlabeled nodeb
K , let the transition probability from
beled node with value
b to its dongle be
, and discount all other transitions fromb
by.
-
!G
.
>

. We then perform harmonic energy minimization
on this augmented graph. Thus, the external classier in-
troduces assignment costs to the energy function, which
play the role of vertex potentials in the random eld.
It
is not difcult to show that the harmonic solution on the
augmented graph is, in the random walk view,

We note that throughout the paper we have assumed the
labeled data to be noise free, and so clamping their values

$


-

	.

,

(10)



_

_
_


_
!

J

_
T
_

K
]
K
_
Z

_
!
$

_
7


_

Z

M

_
T
_

2
a
)
T
T


T
K
_
K
T
_



_


K
_
T
.
T
_


K
T
_
T

_

T
T



T
_



makes sense. If there is reason to doubt this assumption, it
would be reasonable to attach dongles to labeled nodes as
well, and to move the labels to these new nodes.

6. Learning the Weight Matrix

(11)



b
b<.

shown to be useful as a feature selection mechanism which
better aligns the graph structure with the data.

The usual parameter learning criterion is to maximize the
likelihood of labeled data. However, the likelihood crite-

is given
and xed. In this section, we investigate learning weight
functions of the form given by equation (1). We will learn

where
is the entropy of the eld at the individual unlabeled data
,
relying on the maximum principle of harmonic functions

labeled data are xed during training, and moreover likeli-
hood doesnt make sense for the unlabeled data because we
do not have a generative model. We propose instead to use
average label entropy as a heuristic criterion for parameter
learning. The average label entropy
is
dened as

Previously we assumed that the weight matrixE
W s from both labeled and unlabeled data; this will be
the[
rion is not appropriate in this case because the_ values for
of the eld_

b:

I!
bI!
	.
b:
b		
	.
b
pointb . Here we use the random walk interpretation of_
forb$. . Small
which guarantees that+C
entropy implies that_
b:
the intuition that a goodE
/ ) should result in a condent labeling.
perparameters)
we are constraining_ on the labeled datamost of these
W parameters.
small and lends itself well to tuning the[
minimum at 0 as[
+ . As the length scale approaches
that is closest to some labeled point ;
unlabeled point
 s label, put
(2) label with

zero, the tail of the weight function (1) is increasingly sen-
sitive to the distance. In the end, the label predicted for an
unlabeled example is dominated by its nearest neighbors
label, which results in the following equivalent labeling
procedure: (1) starting from the labeled data set, nd the

in the labeled set and re-
peat. Since these are hard labels, the entropy is zero. This
solution is desirable only when the classes are extremely
well separated, and can be expected to be inferior other-
wise.

There are of course many arbitrary labelings of the data that
have low entropy, which might suggest that this criterion
will not work. However, it is important to point out that

arbitrary low entropy labelings are inconsistent with this
constraint. In fact, we nd that the space of low entropy
labelings achievable by harmonic energy minimization is

is close to 0 or 1; this captures
(equivalently, a good set of hy-

There is a complication, however, which is that

has a

(13)

(12)

, where

minimize

is the uniform matrix

matrix
with entries

. The gradient is computed as

This complication can be avoided by smoothing the tran-
sition matrix. Inspired by analysis of the PageRank algo-

rithm in (Ng et al., 2001b), we replace with the smoothed
!C$*.
fR
KNM
.--$@ .
We use gradient descent to nd the hyperparameters[

b
b
b:
j
W can be read off the vector
	

W , which is given by
b:
where the values 

>
,

,
W are sub-matrices of
W and
 . Both
3]

using the fact that]


>
,
f
	.
. Since the original transition matrix
tained by normalizing the weight matrixE
! "
j
KM
KM

#%$'&
#
$)
W .
 "
! "
KM
[+

In the above derivation we use_
!#
$*&
 as label probabilities di-
 "
rectly; that is,k class
.>!
b . If we incorpo-

R
}
%b
bI!


.
b$	.
}
ji
(15)
and we use this probability in place of_
b

rate class prior information, or combine harmonic energy
minimization with other classiers, it makes sense to min-
imize entropy on the combined probabilities. For instance,
if we incorporate a class prior using CMN, the probability
is given by

in (11). The
derivation of the gradient descent rule is a straightforward
extension of the above analysis.

is ob-
, we have that


R

Finally,

(14)

that

7. Experimental Results

We rst evaluate harmonic energy minimization on a hand-
written digits dataset, originally from the Cedar Buffalo
binary digits database (Hull, 1994). The digits were pre-
processed to reduce the size of each image down to a
grid by down-sampling and Gaussian smooth-
ing, with pixel values ranging from 0 to 255 (Le Cun
et al., 1990). Each image is thus represented by a 256-
dimensional vector. We compute the weight matrix (1) with
tested, we perform

.-

.-

9+ . For each labeled set size


_
_
.

V
K
X

K

_

K

_
T
_
_
T
T
_
T
_
_
[
W

W
a

T


!
W




[
W
!
.

V
K
X

.
T
_
_

_

[
W
_

[

_

[

_


[
W
!
T





S




[
W
_

$




[
W
_

\


!
T








[




[


[
W
!
T

k

[
W
!
T
k

(
X


(
X

J
K
(
!
g
J
W
K
T

W
M

Z

K

!
_
,
_
T

_
_
T

_
_

T

_
T
_
B
[
W
!

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

0.5

0

20

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

CMN
1NN
RBF
thresh

80

100

0.5

0

20

40

60

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

CMN
1NN
RBF
thresh

140

160

180

200

0.5

0

10

20

30

80

100

120

labeled set size

CMN + VP
thresh + VP
VP
CMN
thresh

70

80

90

100

40

50

60

labeled set size

40

60

labeled set size

Figure3.Harmonic energy minimization on digits 1vs. 2(left) and on all 10 digits (middle) and combining voted-perceptron with
harmonic energy minimization on odd vs. even digits (right)

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

0.5

0

20

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

CMN
thresh
VP
1NN

80

100

0.5

0

20

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

CMN
thresh
VP
1NN

80

100

0.5

0

20

40

60

labeled set size

CMN
thresh
VP
1NN

80

100

40

60

labeled set size

40

60

labeled set size

Figure4.Harmonic energy minimization on PC vs. MAC (left), baseball vs. hockey (middle), and MS-Windows vs. MAC (right)

10 trials. In each trial we randomly sample labeled data
from the entire dataset, and use the rest of the images as
unlabeled data. If any class is absent from the sampled la-
beled set, we redo the sampling. For methods that incorpo-
from the labeled set with

rate class priors
, we estimate


Laplace (add one) smoothing.

We consider the binary problem of classifying digits 1
vs. 2, with 1100 images in each class. We report aver-
age accuracy of the following methods on unlabeled data:
thresh, CMN, 1NN, and a radial basis function classier

RBF and 1NN are used simply as baselines. The results are
shown in Figure 3. Clearly thresh performs poorly, because

ity of examples are classied as digit 1. This shows the
inadequacy of the weight function (1) based on pixel-wise

(RBF) which classies to class 1 iffE*
E.
 .
the values of_
%ji
are generally close to 1, so the major-
Euclidean distance. However the relative rankings of_
ji

solution_
 , so that the class proportion ts the prior
. This
method is inferior to CMN due to the error in estimating
,

are useful, and when coupled with class prior information
signicantly improved accuracy is obtained. The greatest
improvement is achieved by the simple method CMN. We
could also have adjusted the decision threshold on threshs

and it is not shown in the plot. These same observations
are also true for the experiments we performed on several
other binary digit classication problems.

We also consider the 10-way problem of classifying digits
0 through 9. We report the results on a dataset with in-
tentionally unbalanced class sizes, with 455, 213, 129, 100,
754, 970, 275, 585, 166, 353 examples per class, respec-
tively (noting that the results on a balanced dataset are sim-
ilar). We report the average accuracy of thresh, CMN, RBF,
and 1NN. These methods can handle multi-way classica-
tion directly, or with slight modication in a one-against-all
fashion. As the results in Figure 3 show, CMN again im-
proves performance by incorporating class priors.

Next we report the results of document categorization ex-
periments using the 20 newsgroups dataset. We pick
three binary problems: PC (number of documents: 982)
vs. MAC (961), MS-Windows (958) vs. MAC, and base-
ball (994) vs. hockey (999). Each document is minimally
processed into a tf.idf vector, without applying header re-
moval, frequency cutoff, stemming, or a stopword list. Two
s

is among

10 nearest neighbors or if
bors, as measured by cosine similarity. We use the follow-
ing weight function on the edges:

is among s 10 nearest neigh-

are connected by an edge if
documents
j
!"OP
Q

We use one-nearest neighbor and the voted perceptron al-
gorithm (Freund & Schapire, 1999) (10 epochs with a lin-

(16)

+-

_


T
_


J

T
.
+


.
T









ear kernel) as baselinesour results with support vector ma-
chines are comparable. The results are shown in Figure
4. As before, each point is the average of 10 random tri-
als. For this data, harmonic energy minimization performs
much better than the baselines. The improvement from the
class prior, however, is less signicant. An explanation for
why this approach to semi-supervised learning is so effec-
tive on the newsgroups data may lie in the common use of

quotations within a topic thread: document
of document
quotes part of

although documents far apart in the thread may be quite
different, they are linked by edges in the graphical repre-
sentation of the data, and these links are exploited by the
learning algorithm.

Z quotes part
Z , and so on. Thus,

 ,

7.1. Incorporating External Classiers

We use the voted-perceptron as our external classier. For
each random trial, we train a voted-perceptron on the la-
beled set, and apply it to the unlabeled set. We then use the

0/1 hard labels for dongle values , and perform harmonic
energy minimization with (10). We use!+j. .

We evaluate on the articial but difcult binary problem
of classifying odd digits vs. even digits; that is, we group
1,3,5,7,9 and 2,4,6,8,0 into two classes. There are 400
images per digit. We use second order polynomial kernel
in the voted-perceptron, and train for 10 epochs. Figure 3
shows the results. The accuracy of the voted-perceptron
on unlabeled data, averaged over trials, is marked VP in
the plot. Independently, we run thresh and CMN. Next we
combine thresh with the voted-perceptron, and the result
is marked thresh+VP. Finally, we perform class mass nor-
malization on the combined result and get CMN+VP. The
combination results in higher accuracy than either method
alone, suggesting there is complementary information used
by each.

, results on a toy
dataset are shown in Figure 5. The upper grid is slightly
tighter than the lower grid, and they are connected by a few
data points. There are two labeled examples, marked with
large symbols. We learn the optimal length scales for this
dataset by minimizing entropy on unlabeled data.

7.2. Learning the Weight MatrixE
To demonstrate the effects of estimatingE
the two dimensions, so there is only a single parameter[
approaches the minimum at 0 as[
+ . Under such con-

ditions, the results of harmonic energy minimization are
usually undesirable, and for this dataset the tighter grid
invades the sparser one as shown in Figure 5(a). With
smoothing, the nuisance minimum at 0 gradually disap-
pears as the smoothing factor
grows, as shown in Figure

To simplify the problem, we rst tie the length scales in

to learn. As noted earlier, without smoothing, the entropy

5

4

3

2

1

0

1

2

3
4

5

4

3

2

1

0

1

2

3
4

2

0

(a)

2

4

1

0.95

0.9

0.85

0.8

0.75

y
p
o
r
t

n
e

2

0

(b)

2

4

e =0.1
e =0.01
e =0.001
e =0.0001
unsmoothed

0.7

0.2

0.4

0.6

0.8

1

1.2

1.4

(c)
Figure5.The effect of parameter 
mization. (a) If unsmoothed,  as 
performs poorly. (b) Result at optimal 

on harmonic energy mini-
 , and the algorithm

, smoothed with
(c) Smoothing helps to remove the entropy minimum.

	

	



5(c). When we set

and we reach a minimum entropy of 0.619 bits.

length scale is shown in Figure 5(b), which is able to dis-
tinguish the structure of the two grids.

for each dimension, parameter
learning is more dramatic. With the same smoothing of

stabilizes at 0.65,
In this
is legitimate; it means that the learning al-

on both the labeled and unlabeled data. Harmonic energy
minimization under these parameters gives the same clas-
sication as shown in Figure 5(b).

+-. , the minimum entropy is 0.898
!+-
bits at[
!+-
 . Harmonic energy minimization under this
If we allow a separate[
[ keeps growing towards innity (we use
+-. ,
^!
+-

for computation) while[
[
.+
case[
gorithm has identied the -direction as irrelevant, based
Next we learn[ s for all 256 dimensions on the 1 vs. 2
dimensions sharing the same[
f+ as in previous ex-
periments. Then we compute the derivatives of[
increase. The learned[ s shown in the rightmost plot of
Figure 6 range from 181 (black) to 465 (white). A small[
K (white). We can discern the shapes of a black 1 and

for each
dimension separately, and perform gradient descent to min-
imize the entropy. The result is shown in Table 1. As
entropy decreases, the accuracy of CMN and thresh both

digits dataset. For this problem we minimize the entropy
with CMN probabilities (15). We randomly pick a split of
92 labeled and 2108 unlabeled examples, and start with all

(black) indicates that the weight is more sensitive to varia-
tions in that dimension, while the opposite is true for large

a white 2 in this gure; that is, the learned parameters

+
a

s


-
!

a

!
K
[
(bits)
0.6931
0.6542

CMN

thresh

97.25
98.56

0.73 % 94.70
0.43 % 98.02

1.19 %
0.39 %

start
end

Table1.Entropy of CMN and accuracies before and after learning
 s on the 1vs. 2dataset.

Figure6.Learned  s for 1vs. 2dataset. From left to right:
average 1,average 2,initial  s, learned  s.

exaggerate variations within class 1 while suppressing
variations within class 2. We have observed that with
the default parameters, class 1 has much less variation
than class 2; thus, the learned parameters are, in effect,
compensating for the relative tightness of the two classes in
feature space.

8. Conclusion

We have introduced an approach to semi-supervised learn-
ing based on a Gaussian random eld model dened with
respect to a weighted graph representing labeled and unla-
beled data. Promising experimental results have been pre-
sented for text and digit classication, demonstrating that
the framework has the potential to effectively exploit the
structure of unlabeled data to improve classication accu-
racy. The underlying random eld gives a coherent proba-
bilistic semantics to our approach, but this paper has con-
centrated on the use of only the mean of the eld, which is
characterized in terms of harmonic functions and spectral
graph theory. The fully probabilistic framework is closely
related to Gaussian process classication, and this connec-
tion suggests principled ways of incorporating class priors
and learning hyperparameters; in particular, it is natural
to apply evidence maximization or the generalization er-
ror bounds that have been studied for Gaussian processes
(Seeger, 2002). Our work in this direction will be reported
in a future publication.

