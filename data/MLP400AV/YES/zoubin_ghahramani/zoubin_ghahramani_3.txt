galakis et al. (1993) made a similar reintroduction and extension in the
speech processing community. Once again we mention the book by Elliott
et al. (1995), which also covers learning in this context.

The basis of all the learning algorithms presented by these authors is
the powerful EM algorithm (Baum & Petrie, 1966; Dempster, Laird, & Ru-
bin, 1977). The objective of the algorithm is to maximize the likelihood of
the observed data (equation 4.1) in the presence of hidden variables. Let
us denote the observed data by Y = {y1, . . . , y}, the hidden variables by
X = {x1, . . . , x}, and the parameters of the model by . Maximizing the
likelihood as a function of  is equivalent to maximizing the log-likelihood:

L( ) = log P(Y| ) = log

P(X, Y| ) dX.

(4.5)

(cid:90)

X

log

X

Using any distribution Q over the hidden variables, we can obtain a lower
(cid:90)
bound on L:

(cid:90)

X

(cid:90)
P(Y, X| ) dX = log
(cid:90)
=
= F (Q,  ),



X

X

dX

Q(X) P(X, Y| )
Q(X)
P(X, Y| )
Q(X)

Q(X) log
dX
Q(X) log P(X, Y| ) dX

(cid:90)

X

(4.6a)

(4.6b)

Q(X) log Q(X) dX (4.6c)

(4.6d)

where the middle inequality is known as Jensens inequality and can be
proved using the concavity of the log function. If we dene the energy of
a global conguration (X, Y) to be  log P(X, Y| ), then some readers may
notice that the lower bound F (Q,  )  L( ) is the negative of a quantity
known in statistical physics as the free energy: the expected energy un-
der Q minus the entropy of Q (Neal & Hinton, 1998). The EM algorithm
alternates between maximizing F with respect to the distribution Q and
the parameters , respectively, holding the other xed. Starting from some
initial parameters 0:

E-step:

Qk+1  arg max

Q

F (Q, k)

(4.7a)

A Unifying Review of Linear Gaussian Models
F (Qk+1,  ).

k+1  arg max

M-step:



313

(4.7b)

It is easy to show that the maximum in the E-step results when Q is exactly
the conditional distribution of X: Qk+1(X) = P(X|Y, k), at which point the
bound becomes an equality: F (Qk+1, k) = L(k). The maximum in the
M-step is obtained by maximizing the rst term in equation 4.6c, since the
entropy of Q does not depend on :
k+1  arg max

P(X|Y, k) log P(X, Y| ) dX.

M-step:

(4.8)

(cid:90)



X

This is the expression most often associated with the EM algorithm, but it
obscures the elegant interpretation of EM as coordinate ascent in F (Neal
& Hinton, 1998). Since F = L at the beginning of each M-step and since the
E-step does not change , we are guaranteed not to decrease the likelihood
after each combined EM-step.

Therefore, at the heart of the EM learning procedure is the following
idea: use the solutions to the ltering and smoothing problem to estimate
the unknown hidden states given the observations and the current model
parameters. Then use this ctitious complete data to solve for new model
parameters. Given the estimated states obtained from the inference algo-
rithm, it is usually easy to solve for new parameters. For linear gaussian
models, this typically involves minimizing quadratic forms such as equa-
tion 3.4, which can be done with linear regression. This process is repeated
using these new model parameters to infer the hidden states again, and so
on. We shall review the details of particular algorithms as we present the
various cases; however, we now touch on one general point that often causes
confusion. Our goal is to maximize the total likelihood (see equation 4.1) (or
equivalently maximize the total log likelihood) of the observed data with
respect to the model parameters. This means integrating (or summing) over
all ways in which the generative model could have produced the data. As
a consequence of using the EM algorithm to do this maximization, we nd
ourselves needing to compute (and maximize) the expected log-likelihood
of the joint data, where the expectation is taken over the distribution of
hidden values predicted by the current model parameters and the observa-
tions. Thus, it appears that we are maximizing the incorrect quantity, but
doing so is in fact guaranteed to increase (or keep the same) the quantity of
interest at each iteration of the algorithm.

5 Continuous-State Linear Gaussian Systems

Having described the basic model and learning procedure, we now focus
on specic linear instances of the model in which the hidden state variable
x is continuous and the noise processes are gaussian. This will allow us to

314

Sam Roweis and Zoubin Ghahramani

elucidate the relationship among factor analysis, PCA, and Kalman lter
models. We divide our discussion into models that generate static data and
those that generate dynamic data. Static data have no temporal dependence;
no information would be lost by permuting the ordering of the data points
yt; whereas for dynamic data, the time ordering of the data points is crucial.

5.1 Static Data Modeling: Factor Analysis, SPCA, and PCA. In many
situations we have reason to believe (or at least to assume) that each point in
our data set was generated independently and identically. In other words,
there is no natural (temporal) ordering to the data points; they merely form
a collection. In such cases, we assume that the underlying state vector x has
no dynamics; the matrix A is the zero matrix, and therefore x is simply a
constant (which we take without loss of generality to be the zero vector)
corrupted by noise. The new generative model then becomes:

A = 0  x = w

y = Cx + v

w  N (0, Q)
v  N (0, R) .

(5.1a)
(5.1b)

Notice that since xt is driven only by the noise w and since yt depends only
on xt, all temporal dependence has disappeared. This is the motivation for
the term static and for the notations x and y above. We also no longer use
a separate distribution for the initial state: x1  x  w  N (0, Q).
tion 4.1 to obtain the marginal distribution of y, which is the gaussian,

This model is illustrated in Figure 2. We can analytically integrate equa-

y  N

0, CQCT + R

.

(5.2)

(cid:179)

(cid:180)

Two things are important to notice. First, the degeneracy mentioned
above persists between the structure in Q and C.7 This means there is no
loss of generality in restricting Q to be diagonal. Furthermore, there is ar-
bitrary sharing of scale between a diagonal Q and C. Typically we either
restrict the columns of C to be unit vectors or make Q the identity matrix
to resolve this degeneracy. In what follows we will assume Q = I without
loss of generality.

Second, the covariance matrix R of the observation noise must be re-
stricted in some way for the model to capture any interesting or informative
projections in the state x. If R were not restricted, learning could simply
choose C = 0 and then set R to be the sample covariance of the data, thus
trivially achieving the maximum likelihood model by explaining all of the

7 If we diagonalize Q and rewrite the covariance of y, the degeneracy becomes clear:
. To make Q diagonal, we simply replace C with

0, (CE1/2)(CE1/2)T + R

(cid:179)

y  N
CE.

(cid:180)

A Unifying Review of Linear Gaussian Models

315

x

C

+

w

y

+

v

y

C

x



v

w

Figure 2: Static generative model (continuous state).The covariance matrix of
the input noise w is Q and the covariance matrix of the output noise v is R.
In the network model below, the smaller circles represent noise sources and all
units are linear. Outgoing weights have only been drawn from one hidden unit.
This model is equivalent to factor analysis, SPCA and PCA models depending
on the output noise covariance. For factor analysis, Q = I and R is diagonal. For
SPCA, Q = I and R = I. For PCA, Q = I and R = lim0 I.

structure in the data as noise. (Remember that since the model has reduced
to a single gaussian distribution for y, we can do no better than having
the covariance of our model equal the sample covariance of our data.) Note
that restricting R, unlike making Q diagonal, does constitute some loss of
generality from the original model of equations 5.1.
There is an intuitive spatial way to think about this static generative
model. We use white noise to generate a spherical ball (since Q = I) of
density in k-dimensional state-space. This ball is then stretched and rotated
into p-dimensional observation space by the matrix C, where it looks like a
k-dimensional pancake. The pancake is then convolved with the covariance
density of v (described by R) to get the nal covariance model for y.
We want the resulting ellipsoidal density to be as close as possible to the
ellipsoid given by the sample covariance of our data. If we restrict the shape
of the v covariance by constraining R, we can force interesting information
to appear in both R and C as a result.

Finally, observe that all varieties of ltering and smoothing reduce to the
same problem in this static model because there is no time dependence. We
over a single hidden state
are seeking only the posterior probability P
given the corresponding single observation. This inference is easily done by

(cid:162)

(cid:161)
x|y

316

Sam Roweis and Zoubin Ghahramani

linear matrix projection, and the resulting density is itself gaussian:

(cid:161)
(cid:161)
x|y
x|y

P

P

(cid:162)

(cid:161)
(cid:162) = P
(cid:161)
y|x
(cid:162) = N(cid:161)
y
P
y, I  C

P (x)

(cid:162)

N(cid:161)

(cid:162)|y
= N (Cx, R)|y N (0, I)|x
(cid:162)|x ,

0, CCT + R
 = CT(CCT + R)1,

(5.3a)

(5.3b)

from which we obtain not only the expected value y of the unknown
state but also an estimate of the uncertainty in this value in the form of the
covariance I  C. Computing the likelihood of a data point y is merely
an evaluation under the gaussian in equation 5.2. The learning problem
now consists of identifying the matrices C and R. There is a family of EM
algorithms to do this for the various cases discussed below, which are given
in detail at the end of this review.

5.2 Factor Analysis. If we restrict the covariance matrix R that controls
the observation noise to be diagonal (in other words, the covariance ellipsoid
of v is axis aligned) and set the state noise Q to be the identity matrix,
then we recover exactly a standard statistical model known as maximum
likelihood factor analysis. The unknown states x are called the factors in this
context; the matrix C is called the factor loading matrix, and the diagonal
elements of R are often known as the uniquenesses. (See Everitt, 1984, for
a brief and clear introduction.) The inference calculation is done exactly
as in equation 5.3b. The learning algorithm for the loading matrix and the
uniquenesses is exactly an EM algorithm except that we must take care
to constrain R properly (which is as easy as taking the diagonal of the
unconstrained maximum likelihood estimate; see Rubin & Thayer, 1982;
Ghahramani & Hinton, 1997). If C is completely free, this procedure is called
exploratory factor analysis; if we build a priori zeros into C, it is conrmatory
factor analysis. In exploratory factor analysis, we are trying to model the
covariance structure of our data with p + pk  k(k  1)/2 free parameters8
instead of the p(p + 1)/2 free parameters in a full covariance matrix.

The diagonality of R is the key assumption here. Factor analysis attempts
to explain the covariance structure in the observed data by putting all the
variance unique to each coordinate in the matrix R and putting all the cor-
relation structure into C (this observation was rst made by Lyttkens, 1966,
in response to work by Wold). In essence, factor analysis considers the axis
rotation in which the original data arrived to be special because observation
noise (often called sensor noise) is independent along the coordinates in these
axes. However, the original scaling of the coordinates is unimportant. If we
were to change the units in which we measured some of the components
of y, factor analysis could merely rescale the corresponding entry in R and

8 The correction k(k 1)/2 comes in because of degeneracy in unitary transformations

of the factors. See, for example, Everitt (1984).

A Unifying Review of Linear Gaussian Models

317

row in C and achieve a new model that assigns the rescaled data identical
likelihood. On the other hand, if we rotate the axes in which we measure
the data, we could not easily x things since the noise v is constrained to
have axis aligned covariance (R is diagonal).

EM for factor analysis has been criticized as being quite slow (Rubin
& Thayer, 1982). Indeed, the standard method for tting a factor analysis
model (Joreskog, 1967) is based on a quasi-Newton optimization algorithm
(Fletcher & Powell, 1963), which has been found empirically to converge
faster than EM. We present the EM algorithm here not because it is the
most efcient way of tting a factor analysis model, but because we wish to
emphasize that for factor analysis and all the other latent variable models
reviewed here, EM provides a unied approach to learning. Finally, recent
work in online learning has shown that it is possible to derive a family
of EM-like algorithms with faster convergence rates than the standard EM
algorithm (Kivinen & Warmuth, 1997; Bauer, Koller, & Singer, 1997).

5.3 SPCA and PCA. If instead of restricting R to be merely diagonal,
we require it to be a multiple of the identity matrix (in other words, the
covariance ellipsoid of v is spherical), then we have a model that we will
call sensible principal component analysis (SPCA) (Roweis, 1997). The columns
of C span the principal subspace (the same subspace found by PCA), and
we will call the scalar value on the diagonal of R the global noise level. Note
that SPCA uses 1+ pk k(k 1)/2 free parameters to model the covariance.
Once again, inference is done with equation 5.3b and learning by the EM
algorithm (except that we now take the trace of the maximum likelihood
estimate for R to learn the noise level; see (Roweis, 1997)). Unlike factor
analysis, SPCA considers the original axis rotation in which the data arrived
to be unimportant: if the measurement coordinate system were rotated,
SPCA could (left) multiply C by the same rotation, and the likelihood of the
new data would not change. On the other hand, the original scaling of the
coordinates is privileged because SPCA assumes that the observation noise
has the same variance in all directions in the measurement units used for the
observed data. If we were to rescale one of the components of y, the model
could not be easily corrected since v has spherical covariance (R = I). The
SPCA model is very similar to the independently proposed probabilistic
principal component analysis (Tipping & Bishop, 1997).
If we go even further and take the limit R = lim0 I (while keeping
the diagonal elements of Q nite)9 then we obtain the standard principal
component analysis (PCA) model. The directions of the columns of C are

9 Since isotropic scaling of the data space is arbitrary, we could just as easily take the
limit as the diagonal elements of Q became innite while holding R nite or take both
limits at once. The idea is that the noise variance becomes innitesimal compared to the
scale of the data.

318

Sam Roweis and Zoubin Ghahramani

known as the principal components. Inference now reduces to simple least
squares projection:10

P(x|y)=N(cid:161)
(cid:162)|x ,  = lim
(cid:180)
(cid:179)
y, I  C
|x
(CTC)1CTy, 0
= (x  (CTC)1CTy).

P(x|y)=N

0

CT(CCT + I)1

(5.4a)

(5.4b)

Since the noise has become innitesimal, the posterior over states collapses
to a single point, and the covariance becomes zero. There is still an EM
algorithm for learning (Roweis, 1997), although it can learn only C. For
PCA, we could just diagonalize the sample covariance of the data and take
the leading k eigenvectors multiplied by their eigenvalues to be the columns
of C. This approach would give us C in one step but has many problems.11
The EM learning algorithm amounts to an iterative procedure for nding
these leading eigenvectors without explicit diagonalization.

An important nal comment is that (regular) PCA does not dene a
proper density model in the observation space, so we cannot ask directly
about the likelihood assigned by the model to some data. We can, however,
examine a quantity that is proportional to the negative log-likelihood in the
limit of zero noise. This is the sum squared deviation of each data point
from its projection. It is this cost that the learning algorithm ends up
minimizing and is the only available evaluation of how well a PCA model
ts new data. This is one of the most critical failings of PCA: translating
points by arbitrary amounts inside the principal subspace has no effect on
the model error.

10 Recall that if C is pk with p > k and is rank k, then left multiplication by CT(CCT)1
(which appears not to be well dened because CCT is not invertible) is exactly equivalent
to left multiplication by (CTC)1CT. This is the same as the singular value decomposition
idea of dening the inverse of the diagonal singular value matrix as the inverse of an
element unless it is zero, in which case it remains zero. The intuition is that although CCT
truly is not invertible, the directions along which it is not invertible are exactly those that
CT is about to project out.

11 It is computationally very hard to diagonalize or invert large matrices. It also requires
an enormous amount of data to make a large sample covariance matrix full rank. If we
are working with patterns in a large (thousands) number of dimensions and want to
extract only a few (tens) principal components, we cannot naively try to diagonalize
the sample covariance of our data. Techniques like the snapshot method (Sirovich, 1987)
attempt to address this but still require the diagonalization of an N  N matrix where N
is the number of data points. The EM algorithm approach solves all of these problems,
requiring no explicit diagonalization whatsoever and the inversion of only a kk matrix. It
is guaranteed to converge to the true principal subspace (the same subspace spanned by the
principal components). Empirical experiments (Roweis, 1997) indicate that it converges
in a few iterations, unless the ratio of the leading eigenvalues is near unity.

A Unifying Review of Linear Gaussian Models

319

5.4 Time-Series Modeling: Kalman Filter Models. We use the term dy-
namic data to refer to observation sequences in which the temporal ordering
is important. For such data, we do not want to ignore the state evolution
dynamics, which provides the only aspect of the model capable of capturing
temporal structure. Systems described by the original dynamic generative
model, shown in equations 2.1a and 2.2b, are known as linear dynamical
systems or Kalman lter models and have been extensively investigated by
the engineering and control communities for decades. The emphasis has
traditionally been on inference problems: the famous discrete Kalman lter
(Kalman, 1960; Kalman & Bucy, 1961) gives an efcient recursive solution to
the optimal ltering and likelihood computation problems, while the RTS
recursions (Rauch, 1963; Rauch et al., 1965) solve the optimal smoothing
problem. Learning of unknown model parameters was studied by Shum-
way and Stoffer (1982) (C known) and by Ghahramani and Hinton (1996a)
and Digalakis et al. (1993) (all parameters unknown). Figure 1 illustrates
this model, and the appendix gives pseudocode for its implementation.

We can extend our spatial intuition of the static case to this dynamic
model. As before, any point in state-space is surrounded by a ball (or ovoid)
of density (described by Q), which is stretched (by C) into a pancake in
observation space and then convolved with the observation noise covari-
ance (described by R). However, unlike the static case, in which we always
centered our ball of density on the origin in state-space, the center of the
state-space ball now ows from time step to time step. The ow is accord-
ing to the eld described by the eigenvalues and eigenvectors of the matrix
A. We move to a new point according to this ow eld; then we center our
ball on that point and pick a new state. From this new state, we again ow
to a new point and then apply noise. If A is the identity matrix (not the
zero matrix), then the ow does not move us anywhere, and the state just
evolves according to a random walk of the noise set by Q.

6 Discrete-State Linear Gaussian Models

We now consider a simple modication of the basic continuous state model
in which the state at any time takes on one of a nite number of discrete val-
ues. Many real-world processes, especially those that have distinct modes
of operation, are better modeled by internal states that are not continuous.
(It is also possible to construct models that have a mixed continuous and
discrete state.) The state evolution is still rst-order Markovian dynamics,
and the observation process is still linear with additive gaussian noise. The
modication involves the use of the winner-take-all nonlinearity WTA[],
dened such that WTA[x] for any vector x is a new vector with unity in
the position of the largest coordinate of the input and zeros in all other
positions. The discrete-state generative model is now simply:

xt+1 = WTA[Axt + wt] = WTA[Axt + w]
yt = Cxt + vt = Cxt + v

(6.1a)
(6.1b)

320

Sam Roweis and Zoubin Ghahramani

xt

A

C

+

v

yt

(cid:0)

z

v

yt

WTA

+

w

C

xt

w

A

(cid:0)

z

Figure 3: Discrete state generative model for dynamic data. The WTA[] block
1 block is a unit delay. The
implements the winner-take-all nonlinearity. The z
covariance matrix of the input noise w is Q and the covariance matrix of the
output noise v is R. In the network model below, the smaller circles represent
noise sources and the hidden units x have a winner take all behaviour (indicated
by dashed lines). Outgoing weights have only been drawn from one hidden
unit. This model is equivalent to a hidden Markov model with tied output
covariances.

where A is no longer known as the state transition matrix (although we
will see that matrix shortly). As before, the k-vector w and p-vector v are
temporally white and spatially gaussian distributed noises independent of
each other and of x and y. The initial state x1 is generated in the obvious
way:

(6.2)

x1 = WTA[N(cid:161)

1

, Q1

(cid:162)

]

(though we will soon see that without loss of generality Q1 can be restricted
to be the identity matrix). This discrete state generative model is illustrated
in Figure 3.

6.1 Static Data Modeling: Mixtures of Gaussians and Vector Quanti-
zation. Just as in the continuous-state model, we can consider situations in

A Unifying Review of Linear Gaussian Models

321

which there is no natural ordering to our data, and so set the matrix A to be
the zero matrix. In this discrete-state case, the generative model becomes:

w  N (, Q)
v  N (0, R) .

(6.3)
(6.4)

A = 0  x = WTA[w]
y = Cx + v
(cid:162)

(cid:161)

x = ej

Each state x is generated independently12 according to a xed discrete
probability histogram controlled by the mean and covariance of w. Specif-
is the probability assigned by the gaussian N (, Q)
ically, j = P
to the region of k-space in which the jth coordinate is larger than all others.
(Here ej is the unit vector along the jth coordinate direction.) Notice that
to obtain nonuniform priors j with the WTA[] nonlinearity, we require a
nonzero mean  for the noise w. Once the state has been chosen, the cor-
responding output y is generated from a gaussian whose mean is the jth
column of C and whose covariance is R. This is exactly the standard mixture
of gaussian clusters model except that the covariances of all the clusters are
constrained to be the same. The probabilities j = P
correspond
to the mixing coefcients of the clusters, and the columns of C are the clus-
ter means. Constraining R in various ways corresponds to constraining the
shape of the covariance of the clusters. This model is illustrated in Figure 4.

(cid:161)
x = ej

(cid:162)

To compute the likelihood of a data point, we can explicitly perform the
sum equivalent to the integral in equation 4.1 since it contains only k terms,

(cid:162) = k(cid:88)

i=1

x = ej, y

P

N (Ci, R)|y P (x = ei)

N (Ci, R)|y i,

(6.5)

(cid:161)

P

y

(cid:161)

(cid:161)

i=1

(cid:162) = k(cid:88)
= k(cid:88)
(cid:162)

i=1

where Ci denotes the ith column of C. Again, all varieties of inference and
ltering are the same, and we are simply seeking the set of discrete probabil-
j = 1, . . . , k. In other words, we need to do probabilistic
ities P
classication. The problem is easily solved by computing the responsibilities
(cid:162)
x that each cluster has for the data point y:
(cid:162)
(cid:161)
x = ej, y
(cid:162)
y
P
x = ej
Cj, R
N (Ci, R)|y P (x = ei)

x = ej|y
(cid:161)
(x)j = N(cid:161)
(x)j = P
x = ej|y
(cid:80)

(cid:162)
x = ei, y

(cid:161)
(cid:162) = P
(cid:161)
(cid:162)|y P

(cid:161)
(cid:161)
x = ej, y

P
k
i=1 P

(cid:80)

(6.6a)

(cid:162)

=

k
i=1

12 As in the continuous static case, we again dispense with any special treatment of

the initial state.

322

Sam Roweis and Zoubin Ghahramani



+

WTA

x

C

w

v

w

y

+

v

y

C

x

Figure 4: Static generative model (discrete state). The WTA[] block implements
the winner-take-all nonlinearity. The covariance matrix of the input noise w is
Q and the covariance matrix of the output noise v is R. In the network model
below, the smaller circles represent noise sources and the hidden units x have a
winner take all behaviour (indicated by dashed lines). Outgoing weights have
only been drawn from one hidden unit. This model is equivalent to a mixture of
Gaussian clusters with tied covariances R or to vector quantization (VQ) when
R = lim0 I.

= N(cid:161)
(cid:80)

k
i=1

(cid:162)|y j

Cj, R
N (Ci, R)|y i

.

(6.6b)

The mean x of the state vector given a data point is exactly the vector of
responsibilities for that data point. This quantity denes the entire posterior
distribution of the discrete hidden state given the data point. As a measure
of the randomness or uncertainty in the hidden state, one could evaluate the
entropy or normalized entropy13 of the discrete distribution corresponding
to x. Although this may seem related to the variance of the posterior in
factor analysis, this analogy is deceptive. Since x denes the entire distri-
bution, no other variance measure is needed. Learning consists of nding
the cluster means (columns of C), the covariance R, and the mixing coef-
cients j. This is easily done with EM and corresponds exactly to maximum
likelihood competitive learning (Duda & Hart, 1973; Nowlan, 1991), except

13 The entropy of the distribution divided by the logarithm of k so that it always lies

between zero and one.

A Unifying Review of Linear Gaussian Models

323

that all the clusters share the same covariance. Later we introduce extensions
to the model that remove this restriction.

As in the continuous-state case, we can consider the limit as the ob-
servation noise becomes innitesimal compared to the scale of the data.
What results is the standard vector quantization model. The inference (clas-
sication) problem is now solved by the one-nearest-neighbor rule, using
Euclidean distance if R is a multiple of the identity matrix, or Mahalanobis
distance in the unscaled matrix R otherwise. Similarly to PCA, since the
observation noise has disappeared, the posterior collapses to have all of
its mass on one cluster (the closest), and the corresponding uncertainty
(entropy) becomes zero. Learning with EM is equivalent to using a batch
version of the k-means algorithm such as that proposed by Lloyd (1982).
As with PCA, vector quantization does not dene a proper density in the
observation space. Once again, we examine the sum squared deviation of
each point from its closest cluster center as a quantity proportional to the
likelihood in the limit of zero noise. Batch k-means algorithms minimize
this cost in lieu of maximizing a proper likelihood.

(cid:162)

(cid:161)
xt+1 = ej|xt = ei

6.2 Time-Series Modeling: Hidden Markov Models. We return now
to the fully dynamic discrete-state model introduced in equations 6.1a and
6.2b. Our key observation is that the dynamics described by equation 6.1a are
exactly equivalent to the more traditional discrete Markov chain dynamics
using a state transition matrix T, where Tij = P
. It is easy
to see how to compute the equivalent state transition matrix T given A and
Q above: Tij is the probability assigned by the gaussian whose mean is the ith
column of A (and whose covariance is Q) to the region of k-space in which
the jth coordinate is larger than all others. It is also true that for any transition
matrix T (whose rows each sum to unity), there exist matrices A and Q such
that the dynamics are equivalent.14 Similarly, the initial probability mass
function for x1 is easily computed from 1 and Q1 and for any desired
histogram over the states for x1 there exist a 1 and Q1 that achieve it.

Similar degeneracy exists in this discrete-state model as in the continuous-
state model except that it is now between the structure of A and Q. Since for
any noise covariance Q, the means in the columns of A can be chosen to set
any equivalent transition probabilities Tij, we can without loss of generality
restrict Q to be the identity matrix and use only the means in the columns

14 Although harder to see. Sketch of proof: Without loss of generality, always set the
covariance to the identity matrix. Next, set the dot product of the mean vector with the
k-vector having unity in all positions to be zero since moving along this direction does not
change the probabilities. Now there are (k  1) degrees of freedom in the mean and also
in the probability model. Set the mean randomly at rst (except that it has no projection
along the all-unity direction). Move the mean along a line dened by the constraint that
all probabilities but two should remain constant until one of those two probabilities has
the desired value. Repeat this until all have been set correctly.

324
Sam Roweis and Zoubin Ghahramani
of A to set probabilities. Equivalently, we can restrict Q1 = I and use only
the mean 1 to set the probabilities for the initial state x1.

Thus, this generative model is equivalent to a standard HMM except that
the emission probability densities are all constrained to have the same co-
variance. Likelihood and ltering computations are performed with the so-
called forward (alpha) recursions, while complete smoothing is done with
the forward-backward (alpha-beta) recursions. The EM algorithm for learn-
ing is exactly the well-known Baum-Welch reestimation procedure (Baum
& Petrie, 1966; Baum et al., 1970).

There is an important and peculiar consequence of discretizing the state
that affects the smoothing problem. The state sequence formed by taking the
most probable state of the posterior distribution at each time (as computed
by the forward-backward recursions given the observed data and model
parameters) is not the single state sequence most likely to have produced the
observed data. In fact, the sequence of states obtained by concatenating the
states that individually have maximum posterior probability at each time
step may have zero probability under the posterior. This creates the need for
separate inference algorithms to nd the single most likely state sequence
given the observations. Such algorithms for ltering and smoothing are
called Viterbi decoding methods (Viterbi, 1967). Why was there no need for
similar decoding in the continuous-state case? It turns out that due to the
smooth and unimodal nature of the posterior probabilities for individual
states in the continuous case (all posteriors are gaussian), the sequence of
maximum a posteriori states is exactly the single most likely state trajectory,
so the regular Kalman lter and RTS smoothing recursions sufce. It is
possible (see, for example, Rabiner & Juang, 1986) to learn the discrete-state
model parameters based on the results of the Viterbi decoding instead of
the forward-backward smoothingin other words, to maximize the joint
likelihood of the observations and the single most likely state sequence
rather than the total likelihood summed over all possible paths through
state-space.

7 Independent Component Analysis

There has been a great deal of recent interest in the blind source separation
problem that attempts to recover a number of source signals from obser-
vations resulting from those signals, using only the knowledge that the orig-
inal sources are independent. In the square-linear version of the problem,
the observation process is characterized entirely by a square and invertible
matrix C. In other words, there are as many observation streams as sources,
and there is no delay, echo, or convolutional distortion. Recent experience
has shown the surprising result that for nongaussian distributed sources,
this problem can often be solved even with no prior knowledge about the
sources or about C. It is widely believed (and beginning to be proved theo-

A Unifying Review of Linear Gaussian Models

325

retically; see MacKay, 1996) that high kurtosis source distributions are most
easily separated.

We will focus on a modied, but by now classic, version due to Bell and
Sejnowski (1995) and Baram and Roth (1994) of the original independent
component analysis algorithm (Comon, 1994). Although Bell and Sejnowski
derived it from an information-maximization perspective, this modied
algorithm can also be obtained by dening a particular prior distribution
over the components of the vector xt of sources and then deriving a gradient
learning rule that maximizes the likelihood of the data yt in the limit of zero
output noise (Amari, Cichocki, & Yang, 1996; Pearlmutter & Parra, 1997;
MacKay, 1996). The algorithm, originally derived for unordered data, has
also been extended to modeling time series (Pearlmutter & Parra, 1997).

We now show that the generative model underlying ICA can be obtained
by modifying slightly the basic model we have considered thus far. The
modication is to replace the WTA[] nonlinearity introduced above with
a general nonlinearity g() that operates componentwise on its input. Our
generative model (for static data) then becomes:

x = g(w)
y = Cx + v

w  N (0, Q)
v  N (0, R) .

(7.1a)
(7.1b)

The role of the nonlinearity is to convert the gaussian distributed prior
for w into a nongaussian prior for x. Without loss of generality, we can set
Q = I, since any covariance structure in Q can be be obtained by a linear
transformation of a N (0, I) random variable, and this linear transformation
can be subsumed into the nonlinearity g(). Assuming that the generative
nonlinearity g() is invertible and differentiable, any choice of the generative
nonlinearity results in a corresponding prior distribution on each source
given by the probability density function:

px(x) = N (0, 1)|g1(x)
|g(cid:48)(g1(x))|

.

(7.2)

It is important to distinguish this generative nonlinearity from the non-
linearity found in the ICA learning rule. We call this the learning rule non-
f (), and clarify the distinction between the two nonlinearities
linearity,
below.
Classic ICA is dened for square and invertible C in the limit of vanishing
noise, R = lim0 I. Under these conditions, the posterior density of x
given y is a delta function at x = C
1y, and the ICA algorithm can be
dened in terms of learning the recognition (or unmixing) weights W = C
1,
rather than the generative (mixing) weights C. The gradient learning rule
to increase the likelihood is

W  W

T + f (Wy)yT ,

(7.3)

dx

326
Sam Roweis and Zoubin Ghahramani
where the learning rule nonlinearity f () is the derivative of the implicit
log prior: f (x) = d log px(x)
(MacKay, 1996). Therefore, any generative non-
linearity g() results in a nongaussian prior px(), which in turn results in a
nonlinearity f () in the maximum likelihood learning rule. Somewhat frus-
tratingly from the generative models perspective, ICA is often discussed in
terms of the learning rule nonlinearity without any reference to the implicit
prior over the sources.
A popular choice for the ICA learning rule nonlinearity f () is the
tanh() function, which corresponds to a heavy tailed prior over the sources
(MacKay, 1996):

px(x) =

1

 cosh(x)

.

(7.4)

From equation 7.2, we obtain a general relationship between the cumulative
distribution function of the prior on the sources, cdfx(x), and of the zero-
mean, unit variance noise w,

cdfx(g(w)) = cdfw(w) = 1
2

+ 1
2

erf(w/


2),

(cid:82)




(7.5)

u2du. This
for monotonic g, where erf(z) is the error function 2/
relationship can often be solved to obtain an expression for g. For example,
if px(x) =

z
0 e

1

(cid:179)

(cid:179)
 cosh(x) , we nd that setting

1 + erf(w/
2)

(cid:179) 

tan

g(w) = ln

(cid:180)(cid:180)(cid:180)

(7.6)

4

causes the generative model of equations 7.1 to generate vectors x in which
each component is distributed exactly according to 1/( cosh(x)). This non-
linearity is shown in Figure 5.

ICA can be seen either as a linear generative model with nongaussian
priors for the hidden variables or as a nonlinear generative model with
gaussian priors for the hidden variables. It is therefore possible to derive an
EM algorithm for ICA, even when the observation noise R is nonzero and
there are fewer sources than observations. The only complication is that the
posterior distribution of x given y will be the product of a nongaussian
prior and a gaussian likelihood term, which can be difcult to evaluate.
Given this posterior, the M step then consists of maximizing the expected
log of the joint probability as a function of C and R. The M-step for C is

C  arg max

log P(x) + log P(yi|x, C, R)

,

(7.7)

(cid:174)

i

(cid:88)

(cid:173)

C

i

where i indexes the data points and (cid:104)(cid:105)i denotes expectation with respect to
the posterior distribution of x given yi, P(x|yi, C, R). The rst term does not

A Unifying Review of Linear Gaussian Models

327

15

10

5

0

)
w
(
g


5

10

15

5 4 3 2 1

1

2

3

4

5

0
w

Figure 5: The nonlinearity g() from equation 7.6 which converts a gaussian dis-
tributed source w  N (0, 1) into one distributed as x = g(w)  1/( cosh(x)).

depend on C, and the second term is a quadratic in C, so taking derivatives
with respect to C, we obtain a linear system of equations that can be solved
in the usual manner:

(cid:195)(cid:88)

(cid:33)(cid:195)(cid:88)

(cid:33)1

C 

yi(cid:104)xT(cid:105)i

(cid:104)xxT(cid:105)i

.

(7.8)

i

i

A similar M-step can be derived for R. Since, given x, the generative model
is linear, the M-step requires only evaluating the rst and second moments
of the posterior distribution of x: (cid:104)x(cid:105)i and (cid:104)xxT(cid:105)i. It is not necessary to know
anything else about the posterior if its rst two moments can be computed.
These may be computed using Gibbs sampling or, for certain source priors,
using table lookup or closed-form computation.15 In particular, Moulines
et al. (1997) and Attias and Schreiner (1998) have independently proposed
using a gaussian mixture to model the prior for each component of the
source, x. The posterior distribution over x is then also a gaussian mixture,
which can be evaluated analytically and used to derive an EM algorithm
for both the mixing matrix and the source densities. The only caveat is that
the number of gaussian components in the posterior grows exponentially
in the number of sources,16 which limits the applicability of this method to
models with only a few sources.

15 In the limit of zero noise, R = 0, the EM updates derived in this manner degenerate
to C  C and R  R. Since this does not decrease the likelihood, it does not contradict the
convergence proof for the EM algorithm. However, it also does not increase the likelihood,
which might explain why no one uses EM to t the standard zero-noise ICA model.

16 If each source is modeled as a mixture of k gaussians and there are m sources, then

there are km components in the mixture.

328

Sam Roweis and Zoubin Ghahramani

Alternatively, we can compute the posterior distribution of w given
y, which is the product of a gaussian prior and a nongaussian likelihood.
Again, this may not be easy, and we may wish to resort to Gibbs sampling
(Geman & Geman, 1984) or other Markov chain Monte Carlo methods (Neal,
1993). Another option is to employ a deterministic trick recently used by
Bishop, Svenson, and Williams (1998) in the context of the generative topo-
graphic map (GTM), which is a probabilistic version of Kohonens (1982)
self-organized topographic map. We approximate the gaussian prior via a
nite number (N) of xed points (this is the trick). In other words,

P(w) = N (0, I)  P(w) = N(cid:88)

j=1

(w  wj),

(7.9)

where the wjs are a nite sample from N (0, I). The generative model then
takes these N points, maps them through a xed nonlinearity g, an adaptable
linear mapping C, and adds gaussian noise with covariance R to produce
y. The generative model is therefore a constrained mixture of N gaussians,
where the constraint comes from the fact that the only way the centers can
move is by varying C. Then, computing the posterior over w amounts to
computing the responsibility under each of the N gaussians for the data
point. Given these responsibilities, the problem is again linear in C, which
means that it can be solved using equation 7.8. For the traditional zero-noise
limit of ICA, the responsibilities will select the center closest to the data point
in exactly the same manner as standard vector quantization. Therefore, ICA
could potentially be implemented using EM for GTMs in the limit of zero
output noise.

8 Network Interpretations and Regularization

Early in the modern history of neural networks, it was realized that PCA
could be implemented using a linear autoencoder network (Baldi & Hornik,
1989). The data are fed as both the input and target of the network, and the
network parameters are learned using the squared error cost function. In
this section, we show how factor analysis and mixture of gaussian clusters
can also be implemented in this manner, albeit with a different cost function.
To understand how a probabilistic model can be learned using an au-
toencoder it is very useful to make a recognition-generation decomposition
of the autoencoder (Hinton & Zemel, 1994; Hinton, Dayan, & Revow, 1997).
An autoencoder takes an input y, produces some internal representation
in the hidden layer x, and generates at its output a reconstruction of the
input y in Figure 6. We call the mapping from hidden to output layers the
generative part of the network since it generates the data from a (usually
more compact) representation under some noise model. Conversely, we call
the mapping from input to hidden units the recognition part of the network

A Unifying Review of Linear Gaussian Models

329

generative

weights

recognition

weights

^x

C



^y

y

Figure 6: A network for state inference and for learning parameters of a static
data model. The input y is clamped to the input units (bottom), and the mean
x of the posterior of the estimated state appears on the hidden units above. The
covariance of the state posterior is constant at I C which is easily computed if
the weights  are known. The inference computation is a trivial linear projection,
but learning the weights of the inference network is difcult. The input to hidden
weights are always constrained to be a function of the hidden to output weights,
and the network is trained as an autoencoder using self-supervised learning.
Outgoing weights have only been drawn from one input unit and one hidden
unit.

since it produces some representation in the hidden variables given the in-
put. Because autoencoders are usually assumed to be deterministic, we will
think of the recognition network as computing the posterior mean of the
hidden variables given the input.

The generative model for factor analysis assumes that both the hidden
states and the observables are normally distributed, from which we get the
posterior probabilities for the hidden states in equation 5.3b. If we assume
that the generative weight matrix from the hidden units to the outputs is C
and the noise model at the output is gaussian with covariance R, then the
posterior mean of the hidden variables is x = y, where  = CT(CCT +
R)1. Therefore, the hidden units can compute the posterior mean exactly
if they are linear and the weight matrix from input to hidden units is .
Notice that  is tied to C and R, so we only need to estimate C and R during
(cid:90)
learning. We denote expectations under the posterior state distribution by
(cid:104)(cid:105), for example,

(cid:104)x(cid:105) =

xP(x|y) dx = x.

From the theory of the EM algorithm (see section 4.2), we know that one
way to maximize the likelihood is to maximize the expected value of the
log of the joint probability under the posterior distribution of the hidden
variables:

(cid:104)log P(x, y|C, R)(cid:105).

330

Sam Roweis and Zoubin Ghahramani

Changing signs and ignoring constants, we can equivalently minimize the
following cost function:
C = (cid:104)(y  Cx)TR
= yT R
= (y  Cx)TR

1(y  Cx)(cid:105) + log|R|
1C(cid:104)x(cid:105) + (cid:104)xT CTR
1(y  Cx) + log|R| + trace[CTR

1Cx(cid:105) + log|R|

1y  2yT R

(8.1a)
(8.1b)
(8.1c)

1C].

Here we have dened  to be the posterior covariance of x,

  (cid:104)xxT(cid:105)  (cid:104)x(cid:105)(cid:104)x(cid:105)T = I  C,

and in the last step we have reorganized terms and made use of the fact that
(cid:104)xT CTR

1Cx(cid:105) = trace[CTR

1C(cid:104)xxT(cid:105)].

The rst two terms of cost function in equation 8.1c are just a squared
error cost function evaluated with respect to a gaussian noise model with
covariance R. They are exactly the terms minimized when tting a standard
neural network with this gaussian noise model. The last term is a regular-
ization term that accounts for the posterior variance in the hidden states
given the inputs.17 When we take derivatives of this cost function, we do
not differentiate x and  with respect to C and R. As is usual for the EM
algorithm, we differentiate the cost given the posterior distribution of the
hidden variables. Taking derivatives with respect to C and premultiplying
by R, we obtain a weight change rule,

C  (y  Cx)xT  C.

(8.2)

The rst term is the usual delta rule. The second term is simply a weight-
decay term decaying the columns of C with respect to the posterior co-
variance of the hidden variables. Intuitively, the higher the uncertainty in a
hidden unit, the more its outgoing weight vector is shrunk toward zero. To
summarize, factor analysis can be implemented in an autoassociator by ty-
ing the recognition weights to the generative weights and using a particular
regularizer in addition to squared reconstruction error during learning.

We now analyze the mixture of gaussians model in the same manner. The
recognition network is meant to produce the mean of the hidden variable
given the inputs. Since we assume that the discrete hidden variable is repre-
sented as a unit vector, its mean is just the vector of probabilities of being in
each of its k settings given the inputs, that is, the responsibilities. Assuming
equal mixing coefcients, P(x = ej) = P(x = ei)ij, the responsibilities

hidden states has zero variance (  0) and the regularizer vanishes (CTR

17 PCA assumes innitesimal noise, and therefore the posterior distribution over the
1C  I).

A Unifying Review of Linear Gaussian Models

331

dened in equation 6.6b are

(x)j = P(x = ej|y) =

2

2

(cid:80)
(cid:80)
= CjR

(y  Cj)TR

exp{ 1
i=1 exp{ 1
exp{jy  j}
i=1 exp{iy  i} ,
1 and j = 1
2 CT
j R

k

k

=

1(y  Cj)}

(y  Ci)TR1(y  Ci)}

(8.3a)

(8.3b)

1Cj. Equation 8.3b
where we have dened j
describes a recognition model that is linear followed by the softmax nonlin-
earity, , written in full matrix notation: x = (y  ). In other words, a
simple network could do exact inference with linear input to hidden weights
 and softmax hidden units.

Appealing again to the EM algorithm, we obtain a cost function that when
minimized by an autoassociator will implement the mixture of gaussians.18
The log probability of the data given the hidden variables can be written as
1(y  Cx) + log|R| + const.

2 log P(y|x, C, R) = (y  Cx)TR

C = (y  Cx)TR

Using this and the previous derivation, we obtain the cost function,
1C],

1(y  Cx) + log|R| + trace[CTR

(8.4)
where  = (cid:104)xxT(cid:105)  (cid:104)x(cid:105)(cid:104)x(cid:105)T. The second-order term, (cid:104)xxT(cid:105), evaluates to
a matrix with x along its diagonal and zero elsewhere. Unlike in factor
analysis,  now depends on the input.

To summarize, the mixture of gaussians model can also be implemented
using an autoassociator. The recognition part of the network is linear, fol-
lowed by a softmax nonlinearity. The cost function is the usual squared er-
ror penalized by a regularizer of exactly the same form as in factor analysis.
Similar network interpretations can be obtained for the other probabilistic
models.

9 Comments and Extensions

There are several advantages, both theoretical and practical, to a unied
treatment of the unsupervised methods reviewed here. From a theoretical
viewpoint, the treatment emphasizes that all of the techniques for infer-
ence in the various models are essentially the same and just correspond
to probability propagation in the particular model variation. Similarly, all
the learning procedures are nothing more than an application of the EM

18 Our derivation assumes tied covariance and equal mixing coefcients. Slightly more

complex equations result for the general case.

332

Sam Roweis and Zoubin Ghahramani

algorithm to increase the total likelihood of the observed data iteratively.
Furthermore, the origin of zero-noise-limit algorithms such as vector quan-
tization and PCA is easy to see. A unied treatment also highlights the
relationship between similar questions across the different models. For ex-
ample, picking the number of clusters in a mixture model or state dimension
in a dynamical system or the number of factors or principal components in
a covariance model or the number of states in an HMM are all really the
same question.

From a practical standpoint, a unied view of these models allows us to
apply well-known solutions to hard problems in one area to similar prob-
lems in another. For example, in this framework it is obvious how to deal
properly with missing data in solving both the learning and inference prob-
lems. This topic has been well understood for many static models (Little
& Rubin, 1987; Tresp, Ahmad, & Neuneier, 1994; Ghahramani & Jordan,
1994) but is typically not well addressed in the linear dynamical systems
literature. As another example, it is easy to design and work with models
having a mixed continuous- and discrete-state vector, (for example, hidden
lter HMMs (Fraser & Dimitriadis, 1993), which is something not directly
addressed by the individual literatures on discrete or continuous models.
Another practical advantage is the ease with which natural extensions
to the basic models can be developed. For example, using the hierarchi-
cal mixture-of-experts formalism developed by Jordan and Jacobs (1994)
we can consider global mixtures of any of the model variants discussed.
In fact, most of these mixtures have already been considered: mixtures of
linear dynamical systems are known as switching state-space models (see
Shumway & Stoffer, 1991; Ghahramani & Hinton, 1996b); mixtures of factor
analyzers (Ghahramani and Hinton, 1997) and of pancakes (PCA) (Hin-
ton et al., 1995); and mixtures of HMMs (Smyth, 1997). A mixture of m of
our constrained mixtures of gaussians each with k clusters gives a mixture
model with mk components in which there are only m possible covariance
matrices. This tied covariance approach is popular in speech modeling to
reduce the number of free parameters. (For k = 1, this corresponds to a full
unconstrained mixture of gaussians model with m clusters.)

It is also possible to consider local mixtures in which the conditional
is no longer a single gaussian but a more complicated
probability P
density such as a mixture of gaussians. For our (constrained) mixture of
gaussians model, this is another way to get a full mixture. For HMMs,
this is a well-known extension and is usually the standard approach for
emission density modeling (Rabiner & Juang, 1986). It is even possible to use
constrained mixture models as the output density model for an HMM (see,
for example, Saul & Rahim, 1998, which uses factor analysis as the HMM
output density). However, we are not aware of any work that considers this
variation in the continuous-state cases, for either static or dynamic data.

Another important natural extension is spatially adaptive observation
noise. The idea here is that the observation noise v can have different statis-

(cid:162)

(cid:161)
yt|xt

A Unifying Review of Linear Gaussian Models

333

tics in different parts of (state or observation) space rather than being de-
scribed by a single matrix R. For discrete-mixture models, this idea is well
known, and it is achieved by giving each mixture component a private noise
model. However, for continuous-state models, this idea is relatively unex-
plored and is an interesting area for further investigation. The crux of the
problem is how to parameterize a positive denite matrix over some space.
We propose some simple ways to achieve this. One possibility is replacing
the single covariance shape Q for the observation noise with a conic19 lin-
ear blending of k basis covariance shapes. In the case of linear dynamical
systems or factor analysis, this amounts to a novel type of model in which
the local covariance matrix R is computed as a conic linear combination of
several canonical covariance matrices through a tensor product between
the current state vector x (or equivalently the noiseless observation Cx)
and a master noise tensorR.20 Another approach would be to drop the conic
restriction (allow general linear combinations) and then add a multiple of
the identity matrix to the resulting noise matrix in order to make it positive
denite. A third approach is to represent the covariance shape as the com-
pression of an elastic sphere due to a spatially varying force eld. This rep-
resentation is easier to work with because the parameterization of the eld
is unconstrained, but it is hard to learn the local eld from measurements
of the effective covariance shape. Bishop (1995, sec. 6.3) and others have
considered simple nonparametric methods for estimating input-dependent
noise levels in regression problems. Goldberg, Williams, and Bishop (1998)
have explored this idea in the context of gaussian processes.

It is also interesting to consider what happens to the dynamic models
when the output noise tends to zero. In other words, what are the dynamic
analogs of PCA and vector quantization? For both linear dynamical systems
and HMMs, this causes the state to no longer be hidden. In linear dynamical
systems, the optimal observation matrix is then found by performing PCA
on the data and using the principal components as the columns of C; for
HMMs, C is found by vector quantization of the data (using the codebook
vectors as the columns of C). Given these observation matrices, the state is no
longer hidden. All that remains is to identify a rst-order Markov dynamics
in state-space: this is a simple AR(1) model in the continuous case or a rst-
order Markov chain in the discrete case. Such zero-noise limits are not only
interesting models in their own right, but are also valuable as good choices
for initialization of learning in linear dynamical systems and HMMs.

19 A conic linear combination is one in which all the coefcients are positive.
20 For mixtures of gaussians or hidden Markov models, this kind of linear blend-
ing merely selects the jth submatrix of the tensor if the discrete state is ej. This is yet
another way to recover the conventional full or unconstrained mixture of gaussians or
hidden Markov model emission density in which each cluster or state has its own private
covariance shape for observation noise.

334

Appendix

Sam Roweis and Zoubin Ghahramani

In this appendix we review in detail the inference and learning algorithms
for each of the models. The goal is to enable readers to implement these
algorithms from the pseudocode provided. For each class of model, we rst
present the solution to the inference problem, and then the EM algorithm
for learning the model parameters. For this appendix only, we adopt the
(cid:48)
, not xT. We
notation that the transpose of a vector or matrix is written as x
use T instead of  to denote the length of a time series. We also dene the
binary operator (cid:175) to be the element-wise product of two equal-size vectors
or matrices. Comments begin with the symbol %.

A.1 Factor Analysis, SPCA, and PCA.

A.1.1 Inference. For factor analysis and related models, the posterior
probability of the hidden state given the observations, P(x|y), is gaus-
sian. The inference problem therefore consists of computing the mean and
covariance of this gaussian, x and V = Cov[x]:

(cid:48) + R)1

FactorAnalysisInference(y,C,R)
  C
(cid:48)(CC
x  y
V  I  C
return x, V
Since the observation noise matrix R is assumed to be diagonal and x
(cid:179)
is of smaller dimension than y,  can be more efciently computed using
the matrix inversion lemma:
I  C(I + C
(cid:48)

(cid:48)
1C)1C
R

 = C
(cid:48)

1

1

R

Computing the (log) likelihood of an observation is nothing more than an

(cid:180)
(cid:162)

.

.

R

evaluation under the gaussian N(cid:161)

O, CC

(cid:48) + R

The sensible PCA (SPCA) algorithm is a special case of factor analysis in
which the observation noise is assumed to be spherically symmetric: R = I.
Inference in SPCA is therefore identical to inference for factor analysis.
The traditional PCA algorithm can be obtained as a limiting case of factor
analysis: R = lim0 I. The inverse used for computing  in factor analysis
is no longer well dened. However, the limit of  is well dened:  =
. Also, the posterior collapses to a single point, so V = Cov[x] =
C)1C
(cid:48)
(C
I  C = 0.

(cid:48)

C)1C
(cid:48)

PCAInference(y,C) % Projection onto principal components
  (C
(cid:48)
x  y
return x

A Unifying Review of Linear Gaussian Models

335

A.1.2 Learning. The EM algorithm for learning the parameters of a fac-
tor analyzer with k factors from a zero-mean data set Y = [y1, . . . , yn] (each
column of the p  n matrix Y is a data point) is

FactorAnalysisLearn(Y,k,)

initialize C, R
compute sample covariance S of Y
while change in log likelihood > 

% E step
X, V  FactorAnalysisInference(Y,C,R)
  YX
(cid:48)
  XX
(cid:48) + nV
% M step
C  
1
set diagonal elements of R to Rii  (S  C

(cid:48)/n)ii

end
return C, R

Here FactorAnalysisInference(Y,C,R) has the obvious interpre-
tation of the inference function applied to the entire matrix of observations.
Since  and V do not depend on Y, this can be computed efciently in
matrix form. Since the data appear only in outer products, we can run fac-
tor analysis learning with just the sample covariance. Note also that the
(cid:48) + R| + const.
log-likelihood is computed as  1
2 y
The EM algorithm for SPCA is identical to the EM algorithm for factor
analysis, except that since the observation noise covariance is spherically
(S 

symmetrical, the M-step for R is changed to R  I, where  (cid:80)p

2 log|CC

(cid:48)(CC

(cid:48) + R)1y + n

j=1

(cid:48)

C

)jj/p.
The EM algorithm for PCA can be obtained in a similar manner:

PCALearn(Y,k,)
initialize C
while change in squared reconstruction error > 

% E step
X  PCAInference(Y,C)
  YX
(cid:48)
  XX
(cid:48)
% M step
C  
1

end
return C

336

Sam Roweis and Zoubin Ghahramani

Since PCA is not a probability model (it assumes zero noise), the likeli-
hood is undened, so convergence is assessed by monitoring the squared
reconstruction error.

A.2 Mixtures of Gaussians and Vector Quantization.

A.2.1 Inference. We begin by discussing the inference problem for mix-
tures of gaussians and then discuss the inference problem in vector quanti-
zation as a limiting case. The hidden variable in a mixture of gaussians is a
discrete variable that can take on one of k values. We represent this variable
using a vector x of length k, where each setting of the hidden variable cor-
responds to x taking a value of unity in one dimension and zero elsewhere.
The probability distribution of the discrete hidden variable, which has k 1
degrees of freedom (since it must sum to one), is fully described by the mean
of x. Therefore, the inference problem is limited to computing the posterior
mean of x given a data point y and the model parameters, which are 
(the prior mean of x), C (the matrix whose k columns are the means of y
given each of the k settings of x) and R (the observation noise covariance
matrix).

MixtureofGaussiansInference(y,C,R,) % compute
  0
for i = 1 to k
i  (y  Ci)(cid:48)
 i exp
i
   + i

(cid:170)
1(y  Ci)
R
2 i

% responsibilities

(cid:169) 1

endx  /
return x
A measure of the randomness of the hidden state can be obtained by
evaluating the entropy of the discrete distribution corresponding to x.
Standard VQ corresponds to the limiting case R = lim0 I and equal
priors i = 1/k. Inference in this case is performed by the well known
(1-)nearest-neighbor rule.

VQInference(y,C) % 1-nearest-neighbor
for i = 1 to k

i  (y  Ci)(cid:48)(y  Ci)
endx  ej for j = arg min i
return x

A Unifying Review of Linear Gaussian Models

337

As before, ej is the unit vector along the jth coordinate direction. The squared
distances i can be generalized to a Mahalanobis metric with respect to
some matrix R, and nonuniform priors i can easily be incorporated. As
was the case with PCA, the posterior distribution has zero entropy.

A.2.2 Learning. The EM algorithm for learning the parameters of a mix-

ture of gaussian is:

MixtureofGaussiansLearn(Y,k,)% ML soft competitive learning

initialize C, R, 
while change in log likelihood > 
initialize   0,   0,   0
% E step
for i = 1 to n
xi  MixtureofGaussiansInference(yi,C,R,)
   + yix
(cid:48)
   + xi
i

end
% M step
for j = 1 to k
Cj  j/j
for i = 1 to n

   + xij(yi  Cj)(yi  Cj)(cid:48)

end
end
R  /n
  /n

end
return C, R, 

We have assumed a common covariance matrix R for all the gaussians; the
extension to different covariances for each gaussian is straightforward.

The k-means vector quantization learning algorithm results when we

take the appropriate limit of the above algorithm:

VQLearn(Y,k,) % k-means

initialize C
while change in squared reconstruction error > 

% E step
initialize   0,   0
for i = 1 to n
xi  VQInference(yi,C)
   + yix
(cid:48)
   + xi
i

338

Sam Roweis and Zoubin Ghahramani

end
% M step
for j = 1 to k
Cj  j/j

end

end
return C

A.3 Linear Dynamical Systems.

A.3.1 Inference.

Inference in a linear dynamical system involves com-
puting the posterior distributions of the hidden state variables given the
sequence of observations. As in factor analysis, all the hidden state variables
are assumed gaussian and are therefore fully described by their means and
covariance matrices. The algorithm for computing the posterior means and
covariances consists of two parts: a forward recursion, which uses the ob-
servations from y1 to yt, known as the Kalman lter (Kalman, 1960), and a
backward recursion, which uses the observations from yT to yt+1 (Rauch,
1963). The combined forward and backward recursions are known as the
Kalman or Rauch-Tung-Streibel (RTS) smoother.

t and Vs

To describe the smoothing algorithm it will be useful to dene the follow-
ing quantities: xs
t are, respectively, the mean and covariance matrix
of xt given observations {y1, . . . ys}; xt  xT
t are the full
smoother estimates. To learn the A matrix using EM, it is also necessary to
compute the covariance across time between xt and xt1:

t and Vt  VT

LDSInference(Y,A,C,Q,R,x0
for t = 1 to T % Kalman lter (forward pass)

1,V0

1) % Kalman smoother

end
initialize VT,T1 = (I  KTC)AVT1
T1
for t = T to 2 % Rauch recursions (backward pass)

t

)1

Jt1  Vt1
(cid:48)(Vt1
t1A
+ Jt1(xt  Axt1
xt1  xt1
t1
t1
+ Jt1( Vt  Vt1
Vt1  Vt1
(cid:48)
t1
t1
)J
Vt,t1  Vt
+ Jt( Vt+1,t  AVt
(cid:48)
(cid:48)
t1
t1 if t < T
)J
tJ

)

t

t

t

t

 Axt1
xt1
t1 if t > 1
(cid:48) + Q if t > 1
 AVt1
Vt1
t1A
Kt  Vt1
(cid:48) + R)1
(cid:48)(CVt1
t C
t C
+ Kt(yt  Cxt1
 xt1
xt
t
 KtCVt1
 Vt1
Vt
t

)

t

t

t

t

A Unifying Review of Linear Gaussian Models

339

end

return xt, Vt, Vt,t1 for all t

A.3.2 Learning. The EM algorithm for learning a linear dynamical sys-
tem (Shumway & Stoffer, 1982; Ghahramani & Hinton, 1996a) is given below,
assuming for simplicity that we have only a single sequence of observations:

LDSLearn(Y,k,)

set  (cid:80)

(cid:48)
t

t yty

initialize A, C, Q, R, x0

1, V0
1

while change in log likelihood > 
LDSInference(Y,A,C,Q,R,x0
1,V0
initialize   0,   0,   0
for t = 1 to T
   + ytx
(cid:48)
t
+ Vt
   + xtx
(cid:48)
t
+ Vt,t1 if t > 1
   + xtx
(cid:48)
t1
 VT
   xTx
(cid:48)
 V1
T
   x1x
(cid:48)
1

1) % E step

end
1
2
% M step
C  
1
R  (  C
A  
1
1
 A
Q  (2
 x1
x0
1
 V1
V0
1

(cid:48)

)/T
(cid:48))/(T  1)

end
return A, C, Q, R, x0

1, V0
1

A.4 Hidden Markov Models.

A.4.1 Inference. The forward-backward algorithm computes the poste-
rior probabilities of the hidden states in an HMM and therefore forms the
basis of the inference required for EM. We use the following standard de-
nitions,

t = P(xt, y1, . . . , yt)

= P(yt+1, . . . , yT|xt),

t

(A.1a)

(A.1b)

340

Sam Roweis and Zoubin Ghahramani

where both  and  are vectors of the same length as x. We present the
case where the observations yt are real-valued p-dimensional vectors and
the probability density of an observation given the corresponding state (the
output model) is assumed to be a single gaussian with mean Cxt and
covariance R. The parameters of the model are therefore a k  k transition
matrix T, initial state prior probability vector , an observation mean matrix
C, and an observation noise matrix R that is tied across states:

HMMInference(Y,T,,C,R) % forwardbackward algorithm
for t = 1 to T % forward pass
(yt  Ci)(cid:48)

(cid:170) |R|1/2(2 )p/2

for i = 1 to k
bt,i  exp

(cid:169) 1

2

end

t (cid:80)

1(yt  Ci)
R
if (t = 1) t   (cid:175) b1 else t  [T
i t,i
t  t/t
 1/T

end
T
for t = T  1 to 1 % backward pass

(cid:48)

t1] (cid:175) bt end

(cid:175) bt+1]/t

 T [t+1
t
 t (cid:175) t
(cid:80)
(cid:48)
)
/(
tt
 t[t+1
(cid:175) bt+1]
(cid:48)
 t
ij tij

end
t
t
t
return t, t, t for all t

/(

)

The denitions of  and  in equations A.1a and A.1b correspond to
running the above algorithm without the scaling factors t. These factors,
however, are essential to the numerical stability of the algorithm; otherwise,
for long sequences, both  and  become vanishingly small. Furthermore,
from the s we can compute the log-likelihood of the sequence

log P(y1, . . . , yT) = T(cid:88)

log t,

t=1

which is why it is useful for the above function to return them.

A.4.2 Learning. Again, we assume for simplicity that we have a single
sequence of observations from which we wish to learn the parameters of
an HMM. The EM algorithm for learning these parameters, known as the

A Unifying Review of Linear Gaussian Models

341

Baum-Welch algorithm, is:

HMMLearn(Y,k,) % Baum-Welch

initialize T, , C, R
while change in log likelihood > 
HMMInference(Y,T,,C,R) % E step
% M step
  1
t t
Tij  Tij/

(cid:80)
(cid:96) Ti(cid:96) for all i, j
t t,j for all j
(yt  Cj)(yt  Cj)(cid:48)/T

t t,jyt/
t,j t,j

(cid:80)

T (cid:80)
Cj (cid:80)
R (cid:80)

return T, , C, R

Acknowledgments

We thank Carlos Brody, Sanjoy Mahajan, and Erik Winfree for many fruitful
discussions in the early stages, the anonymous referees for helpful com-
ments, and Geoffrey Hinton and John Hopeld for providing outstanding
intellectual environments and guidance. S.R. was supported in part by the
Center for Neuromorphic Systems Engineering as a part of the National Sci-
ence Foundation Engineering Research Center Program under grant EEC-
9402726 and by the Natural Sciences and Engineering Research Council of
Canada under an NSERC 1967 Award. Z.G. was supported by the Ontario
Information Technology Research Centre.

