Abstract

In recent years, deep learning approaches have gained signicant interest as a
way of building hierarchical representations from unlabeled data. However, to
our knowledge, these deep learning approaches have not been extensively stud-
ied for auditory data.
In this paper, we apply convolutional deep belief net-
works to audio data and empirically evaluate them on various audio classication
tasks. In the case of speech data, we show that the learned features correspond to
phones/phonemes. In addition, our feature representations learned from unlabeled
audio data show very good performance for multiple audio classication tasks.
We hope that this paper will inspire more research on deep learning approaches
applied to a wide range of audio recognition tasks.

Introduction

1
Understanding how to recognize complex, high-dimensional audio data is one of the greatest chal-
lenges of our time. Previous work [1, 2] revealed that learning a sparse representation of auditory
signals leads to lters that closely correspond to those of neurons in early audio processing in mam-
mals. For example, when sparse coding models are applied to natural sounds or speech, the learned
representations (basis vectors) showed a striking resemblance to the cochlear lters in the auditory
cortex. In related work, Grosse et al. [3] proposed an efcient sparse coding algorithm for auditory
signals and demonstrated its usefulness in audio classication tasks.
However, the proposed methods have been applied to learn relatively shallow, one-layer representa-
tions. Learning more complex, higher-level representation is still a non-trivial, challenging problem.
Recently, many promising approaches have been proposed to learn the processing steps of the sec-
ond stage and beyond [4, 5, 6, 7, 8]. These deep learning algorithms try to learn simple features
in the lower layers and more complex features in the higher layers. However, to the best of our
knowledge, these deep learning approaches have not been extensively applied to auditory data.
The deep belief network [4] is a generative probabilistic model composed of one visible (observed)
layer and many hidden layers. Each hidden layer unit learns a statistical relationship between the
units in the lower layer; the higher layer representations tend to become more complex. The deep
belief network can be efciently trained using greedy layerwise training, in which the hidden layers
are trained one at a time in a bottom-up fashion [4]. Recently, convolutional deep belief networks [9]
have been developed to scale up the algorithm to high-dimensional data. Similar to deep belief
networks, convolutional deep belief networks can be trained in a greedy, bottom-up fashion. By
applying these networks to images, Lee et al. (2009) showed good performance in several visual
recognition tasks [9].
In this paper, we will apply convolutional deep belief networks to unlabeled auditory data (such as
speech and music) and evaluate the learned feature representations on several audio classication
tasks. In the case of speech data, we show that the learned features correspond to phones/phonemes.
In addition, our feature representations outperform other baseline features (spectrogram and MFCC)

1

for multiple audio classication tasks. In particular, our method compares favorably with other state-
of-the-art algorithms for the speaker identication task. For the phone classication task, MFCC
features can be augmented with our features to improve accuracy. We also show for certain tasks
that the second-layer features produce higher accuracy than the rst-layer features, which justies
the use of deep learning approaches for audio classication. Finally, we show that our features give
better performance in comparison to other baseline features for music classication tasks. In our
experiments, the learned features often performed much better than other baseline features when
there was only a small number of labeled training examples. To the best of our knowledge, we are
the rst to apply deep learning algorithms to a range of audio classication tasks. We hope that this
paper will inspire more research on deep learning approaches applied to audio recognition tasks.

2 Algorithms
2.1 Convolutional deep belief networks

We rst briey review convolutional restricted Boltzmann machines (CRBMs) [9, 10, 11] as building
blocks for convolutional deep belief networks (CDBNs). We will follow the formulation of [9] and
adapt it to a one dimensional setting. For the purpose of this explanation, we assume that all inputs
to the algorithm are single-channel time-series data with nV frames (an nV dimensional vector);
however, the formulation can be straightforwardly extended to the case of multiple channels.
The CRBM is an extension of the regular RBM [4] to a convolutional setting, in which the weights
between the hidden units and the visible units are shared among all locations in the hidden layer.
The CRBM consists of two layers: an input (visible) layer V and a hidden layer H. The hidden
units are binary-valued, and the visible units are binary-valued or real-valued.
Consider the input layer consisting of an nV dimensional array of binary units. To construct the
hidden layer, consider K nW -dimensional lter weights W K (also referred to as bases throughout
this paper). The hidden layer consists of K groups of nH-dimensional arrays (where nH (cid:44)
nV  nW + 1) with units in group k sharing the weights W k. There is also a shared bias bk for each
group and a shared bias c for the visible units. The energy function can then be dened as:

E(v, h) =  K(cid:88)
nH(cid:88)
nW(cid:88)
i  K(cid:88)
nV(cid:88)

nH(cid:88)

v2

k=1

r=1

j=1

1
2

hk
j W k

nW(cid:88)

E(v, h) =

nV(cid:88)

i=1

bk

r vj+r1  K(cid:88)
nH(cid:88)
r vj+r1  K(cid:88)

hk
j W k

k=1

j=1

bk

j  c
hk
nH(cid:88)

nV(cid:88)

i

k=1

j=1

r=1

k=1

j=1

i=1

Similarly, the energy function of CRBM with real-valued visible units can be dened as:

vi.

(1)

j  c
hk

vi.

(2)

The joint and conditional probability distributions are dened as follows:

1
Z

exp(E(v, h))

P (hk

P (v, h) =
j = 1|v) = sigmoid(( W k v v)j + bk)

P (vi = 1|h) = sigmoid((cid:88)
P (vi|h) = N ormal((cid:88)

(W k f hk)i + c)

k

(for binary visible units)

(W k f hk)i + c, 1)

(for real visible units),

(3)

(4)

(5)

(6)

k

where v is a valid convolution, f is a full convolution,1 and W k
nW j+1. Since all units
in one layer are conditionally independent given the other layer, inference in the network can be
efciently performed using block Gibbs sampling. Lee et al. [9] further developed a convolutional
RBM with probabilistic max-pooling, where the maxima over small neighborhoods of hidden
units are computed in a probabilistically sound way. (See [9] for more details.) In this paper, we use
CRBMs with probabilistic max-pooling as building blocks for convolutional deep belief networks.

(cid:44) W k

j

1Given an m-dimensional vector and an n-dimensional kernel (where m > n), valid convolution gives a

(m  n + 1)-dimensional vector, and full convolution gives a (m + n  1)-dimensional vector.

2

For training the convolutional RBMs, computing the exact gradient for the log-likelihood term is in-
tractable. However, contrastive divergence [12] can be used to approximate the gradient effectively.
Since a typical CRBM is highly overcomplete, a sparsity penalty term is added to the log-likelihood
objective [8, 9]. More specically, the training objective can be written as

minimizeW,b,c

Llikelihood(W, b, c) + Lsparsity(W, b, c),

(7)
where Llikelihood is a negative log-likelihood that measures how well the CRBM approximates the
input data distribution, and Lsparsity is a penalty term that constrains the hidden units to having
sparse average activations. This sparsity regularization can be viewed as limiting the capacity
of the network, and it often results in more easily interpretable feature representations. Once the
parameters for all the layers are trained, we stack the CRBMs to form a convolutional deep belief
network. For inference, we use feed-forward approximation.
2.2 Application to audio data
For the application of CDBNs to audio data, we rst convert time-domain signals into spectro-
grams. However, the dimensionality of the spectrograms is large (e.g., 160 channels). We apply
PCA whitening to the spectrograms and create lower dimensional representations. Thus, the data
we feed into the CDBN consists of nc channels of one-dimensional vectors of length nV , where nc is
the number of PCA components in our representation. Similarly, the rst-layer bases are comprised
of nc channels of one-dimensional lters of length nW .
3 Unsupervised feature learning
3.1 Training on unlabeled TIMIT data
We trained the rst and second-layer CDBN representations using a large, unlabeled speech dataset.
First, we extracted the spectrogram from each utterance of the TIMIT training data [13]. The spec-
trogram had a 20 ms window size with 10 ms overlaps. The spectrogram was further processed using
PCA whitening (with 80 components) to reduce the dimensionality. We then trained 300 rst-layer
bases with a lter length (nW ) of 6 and a max-pooling ratio (local neighborhood size) of 3. We
further trained 300 second-layer bases using the max-pooled rst-layer activations as input, again
with a lter length of 6 and a max-pooling ratio of 3.
3.2 Visualization
In this section, we illustrate what the network learns through visualization. We visualize the rst-
layer bases by multiplying the inverse of the PCA whitening on each rst-layer basis (Figure 1).
Each second-layer basis is visualized as a weighted linear combination of the rst-layer bases.

Figure 1: Visualization of randomly selected rst-layer CDBN bases trained on the TIMIT data.
Each column represents a temporal receptive eld of a rst-layer basis in the spectrogram space.
The frequency channels are ordered from the lowest frequency (bottom) to the highest frequency
(top). All gures in the paper are best viewed in color.
3.2.1 Phonemes and the CDBN features
In Figure 2, we show how our bases relate to phonemes by comparing visualizations of each
phoneme with the bases that are most activated by that phoneme.
For each phoneme, we show ve spectrograms of sound clips of that phoneme (top ve columns in
each phoneme group), and the ve rst-layer bases with the highest average activations on the given
phoneme (bottom ve columns in each phoneme group). Many of the rst-layer bases closely match
the shapes of phonemes. There are prominent horizontal bands in the lower frequencies of the rst-
layer bases that respond most to vowels (for example, ah and oy). The bases that respond most

3

high freq. low freq.Figure 2: Visualization of the four different phonemes and their corresponding rst-layer CDBN
bases. For each phoneme: (top) the spectrograms of the ve randomly selected phones; (bottom)
ve rst-layer bases with the highest average activations on the given phoneme.

to fricatives (for example, s) typically take the form of widely distributed areas of energy in the
high frequencies of the spectrogram. Both of these patterns reect the structure of the corresponding
phoneme spectrograms.
Closer inspection of the bases provides slight evidence that the rst-layer bases also capture more
ne-grained details. For example, the rst and third oy bases reect the upward-slanting pattern
in the phoneme spectrograms. The top el bases mirror the intensity patterns of the corresponding
phoneme spectrograms: a high intensity region appears in the lowest frequencies, and another region
of lesser intensity appears a bit higher up.

3.2.2 Speaker gender information and the CDBN features

In Figure 3, we show an analysis of two-layer CDBN feature representations with respect to the gen-
der classication task (Section 4.2). Note that the network was trained on unlabeled data; therefore,
no information about speaker gender was given during training.

Figure 3: (Left) ve spectrogram samples of ae phoneme from female (top)/male (bottom) speak-
ers.
(Middle) Visualization of the ve rst-layer bases that most differentially activate for fe-
male/male speakers. (Right) Visualization of the ve second-layer bases that most differentially
activate for female/male speakers.

For comparison with the CDBN features, randomly selected spectrograms of female (top left ve
columns) and male (bottom left ve columns) pronunciations of the ae phoneme from the TIMIT
dataset are shown. Spectrograms for the female pronunciations are qualitatively distinguishable by a
ner horizontal banding pattern in low frequencies, whereas male pronunciations have more blurred

4

Example phones ("ah")Example phones ("oy")Example phones ("el")Example phones ("s")First layer basesFirst layer basesFirst layer basesFirst layer basesExample phones (female)First layer bases ("female")Second layer bases ("female")Example phones (male)First layer bases ("male")Second layer bases ("male")patterns. This gender difference in the vowel pronunciation patterns is typical across the TIMIT
data.
Only the bases that are most biased to activate on either male or female speech are shown. The bases
that are most active on female speech encode the horizontal band pattern that is prominent in the
spectrograms of female pronunciations. On the other hand, the male-biased bases have more blurred
patterns, which again visually matches the corresponding spectrograms.

4 Application to speech recognition tasks
In this section, we demonstrate that the CDBN feature representations learned from the unlabeled
speech corpus can be useful for multiple speech recognition tasks, such as speaker identication,
gender classication, and phone classication. In most of our experiments, we followed the self-
taught learning framework [14]. The motivation for self-taught learning comes from situations
where we are given only a small amount of labeled data and a large amount of unlabeled data;2
therefore, one of our main interests was to evaluate the different feature representations given a small
number of labeled training examples (as often assumed in self-taught learning or semi-supervised
learning settings). More specically, we trained the CDBN on unlabeled TIMIT data (as described
in Section 3.1); then we used the CDBN features for classication on labeled training/test data3 that
were randomly selected from the TIMIT corpus.4

4.1 Speaker identication

We evaluated the usefulness of the learned CDBN representations for the speaker identication task.
The subset of the TIMIT corpus that we used for speaker identication has 168 speakers and 10
utterances (sentences) per speaker, resulting in a total of 1680 utterances. We performed 168-way
classication on this set. For each number of utterances per speaker, we randomly selected training
utterances and testing utterances and measured the classication accuracy; we report the results
averaged over 10 random trials.5 To construct training and test data for the classication task,
we extracted a spectrogram from each utterance in the TIMIT corpus. We denote this spectrogram
representation as RAW features. We computed the rst and second-layer CDBN features using the
spectrogram as input. We also computed MFCC features, widely-used standard features for generic
speech recognition tasks. As a result, we obtained spectrogram/MFCC/CDBN representations for
each utterance with multiple (typically, several hundred) frames. In our experiments, we used simple
summary statistics (for each channel) such as average, max, or standard deviation over all the frames.
We evaluated the features using standard supervised classiers, such as SVM, GDA, and KNN.
The choices of summary statistics and hyperparameters for the classiers were done using cross-
validation. We report the average classication accuracy (over 10 random trials) with a varying
number of training examples.
Table 1 shows the average classication accuracy for each feature representation. The results
show that the rst and second CDBN representations both outperform baseline features (RAW and
MFCC). The numbers compare MFCC and CDBN features with as many of the same factors (such as
preprocessing and classication algorithms) as possible. Further, to make a fair comparison between
CDBN features and MFCC, we used the best performing implementation6 among several standard
implementations for MFCC. Our results suggest that without special preprocessing or postprocess-

2In self-taught learning, the labeled data and unlabeled data dont need to share the same labels or the same

generative distributions.

3There are two disjoint TIMIT data sets. We drew unlabeled data from the larger of the two for unsupervised
feature learning, and we drew labeled data from the other data set to create our training and test set for the
classication tasks.

4In the case of phone classication, we followed the standard protocol (e.g., [15]) rather than self-taught

learning framework to evaluate our algorithm in comparison to other methods.

5Details: There were some exceptions to this; for the case of eight training utterances, we followed
Reynolds (1995) [16]; more specically, we used eight training utterances (2 sa sentences, 3 si sentences and
rst 3 sx sentences); the two testing utterances were the remaining 2 sx sentences. We used cross validation
for selecting hyperparameters for classication, except for the case of 1 utterance per speaker, where we used a
randomly selected validation sentence per speaker.

6We used Dan Ellis implementation available at: http://labrosa.ee.columbia.edu/matlab/

rastamat.

5

Table 1: Test classication accuracy for speaker identication using summary statistics
RAW MFCC CDBN L1 CDBN L2 CDBN L1+L2
#training utterances per speaker
46.7% 54.4%
43.5% 69.9%
67.9% 76.5%
80.6% 82.6%
90.4% 92.0%

74.5%
76.7%
91.3%
93.7%
97.9%

72.8%
76.7%
91.8%
93.8%
97.0%

62.8%
66.2%
84.3%
89.6%
95.2%

1
2
3
5
8

Table 2: Test classication accuracy for speaker identication using all frames

#training utterances per speaker MFCC ([16]s method) CDBN MFCC ([16]) + CDBN

1
2
3
5
8

40.2%
87.9%
95.9%
99.2%
99.7%

90.0%
97.9%
98.7%
99.2%
99.7%

90.7%
98.7%
99.2%
99.6%
100.0%

ing (besides the summary statistics which were needed to reduce the number of features), the CDBN
features outperform MFCC features, especially in a setting with a very limited number of labeled
examples.
We further experimented to determine if the CDBN features can achieve competitive performance in
comparison to other more sophisticated, state-of-the-art methods. For each feature representation,
we used the classier that achieved the highest performance. More specically, for the MFCC fea-
tures we replicated Reynolds (1995)s method,7 and for the CDBN features we used a SVM based
ensemble method.8 As shown in Table 2, the CDBN features consistently outperformed MFCC fea-
tures when the number of training examples was small. We also combined both methods by taking a
linear combination of the two classier outputs (before taking the nal classication prediction from
each algorithm).9 The resulting combined classier performed the best, achieving 100% accuracy
for the case of 8 training utterances per speaker.

4.2 Speaker gender classication

We also evaluated the same CDBN features which were learned for the speaker identication task on
the gender classication task. We report the classication accuracy for various quantities of training
examples (utterances) per gender. For each number of training examples, we randomly sampled
training examples and 200 testing examples; we report the test classication accuracy averaged
over 20 trials. As shown in Table 3, both the rst and second CDBN features outperformed the
baseline features, especially when the number of training examples were small. The second-layer
CDBN features consistently performed better than the rst-layer CDBN features. This suggests that
the second-layer representation learned more invariant features that are relevant for speaker gender
classication, justifying the use of deep architectures.

4.3 Phone classication

Finally, we evaluated our learned representation on phone classication tasks. For this experiment,
we treated each phone segment as an individual example and computed the spectrogram (RAW) and
MFCC features for each phone segment. Similarly, we computed the rst-layer CDBN representa-
tions. Following the standard protocol [15], we report the 39 way phone classication accuracy on
the test data (TIMIT core test set) for various numbers of training sentences. For each number of
training examples, we report the average classication accuracy over 5 random trials. The summary

7Details: In [16], MFCC features (with multiple frames) were computed for each utterance; then a Gaussian
mixture model was trained for each speaker (treating each individual MFCC frame as a input example to the
GMM. For the a given test utterance, the prediction was made by determining the GMM model that had the
highest test log-likelihood.

8In detail, we treated each single-frame CDBN features as an individual example. Then, we trained a multi-
class linear SVM for these individual frames. For testing, we computed SVM prediction score for each speaker,
and then aggregated predictions from all the frames. Overall, the highest scoring speaker was selected for the
prediction.

9The constant for the linear combination was xed across all the numbers of training utterances, and it was

selected using cross validation.

6

Table 3: Test accuracy for gender classication problem

#training utterances per gender

1
2
3
5
7
10

RAW MFCC CDBN L1 CDBN L2 CDBN L1+L2
68.4% 58.5%
76.7% 78.7%
79.5% 84.1%
84.4% 86.9%
89.2% 89.0%
91.3% 89.8%

85.8%
92.5%
94.2%
95.8%
96.6%
96.7%

83.6%
92.3%
94.2%
95.6%
96.5%
96.6%

78.5%
86.0%
88.9%
93.1%
94.2%
94.7%

Table 4: Test accuracy for phone classication problem

#training utterances

100
200
500
1000
2000
3696

RAW MFCC MFCC ([15]s method) CDBN L1 MFCC+CDBN L1 ([15])
36.9% 58.3%
37.8% 61.5%
38.7% 64.9%
39.0% 67.2%
39.2% 69.2%
39.4% 70.8%

67.2%
71.0%
75.1%
77.1%
79.2%
80.3%

66.6%
70.3%
74.1%
76.3%
78.4%
79.6%

53.7%
56.7%
59.7%
61.6%
63.1%
64.4%

results are shown in Table 4. In this experiment, the rst-layer CDBN features performed better
than spectrogram features, but they did not outperform the MFCC features. However, by combining
MFCC features and CDBN features, we could achieve about 0.7% accuracy improvement consis-
tently over all the numbers of training utterances. In the realm of phone classication, in which
signicant research effort is often needed to achieve even improvements well under a percent, this
is a signicant improvement. [17, 18, 19, 20]
This suggests that the rst-layer CDBN features learned somewhat informative features for phone
classication tasks in an unsupervised way. In contrast to the gender classication task, the second-
layer CDBN features did not offer much improvement over the rst-layer CDBN features. This
result is not unexpected considering the fact that the time-scale of most phonemes roughly corre-
sponds to the time-scale of the rst-layer CDBN features.

5 Application to music classication tasks
In this section, we assess the applicability of CDBN features to various music classication tasks.

Table 5: Test accuracy for 5-way music genre classication

Train examples

1
2
3
5

RAW MFCC CDBN L1 CDBN L2 CDBN L1+L2
51.6% 54.0%
57.0% 62.1%
59.7% 65.3%
65.8% 68.3%

66.1%
69.7%
70.0%
73.1%

62.5%
67.9%
66.7%
69.2%

64.3%
69.5%
69.5%
72.7%

5.1 Music genre classication

For the task of music genre classication, we trained the rst and second-layer CDBN representa-
tions on an unlabeled collection of music data.10 First, we computed the spectrogram (20 ms window
size with 10 ms overlaps) representation for individual songs. The spectrogram was PCA-whitened
and then fed into the CDBN as input data. We trained 300 rst-layer bases with a lter length of 10
and a max-pooling ratio of 3. In addition, we trained 300 second-layer bases with a lter length of
10 and a max-pooling ratio of 3.
We evaluated the learned CDBN representation for 5-way genre classication tasks. The training
and test songs for the classication tasks were randomly sampled from 5 genres (classical, electric,
jazz, pop, and rock) and did not overlap with the unlabeled data. We randomly sampled 3-second
segments from each song and treated each segment as an individual training or testing example. We
report the classication accuracy for various numbers of training examples. For each number of
training examples, we averaged over 20 random trials. The results are shown in Table 5. In this task,
the rst-layer CDBN features performed the best overall.

10Available from http://ismir2004.ismir.net/ISMIR_Contest.html.

7

5.2 Music artist classication
Furthermore, we evaluated whether the CDBN features are useful in identifying individual artists.11
Following the same procedure as in Section 5.1, we trained the rst and second-layer CDBN rep-
resentations from an unlabeled collection of classical music data. Some representative bases are
shown in Figure 4. Then we evaluated the learned CDBN representation for 4-way artist identi-
cation tasks. The disjoint sets of training and test songs for the classication tasks were randomly
sampled from the songs of four artists. The unlabeled data and the labeled data did not include the
same artists. We randomly sampled 3-second segments from each song and treated each segment as
an individual example. We report the classication accuracy for various quantities of training ex-
amples. For each number of training examples, we averaged over 20 random trials. The results are
shown in Table 6. The results show that both the rst and second-layer CDBN features performed
better than the baseline features, and that either using the second-layer features only or combining
the rst and the second-layer features yielded the best results. This suggests that the second-layer
CDBN representation might have captured somewhat useful, higher-level features than the rst-layer
CDBN representation.

Figure 4: Visualization of randomly selected rst-layer CDBN bases trained on classical music data.

Table 6: Test accuracy for 4-way artist identication

Train examples

1
2
3
5

RAW MFCC CDBN L1 CDBN L2 CDBN L1+L2
56.0% 63.7%
69.4% 66.1%
73.9% 67.9%
79.4% 71.6%

69.2%
76.3%
78.7%
81.4%

67.6%
76.1%
78.0%
80.9%

67.7%
74.2%
75.8%
81.9%

6 Discussion and conclusion
Modern speech datasets are much larger than the TIMIT dataset. While the challenge of larger
datasets often lies in considering harder tasks, our objective in using the TIMIT data was to restrict
the amount of labeled data our algorithm had to learn from. It remains an interesting problem to
apply deep learning to larger datasets and more challenging tasks.
In this paper, we applied convolutional deep belief networks to audio data and evaluated on various
audio classication tasks. By leveraging a large amount of unlabeled data, our learned features
often equaled or surpassed MFCC features, which are hand-tailored to audio data. Furthermore,
even when our features did not outperform MFCC, we could achieve higher classication accuracy
by combining both. Also, our results show that a single CDBN feature representation can achieve
high performance on multiple audio recognition tasks. We hope that our approach will inspire more
research on automatically learning deep feature hierarchies for audio data.
Acknowledgment
We thank Yoshua Bengio, Dan Jurafsky, Yun-Hsuan Sung, Pedro Moreno, Roger Grosse for helpful
discussions. We also thank anonymous reviewers for their constructive comments. This work was
supported in part by the National Science Foundation under grant EFRI-0835878, and in part by the
Ofce of Naval Research under MURI N000140710747.

11In our experiments, we found that artist identication task was more difcult than the speaker identication

task because the local sound patterns can be highly variable even for the same artist.

8

