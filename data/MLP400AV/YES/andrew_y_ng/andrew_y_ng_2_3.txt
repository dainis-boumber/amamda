Abstract

Recursive structure is commonly found in the
inputs of dierent modalities such as natural
scene images or natural language sentences.
Discovering this recursive structure helps us
to not only identify the units that an image or
sentence contains but also how they interact
to form a whole. We introduce a max-margin
structure prediction architecture based on re-
cursive neural networks that can successfully
recover such structure both in complex scene
images as well as sentences. The same algo-
rithm can be used both to provide a competi-
tive syntactic parser for natural language sen-
tences from the Penn Treebank and to out-
perform alternative approaches for semantic
scene segmentation, annotation and classi-
cation. For segmentation and annotation our
algorithm obtains a new level of state-of-the-
art performance on the Stanford background
dataset (78.1%). The features from the im-
age parse tree outperform Gist descriptors for
scene classication by 4%.

1. Introduction

Recursive structure is commonly found in dierent
modalities, as shown in Fig. 1. The syntactic rules
of natural language are known to be recursive, with
noun phrases containing relative clauses that them-
selves contain noun phrases, e.g., . . . the church which
has nice windows . . . . Similarly, one nds nested hier-
archical structuring in scene images that capture both
part-of and proximity relationships. For instance, cars
are often on top of street regions. A large car region

Appearing in Proceedings of the 28 th International Con-
ference on Machine Learning, Bellevue, WA, USA, 2011.
Copyright 2011 by the author(s)/owner(s).

Figure 1. Illustration of our recursive neural network ar-
chitecture which parses images and natural language sen-
tences. Segment features and word indices (orange) are
rst mapped into semantic feature space (blue) and then
recursively merged by the same neural network until they
represent the entire image or sentence. Both mappings and
mergings are learned.

can be recursively split into smaller car regions depict-
ing parts such as tires and windows and these parts can
occur in other contexts such as beneath airplanes or in
houses. We show that recovering this structure helps
in understanding and classifying scene images. In this
paper, we introduce recursive neural networks (RNNs)
for predicting recursive structure in multiple modali-
ties. We primarily focus on scene understanding, a
central task in computer vision often subdivided into
segmentation, annotation and classication of scene
images. We show that our algorithm is a general tool

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

for predicting tree structures by also using it to parse
natural language sentences.

Fig. 1 outlines our approach for both modalities. Im-
ages are oversegmented into small regions which of-
ten represent parts of objects or background. From
these regions we extract vision features and then map
these features into a semantic space using a neu-
ral network. Using these semantic region representa-
tions as input, our RNN computes (i) a score that
is higher when neighboring regions should be merged
into a larger region, (ii) a new semantic feature rep-
resentation for this larger region, and (iii) its class la-
bel. Class labels in images are visual object categories
such as building or street. The model is trained so
that the score is high when neighboring regions have
the same class label. After regions with the same ob-
ject label are merged, neighboring objects are merged
to form the full scene image. These merging decisions
implicitly dene a tree structure in which each node
has associated with it the RNN outputs (i)-(iii), and
higher nodes represent increasingly larger elements of
the image.

The same algorithm is used to parse natural language
sentences. Again, words are rst mapped into a se-
mantic space and then they are merged into phrases
in a syntactically and semantically meaningful order.
The RNN computes the same three outputs and at-
taches them to each node in the parse tree. The class
labels are phrase types such as noun phrase (NP) or
verb phrase (VP).

Contributions.
This is the rst deep learning
method to achieve state-of-the-art results on segmen-
tation and annotation of complex scenes. Our recur-
sive neural network architecture predicts hierarchical
tree structures for scene images and outperforms other
methods that are based on conditional random elds
or combinations of other methods. For scene classi-
cation, our learned features outperform state of the
art methods such as Gist descriptors. Furthermore,
our algorithm is general in nature and can also parse
natural language sentences obtaining competitive per-
formance on maximum length 15 sentences of the Wall
Street Journal dataset. Code for the RNN model is
available at www.socher.org.

2. Related Work
Five key research areas inuence and motivate our
method. We briey outline connections and dierences
between them. Due to space constraints, we cannot do
justice to the complete literature.

Scene Understanding has become a central task in
computer vision. The goal is to understand what ob-

jects are in a scene (annotation), where the objects
are located (segmentation) and what general scene
type the image shows (classication). Some meth-
ods for this task such as (Aude & Torralba, 2001;
Schmid, 2006) rely on a global descriptor which can
do very well for classifying scenes into broad cat-
egories. However, these approaches fail to gain a
deeper understanding of the objects in the scene.
At the same time, there is a myriad of dierent
approaches for image annotation and semantic seg-
mentation of objects into regions (Rabinovich et al.,
2007; Gupta & Davis, 2008). Recently, these ideas
have been combined to provide more detailed scene
understanding (Hoiem et al., 2006; Li et al., 2009;
Gould et al., 2009; Socher & Fei-Fei, 2010).

Our algorithm parses an image; that is, it recursively
merges pairs of segments into super segments in a se-
mantically and structurally coherent way. Many other
scene understanding approaches only consider a at
set of regions. Some approaches such as (Gould et al.,
2009) also consider merging operations. For merged
super segments, they compute new features. In con-
trast, our RNN-based method learns a representation
for super segments. This learned representation to-
gether with simple logistic regression outperforms the
original vision features and complex conditional ran-
dom eld models. Furthermore, we show that the im-
age parse trees are useful for scene classication and
outperform global scene features such as Gist descrip-
tors (Aude & Torralba, 2001).

Syntactic parsing of natural language sentences
is a central task in natural language processing (NLP)
because of its importance in mediating between lin-
guistic expression and meaning. Our RNN architec-
ture jointly learns how to parse and how to represent
phrases in a continuous vector space of features. This
allows us to embed both single lexical units and un-
seen, variable-sized phrases in a syntactically coher-
ent order. The learned feature representations cap-
ture syntactic and compositional-semantic informa-
tion. We show that they can help inform accurate
parsing decisions and capture interesting similarities
between phrases and sentences.

Using NLP techniques in computer vision. The
connection between NLP ideas such as parsing or
grammars and computer vision has been explored be-
fore (Zhu & Mumford, 2006; Tighe & Lazebnik, 2010;
Zhu et al., 2010; Siskind et al., 2007), among many
others. Our approach is similar on a high level, how-
ever, more general in nature. We show that the same
neural network based architecture can be used for both
natural language and image parsing.

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

images which are useful

Deep Learning in vision applications
can
nd lower dimensional representations for xed size
input
for classication
(Hinton & Salakhutdinov, 2006). Recently, Lee et al.
(2009) were able to scale up deep networks to more
realistic image sizes. Using images of single objects
which were all in roughly the same scale, they were
able to learn parts and classify the images into ob-
ject categories. Our approach diers in several funda-
mental ways to any previous deep learning algorithm.
(i) Instead of learning features from raw, or whitened
pixels, we use o-the-shelf vision features of segments
obtained from oversegmented full scene images.
(ii)
Instead of building a hierarchy using a combination of
convolutional and max-pooling layers, we recursively
apply the same network to merged segments and give
each of these a semantic category label. (iii) This is
the rst deep learning work which learns full scene seg-
mentation, annotation and classication. The objects
and scenes vary in scale, viewpoint, lighting etc.

Using deep learning for NLP applications has been
investigated by several people (inter alia Bengio et al.,
2003; Henderson, 2003; Collobert & Weston, 2008). In
most cases, the inputs to the neural networks are mod-
ied to be of equal size either via convolutional and
max-pooling layers or looking only at a xed size win-
dow around a specic word. Our approach is dierent
in that it handles variable sized sentences in a nat-
ural way and captures the recursive nature of natu-
ral language. Furthermore, it jointly learns parsing
decisions, categories for each phrase and phrase fea-
ture embeddings which capture the semantics of their
constituents. In (Socher et al., 2010) we developed an
NLP specic parsing algorithm based on RNNs. That
algorithm is a special case of the one developed in this
paper.

3. Mapping Segments and Words into

Syntactico-Semantic Space

This section contains an explanation of the inputs used
to describe scene images and natural language sen-
tences and how they are mapped into the space in
which the RNN operates.

3.1. Input Representation of Scene Images
We
closely follow the procedure described in
(Gould et al., 2009)
to compute image features.
First, we oversegment an image x into superpix-
els (also called segments) using the algorithm from
(Comaniciu & Meer, 2002).
Instead of computing
multiple oversegmentations, we only choose one set
of parameters. In our dataset, this results in an av-
erage of 78 segments per image. We compute 119

features for the segments as described in Sec. 3.1 of
(Gould et al., 2009). These features include color and
texture features (Shotton et al., 2006), boosted pixel
classier scores (trained on the labeled training data),
as well as appearance and shape features.

Next, we use a simple neural network layer to map
these features into the semantic n-dimensional space
in which the RNN operates. Let Fi be the features
described above for each segment i = 1; : : : ; Nsegs in
an image. We then compute the representation:

ai = f (W semFi + bsem);

(1)
where W sem 2 Rn(cid:2)119 is the matrix of parameters
we want to learn, bsem is the bias and f is applied
element-wise and can be any sigmoid-like function. In
our vision experiments, we use the original sigmoid
function f (x) = 1=(1 + e

(cid:0)x).

3.2. Input Representation for Natural

Language Sentences

to eciently use neural networks

In order
in
NLP, neural
language models (Bengio et al., 2003;
Collobert & Weston, 2008) map words to a vector rep-
resentation. These representations are stored in a
word embedding matrix L 2 Rn(cid:2)jV j
, where jV j is the
size of the vocabulary and n is the dimensionality of
the semantic space. This matrix usually captures co-
occurrence statistics and its values are learned. As-
sume we are given an ordered list of Nwords words
from a sentence x. Each word i = 1; : : : ; Nwords has
an associated vocabulary index k into the columns of
the embedding matrix. The operation to retrieve the
i-th words semantic representation can be seen as a
simple projection layer where we use a binary vector ek
which is zero in all positions except at the k-th index,

ai = Lek 2 Rn:

(2)

As with the image segments, the inputs are now
mapped to the semantic space of the RNN.

4. Recursive Neural Networks for

Structure Prediction

In our discriminative parsing architecture, the goal is
to learn a function f : X ! Y, where Y is the set of all
possible binary parse trees. An input x consists of two
parts: (i) A set of activation vectors fa1; : : : ; aNsegs
g,
which represent input elements such as image segments
or words of a sentence.
(ii) A symmetric adjacency
matrix A, where A(i; j) = 1, if segment i neighbors
j. This matrix denes which elements can be merged.
For sentences, this matrix has a special form with 1s
only on the rst diagonal below and above the main
diagonal.

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

following functions:

f(cid:18)(x) = arg max
^y2T (x)

s(RNN((cid:18); x; y));

(4)

where (cid:18) are all the parameters needed to compute a
score s with an RNN. The score of a tree y is high if
the algorithm is condent that the structure of the tree
is correct. Tree scoring with RNNs will be explained
in detail below. In the max-margin estimation frame-
work (Taskar et al., 2004; Ratli et al., 2007), we want
to ensure that the highest scoring tree is in the set

of correct trees: f(cid:18)(xi) 2 Y (xi; li) for all training in-
stances (xi; li), i = 1; : : : ; n. Furthermore, we want the
score of the highest scoring correct tree yi to be larger
up to a margin dened by the loss . 8i; y 2 T (xi):
s(RNN((cid:18); xi; yi)) (cid:21) s(RNN((cid:18); xi; y)) + (xi; li; y):

These desiderata lead us to the following regularized
risk function:

N

1
N

J((cid:18)) =

ri((cid:18)) +

i=1
ri((cid:18)) = max
^y2T (xi)
max

(cid:0)

yi2Y (xi;li)

(

(

(cid:21)
2

jj(cid:18)jj2; where
)

s(RNN((cid:18); xi; yi))

s(RNN((cid:18); xi; y)) + (xi; li; y)

(5)

)

Minimizing this objective maximizes the correct trees
score and minimizes (up to a margin) the score of the
highest scoring but incorrect tree.

Now that we dened the general learning framework,
we will explain in detail how we predict parse trees
and compute their scores with RNNs.

4.2. Greedy Structure Predicting RNNs

tivation vectors fa1; : : : ; aNsegs

We can now describe the RNN model that uses the ac-
g and adjacency matrix
A (as dened above) as inputs. There are more than
exponentially many possible parse trees and no e-
cient dynamic programming algorithms for our RNN
setting. Therefore, we nd a greedy approximation.
We start by explaining the feed-forward process on a
test input.

Using the adjacency matrix A, the algorithm nds the
pairs of neighboring segments and adds their activa-
tions to a set of potential child node pairs:

C = f[ai; aj] : A(i,j)=1g:

(6)

In the small toy image of Fig. 2, we would have the fol-

lowing pairs: f[a1; a2]; [a1; a3]; [a2; a1]; [a2; a4]; [a3; a1];
[a3; a4]; [a4; a2]; [a4; a3]; [a4; a5]; [a5; a4]g. Each pair of

activations is concatenated and given as input to a

Figure 2. Illustration of the RNN training inputs: An ad-
jacency matrix of image segments or words. A training
image (red and blue are dierently labeled regions) denes
a set of correct trees which is oblivious to the order in
which segments with the same label are merged. See text
for details.

We denote the set of all possible trees that can be
constructed from an input x as T (x). When training
the visual parser, we have labels l for all segments.
Using these labels, we can dene an equivalence set of
correct trees Y (x; l). A visual tree is correct if all adja-
cent segments that belong to the same class are merged
into one super segment before merges occur with super
segments of dierent classes. This equivalence class
over trees is oblivious to how object parts are inter-
nally merged or how complete, neighboring objects are
merged into the full scene image. For training the lan-
guage parser, the set of correct trees only has one el-
ement, the annotated ground truth tree: Y (x) = fyg.
Fig. 2 illustrates this.

4.1. Max-Margin Estimation

Similar to (Taskar et al., 2004), we dene a structured
margin loss (x; l; y) for proposing a parse y for in-
put x with labels l. The loss increases when a seg-
ment merges with another one of a dierent label be-
fore merging with all its neighbors of the same label.
We can formulate this by checking whether the sub-
tree subT ree(d) underneath a nonterminal node d in
y appears in any of the ground truth trees of Y (x; l):

(x; l; y) = (cid:20)

d2N (^y)

1fsubT ree(d) =2 Y (x; l)g;

(3)



where N (y) is the set of non-terminal nodes and (cid:20)
is a parameter. The loss of the language parser
is the sum over incorrect spans in the tree, see
(Manning & Schutze, 1999).

Given the training set, we search for a function f with
small expected loss on unseen inputs. We consider the

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

s = W scorep
(9)
p = f (W [c1; c2] + b)

Figure 3. One recursive neural network which is replicated
for each pair of possible input vectors. This network is
dierent to the original RNN formulation in that it predicts
a score for being a correct merging decision.

neural network. The network computes the potential
parent representation for these possible child nodes:

p(i;j) = f (W [ci; cj] + b):

(7)

With this representation we can compute a local
score using a simple inner product with a row vector
W score 2 R1(cid:2)n:

s(i;j) = W scorep(i;j):

(8)

The network performing these functions is illustrated
in Fig. 3. Training will aim to increase scores of
good segment pairs (with the same label) and decrease
scores of pairs with dierent labels, unless no more
good pairs are left.

After computing the scores for all pairs of neighboring
segments, the algorithm selects the pair which received
the highest score. Let the score sij be the highest
score; we then (i) Remove [ai; aj] from C, as well as
all other pairs with either ai or aj in them. (ii) Update
the adjacency matrix with a new row and column that
reects that the new segment has the neighbors of both
child segments. (iii) Add potential new child pairs to
C:

C = C (cid:0) f[ai; aj]g (cid:0) f[aj; ai]g
C = C [ f[p(i;j); ak] : ak has boundary with i or jg
In the case of the image in Fig. 2, if we merge [a4; a5],
then C = f[a1; a2]; [a1; a3]; [a2; a1]; [a2; p(4;5)]; [a3; a1];
[a3; p(4;5)]; [p(4;5); a2]; [p(4;5); a3]g.

(10)

The new potential parents and corresponding scores of
new child pairs are computed with the same neural net-
work of Eq. 7. For instance, we compute, p(2;(4;5)) =
f (W [a2; p(4;5)]+b); p(3;(4;5)) = f (W [a3; p(4;5)]+b); etc.

The process repeats (treating the new pi;j just like any
other segment) until all pairs are merged and only one
parent activation is left in the set C. This activation
then represents the entire image. Hence, the same
network (with parameters W; b; W score) is recursively
applied until all vector pairs are collapsed. The tree
is then recovered by unfolding the collapsed decisions
down to the original segments which are the leaf nodes

of the tree. The nal score that we need for structure
prediction is simply the sum of all the local decisions:



s(RNN((cid:18); xi; y)) =

d2N (^y)

sd:

(11)

To nish the example, assume the next highest score
was s((4;5);3), so we merge the (4; 5) super segment
with segment 3, so C = f[a1; a2]; [a1; p((4;5);3)]; [a2; a1];
[a2; p((4;5);3)]; [p((4;5);3); a1]; [p((4;5);3); a2]g. If we then
merge segments (1; 2), we get C = f[p(1;2); p((4;5);3)];
[p((4;5);3); p(1;2)]g, leaving us with only the last choice

of merging the dierently labeled super segments. This
results in the bottom tree in Fig. 2.

4.3. Category Classiers in the Tree

One of the main advantages of our approach is that
each node of the tree built by the RNN has associated
with it a distributed feature representation (the par-
ent vector p). We can leverage this representation by
adding to each RNN parent node (after removing the
scoring layer) a simple softmax layer to predict class
labels, such as visual or syntactic categories:

labelp = sof tmax(W labelp):

(12)

When minimizing the cross-entropy error of this soft-
max layer, the error will backpropagate and inuence
both the RNN parameters and the word representa-
tions.

4.4. Improvements for Language Parsing

Since in a sentence each word only has 2 neighbors,
less-greedy search algorithms such as a bottom-up
beam search can be used.
In our case, beam search
lls in elements of the chart in a similar fashion as the
CKY algorithm. However, unlike standard CNF gram-
mars, in our grammar each constituent is represented
by a continuous feature vector and not just a discrete
category. Hence we cannot prune based on category
equality. We could keep the k-best subtrees in each
cell but initial tests showed no improvement over just
keeping the single best constituent in each cell.

Since there is only a single correct tree the second max-
imization in the objective of Eq. 5 can be dropped. For
further details see (Socher et al., 2010).

5. Learning
Our objective J of Eq. 5 is not dierentiable due to
the hinge loss. Therefore, we will generalize gradi-
ent descent via the subgradient method (Ratli et al.,
2007) which computes a gradient-like direction called
the subgradient. Let (cid:18) = (W sem; W; W score; W label)
be the set of our model parameters,1 then the gradi-

1In the case of natural language parsing, W semis re-

placed by the look-up table L.

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

ent becomes:

@J
@(cid:18)

=

1
n



i

@s(yi)

@(cid:18)

(cid:0) @s(yi)
@(cid:18)

+ (cid:21)(cid:18);

(13)

Table 1. Pixel level multi-class segmentation accuracy of
other methods and our proposed RNN architecture on the
Stanford background dataset. TL(2010) methods are re-
ported in (Tighe & Lazebnik, 2010).

where s(yi) = s(RNN((cid:18); xi; ymax(T (xi)))) and s(yi) =
s(RNN((cid:18); xi; ymax(Y (xi;li)))).
In order to compute
Eq. 13 we calculate the derivative by using backprop-
agation through structure (Goller & Kuchler, 1996), a
simple modication to general backpropagation where
error messages are split at each node and then propa-
gated to the children.

We use L-BFGS over the complete training data to
minimize the objective. Generally, this could cause
problems due to the non-dierentiable objective func-
tion. However, we did not observe problems in prac-
tice.

6. Experiments
We evaluate our RNN architecture on both vision and
NLP tasks. The only parameters to tune are n, the
size of the hidden layer; (cid:20), the penalization term for in-
correct parsing decisions and (cid:21), the regularization pa-
rameter. We found that our method is robust to these
parameters, varying in performance by only a few per-
cent for some parameter combinations. With proper
regularization, training accuracy was highly correlated
with test performance. We chose n = 100, (cid:20) = 0:05
and (cid:21) = 0:001.

6.1. Scene Understanding: Segmentation and

Annotation

The vision experiments are performed on the Stan-
ford background dataset2. We rst provide accuracy
of multiclass segmentation where each pixel is labeled
with a semantic class. Like (Gould et al., 2009), we
run ve-fold cross validation and report pixel level ac-
curacy in Table 1. After training the full RNN model
which inuences the leaf embeddings through back-
propagation, we can simply label the superpixels by
their most likely class based on the multinomial dis-
tribution from the softmax layer at the leaf nodes. As
shown in Table 1, we outperform previous methods
that report results on this data, such as the recent
methods of (Tighe & Lazebnik, 2010). We report ac-
curacy of an additional logistic regression baseline to
show the improvement using the neural network layer
instead of the raw vision features. We also tried us-
ing just a single neural network layer followed by a
softmax layer. This corresponds to the leaf nodes of
the RNN and performed about 2% worse than the full
RNN model.

2The dataset is available at

http://dags.stanford.edu/projects/scenedataset.html

Method and Semantic Pixel Accuracy in

%

Pixel CRF, Gould et al.(2009)
Log. Regr. on Superpixel Features
Region-based energy, Gould et al.(2009)
Local Labeling,TL(2010)
Superpixel MRF,TL(2010)
Simultaneous MRF,TL(2010)

RNN (our method)

74.3
75.9
76.4
76.9
77.5
77.5

78.1

Figure 4. Results of multi-class image segmentation and
pixel-wise labeling with recursive neural networks. Best
viewed in color.

On a 2.6GHz laptop our Matlab implementation needs
16 seconds to parse 143 test images. We show seg-
mented and labeled scenes in Fig. 4.

6.2. Scene Classication

The Stanford background dataset can be roughly cat-
egorized into three scene types: city, countryside and
sea-side. We label the images with these three la-
bels and train a linear SVM using the average over all
nodes activations in the tree as features. Hence, we
use the entire parse tree and the learned feature repre-

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

Center Phrase and Nearest Neighbors

All the gures are adjusted for seasonal variations
1. All the numbers are adjusted for seasonal uctuations
2. All the gures are adjusted to remove usual seasonal
patterns
3. All Nasdaq industry indexes nished lower , with -
nancial issues hit the hardest

Knight-Ridder would nt comment on the oer
1. Harsco declined to say what country placed the order
2. Coastal would nt disclose the terms
3. Censorship is nt a Marxist invention

Sales grew almost 7% to $UNK m. from $UNK m.
1. Sales rose more than 7% to $94.9 m. from $88.3 m.
2. Sales surged 40% to UNK b. yen from UNK b.
3. Revenues declined 1% to $4.17 b. from$ 4.19 b.

Fujisawa gained 50 to UNK
1. Mead gained 1 to 37 UNK
2. Ogden gained 1 UNK to 32
3. Kellogg surged 4 UNK to 7

The dollar dropped
1. The dollar retreated
2. The dollar gained
3. Bond prices rallied

Figure 6. Nearest neighbors phrase trees. The learned fea-
ture representations of higher level nodes capture interest-
ing syntactic and semantic similarities between the phrases.
(b.=billion, m.=million)

6.4. Supervised Parsing

In all experiments our word and phrase representations
are 100-dimensional. We train all models on the Wall
Street Journal section of the Penn Treebank using the
standard training (221), development (22) and test
(23) splits.

The nal unlabeled bracketing F-measure
(see
(Manning & Schutze, 1999) for details) of our lan-
guage parser is 90.29%, compared to 91.63% for the
widely used Berkeley parser (Petrov et al., 2006) (de-
velopment F1 is virtually identical with 92.06% for the
RNN and 92.08% for the Berkeley parser). Unlike
most previous systems, our parser does not provide
a parent with information about the syntactic cate-
gories of its children. This shows that our learned,
continuous representations capture enough syntactic
information to make good parsing decisions.

While our parser does not yet perform as well as the
current version of the Berkeley parser, it performs re-
spectably (1.3% dierence in unlabeled F1). On a
2.6GHz laptop our Matlab implementation needs 72
seconds to parse 421 sentences of length less than 15.

6.5. Nearest Neighbor Phrases

In the same way we collected nearest neighbors for
nodes in the scene tree, we can compute nearest neigh-
bor embeddings of multi-word phrases. We embed
complete sentences from the WSJ dataset into the

Figure 5. Nearest neighbor image region trees (of the rst
region in each row): The learned feature representations of
higher level nodes capture interesting visual and semantic
properties of the merged segments below them.

sentations of the RNN. With an accuracy of 88.1%, we
outperform the state-of-the art features for scene cate-
gorization, Gist descriptors (Aude & Torralba, 2001),
which obtain only 84.0%. We also compute a baseline
using our RNN. In the baseline we use as features only
the very top node of the scene parse tree. We note that
while this captures enough information to perform well
above a random baseline (71.0% vs. 33.3%), it does
lose some information that is captured by averaging
all tree nodes.

6.3. Nearest Neighbor Scene Subtrees

In order to show that the learned feature representa-
tions capture important appearance and label infor-
mation even for higher nodes in the tree, we visualize
nearest neighbor super segments. We parse all test
images with the trained RNN. We then nd subtrees
whose nodes have all been assigned the same class la-
bel by our algorithm and save the top nodes vector
representation of that subtree. This also includes ini-
tial superpixels. Using this representation, we com-
pute nearest neighbors across all images and all such
subtrees (ignoring their labels). Fig. 5 shows the re-
sults. The rst image is a random subtrees top node
and the remaining regions are the closest subtrees in
the dataset in terms of Euclidean distance between the
vector representations.

Parsing Natural Scenes and Natural Language with Recursive Neural Networks

syntactico-semantic feature space. In Fig. 6 we show
several example sentences which had similar sentences
in the dataset. Our examples show that the learned
features capture several interesting semantic and syn-
tactic similarities between sentences or phrases.

Hinton, G. E. and Salakhutdinov, R. R. Reducing the
dimensionality of data with neural networks. Science,
313, 2006.

Hoiem, D., Efros, A.A., and Hebert, M. Putting Objects

in Perspective. CVPR, 2006.

7. Conclusion

We have introduced a recursive neural network ar-
chitecture which can successfully merge image seg-
ments or natural language words based on deep learned
semantic transformations of their original features.
Our method outperforms state-of-the-art approaches
in segmentation, annotation and scene classication.

Acknowledgements

We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine
Reading Program under Air Force Research Laboratory
(AFRL) prime contract no. FA8750-09-C-0181 and the
DARPA Deep Learning program under contract number
FA8650-10-C-7020. Any opinions, ndings, and conclusion
or recommendations expressed in this material are those
of the author(s) and do not necessarily reect the view of
DARPA, AFRL, or the US government. We would like to
thank Tianshi Gao for helping us with the feature com-
putation, as well as Jiquan Ngiam and Quoc Le for many
helpful comments.

