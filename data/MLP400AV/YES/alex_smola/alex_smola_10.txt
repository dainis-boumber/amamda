Abstract

Suppose you are given some dataset drawn from an underlying probabil-
ity distribution P and you want to estimate a simple subset S of input
space such that the probability that a test point drawn from P lies outside
of S equals some a priori specied (cid:0) between  and .
We propose a method to approach this problem by trying to estimate a
function f which is positive on S and negative on the complement. The
functional form of f is given by a kernel expansion in terms of a poten-
tially small subset of the training data; it is regularized by controlling the
length of the weight vector in an associated feature space. We provide a
theoretical analysis of the statistical performance of our algorithm.
The algorithm is a natural extension of the support vector algorithm to
the case of unlabelled data.

1 INTRODUCTION

During recent years, a new set of kernel techniques for supervised learning has been devel-
oped [8]. Specically, support vector (SV) algorithms for pattern recognition, regression
estimation and solution of inverse problems have received considerable attention. There
have been a few attempts to transfer the idea of using kernels to compute inner products
in feature spaces to the domain of unsupervised learning. The problems in that domain
are, however, less precisely specied. Generally, they can be characterized as estimating
functions of the data which tell you something interesting about the underlying distribu-
tions. For instance, kernel PCA can be characterized as computing functions which on the
training data produce unit variance outputs while having minimum norm in feature space
[4]. Another kernel-based unsupervised learning technique, regularized principal mani-
folds [6], computes functions which give a mapping onto a lower-dimensional manifold
minimizing a regularized quantization error. Clustering algorithms are further examples of
unsupervised learning techniques which can be kernelized [4].

An extreme point of view is that unsupervised learning is about estimating densities.
Clearly, knowledge of the density of P would then allow us to solve whatever problem
can be solved on the basis of the data. The present work addresses an easier problem: it

proposes an algorithm which computes a binary function which is supposed to capture re-
gions in input space where the probability density lives (its support), i.e. a function such
that most of the data will live in the region where the function is nonzero [5]. In doing so,
it is in line with Vapniks principle never to solve a problem which is more general than the
one we actually need to solve. Moreover, it is applicable also in cases where the density
of the datas distribution is not even well-dened, e.g. if there are singular components.
Part of the motivation for the present work was the paper [1]. It turns out that there is a
considerable amount of prior work in the statistical literature; for a discussion, cf. the full
version of the present paper [3].

2 ALGORITHMS

We rst introduce terminology and notation conventions. We consider training data
x(cid:1) (cid:2) (cid:2) (cid:2) (cid:1) x(cid:1)  X(cid:1) where (cid:3)  N is the number of observations, and X is some set. For
simplicity, we think of it as a compact subset of R N . Let (cid:2) be a feature map X (cid:1) F ,
i.e. a map into a dot product space F such that the dot product in the image of (cid:2) can be
computed by evaluating some simple kernel [8]

k(cid:3)x(cid:1) y(cid:4) (cid:5) (cid:3)(cid:2)(cid:3)x(cid:4) (cid:2) (cid:2)(cid:3)y(cid:4)(cid:4)(cid:1)

(1)

such as the Gaussian kernel

(2)
Indices i and j are understood to range over (cid:1) (cid:2) (cid:2) (cid:2) (cid:1) (cid:3) (in compact notation: i(cid:1) j  (cid:6)(cid:3)(cid:7)).
Bold face greek letters denote (cid:3)-dimensional vectors whose components are labelled using
normal face typeset.

k(cid:3)x(cid:1) y(cid:4) (cid:5) e(cid:1)kx(cid:1)yk(cid:2)c(cid:2)

In the remainder of this section, we shall develop an algorithm which returns a function
f that takes the value (cid:8) in a small region capturing most of the data points, and (cid:3)
elsewhere. Our strategy is to map the data into the feature space corresponding to the
kernel, and to separate them from the origin with maximum margin. For a new point x, the
value f (cid:3)x(cid:4) is determined by evaluating which side of the hyperplane it falls on, in feature
space. Via the freedom to utilize different types of kernel functions, this simple geometric
picture corresponds to a variety of nonlinear estimators in input space.

To separate the data set from the origin, we solve the following quadratic program:

(3)

min

wF(cid:3)(cid:0)R(cid:0)(cid:3)(cid:4)R



kwk (cid:8) 

(cid:5)(cid:1)Pi (cid:4)i (cid:3) (cid:5)

subject to (cid:3)w (cid:2) (cid:2)(cid:3)xi(cid:4)(cid:4) (cid:4) (cid:5) (cid:3) (cid:4)i(cid:1) (cid:4)i (cid:4) (cid:2)

(4)
Here, (cid:0)  (cid:3)(cid:1) (cid:4) is a parameter whose meaning will become clear later. Since nonzero slack
variables (cid:4)i are penalized in the objective function, we can expect that if w and (cid:5) solve this
problem, then the decision function f (cid:3)x(cid:4) (cid:5) sgn(cid:3)(cid:3)w (cid:2) (cid:2)(cid:3)x(cid:4)(cid:4) (cid:3) (cid:5)(cid:4) will be positive for most
examples xi contained in the training set, while the SV type regularization term kwk will
still be small. The actual trade-off between these two goals is controlled by (cid:0). Deriving the
dual problem, and using (1), the solution can be shown to have an SV expansion

f (cid:3)x(cid:4) (cid:5) sgn(cid:0)Xi

(cid:6)ik(cid:3)xi(cid:1) x(cid:4) (cid:3) (cid:5)(cid:1)

(5)

(patterns xi with nonzero (cid:6)i are called SVs), where the coefcients are found as the solu-
tion of the dual problem:

min
(cid:1)



Xij

(cid:6)i(cid:6)jk(cid:3)xi(cid:1) xj (cid:4) subject to  (cid:5) (cid:6)i (cid:5)


(cid:0)(cid:3)

(cid:1) Xi

(cid:6)i (cid:5) (cid:2)

(6)

This problem can be solved with standard QP routines. It does, however, possess features
that sets it apart from generic QPs, most notably the simplicity of the constraints. This can
be exploited by applying a variant of SMO developed for this purpose [3].
The offset (cid:5) can be recovered by exploiting that for any (cid:6) i which is not at the upper or

lower bound, the corresponding pattern x i satises (cid:5) (cid:5) (cid:3)w (cid:2) (cid:2)(cid:3)xi(cid:4)(cid:4) (cid:5)Pj (cid:6)jk(cid:3)xj (cid:1) xi(cid:4).

Note that if (cid:0) approaches , the upper boundaries on the Lagrange multipliers tend to inn-
ity, i.e. the second inequality constraint in (6) becomes void. The problem then resembles
the corresponding hard margin algorithm, since the penalization of errors becomes innite,
as can be seen from the primal objective function (3). It can be shown that if the data set
is separable from the origin, then this algorithm will nd the unique supporting hyperplane
with the properties that it separates all data from the origin, and its distance to the origin is
maximal among all such hyperplanes [3]. If, on the other hand, (cid:0) approaches 1, then the
constraints alone only allow one solution, that where all (cid:6) i are at the upper bound (cid:7)(cid:3)(cid:0)(cid:3)(cid:4).
In this case, for kernels with integral , such as normalized versions of (2), the decision
function corresponds to a thresholded Parzen windows estimator.

To conclude this section, we note that one can also use balls to describe the data in feature
space, close in spirit to the algorithms of [2], with hard boundaries, and [7], with soft
margins. For certain classes of kernels, such as Gaussian RBF ones, the corresponding
algorithm can be shown to be equivalent to the above one [3].

3 THEORY

In this section, we show that the parameter (cid:0) characterizes the fractions of SVs and outliers
(Proposition 1). Following that, we state a robustness result for the soft margin (Proposition
2) and error bounds (Theorem 5). Further results and proofs are reported in the full version
of the present paper [3]. We will use italic letters to denote the feature space images of the
corresponding patterns in input space, i.e. x i (cid:10)(cid:5) (cid:2)(cid:3)xi(cid:4)(cid:2)
Proposition 1 Assume the solution of (4) satises (cid:5) (cid:5) . The following statements hold:
(i) (cid:0) is an upper bound on the fraction of outliers.
(ii) (cid:0) is a lower bound on the fraction of SVs.
(iii) Suppose the data were generated independently from a distribution P (cid:3)x(cid:4) which does
not contain discrete components. Suppose, moreover, that the kernel is analytic and non-
constant. With probability 1, asymptotically, (cid:0) equals both the fraction of SVs and the
fraction of outliers.

The proof is based on the constraints of the dual problem, using the fact that outliers must
have Lagrange multipliers at the upper bound.

Proposition 2 Local movements of outliers parallel to w do not change the hyperplane.

We now move on to the subject of generalization. Our goal is to bound the probability
that a novel point drawn from the same underlying distribution lies outside of the estimated
region by a certain margin. We start by introducing a common tool for measuring the
capacity of a class F of functions that map X to R.

Denition 3 Let (cid:3)X(cid:1) d(cid:4) be a pseudo-metric space,1 let A be a subset of X and (cid:8) (cid:9) . A
set B (cid:7) X is an (cid:8)-cover for A if, for every a  A, there exists b  B such that d(cid:3)a(cid:1) b(cid:4) (cid:5) (cid:8).
The (cid:8)-covering number of A, N d(cid:3)(cid:8)(cid:1) A(cid:4), is the minimal cardinality of an (cid:8)-cover for A (if
there is no such nite cover then it is dened to be ).

1i.e. with a distance function that differs from a metric in that it is only semidenite

The idea is that B should be nite but approximate all of A with respect to the pseudometric
d. We will use the l distance over a nite sample X (cid:5) (cid:3)x(cid:1) (cid:2) (cid:2) (cid:2) (cid:1) x(cid:1)(cid:4) for the pseudo-
metric in the space of functions, dX (cid:3)f(cid:1) g(cid:4) (cid:5) maxi(cid:2)(cid:1)(cid:3) jf (cid:3)xi(cid:4) (cid:3) g(cid:3)xi(cid:4)j(cid:2) Let N(cid:3)(cid:8)(cid:1) F(cid:1) (cid:3)(cid:4) (cid:5)
supXX(cid:0) NdX (cid:3)(cid:8)(cid:1) F(cid:4). Below, logarithms are to base 2.

Theorem 4 Consider any distribution P on X and any (cid:10)  R. Suppose x (cid:1) (cid:2) (cid:2) (cid:2) (cid:1) x(cid:1) are
generated i.i.d.
from P . Then with probability  (cid:3) (cid:11) over such an (cid:3)-sample, if we nd
f  F such that f (cid:3)xi(cid:4) (cid:4) (cid:10) (cid:8) (cid:12) for all i  (cid:6)(cid:3)(cid:7),

Pfx (cid:10) f (cid:3)x(cid:4) (cid:13) (cid:10) (cid:3) (cid:12)g (cid:5) 

(cid:1) (cid:3)k (cid:8) log (cid:1)

(cid:6) (cid:4)(cid:1)

where k (cid:5) dlog N(cid:3)(cid:12)(cid:1) F(cid:1) (cid:3)(cid:4)e.
We now consider the possibility that for a small number of points f (cid:3)x i(cid:4) fails to exceed
(cid:10) (cid:8) (cid:12). This corresponds to having a non-zero slack variable (cid:4) i in the algorithm, where we
take (cid:10) (cid:8) (cid:12) (cid:5) (cid:5)(cid:7)kwk and use the class of linear functions in feature space in the application
of the theorem. There are well-known bounds for the log covering numbers of this class.
Let f be a real valued function on a space X. Fix (cid:10)  R. For x  X, dene

d(cid:3)x(cid:1) f(cid:1) (cid:12)(cid:4) (cid:5) maxf(cid:1) (cid:10) (cid:8) (cid:12) (cid:3) f (cid:3)x(cid:4)g(cid:2)

Similarly for a training sequence X, we dene D(cid:3)X(cid:1) f(cid:1) (cid:12)(cid:4) (cid:5) PxX d(cid:3)x(cid:1) f(cid:1) (cid:12)(cid:4)(cid:2)
Theorem 5 Fix (cid:10)  R. Consider a xed but unknown probability distribution P on the
input space X and a class of real valued functions F with range (cid:6)a(cid:1) b(cid:7). Then with probability
 (cid:3) (cid:11) over randomly drawn training sequences x of size (cid:3), for all (cid:12) (cid:9)  and any f  F,

(cid:1) (cid:3)k (cid:8) log (cid:1)

(cid:6) (cid:4)(cid:1)

P fx(cid:10) f (cid:3)x(cid:4) (cid:13) (cid:10) (cid:3) (cid:12) and x  Xg (cid:5) 
log(cid:2)

(cid:7) 

(cid:7) 

e(cid:1)(cid:7)

(cid:3)m (cid:2)

D(cid:6)X(cid:3)f(cid:3)(cid:7)(cid:7)(cid:3) log(cid:2) (cid:1)(cid:6)b(cid:1)a(cid:7)

where k (cid:5)llog N(cid:3)(cid:12)(cid:7)(cid:1) F(cid:1) (cid:3)(cid:4) (cid:8) (cid:6)b(cid:1)a(cid:7)D(cid:6)X(cid:3)f(cid:3)(cid:7)(cid:7)
The theorem bounds the probability of a new point falling in the region for which f (cid:3)x(cid:4)
has value less than (cid:10) (cid:3) (cid:12), this being the complement of the estimate for the support of the
distribution. The choice of (cid:12) gives a trade-off between the size of the region over which the
bound holds (increasing (cid:12) increases the size of the region) and the size of the probability
with which it holds (increasing (cid:12) decreases the size of the log covering numbers).
The result shows that we can bound the probability of points falling outside the region of
estimated support by a quantity involving the ratio of the log covering numbers (which can
be bounded by the fat shattering dimension at scale proportional to (cid:12)) and the number of
training examples, plus a factor involving the 1-norm of the slack variables. It is stronger
than related results given by [1], since their bound involves the square root of the ratio of
the Pollard dimension (the fat shattering dimension when (cid:12) tends to 0) and the number of
training examples.

The output of the algorithm described in Sec. 2 is a function f (cid:3)x(cid:4) (cid:5) Pi (cid:6)ik(cid:3)xi(cid:1) x(cid:4) which
is greater than or equal to (cid:5) (cid:3) (cid:4)i on example xi. Though non-linear in the input space, this
function is in fact linear in the feature space dened by the kernel k. At the same time the
2-norm of the weight vector is given by B (cid:5) p(cid:6)T K(cid:6), and so we can apply the theorem
with the function class F being those linear functions in the feature space with 2-norm
bounded by B. If we assume that (cid:10) is xed, then (cid:12) (cid:5) (cid:5) (cid:3) (cid:10), hence the support of the
distribution is the set fx (cid:10) f (cid:3)x(cid:4) (cid:4) (cid:10) (cid:3) (cid:12) (cid:5) (cid:10) (cid:3) (cid:5)g, and the bound gives the probability of
a randomly generated point falling outside this set, in terms of the log covering numbers of
the function class F and the sum of the slack variables (cid:4) i. Since the log covering numbers

at scale (cid:12)(cid:7) of the class F can be bounded by O(cid:3) RB 
(cid:7) 
of the 2-norm of the weight vector.

log (cid:3)(cid:4) this gives a bound in terms

Ideally, one would like to allow (cid:10) to be chosen after the value of (cid:5) has been determined,
perhaps as a xed fraction of that value. This could be obtained by another level of struc-
tural risk minimisation over the possible values of (cid:5) or at least a mesh of some possible
values. This result is beyond the scope of the current preliminary paper, but the form of the
result would be similar to Theorem 5, with larger constants and log factors.

Whilst it is premature to give specic theoretical recommendations for practical use yet,
one thing is clear from the above bound. To generalize to novel data, the decision function
to be used should employ a threshold (cid:14) (cid:2) (cid:5), where (cid:14) (cid:13)  (this corresponds to a nonzero (cid:12)).

4 EXPERIMENTS

We apply the method to articial and real-world data. Figure 1 displays 2-D toy examples,
and shows how the parameter settings inuence the solution.

Next, we describe an experiment on the USPS dataset of handwritten digits. The database
contains 		 digit images of size  (cid:9)  (cid:5) ; the last  constitute the test set. We
trained the algorithm, using a Gaussian kernel (2) of width c (cid:5) (cid:2) (cid:2)  (a common value
for SVM classiers on that data set, cf. [2]), on the test set and used it to identify outliers
 it is folklore in the community that the USPS test set contains a number of patterns
which are hard or impossible to classify, due to segmentation errors or mislabelling. In the
experiment, we augmented the input patterns by ten extra dimensions corresponding to the
class labels of the digits. The rationale for this is that if we disregarded the labels, there
would be no hope to identify mislabelled patterns as outliers. Fig. 2 shows the 20 worst
outliers for the USPS test set. Note that the algorithm indeed extracts patterns which are
very hard to assign to their respective classes. In the experiment, which took  seconds on
a Pentium II running at  MHz, we used a (cid:0) value of (cid:18).

0.5, 0.5
0.59, 0.47

0.70

0.1, 0.5
0.24, 0.03

0.5, 0.1
0.65, 0.38

0.48

(cid:0), width c

0.5, 0.5
0.54, 0.43

0.62

0.84

frac. SVs/OLs
margin (cid:5)(cid:7)kwk
Figure 1: First two pictures: A single-class SVM applied to two toy problems; (cid:0) (cid:5) c (cid:5) (cid:2),
domain: (cid:6)(cid:3)(cid:1) (cid:7). Note how in both cases, at least a fraction of (cid:0) of all examples is in the
estimated region (cf. table). The large value of (cid:0) causes the additional data points in the
upper left corner to have almost no inuence on the decision function. For smaller values of
(cid:0), such as (cid:2) (third picture), the points cannot be ignored anymore. Alternatively, one can
force the algorithm to take these outliers into account by changing the kernel width (2):
in the fourth picture, using c (cid:5) (cid:2)(cid:1) (cid:0) (cid:5) (cid:2), the data is effectively analyzed on a different
length scale which leads the algorithm to consider the outliers as meaningful points.

(cid:31)513

9

(cid:31)507

1

(cid:31)458

0

(cid:31)377

1

(cid:31)282

7

(cid:31)216

2

(cid:31)200

3

(cid:31)186

9

(cid:31)179

5

(cid:31)162

0

(cid:31)153

3

(cid:31)143

6

(cid:31)128

6

(cid:31)123

0

(cid:31)117

7

(cid:31)93

5

(cid:31)78

0

(cid:31)58

7

(cid:31)52

6

(cid:31)48

3

Figure 2: Outliers identied by the proposed algorithm, ranked by the negative output of
the SVM (the argument of the sgn in the decision function). The outputs (for convenience
in units of (cid:1)) are written underneath each image in italics, the (alleged) class labels are
given in bold face. Note that most of the examples are difcult in that they are either
atypical or even mislabelled.

5 DISCUSSION

One could view the present work as an attempt to provide an algorithm which is in line
with Vapniks principle never to solve a problem which is more general than the one that
one is actually interested in. E.g., in situations where one is only interested in detecting
novelty, it is not always necessary to estimate a full density model of the data. Indeed,
density estimation is more difcult than what we are doing, in several respects.

Mathematically speaking, a density will only exist if the underlying probability measure
possesses an absolutely continuous distribution function. The general problem of estimat-
ing the measure for a large class of sets, say the sets measureable in Borels sense, is not
solvable (for a discussion, see e.g. [8]). Therefore we need to restrict ourselves to making
a statement about the measure of some sets. Given a small class of sets, the simplest esti-
mator accomplishing this task is the empirical measure, which simply looks at how many
training points fall into the region of interest. Our algorithm does the opposite. It starts
with the number of training points that are supposed to fall into the region, and then esti-
mates a region with the desired property. Often, there will be many such regions  the
solution becomes unique only by applying a regularizer, which in our case enforces that
the region be small in a feature space associated to the kernel. This, of course, implies, that
the measure of smallness in this sense depends on the kernel used, in a way that is no dif-
ferent to any other method that regularizes in a feature space. A similar problem, however,
appears in density estimation already when done in input space. Let p denote a density on
X. If we perform a (nonlinear) coordinate transformation in the input domain X, then the
density values will change; loosely speaking, what remains constant is p(cid:3)x(cid:4) (cid:2) dx, while dx
is transformed, too. When directly estimating the probability measure of regions, we are
not faced with this problem, as the regions automatically change accordingly.

An attractive property of the measure of smallness that we chose to use is that it can also
be placed in the context of regularization theory, leading to an interpretation of the solution
as maximally smooth in a sense which depends on the specic kernel used [3].

The main inspiration for our approach stems from the earliest work of Vapnik and collab-
orators. They proposed an algorithm for characterizing a set of unlabelled data points by
separating it from the origin using a hyperplane [9]. However, they quickly moved on to
two-class classication problems, both in terms of algorithms and in the theoretical devel-
opment of statistical learning theory which originated in those days. From an algorithmic
point of view, we can identify two shortcomings of the original approach which may have
caused research in this direction to stop for more than three decades. Firstly, the original

algorithm in was limited to linear decision rules in input space, secondly, there was no way
of dealing with outliers. In conjunction, these restrictions are indeed severe  a generic
dataset need not be separable from the origin by a hyperplane in input space. The two mod-
ications that we have incorporated dispose of these shortcomings. Firstly, the kernel trick
allows for a much larger class of functions by nonlinearly mapping into a high-dimensional
feature space, and thereby increases the chances of separability from the origin. In partic-
ular, using a Gaussian kernel (2), such a separation exists for any data set x (cid:1) (cid:2) (cid:2) (cid:2) (cid:1) x(cid:1): to
see this, note that k(cid:3)xi(cid:1) xj(cid:4) (cid:9)  for all i(cid:1) j, thus all dot products are positive, implying that
all mapped patterns lie inside the same orthant. Moreover, since k(cid:3)x i(cid:1) xi(cid:4) (cid:5)  for all i,
they have unit length. Hence they are separable from the origin. The second modication
allows for the possibility of outliers. We have incorporated this softness of the decision
rule using the (cid:0)-trick and thus obtained a direct handle on the fraction of outliers.
We believe that our approach, proposing a concrete algorithm with well-behaved compu-
tational complexity (convex quadratic programming) for a problem that so far has mainly
been studied from a theoretical point of view has abundant practical applications. To turn
the algorithm into an easy-to-use black-box method for practicioners, questions like the
selection of kernel parameters (such as the width of a Gaussian kernel) have to be tackled.
It is our expectation that the theoretical results which we have briey outlined in this paper
will provide a foundation for this formidable task.

Acknowledgement. Part of this work was supported by the ARC and the DFG (# Ja
379/9-1), and done while BS was at the Australian National University and GMD FIRST.
AS is supported by a grant of the Deutsche Forschungsgemeinschaft (Sm 62/1-1). Thanks
to S. Ben-David, C. Bishop, C. Schnorr, and M. Tipping for helpful discussions.

