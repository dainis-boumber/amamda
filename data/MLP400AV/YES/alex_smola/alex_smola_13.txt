1 Introduction

Principal component analysis (PCA) is a powerful technique for extracting
structure from possibly high-dimensional data sets. It is readily performed
by solving an eigenvalue problem or using iterative algorithms that estimate
principal components (for reviews of the existing literature, see Jolliffe, 1986,
and Diamantaras & Kung, 1996). PCA is an orthogonal transformation of
the coordinate system in which we describe our data. The new coordinate
values by which we represent the data are called principal components. It is
often the case that a small number of principal components is sufcient to
account for most of the structure in the data. These are sometimes called
factors or latent variables of the data.

We are interested not in principal components in input space but in prin-
cipal components of variables, or features, which are nonlinearly related to
the input variables. Among these are variables obtained by taking arbitrary
higher-order correlations between input variables. In the case of image anal-
ysis, this amounts to nding principal components in the space of products
of input pixels.

To this end, we are computing dot products in feature space by means of
kernel functions in input space. Given any algorithm that can be expressed
solely in terms of dot products (i.e., without explicit usage of the variables
themselves), this kernel method enables us to construct different nonlinear
c(cid:176) 1998 Massachusetts Institute of Technology

Neural Computation 10, 12991319 (1998)

1300

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

versions of it (Aizerman, Braverman, & Rozonoer, 1964; Boser, Guyon, &
Vapnik, 1992). Although this general fact was known (Burges, private com-
munication), the machine learning community has made little use of it, the
exception being support vector machines (Vapnik, 1995). In this article, we
give an example of applying this method in the domain of unsupervised
learning, to obtain a nonlinear form of PCA.

In the next section, we review the standard PCA algorithm. In order to
be able to generalize it to the nonlinear case, we formulate it in a way that
uses exclusively dot products. In section 3, we discuss the kernel method
for computing dot products in feature spaces. Together, these two sections
form the basis for section 4, which presents the proposed kernel-based algo-
rithm for nonlinear PCA. First experimental results on kernel-based feature
extraction for pattern recognition are given in section 5. We conclude with a
discussion (section 6) and an appendix containing some technical material
that is not essential for the main thread of the argument.

2 PCA in Feature Spaces
Given a set of centered observations xk, k = 1, . . . , M, xk  RN,
PCA diagonalizes the covariance matrix,1

M(cid:88)

j=1

C = 1
M

(cid:62)
xjx
j

.

(cid:80)

M

k=1 xk = 0,

(2.1)

To do this, one has to solve the eigenvalue equation,

(cid:80)

v = Cv,

(2.2)
for eigenvalues   0 and v  RN\{0}. As Cv = 1
(xjv)xj, all solutions
v with  (cid:54)= 0 must lie in the span of x1, . . . , xM; hence, equation 2.2 in that
case is equivalent to

M
j=1

M

(xk  v) = (xk  Cv) for all k = 1, . . . , M.

(2.3)

In the remainder of this section, we describe the same computation in an-
other dot product space F, which is related to the input space by a possibly
nonlinear map,

 : RN  F, x (cid:55) X.

(2.4)

1 More precisely, the covariance matrix is dened as the expectation of xx

; for conve-
nience, we shall use the same term to refer to the estimate in equation 2.1 of the covariance
matrix from a nite sample.

(cid:62)

Nonlinear Component Analysis

1301

Note that F, which we will refer to as the feature space, could have an
arbitrarily large, possibly innite, dimensionality. Here and in the following,
uppercase characters are used for elements of F, and lowercase characters
denote elements of RN.
M
k=1
(xk) = 0 (we shall return to this point later). Using the covariance matrix
in F,

Again, we assume that we are dealing with centered data, that is

(cid:80)



(xj)(xj)(cid:62)

(2.5)

M(cid:88)

j=1

C = 1
M

(if F is innite dimensional, we think of (xj)(xj)(cid:62)
as the linear operator
that maps X  F to (xj)((xj)  X)) we now have to nd eigenvalues   0
and eigenvectors V  F\{0} satisfying,

V = CV.

(2.6)
Again, all solutions V with  (cid:54)= 0 lie in the span of (x1), . . . , (xM). For
us, this has two useful consequences. First, we may instead consider the set
of equations,

((xk)  V) = ((xk)  CV) for all k = 1, . . . , M,

and, second, there exist coefcients i (i = 1, . . . , M) such that,

(2.7)

(2.8)

M(cid:88)

i=1

i((xk)  M(cid:88)

j=1

Combining equations 2.7 and 2.8, we get

i(xi).

V = M(cid:88)
M(cid:88)

i=1



i=1

i((xk)  (xi)) = 1
M

for all k = 1, . . . , M.
Dening an M  M matrix K by

Kij := ((xi)  (xj)),

this reads

MK = K2,

(xj))((xj)  (xi))

(2.9)

(2.10)

(2.11)

1302

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

where  denotes the column vector with entries 1, . . . , M. To nd solutions
of equation 2.11, we solve the eigenvalue problem,

M = K,

(2.12)

for nonzero eigenvalues. A justication of this procedure is given in ap-
pendix A.
Let 1  2    M denote the eigenvalues of K (i.e., the solutions
M of equation 2.12), and 1, . . . , M the corresponding complete set of
eigenvectors, with p being the rst nonzero eigenvalue (assuming  (cid:54) 0).
We normalize p, . . . , M by requiring that the corresponding vectors in F
be normalized, that is,

(Vk  Vk) = 1 for all k = p, . . . , M.

(2.13)

By virtue of equations 2.8 and 2.12, this translates into a normalization
condition for p, . . . , M:

1 = M(cid:88)

i,j=1

((xi)  (xj)) = M(cid:88)

k
i

k
j

= (k  Kk) = k(k  k).

i,j=1

k
i

k
j Kij

(2.14)

For the purpose of principal component extraction, we need to compute
projections onto the eigenvectors Vk in F (k = p, . . . , M). Let x be a test
point, with an image (x) in F; then

((xi)  (x))

k
i

(2.15)

(Vk  (x)) = M(cid:88)

i=1

may be called its nonlinear principal components corresponding to .

In summary, the following steps were necessary to compute the principal
components: (1) compute the matrix K, (2) compute its eigenvectors and
normalize them in F, and (3) compute projections of a test point onto the
eigenvectors.2

For the sake of simplicity, we have made the assumption that the obser-
vations are centered. This is easy to achieve in input space but harder in F,
because we cannot explicitly compute the mean of the (xi) in F. There is,
however, a way to do it, and this leads to slightly modied equations for
kernel-based PCA (see appendix B).

2 Note that in our derivation we could have used the known result (e.g., Kirby &
Sirovich, 1990) that PCA can be carried out on the dot product matrix (xi  xj)ij instead of
equation 2.1; however, for the sake of clarity and extendability (in appendix B, we shall
consider the question how to center the data in F), we gave a detailed derivation.

Nonlinear Component Analysis

1303

Before we proceed to the next section, which more closely investigates
the role of the map , the following observation is essential:  can be an
arbitrary nonlinear map into the possibly high-dimensional space F, for ex-
ample, the space of all dth order monomials in the entries of an input vector.
In that case, we need to compute dot products of input vectors mapped by ,
at a possibly prohibitive computational cost. The solution to this problem,
described in the following section, builds on the fact that we exclusively
need to compute dot products between mapped patterns (in equations 2.10
and 2.15); we never need the mapped patterns explicitly.

3 Computing Dot Products in Feature Spaces
In order to compute dot products of the form ((x)  (y)), we use kernel
representations,

k(x, y) = ((x)  (y)),

(3.1)

which allow us to compute the value of the dot product in F without having
to carry out the map . This method was used by Boser et al. (1992) to extend
the Generalized Portrait hyperplane classier of Vapnik and Chervonenkis
(1974) to nonlinear support vector machines. To this end, they substitute a
priori chosen kernel functions k for all occurrences of dot products, obtaining
decision functions

(cid:33)

f (x) = sgn

ik(x, xi) + b

.

(3.2)

(cid:195)
(cid:96)(cid:88)

i=1

Aizerman et al. (1964) call F the linearization space, and use it in the context
of the potential function classication method to express the dot product
between elements of F in terms of elements of the input space. If F is high-
dimensional, we would like to be able to nd a closed-form expression for k
that can be efciently computed. Aizerman et al. (1964) consider the possi-
bility of choosing k a priori, without being directly concerned with the cor-
responding mapping  into F. A specic choice of k might then correspond
to a dot product between patterns mapped with a suitable . A particularly
useful example, which is a direct generalization of a result proved by Poggio
(1975, lemma 2.1) in the context of polynomial approximation, is

d

xj  yj

 N(cid:88)
N(cid:88)

j=1

j1,...,jd=1

(x  y)d =

=

 . . .  xjd

 yj1

 . . .  yjd

= (Cd(x)  Cd(y)),

xj1

(3.3)

1304

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

where Cd maps x to the vector Cd(x) whose entries are all possible dth degree
ordered products of the entries of x. For instance (Vapnik, 1995), if x =
(x1, x2), then C2(x) = (x2
, x1x2, x2x1), or, yielding the same value of the
dot product,

2 x1x2).

2(x) = (x2

(3.4)

, x2
2

1

, x2
2

,

1

For this example, it is easy to verify that (x  y)2 = (x2

2 y1y2)(cid:62) = (2(x)  2(y)). In general, the function

1

, x2
2

,


2 x1x2)(y2
1

, y2
2

,

k(x, y) = (x  y)d

(3.5)

corresponds to a dot product in the space of dth-order monomials of the
input coordinates. If x represents an image with the entries being pixel
values, we can thus easily work in the space spanned by products of any
d pixelsprovided that we are able to do our work solely in terms of dot
products, without any explicit use of a mapped pattern d(x). The latter lives
in a possibly very high-dimensional space: even though we will identify
terms like x1x2 and x2x1 into one coordinate of F, as in equation 3.4, the
d!(N1)! and thus grows like Nd. For instance, 16
dimensionality of F still is (N+d1)!
16 pixel input images and a polynomial degree d = 5 yield a dimensionality
of 1010. Thus, using kernels of the form in equation 3.5 is our only way to
take into account higher-order statistics without a combinatorial explosion
of time and memory complexity.

The general question that function k does correspond to a dot product in
some space F has been discussed by Boser et al. (1992) and Vapnik (1995):
Mercers theorem of functional analysis implies that if k is a continuous ker-
nel of a positive integral operator, there exists a mapping into a space where
k acts as a dot product (for details, see appendix C). Besides equation 3.5,
radial basis functions,

(cid:181)

(cid:182)

,

k(x, y) = exp

(cid:107)x  y(cid:107)2
2  2

and sigmoid kernels,

k(x, y) = tanh((x  y) + ),

(3.6)

(3.7)

have been used in support vector machines. These different kernels allow
the construction of polynomial classiers, radial basis function classiers,
and neural networks with the support vector algorithm, which exhibit very
similar accuracy. In addition, they all construct their decision functions from
an almost identical subset of a small number of training patterns, the support
vectors (Scholkopf, Burges, & Vapnik, 1995).

Nonlinear Component Analysis

1305

The application of equation 3.1 to our problem is straightforward. We
simply substitute an a priori chosen kernel function k(x, y) for all occur-
rences of ((x)(y)). The choice of k then implicitly determines the mapping
 and the feature space F.

4 Kernel PCA

4.1 The Algorithm. To perform kernel-based PCA (see Figure 1), hence-
forth referred to as kernel PCA, the following steps have to be carried out.
First, we compute the matrix Kij = (k(xi, xj))ij. Next, we solve equation 2.12
by diagonalizing K and normalize the eigenvector expansion coefcients n
by requiring n(n  n) = 1. To extract the principal components (corre-
sponding to the kernel k) of a test point x, we then compute projections onto
the eigenvectors by (cf. equation 2.15 and Figure 2),

(Vn  (x)) = M(cid:88)

i=1

n
i k(xi, x).

(4.1)

If we use a kernel as described in section 3, we know that this procedure
exactly corresponds to standard PCA in some high-dimensional feature
space, except that we do not need to perform expensive computations in that
space. In practice, our algorithm is not equivalent to the form of nonlinear
PCA that can be obtained by explicitly mapping into the feature space F.
Even though the rank of the matrix K is always limited by the sample size, we
may not be able to compute this matrix if the dimensionality is prohibitively
high. In that case, using kernels is imperative.

4.2 Properties of (Kernel) PCA. If we use a kernel that satises the con-
ditions given in section 3, we know that we are in fact doing a standard
PCA in F. Consequently, all mathematical and statistical properties of PCA
(see, e.g., Jolliffe, 1986; Diamantaras & Kung, 1996) carry over to kernel-
based PCA, with the modications that they become statements concerning
F rather than RN. In F, we can thus assert that PCA is the orthogonal basis
transformation with the following properties (assuming that the eigenvec-
tors are sorted in descending order of the eigenvalue size): (1) the rst q
(q  {1, . . . , M}) principal components, that is, projections on eigenvectors,
carry more variance than any other q orthogonal directions, (2) the mean-
squared approximation error in representing the observations by the rst
q principal components is minimal, (3) the principal components are un-
correlated, and (4) the rst q principal components have maximal mutual
information with respect to the inputs (this holds under gaussian assump-
tions, and thus depends on the data and the chosen kernel).

We conclude this section by noting one general property of kernel PCA
in input space: for kernels that depend on only dot products or distances

1306

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

linear PCA

k(x,y) = (x.y)
R2

x

x

x

x

x

x

x
x
x

x

x
x

kernel PCA

R2

x
x
x

x

x
x

k

x

x

x

x

x

x

e.g. k(x,y) = (x.y)d

x
x

x

x

x

x

x

x
x

x

x
x

F

Figure 1: The basic idea of kernel PCA. In some high-dimensional feature space
F (bottom right), we are performing linear PCA, just like a PCA in input space
(top). Since F is nonlinearly related to input space (via ), the contour lines of
constant projections onto the principal eigenvector (drawn as an arrow) become
nonlinear in input space. Note that we cannot draw a preimage of the eigenvector
in input space, because it may not even exist. Crucial to kernel PCA is the fact
that there is no need to carry out the map into F. All necessary computations
are carried out by the use of a kernel function k in input space (here: R2).

in input space (as all the examples that we have given so far do), kernel
PCA has the property of unitary invariance, following directly from the
fact that both the eigenvalue problem and the feature extraction depend on
only kernel values. This ensures that the features extracted do not depend
on which orthonormal coordinate system we use for representing our input
data.

4.3 Computational Complexity. A fth-order polynomial kernel on a
256-dimensional input space yields a 1010-dimensional feature space. For
two reasons, kernel PCA can deal with this huge dimensionality. First, we
do not need to look for eigenvectors in the full space F, but just in the sub-
space spanned by the images of our observations xk in F. Second, we do not

F
Nonlinear Component Analysis

1307

a 1
k

a 2
k

a 3
k

a 4
k

i k (xi,x)

feature value
(V.F
(x)) = S a
weights (eigenvector
coefficients)
comparison: k(xi,x)

sample x1, x2, x3,...

input vector x

Figure 2: Feature extraction architecture in kernel PCA (cf. equation 4.1). In the
rst layer, the input vector is compared to the sample via a kernel function,
chosen a priori (e.g., polynomial, gaussian, or sigmoid). The outputs are then
linearly combined using weights, which are found by solving an eigenvector
problem.

need to compute dot products explicitly between vectors in F (which can
be impossible in practice, even if the vectors live in a lower-dimensional
subspace) because we are using kernel functions. Kernel PCA thus is com-
putationally comparable to a linear PCA on (cid:96) observations with an (cid:96)  (cid:96)
dot product matrix. If k is easy to compute, as for polynomial kernels, for
example, the computational complexity is hardly changed by the fact that
we need to evaluate kernel functions rather than just dot products. Further-
more, when we need to use a large number (cid:96) of observations, we may want
to work with an algorithm for computing only the largest eigenvalues, as,
for instance, the power method with deation (for a discussion, see Dia-
mantaras & Kung, 1996). In addition, we can consider using an estimate of
the matrix K, computed from a subset of M < (cid:96) examples, while still extract-
ing principal components from all (cid:96) examples (this approach was chosen in
some of our experiments described below).

The situation can be different for principal component extraction. There,
we have to evaluate the kernel function M times for each extracted principal
component (see equation 4.1), rather than just evaluating one dot product
as for a linear PCA. Of course, if the dimensionality of F is 1010, this is still
vastly faster than linear principal component extraction in F. Still, in some
cases (e.g., if we were to extract principal components as a preprocessing
step for classication), we might want to speed things up. This can be done
(cid:80)(cid:96)
by a technique proposed by Burges (1996) in the context of support vector
machines. In the present setting, we approximate each eigenvector V =
j(zj), where

i(xi) (see equation 2.8) by another vector V =(cid:80)

i=1

m
j=1

S
1308
Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller
m < (cid:96) is chosen a priori according to the desired speedup, and zj  RN, j =
1, . . . , m. This is done by minimizing the squared difference  = (cid:107)V  V(cid:107)2.
The crucial point is that this also can be done without explicitly dealing with
the possibly high-dimensional space F. As

 = (cid:107)V(cid:107)2 + m(cid:88)

i,j=1

(cid:96)(cid:88)

m(cid:88)

ijk(zi, zj)  2

ijk(xi, zj),

i=1

j=1

(4.2)

the gradient of  with respect to the j and the zj is readily expressed in
terms of the kernel function; thus,  can be minimized by gradient descent.
Finally, although kernel principal component extraction is computation-
ally more expensive than its linear counterpart, this additional investment
can pay back afterward. In experiments on classication based on the ex-
tracted principal components, we found that when we trained on nonlinear
features, it was sufcient to use a linear support vector machine to con-
struct the decision boundary. Linear support vector machines, however, are
much faster in classication speed than nonlinear ones. This is due to the
fact that for k(x, y) = (x  y), the support vector decision function (see equa-
ixi as
f (x) = sgn((x  w) + b). Thus the nal stage of classication can be done
extremely fast.

tion 3.2) can be expressed with a single weight vector w = (cid:80)(cid:96)

i=1

4.4 Interpretability and Variable Selection. In PCA, it is sometimes de-
sirable to be able to select specic axes that span the subspace into which
one projects in doing principal component extraction. In this way, it may, for
instance, be possible to choose variables that are more accessible to interpre-
tation. In the nonlinear case, there is an additional problem: some directions
in F do not have preimages in input space. To make this plausible, note that
the linear span of the training examples mapped into feature space can have
dimensionality up to M (the number of examples). If this exceeds the di-
mensionality of input space, it is rather unlikely that each vector of the form
in equation 2.8 has a preimage. To get interpretability, we thus need to nd
directions in input space (i.e., input variables) whose images under  span
the PCA subspace in F. This can be done with an approach akin to the one
already described. We could parameterize our set of desired input variables
and run the minimization of equation 4.2 only over those parameters. The
parameters can be, for example, group parameters, which determine the
amount of translation, say, starting from a set of images.

4.5 Dimensionality Reduction, Feature Extraction, and Reconstruction.
Unlike linear PCA, the proposed method allows the extraction of a number
of principal components that can exceed the input dimensionality. Suppose
that the number of observations M exceeds the input dimensionality N. Lin-
ear PCA, even when it is based on the M M dot product matrix, can nd at

Nonlinear Component Analysis

1309

most N nonzero eigenvalues; they are identical to the nonzero eigenvalues
of the N  N covariance matrix. In contrast, kernel PCA can nd up to M
nonzero eigenvaluesa fact that illustrates that it is impossible to perform
kernel PCA directly on an N  N covariance matrix. Even more features
could be extracted by using several kernels.
Being just a basis transformation, standard PCA allows the reconstruction
of the original patterns xi, i = 1, . . . , (cid:96), from a complete set of extracted
principal components (xi  vj), j = 1, . . . , (cid:96), by expansion in the eigenvector
basis. Even from an incomplete set of components, good reconstruction
is often possible. In kernel PCA, this is more difcult. We can reconstruct
the image of a pattern in F from its nonlinear components; however, if we
have only an approximate reconstruction, there is no guarantee that we can
nd an exact preimage of the reconstruction in input space. In that case,
we would have to resort to an approximation method (cf. equation 4.2).
Alternatively, we could use a suitable regression method for estimating the
reconstruction mapping from the kernel-based principal components to the
inputs.

5 Experiments

5.1 Toy Examples. To provide some insight into how PCA in F be-
haves in input space, we show a set of experiments with an articial two-
dimensional data set, using polynomial kernels (cf. equation 3.5) of degree 1
through 4 (see Figure 3). Linear PCA (on the left) leads to only two nonzero
eigenvalues, as the input dimensionality is 2. In contrast, nonlinear PCA al-
lows the extraction of further components. In the gure, note that nonlinear
PCA produces contour lines (of constant feature value), which reect the
structure in the data better than in linear PCA. In all cases, the rst principal
component varies monotonically along the parabola underlying the data.
In the nonlinear cases, the second and the third components show behav-
ior that is similar for different polynomial degrees. The third component,
which comes with small eigenvalues (rescaled to sum to 1), seems to pick
up the variance caused by the noise, as can be nicely seen in the case of
degree 2. Dropping this component would thus amount to noise reduction.
Further toy examples, using radial basis function kernels (see equation 3.6)
and neural networktype sigmoid kernels (see equation 3.7), are shown in
Figures 4 and 5.

5.2 Character Recognition. In this experiment, we extracted nonlinear
principal components from a handwritten character database, using ker-
nel PCA in the form given in appendix B. We chose the US Postal Service
(USPS) database of handwritten digits collected from mail envelopes in Buf-
falo. This database contains 9298 examples of dimensionality 256; 2007 of
them make up the test set. For computational reasons, we decided to use
a subset of 3000 training examples for the matrix K. To assess the utility of

1310

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

Eigenvalue=0.709

Eigenvalue=0.621

Eigenvalue=0.570

Eigenvalue=0.552

1
1
1
1
0.5
0.5
0.5
0.5
0
0
0
0
0.5
0.5
0.5
0.5
1
1
1
1
1
1
1
1
Eigenvalue=0.418
Eigenvalue=0.395
Eigenvalue=0.345
Eigenvalue=0.291

0

0

0

0

1
1
1
1
0.5
0.5
0.5
0.5
0
0
0
0
0.5
0.5
0.5
0.5
1
1
1
1
1
1
1
1
Eigenvalue=0.021
Eigenvalue=0.026
Eigenvalue=0.034
Eigenvalue=0.000

0

0

0

0

1
0.5
0
0.5
1

1
0.5
0
0.5
1
1

0

1
0.5
0
0.5
1
1

0

1
0.5
0
0.5
1
1

0

0

1

i

Figure 3: Two-dimensional toy example, with data generated in the following
way: x values have uniform distribution in [1, 1], y values are generated from
+ , where  is normal noise with standard deviation 0.2. From left to
yi = x2
right, the polynomial degree in the kernel (see equation 3.5) increases from 1 to 4;
from top to bottom, the rst three eigenvectors are shown in order of decreasing
eigenvalue size. The gures contain lines of constant principal component value
(contour lines); in the linear case, these are orthogonal to the eigenvectors. We
did not draw the eigenvectors; as in the general case, they live in a higher-
dimensional feature space.

the components, we trained a soft margin hyperplane classier (Vapnik &
Chervonenkis, 1974; Cortes & Vapnik, 1995) on the classication task. This
is a special case of support vector machines, using the standard dot prod-
uct as a kernel function. It simply tries to separate the training data by a
hyperplane with large margin.

Table 1 illustrates two advantages of using nonlinear kernels. First, per-
formance of a linear classier trained on nonlinear principal components is
better than for the same number of linear components; second, the perfor-
mance for nonlinear components can be further improved by using more
components than is possible in the linear case. The latter is related to the
fact that there are many more higher-order features than there are pixels
in an image. Regarding the rst point, note that extracting a certain num-
ber of features in a 1010-dimensional space constitutes a much higher re-
duction of dimensionality than extracting the same number of features in
256-dimensional input space.

Nonlinear Component Analysis

1311

Figure 4: Two-dimensional toy example with three data clusters (gaussians with
standard deviation 0.1, depicted region: [1, 1] [0.5, 1]): rst eight nonlinear
principal components extracted with k(x, y) = exp( (cid:107)xy(cid:107)2
). Note that the rst
two principal components (top left) nicely separate the three clusters. Compo-
nents 35 split up the clusters into halves. Similarly, components 68 split them
again, in a way orthogonal to the above splits. Thus, the rst eight components
divide the data into 12 regions. The Matlab code used for generating this gure
can be obtained from http://svm.rst.gmd.de.

0.1

Figure 5: Two-dimensional toy example with three data clusters (gaussians with
standard deviation 0.1, depicted region: [1, 1] [0.5, 1]): rst three nonlinear
principal components extracted with k(x, y) = tanh
. The rst two
principal components (top left) are sufcient to separate the three clusters, and
the third component splits the clusters into halves.

(cid:161)
2(x  y) + 1

(cid:162)

For all numbers of features, the optimal degree of kernels to use is around
4, which is compatible with support vector machine results on the same data
set (Scholkopf, Burges, & Vapnik, 1995). Moreover, with only one exception,
the nonlinear features are superior to their linear counterparts. The resulting
error rate for the best of our classiers (4.0%) is competitive with convolu-
tional ve-layer neural networks (5.0% were reported by LeCun et al., 1989)
and nonlinear support vector classiers (4.0%, Scholkopf, Burges, & Vapnik,
1995); it is much better than linear classiers operating directly on the image
data (a linear support vector machine achieves 8.9%; Scholkopf, Burges, &
Vapnik, 1995). These encouraging results have been reproduced on an object
recognition task (Scholkopf, Smola, & M uller, 1996).

1312

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

Table 1: Test Error Rates on the USPS Handwritten Digit Database.

Number of components

32
64
128
256
512
1024
2048

Test Error Rate for Degree

1

9.6
8.8
8.6
8.7
N.A.
N.A.
N.A.

2

8.8
7.3
5.8
5.5
4.9
4.9
4.9

3

8.1
6.8
5.9
5.3
4.6
4.3
4.2

4

8.5
6.7
6.1
5.2
4.4
4.4
4.1

5

9.1
6.7
5.8
5.2
5.1
4.6
4.0

6

9.3
7.2
6.0
5.4
4.6
4.8
4.3

7

10.8
7.5
6.8
5.4
4.9
4.6
4.4

Note: Linear support vector machines were trained on nonlinear principal com-
ponents extracted by PCA with kernel (3.5), for degrees 1 through 7. In the case
of degree 1, we are doing standard PCA, with the number of nonzero eigenval-
ues being at most the dimensionality of the space, 256. Clearly, nonlinear principal
components afford test error rates that are superior to the linear case (degree 1).

6 Discussion

6.1 Feature Extraction for Classication. This article presented a new
technique for nonlinear PCA. To develop this technique, we made use of
a kernel method so far used only in supervised learning (Vapnik, 1995).
Kernel PCA constitutes a rst step toward exploiting this technique for a
large class of algorithms.

In experiments comparing the utility of kernel PCA features for pattern
recognition using a linear classier, we found two advantages of nonlin-
ear kernels. First, nonlinear principal components afforded better recog-
nition rates than corresponding numbers of linear principal components;
and, second, the performance for nonlinear components can be improved
by using more components than is possible in the linear case. We have not
yet compared kernel PCA to other techniques for nonlinear feature extrac-
tion and dimensionality reduction. We can, however, compare results with
other feature extraction methods used in the past by researchers working
on the USPS classication problem. Our system of kernel PCA feature ex-
traction plus linear support vector machine, for instance, performed better
than LeNet1 (LeCun et al., 1989). Although the latter result was obtained
a number of years ago, LeNet1 nevertheless provides an architecture that
contains a great deal of prior information about the handwritten character
classication problem. It uses shared weights to improve transformation
invariance and a hierarchy of feature detectors resembling parts of the hu-
man visual system. In addition, our features were extracted without taking
into account that we want to do classication. Clearly, in supervised learn-
ing, where we are given a set of labeled observations (x1, y1), . . . , (x(cid:96), y(cid:96)), it

Nonlinear Component Analysis

1313

would seem advisable to make use of the labels not only during the training
of the nal classier but also in the stage of feature extraction.

Finally, we note that a similar approach can be taken in the case of re-

gression estimation.

6.2 Feature Space and the Curse of Dimensionality. We are doing PCA
in 1010-dimensional feature spaces, yet getting results in nite time that
are comparable to state-of-the-art techniques. In fact, however, we are not
working in the full feature space, but in a comparably small linear subspace
of it, whose dimension equals at most the number of observations. The
method automatically chooses this subspace and provides a means of tak-
ing advantage of the lower dimensionality. An approach that consisted in
explicitly of mapping into feature space and then performing PCA would
have severe difculties at this point. Even if PCA was done based on an
M  M dot product matrix (M being the sample size), whose diagonaliza-
tion is tractable, it would still be necessary to evaluate dot products in a
1010-dimensional feature space to compute the entries of the matrix in the
rst place. Kernel-based methods avoid this problem; they do not explicitly
compute all dimensions of F (loosely speaking, all possible features), but
work only in a relevant subspace of F.

6.3 Comparison to Other Methods for Nonlinear PCA. Starting from
some of the properties characterizing PCA (see above), it is possible to de-
velop a number of possible generalizations of linear PCA to the nonlinear
case. Alternatively, one may choose an iterative algorithm that adaptively
estimates principal components and make some of its parts nonlinear to
extract nonlinear features.

Rather than giving a full review of this eld here, we briey describe
ve approaches and refer readers to Diamantaras and Kung (1996) for more
details.

6.3.1 Hebbian Networks.

Initiated by the pioneering work of Oja (1982),
a number of unsupervised neural network algorithms computing principal
components have been proposed. Compared to the standard approach of
diagonalizing the covariance matrix, they have advantagesfor instance,
when the data are nonstationary. Nonlinear variants of these algorithms
are obtained by adding nonlinear activation functions. The algorithms then
extract features that the authors have referred to as nonlinear principal
components. These approaches, however, do not have the geometrical in-
terpretation of kernel PCA as a standard PCA in a feature space nonlinearly
related to input space, and it is thus more difcult to understand what ex-
actly they are extracting.

6.3.2 Autoassociative Multilayer Perceptrons. Consider a linear three-
layer perceptron with a hidden layer smaller than the input. If we train

1314

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

it to reproduce the input values as outputs (i.e., use it in autoassociative
mode), then the hidden unit activations form a lower-dimensional repre-
sentation of the data, closely related to PCA (see, for instance, Diamantaras
& Kung, 1996). To generalize to a nonlinear setting, one uses nonlinear acti-
vation functions and additional layers.3 While this can be considered a form
of nonlinear PCA, the resulting network training consists of solving a hard
nonlinear optimization problem, with the possibility of getting trapped in
local minima, and thus with a dependence of the outcome on the starting
point of the training. Moreover, in neural network implementations, there is
often a risk of getting overtting. Another drawback of neural approaches
to nonlinear PCA is that the number of components to be extracted has to
be specied in advance. As an aside, note that hyperbolic tangent kernels
can be used to extract neural networktype nonlinear features using kernel
PCA (see Figure 5). The principal components of a test point x in that case
take the form (see Figure 2)

i tanh  ((xi, x) + ).

(cid:80)

n

i

6.3.3 Principal Curves. An approach with a clear geometric interpreta-
tion in input space is the method of principal curves (Hastie & Stuetzle,
1989), which iteratively estimates a curve (or surface) capturing the struc-
ture of the data. The data are mapped to the closest point on a curve, and
the algorithm tries to nd a curve with the property that each point on the
curve is the average of all data points projecting onto it. It can be shown
that the only straight lines satisfying the latter are principal components,
so principal curves are indeed a generalization of the latter. To compute
principal curves, a nonlinear optimization problem has to be solved. The
dimensionality of the surface, and thus the number of features to extract, is
specied in advance.

6.3.4 Locally Linear PCA.

In cases where a linear PCA fails because the
dependences in the data vary nonlinearly with the region in input space, it
can be fruitful to use an approach where linear PCA is applied locally (e.g.,
Bregler & Omohundro, 1994). Possibly kernel PCA could be improved by
taking locality into account.

6.3.5 Kernel PCA. Kernel PCA is a nonlinear generalization of PCA in
the sense that it is performing PCA in feature spaces of arbitrarily large
(possibly innite) dimensionality, and if we use the kernel k(x, y) = (x  y),
we recover standard PCA. Compared to the above approaches, kernel PCA
has the main advantage that no nonlinear optimization is involved; it is

3 Simply using nonlinear activation functions in the hidden layer would not sufce.
The linear activation functions already lead to the best approximation of the data (given
the number of hidden nodes), so for the nonlinearities to have an effect on the components,
the architecture needs to be changed to comprise more layers (see, e.g., Diamantaras &
Kung, 1996).

Nonlinear Component Analysis

1315

essentially linear algebra, as simple as standard PCA. In addition, we need
not specify the number of components that we want to extract in advance.
Compared to neural approaches, kernel PCA could be disadvantageous if
we need to process a very large number of observations, because this results
in a large matrix K. Compared to principal curves, kernel PCA is harder to
interpret in input space; however, at least for polynomial kernels, it has a
very clear interpretation in terms of higher-order features.

7 Conclusion

Compared to other techniques for nonlinear feature extraction, kernel PCA
has the advantages that it requires only the solution of an eigenvalue prob-
lem, not nonlinear optimization, and by the possibility of using different
kernels, it comprises a fairly general class of nonlinearities that can be used.
Clearly the last point has yet to be evaluated in practice; however, for the
support vector machine, the utility of different kernels has already been
established. Different kernels (polynomial, sigmoid, gaussian) led to ne
classication performances (Scholkopf, Burges, & Vapnik, 1995). The gen-
eral question of how to select the ideal kernel for a given task (i.e., the
appropriate feature space), however, is an open problem.

The scene has been set for using the kernel method to construct a wide va-
riety of rather general nonlinear variants of classical algorithms. It is beyond
our scope here to explore all the possibilities, including many distance-based
algorithms, in detail. Some of them are currently being investigatedfor
instance, nonlinear forms of k-means clustering and kernel-based indepen-
dent component analysis (Scholkopf, Smola, & M uller, 1996).

Linear PCA is being used in numerous technical and scientic applica-
tions, including noise reduction, density estimation, image indexing and
retrieval systems, and the analysis of natural image statistics. Kernel PCA
can be applied to all domains where traditional PCA has so far been used
for feature extraction and where a nonlinear extension would make sense.

Appendix A: The Eigenvalue Problem in the Space of Expansion
Coefcients

Being symmetric, K has an orthonormal basis of eigenvectors (i)i with cor-
responding eigenvalues i; thus, for all i, we have Ki = ii (i = 1, . . . , M).
eigenvector basis as  =(cid:80)
(cid:80)
To understand the relation between equations 2.11 and 2.12, we proceed as
(cid:80)
follows. First, suppose ,  satisfy equation 2.11. We may expand  in Ks
i aiii =
i . This in turn
means that for all i = 1, . . . , M,

M
i=1 aii. Equation 2.11 then reads M
i i, or, equivalently, for all i = 1, . . . , M, Maii = ai2

i ai2

M = i or ai = 0 or i = 0.

(A.1)

1316

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

Note that the above are not exclusive ors. We next assume that ,  satisfy
equation 2.12, to carry out a similar derivation. In that case, we nd that
i aiii, that is, for all i =
equation 2.12 is equivalent to M
1, . . . , M,

(cid:80)

i aii = (cid:80)

M = i or ai = 0.

(A.2)

Comparing equations A.1 and A.2, we see that all solutions of the latter
satisfy the former. However, they do not give its full set of solutions: given a
solution of equation 2.12, we may always add multiples of eigenvectors of K
with eigenvalue 0 and still satisfy equation 2.11, with the same eigenvalue.
This means that there exist solutions of equation 2.11 that belong to different
eigenvalues yet are not orthogonal in the space of the k. It does not mean,
(cid:80)
however, that the eigenvectors of C in F are not orthogonal. Indeed, if 
((xj) (cid:80)
is an eigenvector of K with eigenvalue 0, then the corresponding vector
i(xi) is orthogonal to all vectors in the span of the (xj) in F, since
i(xi) = 0.
Thus, the above difference between the solutions of equations 2.11 and 2.12
is irrelevant, since we are interested in vectors in F rather than vectors in
the space of the expansion coefcients of equation 2.8. We thus only need
to diagonalize K to nd all relevant solutions of equation 2.11.

i(xi)) = (K)j = 0 for all j, which means that

(cid:80)

i

i

i

Appendix B: Centering in High-Dimensional Space

Given any  and any set of observations x1, . . . , xM, the points

(xi) := (xi)  1
M

(xi)

i=1

(B.1)

are centered. Thus, the assumptions of section 2 now hold, and we go on
to dene covariance matrix and Kij = ( (xi)  (xj)) in F. We arrive at the
already familiar eigenvalue problem,

  = K ,

the points in equation B.1, V =(cid:80)

(B.2)
with  being the expansion coefcients of an eigenvector (in F) in terms of
(xi). Because we do not have the
centered data (see equation B.1), we cannot compute K directly; however,
we can express it in terms of its noncentered counterpart K. In the following,
we shall use Kij = ((xi) (xj)) and the notations 1ij = 1 for all i, j, (1M)ij :=
1/M, to compute Kij = ( (xi)  (xj)):

M
i=1

i

(cid:33)

M(cid:88)

M(cid:88)

M(cid:88)

m=1

(cid:195)

Kij =

((xi)  1
M

(xm))  ((xj)  1
M

(xn))

n=1

(B.3)

Nonlinear Component Analysis

M(cid:88)

M(cid:88)
= Kij  1
M
n=1
= (K  1MK  K1M + 1MK1M)ij.

1imKmj  1
M

m=1

1317

M(cid:88)

m,n=1

1imKmn1nj

Kin1nj + 1
M2

( Vk  (t)) = M(cid:88)

We thus can compute K from K and then solve the eigenvalue problem
(see equation B.2). As in equation 2.14, the solutions k are normalized
by normalizing the corresponding vectors Vk in F, which translates into
k( k  k) = 1. For feature extraction, we compute projections of centered
-images of test patterns t onto the eigenvectors of the covariance matrix
of the centered points,

k
i

( (xi)  (t)).

i=1

(B.4)
Consider a set of test points t1, . . . , tL, and dene two L  M matrices
(cid:80)
= ((ti)  (xj)) and Ktest
(xm))  ((xj) 
M
by Ktest
m=1
(xn))). As in equation B.3, we express Ktest in terms of Ktest, and
ij
M
1
n=1
arrive at Ktest = Ktest1
M is the L M matrix
MKKtest1M+1
M
(cid:48)
(cid:48)
(cid:48)
MK1M, where 1
with all entries equal to 1/M.

= (((ti)  1

(cid:80)

M

ij

Appendix C: Mercer Kernels

Mercers theorem of functional analysis (e.g., Courant & Hilbert, 1953) gives
conditions under which we can construct the mapping  from the eigen-
function decomposition of k. If k is the continuous kernel of an integral
k(x, y) f (x) dx, which is positive, that is,

operator K : L2  L2, (K f )(y) =(cid:82)

(cid:90)

(C.1)

(C.2)

f (x)k(x, y) f (y) dx dy  0 for all

f  L2,

then k can be expanded into a uniformly convergent series,

k(x, y) =

(cid:88)
(cid:112)
 : x (cid:55) (

i=1

with i  0. In this case,
11(x),

(cid:112)

ii(x)i(y),

(C.3)
is a map into F such that k acts as the given dot product, that is, ((x)(y)) =
k(x, y).

22(x), . . .)

Although formulated originally for the case where the integral operator
acts on functions f from L2([a, b]), Mercers theorem also holds if f is dened
on a space of arbitrary dimensionality, provided that it is compact (e.g.,
Dunford & Schwartz, 1963).

1318

Bernhard Scholkopf, Alexander Smola, and Klaus-Robert M uller

Acknowledgments

A. S. and B. S. were supported by grants from the Studienstiftung des deuts-
chen Volkes. B. S. thanks the GMD First for hospitality during two visits.
A. S. and B. S. thank V. Vapnik for introducing them to kernel representations
of dot products during joint work on support vector machines. Thanks to
AT&T and Bell Laboratories for letting us use the USPS database and to
L. Bottou, C. Burges, and C. Cortes for parts of the soft margin hyperplane
training code. This work proted from discussions with V. Blanz, L. Bottou,
C. Burges, H. B ulthoff, P. Haffner, Y. Le Cun, S. Mika, N. Murata, P. Simard,
S. Solla, V. Vapnik, and T. Vetter. We are grateful to V. Blanz, C. Burges, and
S. Solla for reading a preliminary version of the article.

