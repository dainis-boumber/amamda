ABSTRACT
Targeting interest to match a user with services (e.g. news,
products, games, advertisements) and predicting friendship
to build connections among users are two fundamental tasks
for social network systems. In this paper, we show that the
information contained in interest networks (i.e. user-service
interactions) and friendship networks (i.e. user-user connec-
tions) is highly correlated and mutually helpful. We propose
a framework that exploits homophily to establish an inte-
grated network linking a user to interested services and con-
necting dierent users with common interests, upon which
both friendship and interests could be eciently propagated.
The proposed friendship-interest propagation (FIP) frame-
work devises a factor-based random walk model to explain
friendship connections, and simultaneously it uses a coupled
latent factor model to uncover interest interactions. We dis-
cuss the exibility of the framework in the choices of loss ob-
jectives and regularization penalties and benchmark dier-
ent variants on the Yahoo! Pulse social networking system.
Experiments demonstrate that by coupling friendship with
interest, FIP achieves much higher performance on both in-
terest targeting and friendship prediction than systems using
only one source of information.

Categories and Subject Descriptors
H.5.3 [Information systems]: Web-based Interaction; H.3.3
[Information search and retrieval]: Information ltering

General Terms
Algorithms, Performance, Experimentation

Keywords
Social network, link prediction, interest targeting

1.

INTRODUCTION

Online social networking services have brought to the pub-
lic a new style of social lives parallel to our day-to-day oine
activities. Popular social network sites, such as Facebook,
Linkedin and Twitter have already gathered billions of ex-
tensively acting users and are still attracting thousands of

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2011, March 28April 1, 2011, Hyderabad, India.
ACM 978-1-4503-0632-4/11/03.

enthusiastic newbies each day. Doubtlessly, social networks
have become one of todays major platforms for building
friendship and sharing interests.

service item
user
friendship
interest

Figure 1: A social network graph. The connections
consist of both (unipartite) edges within the user-
user friendship network and bipartite user-item in-
teractions in the interest network.

Fundamental to all social network services is the goal to
eectively model the interests of a user and the friendship
between users [21]. On the one hand, by capturing a users
interests and accordingly exploiting the opportunity to serve
her/him with potentially interesting service items (e.g. news,
games, advertisements, products), one can improve the sat-
isfaction of a users participation and boost the revenue of a
social network site as well (e.g. via product purchases, vir-
tual transactions, advertisement clicks). On the other hand,
connecting people with common interests is not only impor-
tant for improving existing users loyalty, but also helps to
attract new costumers to boost the sites trac.
In fact,
friendship prediction (a.k.a.
link prediction) and interest
targeting (a.k.a. service recommendation) are two important
tools available in almost all the major social network sites.
Both activities which occur routinely in a social network
have accrued a tremendous wealth of interaction traces, both
among users (i.e. friendship network) and between users and
service items (i.e. interest network). Figure 1 depicts a typi-
cal topology of a heterogeneous graph in the context of social
networks.
1.1 Interests and Friendship

Modeling user interests and friendship in social networks
raises unique challenges to both research and engineering
communities. The information about a users behaviors is
often scattered in both friendship and interest networks, in-
volving other users that are closely connected to the user

WWW 2011  Session: Temporal DynamicsMarch 28April 1, 2011, Hyderabad, India537and dierent activities that the user has engaged in. A fun-
damental mechanism that drives the dynamics of networks
is the underlying social phenomenon of homophily [18]: peo-
ple with similar interest tend to connect to each other and
people of similar interest are more likely to be friends.

Traditional user proling approaches often do not take
full advantage of this fact. Instead they either employ fea-
ture engineering to generate hand-crafted meta-descriptors
as ngerprint for a user [26, 5] or they extract a set of latent
features by factorizing a users registered prole data; for
example, by means of sparse coding [12] or latent Dirichlet
allocation [2]. These approaches could be inaccurate be-
cause neither user friendship nor user behavior information
is taken into account.

Recent approaches resort to collaborative ltering (CF)
techniques [3, 23, 1, 10] to prole user interests by collabo-
ratively uncovering user behaviors, where users are assumed
to be unrelated to each other. While CF performs well in
recommendation systems where decisions are mainly made
individually and independently, it could fail in the context
of social networks where user interactions substantially in-
uence decision making [7, 18].

Modeling friendship is equally challenging. A typical so-
cial network is a graph both large and sparse, involving hun-
dreds of millions of users with each being connected to only
a tiny proportion of the whole virtual world. This property
rules out traditional spectral algorithms for graph mining
[19, 20] and calls for algorithms that are both ecient to
handle large scale connections and capable of reliably learn-
ing from rare, noisy and largely missing observations. Un-
fortunately, progress on this topic to date is limited [13].
1.2 Friendship Interest Propagation

This paper exploits the important role homophily plays
in social networks. We show that friendship and interest in-
formation is highly correlated (i.e. closely-connected friends
tend to have similar interests) and mutually helpful (i.e.
much higher performance for both friendship prediction and
interest targeting could be achieved if coupling the two pro-
cesses to exploit both sources of evidence simultaneously).
We present a friendship-interest propagation (FIP) model
that integrates the learning for interest targeting and friend-
ship prediction into one single process.

The key idea in FIP is to associate latent factors with both
users and items, and to dene coupled models to encode both
interest and friendship information. In particular, FIP de-
nes a shared latent factor to assure dynamical interaction
between friendship network and interest network during the
learning process. In doing so, FIP integrates both interest
and friendship networks to connect a user to both items of
potential interest and other users with similar interests. FIP
hereby provides a single unied framework to address both
link prediction and interest targeting while enjoying the re-
sources of both sources of evidence. Experiments on Yahoo!
Pulse demonstrate that, by coupling friendship with inter-
est, FIP achieves much higher performance on both tasks.

The contributions of this work are three-fold:

1. We present the friendship-interest propagation model
that propagates two dierent types of evidence through
heterogeneous connections.

2. We formulate the FIP model in a computational frame-
work, discuss the exibility in the choices of loss objec-
tives (e.g. (cid:2)2, logistic regression, Hubers loss) and reg-

ularization penalties (e.g. sparse coding, (cid:2)2 penalties)
and we benchmark dierent variants in a real-world
social networking system;

3. For the implementation of FIP, we present a built-in
scheme for bias correction based on pseudo-negative
sampling to avoid overtting, and we also deliver an
optimization package that allows distributed optimiza-
tion on streaming data.

Outline: 2 describes the background. 3 presents the de-
tailed FIP model and our distributed implementation. 4
reports experiments and results. 5 reviews related work
and 6 summarizes the results.

2. PROBLEM DEFINITION
We begin by briey reviewing the state-of-the-art. This
will come in handy as we will link them to our model in 3.
Modeling dyadic interactions is the heart of many web ap-
plications, including link prediction and interest targeting.
Typically, a pair of instances from two parties (such as users
and items), i  I and j  J , interact with each other with
a response yij  Y. The mapping

{(i, j)  yij where i  I, j  J }

constitutes a large matrix Y  Y|I||J |
, of which only a tiny
proportion of entries are observable; the goal is to infer the
value of a missing entry yij, given an incoming pair (i, j).
Essentially, the observed interactions dene a graph, either
unipartite (when I = J ) or bipartite. The task amounts to
propagating the sparse observations to the remainder (un-
observed) part of the matrix. For convenience we will hence-
forth refer to i as user and j as item unless stated otherwise.
2.1 Interest Targeting

Interest targeting, or (service) recommendation, works with

a bipartite graph between two dierent parties, e.g. user i
and item j. It aims at matching the best item j
to a given
user i. We consider collaborative ltering (CF) approaches,
which tackle the problem by learning from past interactions.



Neighborhood models. A popular approach to CF is
based on the principle of locality of dependencies, which
assumes that the interaction between user i and item j
can be restored solely upon the observations of neighboring
users or items [24, 17]. Such neighborhood-based models
therefore propagate similar items to a particular user (item-
oriented) or recommend a particular item to similar users
(user-oriented). Basically, it predicts the interest of user i
to item j by averaging the neighboring observations. For
instance, the user-oriented model uses:
i(cid:2)i ii(cid:2) yi(cid:2)j

(cid:2)
(cid:2)

yij =

i(cid:2)i ii(cid:2)

,

where ii(cid:2) measures the similarity, e.g. Pearson correlation
coecient, between user i and its neighbor i

(cid:4)  i.

Latent factor models. This class of methods attempt to
learn informative latent factors to uncover the dyadic in-
teractions. The basic idea is to associate latent factors,1

1Throughout this paper, we assume each latent factor 
contains a constant component so as to absorb user/item-
specic oset into latent factors.

WWW 2011  Session: Temporal DynamicsMarch 28April 1, 2011, Hyderabad, India538k for each user i and j  R

i  R
k for each item j, and
assume a multiplicative model for the interaction response

p(yij|i, j) = p(yij|

(cid:5)
i j ; ).

This way the factors could explain past interactions and in
turn make prediction for future ones. This model implicitly
encodes the Aldous-Hoover theorem [6] for exchangeable ma-
trices  yij are independent from each other given i and
j . Parameter estimation for the model reduces to a low-
rank approximation of the matrix Y that naturally embeds
both users and items into a vector space in which the inner
product 

(cid:5)
i j directly reect the semantic relatedness.

Latent factor models have gained tremendous successes
in recommendation systems and have even become the cur-
rent state-of-the-art for CF [10, 1]. A known drawback for
such models is that, because it is learned only upon past
interactions, the generalization performance is usually poor
for completely new entities, i.e. unseen users or items, for
which the observations are missing at the training stage.
This scenario is well-known as the cold-start problem in
recommendation systems. The recently proposed regression
based latent factor model (RLFM) [1] addresses this problem
by incorporating entity features into latent factor learning.
The key idea is to use observable features to explain the
learned latent variables (e.g. by regression or factorization).
Suppose for each user and each item, there are observable
features, xi for i (e.g. users demographic information, self-
crafted registration proles) and xj for j (e.g. content of a
document, description of a product), as shown in Figure 2,
RLFM [1] assumes the following dependencies:
yij  p(yij|

i  p(i|xi) j  p(j|xj )

(cid:5)
i j ; ).

Neighborhood based latent factor models. It is nat-
ural to combine the neighborhood models and latent factor
models. A recent example is discussed [9], where the basic
idea is to apply the locality of dependencies directly to the
latent factors, for example:
i(cid:2)i ii(cid:2) i(cid:2)
i(cid:2)i ii(cid:2)

yij  p(yij| 
(cid:5)
i j; ).

(cid:2)
(cid:2)

i =

(1)

This model2 which is quite similar to [9] was deployed on
the Netix data yielding signicantly better performances
over both pure-neighborhood and pure latent factor models.
2.2 Friendship Prediction

Friendship (link) prediction recommends users to other
users in the hope of acquainting people who were previously
not connected in the network (or even unfamiliar with each
other). Unlike interest targeting, the user network is unipar-
(cid:4)
tite. For a pair of users (i, i
) the observation whether they
are connected is a binary value Sii(cid:2) . Link prediction cru-
cially inuences both the trac and the revenue of a social
network and it is hence recognized as one of the key tasks
in social network analysis.

Ideally, our goal is to learn a distribution over jointly ex-
changeable matrices (e.g. by applying the Aldous-Hoover
factorization theorem). For reasons of practicality we pick a
nite-dimensional factorization instead, which we shall dis-
cuss in the next section. Before we do so, let us briey
review existing approaches. Some of them employ random
walk methods [14, 22] or spectral graph algorithms [19, 20].
2In this case the set of neighbors i contains i with ii = 1.

ix

jx


i


j

jx


j

ijy



y

(a)

ix


i

(b)

ijy



y

'ix


'i

'iis



s

Figure 2: Graphical representations of (a) regres-
sion based latent factor model (RLFM) and (b)
friendship-interest propagation model (FIP).

Random Walk. A random walk on the graph S is a
reversible Markov chain on the vertexes I. The transi-
(cid:4)
tion probability from the vertex i to vertex i
is dened
(cid:4)|i) = sii(cid:2) /di. Here di denotes the degree of vertex i;
p(i
(cid:4)
sii(cid:2) the connection weight between nodes i and i
. Vertexes
are considered close whenever the hitting time is small or
whenever the diusion probability is large.
Spectral Algorithms. For the given network S, the un-
normalized Laplacian is dened by L = D  S, where D is a
diagonal matrix with Dii = di. Spectral algorithms diuse
the connections by maximizing the spectral smoothness to
obtain the intrinsic kinship dened by the dominant eigen-
vectors of the Laplacian

sii(cid:2) (cid:2)ui  ui(cid:2)(cid:2)2 = 2U LU(cid:5), where U = [u1, . . . , u|I|].

(2)

(cid:3)

i,i(cid:2)

3. MODEL

We now consider interest targeting and link prediction in
the context of social network, where evidence for both in-
terest and friendship are available, allowing us to solve both
tasks in a single framework. The rationale is that friendship
and interest information are to some degree correlated,3 i.e.
the network exhibits homophily [18] and the propagation
of friendship and interest would be mutually reinforcing if
modeled jointly.

In this section we present our model of friendship-interest
propagation (FIP). We start with a probabilistic formula-
tion, discuss dierent variants of the model and its imple-
mentation within an optimization framework, and then dis-
tinguish our model from existing works.
3.1 Probabilistic Model

The nontrivial correlation between interest and friendship
motivates joint modeling of both sources of evidence. As
shown in Figure 2, the friendship-interest propagation(FIP)
model simultaneously encodes the two heterogeneous types
of dyadic relationships: the user-item interactions {yij|i 
I, j  J }, and user-user connections {sii(cid:2)|i, i
(cid:4)  I}. Our
model is built on latent factor models.

3Empirical analysis on Yahoo! Pulse illustrates that the
interest correlation (Pearson score, max 1.0) between two
directly-linked friends is 0.43, much higher than average.

WWW 2011  Session: Temporal DynamicsMarch 28April 1, 2011, Hyderabad, India539Modeling Interest Evidence. To characterize the user-
item dyads, yij , we assume that for each user i and item
j there exist observable properties xi (e.g. a users self-
crafted registration les) and xj (e.g. a textual description of
a service item)4. Moreover, we also assume that there exist
some subtle properties which cannot be observed directly,
such as a users interests, a service items semantic topics.
We denote these latent features by i for i and j for j
respectively. We assume the response yij depends on both
types of features (i.e. observable and latent):
yij  p(yij|i, j , xi, xj, ),
i  p(i|xi) j  p(j|xj )
where  denotes the set of hyper-parameters. To design a
concrete model, one needs to specify distributions for the
dependencies, i|xi, j|xj, and yij|xi, xj, i, j .

This model is essentially an integration of collaborative
ltering [1] and content ltering [4]. On the one hand, if the
user i or item j has no or merely non-informative observable
features such that we have access to only their identity and
past interactions, the model degrades to a factorization-style
collaborative ltering algorithms [23]. On the other hand, if
we assume that i and j are irrelevant, for instance, if i or
j is totally new to the system such that there is no interac-
tion involving either of them as in a cold-start setting, this
model becomes the classical feature-based recommendation
algorithms [3, 31, 4], which predict the interaction response
yij purely based on the observed properties of i and j, and
are commonly used in, e.g. webpage ranking [31], advertise-
ment targeting [3], and content recommendation [4].

Modeling Friendship Evidence. We now extend the in-
terest model to incorporate the social friendship-connection
information among users. For this purpose, we dene a ran-
dom walk process for user-user networking. But unlike tra-
ditional random walk models [14, 22], we assume a user i
is fully characterized by her observable features xi and la-
tent factor i, and devise the following model for user-user
transition:

i  p(i|xi, ) and sii(cid:2)  p(sii(cid:2)|i, i(cid:2) , xi, xi(cid:2) , ),

(3)
(cid:4)
where sii(cid:2) reects an observed state transition from i to i
.
Unlike in random walk models where proximity in a graph is
simply used to smooth secondary estimators of parameters
(e.g. reachability, hitting times), we make direct use of it
to model the latent variables i. Note that whenever we
restrict the norm of i (e.g. by (cid:2)2 regularization) and when
(cid:5)
i i(cid:2) to assess similarity, we
we use an inner product model 
approximately recover the graph Laplacian of Eqn.(2).

In this way our model integrates two dierent methodolo-
gies  collaborative ltering and random walks. It is dier-
ent from traditional random walk models in which transition
probability is dened solely based on graph topologies. It
is also dierent from traditional CF models in that it is de-
ned on unipartite dyadic relationships. By doing so, this
integrated model not only allows learning of latent factors to
capture graph topologies, but it also alleviates certain crit-
ical issues in random walks: for example, it naturally han-
dles heterogeneous graphs (e.g. a compound graph consist-
ing of both unipartite and bipartite connections such as Fig-
ure 1), and it also makes applicable computationally-ecient

4Whenever we do not have access to these properties we
simply default to the expected value of the latent variables,
which is easily achieved in a probabilistic model.

sequential learning algorithms (e.g. stochastic gradient de-
scent), avoiding directly manipulating large matrices.
Friendship-Interest Propagation model. Based on the
above descriptions, we nally summarize the overall FIP
model in Figure 2 and the table below. Note that the tu-
ples (i, xi, i) now play double duty in encoding interest
(cid:4)
, sii(cid:2) )
interactions (i, j, yij) and friendship connections (i, i
simultaneously. Learning shared factors from coupled rela-
tionships gives us both more evidence and more constraints
to work with, and in turn leads to better generalization.

The Friendship-Interest Propagation (FIP) model.

 i  I
 j  J
 i  I, j  J
 i, i
(cid:4)  I

i  p(i|xi, )
j  p(j|xj, )
yij  p(yij|i, j, xi, xj, )
sii(cid:2)  p(sii(cid:2)|i, i(cid:2) , xi, xi(cid:2) , )

3.2 Model Specication

So far we deliberately described the FIP model in terms
of general dependencies between random variables to make
it explicit that the model is quite a bit more general than
what can be achieved by an inner product model. Here, we
specify the model within an optimization framework.

For computational convenience we assume linear depen-
dencies between xi and i plus a noise term5 . This means

i = Axi + i where E [i] = 0.
j = Bxj + j where E [j] = 0.

(4)
(5)

 is typically assumed to be Gaussian or Laplace. Whenever
nonlinearity in x is desired we can achieve this simply by
using a feature map of x and an associated kernel expan-
sion. Finally, we assume that the dyadic response (e.g. yij )
depends on latent features only through the inner product
(cid:5)
(e.g. 
i j) and on observable features through a bilinear
product (e.g. x

(cid:5)
i W xj) [4]. That is:
yij  p(yij|fij ) where fij = 
(cid:5)
i j + x
sii(cid:2)  p(sii(cid:2)|hii(cid:2) ) where hii(cid:2) = 

(cid:5)
i W xj.

m and xj  R

Here, assume xi  R
mn
and M  R
mm provide a bilinear form which captures the
anity between the observed features for the corresponding
dyads. We also impose Laplace or Gaussian priors on W
and M . One advantage of using an (cid:2)1 (i.e. Laplace) prior
is that it introduces sparsity, which makes (6) equivalent to
sparse-coding [12] and thus improves both compactness and
predictiveness of the learned latent factors .
Given observed responses for the dyads {(i, j)  Oy} and
{(i, i
)  Os}, the problem of minimizing the negative log-
(cid:4)
posterior of FIP boils down to the following objective:

(cid:5)
i i(cid:2) + x

(cid:5)
i M xi(cid:2) .
n, the matrices W  R

min y

(cid:2)(yij, fij ) + s

(cid:2)(sii(cid:2) , hii(cid:2) )

+ I

(i|xi) + J

(6)

(cid:3)
(cid:3)

iI

(i,j)Oy

(cid:3)

(cid:3)

jJ

(i,i(cid:2))Os

(j|xj )

+ W [W ] + M [M ] + A[A] + B[B],

where s are trade-o parameters, (cid:2)(,) denotes a loss
function for dyadic responses. The term (|x) = [] +
5Note that the latent noise term is actually meaningful.
It indicates the deviation of the user/item proles from its
cold-start estimates Axi and Bxj respectively.

WWW 2011  Session: Temporal DynamicsMarch 28April 1, 2011, Hyderabad, India540x(x, ). Here [] is used to penalize the complexity (i.e.
(cid:2)2, (cid:2)1 norm). The term x(x, ) regularizes  by tting the
observed feature x, as dened by (6). This type of regular-
ization are equivalent to applying content factorization (e.g.
LSI, NMF, LDA) to the feature x in terms of a factor  and
bases A

1 or B

1.

The motivations for a computational framework instead
of direct probabilistic inference are mainly two-fold: First,
the two formulations are somewhat equivalent  the dis-
tribution of the dyadic response (e.g. yij ) and its depen-
dence on the prediction (e.g. p(yij|fij )) can be encoded pre-
cisely through the choice of loss functions; likewise, the prior
over the observations or parameters could also be readily
translated into the regularization penalties. Secondly, com-
putational models allow more scalable algorithms, e.g. via
stochastic gradient descent, whereas probabilistic reasoning
often requires Monte Carlo sampling or quite nontrivial vari-
ational approximations.
3.3 Loss

In our case, both y and s are binary, i.e. yij , sii(cid:2)  {1}.
We performed an extensive study in our experiments com-
paring a large variety of dierent loss functions. For the
convenience of optimization, we limit ourselves to dieren-
tiable (in many cases, also convex) loss functions (see also
Figure 3 for details):

Least Mean Squares: This is the most popularly-used loss
in matrix factorization.
It minimizes the Frobenius
norm of the prediction residue matrix and leads to a
SVD-style algorithm. We have the loss

(cid:2)2(y, f ) =

(1  yf )

2

.

1
2

(7)

Lazy Least Mean Squares: This is a slight modication
of (cid:2)2 loss for the purpose of classication [30]. Basi-
cally, it is an iteratively truncated version of the (cid:2)2 loss
via

ll2(y, f ) = min(1, max(0, 1  yf )

2

).

(8)

It has been shown that this loss approximates the clas-
sication error rate in the example space [30].

Logistic regression: This is the loss used in a binary ex-

ponential families model. It is given by

log(y, f ) = log[1 + exp(yf )].

(9)

Huber loss: This is the one-sided variant of Hubers robust
loss function. It is convex and continuously dieren-
tiable via

(y, f ) =

1

2 max(0, 1  yf )2,
 yf,

1
2

if yf > 0.
otherwise.

(10)

(cid:4)

(cid:4)

 loss: Unlike other loss functions, which are all convex
upper bound of the 0-1 loss, the  loss [25] is non-
convex. Both theoretical and empirical studies have
shown appealing advantages of using non-convex loss
over convex ones, such as higher generalization accu-
racy, better scalability, faster convergence to the Bayes
limit [25, 30]. We implement the following version:

(y, f ) =

1

2 max(0, 1  yf )2,
1  1

2 max(0, 1 + yf )2,

if yf > 0.
otherwise.

(11)

2

s
s
o

l

1

0



1

0

yf

1

l2
log
Huber
Psi



2

Figure 3: Least mean squares ((cid:2)2), logistic (log), Hu-
ber and -loss (Psi). We use these four and the lazy
(cid:2)2 (omitted since its shape in parameter space is es-
sentially identical to (cid:2)2) loss for binary classication.

3.4 Bias Correction

A key challenge for learning latent factors from dyadic in-
teractions is that the observations are extremely sparse with
almost exclusively positive interactions observable. That is,
we typically do not observe explicit information that user
i does not like item j. Rather, the fact that we have not
observed (i, j) suggests that i might not even know about
j. In other words, absence of a preference statement or a
social link should not be interpreted absolutely as negative
information.

At the same time, unless we have access to negative sig-
nals, we will almost inevitably obtain an estimator that is
