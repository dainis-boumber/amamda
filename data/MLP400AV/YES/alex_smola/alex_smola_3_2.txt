Abstract

We present a globally convergent method for regularized risk minimization prob-
lems. Our method applies to Support Vector estimation, regression, Gaussian
Processes, and any other regularized risk minimization setting which leads to a
convex optimization problem. SVMPerf can be shown to be a special case of
our approach. In addition to the unied framework we present tight convergence
bounds, which show that our algorithm converges in O(1/) steps to  precision
for general convex problems and in O(log(1/)) steps for continuously differen-
tiable problems. We demonstrate in experiments the performance of our approach.

1

Introduction

In recent years optimization methods for convex models have seen signicant progress. Starting
from the active set methods described by Vapnik [17] increasingly sophisticated algorithms for solv-
ing regularized risk minimization problems have been developed. Some of the most exciting recent
developments are SVMPerf [5] and the Pegasos gradient descent solver [12]. The former computes
gradients of the current solution at every step and adds those to the optimization problem. Joachims
[5] prove an O(1/2) rate of convergence. For Pegasos Shalev-Shwartz et al. [12] prove an O(1/)
rate of convergence, which suggests that Pegasos should be much more suitable for optimization.
In this paper we extend the ideas of SVMPerf to general convex optimization problems and a much
wider class of regularizers. In addition to this, we present a formulation which does not require
the solution of a quadratic program whilst in practice enjoying the same rate of convergence as
algorithms of the SVMPerf family. Our error analysis shows that the rates achieved by this algorithm
are considerably better than what was previously known for SVMPerf, namely the algorithm enjoys
O(1/) convergence and O(log(1/)) convergence, whenever the loss is sufciently smooth. An
important feature of our algorithm is that it automatically takes advantage of smoothness in the
problem.
Our work builds on [15], which describes the basic extension of SVMPerf to general convex prob-
lems. The current paper provides a) signicantly improved performance bounds which match better
what can be observed in practice and which apply to a wide range of regularization terms, b) a vari-
ant of the algorithm which does not require quadratic programming, yet enjoys the same fast rates of
convergence, and c) experimental data comparing the speed of our solver to Pegasos and SVMPerf.
Due to space constraints we relegate the proofs to an technical report [13].

2 Problem Setting

Denote by x  X and y  Y patterns and labels respectively and let l(x, y, w) be a loss function
which is convex in w  W, where either W = Rd (linear classier), or W is a Reproducing Kernel
Hilbert Space for kernel methods. Given a set of m training patterns {xi, yi}m
i=1 the regularized risk

1

functional which many estimation methods strive to minimize can be written as

m(cid:88)

i=1

J(w) := Remp(w) + (w) where Remp(w) :=

1
m

l(xi, yi, w).

(1)

2 (cid:107)w(cid:107)2, and  > 0 is a regularization term. Typically
(w) is a smooth convex regularizer such as 1
 is cheap to compute and to minimize whereas the empirical risk term Remp(w) is computationally
expensive to deal with. For instance, in the case of intractable graphical models it requires approx-
imate inference methods such as sampling or semidenite programming. To make matters worse
the number of training observations m may be huge. We assume that the empirical risk Remp(w) is
nonnegative.

If J is differentiable we can use standard quasi-Newtons methods
like LBFGS even for large values of m [8]. Unfortunately, it is not
straightforward to extend these algorithms to optimize a non-smooth
objective. In such cases one has to resort to bundle methods [3],
which are based on the following elementary observation: for con-
vex functions a rst order Taylor approximation is a lower bound.
So is the maximum over a set of Taylor approximations. Further-
more, the Taylor approximation is exact at the point of expansion.
The idea is to replace Remp[w] by these lower bounds and to opti-
mize the latter in conjunction with (w). Figure 1 gives geometric
intuition. In the remainder of the paper we will show that 1) This ex-
tends a number of existing algorithms; 2) This method enjoys good
rates of convergence; and 3) It works well in practice.

Figure 1: A lower bound on the
convex empirical risk Remp(w)
obtained by computing three tan-
gents on the entire function.

Note that there is no need for Remp[w] to decompose into individual losses in an additive fashion.
For instance, scores, such as Precision@k [4], or SVM Ranking scores do not satisfy this property.
Likewise, estimation problems which allow for an unregularized common constant offset or adaptive
margin settings using the -trick fall into this category. The only difference is that in those cases the
derivative of Remp[w] with respect to w no more decomposes trivially into a sum of gradients.

3 Bundle Methods

3.1 Subdifferential and Subgradient

Before we describe the bundle method, it is necessary to clarify a key technical point. The subgradi-
ent is a generalization of gradients appropriate for convex functions, including those which are not
necessarily smooth. Suppose w is a point where a convex function F is nite. Then a subgradi-
ent is the normal vector of any tangential supporting hyperplane of F at w. Formally  is called a
subgradient of F at w if, and only if,

F (w(cid:48))  F (w) + (cid:104)w(cid:48)  w, (cid:105)

w(cid:48).

(2)

The set of all subgradients at a point is called the subdifferential, and is denoted by wF (w). If
this set is not empty then F is said to be subdifferentiable at w. On the other hand, if this set is a
singleton then, the function is said to be differentiable at w.

3.2 The Algorithm
Denote by wt  W the values of w which are obtained by successive steps of our method, Let
at  W, bt  R, and set w0 = 0, a0 = 0, b0 = 0. Then, the Taylor expansion coefcients of
Remp[wt] can be written as

at+1 := wRemp(wt) and bt+1 := Remp(wt)  (cid:104)at+1, wt(cid:105) .

(3)

Note that we do not require Remp to be differentiable: if Remp is not differentiable at wt we simply
choose any element of the subdifferential as at+1. Since each Taylor approximation is a lower
bound, we may take their maximum to obtain that Remp(w)  maxt (cid:104)at, w(cid:105) + bt. Moreover, by

2

Algorithm 1 Bundle Method()

Initialize t = 0, w0 = 0, a0 = 0, b0 = 0, and J0(w) = (w)
repeat

Find minimizer wt := argminw Jt(w)
Compute gradient at+1 and offset bt+1.
Increment t  t + 1.

until t  

virtue of the fact that Remp is a non-negative function we can write the following lower bounds on
Remp and J respectively:

Rt(w) := max
t(cid:48)t

(cid:104)at(cid:48), w(cid:105) + bt(cid:48) and Jt(w) := (w) + Rt(w).

(4)

By construction Rt(cid:48)  Rt  Remp and Jt(cid:48)  Jt  J for all t(cid:48)  t. Dene

w := argmin

w

wt := argmin

w

J(w),

Jt(w),

and

t := Jt+1(wt)  Jt(wt),
t := min
t(cid:48)t

Jt(cid:48)+1(wt(cid:48))  Jt(wt).

The following lemma establishes some useful properties of t and t.
Lemma 1 We have Jt(cid:48)(wt(cid:48))  Jt(wt)  J(w)  J(wt) = Jt+1(wt) for all t(cid:48)  t. Furthermore,
t is monotonically decreasing with t  t+1  Jt+1(wt+1)  Jt(wt)  0. Also, t upper bounds
the distance from optimality via t  t  mint(cid:48)t J(wt(cid:48))  J(w).

3.3 Dual Problem

Optimization is often considerably easier in the dual space. In fact, we will show that we need
not know (w) at all, instead it is sufcient to work with its Fenchel-Legendre dual () :=
supw (cid:104)w, (cid:105)  (w). If  is a so-called Legendre function [e.g. 10] the w at which the supremum
is attained can be written as w = (). In the sequel we will always assume that  is twice
differentiable and Legendre. Examples include () = 1
Theorem 2 Let   Rt, denote by A = [a1, . . . , at] the matrix whose columns are the
(sub)gradients, let b = [b1, . . . , bt]. The dual problem of

2 (cid:107)(cid:107)2 or () =(cid:80)

i exp[]i.

minimize

w

maximize



(cid:104)at(cid:48), w(cid:105) + bt(cid:48) + (w) is

Jt(w) := max
t(cid:48)t
t () :=  (1A) + (cid:62)b subject to   0 and (cid:107)(cid:107)1 = 1.
J

Furthermore, the optimal wt and t are related by the dual connection wt = (1At).

(5)

(6)

i.e.

(7)

2 (cid:107)w(cid:107)2

2 the Fenchel-Legendre dual is given by () = 1

2 (cid:107)(cid:107)2
Recall that for (w) = 1
commonly used in SVMs and Gaussian Processes. The following corollary is immediate:
Corollary 3 Dene Q := A(cid:62)A,
minimizew max(0, maxt(cid:48)t (cid:104)at(cid:48), w(cid:105) + bt(cid:48)) + 

:= (cid:104)au, av(cid:105). For quadratic regularization,
2 (cid:107)w(cid:107)2

2 the dual becomes

i.e. Quv

2. This is

maximize



 1

2 (cid:62)Q + (cid:62)b subject to   0 and (cid:107)(cid:107)1 = 1.

This means that for quadratic regularization the dual optimization problem is a quadratic program
where the number of variables equals the number of gradients computed previously. Since t is
typically in the order of 10s to 100s, the resulting QP is very cheap to solve. In fact, we dont even
need to know the gradients explicitly. All that is required to dene the QP are the inner products
between gradient vectors (cid:104)au, av(cid:105). Later in this section we propose a variant which does away with
the quadratic program altogether while preserving most of the appealing convergence properties of
Algorithm 1.

3

3.4 Examples

Structured Estimation Many estimation problems [14, 16] can be written in terms of a piecewise
linear loss function

l(x, y, w) = max
y(cid:48)Y

(cid:104)(x, y(cid:48))  (x, y), w(cid:105) + (y, y(cid:48))

(8)

for some suitable joint feature map , and a loss function (y, y(cid:48)). It follows from Section 3.1 that
a subdifferential of (8) is given by
wl(x, y, w) = (x, y)  (x, y) where y := argmax
y(cid:48)Y

(cid:104)(x, y(cid:48))  (x, y), w(cid:105) + (y, y(cid:48)).

(9)

Since Remp is dened as a summation of loss terms, this allows us to apply Algorithm 1 directly
for risk minimization: at every iteration t we nd all maximal constraint violators for each (xi, yi)
pair and compute the composite gradient vector. This vector is then added to the convex program
we have so far.
Joachims [5] pointed out this idea for the special case of (x, y) = y(x) and y  {1}, that is,
binary loss. Effectively, by dening a joint feature map as the sum over individual feature maps and
by dening a joint loss  as the sum over individual losses SVMPerf performs exactly the same
operations as we described above. Hence, for losses of type (8) our algorithm is a direct extension
of SVMPerf to structured estimation.

Exponential Families One of the advantages of our setting is that it applies to any convex loss
function, as long as there is an efcient way of computing the gradient. That is, we can use it for
cases where we are interested in modeling

p(y|x; w) = exp((cid:104)(x, y), w(cid:105)  g(w|x)) where g(w|x) = log

(10)
That is, g(w|x) is the conditional log-partition function. This type of losses includes settings such
as Gaussian Process classication and Conditional Random Fields [1]. Such settings have been
studied by Lee et al. [6] in conjunction with an (cid:96)1 regularizer (w) = (cid:107)w(cid:107)1 for structure discovery
in graphical models. Choosing l to be the negative log-likelihood it follows that

exp(cid:104)(x, y(cid:48)), w(cid:105) dy(cid:48)

(cid:90)

Y

m(cid:88)

m(cid:88)

Remp(w) =

g(w|xi)  (cid:104)(xi, yi), w(cid:105) and wRemp(w) =

Ey(cid:48)p(y(cid:48)|xi;w) [(xi, y(cid:48))]  (xi, yi).

i=1

i=1

This means that column generation methods are therefore directly applicable to Gaussian Process
estimation, a problem where large scale solvers were somewhat more difcult to nd. It also shows
that adding a new model becomes a matter of dening a new loss function and its corresponding
gradient, rather than having to build a full solver from scratch.

4 Convergence Analysis

While Algorithm 1 is intuitively plausible, it remains to be shown that it has good rates of conver-
gence. In fact, past results, such as those by Tsochantaridis et al. [16] suggest an O(1/2) rate,
which would make the application infeasible in practice.
We use a duality argument similar to those put forward in [11, 16], both of which share key tech-
niques with [18]. The crux of our proof argument lies in showing that t  t+1  Jt+1(wt+1) 
Jt(wt) (see Theorem 4) is sufciently bounded away from 0. In other words, since t bounds the
distance from the optimality, at every step Algorithm 1 makes sufcient progress towards the op-
timum. Towards this end, we rst observe that by strong duality the values of the primal and dual
problems (5) and (6) are equal at optimality. Hence, any progress in Jt+1 can be computed in the
dual.
Next, we observe that the solution of the dual problem (6) at iteration t, denoted by t, forms a
feasible set of parameters for the dual problem (6) at iteration t+1 by means of the parameterization
(t, 0), i.e. by padding t with a 0. The value of the objective function in this case equals Jt(wt).

4

To obtain a lower bound on the improvement due to Jt+1(wt+1) we perform a line search along ((1
)t, ) in (6). The constraint   [0, 1] ensures dual feasibility. We will bound this improvement
in terms of t. Note that, in general, solving the dual problem (6) results in an increase which is
larger than that obtained via the line search. The line search is employed in the analysis only for
analytic tractability. We aim to lower-bound tt+1 in terms of t and solve the resultant difference
equation.
Depending on J(w) we will be able to prove two different convergence results.

(a) For regularizers (w) for which(cid:13)(cid:13)2
(b) Under the above conditions, if furthermore (cid:13)(cid:13)2

()(cid:13)(cid:13)  H we rst experience a regime of progress
wJ(w)(cid:13)(cid:13)  H, i.e. the Hessian of J is

linear in t and a subsequent slowdown to improvements which are quadratic in t.

bounded, we have linear convergence throughout.

We rst derive lower bounds on the improvement Jt+1(wt+1) Jt(wt), then the fact that for (b) the
bounds are better. Finally we prove the convergence rates by solving the difference equation in t.
This reasoning leads to the following theorem:
Theorem 4 Assume that (cid:107)wRemp(w)(cid:107)  G for all w  W , where W is some domain of interest

containing all wt(cid:48) for t(cid:48)  t. Also assume that  has bounded curvature, i.e. let(cid:13)(cid:13)2
for all  (cid:8)1 A where   0 and (cid:107)(cid:107)1  1(cid:9). In this case we have
Furthermore, if(cid:13)(cid:13)2

()(cid:13)(cid:13)  H

2 min(1, t/4G2H)  t

2 min(1, t/4G2H).

t  t+1  t

(11)

(12)

wJ(w)(cid:13)(cid:13)  H, then we have
t/2

t  t+1 

/8H
t/4HH

if t > 4G2H/
if 4G2H/  t  H/2
otherwise

Note that the error keeps on halving initially and settles for a somewhat slower rate of convergence
after that, whenever the Hessian of the overall risk is bounded from above. The reason for the
difference in the convergence bound for differentiable and non-differentiable losses is that in the
former case the gradient of the risk converges to 0 as we approach optimality, whereas in the former
case, no such guarantees hold (e.g. when minimizing |x| the (sub)gradient does not vanish at the
optimum).
Two facts are worthwhile noting: a) The dual of many regularizers, e.g. squares norm, squared (cid:96)p
norm, and the entropic regularizer have bounded second derivative. See e.g. [11] for a discussion

()(cid:13)(cid:13)  H is not unreasonable. b) Since the improvements

and details. Thus our condition(cid:13)(cid:13)2

decrease with the size of t we may replace t by t in both bounds and conditions without any ill
effect (the bound only gets worse). Applying the previous result we obtain a convergence theorem
for bundle methods.
Theorem 5 Assume that J(w)  0 for all w. Under the assumptions of Theorem 4 we can give the
following convergence guarantee for Algorithm 1. For any  < 4G2H/ the algorithm converges
to the desired precision after

n  log2

J(0)
G2H +

8G2H



 4

(13)
steps. If furthermore the Hessian of J(w) is bounded, convergence to any   H/2 takes at most
the following number of steps:
J(0)
4G2H +

max(cid:2)0, H  8G2H/(cid:3) +

n  log2

4H


log(H/2)

4HH



(14)

Several observations are in order: rstly, note that the number of iterations only depends logarithmi-
cally on how far the initial value J(0) is away from the optimal solution. Compare this to the result
of Tsochantaridis et al. [16], where the number of iterations is linear in J(0).

5

Secondly, we have an O(1/) dependence in the number of iterations in the non-differentiable
case. This matches the rate of Shalev-Shwartz et al. [12]. In addition to that, the convergence is
O(log(1/)) for continuously differentiable problems.
Note that whenever Remp(w) is the average over many piecewise linear functions Remp(w) behaves
essentially like a function with bounded Hessian as long as we are taking large enough steps not to
notice the fact that the term is actually nonsmooth.

H   since(cid:13)(cid:13)2

wJ(w)(cid:13)(cid:13) =  +(cid:13)(cid:13)2

wRemp(w)(cid:13)(cid:13).

Remark 6 For (w) = 1

2 (cid:107)w(cid:107)2 the dual Hessian is exactly H = 1. Moreover we know that

2 w(cid:62)Qw the dual is (z) = 1

Effectively the rate of convergence of the algorithm is governed by upper bounds on the primal and
dual curvature of the objective function. This acts like a condition number of the problem  for
2 z(cid:62)Q1z, hence the largest eigenvalues of Q and Q1
(w) = 1
would have a signicant inuence on the convergence.
In terms of  the number of iterations needed for convergence is O(1). In practice the iteration
count does increase with , albeit not as badly as predicted. This is likely due to the fact that the
empirical risk Remp(w) is typically rather smooth and has a certain inherent curvature which acts as
a natural regularizer in addition to the regularization afforded by [w].

5 A Linesearch Variant

The convergence analysis in Theorem 4 relied on a one-dimensional line search. Algorithm 1,
however, uses a more complex quadratic program to solve the problem. Since even the simple
updates promise good rates of convergence it is tempting to replace the corresponding step in the
bundle update. This can lead to considerable savings in particular for smaller problems, where the
time spent in the quadratic programming solver is a substantial fraction of the total runtime.

It can be shown that Jt+1(0) = t and 2
 = min(1, t/(cid:107)wt + at+1(cid:107)2
2).

2 (cid:107)w(cid:107)2. Note that
To keep matters simple, we only consider quadratic regularization (w) := 1
t+1((1  )t, ) is a quadratic function in , regardless of the choice of Remp[w].
Jt+1() := J
Hence a line search only needs to determine rst and second derivative as done in the proof
 (cid:107)wJ(wt)(cid:107)2 =
of Theorem 4.
 1
 (cid:107)wt + at+1(cid:107)2. Hence the optimal value of  is given by
(15)
This means that we may update wt+1 = (1  )wt  
 at+1. In other words, we need not store past
gradients for the update. To obtain t note that we are computing Remp(wt) as part of the Taylor
relations. In particular, the fact that w(cid:62)A = (cid:107)w(cid:107)2 means that the only quantity we need to cache
is b(cid:62)t as an auxiliary variable rt in order to compute t efciently. Experiments show that this
simplied algorithm has essentially the same convergence properties.

approximation step. Finally, Rt(wt) is given by(cid:2)w(cid:62)A + b(cid:3) t, hence it satises the same update

Jt+1(0) =  1

6 Experiments

In this section we show experimental results that demonstrate the merits of our algorithm and its
analysis. Due to space constraints, we report results of experiments with two large datasets namely
Astro-Physics (astro-ph) and Reuters-CCAT (reuters-ccat) [5, 12]. For a fair comparison with exist-
ing solvers we use the quadratic regularizer (w) := 
In our rst experiment, we address the rate of convergence and its dependence on the value of . In
Figure 2 we plot t as a function of iterations for various values of  using the QP solver at every
iteration to solve the dual problem (6) to optimality. Initially, we observe super-linear convergence;
this is consistent with our analysis. Surprisingly, even though theory predicts sub-linear speed of
convergence for non-differentiable losses like the binary hinge loss (see (11)), our solver exhibits
linear rates of convergence predicted only for differentiable functions (see (12)). We conjecture
that the average over many piecewise linear functions, Remp(w), behaves essentially like a smooth
function. As predicted, the convergence speed is inversely proportional to the value of .

2(cid:107)w(cid:107)2, and the binary hinge loss.

6

Figure 2: We plot t as a function of the number of iterations. Note the logarithmic scale in t. Left:
astro-ph; Right: reuters-ccat.

Figure 3: Top: Objective function value as a function of time. Bottom: Objective function value as
a function of iterations. Left: astro-ph, Right: reuters-ccat. The black line indicates the nal value
of the objective function + 0.001 .

In our second experiment, we compare the convergence speed of two variants of the bundle method,
namely, with a QP solver in the inner loop (which essentially boils down to SVMPerf) and the line
search variant which we described in Section 5. We contrast these solvers with Pegasos [12] in the
batch setting. Following [5] we set  = 104 for reuters-ccat and  = 2.104 for astro-ph.
Figure 3 depicts the evolution of the primal objective function value as a function of both CPU time
as well as the number of iterations. Following Shalev-Shwartz et al. [12] we investigate the time
required by various solvers to reduce the objective value to within 0.001 of the optimum. This is
depicted as a black horizontal line in our plots. As can be seen, Pegasos converges to this region
quickly. Nevertheless, both variants of the bundle method converge to this value even faster (line
search is slightly slower than Pegasos on astro-ph, but this is not always the case for many other large
datasets we tested on). Note that both line search and Pegasos converge to within 0.001 precision
rather quickly, but they require a large number of iterations to converge to the optimum.

7 Related Research

Our work is closely related to Shalev-Shwartz and Singer [11] who prove mistake bounds for online
algorithms by lower bounding the progress in the dual. Although not stated explicitly, essentially
the same technique of lower bounding the dual improvement was used by Tsochantaridis et al. [16]
to show polynomial time convergence of the SVMStruct algorithm. The main difference however
is that Tsochantaridis et al. [16] only work with a quadratic objective function while the framework

7

proposed by Shalev-Shwartz and Singer [11] can handle arbitrary convex functions. In both cases,
a weaker analysis led to O(1/2) rates of convergence for nonsmooth loss functions. On the other
hand, our results establish a O(1/) rate for nonsmooth loss functions and O(log(1/)) rates for
smooth loss functions under mild technical assumptions.
Another related work is SVMPerf [5] which solves the SVM estimation problem in linear time.
SVMPerf nds a solution with accuracy  in O(md/(2)) time, where the m training patterns
xi  Rd. This bound was improved by Shalev-Shwartz et al. [12] to O(1/) for obtaining an
accuracy of  with condence 1  . Their algorithm, Pegasos, essentially performs stochastic

(sub)gradient descent but projects the solution back onto the L2 ball of radius 1/
. But, as our
experiments show, performing an exact line search in the dual leads to a faster decrease in the value
of primal objective. Note that Pegasos also can be used in an online setting. This, however, only
applies whenever the empirical risk decomposes into individual loss terms (e.g. it is not applicable
to multivariate performance scores).
The third related strand of research considers gradient descent in the primal with a line search to
choose the optimal step size, see e.g. [2, Section 9.3.1]. Under assumptions of smoothness and
strong convexity  that is, the objective function can be upper and lower bounded by quadratic func-
tions  it can be shown that gradient descent with line search will converge to an accuracy of  in
O(log(1/)) steps. The problem here is the line search in the primal, since evaluating the regular-
ized risk functional might be as expensive as computing its gradient, thus rendering a line search in
the primal unattractive. On the other hand, the dual objective is relatively simple to evaluate, thus
making the line search in the dual, as performed by our algorithm, computationally feasible.
Finally, we would like to point out connections to subgradient methods [7]. These algorithms are
designed for nonsmooth functions, and essentially choose an arbitrary element of the subgradient set
to perform a gradient descent like update. Let (cid:107)Jw(w)(cid:107)  G, and B(w, r) denote a ball of radius
r centered around the minimizer of J(w). By applying the analysis of Nedich and Bertsekas [7] to
2(cid:107)w(cid:107)2, Ratliff et al. [9] showed that sub-
the regularized risk minimization problem with (w) := 
gradient descent with a xed, but sufciently small, stepsize will converge linearly to B(w, G/).

