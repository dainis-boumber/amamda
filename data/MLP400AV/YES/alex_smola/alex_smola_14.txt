Abstract. We introduce a family of kernels on graphs based on the
notion of regularization operators. This generalizes in a natural way the
notion of regularization and Greens functions, as commonly used for
real valued functions, to graphs. It turns out that diusion kernels can
be found as a special case of our reasoning. We show that the class of
positive, monotonically decreasing functions on the unit interval leads to
kernels and corresponding regularization operators.

1 Introduction

There has recently been a surge of interest in learning algorithms that operate on
input spaces X other than Rn, specically, discrete input spaces, such as strings,
graphs, trees, automata etc.. Since kernel-based algorithms, such as Support
Vector Machines, Gaussian Processes, Kernel PCA, etc. capture the structure
of X via the kernel K : X  X 7 R, as long as we can dene an appropriate
kernel on our discrete input space, these algorithms can be imported wholesale,
together with their error analysis, theoretical guarantees and empirical success.
One of the most general representations of discrete metric spaces are graphs.
Even if all we know about our input space are local pairwise similarities between
points xi, xj  X, distances (e.g shortest path length) on the graph induced
by these similarities can give a useful, more global, sense of similarity between
objects. In their work on Diusion Kernels, Kondor and Laerty [2002] gave
a specic construction for a kernel capturing this structure. Belkin and Niyogi
[2002] proposed an essentially equivalent construction in the context of approx-
imating data lying on surfaces in a high dimensional embedding space, and in
the context of leveraging information from unlabeled data.

In this paper we put these earlier results into the more principled framework
of Regularization Theory. We propose a family of regularization operators (equiv-
alently, kernels) on graphs that include Diusion Kernels as a special case, and
show that this family encompasses all possible regularization operators invariant
under permutations of the vertices in a particular sense.

2

Alexander Smola and Risi Kondor

Outline of the Paper: Section 2 introduces the concept of the graph Laplacian
and relates it to the Laplace operator on real valued functions. Next we dene
an extended class of regularization operators and show why they have to be es-
sentially a function of the Laplacian. An analogy to real valued Greens functions
is established in Section 3.3, and ecient methods for computing such functions
are presented in Section 4. We conclude with a discussion.

2 Laplace Operators

2 W D 1

Let D be an n n diagonal matrix with Dii =P

j Wij. The Laplacian of G
2 =
2 . The following two theorems are well known results from spectral

An undirected unweighted graph G consists of a set of vertices V numbered 1 to
n, and a set of edges E (i.e., pairs (i, j) where i, j V and (i, j) E  (j, i) E).
We will sometimes write i  j to denote that i and j are neighbors, i.e. (i, j) E.
The adjacency matrix of G is an n n real matrix W , with Wij =1 if i  j, and
0 otherwise (by construction, W is symmetric and its diagonal entries are zero).
These denitions and most of the following theory can trivially be extended to
weighted graphs by allowing Wij  [0,).
is dened as L := DW and the Normalized Laplacian is L := D 1
ID 1
graph theory [Chung-Graham, 1997]:
Theorem 1 (Spectrum of L). L is a symmetric, positive semidenite matrix,
and its eigenvalues 1, 2, . . . , n satisfy 0  i  2. Furthermore, the number
of eigenvalues equal to zero equals to the number of disjoint components in G.
The bound on the spectrum follows directly from Gerschgorins Theorem.
Theorem 2 (L and L for Regular Graphs). Now let G be a regular graph
of degree d, that is, a graph in which every vertex has exactly d neighbors. Then
L = d IW and L = I 1
d L. Finally, W, L, L share the same eigenvectors
{vi}, where vi = 1
L and L can be regarded as linear operators on functions f : V 7 R, or, equiv-
alently, on vectors f = (f1, f2, . . . , fn)>. We could equally well have dened L
by

i W vi = (d  i)1Lvi = (1  d1i)1 Lvi for all i.

d W = 1

2 LD 1

(fi  fj)2 for all f Rn,

(1)

hf , Lfi = f

>

Lf = 1
2

X

ij

which readily generalizes to graphs with a countably innite number of vertices.
The Laplacian derives its name from its analogy with the familiar Laplacian
operator  = 2
on continuous spaces. Regarding (1) as
x2
inducing a semi-norm k f kL = hf , Lfi on Rn, the analogous expression for 
1
dened on a compact space  is
k f k = hf, fi =

(f)  (f) d .

+ . . . + 2
x2
m

f (f) d =

+ 2
x2
2

Z

Z

(2)

Both (1) and (2) quantify how much f and f vary locally, or how smooth they
are over their respective domains.





dierence discretization of  on a regular lattice:
2 ei)  

mX

f(x + 1


xi

xi

f(x) =

f(x  1

2 ei)

2
x2
i

i=1

f  mX
 mX

i=1

f(x + ei) + f(x  ei)  2f(x)

=



2

mX

i=1

1
2

More explicitly, when  = Rm, up to a constant, L is exactly the nite

Kernels and Regularization on Graphs

3

(fx1,...,xi+1,...,xm + fx1,...,xi1,...,xm  2fx1,...,xm) =  1

i=1

2 [Lf]x1,...,xm ,

where e1, e2, . . . , em is an orthogonal basis for Rm normalized to k ei k = ,
the vertices of the lattice are at x = x1e1 + . . . + xmem with integer valued
coordinates xiN, and f x1,x2,...,xm = f(x).
Moreover, both the continuous and the dis-
crete Laplacians are canonical operators on
their respective domains, in the sense that
they are invariant under certain natural
transformations of the underlying space, and
in this they are essentially unique.

Regular grid in two dimensions


















































The Laplace operator  is the unique self-adjoint linear second order dier-
ential operator invariant under transformations of the coordinate system under
the action of the special orthogonal group SOm, i.e. invariant under rotations.
This well known result can be seen by using Schurs lemma and the fact that
SOm is irreducible on Rm.

We now show a similar result for L. Here the permutation group plays a
similar role to SOm. We need some additional denitions: denote by Sn the
group of permutations on {1, 2, . . . , n} with   Sn being a specic permutation
taking i  {1, 2, . . . n} to (i). The so-called dening representation of Sn consists
of n n matrices , such that []i,(i)=1 and all other entries of  are zero.
Theorem 3 (Permutation Invariant Linear Functions on Graphs). Let
L be an n  n symmetric real matrix, linearly related to the n  n adjacency
matrix W , i.e. L = T[W ] for some linear operator L in a way invariant to
permutations of vertices in the sense that

T[W ] = T(cid:2)>

>

(cid:3)

(3)
for any   Sn. Then L is related to W by a linear combination of the follow-
ing three operations: identity; row/column sums; overall sum; row/column sum
restricted to the diagonal of L; overall sum restricted to the diagonal of W .

 W 



Proof Let

Li1i2 = T[W ]i1i2 :=

T i1i2i3i4 Wi3i4

(4)

with T Rn4. Eq. (3) then implies T(i1)(i2)(i3)(i4) = Ti1i2i3i4 for any  Sn.

i3=1

i4=1

nX

nX

4

Alexander Smola and Risi Kondor

The indices of T can be partitioned by the equality relation on their values,
e.g. (2, 5, 2, 7) is of the partition type [ 1 3| 2| 4 ], since i1 = i3, but i26= i1, i46= i1
and i2 6= i4. The key observation is that under the action of the permutation
group, elements of T with a given index partition structure are taken to elements
with the same index partition structure, e.g. if i1 = i3 then (i1) = (i3) and
if i1 6= i3, then (i1)6= (i3). Furthermore, an element with a given index index
partition structure can be mapped to any other element of T with the same
index partition structure by a suitable choice of .

Hence, a necessary and sucient condition for (4) is that all elements of
T of a given index partition structure be equal. Therefore, T must be a linear
combination of the following tensors (i.e. multilinear forms):

Ai1i2i3i4 = 1
B[1,2]
i1i2i3i4 = i1i2
B[2,3]
i1i2i3i4 = i2i3
C [1,2,3]
i1i2i3i4 = i1i2i2i3
C [3,4,1]
i1i2i3i4 = i3i4i4i1
D[1,2][3,4]
i1i2i3i4 = i1i2i3i4
E[1,2,3,4]
i1i2i3i4 = i1i2i1i3i1i4 .

i1i2i3i4 = i1i4
i1i2i3i4 = i3i4

B[1,4]
B[3,4]

B[1,3]
i1i2i3i4 = i1i3
B[2,4]
i1i2i3i4 = i2i4
C [2,3,4]
i1i2i3i4 = i2i3i3i4
C [4,1,2]
i1i2i3i4 = i4i1i1i2
i1i2i3i4 = i1i3i2i4 D[1,4][2,3]
D[1,3][2,4]

i1i2i3i4 = i1i4i2i3

The tensor A puts the overall sum in each element of L, while B[1,2] returns the
the same restricted to the diagonal of L.

Since W has vanishing diagonal, B[3,4], C [2,3,4], C [3,4,1], D[1,2][3,4] and E[1,2,3,4]

produce zero. Without loss of generality we can therefore ignore them.

By symmetry of W , the pairs (B[1,3], B[1,4]), (B[2,3], B[2,4]), (C [1,2,3], C [4,1,2])
have the same eect on W , hence we can set the coecient of the second member
of each to zero. Furthermore, to enforce symmetry on L, the coecient of B[1,3]
and B[2,3] must be the same (without loss of generality 1) and this will give the

row/column sum matrix (P
give the row/column sum restricted to the diagonal: ij [(P

Similarly, C [1,2,3] and C [4,1,2] must have the same coecient and this will
k Wkl)].
Finally, by symmetry of W , D[1,3][2,4] and D[1,4][2,3] are both equivalent to

k Wik) + (P

k Wik) + (P

k Wkl).

the identity map.
The various row/column sum and overall sum operations are uninteresting from
a graph theory point of view, since they do not heed to the topology of the graph.
Imposing the conditions that each row and column in L must sum to zero, we
recover the graph Laplacian. Hence, up to a constant factor and trivial additive
components, the graph Laplacian (or the normalized graph Laplacian if we wish
to rescale by the number of edges per vertex) is the only invariant dierential
operator for given W (or its normalized counterpart W ). Unless stated otherwise,
all results below hold for both L and L (albeit with a dierent spectrum) and we
will, in the following, focus on L due to the fact that its spectrum is contained
in [0, 2].

Kernels and Regularization on Graphs

5

3 Regularization

The fact that L induces a semi-norm on f which penalizes the changes between
adjacent vertices, as described in (1), indicates that it may serve as a tool to
design regularization operators.

3.1 Regularization via the Laplace Operator
We begin with a brief overview of translation invariant regularization operators
on continuous spaces and show how they can be interpreted as powers of . This
will allow us to repeat the development almost verbatim with L (or L) instead.
Some of the most successful regularization functionals on Rn, leading to

kernels such as the Gaussian RBF, can be written as [Smola et al., 1998]

hf, P fi :=

| f()|2 r(kk2) d = hf, r()fi .

(5)
Here f  L2(Rn), f() denotes the Fourier transform of f, r(kk2) is a function
penalizing frequency components | f()| of f, typically increasing in kk2, and
nally, r() is the extension of r to operators simply by applying r to the
spectrum of  [Dunford and Schwartz, 1958]

Z

hf, r()f0i =X

hf, ii r(i)hi, f0i

where {(i, i)} is the eigensystem of . The last equality in (5) holds because
applications of  become multiplications by kk2 in Fourier space. Kernels are
obtained by solving the self-consistency condition [Smola et al., 1998]

i

hk(x,), P k(x0,)i = k(x, x0) .

(6)
One can show that k(x, x0) = (x  x0), where  is equal to the inverse Fourier
transform of r1(kk2). Several r functions have been known to yield good
results. The two most popular are given below:
k(x, x0)

r(kk2)

r()

Gaussian RBF exp

kk2

exp

(cid:19)

(cid:18) 2

2

(cid:18)
(cid:18)

(cid:19)
 1
22kx  x0k2
 1
kx  x0k


(cid:19) X

2i
i! i
i=0
1 + 2

Laplacian RBF 1 + 2kk2
In summary, regularization according to (5) is carried out by penalizing f()
by a function of the Laplace operator. For many results in regularization theory
one requires r(kk2)   for kk2  .

exp

3.2 Regularization via the Graph Laplacian
In complete analogy to (5), we dene a class of regularization functionals on
graphs as

hf , P fi := hf , r(L)fi .

(7)

6

Alexander Smola and Risi Kondor

Fig. 1. Regularization function r(). From left to right: regularized Laplacian (2 = 1),
diusion process (2 = 1), one-step random walk (a = 2), 4-step random walk (a = 2),
inverse cosine.

Here r(L) is understood as applying the scalar valued function r() to the eigen-
values of L, that is,

mX

r(L) :=

r(i) viv>
i ,

(8)

where {(i, vi)} constitute the eigensystem of L. The normalized graph Lapla-
cian L is preferable to L, since Ls spectrum is contained in [0, 2]. The obvious
goal is to gain insight into what functions are appropriate choices for r.

i=1

 From (1) we infer that vi with large i correspond to rather uneven functions
on the graph G. Consequently, they should be penalized more strongly than
vi with small i. Hence r() should be monotonically increasing in .

 Requiring that r(L) (cid:23) 0 imposes the constraint r()  0 for all   [0, 2].
 Finally, we can limit ourselves to r() expressible as power series, since the

latter are dense in the space of C0 functions on bounded domains.

In Section 3.5 we will present additional motivation for the choice of r() in the
context of spectral graph theory and segmentation. As we shall see, the following
functions are of particular interest:

r() = 1 + 2

r() = exp(cid:0)2/2(cid:1)

r() = (aI  )1 with a  2
r() = (aI  )p with a  2
r() = (cos /4)1

(Regularized Laplacian)
(Diusion Process)
(One-Step Random Walk)
(p-Step Random Walk)
(Inverse Cosine)

(9)
(10)
(11)
(12)
(13)

Figure 1 shows the regularization behavior for the functions (9)-(13).

3.3 Kernels
The introduction of a regularization matrix P = r(L) allows us to dene a
Hilbert space H on Rm via hf, fiH := hf , P fi. We now show that H is a
reproducing kernel Hilbert space.

Kernels and Regularization on Graphs

7

k(i, j) =(cid:2)P 1(cid:3)

Theorem 4. Denote by P Rmm a (positive semidenite) regularization ma-
trix and denote by H the image of Rm under P . Then H with dot product
hf, fiH := hf , P fi is a Reproducing Kernel Hilbert Space and its kernel is
ij, where P 1 denotes the pseudo-inverse if P is not invertible.
Proof Since P is a positive semidenite matrix, we clearly have a Hilbert space
on P Rm. To show the reproducing property we need to prove that

(14)
Note that k(i, j) can take on at most m2 dierent values (since i, j  [1 : m]).
In matrix notation (14) means that for all f  H

f(i) = hf, k(i,)iH.

f(i) = f>P Ki,: for all i  f> = f>P K.

(15)

The latter holds if K = P 1 and f  P Rm, which proves the claim.
In other words, K is the Greens function of P , just as in the continuous case. The
notion of Greens functions on graphs was only recently introduced by Chung-
Graham and Yau [2000] for L. The above theorem extended this idea to arbitrary
regularization operators r(L).
Corollary 1. Denote by P = r(L) a regularization matrix, then the correspond-
ing kernel is given by K = r1(L), where we take the pseudo-inverse wherever
necessary. More specically, if {(vi, i)} constitute the eigensystem of L, we have

mX

K =

r1(i) viv>

i where we dene 01  0.

(16)

i=1

3.4 Examples of Kernels

By virtue of Corollary 1 we only need to take (9)-(13) and plug the denition
of r() into (16) to obtain formulae for computing K. This yields the following
kernel matrices:

K = (I + 2 L)1
K = exp(2/2L)
K = (aI  L)p with a  2
K = cos L/4

(Regularized Laplacian)
(Diusion Process)
(p-Step Random Walk)
(Inverse Cosine)

(17)
(18)
(19)
(20)

Equation (18) corresponds to the diusion kernel proposed by Kondor and Laf-
ferty [2002], for which K(x, x0) can be visualized as the quantity of some sub-
stance that would accumulate at vertex x0 after a given amount of time if we
injected the substance at vertex x and let it diuse through the graph along
the edges. Note that this involves matrix exponentiation dened via the limit
K = exp(B) = limn(I+B/n)n as opposed to component-wise exponentiation
Ki,j = exp(Bi,j).

8

Alexander Smola and Risi Kondor

Fig. 2. The rst 8 eigenvectors of the normalized graph Laplacian corresponding to the
graph drawn above. Each line attached to a vertex is proportional to the value of the
corresponding eigenvector at the vertex. Positive values (red) point up and negative
values (blue) point down. Note that the assignment of values becomes less and less
uniform with increasing eigenvalue (i.e. from left to right).

For (17) it is typically more ecient to deal with the inverse of K, as it
avoids the costly inversion of the sparse matrix L. Such situations arise, e.g., in
Gaussian Process estimation, where K is the covariance matrix of a stochastic
process [Williams, 1999].
Regarding (19), recall that (aI  L)p =
((a1)I + W )p is up to scaling terms equiv-
alent to a p-step random walk on the graph
with random restarts (see Section A for de-
tails). In this sense it is similar to the dif-
fusion kernel. However, the fact that K in-
volves only a nite number of products of
matrices makes it much more attractive for
practical purposes. In particular, entries in
Kij can be computed cheaply using the fact
that L is a sparse matrix.

A nearest neighbor graph.

Finally, the inverse cosine kernel treats lower complexity functions almost
equally, with a signicant reduction in the upper end of the spectrum. Figure 2
shows the leading eigenvectors of the graph drawn above and Figure 3 provide
examples of some of the kernels discussed above.

3.5 Clustering and Spectral Graph Theory
We could also have derived r(L) directly from spectral graph theory: the eigen-
vectors of the graph Laplacian correspond to functions partitioning the graph
into clusters, see e.g., [Chung-Graham, 1997, Shi and Malik, 1997] and the ref-
erences therein. In general, small eigenvalues have associated eigenvectors which
vary little between adjacent vertices. Finding the smallest eigenvectors of L can
be seen as a real-valued relaxation of the min-cut problem.3

For instance, the smallest eigenvalue of L is 0, its corresponding eigenvector
2 1n with 1n := (1, . . . , 1)  Rn. The second smallest eigenvalue/eigenvector
is D
pair, also often referred to as the Fiedler-vector, can be used to split the graph

1

3 Only recently, algorithms based on the celebrated semidenite relaxation of the min-
cut problem by Goemans and Williamson [1995] have seen wider use [Torr, 2003] in
segmentation and clustering by use of spectral bundle methods.

Kernels and Regularization on Graphs

9

Fig. 3. Top: regularized graph Laplacian; Middle: diusion kernel with  = 5, Bottom:
4-step random walk kernel. Each gure displays Kij for xed i. The value Kij at vertex
i is denoted by a bold line. Note that only adjacent vertices to i bear signicant value.

*X

X

X

+

=X

into two distinct parts [Weiss, 1999, Shi and Malik, 1997], and further eigenvec-
tors with larger eigenvalues have been used for more nely-grained partitions of
the graph. See Figure 2 for an example.

Such a decomposition into functions of increasing complexity has very de-
sirable properties: if we want to perform estimation on the graph, we will wish
to bias the estimate towards functions which vary little over large homogeneous
portions 4. Consequently, we have the following interpretation of hf, fiH. As-
i ivi, where {(vi, i)} is the eigensystem of L. Then we can
rewrite hf, fiH to yield

sume that f =P

hf , r(L)fi =

ivi,

r(j)vjv>

j

lvl

i r(i).
2

(21)

i

j

l

i

This means that the components of f which vary a lot over coherent clusters
in the graph are penalized more strongly, whereas the portions of f, which are
essentially constant over clusters, are preferred. This is exactly what we want.

3.6 Approximate Computation

Often it is not necessary to know all values of the kernel (e.g., if we only observe
instances from a subset of all positions on the graph). There it would be wasteful
to compute the full matrix r(L)1 explicitly, since such operations typically scale
with O(n3). Furthermore, for large n it is not desirable to compute K via (16),
that is, by computing the eigensystem of L and assembling K directly.

4 If we cannot assume a connection between the structure of the graph and the values
of the function to be estimated on it, the entire concept of designing kernels on
graphs obviously becomes meaningless.

10

Alexander Smola and Risi Kondor

Instead, we would like to take advantage of the fact that L is sparse, and con-
sequently any operation L has cost at most linear in the number of nonzero ele-
ments of L, hence the cost is bounded by O(|E|+ n). Moreover, if d is the largest
i=1 (min(d+1, n))i
operations: at each step the number of non-zeros in the rhs decreases by at most
a factor of d + 1. This means that as long as we can approximate K = r1(L) by
i=0 i Li, signicant savings are possible.
Note that we need not necessarily require a uniformly good approximation
and put the main emphasis on the approximation for small . However, we need
to ensure that (L) is positive semidenite.

degree of the graph, then computing Lpei costs at most | E |Pp1
a low order polynomial, say (L) :=PN
Diusion Kernel: The fact that the series r1(x) = exp(x) =P

m=0()m xm
has alternating signs shows that the approximation error at r1(x) is bounded
by (2)N+1
(N +1)! , if we use N terms in the expansion (from Theorem 1 we know that
kLk  2). For instance, for  = 1, 10 terms are sucient to obtain an error of
the order of 104.
Variational Approximation: In general, if we want to approximate r1() on
[0, 2], we need to solve the L([0, 2]) approximation problem

m!

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) NX

i=0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)      [0, 2]

(22)

minimize

,

 subject to

ii  r1()

Clearly, (22) is equivalent to minimizing sup L k(L) r1(L)k, since the matrix
norm is determined by the largest eigenvalues, and we can nd L such that the
discrepancy between () and r1() is attained. Variational problems of this
form have been studied in the literature, and their solution may provide much
better approximations to r1() than a truncated power series expansion.

4 Products of Graphs

As we have already pointed out, it is very expensive to compute K for arbitrary
r and L. For special types of graphs and regularization, however, signicant
computational savings can be made.

4.1 Factor Graphs

The work of this section is a direct extension of results by Ellis [2002] and
Chung-Graham and Yau [2000], who study factor graphs to compute inverses of
the graph Laplacian.
Denition 1 (Factor Graphs). Denote by (V, E) and (V 0, E0) the vertices V
and edges E of two graphs, then the factor graph (Vf , Ef ) := (V, E) (V 0, E0) is
dened as the graph where (i, i0) Vf if i V and i0 V 0; and ((i, i0), (j, j0))  Ef
if and only if either (i, j) E and i0 = j0 or (i0, j0) E0 and i= j.

Kernels and Regularization on Graphs

11

For instance, the factor graph of two rings is a torus. The nice property of factor
graphs is that we can compute the eigenvalues of the Laplacian on products very
easily (see e.g., Chung-Graham and Yau [2000]):
Theorem 5 (Eigenvalues of Factor Graphs). The eigenvalues and eigen-
vectors of the normalized Laplacian for the factor graph between a regular graph
of degree d with eigenvalues {j} and a regular graph of degree d0 with eigenvalues
{0

l} are of the form:

j,l = d
fact
(i,i0) = ej

d + d0 j + d0

d + d0 0

(23)
i e0l
i0, where ej is an eigenvector of L and

l

and the eigenvectors satisfy ej,l
e0l is an eigenvector of L0.
This allows us to apply Corollary 1 to obtain an expansion of K as

K = (r(L))1 =X

r1(jl) ej,l(cid:0)ej,l(cid:1)>

.

(24)

j,l

While providing an explicit recipe for the computation of Kij without the need
to compute the full matrix K, this still requires O(n2) operations per entry,
which may be more costly than what we want (here n is the number of vertices
of the factor graph).

Two methods for computing (24) become evident at this point: if r has a
special structure, we may exploit this to decompose K into the products and
sums of terms depending on one of the two graphs alone and pre-compute these
expressions beforehand. Secondly, if one of the two terms in the expansion can
be computed for a rather general class of values of r(x), we can pre-compute this
expansion and only carry out the remainder corresponding to (24) explicitly.

4.2 Product Decomposition of r(x)
Central to our reasoning is the observation that for certain r(x), the term 1
r(a+b)
can be expressed in terms of a product and sum of terms depending on a and b
only. We assume that

MX

1

=

n(a)n(b).

r(a + b)

m=1

In the following we will show that in such situations the kernels on factor graphs
can be computed as an analogous combination of products and sums of kernel
functions on the terms constituting the ingredients of the factor graph. Before
we do so, we briey check that many r(x) indeed satisfy this property.

exp((a + b)) = exp(a) exp(b)
 b

(A  (a + b)) =

 a

(cid:18) A
pX

2

(cid:18)p

(cid:18) A
(cid:19)
(cid:19)(cid:18) A

+

2
 a

(cid:19)
(cid:19)n(cid:18) A

(A  (a + b))p =

cos

(a + b)

4

n=0

n
= cos a
4

2
cos b
4

 sin a
4

 b

2
sin b
4

(cid:19)pn

(25)

(26)

(27)

(28)

(29)

12

Alexander Smola and Risi Kondor

In a nutshell, we will exploit the fact that for products of graphs the eigenvalues
of the joint graph Laplacian can be written as the sum of the eigenvalues of the
Laplacians of the constituent graphs. This way we can perform computations on
n and n separately without the need to take the other part of the the product
of graphs into account. Dene

km(i, j) :=X

(cid:18) dl

(cid:19)

l

d + d0

el
iel

l

j and km(i0, j0) :=X

(cid:19)

(cid:18) dl

d + d0

l

l

i0 el
el

j0 .

(30)

Then we have the following composition theorem:
Theorem 6. Denote by (V, E) and (V 0, E0) connected regular graphs of degrees
d with m vertices (and d0, m0 respectively) and normalized graph Laplacians
L, L0. Furthermore denote by r(x) a rational function with matrix-valued exten-
sion r(X). In this case the kernel K corresponding to the regularization operator
r(L) on the product graph of (V, E) and (V 0, E0) is given by

k((i, i0), (j, j0)) =

km(i, j)km(i0, j0)

(31)

Proof Plug the expansion of

1

r(a+b) as given by (25) into (24) and collect terms.

m=1

From (26) we immediately obtain the corollary (see Kondor and Laerty [2002])
that for diusion processes on factor graphs the kernel on the factor graph is
given by the product of kernels on the constituents, that is k((i, i0), (j, j0)) =
k(i, j)k0(i0, j0).

The kernels km and km can be computed either by using an analytic solution
of the underlying factors of the graph or alternatively they can be computed
numerically. If the total number of kernels kn is small in comparison to the
number of possible coordinates this is still computationally benecial.

MX

4.3 Composition Theorems
If no expansion as in (31) can be found, we may still be able to compute ker-
nels by extending a reasoning from [Ellis, 2002]. More specically, the following
composition theorem allows us to accelerate the computation in many cases,
whenever we can parameterize (r(L + I))1 in an ecient way. For this pur-
pose we introduce two auxiliary functions

(cid:18)

(cid:18) d
d + d0 L + d0
(i, j) := (L0 + I)1 =X
d + d0 I
1

K(i, j) :=

r

G0

l

el(i)el(j).

l + 

l

(cid:19)(cid:19)1

=X

(cid:18)

r

(cid:18) dl + d0

(cid:19)(cid:19)1

d + d0

el(i)el(j)

(32)

In some cases K(i, j) may be computed in closed form, thus obviating the need
to perform expensive matrix inversion, e.g., in the case where the underlying
graph is a chain [Ellis, 2002] and K = G.

Kernels and Regularization on Graphs

13

Theorem 7. Under the assumptions of Theorem 6 we have

Z

C

(j0, l0)d =X

v

K((j, j0), (l, l0)) =

1
2i

K(j, l)G0

Kv(j, l)ev

j0 ev
l0

(33)

where C  C is a contour of the C containing the poles of (V 0, E0) including 0.
For practical purposes, the third term of (33) is more amenable to computation.
Proof From (24) we have

K((j, j0), (l, l0)) =X

(cid:18)
Z

r

(cid:18) du + d0v
(cid:18)
X

(cid:19)(cid:19)1
(cid:18) du + d0

d + d0

r

d + d0

C

u

u,v
1
2i

=

j0 ev
l0

l ev

eu
j eu

(cid:19)(cid:19)1

eu
j eu
l

(34)

1

v  

ev
j0 ev

l0 d

X

v

a pole p yields R

Here the second equality follows from the fact that the contour integral over
p d = 2if(p), and the claim is veried by checking the
. The last equality can be seen from (34) by splitting

denitions of K and G0
up the summation over u and v.

f ()

C

5 Conclusions

We have shown that the canonical family of kernels on graphs are of the form
of power series in the graph Laplacian. Equivalently, such kernels can be char-
acterized by a real valued function of the eigenvalues of the Laplacian. Special
cases include diusion kernels, the regularized Laplacian kernel and p-step ran-
dom walk kernels. We have developed the regularization theory of learning on
graphs using such kernels and explored methods for eciently computing and
approximating the kernel matrix.

Acknowledgments This work was supported by a grant of the ARC. The
authors thank Eleazar Eskin, Patrick Haner, Andrew Ng, Bob Williamson and
S.V.N. Vishwanathan for helpful comments and suggestions.

A Link Analysis

Rather surprisingly, our approach to regularizing functions on graphs bears re-
semblance to algorithms for scoring web pages such as PageRank [Page et al.,
1998], HITS [Kleinberg, 1999], and randomized HITS [Zheng et al., 2001]. More
specically, the random walks on graphs used in all three algorithms and the
stationary distributions arising from them are closely connected with the eigen-
system of L and L respectively.

We begin with an analysis of PageRank. Given a set of web pages and links
between them we construct a directed graph in such a way that pages correspond

14

Alexander Smola and Risi Kondor

to vertices and edges correspond to links, resulting in the (nonsymmetric) matrix
W . Next we consider the random walk arising from following each of the links
with equal probability in addition to a random restart at an arbitrary vertex with
probability . This means that the probability distribution over states follows the
discrete time evolution equation

where D is a diagonal matrix with Dii =P
(cid:2)I + (1  )W D1(cid:3) will determine the stationary distribution p(), and the

j Wij and p is the vector of proba-
bilities of being on a certain page. The PageRank is then determined from the
stationary distribution of p. Clearly the largest eigenvalue/eigenvector pair of

(35)

p(t + 1) =(cid:2)I + (1  )W D1(cid:3) p(t)

contribution of the other eigenvectors decays geometrically (one may conjecture
that in practice only few iterations are needed).
Now consider the same formalism in the context of a 1-step random walk
(11): here one computes aI  L = (a  1)I + D 1
a and
setting  = 1a
a yields a matrix with the same spectrum as the linear dierence
equation (35). Furthermore, for all eigenvectors vi of I + (1  )W D1 we can
nd eigenvectors of aI  L of the form D 1

2 . Rescaling by 1

2 W D 1

The main dierence, however, is that while graphs arising from web pages
are directed (following the direction of the link), which leads to asymmetric W ,
the graphs we studied in this paper are all undirected, leading to symmetric W
and L, L. We can now view the assignment of a certain PageRank to a page,
as achieved via the stationary distribution of the random walk, as a means of
nding a simple function on the graph of web pages.

2 vi.

W > 0

In HITS [Kleinberg, 1999] one uses the concept of hubs and authorities to
obtain a ranking between web pages. Given the graph G, as represented by W ,
one seeks to nd the largest eigenvalue of the matrix M :=
, which
can be shown to be equivalent to nding singular value decomposition of W
[Zheng et al., 2001] (the latter is also used if we wish to perform latent semantic
indexing on the matrix W ). More specically, with {vi, i} being the eigensystem
of W W > (we assume that the eigenvalues are sorted in increasing order), one
uses v2

mj as the weight of page j.

to small perturbations. More specically, they usePm

This setting was modied by Zheng et al. [2001] to accommodate for a larger
subspace (Subspace HITS), which renders the system more robust with respect
ij for some mono-
tonically increasing function g() to assess the relevance of page j. The latter,
however, is identical to the diagonal entry of g(W ). Note the similarity to 7,
where we used an essentially rescaled version of W to determine the complex-
ity of the functions under consideration. More specically, if for regular graphs
r(1/d) we can see that the HITS rank assigned to
of order d we set g() =
pages j is simply the length of the corresponding page in feature space as
given by Kii. In other words, pages with a high HITS rank correspond to unit
vectors which are considered simple with respect to the regularizer induced by
the underlying graph.

i=1 g(i)v2

1

(cid:20) 0 W

(cid:21)

