Abstract

We  present  a  simple  sparse  greedy  technique  to  approximate  the
maximum a  posteriori estimate of Gaussian Processes  with much
improved  scaling  behaviour  in  the  sample  size  m.  In  particular,
computational  requirements  are  O(n2m),  storage  is  O(nm),  the
cost  for  prediction  is  0 ( n)  and  the  cost  to  compute  confidence
bounds  is  O(nm),  where  n  :  m.  We  show  how  to  compute  a
stopping  criterion,  give  bounds  on  the  approximation  error,  and
show applications to large scale problems.

1

Introduction

Gaussian processes have become popular because they allow exact Bayesian analysis
with simple matrix manipulations, yet provide good performance.  They share with
Support Vector machines and Regularization Networks the concept of regularization
via Reproducing Kernel Hilbert spaces [3],  that is, they allow the direct specification
of the smoothness properties of the class of functions under consideration.  However,
Gaussian processes are not always the method of choice for large datasets, since they
involve  evaluations  of the  covariance  function  at  m  points  (where  m  denotes  the
sample size)  in order to carry out inference  at a  single additional point.  This may
be rather costly to implement -
practitioners prefer to use only a  small number of
basis functions  (Le.  covariance function evaluations).

Furthermore,  the  Maximum  a  Posteriori  (MAP)  estimate  requires  computation,
storage,  and inversion of the full  m  x  m  covariance  matrix  Kii  =  k( Xi, Xi)  where
Xl! ... ,xm  are training patterns.  While there exist  techniques  [2,  8]  to reduce  the
computational  cost  of finding  an  estimate  to  O(km2 )  rather  than  O(m3 )  when
the covariance matrix contains  a  significant  number of small eigenvalues,  all  these
methods still require computation and storage of the full  covariance matrix.  None
of these methods addresses the problem of speeding up the prediction stage (except
for  the  rare  case  when  the  integral  operator  corresponding  to  the  kernel  can  be
diagonalized analytically  [8]).

We  devise  a  sparse  greedy  method,  similar  to  those  proposed  in  the  context  of
wavelets  [4],  solutions of linear systems  [5]  or  matrix approximation  [7]  that finds

Supported by the DFG  (Sm 62-1)  and the Australian Research Council.

an approximation of the MAP estimate by expanding it in terms of a  small subset
of kernel  functions  k (Xi, . ).  Briefly,  the  technique  works  as  follows:  given  a  set  of
(already  chosen)  kernel  functions,  we  seek  the  additional  function  that  increases
the posterior probability most.  We  add it to the set  of basis functions  and repeat
until the maximum is approximated sufficiently well.  A similar approach for  a  tight
upper bound on the posterior probability gives  a  stopping criterion.

2  Gaussian Process Regression

Consider a  finite  set  X  =  {Xl.'"  xm}  of inputs.  In  Gaussian  Process  Regression,
we  assume  that  for  any  such  set  there  is  a  covariance  matrix  K  with  elements
Kij  =  k( Xi, Xj).  We  assume  that for  each input X there is  a  corresponding output
y(x),  and that these outputs are generated by
(1)
where  t(x)  and  e are  both  normal  random  variables,  with  e rv  N(O, ( 2 )  and
t  =  (t(Xl), ... , t(xm))T  rv  N(O, K).  We  can  use  Bayes  theorem  to  determine  the
distribution of the output y(x) at a  (new) input x.  Conditioned on the data (X,y),
the output y(x) is normally distributed.  It follows that the mean of this distribution
is the maximum a posteriori probability (MAP) estimate of y.  We  are interested in
estimating this mean,  and also  the variance.

y(x) = t(x) + e

It is possible to give an equivalent parametric representation of y  that is more con(cid:173)
venient for  our purposes.  We may assume that the vector y  = (y(Xl)""  ,y(xm))T
of outputs is generated by
(2)
where  a  rv  N(O, K- 1 )  and e rv  N(O, ( 2 1).  Consequently the  posterior  probability
p(aly, X) over  the latent variables a  is proportional to

y=Ka+e,

exp(-2;21Iy - Ka11 2) exp(-!a TKa)

(3)
and  the  conditional  expectation of y(x)  for  a  (new)  location  X is  E[y(x)ly,X]  =
k T aopt,  where  k T  denotes the vector  (k( Xl. x), ... , k (xm, x))  and aopt  is  the value
of a  that maximizes  (3).  Thus,  it suffices  to compute  aopt  before  any  predictions
are required.  The problem of choosing the MAP estimate of a  is  equivalent to the
problem of minimizing the negative log-posterior,

minimize [-y T Ka + !a T  (a2 K + KT K) a]

aEW"

(4)

(ignoring constant terms and rescaling by ( 2 ).  It is easy to show  that the mean of
the conditional distribution of y(x) is k T (K +(21)-ly, and its variance is k(x, x) +
a 2  - k T (K + ( 21)-lk (see,  for  example,  [2]).

3  Approximate Minimization of Quadratic Forms

For Gaussian process regression, searching for an approximate solution to (4)  relies
on the assumption that a set of variables whose posterior probability is close to that
of the mode of the distribution will be a good approximation for  the MAP estimate.
The following  theorem suggests a simple approach to estimating the accuracy of an
approximate  solution to  (4).  It uses  an idea from  [2]  in  a  modified,  slightly  more
general form.

Theorem 1  (Approximation Bounds  for  Quadratic Forms)  Denote by K  E
lRmxm  a positive semidefinite matrix, y, a  E  lRm  and define  the  two quadratic forms

Q(a) := -y T Ka + _aT (a 2 K + KT K)a,

1
2

(5)

(6)
Suppose Q  and Q*  have  minima Qmin  and Q:nn.  Then for  all a, a*  E  IRffl  we  have

Q*(a) := -y T a  + _aT (a2 1 + K)a.

1
2

Q(a):::::  Qmin

:::::  _~IIYI12 - a 2Q*(a*),

Q*(a*):::::  Q;',.in  :::::a-2(_~IIYI12_Q(a)),
with  equalities  throughout when Q(a) = Qmin  and Q*(a*) = Q;',.in.

(7)

(8)

Hence,  by  minimizing  Q*  in  addition  to  Q  we  can  bound  Q's  closeness  to  the
optimum and vice versa.
Proof  The  minimum of Q(a)  is  obtained for  aopt  =  (K + a 21)-1y  (which  also
minimizes Q*), hence
1  T

Qmin  =  -2"Y  K(K +a 1)  y  and Qmin  =  -2"Y  (K +a 1)  y.

(9)
This allows  us  to  combine  Qmin  and Q;',.in  to Qmin + a 2Q;',.in  =  _~llyI12.  Since  by
definition Q (a)  :::::  Qmin  for  all  a  (and likewise  Q* (a*)  :::::  Q;',.in  for  all a*)  we  may
solve  Qmin + a 2Q;',.in  for  either Q or Q*  to obtain lower bounds for  each of the two
quantities.  This proves  (7)  and  (8).


2 -1

1  T

-1

*

2

Equation  (7)  is  useful  for  computing  an  approximation  to  the  MAP  solution,
whereas  (8) can be used to obtain error bars on the estimate.  To see this, note that
in calculating the variance, the expensive quantity to compute is -k T (K +a21)-1k.
However,  this can be found by solving

minimize [-k T a  + ~a T (a 2 1 + K) a] ,

aEIRm

(10)

and the expression inside the parentheses is  Q*(a) with y  =  k  (see  (6)).  Hence,  an
approximate  minimizer  of (10)  gives  an  upper  bound  on the error bars,  and lower
bounds can be obtained from  (8) .
h
I
.
n  practice we  W1  use  t  e  quantly gap  a, a
.- -Q(a)+u2Q * (a*)+~liYli2  ,I.e.  t  e
relative size of the difference between upper and lower bound as stopping criterion.

*)  .- 2(Q(a)+u  Q*(a*)+2liYli)

11

h

(

.



2

1

2

4  A  Sparse Greedy Algorithm

The  central  idea  is  that  in order  to  obtain a  faster  algorithm,  one  has  to  reduce
the number of free  variables.  Denote by P  E  IRffl xn  with m  :::::  nand m,n E  N  an
extension matrix (Le.  p T  is  a  projection) with p T P  = 1.  We  will make the ansatz
(11)

ap := P[3  where [3  E  IRn

and find solutions [3  such that Q(ap)  (or Q*(ap)) is minimized.  The solution is

(12)
Clearly if Pis ofrank m, this will also be the solution of (4)  (the minimum negative
log posterior for  all a  E IRffl ).  In all other cases,  however,  it is an approximation.

[3opt  = (pT (a2 K  + K T K) p) -1 p T K T y.

Computational Cost  of Greedy Decompositions

For a  given P  E  IRffl xn  let us  analyze  the computational cost  involved in the esti(cid:173)
mation procedures.  To  compute  (12)  we  need to evaluate pT Ky which is  O(nm),
(KP)T(KP) which is O(n2m) and invert an n x n matrix, which is O(n3 ).  Hence the
total cost  is  O(n2m).  Predictions then cost only k T a  which is O(n).  Using P  also
to minimize Q*(P[3*)  costs  no  more  than O(n3 ),  which is  needed  to  upper-bound
the log posterior.

For error bars, we  have to approximately minimize (10)  which can done for  a  = P(3
at O(n3 )  cost.  If we  compute (PKpT)-l beforehand, this can be done by at O(n2 )
and likewise for  upper bounds.  We  have  to minimize  -k T K P(3 + !(3T pT ((72 K  +
KT K)P(3  which  costs  O(n2m)  (once  the  inverse  matrices  have  been  computed,
one may,  however,  use them to compute error bars at different  locations,  too, thus
costing  only  O(n2 )).  The  lower  bounds  on  the  error  bars  may  not  be  so  crucial,
since  a  bad estimate will  only lead to overly  conservative confidence  intervals  and
not  have  any other negative effect.  Finally note that  all  we  ever  have  to compute
and  store  is  K P,  i.e.  the  m  x  n  submatrix  of K  rather  than  K  itself.  Table  1
summarizes the scaling behaviour of several optimization algorithms.

Conjugate

Exact
Optimal Sparse
Solution  Gradient  [2]  Decomposition
Memory
O(m~)
Initialization  O(m;j)
Pred. Mean
g~:~)
Error Bars

O(m~)
O(nm:l)
g~~~2)

Sparse Greedy
Approximation
O(nm)
o (K.n:lm)
O(n)

O(nm)
O(n:lm)
O(n2
O(n2m) or O(n2 )  O(K.n2m) or O(n2 )

Table 1:  Computational Cost of Optimization Methods.  Note that n  <t::  m  and also
note  that  the  n  used  in  Conjugate  Gradient,  Sparse  Decomposition,  and  Sparse
Greedy  Approximation  methods  will  differ,  with  neG  ::;  nSD  ::;  nSGA  since  the
search spaces  are more restricted.  K.  =  60  gives  near-optimal results.
Sparse Greedy Approxhnation

Several  choices  for  P  are  possible,  including  choosing  the  principal  components
of K  [8],  using conjugate gradient descent  to minimize Q [2],  symmetric Cholesky
factorization [1],  or using a sparse greedy approximation of K  [7].  Yet these methods
have the disadvantage that they either do not take the specific form of y  into account
[8,  7]  or lead to expansions that cost O(m) for  prediction and require computation
and storage  of the full matrix [8,  2].
If we require a sparse expansion of y (x)  in terms of k( Xi, x) (i.e. many ai in y = k T a
will be 0)  we  must consider matrices P  that are a  collection of unit vectors ei  (here
(ei)j  = Oij).  We  use  a  greedy  approach to  find  a  good  approximation.  First,  for
n  =  1,  we  choose  P  =  ei  such  that  Q(P(3)  is  minimal.  In  this  case  we  could
permit ourselves to consider all possible indices i  E  {I, ... m} and find  the best one
by  trying  out  all  of them.  Next  assume  that  we  have  found  a  good  solution  P(3
where  P  contains  n  columns.  In  order  to  improve  this  solution,  we  may  expand
P  into  the  matrix  Pnew  :=  [Pold, ei]  E  lRmx (n+1)  and  seek  the  best  ei  such  that
Pnew  minimizes  min,8 Q(Pnew(3).  (Performing  a  full  search  over  all  possible  n + 1
out  of m  indices  would  be  too  costly.)  This  greedy  approach  to  finding  a  sparse
approximate solution is described in Algorithm 1.  The algorithm also maintains an
approximate minimum of Q*,  and exploits the bounds of Theorem 1 to determine
when  the  approximation  is  sufficiently  accurate.  (N ote  that  we  leave  unspecified
how  the  subsets  M  ~ I, M*  ~ I*  are  chosen.  Assume  for  now  that  we  choose
M  =  I, M*  =  I*,  the  full  set  of indices  that  have  not  yet  been  selected.)  This
method  is  very  similar  to  Matching  Pursuit  [4]  or  iterative  reduced  set  Support
Vector  algorithms  [6],  with the difference  that the target to be approximated  (the
full solution a) is only given implicitly via Q( a).

Approximation Quality
Natarajan  [5]  studies  the  following  Sparse  Linear  Approximation problem:  Given
A  E  lRmxn ,  b E  lRm ,  E  >  0,  find  X  E  lRn  with minimal number of nonzero  entries
such that IIAx - bl1 2  ::;  E.

If we  define  A  := (a2K  +  KTK)~ and  b := A-1Ky,  then  we  may  write  Q(o)  =
!llb - Aol1 2 +  c  where  c  is  a  constant  independent  of  o.  Thus  the  problem  of
sparse approximate minimization of Q(o)  is  a  special case  of Natarajan's problem
(where the matrix A  is  square,  symmetric,  and positive definite).  In  addition,  the
algorithm considered by Natarajan in [5J  involves sequentially choosing columns of
A to maximally decrease  IIAx - bll.  This is  clearly equivalent  to the sparse greedy
algorithm  described  above.  Hence,  it  is  straightforward  to  obtain  the  following
result from Theorem 2  in [5J.

Theorem  2  (Approximation Rate)  Algorithm  1  achieves  Q(o)  ::;  Q(oopt) +  E
when a  has

n::;  I8n*~E/4)ln(IIA-1KYII)

).1

E

non-zero components,  where n*(E/4)  is  the  minimal number of nonzero  components
in  vectors  a  for  which  Q(o)  ::;  Q(oopt)  +  E/4,  A  =  (a2K  +  KTK)1/2,  and).l  is
the  minimum of the  magnitudes of the  singular values  of A,  the  matrix obtained by
normalizing the  columns  of A.

Randomized Algorithms  for  Subset  Selection

Unfortunately, the approximation algorithm considered above is still too expensive
for large m since each search operation involves O(m) indices.  Yet, if we are satisfied
with finding a  relatively good  index rather than the  best,  we  may resort to selecting
a  random  subset  of size  K  :  m.  In  Algorithm  1,  this  corresponds  to  choosing
M  ~ I, M*  ~ 1*  as  random subsets  of size  K.  In  fact,  a  constant  value  of K  will
typically  suffice.  To  see  why,  we  recall  a  simple  lemma  from  [7J:  the cumulative
distribution  function  of the  maximum  of m  i.i.d.  random  variables  6, ... ,em  is
FO m ,  where  F()  is  the  cdf of ei.  Thus,  in order  to  find  a  column to  add  to  P
that  is  with  probability  0.95  among  the  best  0.05  of all  such  columns,  a  random
subsample of size  ilogO.05/log0.951  = 59  will suffice.

Algorithm  1  Sparse Greedy Quadratic Minimization.
Require:  Training data X  = {Xl, ... , Xm},  Targets y, Noise a 2, Precision E

Initialize index sets I,1*  = {I, ... ,m}j S, S*  = 0.
repeat

Choose M  ~ I,  M*  ~ I*.
Find arg milliEM Q ([P, eiJ,Bopt),  argmilli"EM" Q*  OP*, ei" J,B~Pt)
Move i  from  I  to S, i*  from 1*  to S*.
Set  P:= [P,eiJ,  P*:=  [P*,ei"J.

until Q(P,Bopt} + a2Q*(P,B~Pt) +  !llyl12  ::;  HIQ(P,Bopt} I +  la2Q*(P,B~Pt) +! IIYl121

Output:  Set of indices S, ,Bopt,  (pTKP)-t, and (pT(KTK +a2K)p)-1.

Numerical Considerations
The crucial part is  to obtain the values of Q(P,Bopt}  cheaply  (with P  =  [Pold, eiJ),
provided we  solved the problem for  Pold.  From (12)  one can see  that all that needs
to be done is a rank-I update on the inverse.  In the following we will show that this
can be obtained in O(mn) operations, provided the inverse of the smaller subsystem
is  known.  Expressing the relevant terms using Pold  and ki  we  obtain

pTKT y

pT (KTK +a2K) P

[Pold,eiJTKT y  = (PoidKT y,kJ y)
Poid  (KT K  + a 2 K) Pold  pJd (KT + a21) ~ 1

[

kJ(K +a21)Pold

kJki + a2Kii

Thus  computation of the  terms costs only O(nm),  given  the values  for  Pold'  Fur(cid:173)
thermore, it is  easy to verify that we  can write the inverse of a  symmetric positive
semidefinite matrix as

(13)

where 'Y  := (C + BT A -1 B)-1.  Hence, inversion of pT (KT K  + a 2 K) P  costs only
O(n2 ).  Thus,  to  find  P  of size  m  x  n  takes  O(ltn2m)  time.  For  the  error  bars,
(p T KP)-1  will  generally  be  a  good  starting  value  for  the  minimization of (10),
so  the typical cost  for  (10)  will  be  O(Tmn)  for  some T  <  n,  rather  than O(mn2 ).
Finally, for added numerical stability one may want to use an incremental Cholesky
factorization in  (13)  instead of the inverse of a  matrix.

5  Experiments and Discussion

We  used the Abalone dataset from the VCI Repository to investigate the properties
of the algorithm.  The dataset is of size 4177, split into 4000 training and 177 testing
split  to  analyze  the  numerical  performance,  and  a  (3000,1177)  split  to  assess  the
generalization error (the latter was needed in order to be able to invert (and keep in
memory) the full  matrix K  + a 2 1 for  a  comparison).  The data was rescaled to zero
mean and unit  variance  coordinate-wise.  Finally,  the gender encoding in Abalone
(male/female/infant) was  mapped into {(I, 0, 0), (0, 1, 0), (0,0, I)}.

In all  our  experiments  we  used  Gaussian  kernels  k(x, x')  =  exp( -~) as  co-
variance kernels.  Figure 1 analyzes the speed of convergence for  different  It.

IIx-x'1I2

Figure  1:  Speed  of  Convergence.
We  plot  the  size  of  the  gap  be(cid:173)
tween upper and lower bound of the
log  posterior  (gap( a, a*))  for  the
first  4000 samples from the Abalone
dataset  (a 2  =  0.1  and  2w 2  =  10).
From top to bottom:  subsets of size
1,  2,  5,  10,  20,  50,  100,  200.  The
results  were  averaged over  10  runs.
The relative variance of the gap size
was  less  than 10%.
One can see that that subsets of size
1O-' O'--------,L----,L------,o'---------"c---,L------'-,-------,-L---,L------'-,------,-!  50  and  above  ensure  rapid  conver-

20

40

60

80

100

120

140

160

180

200

Number of Ilerahons

gence.

For the optimal parameters (2a 2  = 0.1  and 2w2  = 10,  chosen after  [7])  the average
test error of the sparse greedy approximation trained until gap(a, a*) < 0.025 on a
(3000,1177) split (the results were averaged over ten independent choices of training
sets.)  was  1.785   0.32,  slightly  worse  than  for  the  GP  estimate  (1.782    0.33).
The  log  posterior  was  -1.572.105 (1   0.005),  the optimal value  -1.571 . 105 (1  
0.005).  Hence for  all practical purposes full  inversion of the covariance matrix and
the sparse greedy  approximation have  statistically indistinguishable generalization
performance.

In a  third experiment  (Table  2)  we  analyzed the number of basis functions needed
to minimize the log posterior to gap(a, a*) < 0.025,  depending on different choices
of the kernel width a.  In all cases,  less  than 10%  of the kernel functions  suffice  to

find a  good minimizer of the log posterior, for  the error bars, even less than 2% are
sufficient.  This is a  dramatic improvement over  previous techniques.

Kernel width 2w:&
Kernels for  log-posterior  373
Kernels for  error  bars

50
270
7961  4943  2627  1716  129  85

2
287

1

5
255

10
257

20
251

Table  2:  Number  of basis  functions  needed  to  minimize  the  log  posterior  on  the
Abalone dataset  (4000  training samples),  depending on the width of the kernel  w.
Also, number of basis functions required to approximate k T (K + 0-21)- l k  which is
needed to compute the error bars.  We averaged over the remaining 177 test samples.

To ensure that our results were not dataset specific and that the algorithm scales well
we  tested it on a  larger synthetic dataset of size 10000 in 20 dimensions distributed
according to N(O, 1).  The data was generated by adding normal noise with variance
0-2  = 0.1  to a function consisting of 200 randomly chosen Gaussians of width 2w 2  =
40  and normally distributed coefficients and centers.

We  purposely chose  an inadequate  Gaussian process  prior  (but correct noise level)
of Gaussians with width 2w 2  = 10 in order to avoid trivial sparse expansions.  After
500  iterations  (i.e.  after  using  5%  of all  basis  functions)  the size  of the gap(cr, cr)
was less  than 0.023  (note that this problem is too large to be solved exactly).

We  believe that sparse greedy approximation methods are a  key technique to scale
up  Gaussian  Process  regression  to  sample  sizes  of 10.000  and beyond.  The  tech(cid:173)
niques  presented  in  the  paper,  however,  are  by  no  means  limited  to  regression.
Work on the solutions of dense quadratic programs and classification problems is in
progress.  The authors thank Bob Williamson and Bernhard Sch6lkopf.

