Abstract

We introduce binary matrix factorization, a novel model for unsupervised ma-
trix decomposition. The decomposition is learned by tting a non-parametric
Bayesian probabilistic model with binary latent variables to a matrix of dyadic
data. Unlike bi-clustering models, which assign each row or column to a single
cluster based on a categorical hidden feature, our binary feature model reects the
prior belief that items and attributes can be associated with more than one latent
cluster at a time. We provide simple learning and inference rules for this new
model and show how to extend it to an innite model in which the number of
features is not a priori xed but is allowed to grow with the size of the data.

1 Distributed representations for dyadic data

One of the major goals of probabilistic unsupervised learning is to discover underlying or hidden
structure in a dataset by using latent variables to describe a complex data generation process. In this
paper we focus on dyadic data: our domains have two nite sets of objects/entities and observa-
tions are made on dyads (pairs with one element from each set). Examples include sparse matrices
of movie-viewer ratings, word-document counts or product-customer purchases. A simple way to
capture structure in this kind of data is to do bi-clustering (possibly using mixture models) by
grouping the rows and (independently or simultaneously) the columns[6, 13, 9]. The modelling as-

sumption in such a case is that movies come in types and viewers in types and that knowing
componential structure: each item (row) has associated with it an unobserved vector of binary
features; similarly each attribute (column) has a hidden vector of binary features. Knowing the
matrixX into (a distribution dened by) the productUWV>
, whereU andV are binary feature
matrices, andW is a real-valued weight matrix. Below, we develop this binary matrix factorization

the type of movie and type of viewer is sufcient to predict the response. Clustering or mixture
models are quite restrictive  their major disadvantage is that they do not admit a componential or
distributed representation because items cannot simultaneously belong to several classes. (A movie,
for example, might be explained as coming from a cluster of dramas or comedies; a viewer as
a single male or as a young mother.) We might instead prefer a model (e.g. [10, 5]) in which
objects can be assigned to multiple latent clusters: a movie might be a drama and have won an Os-
car and have subtitles; a viewer might be single and female and a university graduate. Inference in
such models falls under the broad area of factorial learning (e.g. [7, 1, 3, 12]), in which multiple
interacting latent causes explain each observed datum.

features of the item and the features of the attribute are sufcient to generate (before noise) the
response at that location in the matrix. In effect, we are factorizing a real-valued data (response)

In this paper, we assume that both data items (rows) and attributes (columns) have this kind of

W

U

(B)

(A)

(B) BMF shown pictorally.

(cid:21)
(cid:26)
vj

J

L

=X

f

Figure 1: (A) The graphical model representation of the linear-Gaussian BMF model. The concen-

(BMF) model using Bayesian non-parametric priors over the number and values of the unobserved
binary features and the unknown weights.

(cid:30),w
(cid:11)
wk
(cid:25)k
V>
ik
xij
(cid:18)
tration parameter and Beta weights for the columns ofX are represented by the symbols(cid:21) and(cid:26).
Binary matrix factorization is a model of an dyadic data matrixX with exchangeable rows
and columns. The entries ofX can be real-valued, binary, or categorial; BMF models suitable
for each type are described below. Associated with each row is a latent binary feature vector	i;
similarly each column has an unobserved binary vectorvj. The primary parameters are represented
by a matrixW of interaction weights.X is generated by a xed observation processf applied
XjU;V;W(cid:24)fUWV>;
where are extra parameters specic to the model variant. Three possible parametric forms for
the noise (observation) distributionf are: Gaussian, with meanUWV>
and covariance1=(cid:18);
logistic, with mean1=1exUWV>; and Poisson, with mean (and variance)UWV>
matricesU;V and the weightsW. We adopt the same priors over binary matrices as previously
described in [5]. For nite sized matricesU with rows and columns, we generate a bias(cid:25)k
independently for each columnk using a Beta prior (denotedB) and then conditioned on this bias
generate the entries in columnk independently from a Bernoulli with mean(cid:25)k.
(cid:11)ja(cid:11);b(cid:11)(cid:24)Ga(cid:11);b(cid:11)
(cid:25)kj(cid:11); (cid:24) B(cid:11)=;(cid:12)
Yi=1Yk=1(cid:25)	ikk
1(cid:25)k1	ik=Yk=1(cid:25)kk1(cid:25)kk
Uj(cid:25) (cid:24)
wherek=i	ik. The hyperprior on the concentration(cid:11) is a Gamma distribution (denotedG),
The biases(cid:25) are easily integrated out, which creates dependencies between the rows, although
they remain exchangeable. The resulting prior depends only on the numberk of active features
in each column. An identical prior is used onV, with rows and columns, but with different
concentration prior(cid:21). The variable(cid:12) was set to1 for all experiments.
The appropriate prior distribution over weights depends on the observation distributionf. For
the linear-Gaussian variant, a convenient prior onW is a matrix normal with prior meanW

.
Other parametric forms are also possible. For illustrative purposes, we will use the linear-Gaussian
model throughout this paper; this can be thought of as a two-sided version of the linear-Gaussian
model found in [5].

(elementwise) to the linear inner product of the features and weights, which is the factorization or
approximation of the data:

To complete the description of the model, we need to specify prior distributions over the feature

whose shape and scale hyperparameters control the expected fraction of zeros/ones in the matrix.

K

I

2 BMF model description

(1)

and

hyperpriors:

3 Inference of features and parameters

terior nor to compute its exact marginals). However, as with many other non-parametric Bayesian
models, we can employ Markov Chain Monte Carlo (MCMC) methods to create an iterative proce-
dure which, if run for sufciently long, will produce correct posterior samples.

3.1 Finite binary latent feature matrices

(2)

only in terms of the binary features. This is true, for example, when we place a Gaussian prior on
the weights and use a linear-Gaussian output process.

exchangeable rows and a potentially innite number of columns (although the expected number of
columns which are not entirely zero remains nite). Such a distribution, the Indian Buffet Process
(IBP) was described by [5] and is analogous to the Dirichlet process and the associated Chinese
restaurant process (CRP) [11]. Fortunately, as we will see, inference with this innite prior is not
only tractable, but is also nearly as efcient as the nite version.

WjW;(cid:30)(cid:24)W;1=(cid:30)
(cid:30)ja(cid:30);b(cid:30)(cid:24)Ga(cid:30);b(cid:30)
(cid:18)ja(cid:18);b(cid:18)(cid:24)Ga(cid:18);b(cid:18)

covariance1=(cid:30). The scale(cid:30) of the weights and output precision(cid:18) (if needed) have Gamma
In certain cases, when the prior on the weights is conjugate to the output distribution modelf, the
weights may be analytically integrated out, expressing the marginal distribution of the dataXjU;V
Remarkably, the Beta-Bernoulli prior distribution overU (and similarlyV) can easily be extended
to the case where!1, creating a distribution over binary matrices with a xed number of
As with many other complex hierarchical Bayesian models, exact inference of the latent variablesU
andV in the BMF model is intractable (ie there is no efcient way to sample exactly from the pos-
The posterior distribution of a single entry inU (orV) given all other model parameters is propor-
from integrating out the biases(cid:25) in the Beta-Bernoulli model and is proportional the number of
single entries ofU (orV) can be done using the following updates:
	ik=1jUik;V;W;X=C(cid:11)=i;kXjUik;	ik=1;V;W
	ik=0jUik;V;W;X=C(cid:12)1i;kXjUik;	ik=0;V;W (3)
wherei;k=h6=i	hk,Uik excludes entryik, andC is a normalizing constant. (Conditioning
on(cid:11); and(cid:18) is implicit.) When conditioning onW, we only need to calculate the ratio of likeli-
hoods corresponding to rowi. (Note that this is not the case when the weights are integrated out.)
This ratio is a simple function of the models predictions^xij=h	ihvjwh (when	ik=1) and
^xij=h	ihvjwh (when	ik=0). In the linear-Gaussian case:
(cid:12)1i;k12Xj(cid:18)ijxij^xij2xij^xij2
g	ik=1jUik;V;W;X
(cid:11)=i;k
	ik=0jUik;V;W;X=g
W and hyperparameters. To simplify the presentation, we consider a vectorized representation of
our variables. Letx be an column vector taken column-wise fromX,w be a column vector
taken column-wise fromW andA be a binary matrix which is the kronecker product
V(cid:10)U. (In Matlab notation,x=X:;w=W: andA=kV;U.) In this notation, the
data distribution is written as:xjA;w;(cid:18)(cid:24)Aw;1=(cid:18). Given values forU andV, samples
can be drawn forw,(cid:30), and(cid:18) using the following posterior distributions (where conditioning on
w;(cid:30);(cid:18);a(cid:30);b(cid:30);a(cid:18);b(cid:18) is implicit):
wjx;A(cid:24)(cid:16)(cid:18)A>A(cid:30)1(cid:18)A>x(cid:30)w;(cid:18)A>A(cid:30)1(cid:17)

tional to the product of the conditional prior and the data likelihood. The conditional prior comes

active entries in other rows of the same column plus a term for new activations. Gibbs sampling for

In the linear-Gaussian case, we can easily derive analogous Gibbs sampling updates for the weights

set A

set B

0
0
0
0
0
0
0
0

get:

Let set A have at least one non-zero entry

columns, including the set of columns where

3.2 Innite binary latent feature matrices

and the countably innite number of all-zero
columns. Sampling values for elements in row

One of the most elegant aspects of non-parametric Bayesian modeling is the ability to use a prior
which allows a countably innite number of latent features. The number of instantiated features is
automatically adjusted during inference and depends on the amount of data and how many features
it supports. Remarkably, we can do MCMC sampling using such innite priors with essentially no

(cid:30)jw(cid:24)G(cid:18)a(cid:30)=2;(cid:18)b(cid:30)12ww>ww(cid:19)(cid:19)
(cid:18)jx;A;w(cid:24)G(cid:18)a(cid:18)=2;(cid:18)b(cid:18)12xAw>xAw(cid:19)(cid:19)
Note that we do not have to explicitly compute the matrixA. For computing the posterior of linear-
Gaussian weights, the matrixA>A can be computed asA>A=kV>V;U>U. Similarly,
the expressionA>x is constructed by computingU>XV and taking the elements column-wise.
computational penalty over the nite case. To derive these updates (e.g. for rowi of the matrixU),
it is useful to consider partitioning the columns ofU into two sets as shown below.
in rows other thani. Let set B be all other

the only non-zero entries are found in rowi



rowi
i of set A given everything else is straightfor-



tions (2) and (3); as!1 andk in set A we
	ik=1jUik;V;W=Ci;kXjUik;	ik=1;V;W
	ik=0jUik;V;W=C(cid:12)1i;kXjUik;	ik=0;V;W
interested in the number of entries?B in set B which will be turned on in rowi. Sampling
the number of entries set to1 can be done with Metropolis-Hastings updates. Let?BjB=
Poisson?Bj(cid:11)=(cid:12)1 be the proposal distribution for a move which replaces the currentB
active entries with?B active entries in set B. The reverse proposal isBj?B. The acceptance
probability isi1;B!?B
, whereB!?B is
XjB PoissonBj(cid:11)=(cid:12)1?BjB=Xj?B
BjX?BjB=Xj?B Poisson?Bj(cid:11)=(cid:12)1Bj?B
?BjXBj?B
XjB
This assumes a conjugate situation in which the weightsW are explicitly integrated out of the
model to compute the marginal likelihoodXj?B. In the non-conjugate case, a more compli-
cated proposal is required. Instead of proposing?B, we jointly propose?B and associated feature
parametersw?B from their prior distributions. In the linear-Gaussian model, wherew?B is a set of
?B;w?BjB;wB= Poisson?Bj(cid:11)=(cid:12)1 Normalw?Bj?B;(cid:30)
We need actually sample only the nite portion ofw?B where	ik=1. As in the conjugate case, the
B;wB!?B;w?B=Xj?B;w?B
XjB;wB
The Gibbs updates described above for the entries ofU,V andW are the simplest moves we could

When sampling new values for set B, the columns are exchangeable, and so we are really only

ward, and involves Gibbs updates almost iden-
tical to those in the nite case handled by equa-

weights for features in set B, the proposal distribution is:

acceptance ratio reduces to the ratio of data likelihoods:

0
0
0
0
1
0
0
0

0
0
0
0
0
0
0
0

0
0
0
0
1
0
0
0

0
0
0
0
0
0
0
0

3.3 Faster mixing transition proposals

0
0
1
1
1
0
0
1

1
0
1
0
1
1
0
0

0
1
0
0
0
0
0
0

0
0
0
1
0
0
1
0

1
0
1
1
1
0
0
1

(4)
(5)

(6)

(7)

(8)

make in a Markov Chain Monte Carlo inference procedure for the BMF model. However, these

limited local updates may result in extremely slow mixing. In practice, we often implement larger
moves in indicator space using, for example, Metropolis-Hastings proposals on multiple features

and compute the probability under the conditional prior of proposing the current conguration. The
acceptance probability of such a proposal is (the maximum of unity and) the ratio of likelihoods
between the new proposed conguration and the current conguration.

Split-merge moves may also be useful for efciently sampling from the posterior distribution of
the binary feature matrices. Jain and Neal [8] describe split-merge algorithms for Dirichlet process
mixture models with non-conjugate component distributions. We have developed and implemented
similar split-merge proposals for binary matrices with IBP priors. Due to space limitations, we

random. If they are in the same column, we propose splitting that column; if they are in different
columns, we propose merging their columns. The key difference between this algorithm and the Jain
and Neal algorithm is that the binary features are not constrained to sum to unity in each row. Our

A major reason for building generative models of data is to be able to impute missing data values
given some observations. In the linear-Gaussian model, the predictive distribution at each iteration
of the Markov chain is a Gaussian distribution. The interaction weights can be analytically integrated
out at each iteration, also resulting in a Gaussian posterior, removing sampling noise contributed by
having the weights explicitly represented. Computing the exact predictive distribution, however,
conditional only on the model hyperparameters, is analytically intractable: it requires integrating

for rowi simultaneously. For example, we can propose new values for several columns in rowi
of matrixU by sampling feature values independently from their conditional priors. To compute
the reverse proposal, we imagine forgetting the current conguration of those features for rowi
present here only a sketch of the procedure. Two nonzero entries inU are selected uniformly at
split-merge algorithm also performs restricted Gibbs scans on columns ofU to increase acceptance
over all binary matricesU andV, and all other nuisance parameters (e.g., the weights and preci-
By averaging predictive distributions, our algorithm implicitly integrates overU andV.
experiments, we show samples from the posteriors ofU andV to help explain what the model is
will depend, for example, on the current value of(cid:11) and(cid:21) (higher values will result in more features)
tures. Data consists of vectors of size82
The generation process is as follows: sinceV has the same number of rows as the dimension of the
images,V is xed to be a set of vertical and horizontal bars (when reshaped into an image).U is
sampled from the IBP, and global precisions(cid:18) and(cid:30) are set to1=2. The weightsW are sampled
from zero mean Gaussians. Model estimates ofU andV were initialized from an IBP prior.
the expected reconstruction using MCMC samples ofU,V, andW. Despite the relatively high

A toy problem commonly used to illustrate additive feature or multiple cause models is the bars
problem ([2, 12, 1]). Vertical and horizontal bars are combined in some way to generate data sam-
ples. The goal of the illustration is to show recovery of the latent structure in the form of bars. We
have modied the typical usage of bars to accommodate the linear-Gaussian BMF with innite fea-
where each vector can be reshaped into a square image.

sions). Instead we integrate over these parameters implicitly by averaging predictive distributions
from many MCMC iterations. This posterior, which is conditional only on the observed data and hy-
perparameters, is highly complex, potentially multimodal, and non-linear function of the observed
variables.

In Figure 2 we demonstrate the performance of the linear-Gaussian BMF on the bars data. We train
the BMF with 200 training examples of the type shown in the top row in Figure 2. Some examples
have their bottom halves labeled missing and are shown in the Figure with constant grey values. To
handle this, we resample their values at each iteration of the Markov chain. The bottom row shows

doing, but we stress that the posterior may have signicant mass on many possible binary matrices.
The number of features and their degrees of overlap will vary over MCMC iterations. Such variation

probability.

3.4 Predictions

In our

and precision values (higher weight precision results in less variation in weights).

4 Experiments

4.1 Modied bars problem

noise levels in the data, the model is able to capture the complex relationships between bars and
weights. The reconstruction of vertical bars is very good. The reconstruction of horizontal bars is
good as well, considering that the model has no information regarding the existence of horizontal
bars on the bottom half.

(A) Data samples

(B) Noise-free data

Based solely on the information in the top-half of the original data, these are the noise-free nearest

(C) Initial reconstruction

(D) Mean reconstruction

(E) Nearest neighbour

Figure 2: Bars reconstruction. (A) Bars randomly sampled from the complete dataset. The bottom
half of these bars were removed and labeled missing during learning. (B) Noise-free versions of the
same data. (C) The initial reconstruction. The missing values have been set to their expected value,

0, to highlight the missing region. (D) The average MCMC reconstruction of the entire image. (E)
VW>
neighbours in pixel space.V
Figure 3: Bars features. The top row shows values ofV andWV>
second row shows a sample ofV andWV>
from the Markov chain.WV>
set of basis images which can be added together with binary coefcients (U) to create images.
In Figure 3 we show the generating, or true, values ofV andWV>
features from the Markov chain. Because the model is generated by adding multipleWV>
are fairly similar to the generatingWV>
captured features. The learnedWV>
composed of overlapping bar structure (learnedV).
We train logistic BMF with 100 examples each of digits1,2, and3 from the USPS dataset. In
show the mean and mode (xij=1>0:5) of the BMF reconstruction. In the bottom row we
the average image of the data which have each feature inU on. It is clear that some row features
have distinct digit forms and others are overlapping. In row G, the basis imagesWV>
By adjusting the features that are non-zero in each row ofU, images are composed by adding basis
images together. Finally, in row H we showV. These pixel features mask out different regions in

By examining the features captured by the model, we can understand the performance just described.
along with one sample of those
basis
images shown on the right of Figure 3, multiple bars are used in each image. This is reected in the
, but the former are

the rst ve rows of Figure 4 we again illustrate the ability of BMF to impute missing data values.
The top row shows all 16 samples from the dataset which had their bottom halves labeled missing.
Missing values are lled-in at each iteration of the Markov chain. In the third and fourth rows we

In Section 2 we briey stated that BMF can be applied to data models other than the linear-Gaussian
model. We demonstrate this with a logistic BMF applied to binarized images of handwritten digits.

used to generate the data. The
can be thought of as a

4.2 Digits

have shown the nearest neighbors, in pixel space, to the training examples based only on the top
halves of the original digits.

In the last three rows of Figure 4 we show the features captured by the model. In row F, we show

are shown.

pixel space, which are weighted together to create the basis images. Note that there are features
in rows F and G, and features in row H.

(A)

(B)

(C)

(D)

(E)

(F)

(G)

(H)

represents a bias feature.

4.3 Gene expression data

(A) Digits randomly sampled from the complete dataset. The
Figure 4: Digits reconstruction.
bottom half of these digits were removed and labeled missing during learning. (B) The data shown
to the algorithm. The top half is the original data value. (C) The mean of the reconstruction for
the bottom halves. (D) The mode reconstruction of the bottom halves. (E) The nearest neighbours
of the original data are shown in the bottom half, and were found based solely on the information

from the top halves of the images. (F) The average of all digits for eachU feature. (G) The feature
WV>
reshaped in the form of digits. By adding these features together, which theU features do,
reconstructions of the digits is possible. (H)V reshaped into the form of digits. The rst image
Bayesian special case of our model in which the matrixW is diagonal and the number of bi-
of the data and its expected reconstruction are ordered such that contiguous regions inX were ob-
ing BMF to model gene expression data would be to x certain columns ofU orV with knowledge

nary features is xed. Our goal in this experiment is merely to illustrate qualitatively the ability
of BMF to nd multiple clusters in gene expression data, some of which are overlapping, others
non-overlapping. The data in this experiment consists of rows corresponding to genes and columns
corresponding to patients; the patients suffer from one of two types of acute Leukemia [4]. In Figure
5 we show the factorization produced by the nal state in the Markov chain. The rows and columns

Gene expression data is able to exhibit multiple and overlapping clusters simultaneously; nding
models for such complex data is an interesting and active research area ([10], [13]). The plaid
model[10], originally introduced for analysis of gene expression data, can be thought of as a non-

servable. Some of the many feature pairings are highlighted. The BMF clusters consist of broad,
overlapping clusters, and small, non-overlapping clusters. One of the interesting possibilities of us-

gained from experiments or literature, and to allow the model to add new features that help explain
the data in more detail.

5 Conclusion
We have introduced a new model, binary matrix factorization, for unsupervised decomposition of
dyadic data matrices. BMF makes use of non-parametric Bayesian methods to simultaneously dis-
cover binary distributed representations of both rows and columns of dyadic data. The model ex-
plains each row and column entity using a componential code composed of multiple binary latent
features along with a set of parameters describing how the features interact to create the observed
responses at each position in the matrix. BMF is based on a hierarchical Bayesian model and can be
naturally extended to make use of a prior distribution which permits an innite number of features,
at very little extra computational cost. We have given MCMC algorithms for posterior inference
of both the binary factors and the interaction parameters conditioned on some observed data, and

Figure 5: Gene expression results. (A) The top-left isX sorted according to contiguous features in
the nalU andV in the Markov chain. The bottom-left isV>
and the top-right isU. The bottom-
right isW. (B) The same as (A), but the expected value ofX,^X=UWV>
regions that have both	ik andvj on. For clarity, we have only shown the (at most) two largest

. We have highlighted

contiguous regions for each feature pair.

(A)

(B)

demonstrated the models ability to capture overlapping structure and model complex joint distribu-
tions on a variety of data. BMF is fundamentally different from bi-clustering algorithms because of
its distributed latent representation and from factorial models with continuous latent variables which
interact linearly to produce the observations. This allows a much richer latent structure, which we
believe makes BMF useful for many applications beyond the ones we outlined in this paper.

