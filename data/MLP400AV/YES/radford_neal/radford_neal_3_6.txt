Abstract

We describe two slice sampling methods for taking multivariate steps using the crumb framework. These

methods use the gradients at rejected proposals to adapt to the local curvature of the log-density surface,

a technique that can produce much better proposals when parameters are highly correlated. We evaluate

our methods on four distributions and compare their performance to that of a non-adaptive slice sampling

method and a Metropolis method. The adaptive methods perform favorably on low-dimensional target

distributions with highly-correlated parameters.

1

Introduction

Markov Chain Monte Carlo methods often perform poorly when parameters are highly correlated. Our goal

has been to develop MCMC methods that work well on such distributions without requiring prior knowledge

about the distribution or extensive tuning runs.

Slice sampling (Neal, 2003) is an auxiliary-variable MCMC method based on the idea of drawing points

uniformly from underneath the density surface of a target distribution. If one discards the density coordinate

from the sample, the resulting marginal distribution is the target distribution.

This document presents two samplers in the crumb framework (Neal, 2003, 5.2), a general framework
for slice sampling methods that take multivariate steps. Unlike many MCMC samplers, they perform well

when the target distribution has highly correlated parameters. These methods can improve on univariate

slice sampling in the same way Metropolis with a properly chosen multivariate proposal distribution can

improve on Metropolis with a spherical proposal distribution and on Metropolis updates of one coordinate

at a time.

2 Multivariate Steps in the Crumb Framework

This section describes the crumb framework for taking multivariate steps in slice sampling. The overall

goal is to sample from a target distribution, such as the one with log-density contours shown in gure 1(a).

The dimension of the target distributions parameter space is denoted by p. (The example in the gure has

1

(a)

(b)

Figure 1: (a) Contours of a target distribution. (b) A slice, Sy0 , and the current target component, x0.

p = 2.) The state space of the Markov chain for slice sampling has dimension p + 1, of which p correspond

to the target distribution and one to the current slice level.

Slice sampling alternates between sampling the target coordinates and the density coordinate. Let f (x)
be proportional to the density function of the target distribution, and let (x0, y0) be the current state in
the augmented sample space, where x0  Rp and y0  R. To update the density component, y0, we sample
uniformly from [0, f (x0)]. Updating the target component, x0, is more dicult. Let Sy0 be the slice through
the distribution at level y0:

Sy0 = {x : f (x)  y0}

(1)

The set Sy0 is outlined by a thick line in gure 1(b). The diculty in updating the target component is that
we rarely have an analytic expression for the boundary of Sy0, which may not even be a connected curve, so
we cannot sample uniformly from Sy0 as we would like to. The methods of this document instead perform
updates that leave the uniform distribution on Sy0 invariant, leaving the resulting chain with the desired
stationary distribution.

These methods begin by sampling a crumb, c1, from a distribution that depends on x0, then drawing
a proposal, x1, from the distribution of points that could have generated c1, treating the crumb as data and
x0 as an unknown. An example crumb and proposal are drawn in gure 2(a), with the distribution of c1
shown as a dashed line and the distribution of x1 shown as a dotted line.

If x1 were inside Sy0, we would accept it as the new value for the target component of the state. In
gure 2(a), it is not, so we draw a new crumb, c2, and draw a new proposal, x2, from the distribution of
x0 given both c1 and c2 as data. This is plotted in gure 2(b). While the distribution of proposed moves is
determined by the crumbs and their distributions, we can choose the distribution of the crumbs arbitrarily,

using the densities and gradients at rejected proposals if we desire.

In the example of gure 2(b), x2 is in Sy0, so we accept x2 as the new target component. If it were
not, we would keep drawing crumbs, adapting their distributions so that the proposal distribution would
be as close as possible to uniform sampling over Sy0, and metaphorically following these crumbs back to x0
by drawing proposals from the distribution of x0 conditional on having observed the crumbs (Grimm and
Grimm, 2008).

2

x0l(a)

(b)

Figure 2: (a) The rst crumb, c1, and a proposal, x1. The distribution of c1 is shown as dashed. The
distribution of x1 is shown as dotted.
In this gure and subsequent ones, probability distributions in a
two-dimensional space are represented by ellipses such that a uniform distribution over the ellipse has the
same mean and covariance as the represented distribution. This method allows multiple distributions to be
drawn in the same plot. (b) The second crumb, c2, and a proposal, x2. The distribution of c2 is shown
as dashed. The distribution of x2 is shown as dotted. The distribution for c2 has been updated using the
method described in section 4.

Neal (2003, 5.2) has more information on the crumb framework, including a proof that methods in this

framework leave the target distribution invariant.

3 Overview of Adaptive Gaussian Crumbs

We now specialize the crumb framework to Gaussian crumbs and proposals, using log densities at proposals

and their gradients to choose the crumb covariances. Without violating detailed balance, the crumb distri-

bution can depend on these log densities and gradients. We assume that while computing the log density at

a proposal, we can compute its gradient with minimal additional cost.

Ideally, the proposal distribution would be a uniform distribution over Sy0 . To approximate uniform
sampling over Sy0, we attempt to draw a sequence of crumbs that results in a Gaussian proposal distribution
with the same covariance as a uniform distribution over the slice.

In both adaptive methods discussed in this document, the rst crumb has a multivariate Gaussian

distribution:

c1  N (x0, W 1

1

) where W1 = 2
c I

(2)

The standard deviation of the initial crumb, c, is the only tuning parameter for either method that is
modied in normal use. The distribution for x0 given c1 is a Gaussian with mean c1 and precision matrix
W1, so we draw a proposal from this distribution:

x1  N (c1, W 1

1

)

(3)

If f (x1) is at least y0, then x1 is inside the slice, so we accept x1 as the target component of the next state
of the chain. This update leaves the density component, y0, unchanged, though it is usually forgotten after

3

x0lc1lx1lx0lc2lx2la proposal is accepted, anyway.

When the proposal is not in the slice, we choose a dierent covariance matrix, Wk+1, for the next crumb,
so that the covariance of the next proposal will look more like that of uniform sampling over Sy0. The two
methods proposed in this document dier in how they make that choice; sections 4 and 5 describe the details.
After sampling k crumbs from Gaussians with mean x0 and precision matrices (W1, . . . , Wk), the distri-

bution for the kth proposal (the posterior for x0 given the crumbs) is:

xk  N (ck, 1
k )

where k = W1 +  + Wk

and ck = 1

k (W1c1 +  + Wkck)

(4)

(5)

(6)

If xk  Sy0 that is, if f (xk)  y0we accept xk as the target component. Otherwise, we must choose
a covariance for the distribution of (k + 1)th crumb, draw a new proposal, and repeat until a proposal is

accepted.

4 First Method: Matching the Slice Covariance

In the method described in this section, we attempt to nd Wk+1 so that the (k + 1)th proposal distribution
has the same conditional variance as uniform sampling from Sy0 in the direction of the gradient of log f ()
at xk. This gradient is a good guess at the direction in which the proposal distribution is least like Sy0 .
Figure 3(a) shows an example of this. We plotted the gradients of the log density at thirty points drawn

from the same distribution (shown as a dotted line) as a rejected proposal; most of these gradients point in

the direction where the proposal variance is least like the slice. Generally, in an ill-conditioned distribution,

these gradients do not point towards the mode, they point towards the nearest point on the slice.

(In

a well-conditioned distribution, the directions to the mode and to the nearest point on the slice will be

similar.)

4.1 Choosing Crumb Covariances

To compute Wk+1, we will estimate the second derivative of the target distribution in the direction of the
gradient, assuming the target distribution is approximately locally Gaussian. Consider the (approximately)

parabolic cut through the log-density surface in gure 3(b), which has the equation:

(cid:96) =  1
2

t2 + t + 

(7)

t is a parameter that is zero at xk and increases in the direction of the gradient; , , and  are unknown
constants we wish to estimate. The coecient  1
2 is arbitrary; this choice makes  equal to the negative of
the second derivative of the parabola. We already had to compute f (xk) so that we could determine whether
xk was in Sy0; we assume that  log f (xk) was computed simultaneously. To x two degrees of freedom of

4

(a)

(b)

(c)

Figure 3: (a) shows gradients at thirty points drawn from the rst proposal distribution (shown as a dotted
line). These gradients tend to point towards the slice (shown as a thick line). (b) shows a parabolic cut
through the log-density surface that goes through a rejected proposal, x1, in the direction of the gradient at
x1, shown as an arrow. u1 is a point on the cut dened by equation 12. (c) adds a parabolic cut through
the mode in the same direction. These two parabolas have approximately the same second derivative. The
distance between the two arms of the second parabola when the vertical coordinate is equal to log y0 is shown
as a double-ended arrow, equal to d in equation 16.

the parabola, we plug these quantities into equation 7 and its derivative with respect to t:

log f (xk) =  1
2

  02 +   0 +  = 

(cid:107) log f (xk)(cid:107) =   0 +  = 

(8)

(9)

We still have one degree of freedom left, so we must evaluate log f () at another point on the parabola.
We choose a point as far away from xk as ck is, hoping that this distance is within the range where the
distribution is approximately Gaussian. Let  be this distance:

Let g be the normalized gradient at xk:

 = (cid:107)xk  ck(cid:107)

g =

 log f (xk)
(cid:107) log f (xk)(cid:107)

Then, the point uk is dened to be  away from xk in the direction g:

uk = xk +   g

(10)

(11)

(12)

In equation 7, uk corresponds to t = . We evaluate the density at uk to x the parabolic cuts third degree
of freedom, and plug this into equation 7:

log f (uk) =  1
2

2 +  + 

(13)

5

x0lx1lx0lx1lu1lx0lx1lu1lSolving equations 8, 9, and 13 for  gives:

(cid:0)log f (uk)  log f (xk)  (cid:107) log f (xk)(cid:107)  (cid:1)

 =  2
2

(14)

We can now use  to approximate the conditional variance in the direction g. If the Hessian is locally
approximately constant, as it is for Gaussians, a cut through the mode in the direction of  log f (xk) would
have the same second derivative as the cut through xk. This second parabola, shown in gure 3(c), has the
equation:

t2 + M

(15)

M is the log density at the mode. We set t = 0 at the mode, so there is no linear term. For now, assume
f () is unimodal and that M was computed with the conjugate gradient method (Nocedal and Wright, 2006,
ch. 5) or some other similar procedure before starting the Markov chain. We solve equation 15 for the
parabolas diameter d at the level of the current slice, log y0, shown as a doubled-ended arrow in gure 3(c):

(cid:96) =  1
2

(cid:114)

d =

8(M  log y0)



(16)

Since the distribution of points drawn from an ellipsoidal slice, conditional on their lying on that particular
one-dimensional cut, is uniform with length d, the conditional variance in the direction of the gradient at xk
is:

2
k+1 =

d2
12

=

2
3

 M  log y0



With this variance, we can construct a crumb precision matrix that will lead to the desired proposal
precision matrix. We want to draw a crumb ck+1 so that the posterior of x0 given the k crumbs has a
variance equal to 2
k+1 in the direction g. Using equation 5, the precision of the proposal given these crumbs
is:

k+1 = k + Wk+1

(18)

If we multiply both sides of equation 18 by gT on the left and g on the right, the left side is the conditional
precision in the direction g.

We would like to choose Wk+1 so that this conditional precision is 2
equation 19 with that:

k+1, so we replace the left hand side of

(17)

(19)

(20)

(cid:1)g
gT k+1g = gT(cid:0)k + Wk+1
(cid:1)g
k+1 = gT(cid:0)k + Wk+1

2

As will be discussed in section 4.3, computation will be particularly easy if we choose Wk+1 to be a scaled
copy of k with a rank-one update, so we choose Wk+1 to be of the form:

Wk+1 = k + ggT

(21)

 and  are unknown scalars.  controls how fast the precision as a whole increases. If we would like the

variance in directions other than g to shrink by 9/10, for example, we choose  = 1/9. Since  is constant,

there will be exponentially fast convergence of the proposal covariance in all directions, which allows quick

6

recovery from overly diuse initial crumb distributions. For this method,  is not a critical choice;  = 1 is

reasonable. Substituting equation 21 into equation 20 gives:

k+1 = gT(cid:0)k + k + ggT(cid:1)g

2

Noting that gT g = 1, we solve for , and set:

 = max{2

k+1  (1 + )gT kg, 0}

(22)

(23)

 is restricted to be positive to guarantee positive deniteness of the crumb covariance. (By choosing 

simultaneously, we could perhaps encounter this restriction less frequently, but we have not explored this.)
Once we know , we then compute Wk+1 using equation 21.

The resulting crumb distribution is:

ck+1  N (x0, W 1
k+1)

(24)

After drawing such a crumb, we draw a proposal according to equations 4 to 6, accepting or rejecting
depending on whether f (xk+1)  y0, drawing more crumbs and adapting until a proposal is accepted.

4.2 Estimating the Density at the Mode

We now modify the method to remove the restriction that the target distribution be unimodal and remove
the requirement that we precompute the log density at the mode. Estimating M each time we update
x0 instead of precomputing it allows M to take on values appropriate to local modes. Since the proposal
distribution only approximates the slice even in the best of circumstances, it is not essential that the estimate

of M be particularly good.

To estimate M , we initialize M to the slice level, log y0, before drawing the rst crumb. Then, every
time we t the parabola described by equation 7, we update M to be the maximum of the current value of

M and the estimated peak of the parabola. As more crumbs are drawn, M becomes a better estimate of
the local maximum. We could also use the values of the log density at rejected proposals and at the {uk} to
bound M , but if the log density is locally concave, the log densities at the peaks of the parabola will always

be larger than these values.

4.3 Ecient Computation of the Crumb Covariance

This section describes a method for using Cholesky factors of precision matrices to make implementation of

the covariance-matching method of section 4.1 ecient. If implemented naively, the method of section 4.1
would use O(p3) operations when drawing a proposal with equations 4 and 6. We would like to avoid this.
One way is to represent Wk and k by their upper-triangular Cholesky factors Fk and Rk, where F T
k Fk = Wk
and RT

k Rk = k.

First, we must draw proposals eciently. If z1 and z2 are p-vectors of standard normal variates, we can

7

replace the crumb and proposal draws of equations 24 and 4 with:

ck = x0 + F 1
k z1
xk = ck + R1
k z2
Since Cholesky factors are upper-triangular, evaluation of F 1
k z1 and R1
O(p2) operations.

(25)

(26)

k z2 by backward substitution takes

We must also update the Cholesky factors eciently. We replace the updates of Wk and k in equations 21

and 18 with:




Fk+1 = chud(

Rk+1 = chud(



Rk,

1 + Rk,

g)


g)

(27)

(28)

Here, chud(R, v) is the Cholesky factor of RT R + vvT . The function name is an abbreviation for Cholesky
update. It can be computed with the LINPACK routine DCHUD, which uses Givens rotations to compute
the update in O(p2) operations (Dongarra et al., 1979, ch. 10).

Finally, we would like to compute the proposal mean eciently. We do this by keeping a running sum of
the un-normalized crumb mean (the parenthesized expression in equation 6), which we will represent by c
k.
Dene:

k = W1c1 +  + Wkck
c
= c
= c

k1 + Wkck
k1 + F T

k Fkck

Then, using forward and backward substitution, we can compute the normalized crumb mean, ck, as:

ck = R1

k RT

k c

k

(29)

(30)

(31)

(32)

This way, we can compute ck in O(p2) operations and do not need to save all the crumbs and crumb
covariances.

With these changes, the resulting algorithm is numerically stable even with ill-conditioned target dis-
tributions. Each crumb and proposal draw takes O(p2) operations. Figure 4 shows pseudocode for this
method.

5 Second Method: Shrinking the Rank of the Crumb Covariance

The method of section 4 attempts to adapt the crumb distribution so that the proposal distribution matches
the shape of the slice. However, it often cant due to positive-deniteness constraints, requiring the max{,}
operation in equation 23. Even when it can perform the adaptation, it may not be appropriate if the

underlying distribution is not approximately Gaussian.

This section describes a dierent method, also based on the framework of section 3. Instead of attempting

to match the conditional variance in the direction of the gradient, it just sets it to zero. This is reasonable

8

Slice Sampling with Covariance Matching

Initialize N , f , x0  Rp, c, and .
X  ( )
repeat N times:

M  log f (x0)
e  draw from Exponential(1)
y0  M  e
R  1
c I
F  1
c I
c  0
repeat until a proposal is accepted:

z  draw from Np(0, I)
c  x0 + F 1z
c  c + F T F c
c  R1RT c
z  draw from Np(0, I)
x  c + R1z
y  log f (x)
if y  y0:

proposal accepted, break

end (if)
G   log f (x)
g  G/(cid:107)G(cid:107)
  (cid:107)x  c(cid:107)
u  x + g
(cid:96)u  log f (u)
  22 ((cid:96)u  y  (cid:107)G(cid:107))
(cid:96)x,u  1
M  max{M, (cid:96)x,u}
2  2

 + y

My0

(cid:107)G(cid:107)2

2

3



  max(cid:8)0, 2  (1 + )gT RT Rg(cid:9)
(cid:16)
R  chud(cid:0)

(cid:17)
g(cid:1)

F  chud

1 + R,

g


R,



end (repeat)
append x to X.
x0  x
end (repeat)
return X

Figure 4: This gure shows the adaptive algorithm of section 4. The variables are mostly the same as in the
text, with a few exceptions: To avoid underow, we set y = log y and y0 = log y0; see Neal (2003, 4) for
discussion. The k subscript is dropped since there is no need to keep copies of the values from any but the
most recent iteration. The variable (cid:96)x,u indicates the peak of the parabolic cut through x and u; N indicates
the number of samples to draw. The generated samples are stored in the ordered set X.

9

in that the gradient at a proposal probably points in a direction where the variance is small, so it is more

ecient to move in a dierent direction.

With this method, the crumb covariance is zero in some directions and spherically symmetric in the rest,

so its simplest representation is the pair (w, J), where J is a matrix of orthonormal columns of directions in
which the conditional variance is zero, and w2 is the variance in the other directions.

Dene P (J, v) to be the function that returns the component of vector v orthogonal to the columns of J:

v  JJ T v

v

P (J, v) =

if J has at least one column

if J has no columns

(33)

For simplicity of computation, since the crumbs are located in a common subspace with x0, this method will
consider the origin to be at x0 except when calling f (), which is provided by the user, and when returning
samples to the user. Each crumb is drawn by:

ck = c P (J, z)

where z  Np(0, I)

(34)

When the rst crumb is drawn, J has no columns, so P (J, z) = z and the rst crumb has the distribution

specied in section 3.

Given k crumbs and the covariances of their distributions, we know that x0 must lie in the intersection
of the subspaces of their covariances. Since the subspace cj is drawn from contains the subspace ck is
drawn from when k > j, this is equivalent to saying that x0 must lie in the subspace of cks covariance, the
orthogonal complement of J. So, the precision of the posterior for x0 in the direction of columns of J is
innite. It is equal to k2
in all other directions, since there are k crumbs, each with spherical variance
equal to 2
c in the subspace they were drawn in. The mean of the proposal distribution (with origin x0) is
the projection of:

c

c c1 +  + 2
2
c ck

c =
= k1 (c1 +  + ck)

k2

c

(35)

Therefore, to draw a proposal, we draw a vector of standard normal variates, project it into the orthogonal
complement of J, scale by c/
original origin, the proposal is:

k, and add c, also projected into the orthogonal complement of J. With the



xk = x0 + P (J, c) + (c/



k)  P (J, z)

where z  Np(0, I)

(36)

If the proposal is in the slice (that is, if f (xk)  y0), we accept it. Otherwise, we adapt the crumb distribution.
If J has p  1 columns, we cant add any more without the crumb covariance having a rank of zero, so we
do not adapt in that case. Otherwise, we add a single new column in the direction of  log f (xk), projected

10

into the directions not already spanned by J, which we denote by g. Thus, the new value of J would be:

(cid:20)

Jk+1 =

Jk

(cid:21)

g
(cid:107)g(cid:107)

where

g = P (Jk, log f (xk))

(37)

(38)

To prevent meaningless adaptations, we only perform this operation when the angle between the gradient
and its projection into the nullspace of J is less than 60. Equivalently, we only adapt when:

gT log f (xk)
(cid:107)g(cid:107)(cid:107) log f (xk)(cid:107) >

1
2

(39)

After possibly updating J, we draw another crumb and proposal, repeating until a proposal is accepted. The

method of this section is summarized in gure 5.

A variation on the method (which we use in the implementation tested in section 7) shrinks the crumb
standard deviation in the nonzero directions, c, by a constant factor each time a crumb is drawn; section
7 uses 0.9. This results in exponentially falling proposal variance, which, as described in section 4.1, allows

large initial crumb variances to be used.

6 Procedure for Evaluation

Figures 6 and 7 demonstrate the performance of the methods described in this document. Figure 6 demon-

strates the performance of four samplers on a Gaussian distribution (to be described in section 7). It contains

two graphs, one for each of two gures of merit. The top graph plots log density function evaluations per

independent sample against a tuning parameter. The bottom graph plots processor-seconds per independent

sample against a tuning parameter. For both gures of merit, smaller is better.

Each of the four panes in each graph contains data from a single sampler, with each point representing

a run with a specic tuning parameter. The tuning parameter for each sampler has the same scale; the

sampler initially attempts to take steps roughly that size.

Both gures of merit require us to determine the correlation length, the number of correlated samples

equivalent to an independent sample. For the runs done here, an AR(1) model captures the necessary

structure. For each parameter, we t the following model:

Xt = E(Xt) +   Xt1 + at

where at is a noise process. Then, the number of samples equivalent to a single independent sample is:

 =

var(at)

var(Xt)  (1  )2

(40)

(41)

This formula is based on the effectiveSize function in CODA (Plummer et al., 2006), which uses the
spectral approach of Heidelberger and Welch (1981). Wei (2006, pp. 276278) has a more in-depth discussion

of the spectrum of AR processes. To estimate a correlation length for a multivariate distribution, we take

the largest estimated correlation length of each of its parameters. This is not valid in general, but is an

11

Crumbs with Shrinking-Rank Covariance

Initialize N , f , x0  Rp, and c.
X  ( )
repeat N times:

e  draw from Exponential(1)
y0  log f (x0)  e
J  [ ]
k  0
c  0
repeat until a proposal is accepted:

k  k + 1
z  draw from Np(0, I)
c  cz

c  k1(cid:0)(k  1)c + c(cid:1)

z  draw from Np(0, I)
x  x0 + P
J, c + c
z
k
if log f (x)  y0:

(cid:16)

(cid:17)

proposal accepted, break

end (if)
if J has less than p  1 columns:

g  P (J, log f (x))
if gT log f (x) > 1
J  [ J g/(cid:107)g(cid:107) ]

2(cid:107)g(cid:107)(cid:107) log f (x)(cid:107):

end (if)

end (if)
end (repeat)
append x to X.
x0  x
end (repeat)
return X

Figure 5: The adaptive algorithm of section 5. This diers from that section in that the projection into the
nullspace of J is delayed until drawing a proposal, reducing the number of matrix products computed.

12

acceptable approximation for this experiment.

Most chains are displayed with a circle indicating an estimate of the gure of merit, with a line indicating

a nominal 95% condence interval. The intervals are based on normality of the increments of the AR(1)

process, so they should be viewed as a lower bound on the uncertainty of the point estimates. Chains whose

gures are plotted as question marks were estimated as having an eective sample size of less than four.

Figure 7 contains information on four dierent samplers. The panes of gure 7 are similar to those of

gure 6, and each is labeled with the distribution and the sampler the chains in that pane come from. The

columns of panes correspond to samplers; the rows of panes correspond to distributions. By reading across,

one can see how dierent samplers perform on a given distribution. By reading down, one can see how the

performance of a given sampler varies from distribution to distribution.

Every chain has 150,000 samples. In general, the chain length does not aect the results; we will point

out exceptions to this.

7 Results of Evaluation

We simulated four samplers on four distributions for twelve dierent tuning parameters each. The samplers

we considered are:

 Covariance-Matching: This is the method described in section 4. The tuning parameter is c.
 Shrinking-Rank: This is the method described in section 5. The tuning parameter is c.
 Non-Adaptive Crumbs: This is a non-adaptive variant of the general method of section 3. It is like the
method of section 5, but never shrinks rank. However, like that method, it scales the crumb standard
deviation down by 0.9 after each proposal. The tuning parameter is c.

 Metropolis (with Trials): This method is a Metropolis sampler that uses trial runs to automatically
determine a suitable Gaussian proposal distribution. The tuning parameter species the standard

deviation of the proposal distribution for the rst trial run.

The distributions we considered are:

 N4( = 0.999): This is a four-dimensional Gaussian with highly-correlated parameters. The marginal
variances for each parameter are one; each parameter has a correlation of 0.999 with each other pa-

rameter. The covariance matrix has a condition number of about 2900 and is not diagonal.

 N4( = 0.3329): This is a four-dimensional Gaussian with negatively-correlated parameters. The
marginal variances for each parameter are one, and each parameter has a correlation of 0.3329 with
each other parameter. The covariance has a condition number of about 2900, like N4( = 0.999), but
instead of one large eigenvalue and three small ones, this distribution has one small eigenvalue and

three large ones.

 Eight Schools: This is a multilevel model in ten dimensions, consisting of eight group means and

hyperparameters for their mean and variance. It comes from Gelman et al. (2004, pp. 138145).

13

Figure 6: The performance of four MCMC samplers on N4( = 0.999). See section 6 for a description of the
graphs and section 7 for discussion.

 Ten-Component Mixture: This is a ten-component Gaussian mixture in R10. Each mode is a spherically
symmetric Gaussian with unit variance. The modes are uniformly distributed on a hypercube with

edge-length ten.

By comparing the top and bottom graphs of gure 6, we see that for N4( = 0.999), processor time
and number of density evaluations tell the same story. The plots of function evaluations and the plots of

processor time are nearly identical except for their vertical scales. This is true of the other distributions as

well, so we do not repeat the processor-time plots for the others.

Figure 6 (and the identical rst row of gure 7) shows the performance of the four methods on a highly-
correlated four-dimensional Gaussian, N4( = 0.999). Both adaptive slice sampling methods perform well
once the tuning parameter is at least the same order as the standard deviation of the target distribution.

This hockey-stick performance curve is characteristic of slice sampling methods. The non-adaptive slice

sampling method always takes steps of the order of the smallest eigenvalue once its tuning parameter is at

least that large, so its performance is bad, but after this threshold, its performance does not depend much

on the tuning parameter. The Metropolis sampler also fails to capture the shape of the distribution, a result

that depends more on chain length. Had the chain been longer, the preliminary runs the sampler uses to

14

Comparison of density evaluations on N[4](rho=0.999)evals per indep. sample10^110^210^310^410^510^60.010.1110100100010000llllllllllll?CovarianceMatching0.010.1110100100010000lllllllllllllShrinkingRank0.010.1110100100010000llllllllllll?NonAdaptive Crumbs0.010.1110100100010000lllllllllllllMetropolis (with Trials)Comparison of processor usage on N[4](rho=0.999)scale tuning parametercpu sec. per indep. sample10^210^110^010^110^210^30.010.1110100100010000llllllllllll?CovarianceMatching0.010.1110100100010000lllllllllllllShrinkingRank0.010.1110100100010000llllllllllll?NonAdaptive Crumbs0.010.1110100100010000lllllllllllllMetropolis (with Trials)Figure 7: The performance of four MCMC samplers on four distributions. See section 6 for a description of
the graph and section 7 for discussion.

15

Four Samplers on Four Distributionsscale tuning parameterevals per indep. sample10^110^210^310^410^510^60.010.1110100100010000llllllllll???CovarianceMatchingTenComponent Mixture0.010.1110100100010000llllllllll???ShrinkingRankTenComponent Mixture0.010.1110100100010000llllllllll???NonAdaptive CrumbsTenComponent Mixture0.010.1110100100010000llllllll?????Metropolis (with Trials)TenComponent Mixture10^110^210^310^410^510^6llllllllll???CovarianceMatchingEight Schoolsllllllllll???ShrinkingRankEight Schoolslllllllllll??NonAdaptive CrumbsEight Schoolsllll?????????Metropolis (with Trials)Eight Schools10^110^210^310^410^510^6llllllllllll?CovarianceMatchingN4(r=-0.3329)llllllllllll?ShrinkingRankN4(r=-0.3329)lllllllllllllNonAdaptive CrumbsN4(r=-0.3329)lllllllllllllMetropolis (with Trials)N4(r=-0.3329)10^110^210^310^410^510^6llllllllllll?CovarianceMatchingN4(r=0.999)lllllllllllllShrinkingRankN4(r=0.999)llllllllllll?NonAdaptive CrumbsN4(r=0.999)lllllllllllllMetropolis (with Trials)N4(r=0.999)estimate a proposal distribution might have worked better, leading to average performance comparable to

the adaptive slice samplers.

The second row of gure 7 shows sampler performance on a similar but negatively correlated four-
dimensional Gaussian, N4( = 0.3329). The adaptive slice sampling methods continue to perform well on
this distribution, and the non-adaptive sampler improves somewhat. The Metropolis sampler improves a

great deal; on this target distribution, it is able to choose a reasonable proposal distribution, so it performs
comparably to the adaptive slice sampling methods.

The third row of gure 7 shows sampler performance on Eight Schools. The minimum threshold appears

again for both adaptive slice samplers as well as the non-adaptive slice sampler. Since the condition number of

the covariance of this distribution is only about seven, adaptivity is not as important, though slice samplings

robustness to improper tuning parameters remains important. The adaptive Metropolis method again fails

to identify a reasonable proposal distribution for small tuning parameters, and fails to generate any proposal

distribution at all for large ones. This is partially a reection on this particular implementation, which only

tries preliminary runs with proposal distribution standard deviations within four orders of magnitude of the

tuning parameter.

The bottom row of gure 7, which shows performance on Ten-Component Mixture, has a similar pattern.

The results are more erratic since the distribution has multiple, moderately-separated modes, and none of

the samplers are designed to perform well on multimodal distributions.

8 Discussion

The adaptive slice sampling methods of sections 4 and 5 generally perform at least as well as non-adaptive

slice sampling methods and Metropolis. Slice sampling in general tends to be more robust to imperfect

choice of tuning parameters than Metropolis. Preliminary chains are usually unnecessary, avoiding the

hassle of manual management of these runs or the idiosyncratic performance of automatic evaluation of the

runs. The main disadvantage of the adaptive slice samplers relative to Metropolis and non-adaptive slice

sampling is that they require the log density to have an analytically computable gradient, though this is a

standard requirement in numerical optimization, and experience in that domain has shown that computing

the gradient is often straightforward.

The two adaptive slice sampling methods tend to perform similarly to each other. The shrinking-rank

method usually performs slightly better, but this advantage can be mitigated by making the approximation
log f (uk)  log y0. There is no theoretical justication for this, but it cuts the number of log density
evaluations by half with negligible performance cost, making the performance of the two adaptive methods
indistinguishable. The shrinking-rank method is simpler, though, and requires only O(min(k, p)p) operations
to draw the kth crumb and proposal, slightly better than the O(p2) operations needed by the covariance-
matching method.

Like most variations of multivariate slice sampling, both the non-adaptive and adaptive methods described

here do not work well for target distributions in spaces higher than a few dozen. The variation in log density

of samples increases with dimension, but slice sampling takes steps in the log density of only order one each

iteration. So, in high-dimensional spaces, samples tend to be highly correlated.

Due to this poor scaling with dimensionality, the methods of this document have limited usefulness on

16

their own. They may be useful in highly-correlated low-dimensional problems, though, and can be used to

take steps in highly-correlated low-dimensional subspaces as part of larger sampling schemes. We hope to

address this limitation in future work, possibly with polar slice sampling (Roberts and Rosenthal, 2002).

An R implementation of these methods is available at http://www.cs.utoronto.ca/~radford.

