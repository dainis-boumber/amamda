Abstract. The inferential problem of associating data to mixture components is dif-
(cid:12)cult when components are nearby or overlapping. We introduce a new split-merge
Markov chain Monte Carlo technique that e(cid:14)ciently classi(cid:12)es observations by splitting
and merging mixture components of a nonconjugate Dirichlet process mixture model.
Our method, which is a Metropolis-Hastings procedure with split-merge proposals, sam-
ples clusters of observations simultaneously rather than incrementally assigning observa-
tions to mixture components. Split-merge moves are produced by exploiting properties
of a restricted Gibbs sampling scan. A simulation study compares the new split-merge
technique to a nonconjugate version of Gibbs sampling and an incremental Metropolis-
Hastings technique. The results demonstrate the improved performance of the new
sampler.

Keywords: Bayesian model, Markov chain Monte Carlo, split-merge moves, nonconjugate prior

1 Introduction

Bayesian mixture models have gained in popularity as an alternative to traditional
density estimation and clustering techniques. In particular, Bayesian mixture models in
which a Dirichlet process prior de(cid:12)nes the mixing distribution are of interest due to their
(cid:13)exibility in (cid:12)tting a countably in(cid:12)nite number of components (Ferguson (1983)). Much
of the recent research related to the Dirichlet process mixture model has been devoted
to developing computational techniques, usually Markov chain Monte Carlo methods,
to sample from its posterior distribution (Neal (2000), MacEachern and M(cid:127)uller (1998)).
Other techniques to estimate the Dirichlet process model include sequential importance
sampling (MacEachern et al. (1999)) and variational methods (Blei and Jordan (2004)).
The practical utility of these methods is illustrated by their recent use for complex bi-
ological and genetics problems, such as haplotype reconstruction (Xing et al. (2004)),
estimation of rates of non-synonymous and synonymous nucleotide substitutions as evi-
dence for natural selection in evolutionary biology problems (Huelsenbeck et al. (2006)),
and determination of di(cid:11)erential gene expression (Do et al. (2005)).

The focus of this article is on Markov chain sampling for nonconjugate Dirichlet pro-
cess mixture models, building on our previous work for conjugate models (Jain and Neal
(2004)). Conjugate models are appropriate for some problems, which is convenient due

(cid:3)Division of Biostatistics and Bioinformatics, Department of Family and Preventive Medicine, Uni-

versity of California at San Diego, La Jolla, CA, mailto:sojain@ucsd.edu

yDepartment of Statistics and Department of Computer Science, University of Toronto, Toronto,

Ontario, Canada, http://www.cs.toronto.edu/~radford/

c(cid:13) 2007 International Society for Bayesian Analysis

ba0007

446

Splitting and Merging Components of a Nonconjugate DPMM

to the analytical tractability of these priors. However, in many situations, conjugate
priors can be too restrictive. Forcing conjugacy on the model can lead to undesirable
or even nonsensical priors. A classic example is a simple model for normally distributed
data, where conjugacy requires an assumption that the mean and variance are a priori
dependent, which is often unrealistic in actual problems.

Computationally, Markov chain sampling procedures can operate di(cid:11)erently depend-
ing on whether conjugacy is assumed. In the conjugate case, we can analytically in-
tegrate away the mixing proportions for the components and the parameters for each
component. This leads to Markov chain Monte Carlo procedures that update only
the latent indicator variable associating mixture components with data observations
(MacEachern (1994), Neal (1992)). However, in the nonconjugate case, the parameters
of the model cannot be integrated away and must be included in the Markov chain
update. Further, since we lose the advantage of analytic tractability, computational
di(cid:14)culties arise, which makes it more di(cid:14)cult, but not impossible, to construct valid
Markov chain Monte Carlo procedures.

Nonconjugate Markov chain sampling methods based on the Gibbs sampler have
been proposed previously; see, for instance, MacEachern and M(cid:127)uller (1998) and Neal
(2000). When the mixture components are nearby or overlapping, these incremental
samplers (as well as those for conjugate models) su(cid:11)er from computational di(cid:14)culties,
such as remaining stuck in isolated modes and poor mixing between components.

Alternative nonincremental Markov chain samplers for the Dirichlet process mixture
model based on split-merge moves have been proposed by Green and Richardson (2001)
and by ourselves (Jain and Neal (2004)). In a single iteration, these methods can split a
mixture component moving all observations to an appropriate new component, or merge
two distinct components together. The Green and Richardson (2001) method is based
on the reversible-jump procedure, in which numerous ways to propose a split move are
possible. Since speci(cid:12)c moment conditions must be preserved, the split-merge proposals
are model-dependent. Jain and Neal (2004) introduce a Metropolis-Hastings technique
with split-merge proposals for conjugate Dirichlet process mixture models. The inno-
vation in this work is exploiting properties of a Gibbs sampling scan to construct split-
merge moves, such that their Metropolis-Hastings proposals are model-independent. In
this article, we extend the conjugate split-merge technique to a class of nonconjugate
Dirichlet process mixture models by developing a novel scheme to incorporate the model
parameters into the sampling procedure.

This article is organized as follows. Section 2 de(cid:12)nes the nonconjugate Dirichlet
process mixture model. Section 3 brie(cid:13)y describes the Metropolis-Hastings split-merge
technique based on Gibbs sampling proposals. The new split-merge technique for a class
of nonconjugate models is proposed in Section 4. Next, in Section 5, we illustrate the
utility of our method in by comparing it to an auxiliary Gibbs sampling method (Neal
(2000), Algorithm 8). Section 6 is a general discussion and concluding remarks. Details
of a simulation study are provided in the Appendix in Section 7.

S. Jain and R. M. Neal

2 The model

447

The Dirichlet process mixture model takes the following hierarchical model form for
observed data y = (y1; : : : ; yn) that is considered exchangeable:

yi j (cid:18)i (cid:24) F ((cid:18)i)
(cid:18)i j G (cid:24) G

G (cid:24) DP (G0; (cid:11))

(1)

Here, F ((cid:18)i) is a component parameterized by (cid:18)i from a parametric distribution whose
density will be written as f (y; (cid:18)). G is the mixing distribution. G0 de(cid:12)nes a base
distribution for the Dirichlet process (DP ) prior, while (cid:11) is a concentration parameter
that takes values greater than zero. The usual conditional independence assumptions
for a hierarchical model apply, so that the only dependencies are those that are explicitly
shown.

Realizations of the Dirichlet process are discrete with probability one. A conse-
quence of this is that the mixture model in equation (1) can be viewed as a countably
in(cid:12)nite mixture model (Ferguson (1983)). This is evident when we simplify the model
in equation (1) by integrating G over its prior distribution. The (cid:18)i follow a generalized
Polya urn scheme (Blackwell and MacQueen (1973)) and the prior distribution for the
(cid:18)i may be represented by the following conditional distributions:

(cid:18)1 (cid:24) G0

(cid:18)i j (cid:18)1; : : : ; (cid:18)i(cid:0)1 (cid:24)

1

i(cid:0)1+(cid:11)

i(cid:0)1

Xj=1

(cid:14)((cid:18)j ) +

(cid:11)

i(cid:0)1+(cid:11)

G0

(2)

where (cid:14)((cid:18)j ) is the distribution which is a point mass at (cid:18)j .

We can represent the fact that (2) results in some of the (cid:18)i being identical by setting
(cid:18)i = (cid:30)ci , where ci represents the latent class associated with observation i, and all (cid:30)c are
independently drawn from G0. The Polya urn scheme for sampling the (cid:18)i is equivalent
to the following scheme for sampling the latent variables, ci, and associated (cid:30)c:

P (ci = c j c1; : : : ; ci(cid:0)1) =

P (ci 6= cj for all j < i j c1; : : : ; ci(cid:0)1) =

ni;c

i (cid:0) 1 + (cid:11)

(cid:11)

i (cid:0) 1 + (cid:11)

;

for c 2 fcjgj<i

(3)

where ni;c is the number of ck for k < i that are equal to c. The probabilities shown
in (3) de(cid:12)ne the Dirichlet process model. This notation will be employed in subsequent
sections.

3 Jain and Neals conjugate split-merge procedure

We have previously introduced a split-merge Metropolis-Hastings procedure for conju-
gate Dirichlet process mixture models (Jain and Neal (2004); Jain (2002)). In the con-
jugate version of the algorithm, we assume that F is conjugate to G0 in equation (1), so

448

Splitting and Merging Components of a Nonconjugate DPMM

the model parameters, (cid:30)c, in addition to the mixing distribution, G, can be integrated
away. The state of the Markov chain consists only of the mixture component indicators,
ci.

This sampler proposes nonincremental moves that can produce major changes to
the con(cid:12)guration of observations to mixture components in a single iteration. The
split-merge proposals are evaluated by a Metropolis-Hastings procedure, in which split
proposals are constructed by exploiting properties of a restricted Gibbs sampling scan
on the component indicators, ci. The Gibbs sampling scan is restricted in that it is
only performed on a subset of the data (the observations associated with the merged
component that is proposed to be split) and will only allocate observations between two
mixture components.

To achieve more reasonable split proposals, several intermediate restricted Gibbs
sampling scans are conducted prior to the (cid:12)nal restricted Gibbs sampling scan, which is
used to calculate the Metropolis-Hastings acceptance probability. The result of the last
intermediate Gibbs sampling scan is denoted as the random launch state, from which the
restricted Gibbs sampling transition probability is explicitly calculated. The number of
intermediate restricted Gibbs sampling scans is considered a tuning parameter of this
algorithm.

Note that for a merge proposal, there is only one way to combine items in two
components to one component. However, deciding whether to accept or reject a merge
proposal requires hypothetical consideration of the reverse split, which requires compu-
tations similar to those done for an actual split. A description of the steps involved in
this algorithm, details to compute the Metropolis-Hastings acceptance probability, and a
discussion of the validity of the conjugate version of the split-merge Metropolis-Hastings
algorithm are provided in Jain and Neal (2004).

4 The nonconjugate split-merge procedure

We adapt Jain and Neals conjugate split-merge Markov chain procedure described in
Section 3 to accommodate models with nonconjugate priors. As mentioned earlier,
because conjugate priors are not appropriate for all modeling situations, much of the
recent Bayesian mixture modeling literature has been dedicated to nonconjugate al-
gorithms (for instance, MacEachern and M(cid:127)uller (1998), Green and Richardson (2001),
and Neal (2000)). A major impediment in designing nonconjugate procedures is the
computational di(cid:14)culty that arises when the model is no longer analytically tractable.

We say the model is nonconjugate when G0 is not conjugate to F in the mixture
model (equation 1). Aside from being unable to simplify the state of the Markov chain
by integrating away the model parameters, (cid:30), the main obstacle occurs when trying to
sample for a new mixture component. When a ci is updated, it can be set either to
one of the other components currently associated with some observation or to a new
mixture component. The probability of setting ci to a new component involves the

integral, R F (yi; (cid:30)) dG0((cid:30)), which is analytically intractable in most nonconjugate situ-

S. Jain and R. M. Neal

449

ations. Allowances that some previous nonconjugate methods have made when dealing
with this integral include approximating the true posterior distribution by another sta-
tionary distribution (which can be extremely detrimental) or creating model-speci(cid:12)c ad
hoc algorithms (which fail to generalize well).

Neal (2000) proposed two incremental Markov chain sampling procedures: Gibbs
sampling with auxiliary parameters (Algorithm 8), and an incremental Metropolis-
Hastings technique (Algorithm 5). These are exact Markov chain Monte Carlo methods
that sample the correct posterior distribution and are straightforward to implement.
However, in situations where the mixture components are nearby or similar in struc-
ture, these incremental methods performance is analogous to the incremental methods
for conjugate models (see Jain and Neal (2004)). To overcome their problems, such
as remaining stuck in isolated modes and poor mixing between mixture components,
we have developed a nonincremental split-merge alternative. In the next section, we
compare empirically the performance of the new sampler to Neals two incremental
algorithms.

In this article, we show how such a nonincremental split-merge procedure can be
applied when the model uses a particular type of nonconjugate prior, the conditionally
conjugate family of priors.
In conditionally conjugate models, it is still impossible

to e(cid:14)ciently compute the integral, R F (yi; (cid:30)) dG0((cid:30)). However, the pair F and G0

are conditionally conjugate in one model parameter if the remaining parameters are
held (cid:12)xed. A well-known instance of this is the following Normal model. Suppose
the observations, y1; : : : ; yn, are distributed as F (yi; (cid:22); (cid:27)2) = Normal(yi; (cid:22); (cid:27)2), and
the prior is G0((cid:22); (cid:27)(cid:0)2) = Normal((cid:22); w; B(cid:0)1) (cid:1) Gamma((cid:27)(cid:0)2; r; R). The distributions,
F (yi; (cid:22); (cid:27)2) and G0((cid:22); (cid:27)(cid:0)2), are conjugate in (cid:22) when (cid:27)2 is (cid:12)xed, and conjugate in (cid:27)2
if (cid:22) is (cid:12)xed. But, the joint posterior distribution is not analytically tractable. For the
sake of brevity, when this nonconjugate Normal-Gamma prior is applied to a Normal
mixture model, we will refer to it as the Normal-Gamma mixture model. Note, however,
that this model using a conjugate prior, in which the mean and variance are a priori
dependent, is sometime referred to similarly.

4.1 Restricted Gibbs sampling split-merge proposals

The conjugate split-merge algorithm of Section 3 cannot be applied directly to the con-
ditionally conjugate case, but the basic mechanism of creating restricted Gibbs sampling
split-merge proposals can still be applied. Since the model parameters, (cid:30)c, cannot be
integrated away, the state of the Markov chain for the split-merge sampler consists of
both the component indicators and model parameters, denoted by (cid:13) = (c; (cid:30)), where
c = (c1; : : : ; cn) and (cid:30) = ((cid:30)c : c 2 fc1; : : : ; cng).

Conditional conjugacy in the model is required so that restricted Gibbs sampling
scans can be performed to allocate observations reasonably between two mixture com-

ponents. During these scans, we do not need to compute the integral, R F (yi; (cid:30)) dG0((cid:30)),

since we are only allocating observations between two known components that have at
least one observation already assigned to them. For a nonconjugate model, a restricted

450

Splitting and Merging Components of a Nonconjugate DPMM

Gibbs sampling scan also updates the parameters for the a(cid:11)ected mixture components,
while holding the parameters of the other components (cid:12)xed. Note that use of a re-
stricted Gibbs sampling scan (and consequently, conditional conjugacy) is only crucial
for the (cid:12)nal Gibbs sampling scan from the launch state, since it allows the Metropolis-
Hastings proposal density can be calculated. The intermediate scans could be replaced
by some other type of Markov chain update.

Due to the inclusion of the model parameters, when two separate components are
being merged to a single component, there is no longer only one possible component
to merge into. The merged component is now de(cid:12)ned by component parameters,
which must be accounted for in the Metropolis-Hastings acceptance probability (in Sec-
tion 4.3). The algorithm addresses this problem by conducting intermediate restricted
Gibbs sampling for the merged components parameters to arrive at a launch state (in
a similar fashion as the \split" intermediate Gibbs sampling). From this launch state,
one (cid:12)nal restricted Gibbs sampling scan is performed to obtain the model parameters
of the proposed merged component. The number of intermediate Gibbs sampling scans
for the merged components parameters is an additional tuning parameter in this al-
gorithm. In this generalized version of the split-merge algorithm, there are therefore
two launch states, (cid:13) Lsplit and (cid:13)Lmerge , that are necessary in order to calculate Gibbs
sampling transition kernels for the split and merge proposal distributions.

4.2 Restricted Gibbs sampling split-merge procedure for the noncon-

jugate case

Let the state of the Markov chain consist of (cid:13) = (c; (cid:30)) where c = (c1; : : : ; cn) and (cid:30) = ((cid:30)c :
c 2 fc1; : : : ; cng).

1. Select two distinct observations, i and j, at random uniformly.

2. Let S denote the set of observations, k 2 f1; : : : ; ng, for which k 6= i and k 6= j, and

ck = ci or ck = cj .

3. De(cid:12)ne launch states, (cid:13) Lsplit and (cid:13) Lmerge , that will be used to de(cid:12)ne Gibbs sampling

distributions required for the split and merge proposals.

(cid:15) Obtain launch state (cid:13) Lsplit = (cLsplit ; (cid:30)Lsplit ) as follows:

{ If ci = cj , then let c

be set to a new component such that

Lsplit
i

c

c

= ci and c

=2 fc1; : : : ; cng and let c

Lsplit
i
Lsplit
i
pendently with equal probability, to either of the distinct components, c
or c

= cj . For every k 2 S, randomly set c

. Initialize model parameters, (cid:30)

= cj . Otherwise, when ci

Lsplit
k

Lsplit
j

Lsplit
j

6= cj,

let

, inde-
Lsplit
i

Lsplit
Lsplit
i

c

Lsplit
Lsplit
j

c

and (cid:30)

, associated with

Lsplit
j

the two distinct components by drawing new values from their prior distribu-
tion.

{ Modify (cid:13) Lsplit by performing t intermediate restricted Gibbs sampling scans

to update cLsplit , (cid:30)

Lsplit
Lsplit
i

c

, and (cid:30)

Lsplit
Lsplit
j

c

.

(cid:15) Obtain launch state (cid:13) Lmerge = (cLmerge ; (cid:30)Lmerge ) as follows:

S. Jain and R. M. Neal

451

= cLmerge
{ If ci = cj, then let cLmerge
if ci 6= cj , then set cLmerge
= cLmerge
Initialize model parameter, (cid:30)Lmerge

j

j

i

i

c

Lmerge
j

= cj (which is the same as ci). Similarly,
= cj . For every k 2 S, set cLmerge
, associated with the merged component

= cj .

k

by drawing a new value from its prior distribution.

{ Modify (cid:13) Lmerge by performing r intermediate restricted Gibbs sampling scans

to update (cid:30)Lmerge

.

c

Lmerge
j

4. If items i and j are in the same mixture component, i.e. ci = cj , then:

(a) Propose a new assignment of data items to mixture components, denoted as csplit,
and csplit
,
and

in which component ci = cj is split into two separate components, csplit
and propose new values for the corresponding components parameters, (cid:30)split

j

i

c

split
i

. De(cid:12)ne each element of the candidate state, (cid:13) split = (csplit; (cid:30)split), as

(cid:30)split

c

split
j

follows:

i

j

= c

= c

Lsplit
i

Lsplit
i
Lsplit
j

(note that c

(cid:15) Let csplit
(cid:15) Let csplit
(cid:15) By conducting one (cid:12)nal Gibbs sampling scan from the launch state, (cid:13) Lsplit ,
or csplit

be set to either component csplit

(which is the same as cj )

=2 fc1; : : : ; cng)

for every observation k 2 S, let csplit
and draw values for the model parameters, (cid:30)split

k

and (cid:30)split

.

j

i

c

split
i

c

split
j

(cid:15) For observations k =2 S [ fi; jg, let csplit

k

= ck, and for c =2 fcsplit

i

; csplit

j

g, let

(cid:30)split
csplit = (cid:30)c.

(b) Compute the proposal densities, q((cid:13) splitj(cid:13)) and q((cid:13)j(cid:13) split), that will be used to

calculate the Metropolis-Hastings acceptance probability.

(cid:15) Calculate the split proposal density, q((cid:13) splitj(cid:13)), by computing the Gibbs sam-
pling transition kernel from the split launch state, (cid:13) Lsplit , to the (cid:12)nal proposed
state, (cid:13) split. The Gibbs sampling transition kernel is the product of the in-
dividual probabilities of setting each element in the launch state to its (cid:12)nal
proposed value during the (cid:12)nal Gibbs sampling scan.

(cid:15) Calculate the corresponding proposal density, q((cid:13)j(cid:13) split), by computing the
Gibbs sampling transition kernel from the merge launch state, (cid:13) Lmerge , to the
original merged con(cid:12)guration, (cid:13). The Gibbs sampling transition kernel is the
product of the probability of setting each element in the original merge state
(in this case, elements of (cid:30)cj ) to its original value in a (hypothetical) Gibbs
sampling scan from the merge launch state.

(c) Evaluate the proposal by the Metropolis-Hastings acceptance probability a((cid:13) split; (cid:13)).
If the proposal is accepted, (cid:13) split becomes the next state in the Markov chain. If
the proposal is rejected, the original con(cid:12)guration and model parameter, (cid:13), remain
as the next state.

5. Otherwise, if i and j are in di(cid:11)erent mixture components, i.e. ci 6= cj, then:

(a) Propose a new assignment of data items to mixture components, denoted as cmerge,
in which distinct components, ci and cj , are combined into a single component, and
propose a new value for the corresponding merged components model parameter,
(cid:30)merge
. De(cid:12)ne each element of the candidate state, (cid:13) merge = (cmerge; (cid:30)merge), as
follows:

merge
j

c

452

Splitting and Merging Components of a Nonconjugate DPMM

i

i

= cLmerge
= cLmerge

(cid:15) Let cmerge
(cid:15) Let cmerge
(cid:15) For every observation k 2 S, let cmerge
(cid:15) For observations k =2 S [ fi; jg,

k

j

j

(which is the same as cj )

(which is the same as cj )

(cid:30)merge

cmerge = (cid:30)c.

= cLmerge

(which is the same as cj )

= ck, and for c 6= cmerge, let

j
let cmerge

k

(cid:15) Conduct one (cid:12)nal restricted Gibbs sampling scan from the launch state,

(cid:13) Lmerge , in order to draw a new value for the model parameter, (cid:30)merge

c

merge
j

.

(b) Compute the proposal densities, q((cid:13) mergej(cid:13)) and q((cid:13)j(cid:13) merge), that will be used to

calculate the Metropolis-Hastings acceptance probability.

(cid:15) Calculate the merge proposal density, q((cid:13) mergej(cid:13)), by computing the Gibbs
sampling transition kernel from the merge launch state, (cid:13) Lmerge , to the (cid:12)nal
proposed state, (cid:13) merge. The Gibbs sampling transition kernel is the probability
of setting (cid:30)Lmerge
, via one Gibbs sampling
scan.

to its (cid:12)nal proposed value, (cid:30)merge

Lmerge
j

merge
j

c

c

(cid:15) Calculate the corresponding proposal density, q((cid:13)j(cid:13) merge), by computing the
Gibbs sampling transition kernel from the split launch state, (cid:13) Lsplit , to the
original split con(cid:12)guration, (cid:13). The Gibbs sampling transition kernel is the
product of the probabilities of setting each element in the original split state
to its original value in a (hypothetical) Gibbs sampling scan from the split
launch state.

(c) Evaluate the proposal by the Metropolis-Hastings acceptance probability a((cid:13) merge; (cid:13)).

If the proposal is accepted, (cid:13) merge becomes the next state. If the merge proposal
is rejected, the original con(cid:12)guration and model parameters, (cid:13), remain as the next
state.

4.3 The Metropolis-Hastings acceptance probability

The Metropolis-Hastings acceptance probability (Metropolis et al. (1953), Hastings (1970))
takes the following form when updating (cid:13) = (c; (cid:30)):

a((cid:13) (cid:3); (cid:13)) = min(cid:20)1;

q((cid:13)j(cid:13)(cid:3))
q((cid:13) (cid:3)j(cid:13))

P ((cid:13) (cid:3))
P ((cid:13))

L((cid:13)(cid:3)jy)

L((cid:13)jy) (cid:21)

(4)

where (cid:13)(cid:3) is either (cid:13)split or (cid:13) merge depending on the type of proposal.

The prior distribution, P ((cid:13)), will be a product of the individual prior distributions
for c and (cid:30), since they are a priori independent. As before, the prior distribution
for P (c) will be a product of factors in equation (3). The (cid:30)c for di(cid:11)erent mixture
components are independent. Therefore, the prior distribution for P ((cid:13)) is:

P ((cid:30)c)

P ((cid:13)) = P (c) Yc 2 c
k=1((cid:11)+k(cid:0)1) Yc 2 c
Qn

= (cid:11)D Qc 2 c(nc(cid:0)1)!

g((cid:30)c)

(5)

(6)

S. Jain and R. M. Neal

453

where D is the number of distinct mixture components, nc is the count of items belonging
to mixture component c 2 c, and g((cid:30)c) is the prior probability density function for (cid:30)c
for mixture component c 2 c.

For the split proposal, the appropriate ratio of prior distributions is:

P ((cid:13) split)

P ((cid:13))

= (cid:11)

(nsplit
csplit
i

(cid:0)1)! (nsplit
csplit
j
(nci (cid:0)1)! g((cid:30)ci)

(cid:0)1)! g((cid:30)split
csplit
i

) g((cid:30)split
csplit
j

)

(7)

where (cid:13) is the original state in which i and j belong to the same mixture component,
nsplit
are the number of observations associated with each split component.
csplit
i

and nsplit
csplit
j

The ratio of the prior distributions simpli(cid:12)es because the denominator in equation (6)
and factors not associated with components that are directly involved in the Metropolis-
Hastings update cancel.

For the merge proposal, the prior ratio simpli(cid:12)es to:

P ((cid:13)merge)

P ((cid:13))

=

1
(cid:11)

(nmerge
cmerge
i

(cid:0)1)! g((cid:30)merge
cmerge
i

)

(nci (cid:0)1)! (ncj (cid:0)1)! g((cid:30)ci ) g((cid:30)cj )

(8)

where nmerge
denotes the number of observations associated with the single merged
cmerge
i
component. (cid:13) represents the original state in which items i and j belong to separate
components.

The likelihood, L((cid:13)jy), will be a product over n observations:

L((cid:13)jy) =

f (yk; (cid:30)ck )

n

Yk=1

(9)

L((cid:13)jy) can be expressed as a double product over components, c, and items, k 2 f1; : : : ; ng,
associated with each component:

L((cid:13)jy) =

D

Yc=1 Yk : ck=c

f (yk; (cid:30)c)

(10)

where D is the number of distinct components. This expression to calculate the likeli-
hood is often easier to use in real examples.

Likelihood factors involving items associated with components not directly involved
in the split proposal cancel. The ratio of likelihoods in equation (4) reduces to the
following:

L((cid:13)splitjy)

L((cid:13)jy)

=

Yk : csplit

k =csplit

i

f (yk; (cid:30)split
csplit
i

) Yk : csplit

k =csplit

j

f (yk; (cid:30)ci )

Yk : ck=ci

f (yk; (cid:30)split
csplit
j

)

(11)

454

Splitting and Merging Components of a Nonconjugate DPMM

Likewise, for the merge proposal, the ratio of likelihoods is:

L((cid:13)mergejy)

L((cid:13)jy)

=

k

Yk : cmerge
Yk : ck=ci

f (yk; (cid:30)ci ) Yk : ck=cj

f (yk; (cid:30)merge
cmerge
i

)

=cmerge

i

f (yk; (cid:30)cj )

(12)

The Metropolis-Hastings proposal density, q((cid:13) (cid:3)j(cid:13)), is the restricted Gibbs sampling
transition kernel from launch state (cid:13) L to (cid:12)nal state (cid:13) (cid:3). This is a product of the
conditional probabilities of each individual update of the vector c(cid:3) from cL and the
conditional densities of assigning successive components of (cid:30)L to their (cid:12)nal values, (cid:30)(cid:3).

Typically, for each mixture component, (cid:30) is composed of more than one model
parameter, i.e. each (cid:30)c can be a vector of parameters. For example, in the normal
model, there are two parameters per component, (cid:30)c = ((cid:22)c; (cid:27)2
c ). In a Gibbs sampling
scan, each element of parameter (cid:30)c is updated individually, while holding the other
elements of (cid:30)c (cid:12)xed. A single element of (cid:30)c is updated in a restricted Gibbs sampling
scan by drawing a new value from its full conditional distribution.

We will denote the product of conditional probabilities obtained from one full scan
of restricted Gibbs sampling as PGS . Since (cid:13) is comprised of both c and (cid:30), for clarity,
we can split the Gibbs sampling transition kernel into its factors. The order of updating
the variables does not a(cid:11)ect the validity of the method, but for presentation purposes,
we assume that Gibbs sampling updates (cid:30) (cid:12)rst (as is done in the later examples):

q((cid:13) (cid:3)j(cid:13)) = PGS((cid:30)(cid:3) j (cid:30)L; cL; y) (cid:1) PGS(c(cid:3) j cL; (cid:30)(cid:3); y)

(13)

An individual update of a particular ck is as follows:

P (ck j c(cid:0)k; (cid:30)ck ; yk) =

n(cid:0)k;ck f (yk; (cid:30)ck )

n(cid:0)k;ci f (yk; (cid:30)ci ) + n(cid:0)k;cj f (yk; (cid:30)cj )

(14)

where c(cid:0)k represents the cl for l 6= k in S [ fi; jg, n(cid:0)k;c is the number of cl for l 6= k in
S[fi; jg that are equal to c, and f (yk; (cid:30)c) is the likelihood. Here, ck is restricted to being
either ci or cj . Each time a ck or (cid:30)ck is incrementally modi(cid:12)ed during a restricted Gibbs
sampling scan, it is immediately used in the subsequent Gibbs sampling computation.

The required ratios for the split and merge proposals are shown below in equa-
tions (15) and (16), respectively. For the merge proposal, there is still only one way
to combine items in two components into one component, so PGS(cjcLmerge ; (cid:30); y) = 1
in equation (15). The same is true for P (cmergejcLmerge ; (cid:30)merge; y) in equation (16).
However, since speci(cid:12)c parameters now de(cid:12)ne the mixture components, there are nu-
merous possibilities for choosing a particular mixture component. We address this, in a
similar method as the split scenario, by conducting intermediate Gibbs sampling scans
to decide the value of the merged components parameters. One (cid:12)nal Gibbs sampling
scan is conducted from the launch state to calculate the Gibbs sampling transition
kernel.

S. Jain and R. M. Neal

455

The ratio of transition densities for the split proposal is:

q((cid:13)j(cid:13) split)
q((cid:13) splitj(cid:13))

=

=

PGS ((cid:30)split
split
i

c

j(cid:30)

Lsplit
split
i

c

ci

PGS ((cid:30)ci j(cid:30)Lmerge
; cLsplit ; y) PGS ((cid:30)split
split
j

c

; cLmerge ; y) PGS (cjcLmerge ; (cid:30); y)

j (cid:30)

Lsplit
split
j

c

; cLsplit ; y) PGS (csplitjcLsplit ; (cid:30)split; y)

PGS ((cid:30)split

c

split
i

j(cid:30)

Lsplit
split
i

c

; c

PGS ((cid:30)ci j(cid:30)Lmerge

; cLmerge ; y)

ci
Lsplit ; y) PGS ((cid:30)split

j(cid:30)

Lsplit
split
j

c

; c

Lsplit ; y) PGS (c

splitjc

Lsplit ; (cid:30)

split; y)

c

split
j

(15)

To calculate q((cid:13)j(cid:13) split), the same intermediate Gibbs sampling operations that are
performed when proposing a merge must be conducted here to arrive at a suitable
merge launch state, even though no actual merge is performed. The Gibbs sampling
transition probability is calculated from the launch state (which is the last intermediate
Gibbs sampling state) to the original merged state. These operations are necessary to
produce the correct proposal ratios.

For the merge proposal, the ratio of transition densities is:

q((cid:13)j(cid:13)merge)
q((cid:13)mergej(cid:13))

=

=

PGS ((cid:30)ci j(cid:30)

Lsplit
ci

PGS ((cid:30)merge
merge
i

c

; cLsplit ; y) PGS ((cid:30)cj j(cid:30)
j(cid:30)

Lmerge
merge
c
i

Lsplit
cj

; cLsplit ; y) PGS (cjcLsplit ; (cid:30); y)

; cLmerge ; y) PGS (cmergejcLmerge ; (cid:30)merge; y)

PGS ((cid:30)ci j(cid:30)

Lsplit
ci

; cLsplit ; y) PGS ((cid:30)cj j(cid:30)

Lsplit
cj

; cLsplit ; y) PGS (cjcLsplit ; (cid:30); y)

PGS ((cid:30)merge
merge
i

c

j(cid:30)

Lmerge
merge
c
i

; cLmerge ; y)

(16)

To obtain q((cid:13)j(cid:13)merge), we similarly perform the same intermediate Gibbs sampling
moves when proposing a split, even though no actual split is proposed (since it is
already known). This time the Gibbs sampling transition probability is calculated from
the launch state to the original split state. This ensures correct proposal ratios.

The number of intermediate Gibbs sampling scans used to arrive at suitable launch
states for both split and merge proposals are tuning parameters of this algorithm. There
is an additional tuning parameter for the nonconjugate split-merge procedure that is
not present in the conjugate version, which did not require a merge launch state.

4.4 Validity of the algorithm

The nonconjugate split-merge procedure described here is justi(cid:12)ed as a valid two-stage
random Metropolis-Hastings procedure. In the (cid:12)rst stage, we randomly select of obser-
vations i and j to decide which subset of Metropolis-Hastings proposals will be consid-
ered. In the second stage, we randomly select a launch state from among all possible
launch states (given the selection of observations i and j), by means of intermediate
Gibbs sampling scans. We then perform a standard Metropolis-Hastings update with a
proposal distribution that depends on the selection of i and j and on the launch state.

456

Splitting and Merging Components of a Nonconjugate DPMM

As discussed by Tierney (1994), a random selection among transitions (in this case, via
random selection of a proposal distribution) is a valid way of constructing Markov chain
Monte Carlo algorithms, as long as all the transitions that might be selected are valid
on their own.

A subtle clari(cid:12)cation should be pointed out regarding the construction of the Metropolis-

Hastings acceptance probability for the nonconjugate procedure. When a split is pro-
posed from a merged state, only one (cid:30)c is included in the equations, since the merged
component has only one set of parameters associated with it now. We happen to ini-
tially pick (cid:30)cj to be associated with the observations in the merged component, but this
is equivalent to initially selecting (cid:30)ci since the labels are irrelevant. To avoid changing
dimensions when we compute the Metropolis-Hastings acceptance probability, we could
include the appropriate (cid:30)ci terms in the computations. Since (cid:30)ci is an extra parameter
for the merged component that is no longer associated with the data, we choose to
propose a new value for it during the restricted Gibbs sampling scan by drawing from
its prior distribution. This choice conveniently allows the prior density for this term to
implicitly cancel with the corresponding term in the proposal density of the acceptance
probability, showing that the change in dimensionality is not a problem. Consider the
following set-up for the prior and proposal ratios for a split proposal which include the
(cid:30)ci terms. We intentionally omit the likelihoods and indicator terms for simplicity and
space considerations:

P ((cid:30)split
csplit
i

) P ((cid:30)split
csplit
j

)

P ((cid:30)ci ) P ((cid:30)cj )

PGS((cid:30)ci j(cid:30)Lmerge
ci
j(cid:30)Lsplit
csplit
i

PGS((cid:30)split
csplit
i

; cLmerge ) PGS((cid:30)cj j(cid:30)Lmerge
cj
; cLsplit; y)PGS ((cid:30)split
csplit
j

j(cid:30)Lsplit
csplit
j

; cLmerge ; y)

; cLsplit; y)

ci

The proposal factor, PGS ((cid:30)cij(cid:30)Lmerge

; cLmerge ) does not depend on the data, since
the (cid:30)cj factor has been selected earlier to be the merged components parameter. There-
fore, a new draw from (cid:30)ci s conditional distribution will be equivalent to drawing a new
value from its prior distribution, and this will cancel with the prior term, P ((cid:30)ci ). As
a result, the ratios described earlier do not need to include these terms. The identical
situation occurs in the case when a merge is proposed from an original split state and
is handled similarly.

Note that it is possible to propose any con(cid:12)guration of observations from any ini-
tial state via a sequence of split and then merge proposals. However, to ensure (cid:30)-
irreducibility on a continuous state space, it must be possible to propose any set of
parameter values for each component. This will be true if each individual restricted
Gibbs sampling conditional distribution for parameters of components that are involved
in a particular split or merge update has a positive probability density of proposing any
value. To ensure that the split-merge algorithm is well-de(cid:12)ned, the model should satisfy
the condition that the distributions F (yi; (cid:18)i) be mutually absolutely continuous for all
(cid:18) in the support of G0.

S. Jain and R. M. Neal

457

5 Performance of the nonconjugate split-merge proce-

dure

Suppose we consider a Normal mixture model, in which the data, y = (y1; : : : ; yn), are
independent and identically distributed, such that each observation, yi, given the class,
ci, has m Normally distributed attributes, (yi1; : : : ; yim). An observations attributes
are independent given the class, ci. The Normal mixture model is commonly used in
Bayesian mixture analysis because of its simplicity in constructing conditional distribu-
tions and (cid:13)exibility in modeling a number of heterogeneous populations simultaneously.

5.1 The Normal mixture model with Normal-Gamma prior

We model data from a mixture of Normal distributions using a Dirichlet process mixture
model with Normal-Gamma prior, as follows:

yi j (cid:22)i; (cid:28)i (cid:24) F (yi; (cid:22)i; (cid:28)i) = N (yi; (cid:22)i; (cid:28) (cid:0)1
((cid:22)i; (cid:28)i) j G (cid:24) G

i

I m)

G (cid:24) DP (G0; (cid:11))

G0((cid:22); (cid:28) ) = N ((cid:22); w; B(cid:0)1) (cid:1) Gamma ((cid:28) ; r; R)

(17)

where (cid:28) , the precision parameter, is (cid:27)(cid:0)2. Hyperpriors could be placed on w; B; r, and
R to add another stage to this hierarchy if desired. Here, we consider these parameters
to be known.

The probability density function for the prior distribution of (cid:22) given in (17) is:

g((cid:22) j w; B) =(cid:18) B
2(cid:25)(cid:19)

1

2

exp(cid:18) (cid:0)B

2

((cid:22) (cid:0) w)2(cid:19)

where B is a precision parameter.

The probability density function for the prior for (cid:28) is:

g((cid:28) j r; R) =

1

Rr (cid:0)(r)

R (cid:19)
(cid:28) r(cid:0)1exp(cid:18) (cid:0)(cid:28)

(18)

(19)

This parameterization of the Gamma density is adopted throughout this section.

These priors, equations (18) and (19), are necessary to compute the priors for the

parameters in the Metropolis-Hastings acceptance probability of equation (4).

It is straightforward to set up the conditional distributions required for the restricted
Gibbs sampling in the split-merge procedure used in the Metropolis-Hastings proposal
densities. For the model parameters, this amounts to sampling from the marginal
posterior distributions for a particular parameter of component c. The conditional
posterior distribution for (cid:22)ch (when (cid:28)ch is known) for a speci(cid:12)c attribute h is:

(cid:22)ch j c; y; (cid:28)ch; w; B (cid:24) N(cid:18) w B + (cid:22)ych nc (cid:28)ch

B + nc (cid:28)ch

;

1

B + nc (cid:28)ch(cid:19)

(20)

458

Splitting and Merging Components of a Nonconjugate DPMM

where nc is the number of observations belonging to component c and (cid:22)ych is the mean
of these observations for attribute h.

Similarly, if (cid:22)ch is (cid:12)xed, the conditional posterior distribution for (cid:28)ch for a particular

attribute h is:

(cid:28)ch j c; y; (cid:22)ch; r; R (cid:24) Gamma0
BBB@

r +

nc
2

;

R(cid:0)1 +

1

1

2 Xk:ck=c

(ykh (cid:0) (cid:22)ch)2

1
CCCA

(21)

The conditional posterior distribution for an indicator variable, ci, is obtained by
combining the probability of the data (given in equation 17) given a value for ci with
the prior for indicators, P (c). This yields for c 2 fcjgj6=i:

P (ci = c j c(cid:0)i; (cid:22)c; (cid:28)c; yi) / P (ci = c j c(cid:0)i) (cid:1) P (yi j (cid:22)c; (cid:28)c; c(cid:0)i)

(22)

/ n(cid:0)i;c

(cid:28)

m

Yh=1

1

2

ch exp(cid:18) (cid:0)(cid:28)ch

2

(yih (cid:0) (cid:22)ch)

2(cid:19)

These conditional distributions are also employed in computations required for Gibbs
sampling with auxiliary parameters and incremental Metropolis-Hastings updates that
will be used as comparisons to the nonconjugate split-merge technique later in this
article.

The likelihood used in computing acceptance probabilities for split-merge updates
is much simpler to obtain than in the conjugate case, since the parameters are not inte-
grated away. For the mixture of Normals, the likelihood (given component indicators)
is

L((cid:13)jy) =

D

Yc=1 Yk : ck=c

1

2

m

Yh=1 (cid:16) (cid:28)ch
2(cid:25)(cid:17)

exp(cid:18) (cid:0)(cid:28)ch

2

(ykh (cid:0) (cid:22)ch)2(cid:19)

Interchanging the products over k and h of equation (23) yields the following:

L((cid:13)jy) =

nc
2

D

Yc=1

m

Yh=1 (cid:16) (cid:28)ch
2(cid:25)(cid:17)

exp  (cid:0)(cid:28)ch

2 Xk:ck =c

(ykh (cid:0) (cid:22)ch)2!

(23)

(24)

5.2

Illustration: Beetle Data

The Dirichlet process mixture model is a useful tool in model-based, unsupervised cluster
analysis. We illustrate the practical utility of our split-merge algorithm with a six-
dimensional data set from Lubischew (1962) that has been previously used by West et al.
(1994). The data consists of six measurements of physical characteristics of three species

S. Jain and R. M. Neal

459

of male beetles for a total of n = 74 beetles. The three species are chactocnema
concina, chactocnema heikertinger, and chactocnema heptapotamica, in which nconc =
21, nheik = 31, and nhept = 22.

The measurements for the ith beetle are denoted as: yij = (yi1; : : : ; yi6) for i =

(1; : : : ; 74). The six measurements are:

y:1 = width of the (cid:12)rst joint
y:2 = width of the second joint
y:3 = maximal width of the aedeagus
y:4 = front angle of the aedeagus
y:5 = maximal width of the head
y:6 = aedeagus side-width

^(cid:22)1 = 177:3
^(cid:22)2 = 124:0
^(cid:22)3 = 50:4
^(cid:22)4 = 134:8
^(cid:22)5 = 13:0
^(cid:22)6 = 95:4

^(cid:27)2
1 = 865:1
^(cid:27)2
2 = 71:9
^(cid:27)2
3 = 7:6
^(cid:27)2
4 = 107:1
^(cid:27)2
5 = 4:6
^(cid:27)2
6 = 204:6

The objective of our analysis is to recover the three latent classes corresponding to the
three di(cid:11)erent species of beetles without using the species information in the analysis.
We apply the Normal-Gamma Dirichlet process mixture model to this data, identical to
equation 17. The Dirichlet process parameter, (cid:11), is set to one. The values for the priors
of the parameters have been set for each dimension as follows: wj = (w1; : : : ; w6) =
(100; 100; 50; 100; 25; 100), B(cid:0)1
6 ) = (500; 100; 25; 100; 25; 150) where B
is a precision parameter, r = 1 across all six dimensions, and R = 5 across all six
dimensions.

1 ; : : : ; B(cid:0)1

j = (B(cid:0)1

We applied the nonconjugate split-merge algorithm (5,1,1,5), in which (cid:12)ve interme-
diate Gibbs sampling scans were each used to reach the launch states for the split and
merge proposals. One split-merge update was used in a single iteration and one (cid:12)nal
incremental Gibbs sampling scan was conducted after the (cid:12)nal split-merge update. For
comparison purposes, we considered the Gibbs sampling technique of Neal (2000) with
v = 3 auxiliary components to this data. Computation time per iteration is similar
for both algorithms. For each algorithm, results are provided for the case in which all
observations are initially assigned to the same mixture component, and each algorithm
is run for 5000 iterations.

From the two top trace plots given in Figure 1, it is evident that Gibbs sampling is
unable to separate the data and leaves all observations in the same mixture component.
It is clear that Gibbs sampling will take longer to reach equilibrium. On the other
hand, split-merge splits the data into three major clusters (corresponding to the correct
proportion of observations to species, i.e. 42%, 30% and 28%.) within the (cid:12)rst twenty
iterations.

To generate the two bottom trace plots in Figure 1, we set the prior values of wj
and B(cid:0)1 to be more re(cid:13)ective of the data. The values used are: wj = (w1; : : : ; w6) =
(100; 100; 50; 100; 10; 100) and B(cid:0)1
Gibbs sampling does recover the three di(cid:11)erent species groups almost immediately, it is
important to note that it becomes stuck in a low probability two-component con(cid:12)gura-
tion and mixes poorly. However, split-merge continues to mix well in a three-component
con(cid:12)guration.

6 ) = (800; 100; 10; 100; 10; 200). While

1 ; : : : ; B(cid:0)1

j = (B(cid:0)1

As a (cid:12)nal check, the simulations were repeated by starting the simulation from

460

Splitting and Merging Components of a Nonconjugate DPMM

a typical state of the competing methods apparent equilibrium distribution. Gibbs
sampling stayed in the three-component state that it was started from, con(cid:12)rming that
the three-component state has high posterior probability, and that the di(cid:11)erence seen
is not the result of some bug in the split-merge procedure. When the simulations were
repeated using an initial state in which each observation is in a di(cid:11)erent component,
the Gibbs sampler is able to reach equilibrium sooner and performs better.

The results from the beetle data illustration show that Gibbs sampling experiences
a long burn-in time compared to the nonconjugate split-merge technique and is not
always suitable for high-dimensional analysis. While it is true that the values of the
priors for the parameters may not be ideal and that more realistic values may yield
better sampling, often in real data analysis, there is no a priori information to suggest
reasonable priors. A Markov chain Monte Carlo technique that can overcome poor
choices in priors is preferred, as illustrated here, since this leads to shorter burn-in
times and full exploration of the posterior distribution.

6 Discussion

The nonincremental split-merge procedure for nonconjugate models introduced in this
article avoids the problem of being trapped in local modes, allowing the posterior dis-
tribution to be fully explored. In general, the nonconjugate split-merge procedure can
become computationally expensive, but when Gibbs sampling or some other incremen-
tal procedure fails to reach equilibrium in a sensible amount of time, this procedure
becomes necessary. Another related issue is burn-in time. Even if an incremental pro-
cedure reaches stationarity within a desired time limit, one must often discard a large
number of early iterations, which can lead to poor estimates. In split-merge type sit-
uations, the computational burden of using a nonincremental procedure is o(cid:11)set by its
quick burn-in and dramatic improvement in performance. To further improve sampling
performance in which both large changes to the clustering con(cid:12)guration and small re(cid:12)ne-
ments are required, we recommend combining split-merge and Gibbs sampling updates
as a way to reap the bene(cid:12)ts of both samplers.

In higher dimensions, split-merge procedures continue to work well as the compo-
nents are moved closer together. Convergence to the equilibrium distribution is rela-
tively quick. It is possible that the split-merge procedure may break down for very high
dimensional problems, because appropriate splits will be rejected, since it will become
unlikely that a merge operation from the split state would produce the same merged
parameter values as the current state. However, we have not encountered an example
of this. Perhaps this issue arises only in situations where the dimensionality is in the
hundreds.

A possible extension of the split-merge technique is to employ the Dahl (2003) se-
quentially allocated split-merge sampler as a method to initialize the intermediate Gibbs
sampling step. This method could potentially provide a better starting state than our
method of performing a random split of items and selecting values for the parameters
from the prior.

S. Jain and R. M. Neal

7 Appendix

461

The purpose of the following simulation study is to classify observations into appropriate
latent classes using the Normal-Gamma Dirichlet process mixture model. We can make
this problem computationally more di(cid:14)cult by increasing the dimensionality of the data
and by moving the components closer together. Various combinations of these factors
were tested on all procedures. We found that the split-merge procedures outperformed
the incremental procedures even in very low-dimensional problems, in which distinct
components were visible by eye, showing the di(cid:14)culty that incremental samplers have
in reaching equilibrium even in simple problems when the components are similar.

We will consider two simulated data sets with a (cid:12)nite number of components. We
expect that the Dirichlet process mixture model will model the (cid:12)nite situation perfectly
well without problems such as over(cid:12)tting, even though the model allows an in(cid:12)nite
number of components. For each of the two examples, the data are composed of (cid:12)ve
equally-probable mixture components, in which each component is a distribution over
m dimensions. To maintain uniformity amongst the examples, we generated n = 100
observations, strati(cid:12)ed so that 20 observations came from each of the (cid:12)ve mixture
components.

Data for the two examples were randomly generated from the mixture distributions
shown in Tables 1 and 2. Scatterplots of the data are shown in Figures 2 and 3. A
standard deviation of 0.2 was selected for all Normal distributions, so that only the
means would vary. The (cid:12)rst example holds the dimensionality at two. The second
example di(cid:11)ers from the (cid:12)rst in that the dimensionality is increased to three, and the
components are closer together.
Intentional asymmetry is introduced so that three
components are more similar than the other two. This is intended to test whether the
nonconjugate split-merge techniques can split in three ways.

The Dirichlet process parameter, (cid:11), is set to one for all demonstrations. Recall that
a small value of (cid:11) places stronger belief that the number of mixture components in
the data is likely to be small. The parameters of the priors for the parameters on the
component distributions have been set to the same values over all dimensions as follows:
w = 5, B = 1=12, r = 1, and R = 5. Here, B is a precision parameter. For consistency,
these parameters are (cid:12)xed at these values for all simulations. In actual problems, these
parameters could be set either by prior knowledge or given higher-level priors.

7.1 Performance

For the two examples, two incremental procedures, Gibbs sampling with v = 3 auxil-
iary variables, and an incremental Metropolis-Hastings method, are compared to four
versions of the nonconjugate split-merge procedure. We use four parameters to describe
the various split-merge procedures:

1. Number of intermediate Gibbs sampling scans to reach the launch state for a split

proposal

462

Splitting and Merging Components of a Nonconjugate DPMM

Table 1: True mixture distribution for Example 1.

c
1
2
3
4
5

P (ci = c)

P (yihjci = c); h = 1; 2

0.2
0.2
0.2
0.2
0.2

N(2.0, 0.04) N(3.0, 0.04)
N(3.0, 0.04) N(2.0, 0.04)
N(3.3, 0.04) N(3.3, 0.04)
N(8.0, 0.04) N(9.0, 0.04)
N(9.0, 0.04) N(8.5, 0.04)

Table 2: True mixture distribution for Example 2.

c
1
2
3
4
5

P (ci = c)

P (yihjci = c); h = 1; 2; 3

0.2
0.2
0.2
0.2
0.2

N(2.0, 0.04) N(2.0, 0.04) N(3.0, 0.04)
N(2.0, 0.04) N(3.0, 0.04) N(2.0, 0.04)
N(2.0, 0.04) N(2.5, 0.04) N(2.5, 0.04)
N(8.0, 0.04) N(8.0, 0.04) N(8.0, 0.04)
N(8.0, 0.04) N(9.0, 0.04) N(9.0, 0.04)

S. Jain and R. M. Neal

463

2. Number of split-merge updates done in a single overall iteration

3. Number of complete incremental Gibbs sampling scans after the (cid:12)nal split-merge

update

4. Number of intermediate Gibbs sampling scans to reach the launch state for a

merge proposal

The four split-merge procedures we tested are described using these numbers as Split-
Merge (0,1,0,0), Split-Merge (5,1,0,5), Split-Merge (0,1,1,0), and Split-Merge (5,1,1,5).

We compared the split-merge procedures with both the auxiliary variable and

Metropolis-Hastings incremental samplers because we did not know beforehand which
incremental method would perform better in situations where splits and merges might
be necessary. Performance of the auxiliary variable Gibbs sampling is expected to
improve as we increase the number of auxiliary components, except that it also takes
longer per iteration (Neal (2000)). We did vary this parameter, but will report (cid:12)ndings
for v = 3 for all examples, since this version is comparable to the best version of split-
merge in terms of computation time per iteration. As the incremental (cid:12)nal scan for
the split-merge procedure, Gibbs sampling with one auxiliary variable is used for all
examples.

Performance measures that were considered include trace plots over time (Figures 4
and 5) and computation time per iteration (Table 3). The trace plots show (cid:12)ve values
which represent the fractions of observations associated with the most common, two
most common, three most common, four most common, and (cid:12)ve most common mixture
components. Since each of the (cid:12)ve components appear equally in the samples, if the
true situation were captured exactly, the (cid:12)ve traces would occur at values of 0.2, 0.4,
0.6, 0.8, and 1.0.

For each algorithm, all observations were assigned to the same mixture component
for the initial state, and each algorithm was run for 5000 iterations. All simulations
were performed on Matlab, Version 6.1, on a Dell Precision 530 workstation (which has
a 1.7 GHz Pentium 4 processor). Note that the computation times reported include the
extra time spent due to Matlabs ine(cid:14)ciencies when copying and incrementally updating
arrays, which are not inherent in the algorithm.

7.1.1 Example 1

The three types of procedures, incremental Metropolis-Hastings, incremental Gibbs sam-
pling with auxiliary variables, and split-merge, correctly classify the data in Figure 2
into (cid:12)ve distinct clusters. The main di(cid:11)erence in performance is the number of burn-in
iterations that must be discarded.

The trace plots in Figure 4 show that Gibbs sampling with three auxiliary param-
eters has fewer burn-in iterations than the incremental Metropolis-Hastings method
(compare 1000 to 3200 burn-in iterations). However, since the incremental Metropolis-
Hastings method is approximately 5.5 times faster per iteration than the auxiliary Gibbs

464

Splitting and Merging Components of a Nonconjugate DPMM

Table 3: Time per iteration (in seconds) for the algorithms tested.

Algorithm

Example 1 Example 2

Incremental M-H
Gibbs Sampling
Split-Merge (0,1,0,0)
Split-Merge (0,1,1,0)
Split-Merge (5,1,0,5)
Split-Merge (5,1,1,5)

0.08
0.45
0.05
0.27
0.16
0.40

0.09
0.60
0.10
0.35
0.24
0.53

sampling method, it actually converges sooner with respect to computation time. Split-
Merge (5,1,0,5) almost immediately splits the data into (cid:12)ve components, but notice
that the proportions do not occur at exactly 0.2 intervals until after the (cid:12)rst thousand
iterations. It takes this procedure longer to move a few singleton observations between
components, since there is no (cid:12)nal incremental update to make these minor adjust-
ments. In (cid:12)ve thousand iterations, it is not clear if Split-Merge (5,1,0,5) has actually
reached the equilibrium distribution. Split-Merge (0,1,0,0) does not reach the equilib-
rium distribution in the (cid:12)ve thousand iterations shown. Because the split and merge
proposals have no intermediate Gibbs sampling scans, the proposals are not expected to
be realistic. Split-Merge (0,1,0,0) is essentially a simple random split procedure, except
that one restricted Gibbs sampling scan is conducted to reach the (cid:12)nal state, which of
course will not lead to reasonable split and merge proposals.

However, either by adding intermediate Gibbs sampling scans (as in the case of Split-
Merge (5,1,0,5)) or adding a (cid:12)nal full incremental scan (as in Split-Merge (0,1,1,0)),
the correct proportion of items in each cluster is established. Split-Merge (0,1,1,0)
eventually reaches the (cid:12)ve component con(cid:12)guration after 500 burn-in iterations. The
(cid:12)nal procedure of Figure 4, Split-Merge (5,1,1,5), (cid:12)nds the (cid:12)ve components immediately,
and it appears that there is negligible burn-in (four iterations). The computation time
per iteration is higher for Split-Merge (5,1,1,5) versus Split-Merge (0,1,1,0) and (5,1,0,5),
but the computation time to equilibrium is much lower.

7.1.2 Example 2

Example 2 has three dimensions and the mixture components are close together. A
perspective scatterplot of the data is given in Figure 3, and it shows that the compo-
nents are di(cid:14)cult to distinguish. Given the priors selected, there is signi(cid:12)cant posterior
probability for both the four and (cid:12)ve mixture component con(cid:12)gurations. Only Split-
Merge (5,1,0,5) and Split-Merge (5,1,1,5) mix between these con(cid:12)gurations, as observed
in Figure 5. The incremental samplers and the split-merge procedures with zero in-
termediate restricted Gibbs sampling scans do not (cid:12)nd the (cid:12)ve components over the
5000 iterations, but are stuck in either two or four components. If each item is initially
assigned to a di(cid:11)erent mixture component (plots not included), these samplers do split
the data into (cid:12)ve components, but take a long time to move to four components, indi-

S. Jain and R. M. Neal

465

cating poor mixing. Here, the problem is that the deletion of a component is rare under
both incremental updates and poor split-merge proposals.

Comparing further the two procedures that appear to converge, the autocorrelation
time for trace 1 is much lower for Split-Merge (5,1,1,5) than Split-Merge (5,1,0,5) (126 vs.
718). For the autocorrelation time of an indicator variable, I26;57, coding if observations
26 and 57 are in the same component, the time is much lower for Split-Merge (5,1,1,5)
(38 vs. 417). Even though both algorithms do mix between the two con(cid:12)gurations and
Split-Merge (5,1,0,5) is faster per iteration, the improvement in autocorrelation time for
Split-Merge (5,1,1,5) cannot be ignored. The extra full scan of incremental sampling
for minor adjustments is worth the computational e(cid:11)ort.

7.1.3 Summary of (cid:12)ndings

It appears that split-merge moves are necessary in nonconjugate problems of this sort.
Incremental samplers perform adequately when the components are distinct clusters in
low dimensions, but as the components become more di(cid:14)cult to distinguish, these sam-
plers take much longer to reach equilibrium. It is important to note that the incremental
samplers begin to break down even in low dimensions. The split-merge procedures are
able to handle three-way splits without any problems, although this is done by two
two-way splits.

The split-merge procedure with several intermediate Gibbs sampling scans followed
by an incremental full scan is the best version of the split-merge procedure. The split-
merge method relies on proposing appropriate new clusters, which is accomplished by
conducting several intermediate scans to reach the split and merge launch states. The
split-merge methods generally have a longer computation time per iteration. However,
in the case of the Gibbs sampling procedure with v = 3 auxiliary parameters, the
best version of the split-merge procedure, Split-Merge (5,1,1,5), is slightly faster in our
implementation (see Table 3). Therefore, there does not appear to be any advantage in
using only incremental procedures for these types of problems.

