Abstract

In order to compare learning algorithms, experimental results reported in the machine
learning literature often use statistical tests of signi(cid:12)cance to support the claim that a new
learning algorithm generalizes better. Such tests should take into account the variability
due to the choice of training set and not only that due to the test examples, as is often
the case. This could lead to gross underestimation of the variance of the cross-validation
estimator, and to the wrong conclusion that the new algorithm is signi(cid:12)cantly better
when it is not. We perform a theoretical investigation of the variance of a cross-validation
estimator of the generalization error that takes into account the variability due to the
randomness of the training set as well as test examples. Our analysis shows that all the
variance estimators that are based only on the results of the cross-validation experiment
must be biased. This analysis allows us to propose new estimators of this variance. We
show, via simulations, that tests of hypothesis about the generalization error using those
new variance estimators have better properties than tests involving variance estimators
currently in use and listed in (Dietterich, 1998). In particular, the new tests have correct
size and good power. That is, the new tests do not reject the null hypothesis too often
when the hypothesis is true, but they tend to frequently reject the null hypothesis when
the latter is false.

Keywords: Generalization error, cross-validation, variance estimation, hypothesis tests,

size, power.

Corresponding Author: Yoshua Bengio, Dept.

IRO, Universit(cid:19)e de Montr(cid:19)eal, 2920
Chemin de la Tour, Montr(cid:19)eal, Qu(cid:19)ebec, Canada H3T 1J8, suite #2194. Tel:(514) 343-6804,
Fax: (514) 343-5834 e-mail: Yoshua.Bengio@umontreal.ca

1

1 Generalization Error and its Estimation

In order to compare learning algorithms, experimental results reported in the machine learning
literature often use statistical tests of signi(cid:12)cance. Unfortunately, these tests often rely solely
on the variability due to the test examples and do not take into account the variability due
to the randomness of the training set. We perform a theoretical investigation of the
variance of a cross-validation estimator of the generalization error that takes into account the
variability due to the choice of training sets as well as of test examples (hold-out set). When
applying a learning algorithm (or comparing several algorithms), one is typically interested
in estimating its generalization error. Its estimation is rather trivial through cross-validation
or the bootstrap. Providing a variance estimate of the cross-validation estimator, so that
hypothesis testing and/or con(cid:12)dence intervals are possible, is more di(cid:14)cult, especially, as
pointed out in (Hinton, Neal, Tibshirani, & DELVE team members, 1995), if one wants to
take into account various sources of variability such as the choice of the training set (Breiman,
1996) or initial conditions of a learning algorithm (Kolen & Pollack, 1991). A notable e(cid:11)ort
in that direction is Dietterichs work (Dietterich, 1998). See also the review of bounds of the
accuracy of various cross-validation estimates in (Devroye, Gyro(cid:12), & Lugosi, 1996). Building
upon (Dietterich, 1998), in this paper we take into account the variability due to the choice of
training sets and test examples. Speci(cid:12)cally, an investigation of the variance to be estimated
allows us to provide two new variance estimators, one of which is conservative by construction.
The choice of estimator for the variance of an average test error (or of the di(cid:11)erence between
the average error made by two algorithms) is very important: a poor choice of estimator
(especially if it is liberal, i.e. underestimates the variance) could lead to a profusion of
publications in which method A is incorrectly claimed to be better than a previously proposed
method B. Because of the approximately normal behavior of average error, an underestimation
of the standard deviation by a factor 2, say, can yield to about 6 times more \false claims" than
would have been expected for a test level of 5%. If the habit of making rigorous statistical
comparisons and in particular avoiding the use of a liberal estimator is not ingrained in
the machine learning community, it could be tempting for many researchers to use a liberal
estimate of variance when comparing their preferred algorithm against the competition. For
this reason, it is very important that reviewers insist on analyses of results that avoid liberal
estimators of variance (for con(cid:12)dence intervals or to test the null hypothesis of method A
being not better than method B).

Let us de(cid:12)ne what we mean by \generalization error" and say how it will be estimated in
1 = fZ1; : : : ; Zng. For example,
this paper. We assume that data is available in the form Z n
in the case of supervised learning, Zi = (Xi; Yi) 2 Z (cid:18) Rp+q , where p and q denote the
dimensions of the Xis (inputs) and the Yis (outputs). We also assume that the Zis are
independent with Zi (cid:24) P (Z), where the generating distribution P is unknown. Let L(D; Z),
where D represents a subset of size n1 (cid:20) n taken from Z n
1 , be a function from Z n1 (cid:2) Z to R.
For instance, this function could be the loss incurred by the decision that a learning algorithm
trained on D makes on a new example Z.
1 ; Zn+1)] where Zn+1 (cid:24) P (Z) is independent
1 here). Note that the above

We are interested in estimating n(cid:22) (cid:17) E[L(Z n
1 . The subscript n stands for the size of the training set (Z n

of Z n

2

expectation is taken over Z n
1 and Zn+1, meaning that we are interested in the performance of
an algorithm rather than the performance of the speci(cid:12)c decision function it yields on the data
at hand. Dietterich (Dietterich, 1998) has introduced a taxonomy of statistical questions in
Machine Learning, which we briey summarize here. At the top level is whether the questions
refer to single or multiple domains (type 9). For single domains, Dietterich distinguishes
between the analysis of a single (given or already trained) predictor (types 1 through 4) and
the analysis of a learning algorithm that can yield such predictors given a training set (types
5 through 8). For the former, the training set is considered (cid:12)xed, whereas for the latter the
training set is considered to be random. In this paper, we are concerned with the analysis of
the performance of learning algorithms (types 5 through 8), not of particular trained predictors.
Dietterich further splits types 5 through 8 according to whether the sample size is large or
not and whether one is interested in the generalization error of a single algorithm or wants to
compare the generalization errors of various algorithms.

Let us now introduce some notation and de(cid:12)nitions. We shall call n(cid:22) the generalization

error even though it can go beyond that as we now illustrate. Here are two examples.

(cid:15) Generalization error

We may take as our basic measurement

L(D; Z) = L(D; (X; Y )) = Q(F (D)(X); Y );

(1)

where F represents a learning algorithm that yields a function f = F (D)
(F (D) : Rp ! Rq ), when training the algorithm on D, and Q is a loss function measur-
ing the inaccuracy of a decision f(X) when Y is observed. For instance, for classi(cid:12)cation
problems, we could have

Q(^y; y) = I[^y 6= y];

where I[

] is the indicator function, and in the case of regression,

Q(^y; y) =k ^y  y k2;

(2)

(3)

where k (cid:1) k is the Euclidean norm. In that case n(cid:22) = E[L(Z n
tion error of algorithm F on data sampled from P .

1 ; Zn+1)] is the generaliza-

(cid:15) Comparison of generalization errors

Sometimes, what we are interested in is not the performance of algorithms per se, but
how two algorithms compare with each other. In that case we may want to consider

L(D; Z) = L(D; (X; Y )) = Q(FA(D)(X); Y )  Q(FB(D)(X); Y );

(4)

where FA(D) and FB(D) are decision functions obtained when training two algorithms
(respectively A and B) on D, and Q is a loss function. In this case n(cid:22) would be a
di(cid:11)erence of generalization errors.

The generalization error is often estimated via some form of cross-validation. Since there

3

are various versions of the latter, we lay out the speci(cid:12)c form we use in this paper.

j = f1; : : : ; ng n Sj denote the complement of Sj.

(cid:15) Let Sj be a random set of n1 distinct integers from f1; : : : ; ng(n1 < n). Here n1 is not
random and represents the size of a training set. We shall let n2 = n  n1 be the size of
the corresponding test set (or hold-out set).
(cid:15) Let S1; : : : SJ be such random index sets (of size n1), sampled independently of each
other, and let Sc
(cid:15) Let ZSj = fZiji 2 Sjg be the training set obtained by subsampling Z n
1 according to the
g.
= fZiji 2 Sc
random index set Sj. The corresponding test set (or hold-out set) is ZSc
(cid:15) Let L(j; i) = L(ZSj ; Zi). According to (1), this could be the error an algorithm trained
on the training set ZSj makes on example Zi. According to (4), this could be the
di(cid:11)erence of such errors for two di(cid:11)erent algorithms.
(cid:15) Let ^(cid:22)j = 1
ZSc

L(j; i) denote the usual \average test error" measured on the test set

P

i2Sc

j

n2

.

j

j

j

Then the cross-validation estimate of the generalization error considered in this paper is

JX

j=1

n1 ^(cid:22)J =
n2

1
J

^(cid:22)j:

(5)

Note that this an unbiased estimator of n1(cid:22) = E[L(Z n1
as n(cid:22).

1 ; Zn+1)], which is not quite the same

This paper is about the estimation of the variance of the above cross-validation estimator.
There are many variants of cross-validation, and the above variant is close to the popular
K-fold cross-validation estimator, which has been found more reliable than the leave-one-out
estimator (Kohavi, 1995). It should be noted that our goal in this paper is not to compare
algorithms in order to perform model selection (i.e.
to choose exactly one among several
learning algorithms for a particular task, given a data set on which to train them). The use of
cross-validation estimators for model selection has sparked a debate in the last few years (Zhu
& Rohwer, 1996; Goutte, 1997) related to the \no free lunch theorem" (Wolpert & Macready,
1995), since cross-validation model selection often works well in practice but it is probably
not a universally good procedure.

This paper does not address the issue of model selection but rather that of estimating
n1 ^(cid:22)J.
the uncertainty in a cross-validation type of estimator for generalization error, namely n2
For this purpose, this paper studies estimators of the variance of n2
n1 ^(cid:22)J (in the sense that
1 had been sampled
di(cid:11)erent values of n2
from the same unknown underlying distribution P and di(cid:11)erent random index sets Sjs had
been generated). The application of the estimators studied in this paper may be for example
(1) to provide con(cid:12)dence intervals around estimated generalization error, or (2) to perform
a hypothesis test in order to determine whether an algorithms estimated performance is

n1 ^(cid:22)J would have been obtained if a di(cid:11)erent data set Z n

4

signi(cid:12)cantly above or below the performance of another algorithm. The latter is very important
when researchers introduce a new learning algorithm and they want to show that it brings a
signi(cid:12)cant improvement with respect to previously known algorithms.

We (cid:12)rst study theoretically the variance of n2

n1 ^(cid:22)J in Section 2. This will lead us to two
new variance estimators we develop in Section 3. Section 4 shows how to test hypotheses or
construct con(cid:12)dence intervals. Section 5 describes a simulation study we performed to see
how the proposed statistics behave compared to statistics already in use. Section 6 concludes
the paper. Before proceeding with the rest of the paper, some readers may prefer to read
Appendix A.0 that presents some statistical prerequisites relevant to the rest of the paper.

^(cid:22)J ]

2 Analysis of V ar[ n2
n1
In this section, we study V ar[ n2
n1 ^(cid:22)J] and discuss the di(cid:14)culty of estimating it. This section is
important as it enables us to understand why some inference procedures about n1(cid:22) presently
in use are inadequate, as we shall underline in Section 4. This investigation also enables us to
develop estimators of V ar[ n2
n1 ^(cid:22)J] in Section 3. Before we proceed, we state a lemma that will
prove useful in this section, and later ones as well.

Lemma 1 Let U1; : : : ; UK be random variables with common mean (cid:12) and the following co-
variance structure

V ar[Uk] = (cid:14); 8k

C ov[Uk; Uk0] = ; 8k 6= k

0

:

P
(cid:14) be the correlation between Uk and Uk0 (k 6= k
k=1(Uk  (cid:22)U)2 be the sample mean and sample variance respectively. Then

0). Let (cid:22)U = k

1

k=1 Uk and
K

P

Let (cid:25) = 
U = 1
S2
K1
1. V ar[ (cid:22)U] =  + ((cid:14))

K

(cid:16)

K = (cid:14)

(cid:25) + 1(cid:25)

K

:

(cid:17)

2. If the stated covariance structure holds for all K (with  and (cid:14) not depending on K),

then

(cid:15)  (cid:21) 0,
(cid:15) limK!1V ar[ (cid:22)U] = 0 ,  = 0.
U ] = (cid:14)  .

3. E[S2

Proof

1. This result is obtained from a standard development of V ar[ (cid:22)U].
2. If  < 0, then V ar[ (cid:22)U] would eventually become negative as K is increased. We thus
conclude that  (cid:21) 0. From item 1, it is obvious that V ar[ (cid:22)U] goes to zero as K goes to
in(cid:12)nity if and only if  = 0.

5

3. Again, this only requires careful development of the expectation. The task is somewhat

easier if one uses the identity

U =
S2

1

K  1

 (cid:22)U 2) =

(U 2
k

1

2K(K  1)

(Uk  Uk0)2:

KX

KX

k0=1

k=1

KX

k=1

Although we only need it in Section 4, it is natural to introduce a second lemma here as

it is a continuation of Lemma 1.

P
Lemma 2 Let U1; : : : ; UK ; UK+1 be random variables with mean, variance and covariance as
described in Lemma 1. In addition, assume that the vector (U1; : : : ; UK ; UK+1) follows the
k=1(Uk 
multivariate Gaussian distribution. Again, let (cid:22)U = K
U = 1
K1
(cid:22)U)2 be respectively the sample mean and sample variance of U1; : : : ; UK. Then

k=1 Uk and S2
K

P

1

K

p
1  (cid:25) UK+1(cid:12)p
q
S2
U
p
K( (cid:22)U(cid:12))p

(cid:24) tK1;

1.

2.

where (cid:25) = 
of freedom.
Proof See Appendix A.1.

1+(K1)(cid:25)

(cid:24) tK1;

1(cid:25)
(cid:14) as in Lemma 1, and tK1 refers to Students t distribution with (K  1) degrees

S2
U

To study V ar[ n2

n1 ^(cid:22)J] we need to de(cid:12)ne the following covariances. In the following, Sj and
Sj0 are independent random index sets, each consisting of n1 distinct integers from f1; : : : ; ng.
Also, expectations are totally unconditional, that is expectations (as well as variances and
covariances) are taken over Z n

0.
1 , Sj, Sj0, i and i

1

1

1 ; Zn1+1)]] + V arSj;i[ n1(cid:22)] = V ar[L(Z n1

j . To establish that
[L(ZSj ; Zi)jSj; i]] +
[L(ZSj ; Zi)jSj; i]]. Now the distribution of L(ZSj ; Zi) does not depend on
1 ; Zn1+1). Thus
1 ; Zn1+1)] depends only on

(cid:15) Let (cid:27)0 = (cid:27)0(n1) = V ar[L(j; i)] when i is randomly drawn from Sc
(cid:27)0 does not depend on n2 we note that V ar[L(j; i)] = ESj ;i[V arZn
V arSj ;i[EZn
the particular realization of Sj and i, it is just the distribution of L(Z n1
(cid:27)0 = ESj ;i[V ar[L(Z n1
n1, not on n2.
(cid:15) Let (cid:27)1 = (cid:27)1(n1; n2) = V ar[^(cid:22)j].
(cid:15) Let (cid:27)2 = (cid:27)2(n1; n2) = C ov[L(j; i); L(j
dently drawn from Sc
(cid:15) Let (cid:27)3 = (cid:27)3(n1) = C ov[L(j; i); L(j; i
sampled without replacement from Sc
show that (cid:27)3 does not depend on n2.

0)] for i; i
0 are
j . Using a similar argument as for (cid:27)0 allows one to

0, that is i and i

0 randomly and indepen-

0)], with j 6= j
; i

j and i 6= i

j0 respectively.

0, i and i

0 2 Sc

j and Sc

0

6

Let us look at the mean and variance of ^(cid:22)j (i.e., over one set) and n2

Concerning expectations, we obviously have E[^(cid:22)j] = n1(cid:22) and thus E[ n2
Lemma 1, we have

n1 ^(cid:22)J (i.e. over J sets).
n1 ^(cid:22)J] = n1(cid:22). From

(cid:27)1 = (cid:27)1(n1; n2) = V ar[^(cid:22)j] = (cid:27)3 + (cid:27)0  (cid:27)3

(n2  1)(cid:27)3 + (cid:27)0

n2

:

=

n2

For j 6= j

0, we have

C ov[^(cid:22)j; ^(cid:22)j0] =

X

i2Sc

j

1
n2
2

and therefore (using Lemma 1 again)

X

i02Sc
j0

(cid:18)

n1 ^(cid:22)J] = (cid:27)2 + (cid:27)1  (cid:27)2

V ar[ n2

J

= (cid:27)1

(cid:26) +

C ov[L(j; i); L(j

0

0
; i

)] = (cid:27)2;

(cid:19)

1  (cid:26)
J

= (cid:27)2 + (cid:27)3  (cid:27)2

J

+ (cid:27)0  (cid:27)3

n2J

;

(6)

(7)

(8)

where (cid:26) = (cid:27)2
= corr[^(cid:22)j; ^(cid:22)j0]: Asking how to choose J amounts to asking how large is (cid:26). If it
(cid:27)1
is large, then taking J > 1 (rather than J = 1) does not provide much improvement in the
estimation of n1(cid:22). We provide some guidance on the choice of J in Section 5.

Equation (8) lends itself to an interesting interpretation. First we get that (cid:27)2 = V ar[ n2

n1 ^(cid:22)1]

with

JX

j=1

1(cid:0)
(cid:1)

n
n1

n2

^(cid:22)j =

X

X

s2C(f1;:::;ng;n1)

i2f1;:::;ngns

L(Zs; Zi);

n1 ^(cid:22)1 = lim
n2
J!1

n1 ^(cid:22)J = lim
n2
J!1

1
J

(cid:0)

(cid:1)

where C(f1; : : : ; ng; n1), as de(cid:12)ned in Appendix A.2, is the set of all possible subsets of n1
distinct integers from f1; : : : ; ng. We justify the last equality as follows. What happens when
n
n2 di(cid:11)erent ways to choose a training
J goes to in(cid:12)nity is that all possible errors (there are
n1
n1 ^(cid:22)1 is like
set and a test example) appear with relative frequency
( n
n1
possible training sets are chosen exactly once. Briey, sampling
n1 ^(cid:22)J except that all
n2
in(cid:12)nitely often with replacement is equivalent to sampling exhaustively without replacement
n1 ^(cid:22)JjZ n
1 ] .
(i.e. a census). We also have n2
Thus (cid:27)2 = V ar[E[ n2
We shall often encounter (cid:27)0; (cid:27)1; (cid:27)2 and (cid:27)3 in the future, so some knowledge about those

1 ]] so that we must have E[V ar[ n2

1 ] 8j and therefore n2

n1 ^(cid:22)1 = ESj [^(cid:22)jjZ n

. In other words, n2

n1 ^(cid:22)1 = E[ n2

n1 ^(cid:22)JjZ n

n1 ^(cid:22)JjZ n

1 ]] = (cid:27)1(cid:27)2

J

1
)n2

(cid:0)

(cid:1)

n
n1

.

quantities is valuable. Heres what we can say about them.
Proposition 1 For given n1 and n2, we have 0 (cid:20) (cid:27)2 (cid:20) (cid:27)1 (cid:20) (cid:27)0 and 0 (cid:20) (cid:27)3 (cid:20) (cid:27)1.
Proof For j 6= j

0 we have

q

(cid:27)2 = C ov[^(cid:22)j; ^(cid:22)j0] (cid:20)

V ar[^(cid:22)j]V ar[^(cid:22)j0] = (cid:27)1:

Since (cid:27)0 = V ar[L(j; i)]; i 2 Sc
V ar[L(j; i)] = (cid:27)0. The fact that limJ!1 V ar[ n2

j and ^(cid:22)j is the mean of the L(j; i)s, then (cid:27)1 = V ar[^(cid:22)j] (cid:20)

n1 ^(cid:22)J] = (cid:27)2 provides the inequality 0 (cid:20) (cid:27)2.

7

Regarding (cid:27)3, we deduce (cid:27)3 (cid:20) (cid:27)1 from (6) while 0 (cid:20) (cid:27)3 is derived from the fact that

limn2!1 V ar[^(cid:22)j] = (cid:27)3.
Naturally the inequalities are strict provided L(j; i) is not perfectly correlated with L(j; i
^(cid:22)j is not perfectly correlated with ^(cid:22)j0, and the variances used in the proof are positive.
n1 ^(cid:22)J is how n1, n2 and J a(cid:11)ect its variance.

A natural question about the estimator n2

0),

Proposition 2 The variance of n2
Proof

n1 ^(cid:22)J is non-increasing in J and n2.

(cid:15) V ar[ n2

n1 ^(cid:22)J] is non-increasing (decreasing actually, unless (cid:27)1 = (cid:27)2) in J as obviously
seen from (8). This means that averaging over many train/test improves the estimation
of n1(cid:22).

(cid:15) From (8), we see that to show that V ar[ n2

n1 ^(cid:22)J] is non-increasing in n2, it is su(cid:14)cient
to show that (cid:27)1 and (cid:27)2 are non-increasing in n2. For (cid:27)1, this follows from (6) and
Proposition 3. Regarding (cid:27)2, we show in Appendix A.2 that it is non-increasing in n2.
All this to say that for a given n1, the larger the test set size, the better the estimation
of n1(cid:22).

The behavior of V ar[ n2

n1 ^(cid:22)J] with respect to n1 is unclear, but we conjecture as follows.

The variability in n2

n1 ^(cid:22)J] should decrease in n1.

Conjecture 1 In most situations, V ar[ n2
n1 ^(cid:22)J comes from two sources: sampling decision rules
Argument 1
(training process) and sampling testing examples. Holding n2 and J (cid:12)xed freezes the second
source of variation as it solely depends on those two quantities, not n1. The problem to solve
becomes: how does n1 a(cid:11)ect the (cid:12)rst source of variation? It is not unreasonable to expect
that the decision function yielded by a \stable" learning algorithm is less variable when the
training set is larger. See (Kearns & Ron, 1997) showing that for a large class of algorithms
including those minimizing training error, cross-validation estimators are not much worse
than the training error estimator (which itself improves in O(V Cdim=n1) as the size of the
training set increases (Vapnik, 1982)). Therefore we conclude that, for many cases of interest,
n1 ^(cid:22)J]) is decreasing in
the (cid:12)rst source of variation, and thus the total variation (that is V ar[ n2
n1.

Regarding the estimation of V ar[ n2

asedly ((cid:27)1  (cid:27)2), ((cid:27)0  (cid:27)3) and ((cid:27)2 + ( n1(cid:22))2).

n1 ^(cid:22)J], we show below that we can easily estimate unbi-

(cid:15) From Lemma 1, we obtain readily that the sample variance of the ^(cid:22)js (call it S2
in equation (9)) is an unbiased estimate of (cid:27)1  (cid:27)2 = (cid:27)3  (cid:27)2 + (cid:27)0(cid:27)3
this result. Given Z n
containing all
(S2
^(cid:22)j

as
. Let us interpret
1 , the ^(cid:22)js are J independent draws (with replacement) from a hat
possible values of the ^(cid:22)js. The sample variance of those J observations
1 , i.e. an unbiased

) is therefore an unbiased estimator of the variance of ^(cid:22)j, given Z n

(cid:0)

(cid:1)

n
n1

^(cid:22)j

n2

1Here we are not trying to prove the conjecture but to justify our intuition that it is correct.

8

estimator of V ar[^(cid:22)jjZ n
expectation of the sample variance. Indeed, we have

1 ], not V ar[^(cid:22)j]. This permits an alternative derivation of the

EZn

1 ;S:[S2

^(cid:22)j ] = EZn

jZ n
[ES:[S2
1 ]] = EZn
^(cid:22)j
1 ;Sj[^(cid:22)j]  V arZn

[V arSj [^(cid:22)jjZ n
1 ]]
[ESj [^(cid:22)jjZ n

1

1

= V arZ n

n1 ^(cid:22)1] = (cid:27)1  (cid:27)2;
[ n2
where S: denotes the random index sets (S1; : : : ; SJ). Note that E[^(cid:22)jjZ n
n1 ^(cid:22)1 and
1 ] = n2
V ar[ n2

n1 ^(cid:22)1] = (cid:27)2 both come from the discussion after equation (8).

1 ]] = (cid:27)1  V arZn

1

1

(cid:15) For a given j, the sample variance of the L(j; i)s (i 2 Sc
j ) is unbiased for (cid:27)0  (cid:27)3
according to Lemma 1 again. We may average these sample variances over j to obtain
a more accurate estimate of (cid:27)0  (cid:27)3.
(cid:15) From equation (7) we have E[^(cid:22)j ^(cid:22)j0] = (cid:27)2 + ( n1(cid:22))2 for j 6= j
the ^(cid:22)j ^(cid:22)j0 will be unbiased for ((cid:27)2 + ( n1(cid:22))2).

0, so the sample average of

Since we can estimate ((cid:27)1  (cid:27)2), ((cid:27)0  (cid:27)3) and ((cid:27)2 + ( n1(cid:22))2) without bias, we are thus able to
estimate unbiasedly any linear combination of ((cid:27)0  (cid:27)3), ((cid:27)3  (cid:27)2) and ((cid:27)2 + ( n1(cid:22))2). This is
not su(cid:14)cient to estimate V ar[ n2
n1 ^(cid:22)J] shown in (8) unbiasedly. We now tackle the question of
whether or not there exists an unbiased estimator of V ar[ n2
n1 ^(cid:22)J]. Potential estimators may be
put in two classes: (i) those that are linear and/or quadratic in the L(j; i)s, (ii) those that are
not. Because of the general framework of the paper, it is impossible to say anything about the
distribution of the L(j; i)s beyond their means and covariances (to say anything more requires
1 , the learning algorithms and the loss function L ).
assumptions about the distribution of Z n
Hence we are only able to derive mathematical expectations for estimators within class (i).
We obviously cannot identify an unbiased estimator of V ar[ n2
n1 ^(cid:22)J] in class (ii) since we cannot
derive expectations in this class. The following proposition shows that there is no unbiased
estimator of V ar[ n2

n1 ^(cid:22)J] in class (i).

Proposition 3 There is no general unbiased estimator of V ar[ n2
L(j; i)s in a quadratic and/or linear way.
Proof Let ~Lj be the vector of the L(j; i)s involved in ^(cid:22)j and ~L be the vector obtained by
concatenating the ~Ljs; ~L is thus a vector of length n2J. We know that ~L has expectation
n1(cid:22)1n2J and variance

n1 ^(cid:22)J] that involves the

n2J + ((cid:27)3  (cid:27)2)IJ  (1n21
n2) + ((cid:27)0  (cid:27)3)In2J ;
0
0
V ar[~L] = (cid:27)21n2J1

where Ik is the identity matrix of order k, 1k is the k (cid:2) 1 vector (cid:12)lled with 1s and  denotes
n1 ^(cid:22)J] of the following form
Kroneckers product. We consider estimators of V ar[ n2
0~L
A~L + b

^V [ n2

Using the fact that 10

0
n1 ^(cid:22)J] = ~L
n2J A1n2J = trace(A1n2J10

n2J), we have

E[ ^V [ n2

0
n1 ^(cid:22)J]] = trace(AV ar[~L]) + E[~L]

0
AE[~L] + b

E[~L]

9

= ((cid:27)3  (cid:27)2)trace(A(IJ  (1n21
n2))) + ((cid:27)0  (cid:27)3)trace(A)
0
0
0
n2J A1n2J + n1(cid:22)b
+ ((cid:27)2 + n1(cid:22)2)1

1n2J :

Since we wish ^V [ n2
rid of n1(cid:22) in the above expectation. Once those restrictions are incorporated into ^V [ n2
we have

n1 ^(cid:22)J] to be unbiased for V ar[ n2

n1 ^(cid:22)J], we want 0 = b

n2J A1n2J to get
n1 ^(cid:22)J],

01n2J = 10

n2))) + ((cid:27)0  (cid:27)3)trace(A):
n1 ^(cid:22)J]] = ((cid:27)3  (cid:27)2)trace(A(IJ  (1n21
0

E[ ^V [ n2
n1 ^(cid:22)J] is not a linear combination of ((cid:27)3  (cid:27)2) and ((cid:27)0  (cid:27)3) alone, we conclude

Since V ar[ n2
that ^V [ n2

n1 ^(cid:22)J] cannot be unbiased for V ar[ n2

n1 ^(cid:22)J] in general.

3 Estimation of V ar[ n2
n1

^(cid:22)J]

J = V ar[ n2
n1 ^(cid:22)J is as de(cid:12)ned in (5). We
We are interested in estimating n2
n1(cid:27)2
provide two new estimators of V ar[ n2
n1 ^(cid:22)J] that shall be compared, in Section 5, to estimators
currently in use and presented in Section 4. The (cid:12)rst estimator is simple but may have a
positive or negative bias for the actual variance V ar[ n2
n1 ^(cid:22)J]. The second estimator is meant
to lead to conservative inference (see Appendix A.0), that is, if our Conjecture 1 is correct,
its expected value exceeds the actual variance V ar[ n2

n1 ^(cid:22)J] where n2

n1 ^(cid:22)J].

3.1 First Method: Approximating (cid:26)

Let us recall from (5) that n2

n1 ^(cid:22)J = 1

J

j=1 ^(cid:22)j. Let
J

^(cid:22)j =
S2

P

JX

j=1

1
J  1
(cid:18)

(cid:27)1

(cid:26) +

be the sample variance of the ^(cid:22)js. According to Lemma 1,

E[S2

^(cid:22)j ] = (cid:27)1(1  (cid:26)) =

(cid:16)

(cid:17)

1  (cid:26)
(cid:26) + 1(cid:26)

J

(^(cid:22)j  n2
(cid:19)

n1 ^(cid:22)J)2

(cid:16)

1  (cid:26)
J

(cid:27)1

=

(cid:26) + 1(cid:26)
J
J + (cid:26)
1(cid:26)

1

(cid:17)

=

V ar[ n2
n1 ^(cid:22)J]
J + (cid:26)
1(cid:26)

1

;

(9)

(10)

1

J + (cid:26)
1(cid:26)

is an unbiased estimator of V ar[ n2

so that
n1 ^(cid:22)J]. The only problem is that
S2
^(cid:22)j
(cid:26) = (cid:26)(n1; n2) = (cid:27)2(n1;n2)
(cid:27)1(n1;n2), the correlation between the ^(cid:22)js, is unknown and di(cid:14)cult to estimate.
Indeed, neither (cid:27)1 nor (cid:27)2 can be written as a linear combination of n1(cid:22), ((cid:27)2 + ( n1(cid:22))2),
((cid:27)0  (cid:27)3) and ((cid:27)3  (cid:27)2), the only quantities we know how to estimate unbiasedly (besides
linear combinations of these). We use a very naive surrogate for (cid:26) as follows. Let us recall
L(ZSj ; Zi). For the purpose of building our estimator, let us proceed as
that ^(cid:22)j = 1
if L(ZSj ; Zi) depended only on Zi and n1, i.e. the loss does not depend on the actual n1
n2
examples (ZSj ) used for training but only on the number of training examples (n1) and on

P

i2Sc

j

10

the testing example (Zi). Then it is not hard to show that the correlation between the ^(cid:22)js
becomes

. Indeed, when L(ZSj ; Zi) = f(Zi), we have

n2

n1+n2

^(cid:22)1 =

1
n2

I1(i)f(Zi)

and

^(cid:22)2 =

1
n2

I2(k)f(Zk);

nX

k=1

nX

i=1

"

nX

i=1

1
n2
2

nX

i=1

#

n

1

where I1(i) is equal to 1 if Zi is a test example for ^(cid:22)1 and is equal to 0 otherwise. Naturally,
I2(k) is de(cid:12)ned similarly. We obviously have V ar[^(cid:22)1] = V ar[^(cid:22)2] with
V ar[^(cid:22)1] = E[V ar[^(cid:22)1jI1(:)]]+V ar[E[^(cid:22)1jI1(:)]] = E
where I1(:) denotes the n (cid:2) 1 vector made of the I1(i)s. Moreover,

+V ar[E[f(Z1)]] =

V ar[f(Z1)]

(cid:20)

(cid:21)

n2

n2

V ar[f(Z1)]

;

C ov[^(cid:22)1; ^(cid:22)2] = E[C ov[^(cid:22)1; ^(cid:22)2jI1(:); I2(:)]] + C ov[E[^(cid:22)1jI1(:); I2(:)]; E[^(cid:22)2jI1(:); I2(:)]]

= E

I1(i)I2(i)V ar[f(Zi)]

+ C ov[E[f(Z1)]; E[f(Z1)]]

V ar[f(Z1)]

=

n2
2

n2
n2 + 0 =
2

V ar[f(Z1)]

;

n2

(cid:17)

(cid:16)

(cid:16)

Therefore our (cid:12)rst estimator of V ar[ n2

(cid:17)
so that the correlation between ^(cid:22)1 and ^(cid:22)2 (^(cid:22)j and ^(cid:22)j0 with j 6= j
0 in general) is n2
n .
S2
^(cid:22)j

where (cid:26)o = (cid:26)o(n1; n2) =
, that is
n1 ^(cid:22)J]
By construction, (cid:26)o will be a good substitute for (cid:26) when L(ZSj; Z) does not depend much
on the training set ZSj , that is when the decision function of the underlying algorithm does
not change too much when di(cid:11)erent training sets are chosen. Here are instances where we
might suspect this to be true.

. This will tend to overestimate or underestimate V ar[ n2

n1+n2
according to whether (cid:26)o > (cid:26) or (cid:26)o < (cid:26).

J + (cid:26)o
1(cid:26)o

n1 ^(cid:22)J] is

J + n2
n1

S2
^(cid:22)j

1

(cid:15) The capacity (VC dimension) of the algorithm is not too large relative to the size of the

training set (for instance a parametric model that is not too complex).

(cid:15) The algorithm is robust relative to perturbations in the training set. For instance,
one could argue that the support vector machine (Burges, 1998) would tend to fall in
this category. Classi(cid:12)cation and regression trees (Breiman, Friedman, Olshen, & Stone,
1984) however will typically not have this property as a slight modi(cid:12)cation in data
may lead to substantially di(cid:11)erent tree growths so that for two di(cid:11)erent training sets,
the corresponding decision functions (trees) obtained may di(cid:11)er substantially on some
regions. K-nearest neighbors techniques will also lead to substantially di(cid:11)erent decision
functions when di(cid:11)erent training sets are used, especially if K is small.

11

3.2 Second Method: Overestimating V ar[ n2

n1 ^(cid:22)J ]

Our second method aims at overestimating V ar[ n2
n1 ^(cid:22)J]. As explained in Appendix A.0, this
leads to conservative inference, that is tests of hypothesis with actual size less than the nominal
size. This is important because techniques currently in use have the opposite defect, that is
they tend to be liberal (tests with actual size exceeding the nominal size), which is normally
regarded as less desirable than conservative tests.

2

n

0
1

n

(cid:27)2
J

n1(cid:27)2

n1(cid:27)2

1 = b n
0

2

J] = n2
^(cid:27)2
0
n
1

J = V ar[ n2

J = V ar[ n2
(cid:27)2
0
n
1

We have shown at the end of Section 2 that n2

n1 ^(cid:22)J] could not be estimated
unbiasedly without some prior knowledge about (cid:27)0; (cid:27)1; (cid:27)2; (cid:27)3 (we showed after (10) how this
is known). However, as we show below, we may estimate unbiasedly
can be done when (cid:26) = (cid:27)2
(cid:27)1
n2
^(cid:27)2
J be the
0
n
0
1
unbiased estimator, developed below, of the above variance. Since n
1 < n1, we have (according
^(cid:22)J] (cid:21) V ar[ n2
to Conjecture 1) V ar[ n2
J, that
0
n
(cid:21) n2
1
is E[ n2
J.
n1(cid:27)2
0
n
1

c  n2 < n1 (we assume n2 < b n

^(cid:27)2
J will tend to overestimate n2

n1 ^(cid:22)J], so that n2
0
1

c). Let n2

^(cid:22)J] where n

Heres how we may estimate n2
0
n
1

J without bias. The main idea is that we can get two
(cid:27)2
independent instances of n2
J without bias. Of course
(cid:27)2
0
n
1
variance estimation from only two observations is noisy. Fortunately, the process by which
this variance estimate is obtained can be repeated at will, so that we may have many unbiased
estimates of n2
0
n
1

J. Averaging these yields a more accurate estimate of n2
(cid:27)2
0
n
1

^(cid:22)J which allows us to estimate n2
0
n
1

1 into two distinct data sets, D1 and Dc

Obtaining a pair of independent n2
0
n
1

^(cid:22)Js is simple. Suppose, as before, that our data set Z n
1
consists of n = n1 +n2 examples. For simplicity, assume that n is even2. We have to randomly
c each. Let ^(cid:22)(1) be the
split our data Z n
statistic of interest ( n2
^(cid:22)J) computed on D1. This involves, among other things, drawing J
0
n
1
train/test subsets from D1, respectively of size n
(1) be the statistic computed
1 are independent data sets 3,
on Dc
so that (^(cid:22)(1)  ^(cid:22)(1)+^(cid:22)c
(1))2 is unbiased for n2
J. This
(cid:27)2
0
m, with Dm [ Dc
n
1
splitting process may be repeated M times. This yields Dm and Dc
m = Z n
1 ,
Dm \ Dc
(m))
that is such that

(1) are independent since D1 and Dc
2(^(cid:22)(1)  ^(cid:22)c
)2 + (^(cid:22)c
m = ; and jDmj = jDc
"
#

c for m = 1; : : : ; M. Each split yields a pair (^(cid:22)(m); ^(cid:22)c

0
1 and n2. Let ^(cid:22)c

1. Then ^(cid:22)(1) and ^(cid:22)c

1, of size b n

 ^(cid:22)(1)+^(cid:22)c

j = b n

)2 = 1

J.
(cid:27)2

(1)

2

(1)

2

m

2

(1)

2

(^(cid:22)(m)  ^(cid:22)c

(m))2

E

2

=

1
2

V ar[^(cid:22)(m)  ^(cid:22)c

(m)] =

V ar[^(cid:22)(m)] + V ar[^(cid:22)c

(m)]

2

= n2
0
n
1

(cid:27)2
J :

2When n is odd, everything is the same except that splitting the data in two will result in a leftover
1 n(Dm[Dc
m)

m are still disjoint subsets of size b n

c from Zn

1 , but Zn

2

observation that is ignored. Thus Dm and Dc
is a singleton instead of being the empty set.

3Independence holds if the train/test subsets selection process in D1 is independent of the process in Dc
1.

Otherwise, ^(cid:22)1 and ^(cid:22)c

1 may not be independent, but they are uncorrelated, which is all we actually need.

12

This allows us to use the following unbiased estimator of n2
0
n
1

J:
(cid:27)2

MX

m=1

^(cid:27)2
J =

1
2M

n2
0
n
1

(cid:17)

(cid:16)

(^(cid:22)(m)  ^(cid:22)c

(m))2:

(11)

1

Note that, according to Lemma 1, the variance of the proposed estimator is V ar[ n2
^(cid:27)2
J] =
0
n
(m0))2] for m 6= m
4V ar[(^(cid:22)(m) ^(cid:22)c
0. We
1
may deduce from Lemma 1 that r > 0, but simulations yielded r close to 0, so that V ar[ n2
^(cid:27)2
J]
0
n
1
decreased roughly like 1

M . We provide some guidance on the choice of M in Section 5.

with r = Corr[(^(cid:22)(m) ^(cid:22)c

(m))2; (^(cid:22)(m0) ^(cid:22)c

r + 1r

(m))2]

M

Note that the computational e(cid:11)ort to obtain this variance estimator is proportional to
JM. We could reduce this by a factor J if we use n2
n1 ^(cid:22)J], but
0
n
(cid:21) n2
1
we suspect that overestimation might be too gross as E[ n2
J.
n1(cid:27)2
0
n
1
We considered this ultra-conservative estimator of n2
J when we performed the simulations
presented in Section 5 but, as we suspected, the resulting inference was too conservative. We
do not show the results to avoid overcrowding the paper. We could also have gone half way
by using n2
0
n
1

< J, but we did not pursue this for the same reason as above.

1 to overestimate V ar[ n2
^(cid:27)2

^(cid:27)2
J0 with 1 < J

1] = n2
^(cid:27)2
0
n
1

(cid:21) n2
0
n
1

n1(cid:27)2

(cid:27)2
J

(cid:27)2
1

0

4 Inference about n1(cid:22)
We present seven di(cid:11)erent techniques to perform inference (con(cid:12)dence interval or test) about
n1(cid:22). The (cid:12)rst three are methods already in use in the machine-learning community, the
others are methods we put forward. Among these new methods, two were shown in the
previous section; the other two are the \pseudo-bootstrap" and corrected \pseudo-bootstrap"
(described later). Tests 4 of the hypothesis H0 : n1(cid:22) = (cid:22)0 (at signi(cid:12)cance level (cid:11)) have the
following form

reject H0 if

p
j^(cid:22)  (cid:22)0j > c
^(cid:27)2;

while con(cid:12)dence intervals for n1(cid:22) (at con(cid:12)dence level 1  (cid:11)) will look like

p
p
n1(cid:22) 2 [^(cid:22)  c
^(cid:27)2]:
^(cid:27)2; ^(cid:22) + c

(12)

(13)

Note that in (12) or (13), ^(cid:22) will be an average, ^(cid:27)2 is meant to be a variance estimate of ^(cid:22) and
(using the central limit theorem to argue that the distribution of ^(cid:22) is approximately Gaussian)
c will be a percentile from the N(0; 1) distribution or from Students t distribution. The only
di(cid:11)erence between the seven techniques is in the choice of ^(cid:22), ^(cid:27)2 and c. In this section we lay
out what ^(cid:22), ^(cid:27)2 and c are for the seven techniques considered and comment on whether each
technique should be liberal or conservative based on its political ratio. All this is summarized
in Table 1. The properties (size and power of the tests) of those seven techniques shall be

4At this point, we encourage readers to consult Appendix A.0.

13

investigated in Section 5.

We are now ready to introduce the statistics we will consider in this paper.

1. t test statistic

1 be split into a training set ZS1 of size n1 and a test set ZSc

of
Let the available data Z n
size n2 = n  n1, with n2 relatively large (a third or a quarter of n for instance). One
may consider ^(cid:22) = n2
L is the sample variance
L(1; i) 5. Inference would be based on the
of the L(1; i)s involved in n2
(incorrect) belief that

n1 ^(cid:22)1 to estimate n1(cid:22) and ^(cid:27)2 = S2

n1 ^(cid:22)1 = n

where S2

P

i2Sc

1
2

L
n2

1

1

r
n1 ^(cid:22)1  n1(cid:22)

n2

S2
L
n2

(cid:24) N(0; 1):

(14)

We use N(0; 1) here (instead of tn21 for instance) as n2 is meant to be fairly large
(greater than 50, say).
Lemma 1 tells us that the political ratio here is

(cid:20)

(cid:21) = n2(cid:27)3 + ((cid:27)0  (cid:27)3)

(cid:27)0  (cid:27)3

V ar[ n2
n1 ^(cid:22)1]
S2
L
n2

E

> 1;

so this approach leads to liberal inference. This phenomenon grows worse as n2 increases.
L is a biased estimator of (cid:27)0 (the unconditional variance of L(1; i) =
Note that S2
L(ZS1; Zi); i 62 S1), but is unbiased for the variance of L(1; i) conditional on the train-
6. That is so because, given ZS1, the L(1; i)s are independent variates.
ing set ZS1

5We note that this statistic is closely related to the McNemar statistic (Everitt, 1977) when the problem
at hand is the comparison of two classi(cid:12)cation algorithms, i.e. L is of the form (4) with Q of the form (2).
Indeed, let LAB(1; i) = LA(1; i)  LB(1; i) where LA(1; i) indicates whether Zi is misclassi(cid:12)ed (LA(1; i) = 1)
by algorithm A or not (LA(1; i) = 0); LB(1; i) is de(cid:12)ned likewise. Of course, algorithms A and B share the
same training set (S1) and testing set (Sc
, with njk being the number of times
LA(1; i) = j and LB(1; i) = k, j = 0; 1, k = 0; 1. McNemars statistic is devised for testing H0 : n1 (cid:22) = 0 (i.e.
the LAB(1; i)s have expectation 0) so that one may estimate the variance of the LAB(1; i)s with the mean
of the (LAB(1; i)  0)2s (which is n01+n10

n1 ^(cid:22)1 = n10n01

) rather than with S2

1). We have n2

L. Then (12) becomes

n2

n2

(cid:12)(cid:12)(cid:12)(cid:12) n10  n01

p
n10 + n01

(cid:12)(cid:12)(cid:12)(cid:12) > z1(cid:11)=2;

reject H0 if

with zp the quantile p of N(0; 1), which squared leads to the McNemars test (not corrected for continuity).

6From this, we can rederive that S2

E[S2

L] = E[E[S2

L is biased for the unconditional variance as follows:
LjZS1 ]] = E[V ar[L(1; i)jZS1 ]]

(cid:20) E[V ar[L(1; i)jZS1 ]] + V ar[E[L(1; i)jZS1 ]] = V ar[L(1; i)]:

14

Therefore, although (14) is wrong, we do have

r
n1 ^(cid:22)1  E[ n2

n2

n1 ^(cid:22)1jZS1]

S2
L
n2

(cid:25) N(0; 1)

in so far as n2 is large enough for the central limit theorem to apply. Therefore this
n1 ^(cid:22)1jZS1] = E[L(1; i)jZS1] =
method really allows us to make inference about E[ n2
E[L(ZS1; Zi)jZS1]; i 62 S1, that is the generalization error of the speci(cid:12)c rule obtained by
training the algorithm on ZS1, not the generalization error of the algorithm per se. That
is, according to Dietterichs taxonomy (Dietterich, 1998) briey explained in Section 1,
it deals with questions of type 1 through 4 rather than questions of type 5 through 8.

2. Resampled t test statistic

P

Let us refresh some notation from Section 1. Particularly, let us recall that n2

n1 ^(cid:22)J =
S2
^(cid:22)j
J where S2
1
^(cid:22)j
J
is the sample variance of the ^(cid:22)js (see (9)). Inference would be based on the (incorrect)
belief that

j=1 ^(cid:22)j. The resampled t test technique7 considers ^(cid:22) = n2
J

n1 ^(cid:22)J and ^(cid:27)2 =

r
n1 ^(cid:22)J  n1(cid:22)

n2

S2
^(cid:22)j
J

(cid:24) tJ1:

(15)

Combining (8) and Lemma 1 gives us the following political ratio

(cid:20)

(cid:21) =

V ar[ n2
n1 ^(cid:22)J]
S2
^(cid:22)j
J

E

JV ar[ n2
n1 ^(cid:22)J]
]
E[S2
^(cid:22)j

= J(cid:27)2 + ((cid:27)1  (cid:27)2)

(cid:27)1  (cid:27)2

> 1;

so this approach leads to liberal inference, a phenomenon that grows worse as J increases.
Dietterich (Dietterich, 1998) observed this empirically through simulations.
As argued in Section 2, S2
^(cid:22)j
conditional on Z n

=J actually estimates (without bias) the variance of n2

n1 ^(cid:22)J

1 . Thus while (15) is wrong, we do have
(cid:25) tJ1:

r
n1 ^(cid:22)J  E[ n2

n1 ^(cid:22)JjZ n
1 ]

n2

S2
^(cid:22)j
J

Recall from the discussion following (8) that E[ n2
method really allows us to make inference about n2
we want to make inference about n1(cid:22).

n1 ^(cid:22)JjZ n
n1 ^(cid:22)1. Therefore this
n1 ^(cid:22)1, which is not too useful because

1 ] = n2

7When the problem at hand is the comparison of two classi(cid:12)cation algorithms, i.e. L is of the form (4)
with Q of the form (2), this approach is what Dietterich (Dietterich, 1998) calls the \resampled paired t test"
statistic.

15

3. 5x2 cv t test

Dietterich (Dietterich, 1998)8 split Z n
as in Section 3 and let
~(cid:22)(m) = bn=2c1

X

1 in half M = 5 times to yield D1; Dc

1; : : : ; D5; Dc
5

X

i2Dm

L(Dc

m; Zi):

He then used ^(cid:22) = ~(cid:22)(1), ^(cid:27)2 = ^(cid:27)2
the political ratio is

(m))2 and c = t5;1(cid:11)=2. Note that

i2Dc

m

(m) = bn=2c1
~(cid:22)c

L(Dm; Zi);
P
m=1(~(cid:22)(m)  ~(cid:22)c
= (cid:27)1(bn=2c;bn=2c)
(cid:27)1(bn=2c;bn=2c)  (cid:27)4

Diet = 1
10

5

E[^(cid:27)2]

V ar[~(cid:22)(1)]

where (cid:27)4 = C ov[~(cid:22)(m); ~(cid:22)c
Remarks

(m)].

distant from n(cid:22).

(cid:15) As Dietterich noted, this allows inference for
(cid:15) The choice of M = 5 seems arbitrary.
(cid:15) The statistic was developed under the assumption that the ~(cid:22)(m)s and ~(cid:22)c
(m)s are 10
independent and identically distributed Gaussian variates. Even in this ideal case,

bn=2c(cid:22) which may be substantially

tD =

^(cid:22)  bn=2c(cid:22)

p
^(cid:27)2

=

q

P
~(cid:22)(1)  bn=2c(cid:22)
m=1(~(cid:22)(m)  ~(cid:22)c

5

1
10

(m))2

is not distributed as t5 as assumed in (Dietterich, 1998) because ~(cid:22)(1) and (~(cid:22)(1)~(cid:22)c
are not independent. That is easily (cid:12)xed in two di(cid:11)erent ways:

(1))

{ Take the sum from m = 2 to m = 5 and replace 10 by 8 in the denominator of
 bn=2c(cid:22)) which would lead to tD (cid:24) t5

(16) which would result in tD (cid:24) t4,
p
2( ~(cid:22)(1)+~(cid:22)c
{ Replace the numerator by
(1) and ~(cid:22)(1)  ~(cid:22)c

(1) are independent.

as ~(cid:22)(1) + ~(cid:22)c

(1)

2

In all cases, more degrees of freedom could be exploited; statistics distributed as t8
can be devised by appropriate use of the 10 (assumed) independent variates.

4. Conservative Z

We estimate n1(cid:22) by ^(cid:22) = n2
variance estimate. Since n2
expect that its distribution is approximatively normal. We then proceed as if

^(cid:27)2
J (equation 11) as its conservative
n1 ^(cid:22)J is the mean of many (Jn2 to be exact) L(j; i)s, we may

n1 ^(cid:22)J and use ^(cid:27)2 = n2
0
n
1

(16)

(17)

q
n1 ^(cid:22)J  n1(cid:22)

n2

n2
0
n
1

^(cid:27)2
J

Z =

8Dietterich only considered the comparison of two classi(cid:12)cation algorithms, that is L of the form (4) with

Q of the form (2).

16

J is designed to overestimate V ar[ n2
^(cid:27)2

was a N(0; 1) variate (even though n2
n1 ^(cid:22)J]) to
0
n
1
perform inference, leading us to use c = z1(cid:11)=2 in (12) or (13), where z1(cid:11)=2 is the
percentile 1 (cid:11) of the N(0; 1) distribution. Some would perhaps prefer to use percentile
from the t distribution, but it is unclear what the degrees of freedom ought to be. People
like to use the t distribution in approximate inference frameworks, such as the one we
are dealing with, to yield conservative inference. This is unnecessary here as inference
is already conservative via the variance overestimation. Indeed, the political ratio

n1 ^(cid:22)J]
V ar[ n2
E[ n2
^(cid:27)2
J]
0
n
1

=

n2
n1(cid:27)2
J
n2
(cid:27)2
0
J
n
1

is smaller than 1 if we believe in Conjecture 1.
Regarding the choice of n2 (and thus n1), we may take it to be small relatively to n (the
total number of examples available). One may use n2 = n
10 for instance provided J is
not smallish.

5. Pseudo-Bootstrap

n1 ^(cid:22)J by a procedure similar to the bootstrap (Efron
To estimate the variance of ^(cid:22) = n2
& Tibshirani, 1993), we obtain R other instances of that random variable, by redoing
the computation with di(cid:11)erent splits on the same data Z n
1 ; call these (cid:20)(cid:22)1; : : : ; (cid:20)(cid:22)R. Thus,
in total, (R + 1)J training and testing sets are needed here. Then one could consider
^(cid:27)2 = (cid:20)(cid:27)2, where (cid:20)(cid:27)2 is the sample variance of (cid:20)(cid:22)1; : : : ; (cid:20)(cid:22)R, and take c = tR1;1(cid:11)=2, as
(cid:20)(cid:27)2 has R  1 degrees of freedom. Of course n2
n1 ^(cid:22)J, (cid:20)(cid:22)1; : : : ; (cid:20)(cid:22)R are R + 1 identically
distributed random variables. But they are not independent as we (cid:12)nd, from (7), that
the covariance between them is (cid:27)2. Using Lemma 1, we have
= J(cid:27)2 + ((cid:27)1  (cid:27)2)

n1 ^(cid:22)J]

> 1:

=

V ar[ n2
E[(cid:20)(cid:27)2]

n2
n1(cid:27)2
J
n2
n1(cid:27)2
J

 (cid:27)2

(cid:27)1  (cid:27)2

Note that this political ratio is the same as its counterpart for the resampled t-test
S2
^(cid:22)j
because E[(cid:20)(cid:27)2] = E[
J ]. So the pseudo-bootstrap leads to liberal inference that should
worsen with increasing J just like the resampled t test statistic.
In other words, the
pseudo-bootstrap only provides a second estimator of (cid:27)1(cid:27)2
J which is more complicated
and harder to compute than

J which is also unbiased for (cid:27)1(cid:27)2

S2
^(cid:22)j

.

J

(cid:16)

1

(cid:17)

6. Corrected resampled t-test statistic

S2
^(cid:22)j

, where S2
^(cid:22)j

J is
From our discussion in Section 3, we know that an unbiased estimator of
J + (cid:26)
is the sample variance of the ^(cid:22)js. Unfortunately (cid:26), the
1(cid:26)
correlation between the ^(cid:22)js, is unknown. The resampled t-test boldly puts (cid:26) = 0. We
propose here to proceed as if (cid:26) = (cid:26)0 = n2
as our argument in Section 3 suggests.
So we use ^(cid:27)2 =
. We must say again that this approximation is gross, but
we feel it is better than putting (cid:26) = 0. Furthermore, in the ideal case where the vector

J + n2
n1

n2
n1(cid:27)2

S2
^(cid:22)j

(cid:16)

(cid:17)

n1+n2

1

17

of the ^(cid:22)js follows the multivariate Gaussian distribution and (cid:26) is actually equal to (cid:26)0,
Lemma 2 states that
Finally, let us note that the political ratio

(cid:24) tJ1. This is why we use c = tJ1;1(cid:11)=2.

n1 ^(cid:22)J n1 (cid:22)

p

^(cid:27)2

n2

n1 ^(cid:22)J]

V ar[ n2
E[^(cid:27)2]

=

1

J + (cid:26)
1(cid:26)
J + n2
n1

1

will be greater than 1 (liberal inference) if (cid:26) > (cid:26)0. If (cid:26) < (cid:26)0, the above ratio is smaller
than 1, so that we must expect the inference to be conservative. Having mentioned
earlier that conservative inference is preferable to liberal inference, we therefore hope
that the ad hoc (cid:26)0 = n2

will tend to be larger than the actual correlation (cid:26).

n1+n2

(cid:16)

(cid:17)

7. Corrected pseudo-bootstrap statistic

(cid:17)

(cid:16)

Naturally, the correction we made in the resampled t test can be applied to the pseudo-
(cid:20)(cid:27)2, where (cid:20)(cid:27)2 is the
bootstrap procedure as well. Namely, we note that
sample variance of the (cid:20)(cid:22)rs, is unbiased for n2
J. Naively replacing (cid:26) by (cid:26)0 leads us to
(cid:20)(cid:27)2. Furthermore, in the ideal case where (cid:26) is actually equal to (cid:26)0,
use ^(cid:27)2 =
and the vector made of n2
n1 ^(cid:22)J, (cid:20)(cid:22)1; : : : (cid:20)(cid:22)R follows the multivariate Gaussian distribution,
n1 ^(cid:22)J n1 (cid:22)
(cid:24) tR1. This is why we use c = tR1;1(cid:11)=2. Finally
p
Lemma 2 states that
note that, just like in the corrected resampled t-test, the political ratio is

1 + J (cid:26)
1(cid:26)

1 + Jn2
n1

n1(cid:27)2

n2

^(cid:27)2

n1 ^(cid:22)J]

V ar[ n2
E[^(cid:27)2]

=

1

J + (cid:26)
1(cid:26)
J + n2
n1

1

:

We conclude this section by providing in Table 1 a summary of the seven inference methods

considered in the present section.

5 Simulation study

We performed a simulation study to investigate the power and the size of the seven statistics
considered in the previous section. We also want to make recommendations on the value
n1 ^(cid:22)J. Simulation results will also lead to a
of J to use for those methods that involve
n2
recommendation on the choice of M when the conservative Z is used.

We will soon introduce the three kinds of problems we considered to cover a good range
of possible applications. For a given problem, we shall generate 1000 independent sets of data
1 = fZ1; : : : Zng has been generated, we may
of the form fZ1; : : : ; Zng. Once a data set Z n
compute con(cid:12)dence intervals and/or a tests of hypothesis based on the statistics laid out in
Section 4 and summarized in Table 1. A di(cid:14)culty arises however. For a given n, those seven
methods dont aim at inference for the same generalization error. For instance, Dietterichs
method aims at n=2(cid:22) (we take n even for simplicity), while the others aim at n1(cid:22) where n1
would usually be di(cid:11)erent for di(cid:11)erent methods (e.g. n1 = 2n
10 for

3 for the t-test and n1 = 9n

18

Name
1. t-test (McNemar)
2. resampled t
3. Dietterichs 5 (cid:2) 2 cv
4. conservative Z

5. pseudo-bootstrap
6. corrected resampled t

7. corrected pseudo-bootstrap

n=2

^(cid:22)
n1 ^(cid:22)1
n2
n1 ^(cid:22)J
n2
n=2^(cid:22)1
n1 ^(cid:22)J
n2
n1 ^(cid:22)J
n2
n1 ^(cid:22)J
n2
n1 ^(cid:22)J
n2

(cid:16)
(cid:16)

^(cid:27)2

1
S2
L
n2
1
J S2
^(cid:22)j
^(cid:27)2
Diet
n2
^(cid:27)2
0
J
n
1
(cid:20)(cid:27)2
J + n2
n1
1 + Jn2
n1

1

(cid:17)
(cid:17)

S2
^(cid:22)j

(cid:20)(cid:27)2

tR1;1(cid:11)=2

c

z1(cid:11)=2

tJ1;1(cid:11)=2
t5;1(cid:11)=2
z1(cid:11)=2

tR1;1(cid:11)=2
tJ1;1(cid:11)=2

V ar[^(cid:22)]
E[^(cid:27)2]
n2(cid:27)3+((cid:27)0(cid:27)3)
> 1
(cid:27)0(cid:27)3
1 + J (cid:26)
1(cid:26) > 1
(cid:27)1
(cid:27)1(cid:27)4
n2
n1 (cid:27)2
J
n2
0
(cid:27)2
J
n
1
1 + J (cid:26)
1(cid:26) > 1
1+J (cid:26)
1(cid:26)
1+J n2
n1
1+J (cid:26)
1(cid:26)
1+J n2
n1

< 1

Table 1: Summary description of the seven inference methods considered in relation to the
rejection criteria shown in (12) or the con(cid:12)dence interval shown in (13). zp and tk;p refer to
the quantile p of the N(0; 1) and Student tk distributions respectively. The political ratio,
that is V ar[^(cid:22)]
E[^(cid:27)2] , indicates if inference according to the corresponding method will tend to be
conservative (ratio less than 1) or liberal (ratio greater than 1). See Section 4 for further
details.

n1 ^(cid:22)J). In order to compare the di(cid:11)erent techniques, for a given n, we shall
methods using n2
always aim at n=2(cid:22). The use of the statistics other than Dietterichs 5(cid:2)2 cv shall be modi(cid:12)ed
as follows.

(cid:15) t test statistic

We take n1 = n2 = n
n2 is one third, say, of n, not one half.

2 . This deviates slightly from the normal usage of the t test where

(cid:15) Methods other that the t-test and Dietterichs 5 (cid:2) 2 cv
n1 ^(cid:22)J where J is a free parameter, that is all methods except
For methods involving n2
the t-test and Dietterichs 5 (cid:2) 2 cv, we take n1 = n2 = n
2 . This deviates substantially
from the normal usage where n1 would be 5 to 10 times larger than n2, say. For that
reason, we also take n1 = n
10 (assume n is a multiple of 10 for simplicity).
This is achieved by throwing away 40% of the data. Note that when we will address the
question of the choice of J (and M for the conservative Z), we shall use n1 = 9n
10 and
n2 = n

10, more in line with the normal usage.

2 and n2 = n

(cid:15) Conservative Z

For the conservative Z, we need to explain how we compute the variance estimate.
Indeed, formula (11) suggests that we have to compute n2
J whenever n1 = n2 = n
2 !
What we do is that we choose n2 as we would normally do (10% of n here) and do the
J = n=10
variance calculation as usual ( n2
^(cid:27)2
J). However, as explained above, we use
n=2
n=2 ^(cid:22)J and n2
n=2 ^(cid:22)J instead of n2
nn2 ^(cid:22)J as the cross-validation estimators. Recall
J was decreasing in n1 and n2. Consequently
that we have argued in Section 2 that n2
n1(cid:27)2

n=2 ^(cid:22)J = n=10

2n=5^(cid:27)2

n=2n2

0 ^(cid:27)2

19

the variances of n=2
acts as a conservative variance estimate, that is

n=2 ^(cid:22)J and n2

n=2^(cid:22)J are smaller than n2

n=2n2

J, so that n2
(cid:27)2

n=2n2

^(cid:27)2
J still

E[ n2

n=2n2

J] = n2
^(cid:27)2

n=2n2

J = V ar[ n2
(cid:27)2

n=2n2

^(cid:22)J] (cid:21) V ar[ n2

n=2 ^(cid:22)J] (cid:21) V ar[ n=2

n=2 ^(cid:22)J]:

Thus the variance overestimation will be more severe in the case of n=2

n=2 ^(cid:22)J.

We consider three kinds of problems to cover a good range of possible applications:

1. Prediction in simple normal linear regression

We consider the problem of estimating the generalization error in a simple Gaussian
X) and Y jX (cid:24) N(a +
regression problem. We thus have Z = (X; Y ) with X (cid:24) N((cid:22)X; (cid:27)2
Y jX is constant (does not depend on X). The learning algorithms are
bX; (cid:27)2

Y jX) where (cid:27)2
(A) Sample mean

P

i2S Yi = (cid:22)YS , that is the mean of the
The decision function is FA(ZS)(X) = 1
n1
Y s in the training set ZS. Note that this decision function does not depend on X.
We use a quadratic loss, so that LA(j; i) = (FA(ZSj )(Xi)  Yi)2 = ( (cid:22)YSj

 Yi)2.

(B) Linear regression

The decision function is FB(ZS)(X) = ^aS +^bSX where ^aS and ^bS are the intercept
and the slope of the ordinary least squares regression of Y on X performed on
the training set ZS. Since we use a quadratic loss, we therefore have LB(j; i) =
(FB(ZSj )(Xi)  Yi)2 = (^aSj + ^bSj Xi  Yi)2.

On top of inference about the generalization errors of algorithm A ( n1(cid:22)A) and algorithm
B ( n1(cid:22)B), we also consider inference about n1(cid:22)AB = n1(cid:22)A  n1(cid:22)B, the di(cid:11)erence
of those generalization errors. This inference is achieved by considering LAB(j; i) =
LA(j; i)  LB(j; i).
Table 2 describes the four simulations we performed for the regression problem. For
instance, in Simulation 1, we generated 1000 samples of size 200, with (cid:22)x = 10, (cid:27)2
X = 1,
It is shown in (Nadeau & Bengio, 1999) that
a = 100, b = 1 and
n1(cid:22)A = n1+1
Y jX. Thus the (cid:12)rst and third
n1
simulation correspond to cases where the two algorithms generalize equally well (for
2 ); in the second and fourth case, the linear regression generalizes better than
n1 = n
the sample mean 9. The table also provides some summary con(cid:12)dence
intervals for
quantities of interest, namely n1(cid:22), (cid:26)(n1; n2) = (cid:27)2(n1;n2)

Y jX = 97.
(cid:27)2
Y jX + b2(cid:27)2
X) and n1(cid:22)B = n1+1
n1

n12
n13 (cid:27)2

(cid:27)1(n1;n2) and r.

((cid:27)2

2. Classi(cid:12)cation of two Gaussian populations

We consider the problem of estimating the generalization error in a classi(cid:12)cation problem

9The parameters of the simulations displayed in Tables 2, 3 and 4 were more or less chosen arbitrarily.
However, e(cid:11)orts were made so that one or two simulations for each problem would correspond to n1 (cid:22)AB = 0
(i.e. n1 (cid:22)A = n1 (cid:22)B).

20

n
(cid:22)X
a
b
(cid:27)2
X
(cid:27)2
Y jX
n=2(cid:22)A (cid:3)
n=2(cid:22)B
n=2(cid:22)AB (cid:3)
9n=10(cid:22)A (cid:3)
9n=10(cid:22)B
9n=10(cid:22)AB (cid:3)
2 )
(cid:26)A( n
2 ; n
2 )
(cid:26)B( n
2 ; n
(cid:26)AB( n
2 )
2 ; n
(cid:26)A( n
10)
2 ; n
10)
(cid:26)B( n
2 ; n
(cid:26)AB( n
10)
2 ; n
(cid:26)A( 9n
10)
10 ; n
10)
(cid:26)B( 9n
10 ; n
(cid:26)AB( 9n
10)
10 ; n
rA
rB
rAB

Simulation 1

Simulation 2

Simulation 3

Simulation 4

200
10
100
1
1
97

200
10
100
2
2
64

2000
10
100
0.1
1

9.97

2000
10
100
0.1
5
9

[98.77,100.03]
[98.69,99.96]
[-0.03,0.19]
[98.19, 99.64]
[97.71, 99.16]
[0.36,0.60 ]
[0.466,0.512]
[0.467,0.514]
[0.225,0.298]
[0.148,0.179]
[0.152,0.183]
[0.103,0.143]
[0.090,0.115]
[0.092,0.117]
[0.062,0.091]
[0.021,0.034]
[0.022,0.034]
[0.154,0.203]

[72.30,73.25]
[64.89,65.73]
[7.25,7.68]

[71.92, 72.99]
[64.30,65.24]
[7.45,7.93 ]
[0.487,0.531]
[0.473,0.517]
[0.426,0.482]
[0.165,0.193]
[0.156,0.183]
[0.146,0.184]
[0.094,0.117]
[0.089,0.111]
[0.084,0.109]
[0.027,0.040]
[0.028,0.043]
[0.071,0.095]

[9.961,10.002]
[9.961,10.002]
[-0.001,0.001]
[9.952,9.998]
[9.948,9.993]
[0.003,0.006]
[0.484,0.531]
[0.483,0.530]
[0.226,0.282]
[0.162,0.194]
[0.162,0.194]
[0.089,0.128]
[0.090,0.111]
[0.090,0.111]
[0.059,0.085]
[-0.003,0.008]
[-0.003,0.008]
[0.163,0.202]

[9.040,9.075]
[8.999,9.034]
[0.039,0.043]
[9.026,9.067 ]
[8.982,9.023 ]
[0.042,0.047]
[0.471,0.515]
[0.472,0.517]
[0.399,0.455]
[0.147,0.176]
[0.147,0.175]
[0.131,0.165]
[0.088,0.108]
[0.088,0.108]
[0.086,0.109]
[-0.001,0.008]
[-0.001,0.009]
[0.087,0.114]

Y jX as shown in the table. 95% con(cid:12)dence intervals for n1(cid:22), (cid:26)(n1; n2) = (cid:27)2(n1;n2)
(cid:27)1(n1)

Table 2: Description of four simulations for the simple linear regression problem. In each of
the four simulations, 1000 independent samples of size n where generated with (cid:22)X, a, b, (cid:27)2
X
and (cid:27)2
and
r = Corr[(^(cid:22)(m)  ^(cid:22)c
(m0))2] de(cid:12)ned after (11) are provided. The subscripts A,
B and AB indicates whether we are working with LA, LB or LAB. An asterisk besides (cid:22)
indicates that powers of tests for that (cid:22) are displayed in a (cid:12)gure.

(m))2; (^(cid:22)(m0)  ^(cid:22)c

21

with two classes. We thus have Z = (X; Y ) with P rob(Y = 1) = P rob(Y = 0) = 1
2,
XjY = 0 (cid:24) N((cid:22)0; (cid:6)0) and XjY = 1 (cid:24) N((cid:22)1; (cid:6)1). The learning algorithms are
(A) Regression tree

We train a least square regression tree 10 (Breiman et al., 1984) of Y against X
and the decision function is FA(ZS)(X) = I[NZS (X) > 0:5] where NZS(X) is the
leaf value corresponding to X of the tree obtained when training on ZS. Thus
LA(j; i) = I[FA(ZSj )(Xi) 6= Yi] is equal to 1 whenever this algorithm misclassi(cid:12)es
example i when the training set is ZSj ; otherwise it is 0.

(B) Ordinary least squares linear regression

X > 1

We perform the regression of Y against X and the decision function is FB(ZS)(X) =
0
2] where ^(cid:12)S is the ordinary least squares regression coe(cid:14)cient estimates11
I[ ^(cid:12)
obtained by training on the set ZS. Thus LB(j; i) = I[FB(ZSj )(Xi) 6= Yi] is equal
ZS
to 1 whenever this algorithm misclassi(cid:12)es example i when the training set is ZSj;
otherwise it is 0.

On top of inference about the generalization errors n1(cid:22)A and n1(cid:22)B associated with
those two algorithms, we also consider inference about n1(cid:22)AB = n1(cid:22)A  n1(cid:22)B =
E[LAB(j; i)] where LAB(j; i) = LA(j; i)  LB(j; i).
Table 3 describes the four simulations we performed for the Gaussian populations clas-
si(cid:12)cation problem. Again, we considered two simulations with n = 200 and two sim-
ulations with n = 2000. We also chose the parameters (cid:22)0, (cid:22)1, (cid:6)0 and (cid:6)1 in such a
way that in Simulations 2 and 4, the two algorithms generalize equally well; in Simula-
tions 1 and 3, the linear regression generalizes better than the regression tree. The table
also provides some summary con(cid:12)dence intervals for quantities of interest, namely n1(cid:22),
(cid:26)(n1; n2) = (cid:27)2(n1;n2)

(cid:27)1(n1;n2) and r.
3. Classi(cid:12)cation of letters

We consider the problem of estimating generalization errors in the Letter Recognition
classi(cid:12)cation problem (Blake, Keogh, & Merz, 1998). The learning algorithms are

(A) Classi(cid:12)cation tree

We train a classi(cid:12)cation tree (Breiman et al., 1984) 12 to obtain its decision function
FA(ZS)(X). Here the classi(cid:12)cation loss function LA(j; i) = I[FA(ZSj )(Xi) 6= Yi] is
equal to 1 whenever this algorithm misclassi(cid:12)es example i when the training set is
ZSj ; otherwise it is 0.

10The function tree in Splus 4.5 for Windows with default options and no pruning was used to train the

regression tree.

11 ^(cid:12)ZS includes an intercept and correspondingly 1 was included in the input vector X.
12We used the function tree in Splus version 4.5 for Windows. The default arguments were used and no
pruning was performed. The function predict with option type=\class" was used to retrieve the decision
function of the tree.

22

Simulation 1

Simulation 2

Simulation 3

Simulation 4

n
(cid:22)0
(cid:22)1
(cid:6)0
(cid:6)1
n=2(cid:22)A (cid:3)
n=2(cid:22)B
n=2(cid:22)AB (cid:3)
9n=10(cid:22)A (cid:3)
9n=10(cid:22)B
9n=10(cid:22)AB (cid:3)
(cid:26)A( n
2 )
2 ; n
2 )
(cid:26)B( n
2 ; n
(cid:26)AB( n
2 )
2 ; n
(cid:26)A( n
10)
2 ; n
10)
(cid:26)B( n
2 ; n
(cid:26)AB( n
10)
2 ; n
(cid:26)A( 9n
10)
10 ; n
(cid:26)B( 9n
10)
10 ; n
(cid:26)AB( 9n
10)
10 ; n
rA
rB
rAB

200
(0,0)
(1,1)
I2
1
2 I2

[0.249,0.253]
[0.204,0.208]
[0.044,0.046]
[0.247,0.252]
[0.201,0.205]
[0.044,0.049]
[0.345,0.392]
[0.418,0.469]
[0.128,0.154]
[0.189,0.223]
[0.150,0.182]
[0.100,0.124]
[0.137,0.166]
[0.089,0.112]
[0.077,0.096]
[0.007,0.018]
[0.006,0.017]
[0.010,0.021]

200
(0,0)
(1,1)
I2
1
6 I2

[0.146,0.149]
[0.146,0.148]
[-0.001,0.002]
[0.142,0.147]
[0.142,0.145]
[-0.001,0.003]
[0.392,0.438]
[0.369,0.417]
[0.174,0.205]
[0.224,0.260]
[0.135,0.163]
[0.130,0.157]
[0.156,0.187]
[0.077,0.097]
[0.090,0.111]
[0.025,0.039]
[0.023,0.037]
[0.007,0.017]

2000
(0,0)
(1,1)
I2
1
2 I2

[0.247,0.248]
[0.200,0.201]
[0.0467,0.0475]
[0.235,0.237]
[0.199,0.200]
[0.036,0.037]
[0.354,0.400]
[0.462,0.508]
[0.120,0.146]
[0.190,0.225]
[0.141,0.170]
[0.087,0.106]
[0.113,0.137]
[0.080,0.102]
[0.049,0.065]
[-0.005,0.003]
[-0.003,0.007]
[-0.003,0.006]

2000
(0,0)
(1,1)
I2

0:173I2

[0.142,0.143]
[0.142,0.143]

[1 (cid:2) 104; 8 (cid:2) 104]

[0.132,0.133]
[0.142,0.143]
[-0.011,-0.009]
[0.380,0.423]
[0.388,0.432]
[0.179,0.211]
[0.207,0.242]
[0.129,0.156]
[0.112,0.138]
[0.126,0.153]
[0.081,0.100]
[0.078,0.100]
[-0.003,0.006]
[-0.003,0.006]
[-0.001,0.009]

Table 3: Description of four simulations for the classi(cid:12)cation of two Gaussian populations.
In each of the four simulations, 1000 independent samples of size n where generated with (cid:22)0,
(cid:22)1, (cid:6)0, (cid:6)1 as shown in the table. 95% con(cid:12)dence intervals for n1(cid:22), (cid:26)(n1; n2) = (cid:27)2(n1;n2)
(cid:27)1(n1) and
r = Corr[(^(cid:22)(m)  ^(cid:22)c
(m0))2] de(cid:12)ned after (11) are provided. The subscripts A,
B and AB indicates whether we are working with LA, LB or LAB. An asterisk besides (cid:22)
indicates that powers of tests for that (cid:22) are displayed in a (cid:12)gure.

(m))2; (^(cid:22)(m0)  ^(cid:22)c

23

(B) First nearest neighbor

We apply the (cid:12)rst nearest neighbor rule with a distorted distance metric to pull
down the performance of this algorithm to the level of the classi(cid:12)cation tree (as
in (Dietterich, 1998)). Speci(cid:12)cally, the distance between two vectors of inputs X(1)
and X(2) is

d(X(1); X(2)) =

w2k

(X(1)

i

 X(2)

i

)2

3X

k=1

X

i2Ck

where C1 = f1; 3; 9; 16g, C2 = f2; 4; 6; 7; 8; 10; 12; 14; 15g and C3 = f5; 11; 13g de-
1 respectively. Table 4
note the sets of components that are weighted by w, 1 and w
shows the values of w considered. We have LB(j; i) equal to 1 whenever this algo-
rithm misclassi(cid:12)es example i when the training set is ZSj ; otherwise it is 0.

In addition to inference about the generalization errors n1(cid:22)A and n1(cid:22)B associated with
those two algorithms, we also consider inference about n1(cid:22)AB = n1(cid:22)A  n1(cid:22)B =
E[LAB(j; i)] where LAB(j; i) = LA(j; i)  LB(j; i). We sample, without replacement,
300 examples from the 20000 examples available in the Letter Recognition data base.
Repeating this 1000 times, we obtain 1000 sets of data of the form fZ1; : : : ; Z300g. The
table also provides some summary con(cid:12)dence intervals for quantities of interest, namely
n1(cid:22), (cid:26)(n1; n2) = (cid:27)2(n1;n2)

(cid:27)1(n1;n2) and r.

P

n1 ^(cid:22)25 = 1

25

1 , while the con(cid:12)dence intervals in the tables use 1000 data sets Z n

Before we comment on Tables 2, 3 and 4, let us describe how con(cid:12)dence intervals shown in
those tables were obtained. First, let us point out that con(cid:12)dence intervals for generalization
errors in those tables have nothing to do with the con(cid:12)dence intervals that we may compute
from the statistics shown in Section 4. Indeed, the latter can be computed on a single data
1 as we now explain.
set Z n
n1 ^(cid:22)25, which has expectation n1(cid:22). Recall, from (5) in
For a given data set, we may compute n2
j=1 ^(cid:22)j is the average of 25 crude estimates of the generalization
Section 1, that n2
25
error. Also recall from Section 2 that those crude estimates have the moment structure
(cid:27)1(n1;n2). Call ~(cid:22) = (^(cid:22)1; : : : ; ^(cid:22)25)0 the
displayed in Lemma 1 with (cid:12) = n1(cid:22) and (cid:25) = (cid:26)(n1; n2) = (cid:27)2(n1;n2)
vector of those crude estimates. Since we generate 1000 independent data sets, we have 1000
independent instances of such vectors. As may be seen in the Appendix A.3, appropriate use of
the theory of estimating functions (White, 1982) then yields approximate con(cid:12)dence intervals
for n1(cid:22) and (cid:26)(n1; n2). Con(cid:12)dence intervals for r = Corr[(^(cid:22)(m)  ^(cid:22)c
(m0))2],
de(cid:12)ned in Section 3, are obtained in the same manner we get con(cid:12)dence intervals for (cid:26)(n1; n2).
Namely, we have 1000 independent instances of the vector ((^(cid:22)(1)  ^(cid:22)c
(20))2)0
2n=5 ^(cid:22)15s as we advocate the use of J = 15 later in this section.
where the ^(cid:22)(m)s and ^(cid:22)c
We see that n1(cid:22) may substantially di(cid:11)er for di(cid:11)erent n1. This is most evident in Table 4
where con(cid:12)dence intervals for 150(cid:22) di(cid:11)er from con(cid:12)dence intervals for 270(cid:22) in a noticeable
manner. We see that our very naive approximation (cid:26)0(n1; n2) = n2
is not as bad as one
could expect. Often the con(cid:12)dence intervals for the actual (cid:26)(n1; n2) contains (cid:26)0(n1; n2) 13.

(m))2; (^(cid:22)(m0)  ^(cid:22)c
(1))2; : : : ; (^(cid:22)(20)  ^(cid:22)c

(m) are n=10

n1+n2

13As mentioned before, the corrected pseudo-bootstrap and the corrected resampled t-test are typically used

24

n
w

n=2(cid:22)B (cid:3)
n=2(cid:22)AB (cid:3)
9n=10(cid:22)B
9n=10(cid:22)AB (cid:3)
2 )
(cid:26)B( n
2 ; n
(cid:26)AB( n
2 )
2 ; n
10)
(cid:26)B( n
2 ; n
(cid:26)AB( n
10)
2 ; n
10)
(cid:26)B( 9n
10 ; n
(cid:26)AB( 9n
10 )
10 ; n
rB
rAB

n
w

n=2(cid:22)B (cid:3)
n=2(cid:22)AB (cid:3)
9n=10(cid:22)B
9n=10(cid:22)AB (cid:3)
(cid:26)B( n
2 )
2 ; n
(cid:26)AB( n
2 )
2 ; n
10)
(cid:26)B( n
2 ; n
(cid:26)AB( n
10)
2 ; n
(cid:26)B( 9n
10)
10 ; n
(cid:26)AB( 9n
10 )
10 ; n
rB
rAB

Simulation 1

Simulation 2

Simulation 3

300
1

[0.539,0.542]
[0.150,0.152]
[0.434,0.439]
[0.148,0.153]
[0.310,0.355]
[0.134,0.160]
[0.167,0.198]
[0.122,0.148]
[0.105,0.129]
[0.085,0.105]
[-0.006,0.001]
[-0.004,0.004]
Simulation 4

300
17.25

[0.666,0.669]
[0.023,0.026]
[0.586,0.591]
[-0.003,0.001]
[0.360,0.404]
[0.167,0.198]
[0.200,0.238]
[0.130,0.156]
[0.118,0.143]
[0.085,0.106]
[-0.004,0.004]
[-0.002,0.007]

300
5

[0.593,0.596]
[0.096,0.099]
[0.496,0.501]
[0.086,0.091]
[0.334,0.376]
[0.152,0.180]
[0.182,0.214]
[0.129,0.155]
[0.106,0.131]
[0.085,0.105]
[-0.004,0.004]
[-0.004,0.004]
Simulation 5

300
25

[0.690,0.693]
[-0.001,0.002]
[0.616,0.620]
[-0.033,-0.028]
[0.368,0.413]
[0.170,0.202]
[0.201,0.238]
[0.129,0.155]
[0.125,0.151]
[0.087,0.108]
[-0.005,0.004]
[-0.001,0.009]

300
10

[0.632,0.635]
[0.057,0.060]
[0.544,0.548]
[0.039,0.044]
[0.349,0.393]
[0.160,0.191]
[0.197,0.232]
[0.130,0.156]
[0.115,0.140]
[0.084,0.104]
[-0.004,0.005]
[-0.003,0.005]
Simulation 6

300
2048

[0.779,0.782]
[-0.089,-0.087]
[0.730,0.734]
[-0.147,-0.142]
[0.347,0.392]
[0.178,0.211]
[0.201,0.237]
[0.133,0.162]
[0.119,0.145]
[0.094,0.116]
[0.002,0.012]
[-0.001,0.009]

Table 4: Description of six simulations for the letter recognition problem. In each of the six
simulations, 1000 independent samples of size n = 300 where generated and algorithms A and
B were used with B using the distorted metric factor w shown in the table. 95% con(cid:12)dence
intervals for n1(cid:22), (cid:26)(n1; n2) = (cid:27)2(n1;n2)
(m0))2] de(cid:12)ned
after (11) are provided. The subscripts A B and AB indicates whether we are working with
LA, LB or LAB. An asterisk besides (cid:22) indicates that powers of tests for that (cid:22) are displayed in
a (cid:12)gure. See Table 5 for the results obtained with algorithm A (the same for all 6 simulations).

(cid:27)1(n1) and r = Corr[(^(cid:22)(m)  ^(cid:22)c

(m))2; (^(cid:22)(m0)  ^(cid:22)c

n=2(cid:22)A

9n=10(cid:22)A

[0.691,0.694]

[0.585,0.589]

(cid:26)A( n

2 )
2 ; n

[0.223,0.259]

(cid:26)A( n

10)
2 ; n
[0.137,0.164]

(cid:26)A( 9n
10)
10 ; n
[0.099,0.123]

rA

[0.002,0.013]

Table 5: Con(cid:12)dence intervals for the statistics measured with algorithm A for all 6 simulations
with the letter recognition problem (see Table 4).

25

When this is not the case, the approximation (cid:26)0(n1; n2) usually appears to be reasonably
close to the actual value of the correlation (cid:26)(n1; n2). Furthermore, when we compare two al-
gorithms, the approximation (cid:26)0(n1; n2) is not smaller than the actual value of the correlation
(cid:26)AB(n1; n2), which is good since that indicates that the inference based on the corrected
pseudo-bootstrap and on the corrected resampled t-test will not be liberal, as argued in Sec-
tion 4. We (cid:12)nally note that the correlation r appears to be fairly small, except when we
compare algorithms A and B in the simple linear regression problem. Thus, as we stated at
the end of Section 3, we should expect V ar[ n2
0
n
1

J] to decrease like 1
^(cid:27)2
M .

5.1 Sizes and powers of tests

One of the most important thing to investigate is the size (probability of rejecting the null
hypothesis when it is true) of the tests based on the statistics shown in Section 4 and compare
their powers (probability of rejecting the null hypothesis when it is false). The four panels of
Figure 1 show the estimated powers of the statistics for the hypothesis H0 : n=2(cid:22)A = (cid:22)0 for
various values of (cid:22)0 in the regression problem. We estimate powers (probabilities of rejection)
by proportions of rejection observed in the simulation. We must underline that, despite
appearances, these are not \power curves" in the usual sense of the term (see Appendix A.0).
In a \power curve", the hypothesized value of n=2(cid:22)A is (cid:12)xed and the actual value of n=2(cid:22)A
varies. Here, it is the reverse that we see in a given panel: the actual value of n=2(cid:22)A is
(cid:12)xed while the hypothesized value of n=2(cid:22)A (i.e. (cid:22)0) is varied. We may call this a pseudo-
power curve. We do this because constructing \power curves" would be too computationally
expensive. Nevertheless, pseudo-power curves shown in Figure 1 convey information similar
to conventional \power curves". Indeed, we can (cid:12)nd the size of a test by reading its pseudo-
power curve at the actual value of n=2(cid:22)A (laying between the two vertical dotted lines). We
can also appreciate the progression of the power as the hypothesized value of n=2(cid:22)A and the
actual value of n=2(cid:22)A grow apart. We shall see in Figure 7 that those pseudo-power curves
are good surrogate to \power curves".

Figures 2

through 6 are counterparts of Figure 1 for other problems and/or algorithms.
Power plots corresponding to tests about n=2(cid:22)B in the regression problem and about n=2(cid:22)B in
the classi(cid:12)cation of Gaussian populations problem are not shown since they convey the same
information as Figure 1. However, missing (cid:12)gures are available in (Nadeau & Bengio, 1999).
Note that in order to reduce the number of displayed line types in Figure 1 and its coun-
terparts appearing later, some curves share the same line type. So one must take note of the
following.

(cid:15) In a given panel, you will see four solid curves. They correspond to either the resampled t-
2 . Curves with circled points
10 (40% thrown away); curves without circled points correspond to
2 . Telling apart the resampled t-test and the corrected resampled t-test is easy;

test or the corrected resampled t-test with n2 = n
correspond to n2 = n
n2 = n
the two curves that are well above all others correspond to the resampled t-test.

10 or n2 = n

in cases where training sets are 5 or 10 times larger than test sets. So we must only be concerned with (cid:26)( n
and (cid:26)( 9n

2 ; n
10 )

10 ; n

10 ).

26

R
E
W
O
P

R
E
W
O
P

0
1

.

8

.

0

6

.

0

4
0

.

2

.

0

0
0

.

0

.

1

8
0

.

6

.

0

4

.

0

2
0

.

.

0
0

SIMULATION  2

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

SIMULATION  1

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

R
E
W
O
P

70

80

90

100

mu0

110

120

130

50

60

70

mu0

80

90

SIMULATION  4

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

SIMULATION  3

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0
1

.

8
0

.

6
0

.

4
0

.

2

.

0

.

0
0

9.0

9.5

10.0

mu0

10.5

11.0

8.0

8.5

9.0

mu0

9.5

10.0

Figure 1: Powers of the tests about H0 : n
(cid:22)A = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for the
regression problem. Each panel corresponds to one of the simulations design described in
Table 2. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for the actual
(cid:22)A shown in Table 2, therefore that is where the actual size of the tests may be read. The
solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated probabilities
of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than 10% (at
signi(cid:12)cance level 5%). Where it matters J = 15, M = 10 and R = 15 were used.

n
2

2

27

SIMULATION  1

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0

mu0

SIMULATION  3

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

10

5

R
E
W
O
P

R
E
W
O
P

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

0
1

.

8
0

.

6
0

.

4
0

.

2

.

0

0
0

.

SIMULATION  2

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0

.

1

8
0

.

6

.

0

4
0

.

2
0

.

.

0
0

5

10

0

5

10

15

mu0

SIMULATION  4

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0

.

1

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

0.10

0.05

0.00

mu0

0.05

0.10

0.10

0.05

0.00

0.05

0.10

0.15

mu0

n
2

(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for
Figure 2: Powers of the tests about H0 :
the regression problem. Each panel corresponds to one of the simulations design described
in Table 2. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for the actual
(cid:22)AB shown in Table 2, therefore that is where the actual size of the tests may be read. The
solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated probabilities
of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than 10% (at
signi(cid:12)cance level 5%). Where it matters J = 15, M = 10 and R = 15 were used.

n
2

28

SIMULATION  1

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.25

mu0

SIMULATION  3

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.15

0.20

R
E
W
O
P

R
E
W
O
P

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

0
1

.

8
0

.

6

.

0

4
0

.

2

.

0

.

0
0

R
E
W
O
P

0
1

.

8
0

.

6

.

0

4

.

0

2

.

0

0
0

.

0.30

0.35

0.05

0.10

R
E
W
O
P

0
1

.

8
0

.

6
0

.

4
0

.

2

.

0

.

0
0

SIMULATION  2

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.15

mu0

SIMULATION  4

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.20

0.25

0.20

0.22

0.24

0.26

0.28

0.30

0.10

0.12

0.14

0.16

0.18

mu0

mu0

n
2

Figure 3: Powers of the tests about H0 :
(cid:22)A = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for
the classi(cid:12)cation of Gaussian populations problem. Each panel corresponds to one of the
simulations design described in Table 3. The dotted vertical lines correspond to the 95%
(cid:22)A shown in Table 3, therefore that is where the actual
con(cid:12)dence interval for the actual
size of the tests may be read. The solid horizontal line displays the nominal size of the tests,
i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal line are
signi(cid:12)cantly greater than 10% (at signi(cid:12)cance level 5%). Where it matters J = 15, M = 10
and R = 15 were used.

n
2

29

SIMULATION  1

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.05

mu0

SIMULATION  3

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.05

0.00

R
E
W
O
P

R
E
W
O
P

0

.

1

8

.

0

6
0

.

4
0

.

2

.

0

.

0
0

0
1

.

8

.

0

6

.

0

4

.

0

2

.

0

.

0
0

SIMULATION  2

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0

.

1

8

.

0

6
0

.

4
0

.

2

.

0

.

0
0

0.10

0.15

0.10

0.05

0.00

mu0

0.05

0.10

SIMULATION  4

40% thrown away
resampled t
pseudobootstrap
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0
1

.

8
0

.

6
0

.

4

.

0

2

.

0

.

0
0

0.00

0.02

0.04

0.06

0.08

0.10

0.04

0.02

mu0

0.00

mu0

0.02

0.04

n
2

(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for
Figure 4: Powers of the tests about H0 :
the classi(cid:12)cation of Gaussian populations problem. Each panel corresponds to one of the
simulations design described in Table 3. The dotted vertical lines correspond to the 95%
(cid:22)AB shown in Table 3, therefore that is where the actual
con(cid:12)dence interval for the actual n
size of the tests may be read. The solid horizontal line displays the nominal size of the tests,
i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal line are
signi(cid:12)cantly greater than 10% (at signi(cid:12)cance level 5%). Where it matters J = 15, M = 10
and R = 15 were used.

2

30

SIMULATION  1

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.45

0.50

0.55

mu0

SIMULATION  5

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

R
E
W
O
P

0

.

1

8
0

.

6
0

.

4
0

.

2

.

0

.

0
0

0
1

.

8

.

0

6

.

0

4

.

0

2

.

0

0
0

.

SIMULATION  4

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0

.

1

8

.

0

6

.

0

4
0

.

2

.

0

.

0
0

0.60

0.65

0.55

0.60

0.65

0.70

0.75

mu0

SIMULATION  6

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0
1

.

8
0

.

6

.

0

4

.

0

2

.

0

0
0

.

0.60

0.65

0.70

mu0

0.75

0.80

0.70

0.75

0.80

0.85

0.90

mu0

n
2

(cid:22)B = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for
Figure 5: Powers of the tests about H0 :
the letter recognition problem.
Each panel corresponds to one of the simulations design
described in Table 4. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for
(cid:22)B shown in Table 4, therefore that is where the actual size of the tests may
the actual
be read. The solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated
probabilities of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than
10% (at signi(cid:12)cance level 5%). Where it matters J = 15, M = 10 and R = 15 were used.

n
2

31

SIMULATION  1

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

0.05

0.10

0.15

mu0

SIMULATION  5

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

R
E
W
O
P

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2

.

0

.

0
0

SIMULATION  4

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0
1

.

8
0

.

6

.

0

4

.

0

2
0

.

0
0

.

0.20

0.25

0.10

0.05

0.00

0.05

0.10

0.15

mu0

SIMULATION  6

40% thrown away
resampled t
conservative Z
ttest (McNemar)
Dietterichs 5x2cv

R
E
W
O
P

0
1

.

8
0

.

6
0

.

4

.

0

2

.

0

0
0

.

0.10

0.05

0.00

mu0

0.05

0.10

0.20

0.15

0.10

0.05

0.00

mu0

n
2

(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 for
Figure 6: Powers of the tests about H0 :
the letter recognition problem.
Each panel corresponds to one of the simulations design
described in Table 4. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for
(cid:22)AB shown in Table 4, therefore that is where the actual size of the tests may
the actual n
be read. The solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated
probabilities of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than
10% (at signi(cid:12)cance level 5%). Where it matters J = 15, M = 10 and R = 15 were used.

2

32

(cid:15) The dotted curves depict the conservative Z test with either n2 = n
or n2 = n

2 (when it is not circled).

10 (when it is circled)

(cid:15) You might have noticed that the pseudo-bootstrap and the corrected pseudo-bootstrap
do not appear in Figure 1 and all its counterparts (except Figure 3 and Figure 4).
We ignored them because, as we anticipated from political ratios shown in Table 1,
the pseudo-bootstrap test behaves like the resampled t-test and the corrected pseudo-
bootstrap test behaves like the corrected resampled t-test. If we dont ignore the pseudo-
bootstrap, some (cid:12)gures become too crowded. We made an exception and plotted curves
corresponding to the pseudo-bootstrap in Figures 3 and 4.
In those two (cid:12)gures, the
pseudo-bootstrap and corrected pseudo-bootstrap curves are depicted with solid curves
(just like the resampled t-test and corrected resampled t-test) and obey the same logic
that applies to resampled t-test and corrected resampled t-test curves. What you must
notice is that these (cid:12)gures look like the others except that where you would have seen
a single solid curve, you now see two solid curves that nearly overlap. That shows how
similar the resampled t-test and the pseudo-bootstrap are. This similitude is present
(cid:22)AB in the classi(cid:12)cation
for all problems, no just for the inference about
of Gaussian populations (Figures 3 and 4). We chose to show the pseudo-bootstrap
curves in Figures 3 and 4 because this is where the plots looked the least messy when
the pseudo-bootstrap curves were added.

(cid:22)A or

n
2

n
2

Heres what we can draw from those (cid:12)gures.
(cid:15) The most striking feature of those (cid:12)gures is that the actual size of the resampled t-test
and the pseudo-bootstrap procedure are far away from the nominal size 10%. This is
what we expected in Section 4. The fact that those two statistics are more liberal when
n2 = n
10 (40% of the data thrown away) suggests that
(cid:26)(n1; n2) is increasing in n2. This is in line with what one can see in Tables 2, 3 and 4,
and the simple approximation (cid:26)0(n1; n2) = n2

2 than they are when n2 = n

.

n1+n2

(cid:15) We see that the sizes of the corrected resampled t-test (and corrected pseudo-bootstrap)
are in line with what we could have forecasted from Tables 2, 3 and 4. Namely the test is
liberal when (cid:26)(n1; n2) > (cid:26)0(n1; n2), conservative when (cid:26)(n1; n2) < (cid:26)0(n1; n2), and pretty
much on target when (cid:26)(n1; n2) does not di(cid:11)er signi(cid:12)cantly from (cid:26)0(n1; n2). For instance,
on Figure 1 the sizes of the corrected resampled t-test are close to the nominal 10%. We
see in Table 2 that (cid:26)A(n1; n2) does not di(cid:11)er signi(cid:12)cantly from (cid:26)0(n1; n2). Similarly, in
Figures 3 and 5, the corrected resampled t-test appears to be signi(cid:12)cantly liberal when
n2 = n
10) is signi(cid:12)cantly greater
10) = 1
than (cid:26)0( n

10 (40% of the data thrown away) 14. We see that (cid:26)A( n

10) is signi(cid:12)cantly greater than (cid:26)0( n

6 in Table 3, and (cid:26)B( n

10) = 1

2 ; n

2 ; n

2 ; n

2 ; n

6

14Actually in Figure 2 we do see that the corrected resampled t-test with n2 = n

and 4 despite the fact that (cid:26)AB( n
is barely signi(cid:12)cantly smaller than 1
V ar[ ^(cid:22)]
E[^(cid:27)2]
particular case is that the distribution of n2

10 is liberal in Simulations 2
2 ; n
2 ; n
10 )
6 in Simulation 4. But, as mentioned in Appendix A.0, the political ratio
is not the only thing determining whether inference is liberal or conservative. What happens in this
n1 ^(cid:22)1 did not appear to su(cid:11)er from this problem.

10 ) do not di(cid:11)er signi(cid:12)cantly from 1

6 in Simulation 2 and (cid:26)AB( n

n1 ^(cid:22)15 is asymmetric; n2

33

2 ; n

2 ) = 1

2 ; n

2 ; n

2 ) = 1

in Table 4. However, in those same (cid:12)gures, we see that the corrected resampled t-test
that do not throw data away is conservative and, indeed, we can see that (cid:26)A( n
2 ) is
2 ) is signi(cid:12)cantly smaller
signi(cid:12)cantly smaller than (cid:26)0( n
than (cid:26)0( n
2 in Table 4.
(cid:15) The conservative Z with n2 = n
10 (so
= 5, more in line with normal usage), the conservative Z has more interesting
that n1
n2
properties. It does not quite live up to its name since it is at times liberal, but barely so.
Its size is never very far from 10% (like 20% for instance), making it the best inference
procedure among those considered in terms of size.

2 is too conservative. However, when n2 = n

2 in Table 3, and (cid:26)B( n

2 ; n

(cid:15) The t-test and Dietterichs 5 (cid:2) 2 cv are usually well behaved in term of size, but they

are sometimes fairly liberal as can be seen in some panels of Figures 2, 3, 4 and 5.

(cid:15) When their sizes are comparable, the powers of the t-test, Dietterichs 5 (cid:2) 2 cv, conser-
vative Z throwing out 40% of the data and corrected resampled t-test throwing out 40%
of the data are fairly similar. If we have to break the tie, it appears that the t-test is the
most powerful, Dietterichs 5 (cid:2) 2 cv is the least powerful procedure and the corrected
resampled t-test and the corrected conservative Z lay in between. The fact that the
conservative Z and the corrected resampled t-test perform well despite throwing 40% of
the data indicates that these methods are very powerful compared to Dietterichs 5 (cid:2) 2
cv and the t-test. This may be seen in Figure 1 where the size of the corrected resam-
pled t-test with the full data is comparable to the size of other tests. The power of the
corrected resampled t-test is then markedly superior to the powers of other tests with
comparable size. In other (cid:12)gures, we see the power of the corrected resampled t-test
with full data and/or conservative Z with full data catch on (as we move away from the
null hypothesis) the powers of other methods that have larger size.

from Table 4 that we have simulated data with

As promised earlier, we now illustrate that pseudo-power curves are good surro-
gates to actual real power curves. For the letter recognition problem, we have the op-
portunity to draw real power curves since we have simulated data under six di(cid:11)erent
150(cid:22)B approxima-
schemes. Recall
tively equal to 0:541; 0:595; 0:634; 0:668; 0:692; 0:781 and 150(cid:22)AB approximatively equal to
0:151; 0:098; 0:059; 0:025; 0:001;0:088 in Simulations 1 through 6 respectively. The circled
lines in Figure 7 depict real power curves. For instance, in the left panel, the power of
tests for H0 :
150(cid:22)B = 0:692 has been obtained in all six simulations, enabling us to draw
the circled curves. The non-circled curves correspond to what we have been plotting so
far. Namely, in Simulation 5, we computed the powers of tests for H0 :
150(cid:22)B = (cid:22)0 with
(cid:22)0 = 0:541; 0:595; 0:634; 0:668; 0:692; 0:781, enabling us to draw the non-circled curves. We

The comparison of algorithm A and B for the regression problem is the only place where this phenomenon was
substantial in our simulation. That is why curves (other than t-test and Dietterichs 5(cid:2) 2 cv that are based on
n1 ^(cid:22)1) are asymmetric and bottom out before the actual value of n=2(cid:22)AB (laying between the vertical dotted
n2
lines). We dont observe this in other (cid:12)gures.

34

R
E
W
O
P

0

.

1

8
0

.

6

.

0

4

.

0

2

.

0

0
0

.

Corrected resampled t
Dietterichs 5x2cv
ttest (McNemar)
Conservative Z

R
E
W
O
P

0

.

1

8

.

0

6
0

.

4

.

0

2

.

0

0
0

.

Corrected resampled t
Dietterichs 5x2cv
ttest (McNemar)
Conservative Z

0.55

0.60

0.65

0.70

0.75

0.05

0.00

0.05

0.10

0.15

MU

MU

Figure 7: Real power curves (circle lines) and pseudo-power curves (not circled) in the letter
recognition problem. In the left panel, we see \real" and \pseudo" power curves for the the
null hypothesis H0 : 150(cid:22)B = 0:692. In the right panel, we see \real" and \pseudo" power
curves for the the null hypothesis H0 : 150(cid:22)AB = 0:001. See the end of Section 5.1 for more
details on their constructions. Here, the \corrected resampled t" and the \conservative Z"
statistics are those which do not throw away data.

see that circled and non-circled curves agree relatively well, leading us to believe that our
previous plots are good surrogates to real power curves.

5.2 The choice of J

10 and n2 = n

In Section 5.1, the statistics involving n2
n1 ^(cid:22)J used J = 15. We look at how those statistics
behave with varying Js, in order to formulate a recommendation on the choice of J. We
10, which correspond to a more natural usage for
are going to do so with n1 = 9n
these statistics. Of the seven statistics displayed in Section 4 (see also Table 1), (cid:12)ve involved
n1 ^(cid:22)J. We ignore the pseudo-bootstrap and the corrected pseudo-bootstrap as political ratios
n2
provided in Section 4 and empirical evidence in Section 5.1 suggest that these statistics are
virtually identical to the resampled t-test and the corrected resampled t-test (but require a lot
more computation). We therefore only consider the resampled t-test, the corrected resampled
t-test and the conservative Z here.

The investigation of the properties of those statistics will again revolve around their sizes
and powers. You will therefore see that (cid:12)gures in this section (Figures 8 to 12) are similar
to those of the Section 5.1. Note that (cid:12)gures corresponding to 9n=10(cid:22)B are not shown as
they convey no additional information. However, missing (cid:12)gures are available in (Nadeau &
Bengio, 1999).
In a given plot, we see the powers of the three statistics when J = 5, J = 10,
J = 15 and J = 25. Therefore a total of twelve curves are present in each plot.

Heres what we can draw from those (cid:12)gures.

35

0

.

1

8

.

0

6

.

0

4
0

.

2
0

.

0
1

.

8
0

.

6

.

0

4
0

.

.

2
0

R
E
W
O
P

R
E
W
O
P

SIMULATION  1

corrected resampled t
conservative Z
resampled t

25
15
10

5

25

15

10

5

25
15
10

5

25
15
10

5

0

.

1

8

.

0

6

.

0

4
0

.

2
.
0

1525

10

5

25
1015

5

R
E
W
O
P

60

70

80

90

100

110

120

130

50

60

mu0

SIMULATION  3

corrected resampled t
conservative Z
resampled t

25
15
10

5

25
1015
5

9.0

9.5

25
1015
5

25

15
10

5

10.0

mu0

0

.

1

8

.

0

6

.

0

4
0

.

2
0

.

25
15
10

5

1525
10

5

R
E
W
O
P

10.5

11.0

8.0

8.5

SIMULATION  2

corrected resampled t
conservative Z
resampled t

25

15

10

5

70

mu0

SIMULATION  4

corrected resampled t
conservative Z
resampled t

25
15
10

5

25
15
10
5

80

90

25
15
10

5

25
15
10

5

9.5

10.0

25

15

10

5

9.0

mu0

25
15
10

5

25
15
10
5

Figure 8: Powers of the tests about H0 :
9n=10(cid:22)A = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0
and J for the regression problem. Each panel corresponds to one of the simulations design
described in Table 2. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for
the actual 9n=10(cid:22)A shown in Table 2, therefore that is where the actual size of the tests may
be read. The solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated
probabilities of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than
10% (at signi(cid:12)cance level 5%). For the conservative Z, M = 10 was used.

36

R
E
W
O
P

R
E
W
O
P

0
1

.

8
0

.

6

.

0

4
0

.

2
0

.

0
0

.

0
1

.

8

.

0

6
0

.

4

.

0

2
0

.

.

0
0

25
15
10

5

10

5

25
15
10

5

0.10

0.05

25

15

10

5

25

15

10

5

SIMULATION  1

corrected resampled t
conservative Z
resampled t

25

15

10

5

0

mu0

SIMULATION  3

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.00

mu0

25
15
10

5

25
15
10

5

25
15
10
5

0

.

1

8

.

0

6
0

.

4

.

0

2
0

.

R
E
W
O
P

25

15

10

5

SIMULATION  2

corrected resampled t
conservative Z
resampled t

25
15
10

5

25

15
10

5

1525
10
5

25
15
10

5

5

10

0

5

10

15

20

25

15

10

5

25
1015
5

0
1

.

8

.

0

6

.

0

4
0

.

.

2
0

R
E
W
O
P

mu0

SIMULATION  4

corrected resampled t
conservative Z
resampled t

25
15
10

5

25
1015
5

25

15

10

5

25
15
10

5

0.05

0.10

0.05

0.00

0.05

0.10

0.15

mu0

9n=10(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0
Figure 9: Powers of the tests about H0 :
and J for the regression problem. Each panel corresponds to one of the simulations design
described in Table 2. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for
the actual 9n=10(cid:22)AB shown in Table 2, therefore that is where the actual size of the tests may
be read. The solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated
probabilities of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than
10% (at signi(cid:12)cance level 5%). For the conservative Z, M = 10 was used.

37

SIMULATION  1

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.25

mu0

SIMULATION  3

corrected resampled t
conservative Z
resampled t

25

15

10

5

25
15
10

5

25
15

10

5

R
E
W
O
P

R
E
W
O
P

8
0

.

6
0

.

4

.

0

2

.

0

0

.

1

8
0

.

6
0

.

4

.

0

2
0

.

.

0
0

25
15
10
5

0.15

0.20

25
15
10

5

25
15
10

5

25

15

10

5

0
1

.

8

.

0

6
0

.

4
0

.

2
0

.

R
E
W
O
P

25
15
10

5

25
15
10
5

0.30

0.35

0.05

0.10

1525
10

5

25
15
10

5

0

.

1

8
0

.

6

.

0

4

.

0

2
0

.

R
E
W
O
P

SIMULATION  2

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.15

mu0

SIMULATION  4

corrected resampled t
conservative Z
resampled t

25

15

10

5

25
15
10

5

25

15

10

5

1525
10

5

0.20

0.25

1525
10

5

25
15

10

5

25

15

10

5

0.20

0.22

0.24

0.26

0.28

0.10

0.12

0.14

0.16

0.18

mu0

mu0

9n
10

(cid:22)A = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 and
Figure 10: Powers of the tests about H0 :
J for the classi(cid:12)cation of Gaussian populations problem.
Each panel corresponds to one
of the simulations design described in Table 3. The dotted vertical lines correspond to the
(cid:22)A shown in Table 3, therefore that is where the
95% con(cid:12)dence interval for the actual
actual size of the tests may be read. The solid horizontal line displays the nominal size of the
tests, i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal line
are signi(cid:12)cantly greater than 10% (at signi(cid:12)cance level 5%). For the conservative Z, M = 10
was used.

9n
10

38

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0
1

.

8
0

.

6
0

.

4

.

0

.

2
0

R
E
W
O
P

R
E
W
O
P

25
15

10

5

0.05

0.00

25
15
10

5

25
15
10
5

25

15

10

5

SIMULATION  1

corrected resampled t
conservative Z
resampled t

25

15
10

5

0.05

mu0

SIMULATION  3

corrected resampled t
conservative Z
resampled t

25

15
10

5

25
1015
5

25

15

10

5

25

15

10

5

0
1

.

8

.

0

6

.

0

4
0

.

2

.

0

R
E
W
O
P

25
15

10

5

0.10

0.15

0.10

0.05

25
15

10

5

25
15
10

5

0
1

.

8

.

0

6

.

0

4

.

0

.

2
0

R
E
W
O
P

SIMULATION  2

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.00

mu0

SIMULATION  4

corrected resampled t
conservative Z
resampled t

25

15
10

5

25
15
10

5

25

15

10

5

25

15
10

5

25

15

10

5

25
15
10

5

0.05

0.10

25

15
10

5

0.00

0.02

0.04

0.06

0.08

0.04

0.02

0.00

0.02

mu0

mu0

9n
10

(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 and
Figure 11: Powers of the tests about H0 :
J for the classi(cid:12)cation of Gaussian populations problem.
Each panel corresponds to one
of the simulations design described in Table 3. The dotted vertical lines correspond to the
(cid:22)AB shown in Table 3, therefore that is where the
95% con(cid:12)dence interval for the actual
actual size of the tests may be read. The solid horizontal line displays the nominal size of the
tests, i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal line
are signi(cid:12)cantly greater than 10% (at signi(cid:12)cance level 5%). For the conservative Z, M = 10
was used.

9n
10

39

0

.

1

8
0

.

6

.

0

4
0

.

2
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

R
E
W
O
P

R
E
W
O
P

SIMULATION  1

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.15

mu0

SIMULATION  5

corrected resampled t
conservative Z
resampled t

25
15
10

5

25
15
10

5

0.05

0.10

25
15

10

5

25
15
10
5

25

15

10

5

25
15
10

5

0

.

1

8

.

0

6

.

0

4
0

.

2
.
0

R
E
W
O
P

25
15

10

5

25
15
10

5

25
15

10

5

25
15
10

5

0.20

0.25

0.10

0.05

25
15
10

5

0
1

.

8

.

0

6
0

.

4
0

.

.

2
0

R
E
W
O
P

25
15

10

5

25
15
10

5

SIMULATION  4

corrected resampled t
conservative Z
resampled t

25

15

10

5

0.00

mu0

SIMULATION  6

corrected resampled t
conservative Z
resampled t

25
15
10

5

25
15
10

5

0.05

0.10

25
1015

5

25
15
10

5

0.10

0.05

25

15

10

5

0.15

mu0

0.15

0.10

0.05

0.00

0.05

0.25

0.20

mu0

9n
10

(cid:22)AB = (cid:22)0 at level (cid:11) = 0:1 for varying (cid:22)0 and J
Figure 12: Powers of the tests about H0 :
for the letter recognition problem. Each panel corresponds to one of the simulations design
described in Table 4. The dotted vertical lines correspond to the 95% con(cid:12)dence interval for
(cid:22)AB shown in Table 4, therefore that is where the actual size of the tests may
the actual
be read. The solid horizontal line displays the nominal size of the tests, i.e. 10%. Estimated
probabilities of rejection laying above the dotted horizontal line are signi(cid:12)cantly greater than
10% (at signi(cid:12)cance level 5%). For the conservative Z, M = 10 was used.

9n
10

40

(cid:15) Again, the (cid:12)rst thing that we see is that the resampled t-test is very liberal. However,
10) is smaller
2 ). We also see that the statistic is more liberal when J is large,

things were even worse in Section 5.1. That is due to the fact that (cid:26)( 9n
than (cid:26)( n
as it should be according to the theoretical discussion of that statistic in Section 4.

10) and (cid:26)( n

10 ; n

2 ; n

2 ; n

(cid:15) The conservative Z lives up to its name.
(cid:15) Regarding the corrected resampled t-test, the plots again only con(cid:12)rm what we might
have guessed from Tables 2, 3 and 4. Namely the resampled t-test is conservative
10) is
when (cid:26)( 9n
signi(cid:12)cantly smaller then 0.1, and has size very close to 0.1 otherwise. When it is liberal
or conservative, things tend to grow worse when J increases; see Figure 10 for the
liberal case. That makes sense since the political ratio V ar[^(cid:22)]
(see Table 1) is
monotonic in J (increasing when (cid:26) > n2
).

E[^(cid:27)2] = 1+J (cid:26)
1(cid:26)
1+J n2
n1
; decreasing when (cid:26) < n2

10) is signi(cid:12)cantly greater than (cid:26)0( 9n

10) = 0:1, liberal when (cid:26)( 9n

10 ; n

10 ; n

10 ; n

(cid:15) Obviously (from equation (8) or Proposition 2), the greater J is, the greater the power
will be. Note that increasing J from 5 to 10 brings about half the improvement in the
power obtained by increasing J from 5 to 25. Similarly, increasing J from 10 to 15
brings about half the improvement in the power obtained by increasing J from 10 to 25.
With that in mind, we feel that one must take J to be at least equal to 10 as J = 5
leads to unsatisfactory power. Going beyond J = 15 gives little additional power and
is probably not worth the computational e(cid:11)ort. We could tackle this question from a
theoretical point of view. We know from (8) that V ar[ n2
. Take
10)). Increasing J from 1 to 3 reduces the variance
(cid:26) = 0:1 for instance (that is (cid:26)0( 9n
by 60%. Increasing J from 3 to 9 further halves the variance. Increasing J from 9 to 1
only halves the variance. We thus see that the bene(cid:12)t of increasing J quickly becomes
faint.

(cid:16)
(cid:26) + 1(cid:26)

n1 ^(cid:22)J] = (cid:27)1

10 ; n

(cid:17)

J

n1+n2

n1+n2

(cid:15) Since the conservative Z is fairly conservative, it rarely has the same size as the corrected
resampled t-test, making power comparison somewhat di(cid:14)cult. But it appears that the
two methods have equivalent powers which makes sense since they are both based on
n1 ^(cid:22)J. We can see this in Figures 11 and 12 where the two tests have about the same
n2
size and similar power.

Based on the above observations, we believe that J = 15 is a good choice:

it provides
good power with reasonable computational e(cid:11)ort. If computational e(cid:11)ort is not an issue, one
may take J > 15, but must not expect a great gain in power. Another reason in favor of not
taking J too large is that the size of the resampled t-test gets worse with increasing J when
that method is liberal or conservative.

Of course the choice of J is not totally independent of n1 and n2. Indeed, if one uses a
larger test set (and thus a smaller train set), then we might expect (cid:26) to be larger and therefore
J = 10 might then be su(cid:14)ciently large.

Although it is not related to the choice of J, we may comment on the choice of the inference
procedure as (cid:12)gures in this section are the most informative in that regard. If one wants an

41

inference procedure that is not liberal, the obvious choice is the conservative Z. However, if
one prefers an inference procedure with size close to the nominal size (cid:11) and is ready to accept
departures in the liberal side as well as in the conservative side, then the corrected resampled
t appears to be a good choice. However, as we shall see shortly, we can make the conservative
Z more or less conservative by playing with M. The advantage of the corrected resampled t
is that it requires little computing in comparison to the conservative Z.

Finally, we assessed to what extent the pseudo-power curves shown in Figures 8 through
12 are good surrogates to actual real power curves. The counterpart of Figure 7, not shown
here but available in (Nadeau & Bengio, 1999), shows again that the two types of curves agree
well.

5.3 The choice of M

10 and n2 = n

When using the conservative Z, we have so far always used M = 10. We study the behavior
of this statistic for various values of M in order to formulate a recommendation on the choice
of M. Again we consider the case where n1 = 9n
10. The investigation will again
revolve around the size and power of the statistic. We see in Figure 13 ((cid:12)gures for other
problems and/or algorithms convey the same information and are therefore not shown but are
available in (Nadeau & Bengio, 1999))
that the conservative Z is more conservative when
M is large. We see that there is not a great di(cid:11)erence in the behavior of the conservative
Z when M = 10 and when M = 20. For that reason, we recommend using M (cid:20) 10. The
di(cid:11)erence between M = 10 and M = 5 is more noticeable, M = 5 leads to inference that
is less conservative, which is not a bad thing considering that with M = 10 it tends to be a
little bit too conservative. With M = 5, the conservative Z is sometimes liberal, but barely
so. Using M < 5 would probably go against the primary goal of the statistic, that is provide
inference that is not liberal. Thus 5 (cid:20) M (cid:20) 10 appears to be a reasonable choice. Within this
range, pick M large if non-liberal inference is important; otherwise take M small if you want
the size of the test to be closer to the nominal size (cid:11) (you then accept the risk of performing
inference that could be slightly liberal). Of course, computational e(cid:11)ort is linear in M so that
taking M small has an additional appeal.

6 Conclusion
We have tackled the problem of estimating the variance of the cross-validation estimator of
the generalization error. In this paper, we paid special attention to the variability introduced
by the selection of a particular training set, whereas most empirical applications of machine
learning methods concentrate on estimating the variability of the estimate of generalization
error due to the (cid:12)nite test set.

A theoretical investigation of the variance to be estimated shed some valuable insight on
reasons why some estimators currently in use underestimate the variance. We showed that no
general unbiased estimator of the variance of the cross-validation estimator could be found.
This analysis allowed us to construct two variance estimates that take into account both the
variability due to the choice of the training sets and the choice of the test examples. One of the

42

SIMULATION  1

M=20
M=10
M=5

0

mu0

SIMULATION  3

M=20
M=10
M=5

10

5

R
E
W
O
P

R
E
W
O
P

0

.

1

8
0

.

6

.

0

4
0

.

2

.

0

0
0

.

0
1

.

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

SIMULATION  2

M=20
M=10
M=5

R
E
W
O
P

7
0

.

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
0

.

5

10

0

5

10

15

20

mu0

SIMULATION  4

M=20
M=10
M=5

0
1

.

8

.

0

6
0

.

4
0

.

2
0

.

R
E
W
O
P

0.10

0.05

0.00

mu0

0.05

0.10

0.05

0.00

0.05

0.10

0.15

mu0

Figure 13: Powers of the conservative Z (with J = 15) about H0 : 9n=10(cid:22)AB = (cid:22)0 at level
(cid:11) = 0:1 for varying (cid:22)0 and M for the regression problem. Each panel corresponds to one
of the simulations design described in Table 2. The dotted vertical lines correspond to the
9n=10(cid:22)AB shown in Table 2, therefore that is where
95% con(cid:12)dence interval for the actual
the actual size of the tests may be read. The solid horizontal line displays the nominal size
of the tests, i.e. 10%. Estimated probabilities of rejection laying above the dotted horizontal
line are signi(cid:12)cantly greater than 10% (at signi(cid:12)cance level 5%).

43

proposed estimators looks like the 5(cid:2)2 cv method (Dietterich, 1998) and is speci(cid:12)cally designed
to overestimate the variance to yield conservative inference. The other may overestimate or
underestimate the real variance, but is typically not too far o(cid:11) the target.

We performed a simulation where the new techniques put forward were compared to test
statistics currently used in the machine learning community. We tackle both the inference
for a generalization error of an algorithm and the comparison of the generalization errors of
two algorithms. We considered two kinds of problems: classi(cid:12)cation and regression. Various
algorithms were considered:
linear regression, regression trees, classi(cid:12)cation trees and the
nearest neighbor algorithm. Over this wide range of problems and algorithms, we found that
the new tests behave better in terms of size and have powers that are unmatched by any
known techniques (with comparable size).

The simulation also allowed us to recommend values for the parameters involved in the
proposed techniques, namely J around 15 and (for the conservative Z) M between 5 and 10.
If one wants an inference procedure that is not liberal, the natural choice is the conservative
Z. However, if one prefers an inference procedure with size close to the nominal size (cid:11) and is
ready to accept small departures in the liberal side as well as in the conservative side, then
the corrected resampled t test appears to be a good choice. The advantage of the latter is
that it requires little computing in comparison to the conservative Z.
The paper revolved around a speci(cid:12)c cross-validation estimator; one in which we split the
data sets of n examples into a training set (of size n1) and a testing set (of size n2 = n  n1),
and repeat this process J times in an independent manner. So, for instance, the testing sets
of two di(cid:11)erent splits may partially overlap. This contrasts with the most standard cross-
validation estimator for which the testing sets are mutually exclusive. Analyzing the variance
of this standard estimator and providing valid estimates of that variance would be valuable
future work.

