Abstract

We consider the semi-supervised learning problem, where a decision rule
is to be learned from labeled and unlabeled data. In this framework, we
motivate minimum entropy regularization, which enables to incorporate
unlabeled data in the standard supervised learning. Our approach in-
cludes other approaches to the semi-supervised problem as particular or
limiting cases. A series of experiments illustrates that the proposed solu-
tion benets from unlabeled data. The method challenges mixture mod-
els when the data are sampled from the distribution class spanned by the
generative model. The performances are denitely in favor of minimum
entropy regularization when generative models are misspecied, and the
weighting of unlabeled data provides robustness to the violation of the
cluster assumption. Finally, we also illustrate that the method can also
be far superior to manifold learning in high dimension spaces.

Introduction

1
In the classical supervised learning classication framework, a decision rule is to be learned
from a learning set Ln = {xi, yi}n
i=1, where each example is described by a pattern xi  X
and by the supervisors response yi   = {1, . . . , K}. We consider semi-supervised
learning, where the supervisors responses are limited to a subset of Ln.
In the terminology used here, semi-supervised learning refers to learning a decision rule on
X from labeled and unlabeled data. However, the related problem of transductive learning,
i.e. of predicting labels on a set of predened patterns, is addressed as a side issue. Semi-
supervised problems occur in many applications where labeling is performed by human
experts. They have been receiving much attention during the last few years, but some
important issues are unresolved [10].

In the probabilistic framework, semi-supervised learning can be modeled as a missing data
problem, which can be addressed by generative models such as mixture models thanks
to the EM algorithm and extensions thereof [6].Generative models apply to the joint den-
sity of patterns and class (X, Y ). They have appealing features, but they also have major
drawbacks. Their estimation is much more demanding than discriminative models, since
the model of P (X, Y ) is exhaustive, hence necessarily more complex than the model of

This work was supported in part by the IST Programme of the European Community, under the
PASCAL Network of Excellence IST-2002-506778. This publication only reects the authors views.

P (Y |X). More parameters are to be estimated, resulting in more uncertainty in the es-
timation process. The generative model being more precise, it is also more likely to be
misspecied. Finally, the tness measure is not discriminative, so that better models are
not necessarily better predictors of class labels. These difculties have lead to proposals
aiming at processing unlabeled data in the framework of supervised classication [1, 5, 11].
Here, we propose an estimation principle applicable to any probabilistic classier, aiming
at making the most of unlabeled data when they are benecial, while providing a control
on their contribution to provide robustness to the learning scheme.
2 Derivation of the Criterion
2.1 Likelihood
We rst recall how the semi-supervised learning problem ts into standard supervised
learning by using the maximum (conditional) likelihood estimation principle. The learning
set is denoted Ln = {xi, zi}n
i=1, where z  {0, 1}K denotes the dummy variable rep-
resenting the actually available labels (while y represents the precise and complete class
information): if xi is labeled k, then zik = 1 and zi` = 0 for ` 6= k; if xi is unlabeled,
then zi` = 1 for ` = 1, . . . , K.
We assume that labeling is missing at random,
P (z|x, k) = P (z|x, `), for any (k, `) pair, which implies

that is, for all unlabeled examples,

P (k|x, z) =

.

(1)

zkP (k|x)
`=1 z`P (`|x)

PK

Assuming independent examples, the conditional log-likelihood of (Z|X) on the observed
sample is then

L(; Ln) =

n

Xi=1

log  K
Xk=1

zikfk(xi; )! + h(zi) ,

(2)

where h(z), which does not depend on P (X, Y ), is only affected by the missingness mech-
anism, and fk(x; ) is the model of P (k|x) parameterized by .
This criterion is a concave function of fk(xi; ), and for simple models such as the ones
provided by logistic regression, it is also concave in , so that the global solution can
be obtained by numerical optimization. Maximizing (2) corresponds to maximizing the
complete likelihood if no assumption whatsoever is made on P (X) [6].
Provided fk(xi; ) sum to one, the likelihood is not affected by unlabeled data: unlabeled
data convey no information. In the maximum a posteriori (MAP) framework, Seeger re-
marks that unlabeled data are useless regarding discrimination when the priors on P (X)
and P (Y |X) factorize [10]: observing x does not inform about y, unless the modeler
assumes so. Benetting from unlabeled data requires assumptions of some sort on the re-
lationship between X and Y . In the Bayesian framework, this will be encoded by a prior
distribution. As there is no such thing like a universally relevant prior, we should look for
an induction bias exploiting unlabeled data when the latter is known to convey information.

2.2 When Are Unlabeled Examples Informative?
Theory provides little support to the numerous experimental evidences [5, 7, 8] showing
that unlabeled examples can help the learning process. Learning theory is mostly developed
at the two extremes of the statistical paradigm: in parametric statistics where examples are
known to be generated from a known class of distribution, and in the distribution-free Struc-
tural Risk Minimization (SRM) or Probably Approximately Correct (PAC) frameworks.
Semi-supervised learning, in the terminology used here, does not t the distribution-free
frameworks: no positive statement can be made without distributional assumptions, as for

some distributions P (X, Y ) unlabeled data are non-informative while supervised learning
is an easy task. In this regard, generalizing from labeled and unlabeled data may differ
from transductive inference.

In parametric statistics, theory has shown the benet of unlabeled examples, either for spe-
cic distributions [9], or for mixtures of the form P (x) = pP (x|1) + (1  p)P (x|2)
where the estimation problem is essentially reduced to the one of estimating the mixture
parameter p [4]. These studies conclude that the (asymptotic) information content of un-
labeled examples decreases as classes overlap.1 Thus, the assumption that classes are well
separated is sensible if we expect to take advantage of unlabeled examples.

The conditional entropy H(Y |X) is a measure of class overlap, which is invariant to the
parameterization of the model. This measure is related to the usefulness of unlabeled data
where labeling is indeed ambiguous. Hence, we will measure the conditional entropy of
class labels conditioned on the observed variables

H(Y |X, Z) = EX Y Z[log P (Y |X, Z)] ,

(3)

where EX denotes the expectation with respect to X.
In the Bayesian framework, assumptions are encoded by means of a prior on the model
parameters. Stating that we expect a high conditional entropy does not uniquely dene the
form of the prior distribution, but the latter can be derived by resorting to the maximum
entropy principle.2 Let (, ) denote the model parameters of P (X, Y, Z); the maximum
entropy prior verifying E[H(Y |X, Z)] = c, where the constant c quanties how small
the entropy should be on average, takes the form

P (, )  exp (H(Y |X, Z))) ,

(4)

where  is the positive Lagrange multiplier corresponding to the constant c.
Computing H(Y |X, Z) requires a model of P (X, Y, Z) whereas the choice of the diagno-
sis paradigm is motivated by the possibility to limit modeling to conditional probabilities.
We circumvent the need of additional modeling by applying the plug-in principle, which
consists in replacing the expectation with respect to (X, Z) by the sample average. This
substitution, which can be interpreted as modeling P (X, Z) by its empirical distribution,
yields

Hemp(Y |X, Z; Ln) = 

1
n

n

K

Xi=1

Xk=1

P (k|xi, zi) log P (k|xi, zi) .

(5)

This empirical functional is plugged in (4) to dene an empirical prior on parameters ,
that is, a prior whose form is partly dened from data [2].

2.3 Entropy Regularization

Recalling that fk(x; ) denotes the model of P (k|x), the model of P (k|x, z) (1) is
dened as follows:

gk(x, z; ) =

zkfk(x; )
`=1 z`f`(x; )

.

PK

For labeled data, gk(x, z; ) = zk, and for unlabeled data, gk(x, z; ) = fk(x; ).
From now on, we drop the reference to parameter  in fk and gk to lighten notation. The

1This statement, given explicitly by [9], is also formalized, though not stressed, by [4], where
the Fisher information for unlabeled examples at the estimate p is clearly a measure of the overlap
between class conditional densities: Iu(p) = R

pP (x|1)+(1 p)P (x|2) dx.

(P (x|1)P (x|2))2

2Here, maximum entropy refers to the construction principle which enables to derive distributions

from constraints, not to the content of priors regarding entropy.

MAP estimate is the maximizer of the posterior distribution, that is, the maximizer of

C(, ; Ln) = L(; Ln)  Hemp(Y |X, Z; Ln)

=

gk(xi, zi) log gk(xi, zi) , (6)

n

Xi=1

log  K
Xk=1

zikfk(xi)! + 

n

K

Xi=1

Xk=1

where the constant terms in the log-likelihood (2) and log-prior (4) have been dropped.
While L(; Ln) is only sensitive to labeled data, Hemp(Y |X, Z; Ln) is only affected by
the value of fk(x) on unlabeled data.
Note that the approximation Hemp (5) of H (3) breaks down for wiggly functions fk() with
abrupt changes between data points (where P (X) is bounded from below). As a result, it is
important to constrain fk() in order to enforce the closeness of the two functionals. In the
following experimental section, we imposed a smoothness constraint on fk() by adding to
the criterion C (6) a penalizer with its corresponding Lagrange multiplier .

3 Related Work
Self-Training Self-training [7] is an iterative process, where a learner imputes the labels
of examples which have been classied with condence in the previous step. Amini et al.
[1] analyzed this technique and shown that it is equivalent to a version of the classication
EM algorithm, which minimizes the likelihood deprived of the entropy of the partition. In
the context of conditional likelihood with labeled and unlabeled examples, the criterion is

n

Xi=1

log  K
Xk=1

zikfk(xi)! +

K

Xk=1

gk(xi) log gk(xi) ,

which is recognized as an instance of the criterion (6) with  = 1.
Self-condent logistic regression [5] is another algorithm optimizing the criterion for  =
1. Using smaller  values is expected to have two benets: rst, the inuence of unlabeled
examples can be controlled, in the spirit of the EM- [8], and second, slowly increasing
 denes a scheme similar to deterministic annealing, which should help the optimization
process to avoid poor local minima of the criterion.

Minimum entropy methods Minimum entropy regularizers have been used in other con-
texts to encode learnability priors (e.g. [3]). In a sense, Hemp can be seen as a poors man
way to generalize this approach to continuous input spaces. This empirical functional was
also used by Zhu et al. [13, Section 6] as a criterion to learn weight function parameters in
the context of transduction on manifolds for learning.

Input-Dependent Regularization Our criterion differs from input-dependent regular-
ization [10, 11] in that it is expressed only in terms of P (Y |X, Z) and does not involve
P (X). However, we stress that for unlabeled data, the regularizer agrees with the complete
likelihood provided P (X) is small near the decision surface. Indeed, whereas a genera-
tive model would maximize log P (X) on the unlabeled data, our criterion minimizes the
conditional entropy on the same points. In addition, when the model is regularized (e.g.
with weight decay), the conditional entropy is prevented from being too small close to the
decision surface. This will favor putting the decision surface in a low density area.

4 Experiments
4.1 Articial Data
In this section, we chose a simple experimental setup in order to avoid artifacts stemming
from optimization problems. Our goal is to check to what extent supervised learning can
be improved by unlabeled examples, and if minimum entropy can compete with generative
models which are usually advocated in this framework.

The minimum entropy regularizer is applied to the logistic regression model. It is compared
to logistic regression tted by maximum likelihood (ignoring unlabeled data) and logistic
regression with all labels known. The former shows what has been gained by handling
unlabeled data, and the latter provides the crystal ball performance obtained by guessing
correctly all labels. All hyper-parameters (weight-decay for all logistic regression models
plus the  parameter (6) for minimum entropy) are tuned by ten-fold cross-validation.
Minimum entropy logistic regression is also compared to the classic EM algorithm for
Gaussian mixture models (two means and one common covariance matrix estimated by
maximum likelihood on labeled and unlabeled examples, see e.g. [6]). Bad local maxima
of the likelihood function are avoided by initializing EM with the parameters of the true
distribution when the latter is a Gaussian mixture, or with maximum likelihood parameters
on the (fully labeled) test sample when the distribution departs from the model. This ini-
tialization advantages EM, since it is guaranteed to pick, among all local maxima of the
likelihood, the one which is in the basin of attraction of the optimal value. Furthermore,
this initialization prevents interferences that may result from the pseudo-labels given to
unlabeled examples at the rst E-step. In particular, label switching (i.e. badly labeled
clusters) is avoided at this stage.

Correct joint density model
In the rst series of experiments, we consider two-class
problems in an 50-dimensional input space. Each class is generated with equal probability
from a normal distribution. Class 1 is normal with mean (aa . . . a) and unit covariance
matrix. Class 2 is normal with mean (aa . . . a) and unit covariance matrix. Parameter
a tunes the Bayes error which varies from 1 % to 20 % (1 %, 2.5 %, 5 %, 10 %, 20 %).
The learning sets comprise nl labeled examples, (nl = 50, 100, 200) and nu unlabeled
examples, (nu = nl  (1, 3, 10, 30, 100)). Overall, 75 different setups are evaluated, and
for each one, 10 different training samples are generated. Generalization performances are
estimated on a test set of size 10 000.
This benchmark provides a comparison for the algorithms in a situation where unlabeled
data are known to convey information. Besides the favorable initialization of the EM al-
gorithm to the optimal parameters, EM benets from the correctness of the model: data
were generated according to the model, that is, two Gaussian subpopulations with identical
covariances. The logistic regression model is only compatible with the joint distribution,
which is a weaker fulllment than correctness.

As there is no modeling bias, differences in error rates are only due to differences in estima-
tion efciency. The overall error rates (averaged over all settings) are in favor of minimum
entropy logistic regression (14.1  0.3 %). EM (15.6  0.3 %) does worse on average than
logistic regression (14.9  0.3 %). For reference, the average Bayes error rate is 7.7 % and
logistic regression reaches 10.4  0.1 % when all examples are labeled.
Figure 1 provides more informative summaries than these raw numbers. The plots repre-
sent the error rates (averaged over nl) versus Bayes error rate and the nu/nl ratio. The
rst plot shows that, as asymptotic theory suggests [4, 9], unlabeled examples are mostly
informative when the Bayes error is low. This observation validates the relevance of the
minimum entropy assumption. This graph also illustrates the consequence of the demand-
ing parametrization of generative models. Mixture models are outperformed by the simple
logistic regression model when the sample size is low, since their number of parameters
grows quadratically (vs. linearly) with the number of input features.

The second plot shows that the minimum entropy model takes quickly advantage of un-
labeled data when classes are well separated. With nu = 3nl, the model considerably
improves upon the one discarding unlabeled data. At this stage, the generative models do
not perform well, as the number of available examples is low compared to the number of
parameters in the model. However, for very large sample sizes, with 100 times more unla-

40

30

20

10

)

%

(

r
o
r
r
E


t
s
e
T

5

10

15

Bayes Error (%)

15

10

)

%

(

r
o
r
r
E


t
s
e
T

20

5
1

3

10
Ratio n
u

/n
l

30

100

Figure 1: Left: test error vs. Bayes error rate for nu/nl = 10; right: test error vs. nu/nl
ratio for 5 % Bayes error (a = 0.23). Test errors of minimum entropy logistic regression ()
and mixture models (+). The errors of logistic regression (dashed), and logistic regression
with all labels known (dash-dotted) are shown for reference.

beled examples than labeled examples, the generative approach eventually becomes more
accurate than the diagnosis approach.

Misspecied joint density model
In a second series of experiments, the setup is slightly
modied by letting the class-conditional densities be corrupted by outliers. For each class,
the examples are generated from a mixture of two Gaussians centered on the same mean:
a unit variance component gathers 98 % of examples, while the remaining 2 % are gener-
ated from a large variance component, where each variable has a standard deviation of 10.
The mixture model used by EM is slightly misspecied since it is a simple Gaussian mix-
ture. The results, displayed in the left-hand-side of Figure 2, should be compared with the
right-hand-side of Figure 1. The generative model dramatically suffers from the misspec-
ication and behaves worse than logistic regression for all sample sizes. The unlabeled
examples have rst a benecial effect on test error, then have a detrimental effect when
they overwhelm the number of labeled examples. On the other hand, the diagnosis models
behave smoothly as in the previous case, and the minimum entropy criterion performance
improves.

20

15

10

)

%

(

r
o
r
r
E


t
s
e
T

30

25

20

15

10

5

)

%

(

r
o
r
r
E


t
s
e
T

5
1

3

10
Ratio n
u

/n
l

30

100

0
1

3

10
Ratio n
u

/n
l

30

100

Figure 2: Test error vs. nu/nl ratio for a = 0.23. Average test errors for minimum entropy
logistic regression () and mixture models (+). The test error rates of logistic regression
(dotted), and logistic regression with all labels known (dash-dotted) are shown for refer-
ence. Left: experiment with outliers; right: experiment with uninformative unlabeled data.

The last series of experiments illustrate the robustness with respect to the cluster assump-
tion, by testing it on distributions where unlabeled examples are not informative, and where
a low density P (X) does not indicate a boundary region. The data is drawn from two Gaus-
sian clusters like in the rst series of experiment, but the label is now independent of the
clustering: an example x belongs to class 1 if x2 > x1 and belongs to class 2 otherwise:

the Bayes decision boundary is now separates each cluster in its middle. The mixture model
is unchanged. It is now far from the model used to generate data. The right-hand-side plot
of Figure 1 shows that the favorable initialization of EM does not prevent the model to be
fooled by unlabeled data: its test error steadily increases with the amount of unlabeled data.
On the other hand, the diagnosis models behave well, and the minimum entropy algorithm
is not distracted by the two clusters; its performance is nearly identical to the one of train-
ing with labeled data only (cross-validation provides  values close to zero), which can be
regarded as the ultimate performance in this situation.

Comparison with manifold transduction Although our primary goal is to infer a deci-
sion function, we also provide comparisons with a transduction algorithm of the manifold
family. We chose the consistency method of Zhou et al. [12] for its simplicity. As sug-
gested by the authors, we set  = 0.99 and the scale parameter 2 was optimized on test
results [12]. The results are reported in Table 1. The experiments are limited due to the
memory requirements of the consistency method in our naive MATLAB implementation.

Table 1: Error rates (%) of minimum entropy (ME) vs. consistency method (CM), for
a = 0.23, nl = 50, and a) pure Gaussian clusters b) Gaussian clusters corrupted by outliers
c) class boundary separating one Gaussian cluster

500

1500

50

nu
a) ME
10.8  1.5
a) CM 21.4  7.2
b) ME
8.5  0.9
b) CM 22.0  6.7
c) ME
8.7  0.8
c) CM 51.6  7.9

150

9.8  1.9
25.5  8.1
8.3  1.5
25.6  7.4
8.3  1.1
50.5  4.0

8.8  2.0
29.6  9.0
7.5  1.5
29.8  9.7
7.2  1.0
49.3  2.6

8.3  2.6
26.8  7.2
6.6  1.5
27.7  6.8
7.2  1.7
50.2  2.2

The results are extremely poor for the consistency method, whose error is way above min-
imum entropy, and which does not show any sign of improvement as the sample of unla-
beled data grows. Furthermore, when classes do not correspond to clusters, the consistency
method performs random class assignments. In fact, our setup, which was designed for
the comparison of global classiers, is extremely defavorable to manifold methods, since
the data is truly 50-dimensional. In this situation, local methods suffer from the curse
of dimensionality, and many more unlabeled examples would be required to get sensible
results. Hence, these results mainly illustrate that manifold learning is not the best choice
in semi-supervised learning for truly high dimensional data.

4.2 Facial Expression Recognition
We now consider an image recognition problem, consisting in recognizing seven (balanced)
classes corresponding to the universal emotions (anger, fear, disgust, joy, sadness, surprise
and neutral). The patterns are gray level images of frontal faces, with standardized posi-
tions. The data set comprises 375 such pictures made of 140  100 pixels.
We tested kernelized logistic regression (Gaussian kernel), its minimum entropy version,
nearest neigbor and the consistency method. We repeatedly (10 times) sampled 1/10 of
the dataset for providing the labeled part, and the remainder for testing. Although (, 2)
were chosen to minimize the test error, the consistency method performed poorly with
63.81.3 % test error (compared to 86 % error for random assignments). Nearest-neighbor
get similar results with 63.1  1.3 % test error, and Kernelized logistic regression (ignoring
unlabeled examples) improved to reach 53.61.3 %. Minimum entropy kernelized logistic
regression regression achieves 52.0  1.9 % error (compared to about 20 % errors for
human on this database). The scale parameter chosen for kernelized logistic regression
(by ten-fold cross-validation) amount to use a global classier. Again, the local methods

fail. This may be explained by the fact that the database contains several pictures of each
person, with different facial expressions. Hence, local methods are likely to pick the same
identity instead of the same expression, while global methods are able to learn the relevant
directions.
5 Discussion
We propose to tackle the semi-supervised learning problem in the supervised learning
framework by using the minimum entropy regularizer. This regularizer is motivated by the-
ory, which shows that unlabeled examples are mostly benecial when classes have small
overlap. The MAP framework provides a means to control the weight of unlabeled exam-
ples, and thus to depart from optimism when unlabeled data tend to harm classication.

Our proposal encompasses self-learning as a particular case, as minimizing entropy in-
creases the condence of the classier output. It also approaches the solution of transduc-
tive large margin classiers in another limiting case, as minimizing entropy is a means to
drive the decision boundary from learning examples.

The minimum entropy regularizer can be applied to both local and global classiers. As a
result, it can improve over manifold learning when the dimensionality of data is effectively
high, that is, when data do not lie on a low-dimensional manifold. Also, our experiments
suggest that the minimum entropy regularization may be a serious contender to generative
models. It compares favorably to these mixture models in three situations: for small sample
sizes, where the generative model cannot completely benet from the knowledge of the
correct joint model; when the joint distribution is (even slightly) misspecied; when the
unlabeled examples turn out to be non-informative regarding class probabilities.
