Abstract

1.1. Learning a Restricted Boltzmann Machine

Restricted Boltzmann machines were devel-
oped using binary stochastic hidden units.
These can be generalized by replacing each
binary unit by an innite number of copies
that all have the same weights but have pro-
gressively more negative biases. The learning
and inference rules for these Stepped Sig-
moid Units are unchanged. They can be ap-
proximated eciently by noisy, rectied lin-
ear units. Compared with binary units, these
units learn features that are better for object
recognition on the NORB dataset and face
verication on the Labeled Faces in the Wild
dataset. Unlike binary units, rectied linear
units preserve information about relative in-
tensities as information travels through mul-
tiple layers of feature detectors.

1. Introduction

Restricted Boltzmann machines (RBMs) have been
used as generative models of many dierent types
of data including labeled or unlabeled images
(Hinton et al., 2006), sequences of mel-cepstral coef-
cients that represent speech (Mohamed & Hinton,
2010), bags of words
represent documents
(Salakhutdinov & Hinton, 2009), and user ratings of
movies (Salakhutdinov et al., 2007).
In their con-
ditional
form they can be used to model high-
dimensional temporal sequences such as video or mo-
tion capture data (Taylor et al., 2006). Their most im-
portant use is as learning modules that are composed
to form deep belief nets (Hinton et al., 2006).

that

Images composed of binary pixels can be modeled by
an RBM that uses a layer of binary hidden units (fea-
ture detectors) to model the higher-order correlations
between pixels. If there are no direct interactions be-
tween the hidden units and no direct interactions be-
tween the visible units that represent the pixels, there
is a simple and ecient method called Contrastive
Divergence to learn a good set of feature detectors
from a set of training images (Hinton, 2002). We start
with small, random weights on the symmetric connec-
tions between each pixel i and each feature detector j.
Then we repeatedly update each weight, wij, using the
dierence between two measured, pairwise correlations

wij = (<vihj>data  <vihj>recon)

(1)

where  is a learning rate, <vihj>data is the frequency
with which visible unit i and hidden unit j are on to-
gether when the feature detectors are being driven by
images from the training set and < vihj >recon is the
corresponding frequency when the hidden units are be-
ing driven by reconstructed images. A similar learning
rule can be used for the biases.

Given a training image, we set the binary state, hj, of
each feature detector to be 1 with probability

p(hj = 1) =

1

1 + exp(bj  Pivis viwij)

(2)

where bj is the bias of j and vi is the binary state
of pixel i. Once binary states have been chosen for
the hidden units we produce a reconstruction of the
training image by setting the state of each pixel to be
1 with probability

p(vi = 1) =

1

1 + exp(bi  Pjhid hjwij)

(3)

Appearing in Proceedings of the 27 th International Confer-
ence on Machine Learning, Haifa, Israel, 2010. Copyright
2010 by the author(s)/owner(s).

The learned weights and biases implicitly dene a
probability distribution over all possible binary images
via the energy, E(v, h), of a joint conguration of the

ReLUs Improve RBMs

visible and hidden units:

E(v, h) =  X

vihj wij  X

vibi  X

hjbj

(4)

i,j

i

j

p(v) = Ph eE(v,h)
Pu,g eE(u,g)

(5)

1.2. Gaussian units

RBMs were originally developed using binary stochas-
tic units for both the visible and hidden layers
To deal with real-valued data
(Hinton, 2002).
such as
images,
(Hinton & Salakhutdinov, 2006) replaced the binary
visible units by linear units with independent Gaus-
sian noise as rst suggested by (Freund & Haussler,
1994). The energy function then becomes:

in natural

the pixel

intensities

E(v, h) = X

ivis

(vi  bi)2

22
i

 X

bjhj  X

jhid

i,j

vi
i

hjwij

(6)
where i is the standard deviation of the Gaussian
noise for visible unit i.

It is possible to learn the variance of the noise for each
visible unit but this is dicult using binary hidden
units. In many applications, it is much easier to rst
normalise each component of the data to have zero
mean and unit variance and then to use noise-free re-
constructions, with the variance in equation 6 set to 1.
The reconstructed value of a Gaussian visible unit is
then equal to its top-down input from the binary hid-
den units plus its bias. We use this type of noise-free
visible unit for the models of object and face images
described later.

2. Rectied linear units

To allow each unit to express more information,
(Teh & Hinton, 2001) introduced binomial units which
can be viewed as N separate copies of a binary unit
that all share the same bias and weights. A nice side-
eect of using weight-sharing to synthesize a new type
of unit out of binary units is that the mathematics
underlying learning in binary-binary RBMs remains
unchanged. Since all N copies receive the same total
input, they all have the same probability, p, of turn-
ing on and this only has to be computed once. The
expected number that are on is N p and the variance
in this number is N p(1  p). For small p, this acts like
a Poisson unit, but as p approaches 1 the variance be-
comes small again which may not be desireable. Also,
for small values of p the growth in p is exponential in
the total input. This makes learning much less stable

10

8

6

4

2

0
5

0

5

10

Figure 1. A comparison of three dierent ways to model
rectied linear units. The red curve shows the expected
value of the sum of an innite number of binary units with
each having a bias one less than the previous one. The blue
curve is the approximation log(1+exp(x)). The green curve
is the expected value of a rectied linear unit with added
Gaussian noise as described in section 2. The red and blue
curves are virtually indistinguishable.

than for the stepped sigmoid units (SSU) described
next.

A small modication to binomial units makes them
far more interesting as models of real neurons and also
more useful for practical applications. We make an in-
nite number of copies that all have the same learned
weight vector w and the same learned bias, b, but each
copy has a dierent, xed oset to the bias. If the o-
sets are 0.5, 1.5, 2.5, ... the sum of the probabili-
ties of the copies is extremely close to having a closed
form (gure 1):

N

X

i=1

(x  i + 0.5)  log(1 + ex),

(7)

where x = vwT + b.

So the total activity of all of the copies behaves like
a noisy, integer-valued version of a smoothed rectied
linear unit1. Even though log(1 + ex) is not in the
exponential family, we can model it accurately using
a set of binary units with shared weights and xed
bias osets. This set has no more parameters than an
ordinary binary unit, but it provides a much more ex-
pressive variable. The variance in the integer activity
level is (x) so units that are rmly o do not create
noise and the noise does not become large when x is
large.

A drawback of giving each copy a bias that diers
by a xed oset is that the logistic sigmoid function
needs to be used many times to get the probabilities

1If we only use N copies, we need to subtract the term

log(1 + exN ) from the approximation.

ReLUs Improve RBMs

required for sampling an integer value correctly.
It
is possible, however, to use a fast approximation in
which the sampled value of the rectied linear unit is
not constrained to be an integer. Instead it is given by
max(0, x+N (0, (x)) where N (0, V ) is Gaussian noise
with zero mean and variance V . We call a unit that
uses this approximation a N oisyRectied Linear U nit
(NReLU) and this paper shows that NReLUs work
better than binary hidden units for several dierent
tasks.

(Jarrett et al., 2009) have explored various rectied
nonlinearities (including the max(0, x) nonlinearity,
which they refer to as positive part) in the con-
text of convolutional networks and have found them
to improve discriminative performance. Our empirical
results in sections 5 and 6 further support this ob-
servation. We also give an approximate probabilistic
interpretation for the max(0, x) nonlinearity, further
justifying their use.

3. Intensity equivariance

NReLUs have some interesting mathematical proper-
ties (Hahnloser et al., 2003), one of which is very use-
ful for object recognition. A major consideration when
designing an object recognition system is how to make
the output invariant to properties of the input such as
location, scale, orientation, lighting etc. Convolutional
neural networks are often said to achieve translation
invariance but in their pure form they actually achieve
something quite dierent. If an object is translated in
the input image, its representation in a pool of local
lters that have shared weights is also translated. So
if it can be represented well by a pattern of feature
activities when it is in one location, it can also be rep-
resented equally well by a translated pattern of feature
activities when it is another location. We call this
translation equivariance: the representation varies in
the same way as the image. In a deep convolutional
net, translation invaraince is achieved by using sub-
sampling to introduce a small amount of translation
invariance after each layer of lters.

Binary hidden units do not exhibit intensity equivari-
ance, but rectied linear units do, provided they have
zero biases and are noise-free. Scaling up all of the in-
tensities in an image by  > 0 cannot change whether
a zero-bias unit receives a total input above or below
zero. So all of the o units remain o and the re-
mainder all increase their activities by a factor of .
This stays true for many layers of rectied linear units.
When deciding whether two face images come from the
same person, we make use of this nice property of rec-
tied linear units by basing the decision on the cosine

of the angle between the activities of the feature detec-
tors in the last hidden layer. The feature vectors are
intensity equivariant and the cosine is intensity invari-
ant. The type of intensity invariance that is important
for recognition cannot be achieved by simply dividing
all the pixel intensities by their sum. This would cause
a big change in the activities of feature detectors that
attend to the parts of a face when there is a bright
spot in the background.

4. Empirical Evaluation

We empirically compare NReLUs to stochastic bi-
nary hidden units2 on two vision tasks: 1) object
recognition on the Jittered-Cluttered NORB dataset
(LeCun et al., 2004), and 2) face verication on the
Labeled Faces in the Wild dataset (Huang et al.,
2007). Both datasets contain complicated image vari-
ability that make them dicult tasks. Also, they both
already have a number of published results for various
methods, which gives a convenient basis for judging
how good our results are. We use RBMs with binary
hidden units or NReLUs to generatively pre-train one
or more layers of features and we then discriminatively
ne-tune the features using backpropagation. On both
tasks NReLUs give better discriminative performance
than binary units. The discriminative models use the
deterministic version of NReLUs that implement the
function y = max(0, x). For backpropagation, we take
the gradient of this function to be 0 when x  0 and 1
when x > 0 (i.e. we ignore the discontinuity at x = 0).

5. Jittered-Cluttered NORB

NORB is a synthetic 3D object recognition dataset
that contains ve classes of toys (humans, animals,
cars, planes, trucks) imaged by a stereo-pair cam-
era system from dierent viewpoints under dierent
lighting conditions. NORB comes in several versions
 the Jittered-Cluttered version has grayscale stereo-
pair images with cluttered background and a central
object which is randomly jittered in position, size,
pixel intensity etc. There is also a distractor object
placed in the periphery. Examples from the dataset
are shown in gure 2. For each class, there are ten
dierent instances, ve of which are in the training set
and the rest in the test set. So at test time a clas-
sier needs to recognize unseen instances of the same
classes. In addition to the ve object classes, there is
a sixth class whose images contain none of the objects
in the centre. For details see (LeCun et al., 2004).

2We also compared with binomial units but they were

no better than binary units, so we omit those results.

Truck

Car

Plane

ReLUs Improve RBMs

Animal

Human

None

Figure 2. Stereo-pair training cases from the Jittered-
Cluttered NORB training set.

5.1. Training

The stereo-pair images are subsampled from their orig-
inal resolution of 108  108  2 to 32  32  2 to speed
up experiments. They are normalized to be zero-mean
and divided by the average standard deviation of all
the pixels in all the training images. There are 291,600
training cases (48,600 cases per class) and 58,320 test
cases (9,720 cases per class). We hold out 58,320 cases
from the training set and use them as a validation set
for selecting the model architecture (number of hid-
den units and number of layers) and for early stop-
ping. The validation set is created by taking all 9,720
training images of a single (randomly selected) object
instance from each of the ve object classes, and an
equal number of randomly selected images from the
None class.

To train a classier we use a similar approach to
(Larochelle et al., 2007). We rst greedily pre-train
two layers of features, each as an RBM using CD.
Then we use multinomial regression at the top-most
hidden layer to predict the label and discriminatively
ne-tune the parameters in all layers of the classier
(see gure 3). We have tried 1000, 2000 and 4000
units for the rst hidden layer, and 1000 and 2000
units for the second one. Using more units always
gave better classication results, so the architecture
with the best results have 4000 units in the rst layer
and 2000 in the second. We suspect that the results
will be even better with more hidden units.
In all
cases, the pixels are represented by Gaussian units
(Hinton & Salakhutdinov, 2006) and the hidden units
are either NReLUs or stochastic binary units. Pre-
training is done for 300 epochs (in both layers), using
mini-batches of 100 training examples with a learn-
ing rate of 103 applied to the average per-case CD
update, along with momentum (Hinton et al., 2006).

Figure 4 shows a subset of features learned by the rst-
level RBM with 4000 NReLUs in the hidden layer.
Many of these are Gabor-like lters, so NReLUs seem
capable of learning qualitatively sensible features from
images. The classication results (section 5.2) show
that they are quantitatively sensible as well. Figure 5

Figure 3. Network architecture used for
the Jittered-
Cluttered NORB classication task. We greedily pre-train
two hidden layers of NReLUs as RBMs. The class label is
represented as a K-dimensional binary vector with 1-of-K
activation, where K is the number of classes. The classier
computes the probability of the K classes from the second
layer hidden activities h2 using the softmax function.

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005

M
B
R
e
h



t

f

o


s
t
i

n
u



n
e
d
d
h

i


f

o



n
o

i
t
c
a
r
F

0

0.1

0.2

0.3

0.4

0.5

Fraction of training images on which a hidden unit is active

0.6

0.7

0.8

0.9

Figure 5. Histogram of NReLUs binned according to how
often they are active (i.e. has a value above zero) on
training images, as computed on Jittered-Cluttered NORB
for an RBM with 4000 NReLUs in the hidden layer.

is a histogram that shows how often the hidden units
in this RBM have values above zero on the training
set. If a unit is always active, then it would end up
in the rightmost bin of the histogram. Note that an
always-active unit is purely linear since it never gets
rectied. As the histgroam shows, there are no such
units in this model. There is some variety in how often
the units are active. We have looked at the features
that correspond to each of the three peaks in the his-
togram: the peak near 0.2 on the x-axis are Gabor-like
lters, while the peak near 0.6 are point lters. The
smaller peak between 0.4 and 0.5 corresponds mostly
to more global lters.

5.2. Classication Results

Table 1 lists the test set error rates of classiers with a
single hidden layer, using either binary units or NRe-
LUs, with and without pre-training. NReLUs out-

ReLUs Improve RBMs

Figure 4. A subset of the features learned by an RBM on images from Jittered-Cluttered NORB. The RBM has 4000
NReLUs in the hidden layer, and those shown are the 216 features with the highest L2 norm. Sorting by L2 norm tends
to pick features with well-dened Gabor-like weight patterns. Only about 25% of the features are Gabor-like. The rest
consist of lters with more global weight patterns, as well as point lters that copy pixels to the hidden units.

perform binary units, both when randomly initialized
and when pre-trained. Pre-training helps improve the
performance of both unit types. But NReLUs with-
out pre-training are better than binary units with pre-
training.

Table 2 lists the results for classiers with two hidden
layers. Just as for single hidden layer classiers, NRe-
LUs outperform binary units regardless of whether
greedy pre-training is used only in the rst layer, in
both layers, or not at all. Pre-training improves the
results: pre-training only the rst layer and randomly
initializing the second layer is better than randomly
initialized both. Pre-training both layers gives further
improvement for NReLUs but not for binary units.

For comparison, the error rates of some other mod-
els are: multinomial regression on pixels 49.9%, Gaus-
sian kernel SVM 43.3%, convolutional net 7.2%, con-
volutional net with an SVM at the top-most hid-
den layer 5.9%. The last three results are from
(Bengio & LeCun, 2007). Our results are worse than
that of convolutional nets, but 1) our models use heav-
ily subsampled images, and 2) convolutional nets have
knowledge of image topology and approximate trans-
lation invariance hard-wired into their architecture.

Table 2. Test error rates for classiers with two hidden lay-
ers (4000 units in the rst, 2000 in the second), trained on
32  32  2 Jittered-Cluttered NORB images.

Layer 1

Layer 2

NReLU Binary

pre-trained?

pre-trained?

No
Yes
Yes

No
No
Yes

17.6%
16.5%
15.2%

23.6%
18.8%
18.8%

6. Labeled Faces in the Wild

The prediction task for the Labeled Faces in the Wild
(LFW) dataset is as follows: given two face images
as input, predict whether the identities of the faces
are the same or dierent. The dataset contains colour
faces of public gures collected from the web using a
frontal-face detector. The bounding box computed by
the face detector is used to approximately normalize
the faces position and scale within the image. Some
examples from the dataset are shown in gure 6. For
details see (Huang et al., 2007).

Same

Same

Same

Table 1. Test error rates for classiers with 4000 hidden
units trained on 32  32  2 Jittered-Cluttered NORB im-
ages.

Different

Different

Different

Pre-trained? NReLU Binary
23.0%
18.7%

17.8%
16.5%

No
Yes

Figure 6. Face-pair examples from the Labeled Faces in the
Wild dataset.

ReLUs Improve RBMs

6.1. Network architecture

The task requires a binary classier with two sets of
inputs (the two faces).
If we stitch the two inputs
together and treat the result as one extended input
vector, the classiers output will depend on the order
in which the inputs are stitched. To make the clas-
sier symmetric with respect to the inputs, we use a
siamese architecture (Chopra et al., 2005). The idea is
to learn a function that takes a single face as input and
computes some feature vector from it. Given a pair of
faces, this function is applied to both faces separately,
and the two corresponding feature vectors are com-
bined using a xed, symmetric function into a single
representation which is invariant to input order. The
probability of the two faces being the same person is
computed as output from this representation. The en-
tire system, including the feature extractor replicated
over the two faces, can be learned jointly.

Here we choose the feature extractor to be a fully-
connected feedforward layer of NReLUs, pre-trained
as an RBM. We use cosine distance as the symmetric
function that combines the two feature vectors. Co-
sine distance is invariant to rescaling of its inputs,
which when combined with the equivariance of NRe-
LUs makes the entire model analytically invariant to
rescaling of the pixels by a positive scalar. The in-
variance holds regardless of the number of layers of
NReLUs, so it is possible to train deep architectures
with this property. In order to make the feature ex-
tractor exactly equivariant, we do not use biases into
the hidden units. Figure 7 shows the architecture of
our face verication model.

6.2. Training

LFW images are of size 250250 (3 colour channels)
with the face in the centre and a lot of background sur-
rounding it. Recently it has been found that humans
are able to get 94.27% accuracy on the LFW task even
when the centre of the image is masked (Kumar et al.,
2009). To prevent background information from arti-
cially inating the results, we only use a 144  144
window from the centre. The images are then rotated
and scaled such that the coordinates of the eyes are the
same across all images. We further subsample this win-
dow to 3232 (3 channels). The same image normal-
ization procedure used for Jittered-Cluttered NORB is
applied here as well.

LFW contains 13,233 images of 5,749 people. For the
purposes of reporting results, the designers of LFW
have pre-dened 10 splits of the dataset for 10-fold
cross validation, each containing 5,400 training pairs
and 600 test pairs. The number of same and dier-

Figure 7. Siamese network used for the Labeled Faces in
the Wild task. The feature extractor FW contains one
hidden layer of NReLUs pre-trained as an RBM (on single
faces) with parameters W. FW is applied to the face im-
ages IA and IB, and the cosine distance d between the re-
sulting feature vectors FW(IA) and FW(IB) is computed.
The probability of the two faces having the same identity
1+exp((wd+b)) where
is then computed as P r(Same) =
w and b are scalar learnable parameters.

1

ent cases are always equal, both for training and test
sets. The identities of the people in the training and
test sets are always kept disjoint, so at test time the
model must predict on unseen identities. We rst pre-
train a layer of features as an RBM using all 10,800
single faces in the training set of each split, then plug
it into the siamese architecture in gure 7 and discrim-
inatively ne-tune the parameters on pairs of faces. As
before, during pre-training pixels are Gaussian units,
and the hidden units are either NReLUs or stochastic
binary units.

Figure 8 shows 100 of the 4000 features learned by
an RBM on 32  32 colour images with NReLUs in
the hidden layer. Like the NORB model in section
5.1, this model is also pre-trained for 300 epochs on
mini-batches of size 100 with a learning rate of 103
and momentum. The model has learned detectors for
parts of faces like eyes, nose, mouth, eye brows etc.
Some features detect the boundary of the face. There
is a mix of localized lters and more global ones that
detect more than just a single part of the face. The
histogram in gure 9 shows how often the units in
this RBM turn on for the faces in LFW. Unlike the
NORB model, here the histogram has only one peak.
In particular, there are almost no point lters in this
model.

During discriminative ne-tuning, we use a subset of
the Pubg face dataset (Kumar et al., 2009) as a vali-
dation set for selecting the model architecture and for

ReLUs Improve RBMs

ve-fold cross validation. Table 3 lists the average
accuracy of various models, along with the standard
deviations. Models using NReLUs seem to be more
accurate, but the standard deviations are too large to
draw rm conclusions.

The two current best LFW results are 0.8683  0.0034
(Wolf et al., 2009), and 0.8529  0.0123 (Kumar et al.,
2009). The former uses a commercial automatic face
alignment system to normalize the faces, while the lat-
ter uses additional labels (collected manually) that de-
scribe the face, such as ethnicity, sex, age etc. Such
enhancements can be applied to our model as well,
and they are likely to increase accuracy signicantly.
These results may also be beneting from the back-
ground pixels around the face, which we have (mostly)
removed here.

7. Mixtures of Exponentially Many

Linear Models

We have shown that NReLUs work well for discrimina-
tion, but they are also an interesting way of modeling
the density of real-valued, high-dimensional data. A
standard way to do this is to use a mixture of diag-
onal Gaussians. Alternatively we can use a mixture
of factor analysers. Both of these models are expo-
nentially inecient if the data contains componential
structure. Consider, for example, images of pairs of
independent digits. If a mixture model for single digit
images needs N components, a single mixture model
of pairs of digits needs N 2 components. Fortunately,
this exponential growth in the number of components
in the mixture can be achieved with only linear growth
in the number of latent variables and quadratic growth
in the number of parameters if we use rectied linear
hidden units.

Consider using rectied linear units with zero bias to
model data that lies on the surface of a unit hyper-
sphere. Each rectied linear unit corresponds to a
plane through the centre of the hypersphere.
It has
an activity of 0 for one half of the hypersphere and
for the other half its activity increases linearly with
distance from that plane. N units can create 2N re-
gions on the surface of the hypersphere3. As we move

3Assuming the hypersphere is at least N -dimensional.

Table 3. Accuracy on the LFW task for various models
trained on 32  32 colour images.

Pre-trained?

NReLU

Binary

No
Yes

0.7925  0.0173
0.8073  0.0134

0.7768  0.0070
0.7777  0.0109

Figure 8. A subset of the features learned by an RBM on
32 32 colour images from LFW. The RBM has 4000 NRe-
LUs in the hidden layer, and shown above are the 100 fea-
tures with the highest L2 norm.

0.12

0.1

0.08

0.06

0.04

0.02

M
B
R

e
h
t

f
o

s
t
i
n
u

n
e
d
d
h

f
o

n
o
i
t
c
a
r
F

i

0

0.1

0.2

0.3

0.4

Fraction of training images on which a hidden unit is active

0.5

0.6

0.7

0.8

0.9

Figure 9. Histogram of NReLUs binned according to how
often they have a value above zero on single face images in
LFW for an RBM with 4000 NReLUs in the hidden layer.

early stopping. This subset, called the development
set by the creators of Pubg, do not contain any iden-
tities that are in LFW.

6.3. Classication Results

As explained before, after pre-training a single layer
of features as an RBM, we insert it into the siamese
architecture in gure 7 and discriminatively ne-tune
the parameters. We have tried models with 1000, 2000,
4000 and 8000 units in the hidden layer. The dierence
in accuracy is small between 4000 and 8000 units  all
the results in this section are for 4000 units.

The rules of LFW specify that a models accuracy must
be computed using ten-fold cross validation using the
ten pre-specied splits of the dataset. To speed up ex-
periments, we merge two splits into one and perform

ReLUs Improve RBMs

around within each of these regions the subset of units
that are non-zero does not change so we have a lin-
ear model, but it is a dierent linear model in every
region. The mixing proportions of the exponentially
many linear models are dened implicitly by the same
parameters as are used to dene p(v|h) and, unlike
a directed model, the mixing proportions are hard to
compute explicitly (Nair & Hinton, 2008).

implementing an
This is a much better way of
exponentially large mixture of
linear models with
shared latent variables than the method described in
(Hinton et al., 1999) which uses directed linear models
as the components of the mixture and a separate sig-
moid belief net to decide which hidden units should be
part of the current linear model. In that model, it is
hard to infer the values of the binary latent variables
and there can be jumps in density at the boundary be-
tween two linear regions. A big advantage of switch-
ing between linear models at the point where a hidden
unit receives an input of exactly zero is that it avoids
discontinuities in the modeled probability density.

8. Summary

We showed how to create a more powerful type of hid-
den unit for an RBM by tying the weights and biases
of an innite set of binary units. We then approxi-
mated these stepped sigmoid units with noisy rectied
linear units and showed that they work better than bi-
nary hidden units for recognizing objects and compar-
ing faces. We also showed that they can deal with large
intensity variations much more naturally than binary
units. Finally we showed that they implement mix-
tures of undirected linear models (Marks & Movellan,
2001) with a huge number of components using a mod-
est number of parameters.

