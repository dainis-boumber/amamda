ABSTRACT

guage modeling is devoted to estimating terms of the form

We introduce an exponential language model which mod-
els a whole sentence or utterance as a single unit. By avoid-
ing the chain rule, the model treats each sentence as a bag
of features, where features are arbitrary computable prop-
erties of the sentence. The new model is computationally
more efcient, and more naturally suited to modeling global
sentential phenomena, than the conditional exponential (e.g.
Maximum Entropy) models proposed to date. Using the
model is straightforward. Training the model requires sam-
pling from an exponential distribution. We describe the
challenge of applying Monte Carlo Markov Chain (MCMC)
and other sampling techniques to natural language, and dis-
cuss smoothing and step-size selection. We then present a
novel procedure for feature selection, which exploits dis-
crepancies between the existing model and the training cor-
pus. We demonstrate our ideas by constructing and analyz-
ing competitive models in the Switchboard domain, incor-
porating lexical and syntactic information.

1. MOTIVATION AND OUTLINE

Conventional statistical language models estimate the prob-

it into a product of conditional probabilities:

ability of a sentence by using the chain rule to decompose

def
Pr
def

where
 def




 . The vast majority of work in statistical lan-
ing word
! Currently with IBM T.J. Watson Research Center, Yorktown

Pr



	





Pr

Pr





Heights, NY 10598. e-mail: stanchen@watson.ibm.com.

is the history when predict-






The application of the chain rule is technically harm-
less since it uses an exact equality, not an approximation.
This practice is also understandable from a historical per-
spective (statistical language modeling grew out of the sta-
tistical approach to speech recognition, where the search
paradigm requires estimating the probability of individual
words). Nonetheless, it is not always desirable. Terms like

Pr

Pr
 :

 may not be the best way to think about estimating

1. Global sentence information such as grammaticality
or semantic coherence is awkward to encode in a con-
ditional framework. Some grammatical structure was
captured in the structured language model of [1] and
in the conditional exponential model of [2]. But such
structure had to be formulated in terms of partial parse
trees and left-to-right parse states. Similarly, model-
ing of semantic coherence was attempted in the con-
ditional exponential model of [3], but had to be re-
stricted to a limited number of pairwise word correla-
tions.

2. External inuences on the sentence (for example, the
effect of preceding utterances, or dialog level vari-
ables) are equally hard to encode efciently. Further-
more, such inuences must be factored into the pre-
diction of every word in the current sentence, caus-
ing small but systematic biases in the estimation to be
compounded.

3. Pr

Pr


(the Markov




"


$#&%

is typically approximated by





' for some small(

assumption). Even if such a model is improved by
including longer distance information, it still makes
many implicit independence assumptions. It is clear
from looking at language data that these assumptions
are often patently false, and that there are signicant
global dependencies both within and across sentences.


























As a simple example of the limitations of the chain rule
approach, consider one aspect of a sentence: its length. In

the utterance on its probability cannot be modeled directly.

an) -gram based model, the effect of the number of words in
Rather, it is an implicit consequence of the) -gram predic-

tion. This is later corrected during speech recognition by
a word insertion penalty, the usefulness of which proves
that length is an important feature. However, the word inser-
tion penalty can only model length as a geometric distribu-
tion, which does not t well with empirical data, especially
for short utterances.

As an alternative to the conventional conditional for-
mulation, this paper proposes a new exponential language
model which directly models the probability of an entire
sentence or utterance. The new model is conceptually sim-
pler, and more naturally suited to modeling whole-sentence
phenomena, than the conditional exponential models pro-
posed earlier. By avoiding the chain rule, the model treats
each sentence or utterance as a bag of features, where fea-
tures are arbitrary computable properties of the sentence.
The single, universal normalizing constant cannot be com-
puted exactly, but this does not interfere with training (done
via sampling) or with use. Using the model is computation-
ally straightforward. Training the model depends crucially
on efcient sampling of sentences from an exponential dis-
tribution.

In what follows, Section 2 introduces the model and
contrasts it with the conditional exponential models pro-
posed to date. Section 3 discusses training the model: it
lists several techniques for sampling from exponential dis-
tributions, shows how to apply them to the domain of nat-
ural language sentences, and compares their relative efca-
cies. Step-size selection and smoothing are also discussed
here. Section 4 describes experiments we performed with
this model, incorporating lexical and syntactic information.
Section 5 analyzes the results of the experiments, and Sec-
tion 6 summarizes and discusses our ongoing effort and fu-
ture directions. Various portions of this work were rst de-
scribed in [4, 5, 6].

2. WHOLE SENTENCE EXPONENTIAL MODELS

A whole sentence exponential language model has the form:

(1)

is a
universal normalization constant which depends only on the

,+
*$0
-/.
. 1 243




5

76
 s are the parameters of the model,-
where the
 s, and the
8


 s are arbitrary computable prop-
erties, or features, of the sentence .

the prior. For example,*$0

 might be the uniform dis-

is any arbi-
trary initial distribution, sometimes loosely referred to as

98
*$0




tribution, or else it might be derived (using the chain rule)

2.1. Using the Whole Sentence Model

ture those aspects of the data they consider appropriate or

distance dependencies, global sentence properties, as well
as more complex functions based on part-of-speech tagging,
parsing, or other types of linguistic processing.

from a conditional distribution such as an) -gram.
The features
 are selected by the modeler to cap-
protable. These can include conventional) -grams, longer-
probability of a given sentence , one need only calculate
*:0
 and the values of the various features8

 , and use

Equation 1. Thus using the model is straightforward and (as
long as the features are not too complex) computationally
trivial. Because the features could depend on any part of
the sentence, they can in general only be computed after the
entire sentence is known. Therefore, when used for speech
recognition, the model is not suitable for the rst pass of the
recognizer, and should instead be used to re-score N-best
lists.

To use the whole sentence exponential model to estimate the

2.2. Whole Sentence Maximum Entropy Models

(2)

(3)

8


@



The term exponential model refers to any model of the
form (1). A particular type of such a model is the so-called
Maximum Entropy (ME) model, where the parameters
are chosen so that the distribution satises certain linear

These target values are typically set to the expectation of

(for binary features, this simply
means their frequency in the corpus). Then, the constraint
becomes:

constraints. Specically, for each feature8
 , its expec-
tation under*
 :
is constrained to a specic value;
<>=
* of some train-
that feature under the empirical distribution?
ing corpus
 A



8
8
G


54B
F
the initial distribution*$0
Discrimination Information (MDI) solution. If*:0


lution1. Furthermore, if the feature target values;
HI

If the constraints (2) are consistent, there exists a unique
solution within the exponential family (1) which satises
them. Among all (not necessarily exponential) solutions to
equations (2), the exponential solution is the one closest to
in the Kullback-Liebler sense,
and is thus called the Minimum Divergence or Minimum
is uni-
form, this becomes simply the Maximum Entropy (ME) so-

1In the literature, the term Maximum Entropy or ME is used loosely
to refer to both situations, i.e. regardless of whether the initial distribution

<DC



is uniform. We will follow this practice here.

 are

*
6


6

8

8


;





*
.

=

+
E
A
5

F
the empirical expectations over some training corpus (as in
equations (3)), the MDI or ME solution is also the Max-
imum Likelihood solution of the exponential family. For
more information, see [7, 8, 3].

we take

to the
. In particular,

The MDI or ME solution can be found by an iterative
procedure such as the Generalized Iterative Scaling (GIS)

 s. At each itera-
tion, the algorithm improves the
algorithm [9]. GIS starts with arbitrary6
 values by comparing
the expectation of each feature under the current*
target value, and modifying the associated6
$K
>J
<>=:S
NMPORQ
UT
whereL
 determines the step size (see Section 3.2).
computing the expectations<V=$S
GX



quires a summation over all possible sentences , a clearly
Instead, we estimate<V=$S
WT by sampling
from the distribution*

 and using the sample expecta-
tion of8
 . Sampling from an exponential distribution is a

non-trivial task. Efcient sampling is crucial to successful
training. Sampling techniques will be discussed in detail in
Section 3.1. As will be shown later on in this paper, with
these techniques it is possible to train whole-sentence ME
models using very large corpora and very many features.

In training a whole-sentence Maximum Entropy model,
re-

infeasible task.

WT

(4)

As mentioned earlier, the term exponential models
refers to all models of the form (1), whereas the term ME
models refers to exponential models where the parameters
are set to satisfy equation (2). Most of this paper deals with
ME models. For a different training criterion for exponen-
tial models, see Section 2.5.

2.3. Comparison to Conditional ME Models

It is instructive to compare the whole-sentence ME model
with conditional ME models, which have seen considerable
success recently in language modeling [10, 11, 8, 3]. The
conditional model usually has the form:

(5)

Y6

one must compute

recomputed during training for each word position in the

where the features are functions of a specic word-history
here

*$0
Z8
.'1 243



[
0 . More importantly,-
pair, and so is the baseline*
is not a true constant  it depends on and thus must be
training data. Namely, for each training datapoint
[ ,
*$0
1 243

[
\^]&_
wherea
quite severe: training a model that incorporates) -gram and

is the vocabulary. This computational burden is

 def

`8

long-distance word correlation features on some 40 million
words can take hundreds of days of computation [3, p.210].
It is this bottleneck that has hindered the widespread use of
the ME framework in language modeling. In comparison,
in our new model the universal normalization constant need
not be calculated (see Section 2.4). This not only speeds
up training, but it also signicantly speeds up using the
model in applications such as speech recognition. In fact,
when used with simple features, our model can be applied

model employing a comparable number of parameters.

The main drawbacks of the conditional ME model are
thus the severe computational bottleneck of training (espe-

with roughly the same amount of computation as an) -gram
cially of computing-
 ), and the difculties in model-

ing whole-sentence phenomena. The whole-sentence ME
model addresses both of these issues.

A nal note of comparison: A whole-sentence ME model
incorporating the same features as a conditional ME model
is in fact not identical to the latter. This is because the
training procedure used for conditional ME models restricts
the computation of the feature expectations to histories ob-
served in the training data (see [8] or [3, section 4.4]). This
biases the solution in an interesting and sometimes appro-
priate way. For example, consider word trigger features of
the form

ifgih
, and-
,q

/f
8bdcNe

for some wordsm

and
are
correlated in the training data, this will affect the solution
of the conditional ME model. In fact, if they are perfectly
correlated, always co-occurring in the same sentence, the re-

kj
,
and specically consider the two featuresmon$p
andq
. Ifm
n$p
sulting6 s will likely be one half of what their value would
relation betweenm

have been if only one of the features were used. This is ben-
ecial to the model, since it captures correlations that are
likely to recur in new data. However, a whole-sentence ME
model incorporating the same features will not use the cor-
, unless it is explicitly instructed
to do so via a separate feature. This is because the train-
ing data is not actually used in whole-sentence ME training,
except initially to derive the features target values.

andq

otherwise

2.4. Normalization and Perplexity

Z8

Just as it is infeasible to calculate exactly feature expec-
tations for whole-sentence models, it is equally infeasible

*$0

 . Fortunately, this is not necessary for train-

ing: sampling (and hence expectation estimation) can be
, as will be shown in Section 3.
Using the model as part of a classier (e.g., a speech recog-
either, because the
relative ranking of the different classes is not changed by a

to compute the normalization constant-
1s2`3
done without knowing-
nizer) does not require knowledge of-

rX

6

6
6
+
L
;

8
8
B
*
.
8

8
*




+
-



.



S
5



T


-


5



S
5

6



T





+

l
-
q
-
B
.

X

6
single, universal constant. Notice that this is not the case for
conditional exponential models.

Nonetheless, at times it may be desirable to approximate
, perhaps in order to compute perplexity. With the whole-

) over

sentence model this can be done as follows.

Thus Z can be approximated to any desired accuracy from a

be the number of sentences and words in it, respectively. By
denition

1 243
Lettu


v be the unnormalized mod-
ication made to the initial model*:0

 . Then*

*$0
tu . By the normalization constraint we have:
=&x
*$0
tuy^

tu


From which we get:-
=&x
tuy^
0 of sentences drawn from*
0 . Often,
suitably large samplez
is based on an) -gram. A reasonably efcient sampling
technique for) -grams is described later.
To estimate reduction in per-word perplexity ({N{
the*:0 baseline, letzd| be a test set, and let}Dy~| and}|
o
:
z~| 
{{z~|

:
EXPz
{{

s
z~|
{{
]
tu

z~| i arithmetic meanS
d
{N{
EXPz
tu
geometric meanS
{N{
t

where the arithmetic mean is taken with respect to*:0 and
the geometric mean with respect tozd| .
0 , i.e. if the test set is also sampled from the baseline
z~|

distribution, it follows from the law of inequality of aver-
ages that the new perplexity will always be higher. This,
however, is appropriate because any correction to an ini-
tial probability distribution will assign a lower likelihood
(and hence higher perplexity) to data generated by that dis-
tribution.

Substituting in the estimation of Z, the estimated perplexity
reduction is:

It follows then that the perplexity reduction ratio is:

Interestingly, if

T

2.5. Discriminative Training

Until now we discussed primarily Maximum Entropy or
Maximum Likelihood training. However, whole-sentence
exponential models can also be trained to directly minimize
the error rate in an application such as speech recognition.
This is known as Minimum Classication Error (MCE) train-
ing, or Discriminative Training. The log-likelihood of a

by

MPORQ



MWORQ

features is given

The last term is a weighted sum, which can be directly
(albeit only locally) optimized for MCE using a heuristic
grid search such as Powells algorithm, which searches the

whole-sentence exponential model with(
*$0
8


MPORQ
space dened by the6 s. In fact, the second term (MWORQ

can also be assigned a weight, and scores not related to
language modeling can be added to the mix as well, for
joint optimization. This is a generalization of the language
weight and word insertion penalty parameters currently
used in speech recognition. For an attempt in this direction,
see [12].


 )



2.6. Conditional Whole-Sentence Models

So far we have discussed non-conditional models of the

re-introduce the conditional form of the exponential model,
refer to the sentence
history, namely the sequence of sentences from the begin-
ning of the document or conversation up to (but not includ-
ing) the current sentence. The model then becomes:

form*

 . In order to model cross-sentence effects, one can
albeit with some modications. Let
. 1 243


Although the normalization constant is no longer uni-
versal, it is still not needed for N-best rescoring of a speech
recognizers output. This is because rescoring is typically
done one sentence at a time, with all competing hypotheses
sharing the same sentence history.

76










5

We will not pursue models of this type any further in
this paper, except to note that they can be used to exploit
session wide information, such as topics and other dialog
level features.

3. MAXIMUM ENTROPY MODEL TRAINING

3.1. Sampling

is infeasible,

Since explicit summation over all sentences
we will estimate the expectations<
8

this section, we describe several statistical techniques for
sampling from exponential distributions, and evaluate their
efcacy for generating natural language sentences.

T by sampling. In

Gibbs Sampling [13] is a well known technique for
sampling from exponential distributions. It was used in [14]
to sample from the population of character strings. We will
now describe how to use it to generate whole sentences from

-


X

6

8



w
5
B
*

+
-
5
B
.

+
-
<
S
T

+

<
S
T

*
0

*
|

0

-


B




|

0
T


z
*

n
-
K
K
#
5

S
6
T
*
0

*





+
-


.
*
0





8



=
S
. For

the following:

cling through all word positions in some order).

To allow transitions into sentences of any length, we do

3. Choose a word at random according to the distribu-

an unnormalized joint exponential distribution, then present
alternative methods which are more efcient in this domain.

replacement by an ordinary word, which effectively
lengthens the sentence by one word.

sentence. This constitutes a single step in the random
walk in the underlying Markov eld.

To generate a single sentence from*

 , start from any
arbitrary sentence , and iterate as follows:
1. Choose a word position (either randomly or by cy-
\ be the sentence produced by replacing the word
2. Let
in sentence with the word
in position
, calculate*

each word
in the vocabularya
*$0
Z8
1 243
v .



tion
\^]&_
. Place that word in position in the

 The end-of-sentence position is also considered for
 When the last word position in the sentence is picked,
from the Gibbs distribution*
 Draw the initial sentence from a reasonable distri-
or from*
0 . This tends to reduce the necessary num-
 For an initial sentence use the nal (or some in-
 At each iteration, consider only a subset of the vo-

termediate) sentence from a previous random walk.
This again tends to reduce the necessary number of
iterations. However, the resulting sentence may be
somewhat correlated with the previous sample3.

Generating sample sentences from a Gibbs distribution
as described above is quite slow. To speed things up, the
following variations are useful:

the end-of-sentence symbol </s> is also considered.
If chosen, this effectively shortens the sentence by
one word.

After enough iterations of the above procedure, the re-
sulting sentence is guaranteed to be an unbiased sample

2It is not theoretically known how many iterations are needed to prac-
tically guarantee unbiased samples. In our experiments we used several
thousands.

bution, such as a unigram based on the training data,

cabulary for replacement. Any subset can be chosen

ber of iterations per step.

 .2

3This is known as the long chain approach. There is an ongoing
and heated debate in the computational statistics community between long
chain and short chain supporters.

as long as the underlying Markov Chain remains er-
godic. This trades off the computational cost per it-
eration against the mixing rate of the Markov chain
(that is, the rate at which the random walk converges
to the underlying equilibrium distribution).

sentence is accepted with probability

 . This new

affects the sampling efciency; in the experiments reported
in this paper, we used a unigram distribution.

is retained. After all word
positions have been examined, the resulting sentence is added
to the sample, and this process is repeated.4 The distribution

Even with these improvements, Gibbs sampling is not
the most efcient for this domain, as the probability of a
great many sentences must be computed to generate each
sample. Metropolis sampling [15], another Markov Chain
Monte Carlo technique, appears more appropriate for this
situation. An initial sentence is chosen as before. For each
is proposed from a
in that po-

As in Gibbs sampling, adapting the Metropolis algo-
rithm to sentences of variable-length requires care. In one
solution, we pad each sentence with end-of-sentence tokens

chosen word position , a new word
distribution4
to replace the original word
sition, resulting in a proposed new sentence
4	

P
4


Otherwise, the original word
4
 used to generate new word candidates for each position
</s> up to a xed length . A sentence becomes shorter if
distribution4 used to generate new sentence candidates
must be similar to the distribution*

 we are attempting to
In importance sampling, a sample



s
&
erated according to some sentence distribution4
 , which
similarly must be close to*

B9
correct the bias introduced by sampling from4
is counted=R
of from*

 , each sample
=R
F

B9
=R
8
B9
F
 and4 . We evaluated these methods (except Gibbs

In applying Metropolis sampling, instead of replacing a
single word at a time it is possible to replace larger units. In
particular, in independence sampling we consider replac-
ing the whole sentence in each iteration. For efciency, the

4The sampling procedure is still correct if the current sentence is added
to the sample after each word position is examined; however, this process
becomes less well-dened when we consider variable-length sentences.

the last non-</s> token is changed to </s>, longer if the
rst </s> token is changed to something else.

for efcient sampling. To
instead
times, so that

sampling, which proved too slow) on some of the models to

Which sampling method is best depends on the nature of

sample from.

we have

is gen-

(6)



\



\


X

6

\
*

\



S
+

*



.
*

\

.
T




F


B


<
=
S
T

X


B




B


8

F

X





*
8&
8&

'

9
s

Metropolis

0.38 0.07
0.10 0.02
0.08 0.01
0.073 0.008
0.37 0.09

sampling algorithm

independence

0.438 0.001
0.1001 0.0004
0.0834 0.0006
0.0672 0.0005
0.311 0.001

importance

0.439 0.001
0.1001 0.0006
0.0831 0.0006
0.0676 0.0007
0.310 0.002

Table 1: Mean and standard deviation (of mean) of feature
expectation estimates for sentence-length features for three
sampling algorithms over ten runs

specic context, until the end-of-sentence symbol is gen-

from the beginning-of-sentence symbol, and iteratively gen-

be described in Section 4. These models employ a trigram

as the initial distribution*

 . (Generating sentences from
a) -gram model can be done quite efciently: one starts
erates a single word according to the) -gram model and the
erated. Generating a single word from an) -gram model
requires4
 steps. While this computation is not triv-
exponential distribution.) Therefore, taking4


ial, it is far more efcient than sampling directly from an
to be a tri-
gram model for independence and importance sampling is
very effective. To measure the effectiveness of the different
sampling algorithms, we did the following. Using an expo-
nential model with a baseline trigram trained on 3 million
words of Switchboard text ([16]) and a vocabulary of some
15,000 words, for each of the sampling methods we gen-
erated ten independent samples of 100,000 sentences. We
estimated the expectations of a set of features on each sam-
ple, and calculated the empirical variance in the estimate
of these expectations over the ten samples. More efcient
sampling algorithms should yield lower variances.

In our experiments, we found that independence sam-
pling and importance sampling both yielded excellent per-
formance, while word-based Metropolis sampling performed
substantially worse. As an example, we estimated expecta-
tions for sentence-length features of the form

8





1
0

otherwise

if


length

over ten samples of size 100,000. In Table 1, we display
the means and standard deviations of the ten expectation
estimates for each of the ve sentence-length features under
three sampling algorithms.

The efciency of importance and independence sam-
pling depends on the distance between the generating dis-

Once the distance becomes too large, Metropolis sampling
, and the re-

tribution4
 and the desired distribution*

 . If4
*$0
 , that distance will grow with each training iteration.
can be used for one iteration, say iteration(

is arguably a better model than the initial

sulting sample retained. Subsequent iterations can re-use
that sample via importance or independence sampling with

# 
*$
4

 . Note that, even if training were to stop at
# 
,*$
iteration(
model*
0 , since it has moved considerably (by our assump-

Using the techniques we discuss above, training a whole-
sentence ME model is feasible even with large corpora and
many features. And yet, training time is not negligible.
Some ideas for reducing the computational burden which
we have not yet explored include:

tion) towards satisfying the feature constraints.

gence.

the rst few iterations (we only need to know the di-
rection and rough magnitude of the correction to the

 Use only rough estimation (i.e. small sample size) in
6 s); increase sample size when approaching conver-
 Determine the sample size dynamically, based on the
 Add features gradually (this has already proven it-

self effective at least anecdotally, as reported in Sec-
tion 4.1).

number of times each feature was observed so far.

3.2. Step Size

In GIS, the step size for feature update is inversely related to
the number of active features. As sentences typically have
many features, this may result in very slow convergence.
Improved Iterative Scaling (IIS) [14] uses a larger effective
step size than GIS, but requires a great deal more bookkeep-
ing.

(7)

8

5B








nite, we would take

However, when feature expectations are near their target
value, IIS can be closely approximated with equation (4)
is taken to be a weighted average of the feature

only over the sentences in the sample used to calculate ex-
pectations. This technique resulted in convergence in all of
our experiments.

whereL
sum over all sentences; i.e., if the set of sentences were
8
8
G



In our implementation, we approximatedL
 by summing
From equation (4) we can see that if<DC
8
will have6
introduce a Gaussian prior on6

then we
. To smooth our model, we use the
fuzzy maximum entropy method rst described by [17]: We

 values and search for the

maximum a posterior model instead of the maximum like-
lihood model. This has the effect of changing Equation (2)

3.3. Smoothing

n

8



8
8

0

a




f



L


+
X
B
*
*
5

=
S
T

l

p
to

8

for some suitable variance parameter

nique, we found that over-training (overtting) was never a
problem. For a detailed analysis of this and other smoothing
techniques for Maximum Entropy models, see [18].

 . With this tech-

4. FEATURE SETS AND EXPERIMENTS

In this section we describe several experiments with the
new model, using various feature sets and sampling tech-

niques. We start with the simple reconstruction of) -grams

using Gibbs sampling, proceed with longer distance and
class based lexical relations using importance sampling, and
end with syntactic parse-based features. For subsequent
work using semantic features, see [19].

4.1. Validation




To test the feasibility of Gibbs sampling and generally val-
idate our approach, we built a whole-sentence ME model
using a small (10K word) training set of Broadcast News
to be uniform, and used uni-

gram, bigram and trigram features of the form

[20] utterances5. We set*:0

8&



# of times) -gram occurs ino

The features were not introduced all at the same time. In-
stead, the unigram features were introduced rst, and the
model was allowed to converge. Next the bigram features
were introduced, and the model again allowed to converge.
Finally the trigram features were introduced. This resulted
in faster convergence than in the simultaneous introduction
of all feature types. Training was done by Gibbs sampling
throughout.

Below we provide sample sentences generated by Gibbs
sampling from various stages of the training procedure. Ta-
ble 2 lists sample sentences generated by the initial model,

before any training took place. Since the initial6 s were all

set to zero, this is the uniform model. Tables 3 through 5
list sample sentences generated by the converged model af-
ter the introduction of unigram, bigram and trigram features,
respectively. It can be seen from the example sentences that
the model indeed successfully incorporated the information
provided by the respective features.

The model described above incorporates only conven-
tional features which are easy to incorporate in a simple

5Throughout this paper we have been referring to the unit of modeling
as a sentence. But of course, our method can be used to model any
word sequence or utterance, whether or not it is consistent with linguistic
boundaries. Naturally, linguistically induced features may or may not be
applicable to non-sentences.

conditional language mode. This was done for demonstra-
tive purposes only. The model is unaware of the nature or
complexity of the features. Arbitrary features can be accom-
modated with virtually no change in the model structure or
the code.

<s> ENOUGH CARE GREG GETTING IF O. ANSWER NEVER </s>
<s> DEATH YOUVE BOTH THEM RIGHT BACK WELL BOTH </s>
<s> MONTH THATS NEWS ANY YOUVE WROTE MUCH MEAN </s>
<s> A. HONOR WEVE ME GREG LOOK TODAY N. </s>

Table 2: Sentences generated by Gibbs sampling from an

zero, this is the uniform model.

initial (untrained) model. Since all6 s were initialized to

<s> WELL VERY DONT A ARE NOT LIVE THE </s>
<s> I RIGHT OF NOT SO ANGELES IS DONE </s>
<s> I ARE FOUR THIS KNOW DONT ABOUT OF </s>
<s> C. GO ARE TO A IT HAD SO </s>
<s> OFF THE MORE JUST POINT WANT MADE WELL </s>

Table 3: Sentences generated by Gibbs sampling from
a whole-sentence ME model trained on unigram features
only.

<s> DO YOU WANT TO DONT C. WAS YOU </s>
<s> THE I DO YOU HAVE A A US </s>
<s> BUT A LOS ANGELES ASK C. NEWS ARE </s>
<s> WE WILL YOU HAVE TO BE AGENDA AND </s>
<s> THE WAY IS THE DO YOU THINK ON </s>

Table 4: Adding bigram features.

As we mentioned earlier, Gibbs sampling turned out to
be the least efcient of all sampling techniques we consid-
ered. As we will show next, much larger corpora and many
more features can be feasibly trained with the more efcient
techniques.

In our next experiment we used a much larger corpus and a
richer set of features. Our training data consisted of 2,895,000
words (nearly 187,000 sentences) of Switchboard text (SWB)
[16]. First, we constructed a conventional trigram model on
this data using a variation of Kneser-Ney smoothing [21],

4.2. Generalized) -grams and Feature Selection
and used it as our initial distribution*$0
 . We then em-
ployed features that constrained the frequency of word) -
grams (up to) =4), distance-two (i.e. skipping one word)
word) -grams (up to) =3) [3], and class) -grams (up to
) =5) [22]. We partitioned our vocabulary (of some 15,000

words) into 100, 300, and 1000 classes using the word class-
ing algorithm of [23] on our training data.

<
=

;

n
6






<s> WHAT DO YOU HAVE TO LIVE LOS ANGELES </s>
<s> A. B. C. N. N. BUSINESS NEWS TOKYO </s>
<s> BE OF SAYS IM NOT AT THIS IT </s>
<s> BILL DORMAN BEEN WELL I THINK THE MOST </s>
<s> DO YOU HAVE TO BE IN THE WAY </s>

Table 5: Adding trigram features.

threshhold

# features
WER

LM only

avg. rank

LM only

baseline
0
36.53
40.92
27.29
35.20

300
3,500
36.49
40.95
27.26
35.28

30
19,000
36.37
40.68
26.34
34.59

15
52,000
36.29
40.46
26.42
33.93

training
corpus
count
0
0
0

trigram
corpus
count
148
148
148

43512.50
43512.50
43512.50

0

60

7080.50

Table 7: Top-1 WER and average rank of best hypothesis
using varying feature sets.

a class unigram, and indicates that the trigram model over-
generates words from this class. On further examination,
the class turned out to contain a large fraction of the rarest
words. This indicates that perhaps the smoothing of the tri-
gram model could be improved.

0
0
0
0

56
56
42
42

6160.50
6160.50
3444.50
3444.50

KNOW

feature
TALKING TO YOU KNOW
TALKING TO
TALKING/CHATTING TO
YOU KNOW
NICE/HUMONGOUS
TALKING/CHATTING
TO YOU KNOW
HOW ABOUT YOU KNOW
HOW ABOUT
HAVE
KIND OF A
WHILE/SUDDEN
VAGUELY/BLUNTLY

KNOW
KNOW

15389

22604

3382.69

Table 6:

To select specic features we devised the following pro-
cedure. First, we generated an articial corpus by sampling

 s
) -grams with largest discrepancy (according to
 statistic) between training corpus and trigram-generated
corpus of same size;) -grams with   token are distance-
two) -grams;

 notation represents a class whose two
and
most frequent members are
from our initial trigram distribution*
 . This trigram
each) -gram, we compared its count in the trigram corpus
signicantly (using a
 statis-
feature to our model.6 We tried thresholds on the
900, 3,000, 10,000, 20,000 and 52,000) -gram features, re-
) -grams with zero counts were considered to
In Table 6, we display the) -grams with the highest
scores. The majority of these) -grams involve a 4-gram

or 5-gram that occurs zero times in the training corpus and
occurs many times in the trigram corpus. These are clear
examples of longer-distance dependencies that are not mod-
eled well with a trigram model. However, the last feature is

to that in the training corpus. If these two counts differed
test), we added the corresponding

spectively.
have 0.5 counts in this analysis.

tic of 500, 200, 100, 30, and 15, resulting in approximately

corpus was of the same size as the training corpus. For

6The idea of imposing a constraint that is most violated by the current
model was rst proposed by Robert Mercer, who called it nailing down.

We measured the impact of these features by rescoring

each of our feature sets for 50 iterations of iterative scaling;
each complete training run took less than three hours on a
200 MHz Pentium Pro computer.

For each feature set, we trained the corresponding model
to 0. We used importance sampling
to calculate expectations. However, instead of generating an
entirely new sample for each iteration, we generated a single
corpus from our initial trigram model, and re-weighted this
corpus for each iteration using importance sampling. (This
technique may result in mutually inconsistent constraints
for rare features, but convergence can still be assured by

after initializing all6
reducing the step sizeL
 with each iteration.) We trained
speech recognitionE
-best lists (E
ll ) which were gen-

test set of 8,300 words. The trigram*
 served as a base-

line. For each model, we computed both the top-1 word er-
ror rate and the average rank of the least errorful hypothesis.
These gures were computed rst by combining the new
language scores with the existing acoustic scores, and again
by considering the language scores only. Results for the
three largest feature sets are summarized in Table 7 (for the
smaller feature sets improvement was smaller still). While
the specic features we selected here made only a small dif-
ference in N-best rescoring, they serve to demonstrate the
extreme generality of our model: Any computable property
of the sentence which is currently not adequately modeled
can (and should) be added into the model.

erated by the Janus system [24] on a Switchboard/CallHome

4.3. Syntactic Features

In the last set of experiments, we used features based on
variable-length syntactic categories to improve on an ini-
tial trigram model in the Switchboard domain. Our train-
ing dataset was the same Switchboard corpus used in Sec-
tion 4.2.






0





0
Due to the often agrammatical nature of Switchboard
language (informal, spontaneous telephone conversations),
we chose to use a shallow parser that, given an utterance,
produces only a at sequence of syntactic constituents. The
syntactic features were then dened in terms of these con-
stituent sequences.

4.3.1. The Shallow Switchboard Parser

The shallow Switchboard parser [25] was designed to parse
spontaneous, conversational speech in unrestricted domains.
It is very robust and fast for such sentences. First, a series of
preprocessing steps are carried out. These include eliminat-
ing word repetitions, expanding contractions, and cleaning
disuencies. Next, the parser assigns a part-of-speech tag to
each word. For example, the input sentence

Okay I uh you know I think it might be correct

will be processed into

I/NNP think/VBP it/PRPA might/AUX be/VB cor-
rect/JJ

As the next step, the parser breaks the preprocessed and
tagged sentence into one or more simplex clauses, which are
clauses containing an inected verbal form and a subject.
This simplies the input and makes parsing more robust.
In our example above, the parser will generate two simplex
clauses:

simplex 1: I/NNP think/VBP
simplex 2: it/PRPA might/AUX be/VB correct/JJ

Finally, with a set of handwritten grammar rules, the
parser parses each simplex clause into constituents. The
parsing is shallow since it doesnt generate embedded con-
stituents; i.e., the parse tree is at. In the example, simplex
1 has two constituents:

[ np] ( [NP head] I/NNP )
[ vb] ( [VP head] think/VBP )

and simplex 2 has three constituents:

[ np] ( [NP head] it/PRPA )
[ vb] ( might/AUX [VP head] be/VB )
[ prdadj] ( correct/JJ )

The parser sometimes leaves a few function words (e.g.
to, of, in) unparsed in the output. For the purpose of fea-
ture selection, we regarded each of these function words as
a constituent. Counted this way, there are a total of 110
constituent types.

4.3.2. Feature Types

As mentioned above, the shallow parser breaks an input sen-
tence into one or more simplex clauses, which are then fur-
ther broken down into at sequences of constituents. We de-
ned three types of features based solely on the constituent
types; i.e., we ignored the identities of words within the con-
stituents:

1. Constituent Sequence features: for any constituent

tures resembles traditional class trigram features, ex-
cept that they correspond to a variable number of words.

sequence of I LAUGH is np vb while that of I SEE A
BIRD is np vb np.

3. Constituent Trigram features: for any ordered con-

features is a relaxation of Constituent Sequence fea-
tures, since it doesnt require the position and number
of constituents to match exactly. As an example, both

sequence and simplex clause ,8

 =1 if and only
if the constituent sequence of simplex clause ex-
. Otherwise8
actly matches
 =0. For instance,
8 np vb I THINK
+ ,
8 np vb prdadj IT MIGHT BE CORRECT
+ ,
l , and so forth.
8 np vb IT MIGHT BE CORRECT
2. Constituent Set features: for any set of constituents,
8

 =1 if and only if the constituent set of sentence
. Otherwise8
 exactly matches
 =0. This set of
8 np, vb I LAUGH
+ and
8 np, vb I SEE A BIRD
+ , although the constituent

s ,8
s
stituent triplet&
 =1 if and only
s
'
if sentence contains that contiguous sequence at least

once. Otherwise8
 =0. This set of fea-
'
 
the same size as the training corpus, by sampling from*:0
 .
of the initial model*
 , and that adding such a feature
set of
simplex clauses of the articial corpus. Let be
 and{ be the
that in the articial corpus. Let{
and

We ran both corpora through the shallow parser and counted
the occurrences of each candidate feature. If the number of
times a feature was active in the training corpus differed sig-
nicantly from that in the articial corpus, the feature was
considered important and was incorporated into the model.
Our reasoning was that the difference is due to a deciency

will x that deciency.

We assumed that our features occur independently, and
are therefore binomially distributed. More precisely, we had
two independent sets of Bernoulli trials. One is the set of
simplex clauses of the training corpus. The other is the

4.3.3. Feature Selection

We followed the procedure described in Section 4.2 to nd
useful features. We generated an articial corpus, roughly

the number of times a feature occurs in the training corpus

















0
)
if

true occurrence probabilities associated with each corpus.

(see for example [26, page 335]). We incorporated into our

0
We tested the hypotheses
 . Approximating
0 at
the Generalized Likelihood Ratio test, we rejected
condence level
`
%
&%
.

Y
N

0 was rejected.
model those features whose
 
='= had the most signicant
tailed). The feature8
standard score 21.9 in the test, with =2968 occurrences in
the SWB corpus and =1548 in the articial corpus. More

'
='s
interesting is the feature8
4.3, and =0, =19. One may suspect that this is where

Constituent Sequence features: There were 186,903 candi-
date features of this type that occurred at least once in the
two corpora. Of those, 1,935 show a signicant difference
between the two corpora at a 95% condence level (two-

the initial trigram model makes up some unlikely phrases.
Looking at the 19 simplex clauses conrms this:

F , with

4.3.4. Results

-score

SO I HAVE NEVER REALLY INTERESTING
AND THEY MIGHT PRACTICAL
THAT WE HAVE GOOD
THAT YOU COULD LAST
BUT I WOULD SURE
AND YOU CAN CONVENIENT




Similarly, the feature8
 has standard
score -4.0, =16 and =0. This feature stands for a perfectly

plausible simplex clause form that has never been gener-
ated in the articial corpus, probably because it contains a
long-distance dependence. Indeed, the corresponding sim-
plex clauses in SWB are:

\

s

 

WHAT AREA DO YOU WORK IN
WHAT AREA DO YOU LIVE IN
WHAT HOME DO YOU LIVE IN
WHAT EXERCISE DO YOU GET INVOLVED IN




score, 27.8, is8



Constituent Set features: These features are more gen-
eral than Constituent Sequence features and thus there are
fewer of them. A total of 61,741 candidate Constituent Set
features occurred in either corpus, while 1310 showed a sig-
-
nicant difference. The one with the most signicant

==

  , with =10420 and =6971.







s

 had a

OR A TOTALLY
AND A PROPERLY
IF A WHATSOEVER

Like Constituent Sequence features, there were some Con-
stituent Set features that occurred only in the articial cor-
-score of 4.0 with

pus. For example,8
 =0 and =16:



 
='=
corpus, such as8
 with z-score 3.8, =14
and =0.
signicant. The feature8
| with
 =0 and =25 is another good example of the deciencies

Constituent Trigram features: 36,448 candidate features
of this type appeared in the corpora, of which 3,535 were
-score 4.9,

There were also features that only occurred in the SWB

BUT HE NEVER SOME
WE THE GYM EVEN SOME
IT REALLY SOME REALLY BAD
MYSELF SOMETIMES SOME ON CHANNEL 8

of the initial trigram model:


 

\

DOLLARS

4.3.5. Perplexity and Word Error Rate

perplexity under the new maximum entropy model was es-

We incorporated the 1953 Constituent Sequence features,
1310 Constituent Set features, and 3535 Constituent Tri-
gram features into a whole-sentence maximum entropy lan-
guage model, and trained its parameters with the GIS algo-
rithm. The baseline perplexity of a 90,600-word SWB test

set calculated under the initial model*$0 was 81.37. The
timated as 80.49 0.02, a relative improvement of only 1%.

Next, we tested speech recognition word error rate by
N-best list rescoring. A 200-best list with 8,300 words was
used. The WER was 36.53% with the initial model and
36.38% with all of the syntactic features added, a mere 0.4%
relative improvement.

5. ANALYSIS

In trying to understand the disappointing results of the last
section, we analyzed the likely effect of features on the -
nal model. The upper bound on improvement from a single
is the Kullback Liebler distance between

binary feature8
the true distribution of8 (as estimated by the empirical dis-
 ) and*
8
 (the distribution of8 according
tribution?

to the current model) [14, p. 4]. The effect of multiple fea-
tures is not necessarily additive (in fact, it could be supra-
or sub-additive). Nonetheless, the sum of the individual ef-
fects may still give some indication of the likely combined

8

{


{










n






%
.

+
n


%

%











=

=

F

=



=

=



F


=





F



=






=



B



*


effect. For the syntactic features we used, we computed:

 



lR

?

*$0

7

MWORQ

x
x
*$0

 

for which:

n?

is signicantly larger. The second term on the right-hand
side is usually negligible. The two factors affecting this

which translates into an expected perplexity reduction of
, where 10 is the average number of words
in a sentence). The potential impact of these features is ap-

0.43% (
parently very limited. We therefore need to seek features8
?
n?

*:0
MPORQ
 ) and the
number are thus the prevalence of the feature (?
=
 ).
=&xs
log discrepancy between the truth and the model (MWOQ
sense to a human reader? is such a feature (where?
l ). It is, of course, AI-hard to compute. How-
and*

ever, even a rough approximation of it may be quite use-
ful. Based on this analysis, we have subsequently focused
our attention on deriving a smaller number of frequent (and
likely more complex) features, based on the notion of sen-
tence coherence ([27]).

In the features we used, the latter was quite large, but the
former was very small. Thus, we need to concentrate on
more common features.

An ideal feature should occur frequently enough, yet ex-
hibit a signicant discrepancy. Does the sentence make

Frequent features are also computationally preferable.
Because the training bottleneck in whole-sentence ME mod-
els is in estimating feature expectations via sampling, the
computational cost is determined mostly by how rare the
features are and how accurately we want to model them.
The more frequent the features, the less the computation.
Note that computational cost of training depends much less
on the vocabulary, the amount of training data, or the num-
ber of features.

6. SUMMARY AND DISCUSSION

We presented an approach to incorporating arbitrary linguis-
tic information into a statistical model of natural language.
We described efcient algorithms for constructing whole-
sentence ME models, offering solutions to the questions of
sampling, step size and smoothing. We demonstrated our
approach in two domains, using lexical and syntactic fea-
tures. We also introduced a procedure for feature selection
which seeks and exploits discrepancies between an existing
model and the training corpus.

Whole-sentence ME models are more efcient than con-
ditional ME models, and can naturally express sentence-
level phenomena. It is our hope that these improvements

will break the ME usability barrier which heretoforth hin-
dered exploration and integration of multiple knowledge sources.
This will hopefully open the oodgates to experimentation,
by many researchers, with varied knowledge sources which
they believe to carry signicant information. Such sources
may include:

number agreement, parsability, other parser-supplied
information)

 Distribution of verbs and tenses in the sentence
 Various aspects of grammaticality (person agreement,
 Semantic coherence
 Dialog level information
 Prosodic and other time related information (speaking

Since all knowledge sources are incorporated in a uni-
form way, a language modeler can focus on which proper-
ties of language to model as opposed to how to model them.
Attention can thus be shifted to feature induction. Indeed,
we have started working on an interactive feature induction
methodology, recasting it as a logistic regression problem
[27, 19]. Taken together, we hope that these efforts will
help open the door to putting language back into language
modeling [28].

rate, pauses,.. . )

Acknowledgements

We are grateful to Sanjeev Khudanpur, Fred Jelinek and
Prakash Narayan for helpful discussions during the early
stages of this work; to Larry Wasserman for much advice on
sampling techniques; to Klaus Zechner for use of his parser;
and to the reviewers for many thoughtful suggestions and
comments.

