Abstract

Label dissimilarity species that a pair of ex-
amples probably have dierent class labels.
We present a semi-supervised classication
algorithm that learns from dissimilarity and
similarity information on labeled and unla-
beled data. Our approach uses a novel graph-
based encoding of dissimilarity that results
in a convex problem, and can handle both
binary and multiclass classication. Experi-
ments on several tasks are promising.

1 INTRODUCTION

Semi-supervised classication learns a classier from
both labeled and unlabeled data by encoding domain
knowledge on unlabeled data in the model [3, 11, 19].
In this paper we focus on a particular form of domain
knowledge: the label dissimilarity between examples.
We assume we are given a set of dissimilarity pairs
D = {(i, j)}. For (i, j)  D, the two points xi, xj
may be both unlabeled, or one labeled and the other
unlabeled. In either case we know they probably do
not have the same label. The dissimilarity knowledge
can be noisy.

As an example, consider the problem of predicting a
persons political view (left, right) from his/her post-
ings to online blogs. The fact that person B quotes
person A and uses expletives near the quote is a strong
indication that B disagrees with A [9]. Simple text
processing thus allows us to create a dissimilarity pair
(A,B) to reect our knowledge that A and B probably
have dierent labels (political views).

Such dissimilarity knowledge has been extensively
studied in semi-supervised clustering, where such pairs
are known as cannot-links [1, 6, 13, 14, 18], mean-
ing they cannot be in the same cluster. These meth-
ods either directly modify the clustering algorithm, or

change the underlying distance metric. Our method
is dierent in that it specically applies to classica-
tion, and works on discriminant functions. Dissimilar-
ity as negative correlation on discriminant functions
has been discussed in relational learning with Gaus-
sian processes [4], but their formulation is non-convex
and applies only to binary classication. In contrast
our formulation is convex and applicable to multiple
classes.

Our contribution is a convex method that incorporates
both similarity and dissimilarity in semi-supervised
learning. We start with graph-based semi-supervised
classication methods (e.g., [2, 20]), which allows a
natural combination of similarity and dissimilarity.
Existing graph-based semi-supervised learning meth-
ods encode label similarity knowledge, but they cannot
handle dissimilarity easily, as we show in Section 2.
We dene a mixed graph to accommodate both, and
dene the analog of graph Laplacian. We then adapt
manifold regularization [12] to the mixed graph. We
extend our method to multiclass classication in Sec-
tion 3, and present experimental results in Section 4.

2 DISSIMILARITY IN BINARY

CLASSIFICATION

Let
there be n items, of which l are labeled:
{(x1, y1),    , (xl, yl), xl+1,    , xn}. Existing graph-
based semi-supervised classication methods assume
that a graph over the n items is given. The graph is
represented by an n  n matrix W , where wij is the
non-negative edge weight between items i, j. Similar
items have large weights, reecting the domain knowl-
edge (or assumption) that they tend to have similar la-
bels. Such knowledge can be represented as a penalty
term [20] on the discriminant function f : X 7 R:

1
2

n

Xi,j=1

wij(f (xi)  f (xj))2.

(1)

Minimization of (1) tends to force f (xi)  f (xj) when
wij is large. Therefore existing graph-based meth-
ods are able to encode label similarity domain knowl-
edge. The penalty (1) can be written in quadratic form
f Lf , where f = (f (x1),    , f (xn)) and L is known
as the combinatorial graph Laplacian matrix, dened
as L = D  W where D is the diagonal degree matrix

with dii = Pn

j=1 wij.

Existing graph-based methods cannot easily handle
dissimilarity, which is the requirement that two items
have dierent labels. A small or zero weight wij does
not represent dissimilarity between xi and xj; in fact,
a zero edge weight means no preference at all. A neg-
ative weight wij < 0 does encourage a large dier-
ence between f (xi), f (xj), but this creates a num-
ber of problems. First f needs to be bounded or
{, } will be a trivial minimizer. Second, any neg-
ative weight in W will make (1), and ultimately the
whole semi-supervised problem, non-convex. One has
to resort to approximations [10, 15, 16]. It is highly
desirable to keep the optimization problem convex.

2.1 MIXED GRAPHS

Let us assume y  {1, 1} for binary classication.
Our key idea is to encode dissimilarity between i, j as
wij(f (xi) + f (xj))2. Note the summation. This term
is zero if f (xi), f (xj) have the same absolute value but
opposite signs, thus encouraging dierent labels. The
trivial case f (xi) = f (xj) = 0 is avoided by competing
terms in a risk minimization framework (Section 2.2).
The weight wij remains positive, and represents the
strength of our belief in this dissimilarity edge.

Denition 1 A mixed graph over n nodes has sim-
ilarity and dissimilarity edges, and is represented by
two n  n matrices S and W . S species the edge
type: sij = 1 if there is a similarity edge between i, j;
sij = 1 if there is a dissimilarity edge. Non-negative
weights wij  0 represent the strength of the edge, re-
gardless of its type.

The graphs in existing graph-based semi-supervised
learning methods can be viewed as having an all-one S
and the same W . Extending (1) to the mixed graph,
we would like to minimize a new penalty term

1
2

n

Xi,j=1

wij(f (xi)  sijf (xj))2.

(2)

It handles both similarity and dissimilarity, and is
clearly convex in f . Furthermore, we can re-write (2)
in a quadratic form.

Proposition 1 Let M = L + (1  S)  W , where L
is the combinatorial graph Laplacian, 1 is the all-one

matrix, and  is the Hadamard (elementwise) prod-
uct. Then M is positive semi-denite, and f Mf =
1

2 Pi,j wij(f (xi)  sijf (xj))2.

The matrix M is the mixed-graph analog of the graph
Laplacian L. Like the Laplacian, M is positive semi-
denite, as can be seen from (2). If the graph has no
dissimilarity edges, then M = L.

2.2 MANIFOLD REGULARIZATION

WITH DISSIMILARITY

Manifold regularization [2] generalizes graph-based
semi-supervised learning with a regularized risk mini-
mization framework. Let H be the Reproducing Ker-
nel Hilbert Space (RKHS) of a kernel K. Manifold reg-
ularization obtains the discriminant function by solv-
ing

min
f H

l

Xi=1

c(yi, f (xi)) + 1kf k2

H + 2f Lf ,

(3)

where c() is an arbitrary loss function, e.g., the hinge
loss for Support Vector Machines (SVMs), or squared
loss for Regularized Least Squares (RLS) classiers.
As before, f is the vector of discriminant function val-
ues on the n points. The rst two terms in (3) are the
same as in supervised learning, while the third term
is the additional regularization term for graph-based
semi-supervised learning. Because f is dened in H
now, it naturally extends to new test points. Noisy
labels are tolerated by the loss function.

The mixed-graph analog of (3) is

min
f H

l

Xi=1

c(yi, f (xi)) + 1kf k2

H + 2f Mf .

(4)

One can solve the optimization problem (4) directly.
Alternatively one can view the second and third terms
together as regularization by a warped kernel, as pro-
posed in [12]. In this view, one denes a second RKHS
H, which has the same functions as H but a dierent
inner product: hf, giH = hf, giH + f M g, where M
is some positive semi-denite matrix on the n points.
H + f M f . The super-
It follows that kf k2
H is
then equivalent to our semi-supervised learning prob-
lem (4), if we let M = 2
M. Importantly, it is shown
1
in [12] that the kernel K  for the warped RKHS H is
related to the original K as follows:

vised problem minf H Pl

i=1 c(yi, f (xi)) + 1kf k2

H = kf k2

(I + M K)1M kz,

k(x, z) = k(x, z)  kx

(5)
where kx = (k(x1, x),    , k(xn, x)). This allows one
to compute the warped kernel K  from some original
kernel (e.g., RBF) K and the mixed-graph M. There-
fore, to solve (4), we can use K  in conjunction with
standard supervised kernel machine software.

3 DISSIMILARITY IN

MULTICLASS CLASSIFICATION

It is non-trivial to incorporate dissimilarity into mul-
ticlass classication.
1. One-vs-rest does not work with dissimilarity and
semi-supervised learning. Suppose, for example, that
there are three classes, and that xi, xj are two unla-
beled points whose actual labels are 2 and 3, respec-
tively. Let (i, j) be specied as a dissimilarity edge. In
the binary sub-task of class 1 vs. all other classes, how-
ever, this dissimilarity edge should become a similarity
edge, since xi, xj are both in the rest meta-class.
2. One-vs-one does not work either. For any partic-
ular one-vs-one sub-task (say class 1 vs. 2), it is not
clear whether any unlabeled point (say xj which actu-
ally has class 3) should participate in the one-vs-one
semi-supervised learning. If an unlabeled point does
not have one of the two labels, its inclusion will likely
confuse learning.
3. Using the warped kernel (5) in a standard multi-
class kernel machine (e.g., multiclass SVM) does not
work. Multiclass methods use k discriminant functions
f1,    , fk, one for each class. The warped kernel incor-
rectly encourages all discriminant functions to honor
f(xi) + f(xj) = 0, which is unnecessary and poten-
tially harmful.
We found all the above approaches indeed hurt accu-
racy. These experiments are not reported here.

We therefore need to redesign the multiclass objective
in order to incorporate dissimilarity. For simplicity
we focus on multiclass SVMs, but our method works
for other loss functions too. There are several for-
mulations of multiclass SVMs, e.g., [5, 7, 17]. For
our purpose it is important to anchor the discrimi-
nant functions around zero. For this reason we start
with the formulation in [7]. A k-class SVM is de-
ned as the optimization problem of nding functions
f (x) = (f1(x),    , fk(x)) that solve:

min

s.t.

1

l Pl

i=1 Li(f (xi)  yi)+ + Pk
Pk

j=1 fj(xi) = 0,

i = 1    l,

j=1 khjk2
H

(6)

where fj(x) = hj(x)+bj for j = 1    k; hj  H, which
is the RKHS of some kernel K; and bj  R. There are
l labeled training points. L is an l  k matrix, with
the i-th row Li = (1,    , 1, 0, 1,    , 1) being an all-
one vector except the yi-th element which is zero. yi
is the given label for xi. The vector yi = (1/(k 
1),    , 1, 1/(k  1),    ) is an encoding of the label
yi, where the number 1 occurs in the yi-th position.
The plus function is (z)+ = max(0, z).
Intuitively,
(6) means that f (xi) should have elements less than
1/(k  1) for all wrong classes. It is important to
note that the elements of yi and f (xi) sum to zero.

We exploit this sum-to-zero label encoding to repre-
sent dissimilarity as a convex multiclass SVM objec-
tive. To simplify the notation, we will restrict our-
selves to dissimilarity edges with weight 1. Similarity
edges can be added to the formulation easily by us-
ing terms like (f (xi)  f (xj))2 as in [12, 20]. Given a
dissimilarity edge (s, t)  D, the key idea behind our
multiclass dissimilarity formula comes from comparing
f (xs), f (xt) for the good and bad cases. The good
case is when f takes the nominal encoding f (xs) = ys
and f (xt) = yt, and ys 6= yt. By denition ys and yt
have the form (1/(k  1),    , 1, 1/(k  1),    ),
where the elements with value 1 must be at dierent
positions. Hence ys + yt is a vector with two kinds of
elements: (k  2)/(k  1) and 2/(k  1). The bad
case is when ys = yt, so the elements with value 1
coincide. In this case the sum ys + yt has two kinds
of elements: 2 and 2/(k  1). Comparing good and
bad, we do not want any element in f (xs) + f (xt) to
be larger than (k  2)/(k  1). We are therefore led to
the following dissimilarity objective:

k

X(s,t)D

Xj=1

(cid:18)fj(xs) + fj(xt) 

k  2

k  1(cid:19)p

+

,

(7)

which is a sum of plus functions raised to the p-th
power. The advantages of this denition are that it is
convex and simple, and it reduces to our binary SVM
dissimilarity formulation when p = 2, k = 2.

In standard practice, one can combine (6) and (7) as
follows:

min

s.t.

1

l Pl
|D| P(s,t)DPk

i=1 Li(f (xi)  yi)+ + 1Pk

j=1 khjk2
H
k1(cid:17)p
j=1(cid:16)fj(xs) + fj(xt)  k2

+

+ 2

j=1 fj(xi) = 0,

i = 1    n,

(8)

Pk

where n is the sum of the number of unlabeled points
that are involved in any dissimilarity edge, plus the
number of labeled points l. The Representer Theorem
in [7] needs to be extended to include these unlabeled
points [21].
In particular, the minimizing functions
for (8) have the form

fj(x) =

n

Xi=1

cijK(xi, x) + bj for j = 1,    , k

(9)

The essential dierence to supervised learning is that
we now have n rather than l representers in (9).

Using (9), we formulate (8) as a quadratic program.
Note khjk2
j Kcj, where Kst = K(xs, xt) is the
n  n Gram matrix. We let p = 1 in the dissimilarity

H = c

objective (7). This leads to the primal form

i=1 Li(f (xi)  yi)+ + 1Pk

1

l Pl
|D| P(s,t)DPk

j=1(cid:16)fj(xs) + fj(xt)  k2

k1(cid:17)+

j=1 c

j Kcj

+ 2

j=1 fj(xi) = 0,

i = 1    n.

(10)

Pk

min

s.t.

min

s.t.

We dene an l  k matrix Y whose i-th row is y
i .
Substituting (9) into (10), we obtain

1

j=1 c

+1Pk

i=1l Lij(Kicj + bj  Yij)+

l Pj=1k
(s,t)D(cid:16)(Ks + Kt)cj + 2bj  k2
|D| Pj=1k
Pj=1k(Kicj + bj) = 0,

i = 1    n.

j Kcj

k1(cid:17)+

+ 2

(11)

Finally we introduce an l  k matrix  and a |D|  k
matrix  as auxiliary variables. With standard refor-
mulation techniques, we rewrite (11) as

min

1

l Pj=1k

i=1l Lijij + 1Pk

+ 2

(s,t)D stj

|D| Pj=1k

j=1 c

j Kcj

s.t. Kicj + bj  Yij  ij,

i = 1    l, j = 1    k

ij  0,

i = 1    l, j = 1    k

(Ks + Kt)cj + 2bj  k2

k1  stj,

stj  0, (s, t)  D, j = 1    k

Pj=1k(Kicj + bj) = 0,

i = 1    n,

(12)

where the minimization is over c, b, ,  . The quadratic
program has O(nk) variables and constraints.

4 EXPERIMENTS

In the following sections, we empirically demonstrate
the benets of incorporating dissimilarity in several
classication tasks.

4.1 STANDARD BINARY DATASETS

We rst experimented using the standard binary
datasets g50c and mac-windows used in [12] and avail-
able with the authors code at http://people.cs.
uchicago.edu/vikass/research.html. As in [12],
g50c contains 550 examples containing 50 dimensions,
and we use l = 50 labeled samples. Mac-windows has
1946 examples with 7511 dimensions, also with l = 50.

Ideally, we would like to use dissimilarity information
based on domain knowledge. However, without such
expertise available to us, we performed oracle exper-
iments in which we introduce dissimilarity edges be-
tween randomly sampled data points with dierent la-
bels. Because the edges represent ground-truth dis-
similarity, we disallow edges to touch labeled points,

to prevent the true labels propagating throughout the
unlabeled data. Note that the actual label values are
not revealedjust the fact that the points should re-
ceive dierent label classications. Simulating domain
knowledge in this manner is common for cannot-link
clustering and related work. In Section 4.3, we present
results involving real dissimilarity based on domain-
specic heuristics.

22

||xixj ||2

In this subsection, we introduce dissimilarity in the
manifold regularization framework, discussed in Sec-
tion 2.2. Following [12], we start with a Gaussian
base kernel K and encode similarity using k-nearest-
neighbor graphs with Gaussian weights.
Speci-
cally, the weight between kNN points xi and xj is
e
, while all other weights are zero. We then
add the above dissimilarity edges, and assign them a
relatively large weight (see below) to form the mixed-
graph matrix M. Our experiments used the resulting
warped kernel K  in both SVM and RLS classiers.
The methods were implemented using LIBSVM and a
modied version of the code from [12]. We used the
same parameter values as [12]. These had been tuned
in that paper with 5-fold cross validation using simi-
larity only; our dissimilarity results could become even
better with additional parameter tuning.

To compare error rate on unlabeled data used dur-
ing semi-supervised training, and on new unseen test
data, we divided each dataset into four disjoint folds.
We then performed 4-fold cross validation, using each
fold as a test set once. The test set remains unseen
throughout the learning process. The remaining three
folds comprised the training set (labeled and unlabeled
data). For each train/test split, we trained 10 dierent
classiers, each time using a dierent random choice of
labeled examples and dissimilarity edges between un-
labeled examples. The same random choices are made
in all experimental runs, so we can compare results
using paired statistical tests. We report classication
error rate on the unlabeled training set (in-sample per-
formance) and unseen test data (out-of-sample perfor-
mance). Each number is averaged over 4 folds with 10
random trials each. We address two questions in these
standard binary dataset experiments:

How does the number of dissimilarity edges in-
uence mean error rate? We experimented rst
with varying the number of dissimilarity edges in the
graph. Since we have high condence in the oracle
edges, we assign each edge a weight equal to the maxi-
mal similarity edge weight (close to 1 for our datasets).

Figure 1 shows the eect of changing the number
of dissimilarity edges in the g50c and mac-windows
datasets. Figures 1(a,b,e,f) present mean in-sample
and out-of-sample error rates using 5012800 dissimi-

larity edges, as compared to the baseline with 0 dissim-
ilarity edges, using a hinge loss function for c() in (4).
They are similar to LapSVMs, but with dissimilarity
edges. Figures 1(c,d,g,h) display comparable results
using a squared error loss function for c() in (4). These
are similar to LapRLS, but with dissimilarity edges. In
all plots, we show one standard deviation above and
below the error rate curve. The baselines here use only
similarity edges in graph-based semi-supervised learn-
ing. They are equivalent to LapSVM and LapRLS
in [12].

Figure 1 shows the positive impact of dissimilarity
edges. The eect is greater for in-sample performance;
the in-sample points were directly involved in the ker-
nel deformation, so this benet is to be expected.
Our model also generalizes to out-of-sample test data.
To measure statistical signicance, we performed two-
tailed, paired t-tests, comparing the results using each
number of dissimilarity edges to the baseline in each
of the subplots. The circled settings are statistically
signicant at the 0.05 level.

While out-of-sample performance steadily improves in
the mac-windows dataset (Figures 1(f,h)), the g50c
out-of-sample error benets less with 6400 or 12800
dissimilarity edges (Figures 1(b,d)). The increase in
error rate corresponds with near-zero in-sample error
rates, suggesting that the learning algorithm is over-
tting the dissimilarity edges. For this small dataset,
nearly all of the unlabeled points are touched by one
or more of the 640012800 dissimilarity edges. (Mac-
windows is roughly four times as large, so this is not
the case.) It seems the kernel becomes so warped that
it ts the g50c unlabeled points perfectly, but becomes
less eective in classifying unseen test points. Though
we require only f (xi)f (xj) < 0 for xi and xj to be
labeled dierently, the dissimilarity terms encourage
f (xi) = f (xj) for (i, j)  D. We believe that this
unnecessarily stringent requirement is at the root of
the observed overtting when too many dissimilarity
terms are included. While the mechanics are still un-
clear, the inappropriate demand appears to become
overwhelming, and generalization error starts to in-
crease.

What is the eect of the weight assigned to
dissimilarity edges? In the preceding experiments,
we varied the number of dissimilarity edges, but xed
their weights to roughly 1. We next xed the number
of edges at 200, and experimented with varying this
weight by a range of multiplicative factors (Figure 2).
This eectively places more or less condence in the
dissimilarity edges, compared to the similarity edges.
As before, the baseline is either LapSVM or LapRLS,
and does not use any dissimilarity.

Table 1: Mean error rate with varying numbers of dis-
similarity edges in the USPS dataset using the multi-
class SVM formulation.

Dissim. Overall
24.48
24.41
24.32
24.27
23.96
23.63
23.30

baseline 0
10
20
40
80
160
320

In-sample Out-of-sample
24.48
24.40
24.33
24.27
23.99
23.48
23.20

24.48
20.47
23.53
24.17
23.57
24.49
23.57

We observe that in-sample performance tends to ben-
et from stronger weights on dissimilarity edges (Fig-
ures 2(a,c,e,g)). The maximal decrease in mean er-
ror rate appears at a weight of approximately 64,
above which the error rate rises slightly.
In both
datasets, above a weight of approximately 100, the
out-of-sample error rate (Figures 2(b,d,f,h)) dramat-
ically rises above the baseline. This appears to be an-
other case of overttingthe kernel deformation relies
too heavily on the dissimilarity edges, and much useful
similarity information is being ignored. This results in
good in-sample performance, at the expense of correct
classication of new examples.

4.2 STANDARD MULTICLASS DATASET

We next experimented with dissimilarity in multiclass
classication as described in Section 3. We used the
standard multiclass dataset USPS test, which contains
2007 examples with 256 dimensions, each belonging to
one of 10 classes. We used labeled set size l = 50.
This dataset was also used in [12] and is available at
the URL cited above. We solve the quadratic pro-
gram in (12) using the CPLEX QP solver. We exper-
imented using varying numbers of oracle dissimilarity
edges. As before, our dissimilarity edges do not touch
labeled points. We consider those examples involved in
dissimilarity to be the unlabeled set, and the remain-
ing examples (ignored during training) the unseen test
set. We report mean error rates over 10 repeated tri-
als using dierent random labeled sets and dierent
random unlabeled-unlabeled dissimilarity edges. The
1 parameter in (12) was optimized using mean test
set performance without any dissimilarity. Thus, we
are making the baseline as strong as possible. We ar-
bitrarily set 2 = 1. Careful tuning of this parameter
could potentially lead to even better results.

Table 1 presents the overall, in-sample, and out-of-
sample mean error rates using the 2-norm SVM for-
mulation (12) with a varying number of dissimilarity
edges. Statistically signicant reductions in error rate,









0.08

0.06

0.04

0.02

0



No dissim.
Dissim.
102

0.08

0.06

0.04

0.02

0



No dissim.
Dissim.
102

103

104

(c) SE in-sample

0.08

0.06

0.04

0.02

0



No dissim.
Dissim.
102

104
(d) SE out-of-sample

103

0.08

0.06

0.04

0.02

0



No dissim.
Dissim.
102

c
0
5
g

i

s
w
o
d
n
w
-
c
a
m

104
(a) hinge in-sample

103

104
(b) hinge out-of-sample

103









0.10

0.05

0.10

0.05

0.10

0.05

0.10

0.05

No dissim.
Dissim.
102

0



103

104

(e) hinge in-sample

No dissim.
Dissim.
102

0



104
(f) hinge out-of-sample

103

No dissim.
Dissim.
102

0



103

104

(g) SE in-sample

No dissim.
Dissim.
102

0



104
(h) SE out-of-sample

103

Figure 1: Varying the number of dissimilarity edges (x-axis) in the g50c dataset (a-d) and the mac-windows
dataset (e-h). y-axis is mean error rate across 4 folds with 10 random trials each.
Hinge stands for the
hinge loss and SE the squared error loss. The baselines are LapSVM and LapRLS respectively, which have no
dissimilarity edges. Circled settings are statistically signicantly better than the baseline.

compared to the baseline, are indicated in bold face.
The 2-norm multiclass SVM formulation uses the dis-
similarity edges eectively to lower overall and out-of-
sample mean error rate for all amounts of dissimilar-
ity edges that we tested. Note that the baseline has
a higher error rate than reported in [12], and this is
because we used the multiclass SVM formulation of [7]
to allow dissimilarities, not the code in [12].

4.3 POLITICS DATASET

In our nal set of experiments, we create real (instead
of oracle) dissimilarity edges based on domain knowl-
edge. We experimented with the politics.com dis-
cussion board text data from [9]. The task here is
to predict the political aliation of the users post-
ing messages on a political discussion board. We re-
strict ourselves to the 184 users with left (96) and
right (88) political tendencies. The dataset contains
the text of several thousand posts. Quoting behavior
is annotated in the dataset, so we know who quoted
who. Since we are interested in classifying each user
(as opposed to each post), we concatenated together
all posts (excluding quoted text) written by a user. We
removed punctuation and common English words, and
applied stemming. We then formed term frequency-
inverse document frequency (TF-IDF) vectors (see [8])
for each user using word types occurring 10 or more
times, which resulted in 8656 unique terms.

We created dissimilarity edges by the quoting behav-
ior between users. In political discussion boards, users
tend to quote posts by users with diering political
views [9]. For example, users often debate a con-

troversial issue, quoting and disputing each others
previous claims. We declare disagreement between
A and B if B quotes A, and the text adjacent to
the quoted text contains two or more question marks
or exclamation marks, or two or more consecutive
words in all capital letters (i.e., Internet shouting1).
Consider the following illustrative example taken
from the current dataset, where the user Dixie has
quoted and responded to the user deshrubinator:

deshrubinator: You were the one who thought it should
be investigated last week.
Dixie: No I didnt, and I made it clear. You are insane!
YOU are the one with NO ****ING RESPECT FOR
DEMOCRACY!

We create a dissimilarity edge (A,B) if they have ex-
hibited such seemingly hostile behavior toward each
other in more than 2 posts. This thresholding ensures
that we have seen multiple pieces of evidence for dis-
similarity.

It is worth noting that our dissimilarity edges only
need simple text processing, and can be easily de-
ned over unlabeled data (users with unknown politi-
cal view). For this experiment we do not include sim-
ilarity edges, partly because the standard cosine sim-
ilarity on text [8] measures similarity in topics (note
users from dierent parties do talk about the same
topic), rather than sentiment, which is more relevant
to the current task. We will investigate high quality
similarity edges in future work. Therefore we cannot
use LapSVM or LapRLS as our baselines. Instead we

1We also require these words to be more than three char-
acters long to avoid false positives from common Internet
abbreviations like LOL (laugh out loud).





No dissim.
Dissim.

c
0
5
g

0.10

0.05

0



100

102
(a) hinge in-sample

101

No dissim.
Dissim.

0.20

0.15

0.10

0.05

0



100

102
(e) hinge in-sample

101

i

s
w
o
d
n
w
-
c
a
m

No dissim.
Dissim.



No dissim.
Dissim.

0.10

0.05

0.10

0.05

0



100

101

102

(b) hinge out-of-sample

0



100

102
(c) SE in-sample

101





No dissim.
Dissim.

0.10

0.05

0



100

102
(d) SE out-of-sample

101





0.20

0.15

0.10

0.05

0



No dissim.
Dissim.



100

101

102

0.20

0.15

0.10

0.05

0



No dissim.
Dissim.

No dissim.
Dissim.

0.20

0.15

0.10

0.05

(f) hinge out-of-sample

(g) SE in-sample

100

101

102

0



100

102
(h) SE out-of-sample

101

Figure 2: Changing the weight of dissimilarity edges (x-axis) in the g50c dataset (a-d) and the mac-windows
dataset (e-h). y-axis is mean error rate across 4 folds with 10 random trials each. Circled settings are statistically
signicantly better than the baseline.

use the standard supervised SVM and RLS as base-
lines, respectively. Also note that, unlike our exper-
iments with oracle edges, we are now including all
such dissimilarity edges, some of which connect labeled
and unlabeled examples. The only edges discarded are
those between two labeled examples. Our scheme is
realistic with noisy, real edges.

We used a graph of these dissimilarity edges to warp
a linear kernel used in SVM and RLS classication.
We set the labeled set size l = 50 (out of 184) and
ran 10 repeated trials with randomly selected labeled
examples. Out of the possible 103 dissimilarity edges
derived using the above heuristics, the trials included
an average of 93.4 edges (i.e., 9.6 labeled-labeled edges
are ignored). On average, 40.7 examples are involved
in the dissimilarity edges. Table 2 reports the mean
error rate on all unlabeled examples for SVM and RLS
classiers with (SSL) and without (Base) dissimi-
larity edges. The baseline results use unwarped linear
kernels. In both classiers, we observe a statistically
signicant reduction in error rate (p < 0.05 using a
two-tailed, paired t-test); it appears that the real-
world dissimilarity edges aid classication. However
upon closer inspection, we also notice the improvement
comes mostly from in-sample error reduction, and it
does not generalize as well to out-of-sample data like
in previous experiments. We suspect this could be due
to the high initial error rate.

Finally, as a post-experiment study, we investigated
how many of our heuristically derived dissimilarity
edges were actually consistent with the true labels. It
turns out that 85 out of the 103 edges (83%) are in
fact true dissimilarity edges. Thus, we have shown

Table 2: Mean error rates for SVM and RLS with
and without dissimilarity edges on the politics dataset.
Dissimilarity is incorporated through warped kernels.
Both dierences are statistically signicant.

Classier Base error rate
SVM
RLS

45.67  3.28
45.60  3.94

SSL error rate
40.15  4.95
37.99  1.88



5.5%
7.6%

that, even if 17% of the dissimilarity edges represent
false domain knowledge, we can achieve a signicant
improvement in overall error rate.

5 Conclusions

We presented a convex algorithm to encode dissimilar-
ity in semi-supervised learning. We demonstrated that
when such dissimilarity domain knowledge is available,
our algorithm can take advantage of it and improve
classication. The major advantage of our dissimi-
larity encoding formulations (2) and (7) is convexity.
However, they probably specify the relation between
the discriminant function f at dissimilarity samples
xi and xj more than necessary. For example in the
binary case we prefer f (xi) = f (xj), while ideally
it is sucient to require f (xi), f (xj) having opposite
signs. Finding computationally ecient encodings for
this sucient condition is a direction for future re-
search.

[13] Jurgen van Gael and Xiaojin Zhu. Correlation clus-
tering for crosslingual link detection. In International
Joint Conference on Articial Intelligence (IJCAI),
2007.

[14] Kiri Wagsta, Claire Cardie, Seth Rogers, and Stefan
Schrodl. Constrained k-means clustering with back-
ground knowledge.
In International Conference on
Machine Learning (ICML), page 577, 2001.

[15] Martin Wainwright, Tommi Jaakkola, and Alan Will-
sky. MAP estimation via agreement on (hyper)trees:
Message passing and linear-programming approaches.
IEEE Transactions on Information Theory, 51:3697
3717, 2005.

[16] Yair Weiss and William Freeman. On the optimality
of solutions of the max-product belief-propagation al-
gorithm in arbitrary graphs. IEEE Transactions on
Information Theory, 47, 2001.

[17] J. Weston and C. Watkins. Multi-class support vector
machines. Technical Report CSD-TR-98-04, Depart-
ment of Computer Science, Royal Holloway, Univer-
sity of London, 1998.

[18] Eric Xing, Andrew Ng, Michael Jordan, and Stuart
Russell. Distance metric learning with application to
clustering with side-information. In Advances in Neu-
ral Information Processing Systems (NIPS), 2002.

[19] Xiaojin Zhu.

Semi-supervised learning litera-
ture survey.
Technical Report 1530, Computer
Sciences, University of Wisconsin-Madison, 2005.
http://www.cs.wisc.edu/jerryzhu/pub/ssl survey.pdf.

[20] Xiaojin Zhu, Zoubin Ghahramani, and John Laerty.
Semi-supervised learning using Gaussian elds and
harmonic functions. In ICML-03, 20th International
Conference on Machine Learning, 2003.

[21] Xiaojin Zhu and Andrew B. Goldberg.

Semi-
