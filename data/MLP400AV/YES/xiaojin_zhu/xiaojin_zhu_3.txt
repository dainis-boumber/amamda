Abstract

Graph-based methods for semi-supervised learn-
ing have recently been shown to be promising for
combining labeled and unlabeled data in classi-
cation problems. However, inference for graph-
based methods often does not scale well to very
large data sets, since it requires inversion of a
large matrix or solution of a large linear program.
Moreover, such approaches are inherently trans-
ductive, giving predictions for only those points
in the unlabeled set, and not for an arbitrary test
point. In this paper a new approach is presented
that preserves the strengths of graph-based semi-
supervised learning while overcoming the lim-
itations of scalability and non-inductive infer-
ence, through a combination of generative mix-
ture models and discriminative regularization us-
ing the graph Laplacian. Experimental results
show that this approach preserves the accuracy of
purely graph-based transductive methods when
the data has manifold structure, and at the
same time achieves inductive learning with sig-
nicantly reduced computational cost.

1. Introduction

The availability of large data collections, with only limited
human annotation, has turned the attention of a growing
community of machine learning researchers to the problem
of semi-supervised learning. The broad research agenda
of semi-supervised learning is to develop methods that can
leverage a large amount of unlabeled data to build more
accurate classication algorithms than can be achieved us-
ing purely supervised learning. An attractive new family of
semi-supervised methods is based on the use of a graphi-
cal representation of the unlabeled dataexamples of this

Appearing in Proceedings of the 22 nd International Conference
on Machine Learning, Bonn, Germany, 2005. Copyright 2005 by
the author(s)/owner(s).

paradigm include the work of Blum and Chawla (2001);
Zhu et al. (2003); Zhou et al. (2004); Belkin et al. (2004a).

Many graph-based methods are inherently transductive in
nature: a graph is formed with vertices representing the la-
beled and unlabeled data, and a graph algorithm is used
to somehow separate the nodes according to the predicted
class labels. However, when a new data point is presented,
it is unclear how to make a predictionother than to re-
build the graph with the new test point and rerun the graph
algorithm from scratch. Since this may involve solving
a large linear program or inverting a huge matrix, these
procedures have limited generalization ability. Yet semi-
supervised methods should be most attractive when the un-
labeled data set is extremely large, and thus scalability be-
comes a central issue. In this paper we address the prob-
lems of scalability and non-inductive inference by combin-
ing parametric mixture models with graph-based methods.
Our approach is related to, but different from, the recent
work of Delalleau et al. (2005) and Belkin et al. (2004b).

The mixture model has long been recognized as a natural
approach to modeling unannotated data; indeed, some of
the earliest studies of the semi-supervised learning prob-
lem investigated the statistical or learning-theoretic ef-
ciency of estimating mixture models through a combina-
tion of labeled and unlabeled data (Castelli & Cover, 1996;
Ratsaby & Venkatesh, 1995). As a generative model, a
mixture is naturally inductive, and typically has a relatively
small number of parameters. Various applied studies sug-
gested that multinomial mixtures can be effective at us-
ing unlabeled data for classifying text documents (Nigam
et al., 2000), where the learning is typically carried out
using the EM algorithm to estimate the MAP model over
the unlabeled set. However, the anecdotal evidence is that
many more studies were not published because they ob-
tained negative results, showing that learning a mixture
model will often degrade the performance of a model t
using only the labeled data; one published study with these
conclusions is (Cozman et al., 2003). One of the reasons
for this phenomenon is that the data may have a manifold
structure that is incompatible with the generative mixture

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

model; thus EM may have difculty in making the labels
follow the manifold. An illustrative example is given in the
left plot of Figure 1. The desired behavior is shown in the
right plot, which is achieved using the harmonic mixture
model presented in this paper.

Mixture models and graph-based semi-supervised learning
methods make different assumptions about the relation be-
tween the data and the labelsbut these assumptions are
not mutually exclusive. It is possible that the data ts the
component model (a Gaussian, for example) locally, while
the manifold structure appears globally. The present work
attempts to combine the strengths of both approaches. We
show how the mixture model can be combined with the
graph to yield a much smaller backbone graph with nodes
induced from the mixture components. The number of mix-
ture components controls the size of the backbone graph,
leading to computationally efcient algorithms. The har-
monic mixture is a special case of our general framework,
where a harmonic function (Zhu et al., 2003) is induced
over the backbone graph to specify the class membership
of the mixture model. Since the mixture model is genera-
tive it handles new points, while the graph allows the labels
to follow the data manifold. Importantly, our procedure for
combining the mixture model with the backbone graph in-
volves a convex optimization problem.

After a brief overview of mixture models and previous
work on graph-based semi-supervised learning in Sec-
tion 2, we detail our combined approach in Section 3.
Experimental results for synthetic data, handwritten dig-
its recognition, image analysis and text categorization are
given in Section 4. The paper concludes with a discussion
and summary of the results.

2. Background and Notation

Let (xL, yL) = {(x1, y1) . . . (xl, yl)} be the labeled data.
For simplicity we consider binary classication, with y 
{1, 1}. Let xU = {xl+1 . . . xn} be the unlabeled data,
and u = n  l. The letters L and U will be used to repre-
sent the labeled and unlabeled data, respectively. In semi-
supervised learning the goal is to learn a classier from
both (xL, yL) and xU .

2.1. Mixture models

In the standard view of a mixture model for classication,
the generative process is to sample m  Mult(y) from
a multinomial depending on the class y, and to then sam-
ple x  G(m) for some generative model G. We will
work with a different, but equivalent view where a mixture
component m  Mult() is rst sampled from a multino-
mial model  over M outcomes, where M is the number of
mixture components. Then, the label y  Mult(m) and

features x  G(m) are generated (conditionally indepen-
dently) given m. Note that p(y | m) can take soft values
between 0 and 1, enabling classes to share a mixture com-
ponent. In unlabeled data, both the mixing component m
and class label y is latent for each example. The param-
eters of the mixture model are  = {(m, m, m)}M
m=1.
The EM algorithm is the standard procedure for estimat-
ing the parameters to maximize the incomplete likelihood

L() =QiPm,y p(xi | m) p(y | m) m. Combining the

labeled and unlabeled data together, the log likelihood is

`() = XiL
XiU

logXm
logXm

m myi p(xi | m) +

m p(xi | m)

(1)

2.2. Label smoothness on the data graph

Graph-based semi-supervised learning methods are based
on the principle that the label probability should vary
smoothly over the data graph. The graph has n nodes,
with two nodes connected by a (weighted) edge if they are
deemed similar according to some similarity function, cho-
sen by prior knowledge. The graph is thus represented by
the n  n symmetric weight matrix W ; the combinatorial
Laplacian is  = D  W , where the diagonal degree ma-

trix satises Dii =Pj wij.

Label smoothness can be expressed in different ways. We
adopt the energy used in semi-supervised learning by Zhu
et al. (2003), given by

E(f ) =

1
2

nXi,j=1

wij (fi  fj)2 = f >f

(2)

where f is the label posterior vector of a mixture model,
dened as

fi =(cid:26)

(yi, 1)
p(yi = 1 | xi, )

i  L
i  U

That is, fi is the probability that point i has label y = 1
under the mixture model ; since this is a function of the
parameters of the mixture, we will write it also as E().
The energy is small when f varies smoothly over the graph.
Zhu et al. (2003) proposed the use of the harmonic solution
f = 0 subject to the constraints specied by the labeled
data. Since  is the combinatorial Laplacian, this implies
that for i  U, fi is the average of the values of f at neigh-
boring nodes (the harmonic property), which is consistent
with the smoothness assumption. Alternative measures of
smoothness are based on the normalized Laplacian (Zhou
et al., 2004) or spectral transforms (Zhu et al., 2005), and
work similarly in the framework below.

Note that by extending the notion of a weighted graph to
allow self-transitions and possibly negative edge weights,

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

any symmetric matrix with zero row sums can be consid-
ered to be a graph Laplacian. We will make use of this
notion of a generalized Laplacian in the following section.

EM estimation. The second advantage is that, as we will
show below, O(, , ) is a convex function of .

3. Combining Mixture Models and Graph

Regularization

Our goal is to combine the mixture model and graph-based
learning algorithms. Intuitively, to enforce the smoothness
assumption encoded in the weight matrix W , we might reg-
ularize the mixture model by the energy of the posterior
with respect to the graph. This leads naturally to minimiza-
tion of the objective function

O() =  `() + (1  ) E()

(3)

This objective function makes explicit the tension between
maximizing the data likelihood and minimizing the graph
energy. The most direct way of proceeding is to estimate
parameters  = (, , ) to minimize the objective O()
where   [0, 1] is a coefcient that controls the relative
strength of the two terms.

Note that while the energy E(f ) may appear to be the log-
arithm of a prior p(f )  exp(f >f ), it in fact involves
the observed labels yL, since f is xed on the labeled data.
Thus, it is perhaps best thought of as a discriminative com-
ponent of the objective function, while `() is the genera-
tive component. In other words, optimizing O will carry
out a combination of discriminative and generative learn-
ing. This is closely related to, but different from, the graph
regularization framework of Belkin et al. (2004b).

Unfortunately, learning all of the parameters together is
difcultsince the energy E() is discriminative, EM
training is computationally demanding as the M-step does
not have a closed-form solution; moreover, it has the usual
drawback of local minima. We propose instead the follow-
ing two-step approach.

Generative/Graph-DiscriminativeTraining

Select the number of mixture components M, and initialize
the parameters  = (, , ).

1. Train the mixture model  using the objective function

O1() = `() with standard EM.

2. Fix  and , and reestimate the multinomial  to min-

imize O().

Figure2. Training combining `() and E().

Clearly this algorithm is suboptimal in terms of optimizing
the objective function. However it has two important ad-
vantages. One advantage is that the rst step is standard

3.1. Convexity of O

O involves `() and E(). First let us consider `(). In
(1) the sum over i  U is constant w.r.t. . The sum over
i  L can be written as

`L() = XiL, yi=1
XiL, yi=1

MXm=1
MXm=1

log

m m p(xi | m) +

log

m (1  m) p(xi | m)

Since we x  and , the term within the rst sum has the

form logPm amm. It can be directly veried that its Hes-

sian is negative-denite:

(cid:20)  logPm amm

ij

(cid:21) =

aa>

(Pm amm)2 (cid:22) 0

A similar calculation shows that the Hessian for the second
term is negative-denite as well. Thus `L() is concave in
, and `() is convex.

Next let us consider E(). Dene a u  M responsibility
matrix R by Rim = p(m | xi), depending on  and , with
Rm denoting the m-th column. We can write f U = R .
We partition  into labeled and unlabeled parts, with U U
being the submatrix on unlabeled points, LL on labeled
points and so on. The graph energy is written as

E() = f >f

= f >
= f >

LLLf L + 2f >
LLLf L + 2f >

LLU f U + f >
LLU R + >R>U U R

U U U f U

Since U U (cid:23) 0, the Hessian 2R>U U R (cid:23) 0 is positive
semi-denite in . Thus (1)E() is convex in . Putting
it together, O is convex in .

3.2. Special case:  = 0

The graph-discriminative training in step 2 has a very spe-
cial structure, which we now explain. We rst consider the
special case  = 0 and then the general case   [0, 1].

The case  = 0 has a particularly simple closed form
solution and interpretation. Notice that although  = 0,
the solution depends on the incomplete log-likelihood `()
through the choice of  and  learned in step 1.

The parameters  are constrained within [0, 1]M . However
rst let us consider  that minimize E in the unconstrained
problem. The solution to the linear system

E = R> (2U U R + 2U Lf L) = 0

(4)

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Standard EM

Harmonic mixture

Figure1.Predictions of Gaussian mixture models learned with the standard EM algorithm do not follow the manifold structure. Small
dots are unlabeled data. Two labeled points, marked with a white + and a black box, are at roughly the ends of the spirals. Each plot
shows a Gaussian mixture model with M = 36 components, with the ellipses showing contours of the covariance matrices. The central
dots have sizes proportional to the component weight p(m) = m (tiny components are not plotted), and its brightness indicates the
strength of class membership, given by m  p(y = 1 | m): white denotes m = 1, black denotes m = 0, and intermediate gray
denotes values in between. Although the density p(x) is estimated well by the standard mixture t using EM (left),  does not follow
the data manifold. The right plot shows the harmonic mixture, where  is ret to be harmonic on the backbone graph.

is given by

? =  (R>U U R)1 R>U Lf L

(5)

Note that, in general, the constraints 0  m  1 must
also be explicitly enforced. If the above solution lies in the
interior of the hypercube [0, 1]M then it must also be the
solution of the constrained problem.1 In this case, (5) de-
termines the class membership probabilities for each mix-
ture component  the soft label for the unlabeled data is
given by f U = R. Previously unseen test points can be
classied similarly.

Compare the solution (5), which we will refer to as the har-
monic mixture, with the completely graph-based harmonic
function solution (Zhu et al., 2003):

harmonic:
harmonic
mixture:

U U U Lf L

f U = 1
f U = R (R>U U R)1 R>U Lf L

Computationally, obtaining the harmonic mixture requires
the inversion of an M  M matrix, or if the solution lies on

1More generally, the Karush-Kuhn-Tucker optimality condi-
tions imply that the harmonic mixture can be expressed as ? =
 `R>U U R1 `R>U Lf L + , where  is a vector of La-
grange multipliers. Geometrically, this can be viewed as the solu-
tion of an inhomogeneous Dirichlet boundary value problem for
a generalized Laplacian. Computationally if some m are out of
bounds, we clip them as the starting point for constrained convex
optimization, which converges quickly. Pseudo inverse is used if
R is rank decient.

the boundary solving the associated constrained optimiza-
tion problem. Solving the system (4) will be much less
computationally expensive than the u  u matrix inversion
required by harmonic solution, when the number of mix-
ture components M is much smaller than the number of
unlabeled points u. This reduction is possible because the
f U are now obtained by marginalizing the mixture model.

3.3. Graphical interpretation

The procedure just described can be interpreted graphically
in terms of a much smaller backbone graph with supern-
odes induced by the mixture components. The backbone
graph has the same l labeled nodes as in the original graph,
but only M unlabeled supernodes.

By rearranging terms it is not hard to show that in the back-
bone graph, the generalized Laplacian is

0 R(cid:19)>(cid:18)LL LU

U L U U(cid:19)(cid:18)I

0

e = (cid:18)I

0 R(cid:19) (6)
Note that e has zero row sums. The harmonic mixture

parameter  is then a harmonic function on the generalized
Laplacian. The harmonic mixture algorithm is summarized
in Figure 3.

0

Perhaps the best intuition for the backbone graph comes
from considering hard clustering. In this case Rim = 1 if
m is the cluster to which point i belongs, and Rim = 0

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

HarmonicMixtureTraining:  = 0

4. Experiments

Select the number of mixture components M, initialize the
parameters  = (, , ), and form the graph Laplacian .

1. Run standard EM to obtain , , and .

3. Fixing  and , compute

2. Form the generalized Laplacian e, from (6).
? = arg min >eU U  + 2>eU LfL

Output: Harmonic mixture model  = (, ?, )

Figure3. The harmonic mixture algorithm,  = 0

otherwise. Let c(m) = {i | Rim = 1} denote cluster m.
In this case the supernodes are the clusters themselves. Let
wij be the weight between nodes i, j in the original graph.
The equivalent weight between supernodes (s, t) reduces

between a supernode s and a labeled node j  L is wsj =

to wst = Pic(s), jc(t) wij; and the equivalent weight
Pic(s) wij. In this case the solution (5) is also guaranteed

to satisfy the constraints. One can create such a backbone
graph using, for instance, k-means clustering.

3.4. General case:  > 0

In the general case of  > 0 step 2 does not have a closed-
form solution. As m must lie in the interval [0, 1], we
perform constrained convex optimization in this case. The
gradient of the objective function is easily computed. Note
that

`()
m

= XiL, yi=1
PM
XiL, yi=1
PM

m p(xi | m)
k=1 k p(xi | k)k
m p(xi | m)



k=1 k p(xi | k) (1  k)

and E/ was given in (4). One can also use sigmoid
function to transform it into an unconstrained optimization
problem with m = (m) = 1/ (exp(m) + 1) and
optimize the  parameters.

Although the objective function is convex, a good starting
point for  is important for fast convergence. We select an
initial value for  by solving a one dimensional problem
rst. We have two parameters at hand: EM, the solution
from the standard EM algorithm in step 1, and HM, the
harmonic mixture solution from the special case  = 0.
We nd the optimal interpolated coefcient   [0, 1] 0 =
 EM + (1  ) HM that minimizes the objective function.
Then, we start from 0 and use a quasi-Newton algorithm
to nd the global optimum for .

We test harmonic mixtures on synthetic data, handwrit-
ten digits, image analysis and text categorization tasks.
The emphases are on how the harmonic mixtures (denoted
HM below) perform against several baseline methods on
unlabeled data; how they handle unseen data; and whether
they can reduce the problem size. Unless otherwise noted,
the harmonic mixtures are computed with  = 0.

We use three baseline methods: sampling unlabeled data
to create a smaller graph (sample), mixture models con-
structed with the standard EM algorithm (EM), and har-
monic functions on the original large graphs (graph). In
sample, we randomly draw M unlabeled points from
U. We create a small (size l + M) graph with these
and the labeled points, and compute the harmonic func-
tion fi on the small graph rst. The graph computa-
tion cost is thus the same as HM. Then as in (Delalleau
et al., 2005), we compute the labels for other points j by

fj = (Pl+M

i=1 wijfi)/(Pl+M

i=1 wij).

4.1. Synthetic Data

First let us look at the synthetic dataset in Figure 1. It has a
Swiss roll structure, and we hope the labels can follow the
spiral arms. There is one positive and one negative labeled
point, at roughly the opposite ends. We use u = 766 un-
labeled points and an additional 384 points as unseen test
data.

The mixture model and standard EM. To illustrate the
idea, consider a Gaussian mixture model (GMM) with
M = 36 components, each with full covariance. The left
panel shows the converged GMM after running EM. The
GMM models the manifold density p(x) well. However the
component class membership m  p(y = 1 | m) (bright-
ness of the central dots) does not follow the manifold. In
fact  takes the extreme values of 0 or 1 along a somewhat
linear boundary instead of following the spiral arms, which
is undesirable. The classication of data points will not
follow the manifold either.

The graph and harmonic mixtures. Next we combine the
mixture model with a graph to compute the harmonic mix-
tures, as in the special case  = 0. We construct a fully
connected graph on the L  U data points with weighted

edges wij = exp(cid:0)||xi  xj||2/0.01(cid:1). The weight pa-

rameters in all experiments are selected with 5-fold cross
validation. We then reestimate , which are shown in the
right panel of Figure 1. Note  now follow the manifold as
it changes from 0 (black) to approximately 0.5 (gray) and
nally 1 (white). This is the desired behavior.

The particular graph-based method we use needs extra care.
The harmonic function solution f is known to sometimes

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

skew toward 0 or 1. This problem is easily corrected if we
know or have an estimate of the proportion of positive and
negative points, with the Class Mass Normalization heuris-
tic (Zhu et al., 2003). In this paper we use a similar but sim-
pler heuristic. Assuming the two classes are about equal in
size, we simply set the decision boundary at median(f ).

Sensitivity to M. If the number of mixture components M
is too small, the GMM is unable to model p(x) well, let
alone . In other words, the harmonic mixture is sensitive
to M. M has to be larger than a certain threshold so that the
manifold structure can appear. In fact M may need to be
larger than the number of labeled points l, which is unusual
in traditional mixture model methods for semi-supervised
learning. But once M is over the threshold, further increase
should not dramatically change the solution.
In the end
the harmonic mixture may approach the harmonic function
solution when M = u.

Figure 4(top left) shows the classication accuracies on U
as we change M.
graph is the ideal performance. We
nd that HM threshold is around M = 35, at which point
the accuracy jumps up and stabilizes thereafter. This is the
number of mixture components needed for HM to capture
the manifold structure. sample needs far more samples
(M > 400, not shown) to reach 95% accuracy. EM fails
to make the labels to follow the manifold structure regard-
less of the number of mixtures.

Computational savings. HM performs almost as good
as graph but with a much smaller problem size. As Fig-
ure 4(left) shows we only need to invert a 35  35 matrix,
instead of a 766  766 one as required by graph. The dif-
ference can be signicant if U is even larger. There is of
course the overhead of EM training.

Handling unseen data. Because HM is a mixture model,
it naturally handles unseen points. On 384 new test points
HM performs well, with accuracy 95.3% after M  35 as
shown in Figure 4(bottom left). Note graph cannot handle
unseen data and is therefore not shown in the plot.

4.2. Handwritten Digits Recognition

We use the 1vs2 dataset which contains handwritten dig-
its of 1s and 2s. Each gray scale image is 8  8, which
is represented by a 64 dimensional vector of pixel values.
We use l + u = 1600 images as the labeled and unlabeled
set, and 600 additional images as unseen new data to test
induction. The total numbers of 1s and 2s are the same.

The mixture model. We use Gaussian mixture models. To
avoid data sparseness problem, we model each Gaussian
component with a spherical covariance, i.e. diagonal co-
variance matrix with the same variance in all dimensions.
Different components may have different variances. We set
the initial means and variances of the GMM with k-means

algorithm before running EM.

The graph. We use a symmetrized 10-nearest-neighbor
weighted graph on the 1600 images. That is, images i, j
are connected if i is within js 10NN or vice versa, as
measured by Euclidean distance. The weights are wij =

exp(cid:0)||xi  xj||2/1402(cid:1).

Sensitivity to M. As illustrated in the synthetic data,
the number of mixture components M needs to be large
enough for harmonic mixture to work. We vary M and
observe the classication accuracies on the unlabeled data
with different methods. For each M we perform 20 trials
with random L/U split, and plot the mean of classication
accuracies on U in Figure 4(top center). The experiments
were performed with labeled set size xed at l = 10. We
conclude that HM needs only M  150 components to
match the performance of graph. HM outperforms both
sample and EM.

Computational savings. In terms of graph method compu-
tation, we invert a 150  150 matrix instead of the original
1590  1590 matrix for harmonic function. This is good
saving with little sacrice in accuracy.

Handling unseen data. On 600 unseen data points (Figure
4 bottom center), HM is better than sample and EM
too.

The general case  > 0. We also vary the parameter 
between 0 and 1, which balances the generative and dis-
criminative objectives. In our experiments  = 0 always
gives the best accuracies.

4.3. Teapots Image Analysis

We perform binary classication on the Teapots dataset,
which was previously used for dimensionality reduction.
See (Weinberger et al., 2004) for details. The dataset con-
sists of a series of teapot photos, each rotated by a small
angles. Our task is to identify whether the spout points
to the left or the right. Excluding the few images from the
original dataset in which the spout is roughly in the middle,
we arrive at 365 images. We process each image by con-
verting it to gray scale and down-sizing it to 12  16. Each
image is thus represented by a 192-dimensional vector of
pixel values. Nonetheless we believe the dataset resides on
a much lower dimensional manifold, since image pairs in
which the teapot rotates by a small angle are close to each
other. Therefore we expect graph-based semi-supervised
learning methods to perform well on the dataset. We use
273 images as L  U, and the remaining 92 as unseen test
data.

The mixture model. We again use Gaussian mixture mod-
els with spherical covariances. We initialize the models
with k-means before running EM.

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

synthetic data

1 vs. 2

teapots

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55



U
n
o


y
c
a
r
u
c
c
A

1

0.95

0.9

0.85

0.8

0.75

U

n
o

y
c
a
r
u
c
c
A

graph
HM
sample
EM

1

0.9

0.8

0.7

0.6

0.5

0.4



U
n
o


y
c
a
r
u
c
c
A

graph
HM
sample
EM

graph
HM
sample
EM

0.5

0

5

10

15

20

25
M

30

35

40

45

50

0.7

0

20

40

60

80

120

140

160

180

200

100
M

0.3

0

5

10

15

20
M

25

30

35

40

Accuracies on unlabeled training data U

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

n
e
e
s
n
u
n
o




y
c
a
r
u
c
c
A

1

0.95

0.9

0.85

0.8

0.75

n
e
e
s
n
u

n
o

y
c
a
r
u
c
c
A

HM
sample
EM

HM
sample
EM

1

0.9

0.8

0.7

0.6

0.5

0.4

n
e
e
s
n
u



n
o


y
c
a
r
u
c
c
A

HM
sample
EM

0.5

0

5

10

15

20

25
M

30

35

40

45

50

0.7

0

20

40

60

80

120

140

160

180

200

100
M

0.3

0

5

10

15

20
M

25

30

35

40

Accuracies on unseen test data (induction)

Figure4.Sensitivity to M in the synthetic data (left), 1 vs. 2 (center) and teapots (right). Shown are the classication accuracies on U
(top row) and unseen new data (bottom row) as M changes. graph is the harmonic function on the complete L  U graph; HM is the
harmonic mixture, sample is the smaller graph with sampled unlabeled data, and EM is the standard EM algorithm. Note y-axes are
not on the same scale.

The graph. We use a symmetrized 10NN weighted
graph on the 273 images, with weights wij =

exp(cid:0)||xi  xj||2/1002(cid:1).

Sensitivity to M. The classication accuracies on U with
different number of components M is shown in Figure 4
(top right). Each curve is the average of 20 trials.
l is
xed at (merely) 2. With M > 15 components, HM ap-
proaches the graph performance. sample and EM are
clearly worse.

Computational savings. The graph computation for HM
inverts a 15  15 matrix, which is much cheaper than 271 
271 for graph.

Handling unseen data. HM performs similarly on the 92
unseen images (Figure 4 bottom right), achieving high ac-
curacy with small M, and outperforms sample and EM.

4.4. Text Categorization: A Discussion

We also perform text categorization on the PC vs. Mac
groups from the 20-newsgroups data. Of the 1943 docu-
ments, we use 1600 as LU and the rest as unseen test data.
We use a symmetrized 10NN weighted graph on the 1600
documents with weight wuv = exp ((1  cuv)/0.03),
where cuv is the cosine between the tf.idf document vec-

tors u, v. With l = 10, graph accuracy is around 90%. We
use multinomial mixture models on documents. However
unlike other tasks, HM suffers from a loss of transductive
accuracy, and only reaches 83% accuracy on U and unseen
data. It does so with an undesirably large M around 600.
Furthermore HM and sample perform about the same
(though both are better than EM).

Why does HM perform well on other tasks but not on text
categorization? We think the reasons are: 1) The mani-
fold assumption needs to hold strongly. For instance in the
synthetic and the Teapots data the manifolds are evident,
and HM achieved close approximations to graph. The
text data seems to have a weaker manifold structure. 2)
On top of that, the text data has a very high dimensionality
(D = 12008). The curse of dimensionality may prevent a
generative mixture model from tting the manifold well. In
addition the multinomial model may not be appropriate for
creating localized supernodes.

Interestingly we do not have to use generative models. If
we work with  = 0, all we need from the mixture model is
the responsibility R. One can instead rst use simple pro-
totype methods like k-means to cluster the data, and then
train discriminative models to obtain R. This remains a
future research direction.

Harmonic mixtures: combining mixture models and graph-based methods for inductive and scalable semi-supervised learning

5. Related Work

Recently Delalleau et al. (2005) used a small subset of the
unlabeled data to create a small graph for semi-supervised
learning. This is related to the Nystrom method in spec-
tral clustering (Fowlkes et al., 2004), and to the random
landmarks in dimensionality reduction (Weinberger et al.,
2005). Our method is different in that it incorporates a gen-
erative mixture model, which is a second knowledge source
besides the original graph. Our method outperforms ran-
dom subset selection, and can be viewed as a principled
way to carry out the elaborate subset selection heuristics in
(Delalleau et al., 2005).

In terms of handling unseen data, our approach is closely
related to the regularization framework of Belkin et al.
(2004b); Krishnapuram et al. (2005) as graph regulariza-
tion on mixture models. But instead of a regularization
term we used a discriminative term, which allows for the
closed form solution in the special case.

6. Summary

To summarize, our proposed harmonic mixture method
reduces the graph problem size, and handles unseen test
points. It achieves comparable accuracy as the harmonic
function on complete graph for semi-supervised learning.

There are some open questions. One is when  > 0 would
be useful in practice. Another is whether we can use fast
prototype methods instead of EM. Finally, we want to au-
tomatically select the appropriate number of mixture com-
ponents M.

Acknowledgments

We thank the reviewers for their useful comments, and Guy
Lebanon and Lillian Lee for interesting discussions.

