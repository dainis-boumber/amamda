Abstract

Internal models of the environment have an important role to play in adap-
tive systems in general and are of particular importance for the supervised
learning paradigm. In this paper we demonstrate that certain classical prob-
lems associated with the notion of the teacher" in supervised learning can be
solved by judicious use of learned internal models as components of the adap-
tive system. In particular, we show how supervised learning algorithms can be
utilized in cases in which an unknown dynamical system intervenes between
actions and desired outcomes. Our approach applies to any supervised learning
algorithm that is capable of learning in multi-layer networks.

*This paper is a revised version of MIT Center for Cognitive Science Occasional
Paper . We wish to thank Michael Mozer, Andrew Barto, Robert Jacobs, Eric
Loeb, and James McClelland for helpful comments on the manuscript. This project
was supported in part by BRSG  S RR- awarded by the Biomedical Re-
search Support Grant Program, Division of Research Resources, National Institutes
of Health, by a grant from ATR Auditory and Visual Perception Research Labora-
tories, by a grant from Siemens Corporation, by a grant from the Human Frontier
Science Program, and by grant N-	-J-	 awarded by the Oce of Naval
Research.



Recent work on learning algorithms for connectionist networks has seen a pro-
gressive weakening of the assumptions made about the relationship between the
learner and the environment. Classical supervised learning algorithms such as the
perceptron Rosenblatt, 	 and the LMS algorithm Widrow & Ho, 	 made
two strong assumptions:  The output units are the only adaptive units in the
network, and  there is a teacher" that provides desired states for all of the
output units. Early in the development of such algorithms it was recognized that
more powerful supervised learning algorithms could be realized by weakening the
rst assumption and incorporating internal units that adaptively recode the input
representation provided by the environment Rosenblatt, 	. The subsequent
development of algorithms such as Boltzmann learning Hinton & Sejnowski, 	
and backpropagation LeCun, 	; Parker, 	; Rumelhart, Hinton, & Williams,
	; Werbos, 	 have provided the means for training networks with adaptive
nonlinear internal units. The second assumption has also been weakened|learning
algorithms that require no explicit teacher have been developed Becker & Hinton,
		; Grossberg, 	; Kohonen, 	; Linsker, 	; Rumelhart & Zipser, 	.
Such unsupervised" learning algorithms generally perform some sort of clustering
or feature extraction on the input data and are based on assumptions about the
statistical or topological properties of the input ensemble.

In this paper we examine in some detail the notion of the teacher" in the
supervised learning paradigm. We argue that the teacher is less of a liability than
has commonly been assumed and that the assumption that the environment provides
desired states for the output of the network can be weakened signicantly without
abandoning the supervised learning paradigm altogether. Indeed, we feel that an
appropriate interpretation of the role of the teacher is crucial in appreciating the
range of problems to which the paradigm can be applied.

The issue that we wish to address is best illustrated by way of an example.
Consider a skill-learning task such as that faced by a basketball player learning
to shoot baskets. The problem for the learner is to nd the appropriate muscle
commands to propel the ball toward the goal. Dierent commands are appropriate
for dierent locations of the goal in the visual scene; thus, a mapping from visual
scenes to muscle commands is required. What learning algorithm might underly
the acquisition of such a mapping? Clearly, clustering or feature extraction on the
visual input is not sucient. Moreover, it is dicult to see how to apply classical
supervised algorithms to this problem, because there is no teacher to provide muscle
commands as targets to the learner. The only target information provided to the
learner is in terms of the outcome of the movement; that is, the sights and sounds
of a ball passing through the goal.

The general scenario suggested by the example is shown in Figure . Intentions
are provided as inputs to the learning system. The learner transforms intentions
into actions, which are transformed by the environment into outcomes. Actions
are proximal variables; that is, variables that the learner controls directly, while



intention

Learner

action

Environment

outcome

Figure : The distal supervised learning problem. Target values are available for the
distal variables the outcomes" but not for the proximal variables the actions".

outcomes are distal variables, variables that the learner controls indirectly through
the intermediary of the proximal variables. During the learning process, target
values are assumed to be available for the distal variables but not for the proximal
variables. Therefore, from a point of view outside the learning system, a distal
supervised learning task" is a mapping from intentions to desired outcomes. From
the point of view of the learner, however, the problem is to nd a mapping from
intentions to actions that can be composed with the environment to yield desired
distal outcomes. The learner must discover how to vary the components of the
proximal action vector so as to minimize the components of the distal error.

The distal supervised learning problem also has a temporal component. In many
environments the eects of actions are not punctate and instantaneous, but rather
linger on and mix with the eects of other actions. Thus the outcome at any point
in time is inuenced by any of a number of previous actions. Even if there exists a
set of variables that have a static relationship to desired outcomes, the learner often
does not have direct control over those variables. Consider again the example of
the basketball player. Although the ight of the ball depends only on the velocity
of the arm at the moment of releasea static relationshipit is unlikely that the
motor control system is able to control release velocity directly. Rather, the system
outputs forces or torques, and these variables do not have a static relationship to
the distal outcome.

In the remainder of the paper we describe a general approach to solving the dis-
tal supervised learning problem. The approach is based on the idea that supervised
learning in its most general form is a two-phase procedure. In the rst phase the
learner forms a predictive internal model a forward model of the transformation
from actions to distal outcomes. Because such transformations are often not known
a priori, the internal model must generally be learned by exploring the outcomes
associated with particular choices of actions. This auxiliary learning problem is it-
self a supervised learning problem, based on the error between internal, predicted
outcomes and actual outcomes. Once the internal model has been at least par-
tially learned, it can be used in an indirect manner to solve for the mapping from



intentions to actions.

The idea of using an internal model to augment the capabilities of supervised
learning algorithms has also been proposed by Werbos 	, although his perspec-
tive diers in certain respects from our own. There have been a number of further
developments of the idea Kawato, 		; Miyata, 	; Munro, 	; Nguyen &
Widrow, 		; Robinson & Fallside, 		; Schmidhuber, 		, based either on the
work of Werbos or our own unpublished work Jordan, 	; Rumelhart, 	.
There are also close ties between our approach and techniques in optimal control
theory Kirk, 	 and adaptive control theory Goodwin & Sin, 	; Narendra
& Parthasarathy, 		. We discuss several of these relationships in the remainder
of the paper, although we do not attempt to be comprehensive.

Distal supervised learning and forward models

This section and the following section present a general approach to solving distal
supervised learning problems. We begin by describing our assumptions about the
environment and the learner.

We assume that the environment can be characterized by a next-state function
f and an output function g. At time step n (cid:0)  the learner produces an action
un (cid:0) .
In conjunction with the state of the environment xn (cid:0)  the action
determines the next state xn:

xn = f xn (cid:0) ; un (cid:0) :

Corresponding to each state xn there is also a sensation yn:

yn = gxn:





Note that sensations are output vectors in the current formalism|outcomes" in
the language of the introductory section. The next-state function and the output
function together determine a state-dependent mapping from actions to sensations.
In the current paper we assume that the learner has access to the state of the
environment; we do not address issues relating to state representation and state
estimation. State representations might involve delayed values of previous actions
and sensations Ljung & Soderstrom, 	, or they might involve internal state
variables that are induced as part of the learning procedure Mozer & Bachrach,
		. Given the state xn (cid:0)  and given the input pn (cid:0) , the learner produces
an action un (cid:0) :

un (cid:0)  = hxn (cid:0) ; pn (cid:0) :





The choice of time indices in Equations , , and  is based on our focus on the output at time
n. In our framework a learning algorithm alters yn based on previous values of the states, inputs,
and actions.



p [n-1]

Learner

x [n-1]

u [n-1]

Environment

y [n]

Figure : The composite performance system consisting of the learner and the
environment. This system is a mapping from inputs pn (cid:0)  to sensations yn.
The training data fpin (cid:0) ; y
i ng specify desired inputoutput behavior across
the composite system. Note that there is an implicit loop within the environment
such that the output at time n depends on the state at time n (cid:0)  cf. Equation .

A distal supervised learning problem is a set of training pairs fpin (cid:0) ; y

The goal of the learning procedure is to make appropriate adjustments to the input-
to-action mapping h based on data obtained from interacting with the environment.
i ng,
where pin (cid:0)  are the input vectors and y
i n are the corresponding desired sen-
sations. For example, in the basketball problem, the input might be a high-level
intention of shooting a basket, and a desired sensation would be the correspond-
ing visual representation of a successful outcome. Note that the distal supervised
learning problem makes no mention of the actions that the learner must acquire;
only inputs and desired sensations are specied. From a point of view outside the
learning system the training data specify desired inputoutput behavior across the
composite performance system consisting of the learner and the environment see
Figure . From the point of view of the learner, however, the problem is to nd
a mapping from inputs pn (cid:0)  to actions un (cid:0)  such that the resulting distal
sensations yn are the target values yn. That is, the learner must nd a mapping
from inputs to actions that can be placed in series with the environment so as to
yield the desired pairing of inputs and sensations. Note that there may be more
than one action that yields a given desired sensation from any given state; that is,
the distal supervised learning problem may be underdetermined. Thus, in the bas-
ketball example, there may be a variety of patterns of motor commands that yield
the same desired sensation of seeing of the ball pass through the goal.

Forward models

The learner is assumed to be able to observe states, actions, and sensations and can
therefore model the mapping between actions and sensations. A forward model is an
internal model that produces a predicted sensation ^yn based on the state xn (cid:0) 
and the action un (cid:0) . That is, a forward model predicts the consequences of a



u [n-1]

Environment

y [n]

x

[n-1]

+
_

^
y [n]

Forward
Model

Figure : Learning the forward model using the prediction error yn (cid:0) ^yn.

given action in the context of a given state vector. As shown in Figure , the forward
model can be learned by comparing predicted sensations to actual sensations and
using the resulting prediction error to adjust the parameters of the model. Learning
the forward model is a classical supervised learning problem in which the teacher
provides target values directly in the output coordinate system of the learner.

Distal supervised learning

We now describe a general approach to solving the distal supervised learning prob-
lem. Consider the system shown in Figure , in which the learner is placed in series
with a forward model of the environment. This composite learning system is a state-
dependent mapping from inputs to predicted sensations. Suppose that the forward
model has been trained previously and is a perfect model of the environment; that
is, the predicted sensation equals the actual sensation for all actions and all states.

In the engineering literature, this learning process is referred to as system identication"

Ljung & Soderstrom, 	.



x [n-1]

p [n-1]

Learner

u [n-1]

Forward
Model

^
y [n]

Figure : The composite learning system. This composite system maps from inputs
pn (cid:0)  to predicted sensations ^yn in the context of a given state vector.

We now treat the composite learning system as a single supervised learning system
and train it to map from inputs to desired sensations according to the data in the
training set. That is, the desired sensations y
i are treated as targets for the compos-
ite system. Any supervised learning algorithm can be used for this training process;
however, the algorithm must be constrained so that it does not alter the forward
model while the composite system is being trained. By xing the forward model,
we require the system to nd an optimal composite mapping by varying only the
mapping from inputs to actions. If the forward model is perfect, and if the learning
algorithm nds the globally optimal solution, then the resulting state-dependent
input-to-action mapping must also be perfect in the sense that it yields the desired
composite inputoutput behavior when placed in series with the environment.

Consider now the case of an imperfect forward model. Clearly an imperfect
forward model will yield an imperfect input-to-action map if the composite system
is trained in the obvious way, using the dierence between the desired sensation and
the predicted sensation as the error term. This dierence, the predicted performance
error y (cid:0) ^y, is readily available at the output of the composite system, but it
is an unreliable guide to the true performance of the learner. Suppose instead that
we ignore the output of the composite system and substitute the performance error
y (cid:0) y as the error term for training the composite system see Figure . If the
performance error goes to zero the system has found a correct input-to-action map,
regardless of the inaccuracy of the forward model. The inaccuracy in the forward
model manifests itself as a bias during the learning process, but need not prevent
the performance error from going to zero. Consider, for example, algorithms based
on steepest descent. If the forward model is not too inaccurate the system can still
move downhill and thereby reach the solution region, even though the movement is
not in the direction of steepest descent.

To summarize, we propose to solve the distal supervised learning problem by
training a composite learning system consisting of the learner and a forward model
of the environment. This procedure solves implicitly for an input-to-action map by



x [n-1]

y *[n] _

y [n]

p [n-1]

Learner

u [n-1]

Forward
Model

Figure : The composite system is trained using the performance error. The forward
model is held xed while the composite system is being trained.

training the composite system to map from inputs to distal targets. The training
of the forward model must precede the training of the composite system, but the
forward model need not be perfect, nor need it be pre-trained throughout all of state
space. The ability of the system to utilize an inaccurate forward model is important;
it implies that it may be possible to interleave the training of the forward model
and the composite system.

In the remainder of the paper, we discuss the issues of interleaved training,
inaccuracy in the forward model, and the choice of the error term in more detail.
We rst turn to an interesting special case of the general distal supervised learning
problem|that of learning an inverse model of the environment.

Inverse models

An inverse model is an internal model that produces an action un (cid:0)  as a function
of the current state xn (cid:0)  and the desired sensation yn. Inverse models are
dened by the condition that they yield the identity mapping when placed in series
with the environment.

Inverse models are important in a variety of domains. For example, if the en-
vironment is viewed as a communications channel over which a message is to be
transmitted, then it may be desirable to undo the distorting eects of the envi-
ronment by placing it in series with an inverse model Carlson, 	. A second
example, shown in Figure , arises in control system design. A controller receives the
desired sensation yn as input and must nd actions that cause actual sensations
to be as close as possible to desired sensations; that is, the controller must invert



y *[n]

Inverse
Model

x [n-1]

u [n-1]

Environment

y [n]

Figure : An inverse model as a controller.

the transformation from actions to sensations. One approach to achieving this
objective is to utilize an explicit inverse model of the environment as a controller.

Whereas forward models are uniquely determined by the environment, inverse
models are generally not.
If the environment is characterized by a many-to-one
mapping from actions to sensations then there are generally an innite number of
possible inverse models. It is also worth noting that inverses do not always exist|
it is not always possible to achieve a particular desired sensation from any given
state. As we shall discuss, these issues of existence and uniqueness have important
implications for the problem of learning an inverse model.

There are two general approaches to learning inverse models using supervised
learning algorithms: the distal learning approach presented above and an alternative
approach that we refer to as direct inverse modeling" cf. Jordan & Rosenbaum,
		. We begin by describing the latter approach.

Direct inverse modeling

Direct inverse modeling treats the problem of learning an inverse model as a classical
supervised learning problem Widrow & Stearns, 	. As shown in Figure , the
idea is to observe the inputoutput behavior of the environment and to train an
inverse model directly by reversing the roles of the inputs and outputs. Data are
provided to the algorithm by sampling in action space and observing the results in
sensation space.

Although direct inverse modeling has been shown to be a viable technique in
a number of domains Atkeson & Reinkensmeyer, 	; Kuperstein, 	; Miller,
	, it has two drawbacks that limit its usefulness. First, if the environment is
characterized by a many-to-one mapping from actions to sensations, then the di-
rect inverse modeling technique may be unable to nd an inverse. The diculty is
that nonlinear many-to-one mappings can yield nonconvex inverse images, which are

Control system design normally involves a number of additional constraints involving stability
and robustness; thus, the goal is generally to invert the environment as nearly as possible subject
to these additional constraints.



u [n-1]

Environment

y [n]

x

[n-1]

+

_

Inverse
Model

Figure : The direct inverse modeling approach to learning an inverse model.

problematic for direct inverse modeling. Consider the situation shown in Figure .
The nonconvex region on the left is the inverse image of a point in sensation space.
Suppose that the points labelled by Xs are sampled during the learning process.
Three of these points correspond to the same sensation; thus, the training data as
seen by the direct inverse modeling procedure are one-to-many|one input is paired
with many targets. Supervised learning algorithms resolve one-to-many inconsisten-
cies by averaging across the multiple targets the form of the averaging depends on
the particular cost function that is used. As is shown in the gure, however, the
average of points lying in a nonconvex set does not necessarily lie in the set. Thus
the globally optimal minimum-cost solution found by the direct inverse modeling
approach is not necessarily a correct inverse model. We present an example of such
behavior in a following section.

The second drawback with direct inverse modeling is that it is not goal-
directed." The algorithm samples in action space without regard to particular
targets or errors in sensation space. That is, there is no direct way to nd an
action that corresponds to a particular desired sensation. To obtain particular so-
lutions the learner must sample over a suciently wide range of actions and rely on
interpolation.

Finally, it is also important to emphasize that direct inverse modeling is re-
stricted to the learning of inverse models|it is not applicable to the general distal

A set is convex if for every pair of points in the set all points on the line between the points

also lie in the set.



.

Action space

Sensation space

Figure : The convexity problem. The region on the left is the inverse image of
the point on the right. The arrow represents the direction in which the mapping is
learned by direct inverse modeling. The three points lying inside the inverse image
are averaged by the learning procedure, yielding the vector represented by the small
circle. This point is not a solution, because the inverse image is not convex.

supervised learning problem.

The distal learning approach to learning an inverse model

The methods described earlier in this section are directly applicable to the problem
of learning an inverse model. The problem of learning an inverse model can be
treated as a special case of the distal supervised learning problem in which the
input vector and the desired sensation are the same that is, pn (cid:0)  is equal to
yn in Equation . Thus, an inverse model is learned by placing the learner and
the forward model in series and learning an identity mapping across the composite
system.

A fundamental dierence between the distal learning approach and direct in-
verse modeling approach is that rather than averaging over regions in action space,
the distal learning approach nds particular solutions in action space. The globally
optimal solution for distal learning is a set of vectors fuig such that the performance

An interesting analogy can be drawn between the distal learning approach and indirect tech-
niques for solving systems of linear equations.
In numerical linear algebra, rather than solving
explicitly for a generalized inverse of the coecient matrix, solutions are generally found indirectly
e.g., by applying Gaussian elimination to both sides of the equation GA = I, where I is the identity
matrix.



i (cid:0) yig are zero. This is true irrespective of the shapes of the inverse im-
errors fy
i . Vectors lying outside of an inverse image, such as the average
ages of the targets y
vector shown in Figure , do not yield zero performance error and are therefore not
globally optimal. Thus nonconvex inverse images do not present the same funda-
mental diculties for the distal learning framework as they do for direct inverse
modeling.

It is also true that the distal learning approach is fundamentally goal-directed.
The system works to minimize the performance error; thus, it works directly to nd
solutions that correspond to the particular goals at hand.

In cases in which the forward mapping is many-to-one, the distal learning pro-
cedure nds a particular inverse model. Without additional information about the
particular structure of the input-to-action mapping there is no way of predicting
which of the possibly innite set of inverse models the procedure will nd. As is
discussed below, however, the procedure can also be constrained to nd particular
inverse models with certain desired properties.

Distal learning and backpropagation

In this section we describe an implementation of the distal learning approach that
utilizes the machinery of the backpropagation algorithm. It is important to empha-
size at the outset, however, that backpropagation is not the only algorithm that can
be used to implement the distal learning approach. Any supervised learning algo-
rithm can be used as long as it is capable of learning a mapping across a composite
network that includes a previously trained subnetwork; in particular, Boltzmann
learning is applicable Jordan, 	.

We begin by introducing a useful shorthand for describing backpropagation in
layered networks. A layered network can be described as a parameterized mapping
from an input vector x to an output vector y:

y = x; w;



where w is a vector of parameters weights. In the classical paradigm, the procedure
for changing the weights is based on the discrepancy between a target vector y and
the actual output vector y. The magnitude of this discrepancy is measured by a
cost functional of the form:

J =




y (cid:0) yT y (cid:0) y:



J is the sum of squared error at the output units of the network. It is generally
desired to minimize this cost.

Backpropagation is an algorithm for computing gradients of the cost functional.
The details of the algorithm can be found elsewhere e.g., Rumelhart, et al., 	;
our intention here is to develop a simple notation that hides the details. This is



achieved formally by using the chain rule to dierentiate J with respect to the
weight vector w:

rwJ = (cid:0)

T

@y
@w

y (cid:0) y:



This equation shows that any algorithm that computes the gradient of J eectively
multiplies the error vector y (cid:0) y by the transpose Jacobian matrix @y=@wT.
Although the backpropagation algorithm never forms this matrix explicitly back-
propagation is essentially a factorization of the matrix; Jordan, 	, Equation 
nonetheless describes the results of the computation performed by backpropagation.
Backpropagation also computes the gradient of the cost functional with respect
to the activations of the units in the network. In particular, the cost functional J
can be dierentiated with respect to the activations of the input units to yield:

rxJ = (cid:0)

T

@y
@x

y (cid:0) y:



We refer to Equation  as backpropagation-to-weights" and Equation  as back-
propagation-to-activation." Both computations are carried out in one pass of the
algorithm; indeed, backpropagation-to-activation is needed as an intermediate step
in the backpropagation-to-weights computation.

In the remainder of this section we formulate two broad categories of learning
problems that lie within the scope of the distal learning approach and derive ex-
pressions for the gradients that arise. For simplicity it is assumed in both of these
derivations that the task is to learn an inverse model that is, the inputs and the
distal targets are assumed to be identical. The two formulations of the distal learn-
ing framework focus on dierent aspects of the distal learning problem and have
dierent strengths and weaknesses. The rst approach, the local optimization"
formulation, focuses on the local dynamical structure of the environment. Because
it assumes that the learner is able to predict state transitions based on information
that is available locally in time, it depends on prior knowledge of an adequate set
of state variables for describing the environment.
It is most naturally applied to
problems in which target values are provided at each moment in time, although it
can be extended to problems in which target values are provided intermittently as
we demonstrate in a following section. All of the computations needed for the local

The Jacobian matrix of a vector function is simply its rst derivative|it is a matrix of rst
partial derivatives. That is, the entries of the matrix @y=@w are the partial derivatives of the
each of the output activations with respect to each of the weights in the network.

To gain some insight into why a transpose matrix arises in backpropagation, consider a single-
layer linear network described by y = W x, where W is the weight matrix. The rows of W are the
incoming weight vectors for the output units of the network, and the columns of W are the outgoing
weight vectors for the input units of the network. Passing a vector forward in the network involves
taking the inner product of the vector with each of the incoming weight vectors. This operation
corresponds to multiplication by W . Passing a vector backward in the network corresponds to
taking the inner product of the vector with each of the outgoing weight vectors. This operation
corresponds to multiplication by W T , because the rows of W T are the columns of W .



optimization formulation can be performed in feedforward networks, thus there is no
problem with stability. The second approach, the optimization-along-trajectories"
formulation, focuses on global temporal dependencies along particular target tra-
jectories. The computation needed to obtain these dependencies is more complex
than the computation needed for the local optimization formulation, but it is more
exible. It can be extended to cases in which a set of state variables is not known a
priori and it is naturally applied to problems in which target values are provided in-
termittently in time. There is potentially a problem with stability, however, because
the computations for obtaining the gradient involve a dynamical process.

Local optimization

The rst problem formulation that we discuss is a local optimization problem. We
assume that the process that generates target vectors is stationary and consider the
following general cost functional:

J =




Efy (cid:0) yT y (cid:0) yg;



where y is an unknown function of the state x and the action u. The action u is
the output of a parameterized inverse model of the form:

u = hx; y

; w;

where w is a weight vector.

Rather than optimizing J directly, by collecting statistics over the ensemble of
states and actions, we utilize an online learning rule cf. Widrow & Stearns, 	
that makes incremental changes to the weights based on the instantaneous value of
the cost functional:

Jn =




yn (cid:0) ynT yn (cid:0) yn:

	

An online learning algorithm changes the weights at each time step based on the
stochastic gradient of J; that is, the gradient of Jn:

wn +  = wn (cid:0) rwJn;

where  is a step size. To compute this gradient the chain rule is applied to Equa-
tion 	:

T

T

rwJn = (cid:0)

@u
@w

@y
@u

yn (cid:0) yn;



where the Jacobian matrices @y=@u and @u=@w are evaluated at time n(cid:0). The
rst and the third factors in this expression are easily computed: The rst factor
describes the propagation of derivatives from the output units of the inverse model
the action units" to the weights of the inverse model, and the third factor is the



distal error. The origin of the second factor is problematic, however, because the
dependence of y on u is assumed to be unknown a priori. Our approach to obtaining
an estimate of this factor has two parts: First, the system acquires a parameterized
forward model over an appropriate subdomain of the state space. This model is of
the form:

^y = ^f x; u; v;



where v is a vector of weights and ^y is the predicted sensation. Second, the distal
error is propagated backward through the forward model; this eectively multiplies
the distal error by an estimate of the transpose Jacobian matrix @y=@u.

Putting these pieces together, the algorithm for learning the inverse model is

based on the following estimated stochastic gradient:

^rwJn = (cid:0)

T

@u
@w

T

@ ^y
@u

yn (cid:0) yn:



This expression describes the propagation of the distal error yn (cid:0) yn backward
through the forward model and down into the inverse model where the weights are
changed. The network architecture in which these computations take place is shown
in Figure 	. This network is a straightforward realization of the block diagram in
Figure . It is composed of an inverse model, which links the state units and the
input units to the action units, and a forward model, which links the state units
and the action units to the predicted-sensation units.

Learning the forward model

The learning of the forward model can itself be formulated as an optimization prob-
lem, based on the following cost functional:

L =




Efy (cid:0) ^yT y (cid:0) ^yg;

where ^y is of the form given in Equation . Although the choice of procedure for
nding a set of weights v to minimize this cost is entirely independent of the choice
of procedure for optimizing J in Equation , it is convenient to base the learning of
the forward model on a stochastic gradient as before:

rvLn = (cid:0)

T

@ ^y
@v

yn (cid:0) ^yn;



where the Jacobian matrix @ ^y=@v is evaluated at time n (cid:0) . This gradient can be
computed by the propagation of derivatives within the forward model and therefore
requires no additional hardware beyond that already required for learning the inverse
model.

Note that the error term y

n (cid:0) yn is not a function of the output of the forward model;
nonetheless, activation must ow forward in the model because the estimated Jacobian matrix
@ ^y=@u varies as a function of the activations of the hidden units and the output units of the
model.



Inverse Model

Forward Model

State
Units

State
Units

Input
Units

Action
Units

Predicted
-
Sensation
Units

Figure 	: A feedforward network that includes a forward model. The action units
are the output units of the system.

y (cid:0) y
y (cid:0) ^y
y (cid:0) ^y

Name

performance error

prediction error

predicted performance error

Source

environment, environment

environment, model
environment, model

Table : The error signals and their sources

The error signals

It is important to clarify the meanings of the error signals used in Equations  and
. As shown in Table , there are three error signals that can be formed from the
variables y, ^y, and y|the prediction error y(cid:0) ^y, the performance error y(cid:0)y, and
the predicted performance error y (cid:0) ^y. All three of these error signals are available
to the learner because each of the signals y, y and ^y are available individually|the
target y and the actual sensation y are provided by the environment, whereas the
predicted sensation ^y is available internally.



For learning the forward model, the prediction error is clearly the appropriate
error signal. The learning of the inverse model, however, can be based on either the
performance error or the predicted performance error. Using the performance error
see Equation  has the advantage that the system can learn an exact inverse model
even though the forward model is only approximate. There are two reasons for this:
rst, Equation  preserves the minima of the cost functional in Equation 	|they
are zeros of the estimated gradient. That is, an inaccurate Jacobian matrix cannot
remove zeros of the estimated gradient points at which y (cid:0) y is zero, although
it can introduce additional zeros spurious local minima. Second, if the estimated
gradients obtained with the approximate forward model have positive inner product
with the stochastic gradient in Equation , then the expected step of the algorithm
is downhill in the cost. Thus the algorithm can in principle nd an exact inverse
model even though the forward model is only approximate.

There may also be advantages to using the predicted performance error.

In
particular, it may be easier in some situations to obtain learning trials using the
internal model rather than the external environment Rumelhart, Smolensky, Mc-
Clelland, & Hinton, 	; Sutton, 		. Such internal trials can be thought of as a
form of mental practice" in the case of backpropagation-to-weights or planning"
in the case of backpropagation-to-activation. These procedures lead to improved
performance if the forward model is suciently accurate. Exact solutions cannot
be found with such procedures, however, unless the forward model is exact.

Modularity

In many cases the unknown mapping from actions to sensations can be decomposed
into a series of simpler mappings, each of which can be modeled independently. For
example, it may often be preferable to model the next-state function and the output
function separately rather than modeling them as a single composite function. In
such cases, the Jacobian matrix @ ^y=@u can be factored using the chain rule to
yield the following estimated stochastic gradient:

^rwJn = (cid:0)

T

@u
@w

T

T

@ ^x
@u

@ ^y
@x

yn (cid:0) yn:



The estimated Jacobian matrices in this expression are obtained by propagating
derivatives backward through the corresponding forward models, each of which are
learned separately.

Optimization along trajectories

A complete inverse model allows the learner to synthesize the actions that are needed
to follow any desired trajectory. In the local optimization formulation we eectively

This section is included for completeness and is not needed for the remainder of the paper.



assume that the learning of an inverse model is of primary concern and the learning
of particular target trajectories is secondary. The learning rule given by Equation 
nds actions that invert the dynamics of the environment at the current point in
state space, regardless of whether that point is on a desired trajectory or not. In
terms of network architectures, this approach leads to using feedforward networks
to model the local forward and inverse state transition structure see Figure 	.

In the current section we consider a more specialized problem formulation in
which the focus is on particular classes of target trajectories. This formulation is
based on variational calculus and is closely allied with methods in optimal con-
trol theory Kirk, 	; LeCun, 	. The algorithm that results is a form of
backpropagation-in-time" Rumelhart, Hinton, & Williams, 	 in a recurrent
network that incorporates a learned forward model. The algorithm diers from the
algorithm presented above in that it not only inverts the relationship between ac-
tions and sensations at the current point in state space but also moves the current
state toward the desired trajectory.

We consider an ensemble of target trajectories fy

ng and dene the following

cost functional:

J =




Ef

N

Xn=

y

n (cid:0) ynT y

n (cid:0) yng;



where  is an index across target trajectories and y is an unknown function of the
state x and the action u. The action u is a parameterized function of the state
x and the target y
:

u = hx; y

; w:

As in the previous formulation, we base the learning rule on the stochastic gradient
of J, that is, the gradient evaluated along a particular sample trajectory y:

J =




N

Xn=

y

n (cid:0) ynT y

n (cid:0) yn:



The gradient of this cost functional can be obtained using the calculus of variations
see also LeCun, 	, Narendra & Parthasarathy, 		. Letting n represent the
vector of partial derivatives of J with respect to xn, and letting 	n represent
the vector of partial derivatives of J with respect to un, Appendix A shows that
the gradient of J is given by the following recurrence relations:

n +

T

@u
@x

n (cid:0)

T

@y
@x

y

n (cid:0) yn





	

n (cid:0)  =

n =

T

T

@z
@x

@z
@u

n

and

rwJn =

T

@u
@w

n;



State
Units

State
Units

D

D

Input
Units

Action
Units

Predicted
-
Next-State

Units

-
Predicted
Sensation
Units

Figure : A recurrent network with a forward model. The boxes labeled by Ds
are unit delay elements.

where the Jacobian matrices are all evaluated at time step n and z stands for
xn +  thus, the Jacobian matrices @z=@x and @z=@u are the deriva-
tives of the next-state function. This expression describes backpropagation-in-time
in a recurrent network that incorporates a forward model of the next-state func-
tion and the output function. As shown in Figure , the recurrent network is
essentially the same as the network in Figure 	, except that there are explicit con-
nections with unit delay elements between the next-state and the current state.
Backpropagation-in-time propagates derivatives backward through these recurrent
connections as described by the recurrence relations in Equations  and .

As in the local optimization case, the equations for computing the gradient

Alternatively, Figure 	 can be thought of as a special case of Figure  in which the backprop-

agated error signals stop at the state units cf. Jordan, 	.



involve the multiplication of the performance error y (cid:0) y by a series of transpose
Jacobian matrices, several of which are unknown a priori. Our approach to esti-
mating the unknown factors is once again to learn forward models of the underlying
mappings and to propagate signals backward through the models. Thus the Jacobian
matrices @z=@u, @z=@x, and @y=@x in Equations , , and 	 are
all replaced by estimated quantities in computing the estimated stochastic gradient
of J.

In the following two sections, we pursue the presentation of the distal learning
approach in the context of two problem domains. The rst section describes learning
in a static environment, whereas the second section describes learning in a dynamic
environment. In both sections, we utilize the local optimization formulation of distal
learning.

Static environments

An environment is said to be static if the eect of any given action is independent of
the history of previous actions. In static environments the mapping from actions to
sensations can be characterized without reference to a set of state variables. Such
environments provide a simplied domain in which to study the learning of inverse
mappings. In this section, we present an illustrative static environment and focus
on two issues:  the eects of nonconvex inverse images in the transformation from
sensations to actions and  the problem of goal-directed learning.

The problem that we consider is that of learning the forward and inverse kine-
matics of a three-joint planar arm. As shown in Figure  and Figure  the con-
guration of the arm is characterized by the three joint angles q; q; and q, and
the corresponding pair of Cartesian variables x and x. The function that relates
these variables is the forward kinematic function x = gq. It is obtained in closed
form using elementary trigonometry:

" x
x  = " lcosq + lcosq + q + lcosq + q + q

lsinq + lsinq + q + lsinq + q + q  ;



where l; l; and l are the link lengths.

The forward kinematic function gq is a many-to-one mapping|for every
Cartesian position that is inside the boundary of the workspace, there are an in-
nite number of joint angle congurations to achieve that position. This implies that
(cid:0)x is not a function; rather, there are an innite
the inverse kinematic relation g
number of inverse kinematic functions corresponding to particular choices of points
q in the inverse images of each of the Cartesian positions. The problem of learning
an inverse kinematic controller for the arm is that of nding a particular inverse
among the many possible inverse mappings.



(x1, x2 )

q
3

q
2

q
1

Figure : A three-joint planar arm.

*

x

Controller

q

Arm

x

Figure : The forward and inverse mappings associated with arm kinematics.

Simulations

In the simulations reported below, the joint-angle congurations of the arm were
represented using the vector cosq (cid:0) 
 ; cosq; cosqT , rather than the vector
of joint angles. This eectively restricts the motion of the joints to the intervals

(cid:0) 
 , ; , and ; , respectively, assuming that each component of the joint-
 ;
angle conguration vector is allowed to range over the interval (cid:0); . The Cartesian
variables x and x were represented as real numbers ranging over (cid:0); . In all of
the simulations, these variables were represented directly as real-valued activations
of units in the network. Thus, three units were used to represent joint-angle cong-
urations and two units were used to represent Cartesian positions. Further details
on the simulations are provided in Appendix B.



The nonconvexity problem

One approach to learning an inverse mapping is to provide training pairs to the
learner by observing the inputoutput behavior of the environment and reversing
the role of the inputs and outputs. This approach, which we referred to earlier as
direct inverse modeling," has been proposed in the domain of inverse kinematics
by Kuperstein 	. Kupersteins idea is to randomly sample points q in joint
space and to use the real arm to evaluate the forward kinematic function x = gq,
thereby obtaining training pairs x; q for learning the controller. The controller is
learned by optimization of the following cost functional:

J =




Efq (cid:0) qT q (cid:0) qg



where q = hx is the output of the controller.

As we discussed earlier, a diculty with the direct inverse modeling approach
is that the optimization of the cost functional in Equation  does not necessarily
yield an inverse kinematic function. The problem arises because of the many-to-one
nature of the forward kinematic function cf. Figure .
In particular, if two or
more of the randomly sampled points q happen to map to the same endpoint, then
the training data that is provided to the controller is one-to-many. The particular
manner in which the inconsistency is resolved depends on the form of the cost
functional|use of the sum-of-squared error given in Equation  yields an arithmetic
average over points that map to the same endpoint. An average in joint space,
however, does not necessarily yield a correct result in Cartesian space, because
the inverse images of nonlinear transformations are not necessarily convex. This
implies that the output of the controller may be in error even though the system
has converged to the minimum of the cost functional.

In Figure  we demonstrate that the inverse kinematics of the three-joint arm
is not convex. To see if this nonconvexity has the expected eect on the direct inverse
modeling procedure we conducted a simulation in which a feedforward network with
one hidden layer was used to learn the inverse kinematics of the three-joint arm.
The simulation provided target vectors to the network by sampling randomly from
a uniform distribution in joint space. Input vectors were obtained by mapping the
target vectors into Cartesian space according to Equation . The initial value of the
root-mean-square RMS joint-space error was :, ltered over the rst  trials.
After ;  learning trials the ltered error reached asymptote at a value of :.
A vector eld was then plotted by providing desired Cartesian vectors as inputs
to the network, obtaining the joint-angle outputs, and mapping these outputs into
Cartesian space using Equation . The resulting vector eld is shown in Figure .
As can be seen, there is substantial error at many positions of the workspace, even
though the learning algorithm has converged. If training is continued, the loci of
the errors continue to shift, but the RMS error remains approximately constant. Al-
though this error is partially due to the nite learning rate and the random sampling



Figure : The nonconvexity of inverse kinematics. The dotted conguration is an
average in joint space of the two solid congurations.

procedure misadjustment," see Widrow & Stearns, 	, the error remains above
: even when the learning rate is taken to zero. Thus, misadjustment cannot ac-
count for the error, which must be due to the nonconvexity of the inverse kinematic
relation. Note, for example, that the error observed in Figure  is reproduced in
the lower left portion of Figure .

In Figure , we demonstrate that the distal learning approach can nd a par-
ticular inverse kinematic mapping. We performed a simulation that was initialized
with the incorrect controller obtained from direct inverse modeling. The simula-
tion utilized a forward model that had been trained previously the forward model
was trained during the direct inverse modeling trials. A grid of  evenly spaced
positions in Cartesian space was used to provide targets during the second phase
of the distal learning procedure. On each trial the error in Cartesian space was
passed backward through the forward model and used to change the weights of the
controller. After ;  such learning trials  passes through the grid of tar-
gets, the resulting vector eld was plotted. As shown in the gure, the vector error
decreases toward zero throughout the workspace; thus, the controller is converging
toward a particular inverse kinematic function.

The use of a grid is not necessary; the procedure also works if Cartesian positions are sampled

randomly on each trial.



Figure : Near-asymptotic performance of direct inverse modeling. Each vector
represents the error at a particular position in the workspace.

Additional constraints

A further virtue of the distal learning approach is the ease with which it is possible
to incorporate additional constraints in the learning procedure and thereby bias the
choice of a particular inverse function. For example, a minimum-norm constraint
can be realized by adding a penalty term of the form (cid:0)x to the propagated errors
at the output of the controller. Temporal smoothness constraints can be realized by
incorporating additional error terms of the form xn (cid:0) xn (cid:0) . Such constraints
can be dened at other sites in the network as well, including the output units or
hidden units of the forward model. It is also possible to provide additional contextual
inputs to the controller and thereby learn multiple, contextually-appropriate inverse
functions. These aspects of the distal learning approach are discussed in more detail
in Jordan 	, 		.

Goal-directed learning

Direct inverse modeling does not learn in a goal-directed manner. To learn a specic
Cartesian target, the procedure must sample over a suciently large region of joint
space and rely on interpolation. Heuristics may be available to restrict the search
to certain regions of joint space, but such heuristics are essentially prior knowledge



Figure : Near-asymptotic performance of distal learning.

about the nature of the inverse mapping and can equally well be incorporated into
the distal learning procedure.

Distal learning is fundamentally goal-directed. It is based on the performance
error for a specic Cartesian target and is capable of nding an exact solution for a
particular target in a small number of trials. This is demonstrated by the simulation
shown in Figure . Starting from the controller shown in Figure , a particular
Cartesian target was presented for ten successive trials. As shown in Figure , the
network reorganizes itself so that the error is small in the vicinity of the target.
After ten additional trials, the error at the target is zero within the oating-point
resolution of the simulation.

Approximate forward models

We conducted an additional simulation to study the eects of inaccuracy in the
forward model. The simulation varied the number of trials allocated to the learning
of the forward model from  to . The controller was trained to an RMS criterion
of . at the three target positions (cid:0):; :, :; :, and :; :. As
shown in Figure , the results demonstrate that an accurate controller can be
found with an inaccurate forward model. Fewer trials are needed to learn the target
positions to criterion with the most accurate forward model; however, the dropo
in learning rate with less accurate forward models is relatively slight. Reasonably



Figure : Goal-directed learning. A Cartesian target in the lower right portion of
the gure was presented for ten successive trials. The error vectors are close to zero
in the vicinity of the target.

rapid learning is obtained even when the forward model is trained for only  trials,
even though the average RMS error in the forward model is : m after  trials,
compared to : m after  trials.

Further comparisons with direct inverse modeling

In problems with many output variables it is often unrealistic to acquire an inverse
model over the entire workspace. In such cases the goal-directed nature of distal
learning is particularly important because it allows the system to obtain inverse
images for a restricted set of locations. However, the forward model must also be
learned over a restricted region of action space, and there is no general a priori
method for determining the appropriate region of the space in which to sample.
That is, although distal learning is goal-directed in its acquisition of the inverse
model, it is not inherently goal-directed in its acquisition of the forward model.

Because neither direct inverse modeling nor distal

learning is entirely goal-
directed, in any given problem it is important to consider whether it is more rea-
sonable to acquire the inverse model or the forward model in a non-goal-directed



)
n
o

i
r
e
t
i
r
c



o
t



s

l

a

i
r
t
(



g
n

i

n

i

a
r
t


r
e

l
l

o
r
t
n
o
C

3000

2000

1000

0

0

1000

2000

4000
Forward  model  training  (trials)

3000

5000

Figure : Number of trials required to train the controller to an RMS criterion of
. as a function of the number of trials allocated to training the forward model.
Each point is an average over three runs.

manner. This issue is problem-dependent, depending on the nature of the function
being learned, the nature of the class of functions that can be represented by the
learner, and the nature of the learning algorithm. It is worth noting, however, that
there is an inherent tradeo in complexity between the inverse model and the for-
ward model, due to the fact that their composition is the identity mapping. This
tradeo suggests a complementarity between the classes of problems for which direct
inverse modeling and distal learning are appropriate. We believe that distal learning
is more generally useful, however, because an inaccurate forward model is generally
acceptable whereas an inaccurate inverse model is not. In many cases, it may be
preferable to learn an inaccurate forward model that is specically inverted at a
desired set of locations rather than learning an inaccurate inverse model directly
and relying on interpolation.



Dynamic environments: One-step dynamic models

To illustrate the application of distal learning to problems in which the environment
has state, we consider the problem of learning to control a two-joint robot arm. Con-
trolling a dynamic robot arm involves nding the appropriate torques to cause the
arm to follow desired trajectories. The problem is dicult because of the nonlinear
couplings between the motions of the two links and because of the ctitious torques
due to the rotating coordinate systems.

The arm that we consider is the two-link version of the arm shown previously in
Figure . Its conguration at each point in time is described by the joint angles qt
and qt, and by the Cartesian variables xt and xt. The kinematic function
xt = gqt that relates joint angles to Cartesian variables can be obtained by
letting l equal zero in Equation :

" xt
xt  = " lcosqt + lcosqt + qt

lsinqt + lsinqt + qt  ;

where l and l are the link lengths. The state space for the arm is the four-
dimensional space of positions and velocities of the links.

The essence of robot arm dynamics is a mapping between the torques applied
at the joints and the resulting angular accelerations of the links. This mapping is
_q, and q
dependent on the state variables of angle and angular velocity. Let q,
represent the vector of joint angles, angular velocities, and angular accelerations,
respectively, and let  represent the torques. In the terminology of earlier sections,
q and _q together constitute the state" and  is the action." For convenience,
we take q to represent the next-state" see the discussion below. To obtain an
analog of the next-state function in Equation , the following dierential equation
can be derived for the angular motion of the links, using standard Newtonian or
Lagrangian dynamical formulations Craig, 	:

M qq + Cq; _q _q + Gq =  ;



where M q is an inertia matrix, Cq; _q is a matrix of Coriolis and centripetal
terms, and Gq is the vector of torque due to gravity. Our interest is not in the
physics behind these equations per se, but in the functional relationships that they
dene. In particular, to obtain a next-state function," we rewrite Equation  by
solving for the accelerations to yield:

q = M

(cid:0)q (cid:0) Cq; _q _q (cid:0) Gq;



(cid:0)q is always assured Craig, 	. Equation  ex-
where the existence of M
presses the state-dependent relationship between torques and accelerations at each
moment in time: Given the state variables qt and _qt, and given the torque  t,
the acceleration qt can be computed by substitution in Equation . We refer to
this computation as the forward dynamics of the arm.



.
q,

q

..
*q

Controller

t

..
q

Arm

Figure : The forward and inverse mappings associated with arm dynamics.

An inverse mapping between torques and accelerations can be obtained by in-
terpreting Equation  in the proper manner. Given the state variables qt and
_qt, and given the acceleration qt, substitution in Equation  yields the corre-
sponding torques. This algebraic computation is refered to as inverse dynamics.
It should be clear that inverse dynamics and forward dynamics are complemen-
tary computations: Substitution of  from Equation  into Equation  yields the
requisite identity mapping.

These relationships between torques, accelerations, and states are summarized
in Figure . It is useful to compare this gure with the kinematic example shown in
Figure . In both the kinematic case and the dynamic case, the forward and inverse
mappings that must be learned are xed functions of the instantaneous values of the
relevant variables. In the dynamic case, this is due to the fact that the structural
terms of the dynamical equations the terms M , C, and G are explicit functions
of state rather than time. The dynamic case can be thought of as a generalization
of the kinematic case in which additional contextual state variables are needed to
index the mappings that must be learned.

Figure  is an instantiation of Figure , with the acceleration playing the role
of the next-state." In general, for systems described by dierential equations, it
is convenient to dene the notion of next-state" in terms of the time derivative of
one or more of the state variables e.g., accelerations in the case of arm dynamics.
This denition is entirely consistent with the development in preceding sections;
indeed, if the dierential equations in Equation  are simulated in discrete time on
a computer, then the numerical algorithm must compute the accelerations dened
by Equation  to convert the positions and velocities at the current time step into
the positions and velocities at the next time step.

This perspective is essentially that underlying the local optimization formulation of distal

learning.

Because of the amplication of noise in dierentiated signals, however, most realistic implemen-
tations of forward dynamical models would utilize positions and velocities rather than accelerations.
In such cases the numerical integration of Equation  would be incorporated as part of the forward
model.



Learning a dynamic forward model

A forward model of arm dynamics is a network that learns a prediction ^q of the
acceleration q, given the position q, the velocity _q, and the torque  . The appro-
priate teaching signal for such a network is the actual acceleration q, yielding the
following cost functional:

L =




Efq (cid:0) ^qT q (cid:0) ^qg:



The prediction ^q is a function of the position, the velocity, the torque and the
weights:

^q = ^f q; _q;  ; w:

For an appropriate ensemble of control trajectories, this cost functional is minimized
when a set of weights is found such that ^f ; w best approximates the forward
dynamical function given by Equation .

An important dierence between kinematic problems and dynamic problems
is that it is generally infeasible to produce arbitrary random control signals in dy-
namical environments, because of considerations of stability. For example, if  t
in Equation  is allowed to be a stationary white-noise stochastic process, then
the variance of qt approaches innity much like a random walk. This yields
data that is of little use for learning a model. We have used two closely related
approaches to overcome this problem. The rst approach is to produce random
equilibrium positions for the arm rather than random torques. That is, we dene a
new control signal ut such that the augmented arm dynamics are given by:

M qq + Cq; _q _q + Gq = kv _q (cid:0) _u + kpq (cid:0) u;



for xed constants kp and kv. The random control signal u in this equation acts
as a virtual" equilibrium position for the arm Hogan, 	 and the augmented
dynamics can be used to generate training data for learning the forward model. The
second approach also utilizes Equation  and diers from the rst approach only
in the choice of the control signal ut. Rather than using random controls, the
target trajectories themselves are used as controls that is, the trajectories utilized
in the second phase of learning are also used to train the forward model. This
approach is equivalent to using a simple xed-gain proportional-derivative PD
feedback controller to stabilize the system along a set of reference trajectories and
thereby generate training data. Such use of an auxiliary feedback controller is
similar to its use in the feedback-error learning Kawato, et al., 	 and direct
inverse modeling Atkeson & Reinkensmeyer, 	; Miller, 	 approaches. As is
discussed below, the second approach has the advantage that it does not require the
forward model to be learned in a separate phase.

A PD controller is a device whose output is a weighted sum of position errors and velocity
errors. The position errors and the velocity errors are multiplied by xed numbers gains before
being summed.



t fb

t ff

++++

++++

t

Feedforward
Controller

Feedback
Controller

Arm

Forward
Model

++++
++++

----

Figure 	: The composite control system.

Composite control system

The composite system for controlling the arm is shown in Figure 	. The control
signal in this diagram is the torque  , which is the sum of two components:

 =  f f +  f b;

where  f f is a feedforward torque and  f b is the optional feedback torque produced
by the auxiliary feedback controller. The feedforward controller is the learning con-
troller that converges toward a model of the inverse dynamics of the arm. In the early
phases of learning, the feedforward controller produces small random torques, thus
the major source of control is provided by the error-correcting feedback controller.
When the feedforward controller begins to be learned it produces torques that al-
low the system to follow desired trajectories with smaller error, thus the role of
the feedback controller is diminished.
Indeed, in the limit where the feedforward
controller converges to a perfect inverse model, the feedforward torque causes the
system to follow a desired trajectory without error and the feedback controller is

As is discussed below, this statement is not entirely accurate. The learning algorithm itself

provides a form of error-correcting feedback control.



therefore silent assuming no disturbances. Thus the system shifts automatically
from feedback-dominated control to feedforward-dominated control over the course
of learning see also Atkeson & Reinkensmeyer, 	; Kawato, et al., 	; Miller,
	.

There are two error signals utilized in learning inverse dynamics: The prediction
error q (cid:0) ^q and the performance error q (cid:0) q. The prediction error is used to train
the forward model as discussed in the previous section. Once the forward model is
at least partially learned, the performance error can be used in training the inverse
model. The error is propagated backward through the forward model and down into
the feedforward controller where the weights are changed. This process minimizes
the distal cost functional:

J =




Efq (cid:0) qT q (cid:0) qg:

Simulations



The arm was modeled using rigid body dynamics assuming the mass to be uniformly
distributed along the links. The links were modeled as thin cylinders. Details on
the physical constants are provided in Appendix C. The simulation of the forward
dynamics of the arm was carried out using a fourth-order Runge-Kutta algorithm
with a sampling frequency of  Hz. The control signals provided by the networks
were sampled at  Hz.

Standard feedforward connectionist networks were used in all of the simula-
tions. There were two feedforward networks in each simulation|a controller and
a forward model|with overall connectivity as shown in Figure  with the box
labelled Arm" being replaced by a forward model. Both the controller and the
forward model were feedforward networks with a single layer of logistic hidden units.
In all of the simulations, the state variables, torques, and accelerations were repre-
sented directly as real-valued activations in the network. Details of the networks
used in the simulations are provided in Appendix B.

In all but the nal simulation reported below, the learning of the forward model
and the learning of an inverse model were carried out in separate phases. The
forward model was learned in an initial phase by using a random process to drive
the augmented dynamics given in Equation . The random process was a white
noise position signal chosen uniformly within the workspace shown in Figure . The
learning of the forward model was terminated when the ltered RMS prediction error
reached : rad=s.

As noted above, it is also possible to include the numerical integration of ^q as part of the
forward model and learn a mapping whose output is the predicted next-state ^qn; ^_qn. This
approach may be preferred for systems in which dierentiation of noisy signals is a concern.



Figure : The workspace the grey region and four target paths. The trajectories
move from left to right along the paths shown.

trial 0

trial 30

(a)

(b)

Figure : Performance on one of the four learned trajectories. a Before learning.
b After  learning trials.

Learning with an auxiliary feedback controller

After learning the forward model, the system learned to control the arm along
the four paths shown in Figure . The target trajectories were minimum jerk
trajectories of one second duration each. An auxiliary proportional-derivative PD
feedback controller was used, with position gains of : N  m=rad and velocity gains
of : N  m  s=rad. Figure  shows the performance on a particular trajectory
before learning with the PD controller alone and during the th learning trial.
The corresponding waveforms are shown in Figure  and Figure . The middle
graphs in these gures show the feedback torques dashed lines and the feedforward
torques solid lines. As can be seen, in the early phases of learning the torques are



tangential velocity

t
i

m
e

0
.
0

0
.
0

1
.
0

2
.
0

3
.
0

4
.
0




f
e
e
d
b
a
c
k

t
o
r
q
u
e

a
n
d

t
h
e

s
o
l
i

d

l
i

n
e

i
s

t
h
e

f
e
e
d
f
o
r
w
a
r
d

t
o
r
q
u
e
.

a
n
d

t
h
e

s
o
l
i

d

l
i

n
e

i
s

t
h
e

a
c
t
u
a
l

a
n
g
l
e
.

I
n

t
h
e
m
d
d
l
e

i

g
r
a
p
h
s
,

t
h
e

d
o
t
t
e
d

l
i

n
e

i
s

t
h
e

F
i
g
u
r
e



:

B
e
f
o
r
e

l
e
a
r
n
n
g
.

i

I
n

t
h
e

t
o
p

g
r
a
p
h
s
,

t
h
e

d
o
t
t
e
d

l
i

n
e

i
s

t
h
e

r
e
f
e
r
e
n
c
e

a
n
g
l
e

t
i

m
e

0
.
2

0
.
4

0
.
6

0
8

.

1
0

.

angle 1

0
.
0

1
.
0

2
.
0

-
2
.
0

0
.
0

0
.
2

0
.
4

0
.
6

0
.
8

1
.
0

torque

1

-
1
.
0

0
.
0

1
.
0

-
1
.
0

0
.
0

0
.
2

0
.
4

0
.
6

0
.
8

1
.
0

t
i

m
e

torque

2

angle 2

-
0
.
6

0
.
0

-
0
.
4

-
0
.
2

0
.
0

0
.
2

0
.
4

0
.
6

t
i

m
e

0
2

.

0
4

.

0
6

.

0
8

.

1
0

.

1
.
9

2
.
0

2
.
1

2
.
2

2
.
3

2
.
4

2
.
5

t
i

m
e

0
.
0

0
2

.

0
4

.

0
6

.

0
8

.

1
0

.

tangential velocity

t
i

m
e

0
.
0

0
.
0

1
.
0

2
.
0

3
.
0




f
e
e
d
b
a
c
k

t
o
r
q
u
e

a
n
d

t
h
e

s
o
l
i

d

l
i

n
e

i
s

t
h
e

f
e
e
d
f
o
r
w
a
r
d

t
o
r
q
u
e
.

a
n
d

t
h
e

s
o
l
i

d

l
i

n
e

i
s

t
h
e

a
c
t
u
a
l

a
n
g
l
e
.

I
n

t
h
e
m
d
d
l
e

i

g
r
a
p
h
s
,

t
h
e

d
o
t
t
e
d

l
i

n
e

i
s

t
h
e

F
i
g
u
r
e



:

A
f
t
e
r

l
e
a
r
n
n
g
.

i

I
n

t
h
e

t
o
p

g
r
a
p
h
s
,

t
h
e

d
o
t
t
e
d

l
i

n
e

i
s

t
h
e

r
e
f
e
r
e
n
c
e

a
n
g
l
e

t
i

m
e

0
.
2

0
.
4

0
.
6

0
8

.

1
0

.

torque

1

-
3
.
0

0
.
0

-
2
.
0

-
1
.
0

0
.
0

1
.
0

2
.
0

3
.
0

-
1
.
0

0
.
0

angle 1

0
.
0

1
.
0

2
.
0

0
.
2

0
.
4

0
.
6

0
.
8

1
.
0

t
i

m
e

0
.
2

0
.
4

0
.
6

0
.
8

1
.
0

torque

2

angle 2

0
.
0

0
.
0

0
.
1

0
.
2

0
.
3

0
.
4

1
.
9

0
.
0

2
.
0

2
.
1

2
.
2

2
.
3

2
.
4

t
i

m
e

0
2

.

0
4

.

0
6

.

0
8

.

1
0

.

t
i

m
e

0
2

.

0
4

.

0
6

.

0
8

.

1
0

.

generated principally by the feedback controller and in later phases the torques are
generated principally by the feedforward controller.

Learning without an auxiliary feedback controller

An interesting consequence of the goal-directed nature of the forward modeling
approach is that it is possible to learn an inverse dynamic model without using an
auxiliary feedback controller. To see why this is the case, rst note that minimum
jerk reference trajectories and other smooth" reference trajectories change slowly
in time. This implies that successive time steps are essentially repeated learning
trials on the same input vector; thus, the controller converges rapidly to a solution"
for a local region of state space. As the trajectory evolves, the solution tracks the
input; thus, the controller produces reasonably good torques prior to any learning."
Put another way, the distal learning approach is itself a form of error-correcting
feedback control in the parameter space of the controller. Such error correction
must eventually give way to convergence of the weights if the system is to learn an
inverse model; nonetheless, it is a useful feature of the algorithm that it tends to
stabilize the arm during learning.

This behavior is demonstrated by the simulations shown in Figure . The
gure shows performance on the rst learning trial as a function of the learning
rate. The results demonstrate that changing the learning rate essentially changes
the gain of the error-correcting behavior of the algorithm. When the learning rate
is set to ., the system produces nearly perfect performance on the rst learning
trial. This feature of the algorithm makes it important to clarify the meaning of
the learning curves obtained with the distal learning approach. Figure  shows
two such learning curves. The lower curve is the RMS error that is obtained with
a learning rate of .. The upper curve is the RMS error that is obtained when
the learning rate is temporarily set to zero after each learning trial. Setting the
learning rate to zero allows the eects of learning to be evaluated separately from
the error-correcting behavior. The curves clearly reveal that on the early trials the
main contributor to performance is error correction rather than learning.

Combining forward dynamics and forward kinematics

Combining the forward dynamic models of this section with the forward kinematic
models of the preceding section makes it possible to train the controller using Carte-
sian target trajectories. Given that the dynamic model and the kinematic model
can be learned in parallel, there is essentially no performance decrement associated
with using the combined system. In our simulations, we nd that learning times
increase by approximately eight percent when using Cartesian targets rather than
joint angle targets.



mu = 0.0

mu = 0.01

mu = 0.02

mu = 0.05

mu = 0.1

mu = 0.5

Figure : Performance on the rst learning trial as a function of the learning rate.



)

2

s
/
d
a
r

(

r
o
r
r
e
S
M
R



40

30

20

10

0

0

mu = 0.1
mu = 0.0

10

20

30

40

50

trial

Figure : RMS error for zero and non-zero learning rates.

Learning the forward model and the controller simultaneously

The distal learning approach involves using a forward model to train the controller;
thus, learning of the forward model must precede the learning of the controller. It is
not necessary, however, to learn the forward model over the entire state space before
learning the controller|a local forward model is generally sucient. Moreover, as
we have discussed, the distal learning approach does not require an exact forward
model|approximate forward models often suce. These two facts, in conjunction
with the use of smooth reference trajectories, imply that it should be possible to
learn the forward model and the controller simultaneously. An auxiliary feedback
controller is needed to stabilize the system initially; however, once the forward model
begins to be learned, the learning algorithm itself tends to stabilize the system.
Moreover, as the controller begins to be learned, the errors decrease and the eects
of the feedback controller diminish automatically. Thus the system bootstraps itself
toward an inverse model.

The simulation shown in Figure  demonstrates the feasibility of this approach.



trial 0

trial 30

(a)

(b)

Figure : Learning the forward model and the controller simultaneously. a Per-
formance before learning on two of the target trajectories. b Performance after 
learning trials.

Using the same architecture as in previous experiments the system learned four
target trajectories starting with small random weights in both the controller and
the forward model. On each time step two passes of the backpropagation algorithm
were required|one pass with the prediction error q (cid:0) ^q to change the weights
of the forward model, and a second pass with the performance error q (cid:0) q to
change the weights of the controller. An auxiliary proportional-derivative PD
feedback controller was used, with position gains of : N  m=rad and velocity gains
of : N  m  s=rad. As shown in the gure, the system converges to an acceptable
level of performance after  learning trials.

Although the simultaneous learning procedure requires more presentations of
the target trajectories to achieve a level of performance comparable to that of the
two-phase learning procedure, the simultaneous procedure is in fact more ecient
than two-phase learning because it dispenses with the initial phase of learning the
forward model. This advantage must be weighed against certain disadvantages;
in particular, the possibility of instability is enhanced because of the error in the
gradients obtained from the partially-learned forward model.
In practice we nd
that it is often necessary to use smaller step sizes in the simultaneous learning
approach than in the two-phase learning approach. Preliminary experiments have
also shown that is worthwhile to choose specialized representations that enhance the
speed with which the forward model converges. This can be done separately for the
state variable input and the torque input.



Dynamic environments: Simplied models

In the previous section we demonstrated how the temporal component of the dis-
tal supervised learning problem can be addressed by knowledge of a set of state
variables for the environment. Assuming prior knowledge of a set of state variables
is tantamount to assuming that the learner has prior knowledge of the maximum
delay between the time at which an action is issued and the time at which an eect
is observed in the sensation vector. In the current section we present preliminary
results that aim to broaden the scope of the distal learning approach to address
problems in which the maximum delay is not known see also Werbos, 	.

A simple example of such a problem is one in which a robot arm is required
to be in a certain conguration at time T , where T is unknown, and where the
trajectory in the open interval from  to T is unconstrained. One approach to
solving such problems is to learn a one-step forward model of the arm dynamics and
then to use backpropagation-in-time in a recurrent network that includes the forward
model and a controller Jordan, 		; Kawato, 		. In many problems involving
delayed temporal consequences, however, it is neither feasible nor desirable to learn
a dynamic forward model of the environment, either because the environment is
too complex or because solving the task at hand does not require knowledge of the
evolution of all of the state variables. Consider for example the problem of predicting
the height of a splash of water when stones of varying size are dropped into a pond.
It is unlikely that a useful one-step dynamic model could be learned for the uid
dynamics of the pond. Moreover, if the control problem is to produce splashes
of particular desired heights, it may not be necessary to model uid dynamics in
detail. A simple forward model that predicts an integrated quantity|splash height
as a function of the size of the stone|may suce.

Jordan and Jacobs 		 illustrated this approach by using distal learning to
solve the problem of learning to balance an inverted pendulum on a moving cart.
This problem is generally posed as an avoidance control problem in which the only
corrective information provided by the environment is a signal to indicate that failure
has occured Barto, Sutton, & Anderson, 	. The delay between actions forces
applied to the cart and the failure signal is unknown and indeed can be arbitrarily
large. In the spirit of the foregoing discussion, Jordan and Jacobs also assumed that
it is undesirable to model the dynamics of the cart-pole system; thus, the controller
cannot be learned by using backpropagation-in-time in a recurrent network that
includes a one-step dynamic model of the plant.

A unique trajectory may be specied by enforcing additional constraints on the temporal evo-
lution of the actions; however, the only explicit target information is assumed to be that provided
at the nal time step.

In Kawatos work, backpropagation-in-time is implemented in a spatially-unrolled network and
the gradients are used to change activations rather than weights; however, the idea of using a one-
step forward dynamic model is the same. See also Nguyen & Widrow 		 for an application to
a kinematic problem.



The approach adopted by Jordan and Jacobs involves learning a forward model
whose output is an integrated quantity|an estimate of the inverse of the time until
failure. This estimate is learned using temporal dierence techniques Sutton, 	.
At time steps on which failure occurs, the target value for the forward model is unity:

et =  (cid:0) ^zt;

where ^zt is the output of the forward model, and et is the error term used to
change the weights. On all other time steps, the following temporal dierence error
term is used:

et =

 + ^z(cid:0)t + 



(cid:0) ^zt;

which yields an increasing arithmetic series along any trajectory that leads to failure.
Once learned, the output of the forward model is used to provide a gradient for
learning the controller. In particular, because the desired outcome of balancing the
pole can be described as the goal of maximizing the time until failure, the algorithm
learns the controller by using zero minus the output of the forward model as the
distal error signal.

The forward model used by Jordan and Jacobs diers in an important way
from the other forward models described in this paper. Because the time-until-
failure depends on future actions of the controller, the mapping that the forward
model must learn depends not only on xed properties of the environment but also
on the controller. When the controller is changed by the learning algorithm, the
mapping that the forward model must learn also changes. Thus the forward model
must be updated continuously during the learning of the controller. In general, for
problems in which the forward model learns to estimate an integral of the closed-
loop dynamics, the learning of the forward model and the controller must proceed
in parallel.

Temporal dierence techniques provide the distal learning approach with en-
hanced functionality. They make it possible to learn to make long-term predictions
and thereby adjust controllers on the basis on quantities that are distal in time.
They can also be used to learn multi-step forward models.
In conjunction with
backpropagation-in-time, they provide a exible set of techniques for learning ac-
tions on the basis of temporally-extended consequences.

Discussion

In this paper we have argued that the supervised learning paradigm is broader than
is commonly assumed. The distal supervised learning framework extends super-
vised learning to problems in which desired values are available only for the distal

	This technique can be considered as an example of using supervised learning algorithms to solve

a reinforcement learning problem see below.



consequences of a learners actions and not for the actions themselves. This is a sig-
nicant weakening of the classical notion of the teacher" in the supervised learning
paradigm.
In this section we provide further discussion of the class of problems
that can be treated within the distal supervised learning framework. We discuss
possible sources of training data and we contrast distal supervised learning with
reinforcement learning.

How is training data obtained?

To provide support for our argument that distal supervised learning is more realistic
than classical supervised learning it is necessary to consider possible sources of
training data for distal supervised learning. We discuss two such sources, which we
refer to as imitation and envisioning.

One of the most common ways for humans to acquire skills is through imitation.
Skills such as dance or athletics are often learned by observing another person
performing the skill and attempting to replicate their behavior. Although in some
cases a teacher may be available to suggest particular patterns of limb motion, such
direct instruction does not appear to be a necessary component of skill acquisition.
A case in point is speech acquisition|children acquire speech by hearing speech
sounds, not by receiving instruction on how to move their articulators.

Our conception of a distal supervised learning problem involves a set of inten-
tion, desired outcome training pairs. Learning by imitation clearly makes desired
outcomes available to the learner. With regard to intentions, there are three possi-
bilities. First, the learner may know or be able to infer the intentions of the person
serving as a model. Alternatively, an idiosyncratic internal encoding of intentions is
viable as long as the encoding is consistent. For example, a child acquiring speech
may have an intention to drink, may observe another person obtaining water by
uttering the form water," and may utilize the acoustic representation of water"
as a distal target for learning the articulatory movements for expressing a desire to
drink, even though the other person uses the water to douse a re. Finally, when the
learner is acquiring an inverse model, as in the simulations reported in this paper,
the intention is obviously available because it is the same as desired outcome.

Our conception of distal supervised learning problem as a set of training pairs is
of course an abstraction that must be elaborated when dealing with complex tasks.
In a complex task such as dance, it is presumably not easy to determine the choice
of sensory data to be used as distal targets for the learning procedure. Indeed, the
learner may alter the choice of targets once he or she has achieved a modicum of
skill. The learner may also need to decompose the task into simpler tasks and to set
intermediate goals. We suspect that the role of external teachers" is to help with
these representational issues rather than to provide proximal targets directly to the
learner.

Another source of data for the distal supervised learning paradigm is a process
that we refer to as envisioning." Envisioning is a general process of converting



abstract goals into their corresponding sensory realization, without regard to the
actions needed to achieve the goals. Envisioning involves deciding what it would
look like" or feel like" to perform some task. This process presumably involves
general deductive and inductive reasoning abilities as well as experience with similar
tasks. The point that we want to emphasize is that envisioning need not refer to
the actions that are needed to actually carry out a task; that is the problem solved
by the distal learning procedure.

Comparisons with reinforcement learning

An alternative approach to solving the class of problems that we have discussed in
this paper is to use reinforcement learning algorithms Barto, 		; Sutton, 	.
Reinforcement learning algorithms are based on the assumption that the environ-
ment provides an evaluation of the actions produced by the learner. Because the
evaluation can be an arbitrary function, the approach is in principle applicable to
the general problem of learning on the basis of distal signals.

Reinforcement learning algorithms work by updating the probabilities of emit-
ting particular actions. The updating procedure is based on the evaluations received
from the environment. If the evaluation of an action is favorable then the probabil-
ity associated with that action is increased and the probabilities associated with all
other actions are decreased. Conversely, if the evaluation is unfavorable, then the
probability of the given action is decreased and the probabilities associated with all
other actions are increased. These characteristic features of reinforcement learning
algorithms dier in important ways from the corresponding features of supervised
learning algorithms. Supervised learning algorithms are based on the existence of a
signed error vector rather than an evaluation. The signed error vector is generally,
although not always, obtained by comparing the actual output vector to a target
vector. If the signed error vector is small, corresponding to a favorable evaluation,
the algorithm initiates no changes. If the signed error vector is large, corresponding
to an unfavorable evaluation, the algorithm corrects the current action in favor of a
particular alternative action. Supervised learning algorithms do not simply increase
the probabilities of all alternative actions; rather, they choose particular alternatives
based on the directionality of the signed error vector.

It is important to distinguish between learning paradigms and learning algo-
rithms. Because the same learning algorithm can often be utilized in a variety of
learning paradigms, a failure to distinguish between paradigms and algorithms can
lead to misunderstanding. This is particularly true of reinforcement learning tasks
and supervised learning tasks because of the close relationships between evaluative
signals and signed error vectors. A signed error vector can always be converted into
an evaluative signal any bounded monotonic function of the norm of the signed

As pointed out by Barto, Sutton & Anderson 	, this distinction between reinforcement
learning and supervised learning is signicant only if the learner has a repertoire of more than two
actions.



error vector suces; thus, reinforcement learning algorithms can always be used
for supervised learning problems. Conversely, an evaluative signal can always be
converted into a signed error vector using the machinery that we have discussed;
see also Munro, 	; thus, supervised learning algorithms can always be used for
reinforcement learning problems. The denition of a learning paradigm, however,
has more to do with the manner in which a problem is naturally posed than with
the algorithm used to solve the problem. In the case of the basketball player, for
example, assuming that the environment provides directional information such as
too far to the left," too long," or too short," is very dierent from assuming
that the environment provides evaluative information of the form good," better,"
or best". Furthermore, learning algorithms dier in algorithmic complexity when
applied across paradigms: Using a reinforcement learning algorithm to solve a su-
pervised learning problem is likely to be inecient because such algorithms do not
take advantage of directional information. Conversely, using supervised learning al-
gorithms to solve reinforcement learning problems is likely to be inecient because
of the extra machinery that is required to induce a signed error vector.

In summary, although it has been suggested that the dierence between rein-
forcement learning and supervised learning is the latters reliance on a teacher," we
feel that this argument is mistaken. The distinction between the supervised learning
paradigm and the reinforcement learning paradigm lies in the interpretation of envi-
ronmental feedback as an error signal or as an evaluative signal, not the coordinate
system in which such signals are provided. Many problems involving distal credit
assignment may be better conceived of as supervised learning problems rather than
reinforcement learning problems if the distal feedback signal can be interpreted as
a performance error.

Conclusions

There are a number of diculties with the classical distinctions between unsuper-
vised," reinforcement," and supervised" learning. Supervised learning is generally
said to be dependent on a teacher" to provide target values for the output units
of a network. This is viewed as a limitation because in many domains there is no
such teacher. Nevertheless, the environment often does provide sensory information
about the consequences of an action which can be employed in making internal mod-
ications just as if a teacher had provided the information to the learner directly.
The idea is that the learner rst acquires an internal model that allows prediction
of the consequences of actions. The internal model can be used as a mechanism
for transforming distal sensory information about the consequences of actions into
proximal information for making internal modications. This two-phase procedure
extends the scope of the supervised learning paradigm to include a broad range of
problems in which actions are transformed by an unknown dynamical process before
being compared to desired outcomes.



We rst illustrated this approach in the case of learning an inverse model of
a simple static" environment. We showed that our method of utilizing a forward
model of the environment has a number of important advantages over the alternative
method of building the inverse model directly. These advantages are especially
apparent in cases where there is no unique inverse model. We also showed that this
idea can be extended usefully to the case of a dynamic environment. In this case,
we simply elaborate both the forward model and the learner i.e., controller so they
take into account the current state of the environment. Finally, we showed how this
approach can be combined with temporal dierence techniques to build a system
capable of learning from sensory feedback that is subject to an unknown delay.

We also suggested that comparative work in the study of learning can be facili-
tated by making a distinction between learning algorithms and learning paradigms.
A variety of learning algorithms can often be applied to a particular instance of a
learning paradigm; thus, it is important to characterize not only the paradigmatic
aspects of any given learning problem, such as the nature of the interaction between
the learner and the environment and the nature of the quantities to be optimized,
but also the tradeos in algorithmic complexity that arise when dierent classes
of learning algorithms are applied to the problem. Further research is needed to
delineate the natural classes at the levels of paradigms and algorithms and to clarify
the relationships between levels. We believe that such research will begin to pro-
vide a theoretical basis for making distinctions between candidate hypotheses in the
empirical study of human learning.

