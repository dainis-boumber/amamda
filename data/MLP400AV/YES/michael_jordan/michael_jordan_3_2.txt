1. Introduction

Bayesian methods have a number of virtues, particularly their
uniform treatment of uncertainty at all levels of the modeling
process. The formalism also allows ready incorporation of prior
knowledge and the seamless combination of such knowledge
with observed data (Bernardo and Smith 1994, Gelman 1995,
Heckerman, Geiger and Chickering 1995). The elegant seman-
tics, however, often comes at a sizable computational cost  pos-
terior distributions resulting from the incorporation of observed
data must be represented and updated, and this generally involves
high-dimensional integration. The computational cost involved
in carrying out these operations can call into question the viabil-
ity of Bayesian methods even in relatively simple settings, such
as generalized linear models (McCullagh and Nelder 1983). We
concern ourselves in this paper with a particular generalized
linear model  logistic regression  and we focus on Bayesian
calculations that are computationally tractable. In particular we
describe a exible deterministic approximation procedure that
allows the posterior distribution in logistic regression to be rep-
resented and updated efciently. We also show how our meth-
ods permit a Bayesian treatment of a more complex model  a
0960-3174 C(cid:176) 2000 Kluwer Academic Publishers

directed graphical model (a belief network) in which each
node is a logistic regression model.

The deterministic approximation methods that we develop
in this paper are known generically as variational methods.
Variational techniques have been used extensively in the physics
literature (see, e.g., Parisi 1988, Sakurai 1985) and have also
found applications in statistics (Rustagi 1976). Roughly speak-
ing, the objective of these methods is to transform the problem
of interest into an optimization problem via the introduction of
extra degrees of freedom known as variational parameters. For
xed values of the variational parameters the transformed prob-
lem often has a closed form solution, providing an approximate
solution to the original problem. The variational parameters are
adjusted via an optimization algorithm to yield an improving
sequence of approximations. For an introduction to variational
methods in the context of graphical models see Jordan et al.
(1999).

Let us briey sketch the variational method that we develop in
this paper. We study a logistic regression model with a Gaussian
prior on the parameter vector. Our variational transformation
replaces the logistic function with an adjustable lower bound
that has a Gaussian form; that is, an exponential of a quadratic

26

Jaakkola and Jordan

function of the parameters. The product of the prior and the
variationally transformed likelihood thus yields a Gaussian ex-
pression for the posterior (conjugacy), which we optimize varia-
tionally. This procedure is iterated for each successive data point.
Our methods can be compared to the Laplace approximation
for logistic regression (cf. Spiegelhalter and Lauritzen 1990), a
closely related method which also utilizes a Gaussian approxi-
mation to the posterior. To anticipate the discussion in following
sections, we will see that the variational approach has an ad-
vantage over the Laplace approximation; in particular, the use
of variational parameters gives the variational approach greater
exibility. We will show that this exibility translates into im-
proved accuracy of the approximation.

Variational methods can also be contrasted with sampling
techniques, which have become the method of choice in
Bayesian statistics (Thomas, Spiegelhalter and Gilks 1992, Neal
1993, Gilks, Richardson and Spiegelhalter 1996). Sampling
techniques enjoy wide applicability and can be powerful in eval-
uating multi-dimensional integrals and representing posterior
distributions. They do not, however, yield closed form solu-
tions nor do they guarantee monotonically improving approxi-
mations. It is precisely these features that characterize variational
methods.

The paper is organized as follows. First we describe in some
detail a variational approximation method for Bayesian logistic
regression. This is followed by an evaluation of the accuracy
of the method and a comparison to Laplace approximation. We
then extend the framework to belief networks, considering both
complete data and incomplete data. Finally, we consider the dual
of the regression problem and show that our techniques lead to
exactly solvable EM updates.

2. Bayesian logistic regression

We begin with a logistic regression model given by:

P(S D 1j X; (cid:181)) D g((cid:181) T X);

x )

(1)
where g(x) D (1C e
1 is the logistic function, S the binary
response variable, and X D fX1; : : : ; Xrg the set of explana-
tory variables. We represent the uncertainty in the parameter
values (cid:181) via a prior distribution P((cid:181)) which we assume to be a
Gaussian with possibly full covariance structure. Our predictive
distribution is therefore:
P(S j X ) D

P(S j X; (cid:181))P((cid:181)) d(cid:181):

Z

(2)

In order to utilize this distribution we need to be able to compute
the posterior parameter distribution P((cid:181) j D1; : : : ; DT ), where
we assume that each Dt D fSt ; X t
g is a complete ob-
servation. This calculation is intractable for larger n or T , thus
we consider a variational approximation.

; : : : ; X t
r

1

Our approach involves nding a variational transformation
of the logistic function and using this transformed function as
an approximate likelihood. In particular we wish to consider

Fig. 1. A convex function f and its two tangent lines. The locations of
the tangents are indicated with short vertical line segments

transformations that combine readily with a Gaussian prior in
the sense that the Gaussian prior becomes the conjugate prior to
the transformed likelihood. We begin by introducing the type of
variational transformations we will use for this purpose.

2.1. A brief introduction to variational methods

Consider any continuously differentiable convex function f (z).
Figure 1 provides an example of a convex function that we will
make use of later on. Convexity of this function guarantees by
denition that any tangent line always remains below the func-
tion itself. We may thus interpret the collection of all the tangent
lines as a parameterized family of lower bounds for this convex
function (cf. convex duality, Rockafellar 1976). The tangents in
this family are naturally parameterized by their locations. From
the point of view of approximating the convex non-linear func-
tion f , it seems natural to use one of the simpler tangent lines
as a lower bound. To formulate this a little more precisely, let
L(z; z0) be the tangent line at z D z0,

f (z)jzDz0(z  z0);

L(z; z0) D f (z0) C @
@z

(3)
f (z)  L(z; z0) for all z; z0 and f (z0) D
then it follows that
L(z0; z0). In the terminology of variational methods, L(z; z0)
is a variational lower bound of f (z) where the parameter z0
is known as the variational parameter. Since the lower bound
L(z; z0) is considerably simpler (linear in this case) than the non-
linear function f (z), it may be attractive to substitute the lower
bound for f . Note that we are free to adjust the variational para-
meter z0, the location of the tangent, so as to make L(z; z0) as
accurate an approximation of f (z) as possible around the point
of interest, i.e., when z  z0. The quality of this approximation
degrades as z receeds from z0; the rate at which this happens
depends on the curvature of f (z). Whenever the function f has
relatively low curvature as is the case in Fig. 1, the adjustable
linear approximation seems quite attractive.

Bayesian parameter estimation

2.2. Variational methods in bayesian logistic regression

log g(x) D log(1 C e

Here we illustrate how variational methods, of the type described
above, can be used to transform the logistic likelihood function
into a form that readily combines with the Gaussian prior (conju-
gacy). More precisely, the transformed logistic function should
depend on the parameters (cid:181) at most quadratically in the expo-
nent. We begin by symmetrizing the log logistic function:
x=2);

(4)
and noting that f (x) D  log(ex=2Ce
x=2), is a convex function
in the variable x 2. (This is readily veried by taking second
derivatives; the behavior of f (x) as a function of x 2 is shown
in Fig. 1). As discussed above, a tangent surface to a convex
function is a global lower bound for the function and thus we
can bound f (x) globally with a rst order Taylor expansion in
the variable x 2:

 log(ex=2 C e

x ) D x
2

f (x)  f ( ) C @ f ( )
@( 2)

(x 2   2)

(cid:181)



(5)

D  
2

C log g( ) C 1
4

tanh

(6)
Note that this lower bound is exact whenever  2 D x 2. Combin-
ing this result with equation (4) and exponentiating yields the
desired variational transformation of the logistic function:

(x 2   2):


2

(cid:190)
(cid:190)

P(S j X; (cid:181)) D g(Hs)



Hs  



;

2

H 2
s

  2

 ( )


 g( ) exp


(7)
where Hs D (2S  1) (cid:181) T X and ( ) D tanh( =2)=(4 ). We also
introduce the following notation:
P(S j X; (cid:181); )  g( ) exp
(8)
that is, P(S j X; (cid:181); ) denotes the variational lower bound on
the logistic function g(HS). As a lower bound it is no longer
normalized. We refer to equation (8) as a  -transformation of
the conditional probability.

Hs  

 ( )

  2

H 2
S

2

;

For each xed value of Hs we can in fact recover the ex-
act value of the logistic function via a particular choice of the
variational parameter. Indeed, maximizing the lower bound with
respect to  yields  D Hs; substituting this value back into the
lower bound recovers the original conditional probability. For
all other values of  we obtain a lower bound.
The true posterior P((cid:181) j D) can be computed by normalizing
P(S j X; (cid:181))P((cid:181)). Given that this calculation is not feasible in
general, we instead form the bound:

P(S j X; (cid:181)) P((cid:181))  P(S j X; (cid:181); ) P((cid:181))

(9)
and normalize the variational approximation P(S j X; (cid:181); )
P((cid:181)). Given that P((cid:181)) is Gaussian and given our choice of
a Gaussian variational form for P(S j X; (cid:181); ), the normali-
zed variational distribution is a Gaussian. Note that although

27
P(S j X; (cid:181); ) is a lower bound on the true conditional probabil-
ity, our variational posterior approximation is a proper density
and thus no longer a bound. This approximate Bayesian up-
date amounts to updating the prior mean  and the prior co-
variance matrix 6 into the posterior mean and the posterior
covariance matrix. Omitting the algebra we nd that the updates
take the following form:

61

pos

(cid:181)
D 61 C 2( )X X T







(10)

S  1
2

pos D 6pos

61 C

(11)
for a single observation (S; X), where X D [X1::Xr ]T . Suc-
cessive observations can be incorporated into the posterior by
applying these updates recursively.

X

Our work is not nished, however, because the posterior co-
variance matrix depends on the variational parameter  through
( ) and we have yet to specify  . We choose  via an optimiza-
tion procedure; in particular, we nd a value of  that yields a
tight lower bound in equation (9). The fact that the variational
expression in equation (9) is a lower bound is important  it
allows us to use the EM algorithm to perform the optimization.
We derive such an EM algorithm in Appendix A; the result is
the following (closed form) update equation for  :
 2 D Ef((cid:181) T X)2g D X T 6post X C (X T post)2;

(12)
where the expectation is taken with respect to P((cid:181) j D;  old), the
variational posterior distribution based on the previous value
of  . Owing to the EM formulation, each update for  corres-
ponds to a monotone improvement to the posterior approxima-
tion. Empirically we nd that this procedure converges rapidly;
only a few iterations are needed. The accuracy of the approxi-
mation is considered in the following two sections.

Z

P(S j X; (cid:181))P((cid:181) jD) d(cid:181);

To summarize, the variational approach allows us to obtain a
closed form expression for the posterior predictive distribution
in logistic regression:
P(S j X;D) D

(13)
where the posterior distribution P((cid:181) jD) comes from making a
single pass through the data setD D fD1; : : : ; DTg, applying the
updates in equations (10) and (11) after optimizing the associated
variational parameters at each step. The predictive lower bound
P(St j X t ;D) takes the form:
log P(St j X t ;D) D log g(t )  t
2
t C 1
2

 1
2
j6tj
j6j ;

C (t ) 2

T 61

C 1
2

61

(14)

T
t

log

t

t

for any complete observation Dt , where  and 6 signify the
parameters in P((cid:181) jD) and the subscript t refers to the posterior
P((cid:181) jD; Dt ) found by augmenting the data set to include the
point Dt .

28

We note nally that the variational Bayesian calculations pre-
sented above need not be carried out sequentially. We could
compute a variational approximation to the posterior probabil-
ity P((cid:181) jD) by introducing (separate) transformations for each
of the logistic functions in
P(D j (cid:181)) D

g((2St  1)(cid:181) T X t )

P(St j X t ; (cid:181)) D

Y

Y

(15)

t

t

The resulting variational parameters would have to be optimized
jointly rather than one at a time. We believe the sequential
approach provides a cleaner solution.

3. Accuracy of the variational method

The logistic function is shown in Fig. 2(a), along with a vari-
ational approximation for  D 2. As we have noted, for each
value of the variational parameter  , there is a particular point
x where the approximation is exact; for the remaining values of
x the approximation is a lower bound.

Integrating equation (9) over the parameters we obtain a lower
bound on the predictive probability of an observation. The tight-
ness of this lower bound is a measure of accuracy of the approxi-
mation. To assess the variational approximation according to this
measure, we compared the lower bound to the true predictive
likelihood that was evaluated numerically. Note that for a single
observation, the evaluation of the predictive likelihood can be
reduced to a one-dimensional integration problem:

Z

g((2S  1)(cid:181) T X)P((cid:181)) d(cid:181)

P(S j X; (cid:181))P((cid:181)) d(cid:181) D

D

Z
Z 1
1 g((cid:181)0
((cid:181)0

0

0

((cid:181)0

) d(cid:181)0

)P

(16)

) is a Gaussian with mean
where the effective prior P
0 D (2S  1)T X and variance (cid:190) 2 D X T 6 X where the
actual prior distribution P((cid:181)) has mean  and covariance 6.
This reduction has no effect on the accuracy of the varia-
tional transformation and thus it can be used in evaluating the
overall accuracy. Figure 2(b) shows the difference between the
true predictive probability and the variational lower bound for

Jaakkola and Jordan

various settings of the effective mean 0
and variance (cid:190) 2, with
 optimized separately for each different values of 0
and (cid:190) 2.
The fact that the variational approximation is a lower bound
means that the difference in the predictive likelihood is always
positive.

We emphasize that the tightness of the lower bound is not the
only relevant measure of accuracy. Indeed, while a tight lower
bound on the predictive probability assures us that the associated
posterior distribution is highly accurate, the converse is not true
in general. In other words, a poor lower bound does not neces-
sarily imply a poor approximation to the posterior distribution
at the point of interest, only that we no longer have any guaran-
tees of good accuracy. In practice, we expect the accuracy of the
posterior to be more important than that of the predictive prob-
ability since errors in the posterior run the risk of accumulating
in the course of the sequential estimation procedure. We defer
the evaluation of the posterior accuracy to the following section
where comparisons are made to related methods.

4. Comparison to other methods

There are other sequential approximation methods that yield
closed form posterior parameter distributions in logistic regres-
sion models. The method most closely related to ours is that
of Spiegelhalter and Lauritzen (1990), which we refer to as the
S-L approximation in this paper. Their method is based on the
Laplace approximation; that is, they utilize a local quadratic
approximation to the complete log-likelihood centered at
the prior mean . The parameter updates that implement this
approximation are similar in spirit to the variational updates of
equations (10) and (11):

post

D 61 C p(1  p)X X T
61
post D  C (S  p)6post X

(18)
where p D g(T X). Since there are no additional adjustable
parameters in this approximation, it is simpler than the varia-
tional method; however, we would expect this lack of exibility
to translate into less accurate posterior estimates.

(17)

Fig. 2. a) The logistic function (solid line) and its variational form (dashed line) for  D 2: b) The difference between the predictive likelihood
and its variational approximation as a function of g(0

), as described in the text

Bayesian parameter estimation

29

Fig. 3. a) The errors in the posterior means as a function of g(0
the posterior standard deviations as a function of g(0

). Again (cid:190) D 1 for the prior distribution

), where 0

is the prior mean. Here (cid:190) D 1 for the prior. b) The relative errors in

Fig. 4. The plots are the same as in Fig. 3, but now (cid:190) D 2 for the prior distribution

We compared the accuracy of the posterior estimates for the
two methods in the context of a single observation. To sim-
plify the comparison we utilized the reduction described in
the previous section. Since the accuracy of neither method is
affected by this reduction, it sufces for our purposes here to
carry out the comparison in this simpler setting.1 The poste-
rior probability of interest was therefore P((cid:181) j D) / g((cid:181)0
)P((cid:181)0
),
computed for various choices of values for the prior mean 0
and the prior standard deviation (cid:190) . The correct posterior mean
and standard deviations were obtained numerically. Figures 3
and 4 present the results. We plot signed differences in com-
paring the obtained posterior means to the correct ones; relative
errors were used for the posterior standard deviations. The error
measures were left signed to reveal any systematic biases. Note
that the posterior mean from the variational method is not guar-
anteed to be a lower bound on the true mean. Such guarantees
can be given only for the predictive likelihood. As can be seen
in Figs. 3(a) and 4(a) the variational method yields signicantly
more accurate estimates of the posterior means, for both val-
ues of the prior variance. For the posterior variance, the S-L
estimate and the variational estimate appear to yield roughly
comparable accuracy for the small value of the prior variance
(Fig. 3(b)); however, for the larger prior variance, the variational
approximation is superior (Fig. 4(b)). We note that the variational
method consistently underestimates the true posterior variance;

a fact that could be used to rene the approximation. Finally, in
terms of the KL-divergences between the approximate and true
posteriors, the variational method and the S-L approximation
are roughly equivalent for the small prior variance; and again
the variational method is superior for the larger value of the
prior variance. This is shown in Fig. 5.

5. Extension to belief networks

A belief network is a probabilistic model over a set of variables
fSig that are identied with the nodes in an acyclic directed
graph. Letting (i) denote the set of parents of node Si in the
graph, we dene the joint distribution associated with the belief
network as the following product:
P(S1; : : : ; Sn) D

(19)
We refer to the conditional probabilities P(Si j S(i)) as the local
probabilities associated with the belief network.

 S(i)

Y

P

Si



:



i

In this section we extend our earlier work in this paper and
consider belief networks in which logistic regression is used to
dene the local probabilities (such models have been studied in
a non-Bayesian setting by Neal 1992 and by Saul, Jaakkola, and
Jordan 1994). Thus we introduce parameter vectors (cid:181)i , one for
each binary variable Si , and consider models in which each local

30

Jaakkola and Jordan

Fig. 5. KL-divergences between the approximate and the true posterior distribution as a function of g(0
The two approximation methods have (visually) identical curves for (cid:190) D 1

): a) (cid:190) D 2 for the prior. b) (cid:190) D 3.

Fig. 6. a) A complete observation (shaded variables) and the Markov blanket (dashed line) associated with the parameters (cid:181)4. b) An observation
where the value of S4 is missing (unshaded in the gure)
probability P(Si j S(i); (cid:181)i ) is a logistic regression of node Si on
its parents S(i).

problem. We apply the methods developed in the previous sec-
tions directly.

To simplify the arguments in the following sections, we will
consider augmented belief networks in which the parameters
themselves are treated as nodes in the belief network (see Fig. 6).
This is a standard device in the belief network literature and is
of course natural within the Bayesian formalism.

5.1. Complete cases

A complete case refers to a data point in which all of the vari-
ables fSig are observed. If all of the data points are complete
cases, then the methods that we developed in the previous sec-
tion apply immediately to belief networks. This can be seen as
follows. Consider the Markov blankets associated with each of
the parameters (Fig. 6(a)). For complete cases each of the nodes
within the Markov blanket for each of the parameters is observed
(shaded in the diagram). By the independence semantics of be-
lief networks, this implies that the posterior distributions for the
parameters are independent of one another (conditioned on the
observed data). Thus the problem of estimating the posterior dis-
tributions for the parameters reduces to a set of n independent
subproblems, each of which is a Bayesian logistic regression

5.2. Incomplete cases

The situation is substantially more complex when there are in-
complete cases in the data set. Incomplete cases imply that we
no longer have all the Markov blankets for the parameters in
the network. Thus dependencies can arise between the parame-
ter distributions in different conditional models. Let us consider
this situation in some detail. A missing value implies that the
observations arise from a marginal distribution obtained by sum-
ming over the missing values of the unobserved variables. The
marginal distribution is thus a mixture distribution, where each
mixture component corresponds to a particular conguration
of the missing variables. The weight assigned to that compo-
nent is essentially the posterior probability of the associated
conguration (Spiegelhalter and Lauritzen 1990). Note that the
dependencies arising from the missing values in the observa-
tions can make the network quite densely connected (a miss-
ing value for a node effectively connects all of the neighboring
nodes in the graph). The dense connectivity leaves little struc-
ture to be exploited in the exact probabilistic computations in

Bayesian parameter estimation

these networks and tends to make exact probabilistic calculations
intractable.

Our approach to developing Bayesian methods for belief net-
works with missing variables combines two variational tech-
niques. In particular, we augment the  -transformation intro-
duced earlier with a second variational transformation that we
refer to as a q-transformation. While the purpose of the  -
transformation is to convert a local conditional probability into
a form that can be integrated analytically, the purpose of the
q-transformation is to approximate the effect of marginaliz-
ing across missing values associated with one or more parents.
Intuitively, the q-transformation lls in the missing values,
allowing the variational transformation for complete data to be
invoked. The overall result is a closed-form approximation to
the marginal posterior.

The correct marginalization across missing variables is a
global operation that affects all of the conditional models that
depend on the variables being marginalized over. Under the vari-
ational approximation that we describe below, marginalization
is a local operation that acts individually on the relevant condi-
tional models.

5.2.1. Approximate marginalization
Consider the problem of marginalizing over a set of variables S
under a joint distribution:

0

Y

 (cid:181)) D



Si

P

i

 S(i); (cid:181)i



P(S1; : : : ; Sn

:

(20)

If we performed the marginalization exactly, then the result-
ing distribution would not retain the same factorization as the
is involved in more than one of the
original joint (assuming S
conditionals); this can be seen from:

0



 S(i

P

00

Si

00

; (cid:181)i00

)

#

Y
X

i

P


Y

Si

X

S0



S0

i0

 S(i); (cid:181)i


"Y
 D
 S(i0); (cid:181)i0


Si0

P

;

00

i

(21)

0

0

00

(indexed by i

where we have partitioned the product into the set of factors
) and those that do not (indexed
that depend on S
). Marginalization is not generally a local operation on
by i
the individual node probabilities P(Si j S(Si ); (cid:181)i ). Maintaining
such locality, a desirable goal for computational reasons, can be
achieved if we forgo exact marginalization and instead consider
approximations. In particular, we describe a variational approxi-
mation that preserves locality at the expense of providing a lower
bound on the marginal probability instead of an exact result.

To obtain the desired variational transformation, we again ex-
ploit a convexity property. In particular, for a given sequence
pi ; i 2f1; : : : ; ng, consider
i pqi
i ,
where qi is a probability distribution. It is well known that the
geometric average is less than or equal to the arithmetic aver-
age 6i qi pi . (This can be easily established via an invocation of
Jensens inequality). We can exploit this fact as follows. Consider
), and rewrite the marginalization
an arbitrary distribution q(S

the geometric average

0

Q

operation in the following way:

X

S0

P(S1; : : : ; Sn j (cid:181)) D





X
Y

S0



S0

D C(q)

q(S

0

)

q(S0)
P(S1; : : : ; Sn j (cid:181))
Y

P(S1; : : : ; Sn j (cid:181))
q(S
 S(i); (cid:181)i

"Y

q(S0)



Si

P

)

0

i

S0



31

(22)

q(S

0

)

(23)

#

;

(24)

where the inequality comes from transforming the average over
the bracketed term (with respect to the distribution q) into a
geometric average. The third line follows from plugging in the
form of the joint distribution and exchanging the order of the
products. The logarithm of the multiplicative constant C(q) is
the entropy of the variational distribution q:

q(S


Y
X

S0

C(q) D

S0
log C(q) D 

1
q(S0)
0

q(S

0

)

and therefore

0

)

(25)

) log q(S

Let us now make a few observations about the result in equation
(24). First, note that the lower bound in this equation has the
same factored form as the original joint probability. In particu-
lar, we dene the q-transformation of the ith local conditional
probability as follows:



P

Si

 S(i); (cid:181)i ; q

 D

Y



 S(i); (cid:181)i

q(S

0

);

(26)

Si

P

S0

the lower bound in equation (24) is then a product of these q-
transformations. Second, note that all the conditionals are trans-
formed by the same distribution q. A change in q can thus affect
all the transformed conditionals. This means that the dependen-
cies between variables S that would have resulted from exact
have been replaced with effective de-
marginalization over S
pendencies through a shared variational distribution q.

0

0

While the bound in equation (24) holds for an arbitrary vari-
), to obtain a tight bound we need to
ational distribution q(S
). In practice this involves choosing a con-
optimize across q(S
strained class of distributions and optimizing across the class.
The simplest form of variational distribution is the completely
factorized distribution:

0

0

q(S

0
i );

qi (S

(27)

) D mY

iD1

which yields a variational bound which is traditionally referred
to as the mean eld approximation. This simplied approxi-
mation is appropriate in dense models with a relatively large
number of missing values. More generally, one can consider
structured variational distributions involving partial factoriza-
tions that correspond to tractable substructures in the graphical

32

0

model (cf. Saul and Jordan 1996). We consider this topic further
in the following two sections.

Although the main constraint on the choice of q(S

) is the
computational one associated with evaluation and optimization,
there is one additional constraint that must be borne in mind. In
particular, the q-transformed conditional probabilities must be in
a form such that a subsequent  -transformation can be invoked,
yielding as a result a tractable Bayesian integral. A simple way to
meet this constraint is to require that the variational distribution
) should not depend on the parameters (cid:181). As we discuss in
q(S
the following section, in this case all of the q-transformations
simply involve products of logistic functions, which behave well
under the  -transformation.

0

5.2.2. Bayesian parameter updates
The derivation presented in the previous section shows that ap-
proximate variational marginalization across a set of variables
0
can be viewed as a geometric average of the local conditional
S
probabilities:

P(S j S ; (cid:181)) !

P(S j S ; (cid:181))q(S

0

)

(28)

Y

S0

0

where q(S
) is the variational distribution over the missing val-
ues. Note that while the  -transformations are carried out sepa-
rately for each relevant conditional model, the variational distri-
bution q associated with the missing values is the same across
all the q-transformations.

Given the transformation in equation (28), the approximate
Bayesian updates are obtained readily. In particular, when condi-
tioning on a data point that has missing components we rst apply
the q-transformation. This effectively lls in the missing values,
resulting in a transformed joint distribution that factorizes as
in the case of complete observations. The posterior parameter
distributions therefore can be obtained independently for the
parameters associated with the transformed local probabilities.
Two issues need to be considered. First, the transformed con-
ditional probabilities (cf. equation (28)) are products of logistic
functions and therefore more complicated than before. The  -
transformation method, however, transforms each logistic func-
tion into an exponential with quadratic dependence on the para-
meters. Products of such transforms are also exponential with
quadratic dependence on the parameters. Thus the approximate
likelihood will again be Gaussian and if the prior is a multivariate
Gaussian the approximate posterior will also be Gaussian.

The second issue is the dependence of the posterior parame-
ter distributions on the variational distribution q. Once again we
have to optimize the variational parameters (a distribution in this
case) to make our bounds as tight as possible; in particular, we
set q to the distribution that maximizes our lower bound. This
optimization is carried out in conjunction with the optimization
of the  parameters for the transformations of the logistic func-
tions, which are also lower bounds. As we show in Appendix
B.1, the fact that all of our approximations are lower bounds
implies that we can again devise an EM algorithm to perform
the maximization. The updates that are derived in the Appendix

Jaakkola and Jordan

'



are as follows:
61
posi

D 61

i

D 6posi

posi

S(i)ST

(i)

(cid:181)


C 2(i )E
61

i C E

i

Si  1
2



(cid:190)

S(i)

(29)

(30)

where S(i) is the vector of parents of Si , and the expectations
are taken with respect to the variational distribution q.

5.2.3. Numerical evaluation
In this section, we provide a numerical evaluation of our pro-
posed combination of q-transformation and  -transformation.
We study a simple graph that consists of a single node S and
its parents S . In contrast to the simple logistic regression case
analyzed earlier, the parents S are not observed but instead are
distributed according to a distribution P(S ). This distribution,
which we manipulate directly in our experiments, essentially
provides a surrogate for the effects of a pattern of evidence in
the ancestral graph associated with node S (cf. Spiegelhalter and
Lauritzen 1990).
(cid:181) associated with the conditional probability P(S j S ; (cid:181)).

Our interest is in the posterior probability over the parameters
Suppose now that we observe S D 1. The exact posterior

probability over the parameters (cid:181) in this case is given by

#

P((cid:181) j D) /

P(S D 1j S ; (cid:181))P(S )

P((cid:181))

(31)

"X

S

Z "X
Z "X

S

D

Our variational method focuses on lower bounding the evidence
term in brackets. It is natural to evaluate the overall accuracy of
the approximation by evaluating the accuracy of the marginal
data likelihood:
P(D) D

P(S D 1j S ; (cid:181))P(S )

P((cid:181)) d(cid:181)

#

(32)

#

(cid:190) ((cid:181) T S ) P(S )

P((cid:181)) d(cid:181):

(33)

S

We consider two different variational approximations. In the
rst approximation the variational distribution q is left uncon-
strained; in the second we use an approximation that factorizes
across the parents S (the mean eld approximation). We em-
phasize that in both cases the variational posterior approximation
over the parameters is a single Gaussian.

The results of our experiment are shown in Figs. 7 and 8.
Each gure displays three curves, corresponding to the exact
evaluation of the data likelihood P(D) and the two variational
lower bounds. The number of parents in S was 5 and the prior
distribution P((cid:181)) was taken to be a zero mean Gaussian with
a variable covariance matrix. By the symmetry of the Gaussian
distribution and the sigmoid function, the exact value of P(D)
was 0.5 in all cases. We considered several choices of P(S )
and P((cid:181)). In the rst case, the P(S ) were assumed to factorize

Bayesian parameter estimation

33

Fig. 7. Exact data likelihood (solid line), variational lower bound 1 (dashed line), and variational lower 2 (dotted line) as a function of the
stochasticity parameter p of P(S ). In (a) P((cid:181)) D N (0; I =5) and in (b) P((cid:181)) D N (0; 6) where 6 is a sample covariance of 5 random vectors
distributed according to N (0; I =5)

Fig. 8. Exact data likelihood (solid line) and the two variational lower bounds (dashed and dotted lines respectively) as a function of the mixture
parameter pm. In (a) p D 0:1 and in (b) p D 0:3

across the parents and for each S j 2 S ; P(S j D 1) D p leaving
a single parameter p that species the stochasticity of P(S ).
A similar setting would arise when applying the mean eld ap-
proximation in the context of a more general graph. Figure 7
shows the accuracy of the variational lower bounds as a function
of p where in Fig. 7(a) P((cid:181)) D N (0; I =5), i.e., the covariance
matrix is diagonal with diagonal components set to 1/5, and
in Fig. 7(b) P((cid:181)) D N (0; 6) where 6 is a sample covariance
matrix of 5 Gaussian random vectors distributed according to
N (0; I =5). The results of Fig. 7(b) are averaged over 5 inde-
pendent runs. The choice of scaling in N (0; I =5) is made to in-
(cid:181) jj 1. Both gures indicate that the variational
approximations are reasonably accurate and that there is little
difference between the two methods.

sure that jP

j

In Fig. 8 we see how the mean eld approximation (which is
unimodal) deteriorates as the distribution P(S ) changes from
a factorized distribution toward a mixture distribution. More
specically, let P f (S j p) be the (uniform) factorized distri-
bution discussed above with parameter p and let Pm(S ) be a
pure mixture distribution that assigns a probability mass 1/3 to
three different (randomly chosen) congurations of the parents
S . We let P(S ) D (1  pm)P f (S j p) C pm Pm(S ), where

the parameter pm controls the extent to which P(S ) resembles
a (pure) mixture distribution. Figure 8 illustrates the accuracy
of the two variational methods as a function of pm where in
Fig. 8(a) p D 0:1 and in 8(b) p D 0:3. As expected, the mean
eld approximation deteriorates with an increasing pm whereas
our rst variational approximation remains accurate.

6. The dual problem

In the logistic regression formulation (equation (1)), the para-
meters (cid:181) and the explanatory variables X play a dual or sym-
metric role (cf. Nadal and Parga 1994). In the Bayesian logis-
tic regression setting, the symmetry is broken by associating
the same parameter vector (cid:181) with multiple occurrences of the
explanatory variables X as shown in Fig. 9. Alternatively, we
may break the symmetry by associating a single instance of the
explanatory variable X with multiple realizations of (cid:181). In this
sense the explanatory variables X play the role of parameters
while (cid:181) functions as a continuous latent variable. The dual of
the Bayesian regression model is thus a latent variable density
model over a binary response variable S. Graphically, in the

34

Fig. 9. a) Bayesian regression problem. b) The dual problem.

dual interpretation we have a single parameter node for X
whereas separate nodes are required for different realizations of
(cid:181) (illustrated as (cid:181) (i) in the gure) to explain successive obser-
vations S(i). While a latent variable density model over a sin-
gle binary variable is not particularly interesting, we can gen-
eralize the response variable S to a vector of binary variables
S D [S1; : : : ; Sn]T where each component Si has a distinct set
of parameters Xi D [Xi1; : : : ; Xim]T associated with it. The
latent variables (cid:181), however, remain in this dual interpretation the
same for all Si . We note that strictly speaking the dual interpre-
tation would require us to assign a prior distribution over the
new parameters vectors Xi . For simplicity, however, we omit
this consideration and treat Xi simply as adjustable parameters.
The resulting latent variable density model over binary vectors is
akin to the standard factor analysis model (see e.g. Everitt 1984).
This model has already been used to facilitate visualization of
high dimensional binary vectors (Tipping 1999).

We now turn to a more technical treatment of this latent vari-

able model. The joint distribution is given by

Z "Y

#

P(S1; : : : ; Sn j X) D

P(Si j Xi ; (cid:181))

P((cid:181)) d(cid:181)

(34)

i

where the conditional probabilities for the binary observables
are logistic regression models

P(Si j Xi ; (cid:181)) D g((2Si  1)6 j Xi j (cid:181) j )

(35)

1

; : : : ; St
n

We would like to use the EM-algorithm for parameter estimation.
To achieve this we again exploit the variational transformations.
The transformations can be introduced for each of the condi-
tional probability in the joint distribution and optimized sepa-
rately for every observation Dt DfSt
g in the database
consisting only of the values of the binary output variables. As
in the logistic regression case, the transformations change the
unwieldy conditional models into simpler ones that depend on
the parameters only quadratically in the exponent. The vari-
ational evidence, which is a product of the transformed con-
ditional probabilities, retains the same property. Consequently,
under the variational approximation, we can compute the pos-
terior distribution over the latent variables (cid:181) in closed form.
The mean and the covariance of this posterior can be obtained

analogously to the regression case giving

61

t

D 61 C

2

Xi X T
i

X

"

i

61 C


X

 t
i


(cid:181)

i

Jaakkola and Jordan

#



(36)

Xi

(37)

St
i

 1
2

t D 6t

The variational parameters  t
i associated with each observation
and the conditional model can be updated using equation (12)
where X is replaced with Xi , now the vector of parameters
associated with the ith conditional model.

We can solve the M-step of the EM-algorithm by accumu-
lating sufcient statistics for the parameters Xi ; ; 6 based on
the closed form posterior distributions corresponding to the ob-
servations in the data set. Omitting the algebra, we obtain the
following explicit updates for the parameters:

6t

t

X
X

T

t

1
i bi



6  1
T

  1
T
Xi  A
X

X

2

 t
i



t

6t C t T


t

 1=2

St
i

t

(38)

(39)

(40)

(41)

(42)



where

Ai D

bi D

t

and the subscript t denotes the quantities pertaining to the obser-
vation Dt . Note that since the variational transformations that
we expoited to arrive at these updates are all lower bounds, the
M-step necessarily results in a monotonically increasing lower
bound on the log-probability of the observations. This desirable
monotonicity property is unlikely to arise with other types of
approximation methods, such as the Laplace approximation.

7. Discussion

We have exemplied the use of variational techniques in the set-
ting of Bayesian parameter estimation. We found that variational
methods can be exploited to yield closed form expressions that
approximate the posterior distributions for the parameters in lo-
gistic regression. The methods apply immediately to a Bayesian
treatment of logistic belief networks with complete data. We also
showed how to combine mean eld theory with our variational
transformation and thereby treat belief networks with missing
data. Finally, our variational techniques lead to an exactly solv-
able EM algorithm for a latent variable density model  the dual
of the logistic regression problem.

Bayesian parameter estimation

It is also of interest to note that our variational method pro-
vides an alternative to the standard iterative Newton-Raphson
method for maximum likelihood estimation in logistic regres-
sion (an algorithm known as iterative reweighted least squares
or IRLS). The advantage of the variational approach is that it
guarantees monotone improvement in likelihood. We present the
derivation of this algorithm in Appendix C.

Finally, for an alternative perspective on the application of
variational methods to Bayesian inference, see Hinton and van
Camp (1993) and MacKay (1997). These authors have devel-
oped a variational method known as ensemble learning, which
can be viewed as a mean eld approximation to the marginal
likelihood.

Appendix A: Optimization of the variational
parameters

To optimize the variational approximation of equation (9) in the
context of an observation D D fS; X1; : : : ; Xrg we formulate
an EM algorithm to maximize the predictive likelihood of this
Z
observation with respect to  . In other words, we nd  that
maximizes the right hand side of

Z

P(S j X; (cid:181)) P((cid:181)) d(cid:181) 

P(S j X; (cid:181); )P((cid:181)) d(cid:181)

(43)

In the EM formalism this is achieved by iteratively maximizing
the expected complete log-likelihood given by
Q( j  old) D Eflog P(S j X; (cid:181); )P((cid:181))g

(44)
where the expectation is over P((cid:181) j D;  old). Taking the deriva-
tive of Q with respect to  and setting it to zero leads to

@

@

Q( j  old) D  @( )

@

[E((cid:181) T X)2   2] D 0

(45)

As ( ) is a monotonically decreasing function3 the maximum
is obtained at

 2 D E((cid:181) T X)2

(46)

By substituting  for  old above, the procedure can be repeated.
Each such iteration yields a better approximation in the sense of
equation (43).

separately. Thus

P((cid:181)i j Dt ; q) /

"Y

P

S0

#



Si

 S(i); (cid:181)i

q(S

0

)

35

P((cid:181)i )

(47)

The form of this posterior, however, remains at least as unwieldy
as the Bayesian logistic regression problem considered earlier
in the paper. Proceeding analogously, we transform the logis-
tic functions as in equation (7) corresponding to each of the
conditional probabilities in the product and obtain

P((cid:181)i j Dt ; q; i ) /

)

P((cid:181)i )

(48)

"Y



Si

P

S0

#

0

 S(i); (cid:181)i ; i
q(S
#
oq(S
#

 2
i )

Si

)

0

g(i ) e(HSi

i )=2(i )(H 2

P((cid:181)i )

(49)

n

"Y
"


S0

D

D

D

g(i ) e6S0 q(S

0

)f(HSi

i )=2(i )(H 2

 2
i )g

Si

P((cid:181)i )

(50)



gi )=2(i )(EfH 2

g 2
i )

Si

 P




g(i ) e(EfHSi

 S(i); (cid:181)i ; i ; q

Si
D (2Si  1) (cid:181) T

P((cid:181)i )

P((cid:181)i )

(51)

(52)

i S(i); S(i) is the vector of parents of
where HSi
Si , and the expectations are with respect to the variational distri-
bution q. For simplicity, we have not let the variational parameter
i vary independently with the congurations of the missing val-
ues but assumed it to be the same for all such congurations.
This choice is naturally suboptimal but is made here primarily
for notational simplicity (the choice may also be necessary in
cases where the number of missing values is large). Now, since
HSi is linear in the parameters (cid:181)i , the exponent in equation (51)
consisting of averages over HSi and its square with respect to
the variational distribution q, stays at most quadratic in the
parameters (cid:181)i . A multivariate Gaussian prior will be conjugate
to this likelihood, and therefore the posterior will also be Gaus-
sian. The mean posi and covariance 6posi of such posterior are
given by (we omit the algebra)

C 2(i )E
i C E
61

D 61

(cid:181)

(cid:190)

61
posi

S(i) ST

D 6posi



posi

S(i)

(53)

(54)

'



(i)

i

i

Si  1
2

; : : : ; St
n

Appendix B: Parameter posteriors
To ll-in the possible missing values in an observation Dt D
g we employ the q-transformations described in the
fSt
text. As a result, the joint distribution after the approximate
marginalization factorizes as with complete observations. Thus
the posterior distributions for the parameters remain independent
across the different conditional models and can be computed

i

Note that this posterior depends both on the distribution q and
the parameters  . The optimization of these parameters is shown
in Appendix B.1.

B.1. Optimization of the variational parameters

We have introduced two variational parameters: the distribu-
tion q over the missing values, and the  parameters correspond-
ing to the logistic or  -transformations. The metric for optimiz-
ing the parameters comes from the fact that the transformations

Jaakkola and Jordan
P(Si j S(i); (cid:181)i ; i ; q), into the above denition of the Q function.
For clarity we will omit all the terms with no dependence on the
variational distribution q. We obtain:

(

'



X

Q(q j qold) D

)
)

'
 (i )Eq

H 2
Si

'

 (i )E(cid:181)i

H 2
Si

(60)

(61)

Eq

E(cid:181)i

HSi
2

i

(


'
C log C(q) C 
X

HSi
2
CH(q) C 

E(cid:181)i

i

D Eq

where Eq refers to the expectation with respect to the variational
distribution q. The second equation follows by exchanging the
order of the (mutually independent) expectations E(cid:181)i and Eq.
We have also used the fact that log C(q) is the entropy H(q) of q
i S(i), where
(see the text). Recall the notation HSi
S(i), is a binary vector of parents of Si . Before proceeding to
maximize the Q function with respect to q, we explicate the
averages E(cid:181)i in the above formula:

D (2Si  1)(cid:181) T

'
'

E(cid:181)i

E(cid:181)i

HSi

H 2
Si

 D (2Si  1)T
 D
2 C ST

pi S(i)

T

pi S(i)

(i)

(62)

(63)

6 pi S(i)

Here  pi and 6 pi are the mean and the covariance, respectively,
of the posterior P((cid:181)i j i ; qold) associated with the ith conditional
model. Simply inserting these back into the expression for the
2
Q function we get
Q(q j qold) D Eq

((cid:181)

X





T

T

pi S(i)

Si  1
2

pi S(i)  (i )
)

C H(q) C 

 (i )ST

(i)

6 pi S(i)

Now, some of the binary variables Si have a value assignment
based on the observation Dt and the remaining variables will be
averaged over the variational distribution q. Assuming no a priori
constraints on the form of the q distribution, the maximizing q
is the Boltzmann distribution (see e.g. Parisi 1988):

X

(cid:181)

0

) / exp

q(S

Si  1
2

T

pi S(i)  (i )

T

pi S(i)




(cid:190)!

i

i

36

associated with these parameters introduce a lower bound on
the probability of the observations. Thus by maximizing this
lower bound we nd the parameter values that yield the most
accurate approximations. We therefore attempt to maximize the
right hand side of

log P(Dt )  log P(Dt j ; q)


Z
Z
P(Dt j (cid:181); ; q)P((cid:181)) d(cid:181)
Y

 S(i); (cid:181)i ; i ; q

D log

D log

Si

P
C log C(q)

i



P((cid:181)i ) d(cid:181)i

(55)

(56)

(57)

where Dt contains the observed settings of the variables. We have
used the fact that the joint distribution under our approximations
factorizes as with complete cases. Similarly to the case of the
simple Bayesian logistic regression considered previously (see
Appendix A), we can devise an EM-algorithm to maximize the
variational lower bound with respect to the parameters q and  ;
the parameters (cid:181) can be considered as latent variables in this
formulation. The E-step of the EM-algorithm, i.e., nding the
posterior distribution over the latent variables, has already been
described in Appendix B. Here we will consider in detail only
the M-step. For simplicity, we solve the M-step in two phases:
the rst where the variational distribution is kept xed and the
maximization is over  , and the second where these roles are
reversed. We start with the rst phase.

As the variational joint distribution factorizes, the problem
of nding the optimal  parameters separates into independent
problems concerning each of the transformed conditionals. Thus
the optimization becomes analogous to the simple Bayesian
logistic regression considered earlier. Two differences exist: rst,
the posterior over each (cid:181)i is now obtained from equation (52);
second, we have an additional expectation with respect to the
variational distribution q. With these differences the optimiza-
tion is analogous to the one presented in Appendix A above and
we wont repeat it here.

The latter part of our two-stage M-step is new, however, and
we will consider it in detail. The objective is to optimize q while
keeping the  parameters xed to their previously obtained val-
ues. Similarly to the  case we construct an EM-algorithm to
perform this inner loop optimization:

Q(q j qold) D E(cid:181)flog P(Dt ; (cid:181) j ; q)g



Si

 S(i); (cid:181)i ; i ; q





P((cid:181)i )

X

D

'

i

E(cid:181)i

log P
C log C(q)

(59)
where the rst expectation is with respect to P((cid:181) j ; qold), which
factorizes across the conditional probabilities as explained pre-
viously; the expectations E(cid:181)i are over the component distribu-
tions P((cid:181)i j i ; qold), obtained directly from equation (52). Let us
now insert the form of the transformed conditional probabilities,

(64)

2

(65)

(58)

 (i )ST

(i)

6 pi S(i)

Whenever the variational distribution is constrained, however,
such as in the case of a completely factorized distribution, we
may no longer expect to nd the q that maximizes equation (64).
Nevertheless, a locally optimal solution can be found by, for
example, sequentially solving

Q(q j qold) D 0

@
@qk

(66)

Bayesian parameter estimation
with respect to each of the components qk D qk(Sk D 1) in the
factorized distribution.

37
3. This holds for   0. However, since P(S j X; (cid:181); ) is a symmetric
function of  , assuming   0 has no effect on the quality of the
approximation.

Appendix C: Technical note: ML estimation

