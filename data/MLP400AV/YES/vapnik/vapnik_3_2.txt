Abstract

1, . . . , x

We introduce an algorithm for estimating the values of a function
at a set of test points x
m given a set of training points
(x1, y1), . . . , (x, y) without estimating (as an intermediate step)
the regression function. We demonstrate that this direct (trans-
ductive) way for estimating values of the regression (or classica-
tion in pattern recognition) is more accurate than the traditional
one based on two steps, rst estimating the function and then cal-
culating the values of this function at the points of interest.

1 Introduction

Following [6] we consider a general scheme of transductive inference. Find an algo-
rithm A that using both the given set of training data

(1)

(2)

(3)

(4)

and the given set of test data

((x1, y1), . . . (x, y))

(x

1, . . . , x

m)

selects from a set of functions {x 7 f (x)} a function

y = f (x) = fA(x|x1, y1, . . . , x, y, x

1, . . . , x

m)

and provides the minimum to the functional

R(A) = E  m
Xi=1

(y

i  fA(x

i |x1, y1, . . . , x, y, x

1, . . . , x

m))2! .

In (4) expectation is taken over (x1, y1), . . . , (x, y), (x
m) and the
pairs (xi, yi) and (x
i ) are both drawn independently according to the same xed
but unknown distribution F (x, y). For the training data we are given vector x and
the value y, for the test data we are only given x.

1), . . . , (x

m, y

i , y

1, y

Usually, the problem of estimating values of a function at points of interest is
solved in two steps: rst in a given set of functions f (x, ),    one estimates
the regression, i.e the function which minimizes the functional

R() =Z ((y  f (x, ))2dF (x, y),

(5)

(the inductive step) and then using the estimated function y = f (x, ) we calculate
the values at points of interest

y
i = f (x

i , )

(6)

(the deductive step).

Note, however, that the estimation of a function is equivalent to estimating its val-
ues in the continuum points of the domain of the function. Therefore, by solving
the regression problem using a restricted amount of information, we are looking
for a more general solution than is required.
In [6] it is shown that using a di-
rect estimation method one can obtain better bounds than through the two step
procedure.

In this article we develop the idea introduced in [5] for estimating the values of a
function only at the given points.

The material is organized as follows. In Section 1 we consider the classical (induc-
tive) Ridge Regression procedure, and the leaveoneout technique which is used to
measure the quality of its solutions. Section 2 introduces the transductive method
of inference for estimation of the values of a function based on this leaveoneout
technique.
In Section 3 experiments which demonstrate the improvement given
by transductive inference compared to inductive inference (in both regression and
pattern recognition) are presented. Finally, Section 4 summarizes the results.

2 Ridge Regression and the LeaveOneOut procedure

In order to describe our transductive method, let us rst discuss the classical two-
step (inductive plus deductive) procedure of Ridge Regression. Consider the set of
functions linear in their parameters

n

f (x, ) =

ii(x).

(7)

To minimize the expected loss (5), where F (x, y) is unknown, we minimize the
following empirical functional (the socalled Ridge Regression functional [1])

Xi=1

Remp() =

1




Xi=1

(yi  f (xi, ))2 + ||||2

(8)

where  is a xed positive constant, called the regularization parameter. The min-
imum is given by the vector of coecients

 = (x1, y1, . . . , x, y) = (K T K + I)1K T Y

where

Y = (y1, . . . , y)T ,

and K is a matrix with elements:

Kij = j(xi),

i = 1, . . . , ,

j = 1, . . . , n.

(9)

(10)

(11)

n

The problem is to choose the value  which provides small expected loss for training
on a sample S = {(x1, y1), . . . , (x, y)}.

For this purpose, we would like to choose  such that f minimizing (8) also mini-
mizes

R =Z (y  f(x|S))2dF (x, y)dF S.

(12)

Since F (x, y) is unknown one cannot estimate this minimum directly. To solve this
problem we instead use the leaveoneout procedure, which is an unbiased estimator
of (12). The leaveoneout error of an algorithm on the training sample S is

Tloo() =

1




Xi=1

(yi  f(xi|S \ (xi, yi))2 .

(13)

The leaveoneout procedure consists of removing from the training data one el-
ement (say (xi, yi)), constructing the regression function only on the basis of the
remaining training data and then testing the removed element. In this fashion one
tests all  elements of the training data using  dierent decision rules. The mini-
mum over  of (13) we consider as the minimum over  of (12) since the expectation
of (13) coincides with (12) [2].

For Ridge Regression, one can derive a closed form expression for the leaveoneout
error. Denoting

A1

 = (K T K + I)1

(14)

(15)

(16)

(17)

(18)

the error incurred by the leaveoneout procedure is [6]

Tloo() =

1




Xi=1  yi  kT

1  kT

i A1
i A1

 K T Y

 ki !2

where

kt = (1(xt) . . . , n(xt))T .

Let  = 0 be the minimum of (15). Then the vector

Y 0 = K (K T K + 0I)1K T Y

where

K  =


(x
1)
...
1(x

m)

. . . n(x
1)
...
. . . n(x

m)




is the Ridge Regression estimate of the unknown values (y

1, . . . , y

m).

3 LeaveOneOut Error for Transductive Inference

In transductive inference, our goal is to nd an algorithm A which minimizes the
functional (4) using both the training data (1) and the test data (2). We suggest
the following method: predict (y
m) by nding those values which minimize
the leaveoneout error of Ridge Regression training on the joint set

1, . . . , y

(x1, y1), . . . , (x, y), (x

1, y

1), . . . , (x

m, y

m).

(19)

This is achieved in the following way. Suppose we treat the unknown values
(y
m) as variables and for some xed value of these variables we minimize
the following empirical functional

1, . . . , y

Remp(|y

1, . . . , y

m) =

(yi  f (xi, ))2 +

1

 + m  
Xi=1

(y

i  f (x

i , ))2! + ||||2.

m

Xi=1

(20)
This functional diers only in the second term from the functional (8) and corre-
sponds to performing Ridge Regression with the extra pairs (x
m).

1), . . . , (x

m, y

1, y

Suppose that vector Y  = (y
the pairs

1, . . . , y

m) is taken from some set Y   Y such that

(x

1, y

1), . . . , (x

(21)
can be considered as a sample drawn from the distribution F (x, y). In this case the
leaveoneout error of minimizing (20) over the set (19) approximates the functional
(4). We can measure this leaveoneout error using the same technique as in Ridge
Regression. Using the closed form (15) one obtains

m)

m, y

Tloo(|y

1, . . . , y

m) =

1




Xi=1  yi  kT

1  kT
i

i

A1

A1

K T Y

 ki !2

.

(22)

where we denote x = (x1, . . . , x, x

1, . . . , x

m), Y = (y1, . . . , y, y

1, . . . , y

m)T , and

A1

 = ( K T K + I)1

Kij = j(xi),

i = 1, . . . ,  + m,

j = 1, . . . , n.

kt = (1(xt) . . . , n(xt))T .

(23)

(24)

(25)

Now let us rewrite the expression (22) in an equivalent form to separate the terms
with Y from the terms with x. Introducing

and the matrix M with elements

C = I  KA1


K T ,

Mij =

+m

Xk=1

CikCkj
2
Ckk

we obtain the equivalent expression of (22)

Tloo(|y

1, . . . , y

m) =

( Y T M Y ).

1


(26)

(27)

(28)

In order for the Y  which minimize the leaveoneout procedure to be valid it is
required that the pairs (x, y) are drawn according to the distribution F (x, y). To
satisfy this constraint we choose vectors Y  from the set

Y = {Y  : ||Y   Y 0||  R}

where the vector Y 0 is the solution obtained from classical Ridge Regression.

To minimize (28) under constraint (29) we use the functional

T  
loo() = Y T M Y + ||Y   Y 0||2

where  is a constant depending on R.

(29)

(30)

Now, to nd the values at the given points of interest (2) all that remains is to
nd the minimum of (30) in Y . Note that the matrix M is obtained using only
the vectors x and x. Therefore, to nd the minimum of this functional we rewrite
Equation (30) as

T  
loo() = Y T M0Y + 2Y T M1Y + Y T M2Y  + ||Y   Y 0||2

where

M =(cid:12)(cid:12)(cid:12)(cid:12)

M0 M1
M T

1 M2 (cid:12)(cid:12)(cid:12)(cid:12)

(31)

(32)

and M0 is a    matrix, M1 is a   m matrix and M2 is a m  m matrix. Taking
the derivative of (31) in Y  we obtain the condition for the solution

2M1Y + 2M2Y   2Y 0 + 2Y  = 0

which gives the predictions

Y  = (I + M2)1(M1Y + Y 0).

(33)

(34)

In this algorithm (which we will call Transductive Regression) we have two param-
eters to control:  and . The choice of  can be found using the leave-one-out
estimator (15) for Ridge Regression. This leaves  as the only free parameter, the
choice of which can be made according to the variance of the predictions Y 0.

4 Experiments

To compare our onestep transductive approach with the classical twostep ap-
proach we performed a series of experiments on regression problems. We also de-
scribe experiments applying our technique to the problem of pattern recognition.

4.1 Regression

We conducted computer simulations for the regression problem using two datasets
from the DELVE repository: boston and kin-32fh.

The boston dataset is a wellknown problem where one is required to estimate
house prices according to various statistics based on 13 locational, economic and
structural features from data collected by the U.S Census Service in the Boston
Massachusetts area.

The kin-32fh dataset is a realistic simulation of the forward dynamics of an 8 link
all-revolute robot arm. The task is to predict the distance of the end-eector from
a target, given 32 inputs which contain information on the joint positions, twist
angles and so forth.

Both problems are nonlinear and contain noisy data. Our objective is to com-
pare our transductive inference method directly with the inductive method of
Ridge Regression. To do this we chose the set of basis functions i(x) =

Regression which minimized the leaveoneout bound (15). We then used the same
values of these parameters in our transductive approach, and using the basis func-

exp(cid:0)||x  xi||2/22(cid:1), i = 1, . . . , , and found the values of  and  for Ridge
tions i(x) = exp(cid:0)||x  xi||2/22(cid:1) , i = 1, . . . ,  + m, we then chose a xed value

of .

For the boston dataset we followed the same experimental setup as in [4], that
is, we partitioned the training set of 506 observations randomly 100 times into a
training set of 481 observations and a testing set of 25 observations. We chose the
values of  and  by taking the minimum average leaveoneout error over ve more
random splits of the data stepping over the parameter space. The minimum was
found at  = 0.005 and log  = 0.7. For our transductive method, we also chose
the parameter  = 10. In Figure 1a we plot mean squared error (MSE) on the test
set averaged over the 100 runs against log  for Ridge Regression and Transductive
Regression. Transductive Regression outperforms Ridge Regression, especially at
the minimum.

To observe the inuence of the number of test points m on the generalization ability
of our transductive method, we ran further experiments, setting  = /2m for

Transductive Regression
Ridge Regression

0.4

0.6

0.8
Log sigma
(a)

1

1.2

Transductive Regression
Ridge Regression

r
o
r
r

E


t
s
e
T

9.2

9

8.8

8.6

8.4

8.2

8

7.8

7.6

0.15

0.14

0.13

0.12

0.11

0.1

r
o
r
r

E


t
s
e
T

0.09
1

1.5

2.5

3

2
Log sigma
(c)

8

7.95

7.9

r
o
r
r

E


t
s
e
T

7.85

7.8

7.75

7.7

7.65

r
o
r
r

E


t
s
e
T

0.14

0.135

0.13

0.125

0.12

0.115

0.11

0.105

0.1

0.095

0.09

Transductive Regression
Ridge Regression

5

10

15
Test Set Size
(b)

20

25

Transductive Regression
Ridge Regression

50

100

150
Test Set Size
(d)

200

250

Figure 1: A comparison of Transductive Regression to Ridge Regression on the
boston dataset: (a) error rates for varying , (b) varying the test set size, m, and
on the kin-32fh dataset: (c) error rates for varying , (d) varying the test set size.

dierent values of m.
In Figure 1b we plot m against MSE on the testing set,
at log  = 0.7. The results clearly indicate that increasing the test set size gives
improved performance in Transductive Regression. For Ridge Regression, of course,
the size of the testing set has no inuence on the generalization ability.

We then performed similar experiments on the kin-32fh dataset. This time, as we
were interested in large testing sets giving improved performance for Transductive
Regression we chose 100 splits where we took a subset of only 64 observations for
training and 256 for testing. Again the leaveoneout estimator was used to nd the
values  = 0.1 and log  = 2 for Ridge Regression, and for Transductive Regression
we also chose the parameter  = 0.1. We plotted MSE on the testing set against
log  (Figure 1c) and the size of the test set m for log  = 2.75 (also,  = 50/m)
(Figure 1d) for the two algorithms. For large test set sizes our method clearly
outperforms Ridge Regression.

4.2 Pattern Recognition

This technique can also be applied for pattern recognition problems by solving them
based on minimizing functional (8) with y = 1. Such a technique is known as a

Postal
Banana
Diabetes
Titanic
Breast Cancer
Heart
Thyroid

AB ABR




12.3
26.5
22.6
30.4
20.3
4.4

10.9
23.8
22.6
26.5
16.6
4.6

SVM TLD
4.7
5.5
11.4
11.5
23.3
23.5
22.4
22.4
26.0
25.7
15.7
16.0
4.8
4.0

Table 1: Comparison of percentage test error of AdaBoost (AB), Regularized Ad-
aBoost (ABR), Support Vector Machines (SVM) and Transductive Linear Discrim-
ination (TLD) on seven datasets.

Linear Discriminant (LD) technique.

Table 1 describes results of experiments on classication in the following problems:
2 class digit recognition (0  4 versus 5  9) splitting the training set into 23 runs
of 317 observations and considering a testing set of 2000 observations, and six
problems from the UCI database. We followed the same experimental setup as
in [3]: the performance of a classier is measured by its average error over one
hundred partitions of the datasets into training and testing sets. Free parameter(s)
are chosen via validation on the rst ve training datasets. The performance of the
transductive LD technique was compared to Support Vector Machines, AdaBoost
and Regularized AdaBoost [3].

It is interesting to note that in spite of the fact that LD technique is one of the sim-
plest pattern recognition techniques, transductive inference based upon this method
performs well compared to state of the art methods of pattern recognition.

5 Summary

In this article we performed transductive inference in the problem of estimating
values of functions at the points of interest. We demonstrate that estimating the
unknown values via a onestep (transductive) procedure is more accurate than the
traditional twostep (inductive plus deductive) one.

