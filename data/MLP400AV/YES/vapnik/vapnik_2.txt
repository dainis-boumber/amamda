Abstract

New functionals for parameter (model) selection of Support Vector Ma-
chines are introduced based on the concepts of the span of support vec-
tors and rescaling of the feature space. It is shown that using these func-
tionals, one can both predict the best choice of parameters of the model
and the relative quality of performance for any value of parameter.

1

Introduction

Support Vector Machines (SVMs) implement the following idea : they map input vectors
into a high dimensional feature space, where a maximal margin hyperplane is constructed
[6]. It was shown that when training data are separable, the error rate for SVMs can be
characterized by

h = R2/M 2,

(1)

where R is the radius of the smallest sphere containing the training data and M is the mar-
gin (the distance between the hyperplane and the closest training vector in feature space).
This functional estimates the VC dimension of hyperplanes separating data with a given
margin M.
To perform the mapping and to calculate R and M in the SVM technique, one uses a
positive denite kernel K(x, x) which species an inner product in feature space. An
example of such a kernel is the Radial Basis Function (RBF),

K(x, x

) = e||xx

||2/22

.

This kernel has a free parameter  and more generally, most kernels require some param-
eters to be set. When treating noisy data with SVMs, another parameter, penalizing the
training errors, also needs to be set. The problem of choosing the values of these parame-
ters which minimize the expectation of test error is called the model selection problem.

It was shown that the parameter of the kernel that minimizes functional (1) provides a good
choice for the model : the minimum for this functional coincides with the minimum of the
test error [1]. However, the shapes of these curves can be different.

In this article we introduce rened functionals that not only specify the best choice of
parameters (both the parameter of the kernel and the parameter penalizing training error),
but also produce curves which better reect the actual error rate.

The paper is organized as follows. Section 2 describes the basics of SVMs, section 3
introduces a new functional based on the concept of the span of support vectors, section 4
considers the idea of rescaling data in feature space and section 5 discusses experiments of
model selection with these functionals.

2 Support Vector Learning

We introduce some standard notation for SVMs; for a complete description, see [6]. Let
(xi, yi)1i be a set of training examples, xi  Rn which belong to a class labeled by
yi  {1, 1}. The decision function given by a SVM is :

f (x) = sgn  
Xi=1

0

i yiK(xi, x) + b! ,

where the coefcients 0

i are obtained by maximizing the following functional :

W () =



Xi=1

i 

1
2



Xi,j=1

ijyiyjK(xi, xj)

under constraints



(2)

(3)

iyi = 0 and 0  i  C i = 1, ..., .

Xi=1

C is a constant which controls the tradeoff between the complexity of the decision function
and the number of training examples misclassied. SVM are linear maximal margin clas-
siers in a high-dimensional feature space where the data are mapped through a non-linear
function (x) such that (xi)  (xj) = K(xi, xj).
The points xi with i > 0 are called support vectors. We distinguish between those with
0 < i < C and those with i = C. We call them respectively support vectors of the rst
and second category.

3 Prediction using the span of support vectors

The results introduced in this section are based on the leave-one-out cross-validation esti-
mate. This procedure is usually used to estimate the probability of test error of a learning
algorithm.

3.1 The leave-one-out procedure

The leave-one-out procedure consists of removing from the training data one element, con-
structing the decision rule on the basis of the remaining training data and then testing the
removed element. In this fashion one tests all  elements of the training data (using  dif-
ferent decision rules). Let us denote the number of errors in the leave-one-out procedure
by L(x1, y1, ..., x, y). It is known [6] that the the leave-one-out procedure gives an al-
most unbiased estimate of the probability of test error : the expectation of test error for the
machine trained on   1 examples is equal to the expectation of 1
We now provide an analysis of the number of errors made by the leave-one-out procedure.
For this purpose, we introduce a new concept, called the span of support vectors [7].

L(x1, y1, ..., x, y).

3.2 Span of support vectors

Since the results presented in this section do not depend on the feature space, we will
consider without any loss of generality, linear SVMs, i.e. K(xi, xj) = xi  xj.
Suppose that 
n) is the solution of the optimization problem (3).
For any xed support vector xp we dene the set p as constrained linear combinations of
the support vectors of the rst category (xi)i6=p :

1, ..., 0

0 = (0

i = 1, 0  0

i + yiyp0

.

(4)

pi  C




p =
X{i6=p/ 0<0


ixi,



Xi=1, i6=p

i <C}

Note that i can be less than 0.
We also dene the quantity Sp, which we call the span of the support vector xp as the
minimum distance between xp and this set (see gure 1)

S 2
p = d2(xp, p) = min
xp

(xp  x)2.

(5)



1


2 = +inf
 = -inf

3

x 2






x 1











x 3




2 = -1
3 = 2

Figure 1: Three support vectors with 1 = 2 = 3/2. The set 1 is the semi-opened
dashed line.

It was shown in [7] that the set p is not empty and that Sp = d(xp, p)  DSV , where
DSV is the diameter of the smallest sphere containing the support vectors.
Intuitively, the smaller Sp = d(xp, p) is, the less likely the leave-one-out procedure is to
make an error on the vector xp. Formally, the following theorem holds :

Theorem 1 [7] If in the leave-one-out procedure a support vector xp corresponding to
0 < p < C is recognized incorrectly, then the following inequality holds

0
p 

1

Sp max(D, 1/C)

.

This theorem implies that in the separable case (C = ),
the number of errors
made by the leave-one-out procedure is bounded as follows : L(x1, y1, ..., x, y) 
p = 1/M 2 [6]. This is already an
Pp 0
improvement compared to functional (1), since Sp  DSV . But depending on the geome-
try of the support vectors the value of the span Sp can be much less than the diameter DSV
of the support vectors and can even be equal to zero.

p maxp SpD = maxp SpD/M 2, because P 0

We can go further under the assumption that the set of support vectors does not change
during the leave-one-out procedure, which leads us to the following theorem :

Theorem 2 If the sets of support vectors of rst and second categories remain the same
during the leave-one-out procedure, then for any support vector xp, the following equality
holds :

yp(f 0(xp)  f p(xp)) = 0
where f 0 and f p are the decision function (2) given by the SVM trained respectively on the
whole training set and after the point xp has been removed.

pS 2
p

The proof of the theorem follows the one of Theorem 1 in [7].

The assumption that the set of support vectors does not change during the leave-one-out
procedure is obviously not satised in most cases. Nevertheless, the proportion of points
which violate this assumption is usually small compared to the number of support vec-
tors. In this case, Theorem 2 provides a good approximation of the result of the leave-one
procedure, as pointed out by the experiments (see Section 5.1, gure 2).
As already noticed in [1], the larger p is, the more important in the decision function the
support vector xp is. Thus, it is not surprising that removing a point xp causes a change in
the decision function proportional to its Lagrange multiplier p. The same kind of result as
Theorem 2 has also been derived in [2], where for SVMs without threshold, the following
pK(xp, xp). The span Sp takes
inequality has been derived : yp(f 0(xp)  f p(xp))  0
into account the geometry of the support vectors in order to get a precise notion of how
important is a given point.

The previous theorem enables us to compute the number of errors made by the leave-one-
out procedure :

Corollary 1 Under the assumption of Theorem 2, the test error prediction given by the
leave-one-out procedure is

t =

1
L(x1, y1, ..., x, y) =

1


Card{p/ 0

pS 2

p  ypf 0(xp)}

(6)

Note that points which are not support vectors are correctly classied by the leave-one-out
procedure. Therefore t denes the number of errors of the leave-one-out procedure on the
entire training set.
Under the assumption in Theorem 2, the box constraints in the denition of p (4) can
be removed. Moreover, if we consider only hyperplanes passing through the origin, the

constraintP i = 1 can also be removed. Therefore, under those assumptions, the com-

putation of the span Sp is an unconstrained minimization of a quadratic form and can be
done analytically. For support vectors of the rst category, this leads to the closed form
Sp = 1/(K 1
SV )pp, where KSV is the matrix of dot products between support vectors of the
rst category. A similar result has also been obtained in [3].

In Section 5, we use the span-rule (6) for model selection in both separable and non-
separable cases.

4 Rescaling

As we already mentioned, functional (1) bounds the VC dimension of a linear margin clas-
sier. This bound is tight when the data almost lls the surface of the sphere enclosing
the training data, but when the data lie on a at ellipsoid, this bound is poor since the radius
of the sphere takes into account only the components with the largest deviations. The idea
we present here is to make a rescaling of our data in feature space such that the radius of the
sphere stays constant but the margin increases, and then apply this bound to our rescaled
data and hyperplane.

Let us rst consider linear SVMs, i.e. without any mapping in a high dimensional space.
The rescaling can be achieved by computing the covariance matrix of our data and rescaling
according to its eigenvalues. Suppose our data are centered and let (1, . . . , n) be the
normalized eigenvectors of the covariance matrix of our data. We can then compute the
smallest enclosing box containing our data, centered at the origin and whose edges are
parallels to (1, . . . , n). This box is an approximation of the smallest enclosing ellipsoid.
The length of the edge in the direction k is k = maxi |xi  k|. The rescaling consists
of the following diagonal transformation :

D : x  Dx =Xk

k(x  k) k.

Let us consider xi = D1
xi and w = Dw. The decision function is not changed under
this transformation since w  xi = w  xi and the data xi ll a box of side length 1. Thus,
in functional (1), we replace R2 by 1 and 1/M 2 by w
2. Since we rescaled our data in a
box, we actually estimated the radius of the enclosing ball using the -norm instead of
the classical 2-norm. Further theoretical works needs to be done to justify this change of
norm.

In the non-linear case, note that even if we map our data in a high dimensional feature space,
they lie in the linear subspace spanned by these data. Thus, if the number of training data 
is not too large, we can work in this subspace of dimension at most . For this purpose, one
can use the tools of kernel PCA [5] : if A is the matrix of normalized eigenvectors of the
Gram matrix Kij = K(xi, xj) and (i) the eigenvalues, the dot product xi k is replaced
transformation A and nally functional (1) becomes

by kAik and w  k becomes kPi Aikyii. Thus, we can still achieve the diagonal

2
k max

A2

Aikyii)2.

Xk

i

ik(Xi

5 Experiments

To check these new methods, we performed two series of experiments. One concerns the
choice of , the width of the RBF kernel, on a linearly separable database, the postal
database. This dataset consists of 7291 handwritten digit of size 16x16 with a test set
of 2007 examples. Following [4], we split the training set in 23 subsets of 317 training
examples. Our task consists of separating digit 0 to 4 from 5 to 9. Error bars in gures 2a
and 4 are standard deviations over the 23 trials. In another experiment, we try to choose
the optimal value of C in a noisy database, the breast-cancer database1. The dataset has
been split randomly 100 times into a training set containing 200 examples and a test set
containing 77 examples.

Section 5.1 describes experiments of model selection using the span-rule (6), both in the
separable case and in the non-separable one, while Section 5.2 shows VC bounds for model
selection in the separable case both with and without rescaling.

5.1 Model selection using the span-rule

In this section, we use the prediction of test error derived from the span-rule (6) for model
selection. Figure 2a shows the test error and the prediction given by the span for differ-
ent values of the width  of the RBF kernel on the postal database. Figure 2b plots the
same functions for different values of C on the breast-cancer database. We can see that
the method predicts the correct value of the minimum. Moreover, the prediction is very
accurate and the curves are almost identical.

1Available from http://horn.first.gmd.de/raetsch/data/breast-cancer

35

30

25

r
o
r
r

E

20

15

10

5
6

Span prediction

4

2

0

Log sigma

2

4

6

36

34

32

30

r
o
r
r

E

28

26

24

22

20
2

Test error
Span prediction

0

2

4

Log C

6

8

10

12

(a) choice of  in the postal database

(b) choice of C in the breast-cancer database

Figure 2: Test error and its prediction using the span-rule (6).

The computation of the span-rule (6) involves computing the span Sp (5) for every support
vector. Note, however, that we are interested in the inequality S 2
p, rather
than the exact value of the span Sp. Thus, while minimizing Sp = d(xp, p), if we nd a
p, we can stop the minimization because
point x  p such that d(xp, x)2  ypf (xp)/0
this point will be correctly classied by the leave-one-out procedure.

p  ypf (xp)/0

Figure 3 compares the time required to (a) train the SVM on the postal database, (b) com-
pute the estimate of the leave-one-out procedure given by the span-rule (6) and (c) compute
exactly the leave-one-out procedure. In order to have a fair comparison, we optimized the
computation of the leave-one-out procedure in the following way : for every support vector
xp, we take as starting point for the minimization (3) involved to compute f p (the decision
function after having removed the point xp), the solution given by f 0 on the whole training
set. The reason is that f 0 and f p are usually close.
The results show that the time required to compute the span is not prohibitive and is very
attractive compared to the leave-one-out procedure.

c
e
s

n
i

e
m
T

i

120

100

80

60

40

20

0
6

Training
Leaveoneout
Span

4

2

0

Log sigma

2

4

6

Figure 3: Comparison of time required for SVM training, computation of span and leave-
one-out on the postal database

5.2 VC dimension with rescaling

In this section, we perform model selection on the postal database using functional (1) and
its rescaled version. Figure 4a shows the values of the classical bound R2/M 2 for different
values of . This bound predicts the correct value for the minimum, but does not reect the

actual test error. This is easily understandable since for large values of , the data in input
space tend to be mapped in a very at ellipsoid in feature space, a fact which is not taken
into account [4]. Figure 4b shows that by performing a rescaling of our data, we manage
to have a much tighter bound and this curve reects the actual test error, given in gure 2a.

i



m
d
C
V
g
o
L



10

9.5

9

8.5

8

7.5

7

6.5

6

5.5
6

VC Dimension

4

2

0

Log sigma

2

(a) without rescaling

4

6

i

m
d
C
V



120

100

80

60

40

20

0
6

VC Dimension with rescaling

4

2

0

Log sigma

2

(b) with rescaling

4

6

Figure 4: Bound on the VC dimension for different values of  on the postal database. The
shape of the curve with rescaling is very similar to the test error on gure 2.

6 Conclusion

In this paper, we introduced two new techniques of model selection for SVMs. One is based
on the span, the other is based on rescaling of the data in feature space. We demonstrated
that using these techniques, one can both predict optimal values for the parameters of the
model and evaluate relative performances for different values of the parameters. These
functionals can also lead to new learning techniques as they establish that generalization
ability is not only due to margin.

Acknowledgments

The authors would like to thank Jason Weston and Patrick Haffner for helpfull discussions
and comments.

