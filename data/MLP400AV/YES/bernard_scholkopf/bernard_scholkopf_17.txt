Abstract

We explore the use of the so-called zero-norm of the parameters of linear models in learning.
Minimization of such a quantity has many uses in a machine learning context: for variable or
feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple
but practical method for achieving these goals and discuss its relationship to existing techniques
of minimizing the zero-norm. The method boils down to implementing a simple modication of
vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we
investigate which aid our discussion include variable and feature selection on biological microarray
data, and multicategory classication.

1. Introduction
In pattern recognition, many learning machines, given training data xi 2 Rn,i = 1, . . . ,m with corre-
sponding function values yi 2 f(cid:6)1g,i = 1, . . . ,m, make predictions g(x) about points x 2 Rn of the
form:

g(x) = sign(w(cid:1) x + b)

where w 2 Rn and b 2 R. The machines based on linear models include the well-known perceptron
of Rosenblatt (1958) and Support Vector Machines (SVMs), (see e.g., Vapnik, 1995), as well as
other combination methods, for example those of Freund and Schapire (1998). Perhaps because of
their limited power, linear models were not used a lot in the late 80s. At that time, Neural Networks
were very popular and the complex and non-linear decision functions they produced were appealing
compared to the simplicity and somewhat limited discrimination capabilities of linear machines.
The latter were thus forgotten for a while until Boser et al. (1992) introduced the optimal margin
classier that will be later called SVM. Like some earlier work (see for instance Mangasarian,
1965, Vapnik, 1979), the SVM connects machine learning to optimization techniques, however, the
gathering of theoretical analysis and mathematical programming was original and gave birth to the
revival of linear models in the machine learning community. In particular, the use of the kernel
trick to transform a linear machine into a non-linear one was not new, it was already introduced by
Aizerman et al. (1964), but the idea to combine it with mathematical programming was a big step.
It strengthens the interest for such machines which are now used in many applications.

c(cid:13)2003 Jason Weston, Andre Elisseeff, Bernhard Scholkopf and Mike Tipping.

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

Many generalizations of the original work of Boser, Guyon and Vapnik have been proposed.
For instance, Bradley and Mangasarian (1998) describe the following general procedure is given to
discriminate linearly separable data:
kwkp

Problem 1

min
w

p

subject to :
where kwkp =

yi (w(cid:1) xi + b) (cid:21) 1
(cid:17)1/p
(cid:16)

(cid:229) n

j=1 wp
j

is the (cid:96)p-norm of w. When p = 2, this procedure is the same as
the optimal margin method. The generalization capabilities of such linear models have been studied
by many researchers who have shown that, roughly speaking, minimizing the (cid:96)p-norm of w is good
for generalization. Such results hold for p (cid:21) 1. In this paper, we study machines that are obtained as
in Problem 1 in the extreme case when p ! 0, which we will call, in a slight abuse of terminology,1
the minimization of the zero-norm of w, the latter being dened as:

kwk0

0

= cardfwijwi 6= 0g

where card is set cardinality.

Pattern classication, especially in the context of regularization that enforces sparsity of the
weight vector, is deeply connected to the problem of feature selection (Blum and Langley, 1997, pro-
vide an introduction). In the feature selection problem one would like to select a subset of features
while preserving or improving the discriminative ability of a classier. In many supervised learning
problems2 feature selection is important for a variety of reasons: generalization performance, run-
ning time requirements, and constraints and interpretational issues imposed by the problem itself.
Minimization of the zero-norm provides a natural way of directly addressing the feature selection
and pattern classication objectives in a single optimization. However, this is achieved at the cost
of having to solve a very difcult optimization problem which will not necessarily generalize well.
NP-Hardness For p = 0, Amaldi and Kann (1998) show that Problem 1 is NP-Hard: it cannot
even be approximated within 2log1e (n), 8e > 0 unless NP (cid:26) DTIME(npoly log(n)) where DTIME(x)
is the class of deterministic algorithms ending in O(x) steps. It means that under rather general
assumptions, there is no polynomial time algorithm that can approximate the value of the objective
function at optimum N0 within less than N0(2log1e (n)) for all e > 0. The minimization of Problem 1
is thus hopeless and very specic approximations need to be dened with well motivated discussions
and experiments. This is the aim of this paper which presents a novel approximation in the context
of Machine Learning. It turns out that in such a context a simple approximation of Problem 1 leads
to a sufciently small value of its objective function. The term sufciently is relative to the goal
to be achieved. For instance, nding a small number of features so that there exists a linear model
consistent with a training set does not require nding the smallest number of features. Most of the
time reducing the size of the input space is done in order to get better generalization performance
and not in order to have a compact representation of the inputs. So, sometimes nding the smallest
1. Note that for p < 1, k.kp is not a norm, and that we consider here k.kp
p rather than k.kp so that the limit exists when

p ! 0.

2. In our experiments we restrict ourselves to the case of Pattern Recognition. However, the reasoning also applies to

other domains.

1440

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

subset of features is not desirable since it could lead to overtting if the number of training points
is small (for example consider if there are a very large number of noisy features and a very small
training set size then one could pick a small number of the noisy features which appear highly
correlated with the target, but do not generalize to new examples).

Applications The zero norm is directly related to some optimization problems in learning, for ex-
ample in minimizing the number of training errors or nding minimal subsets, e.g., in vector quan-
tization. In some applications we consider, however, the value of the objective function kwk0
0 is not
exactly what has to be minimized. From a machine learning perspective, as we already mentioned,
we are often more interested in the generalization performance. In feature selection problems one is
often interested in parameterizing the problem to control the trade off between number of features
used and training error (as well as regularizing appropriately). This has usually been solved in a
two stage approach (feature selection, followed by minimizing error + regularization) but we show
how, by using the zero norm with a constraint on the minimum number of features allowed, one
can try to perform the error minimization and feature selection in one step. The latter is not re-
stricted to only two-class models: we will derive a similar formulation as Problem 1 for multi-class
discrimination. Feature selection for regression, for example, can also be cast into a mathematical
problem similar to what we are studying here. The duality between feature selection and classica-
tion errors is known ((Amaldi and Kann, 1998)), and one can use the same technique to solve both
problems. Minimizing the classication error on the training set or a trade-off between this error
and the number of desired features is another application we deal with in this paper.

Previous work on minimizing the zero-norm Problem 1 already appeared in the literature: we
mentioned the work of Amaldi and Kann who studied its computational complexity. In the Machine
Learning community, the rst work we are aware of concerning this problem is that of Bradley and
Mangasarian. Bradley et al. (1995) and Bradley and Mangasarian (1998) propose an approximation
method called FSV, which is used for variable selection and, later, by Fung et al. (2000), for nding
sparse kernel expansions. It is based on an approximation of the step function:

kwk0

0 = cardfwijwi 6= 0g (cid:25) (cid:229)

1 e
a

jwij

i

is a parameter that must be chosen. Bradley and Mangasarian suggest to set the value of a
where a
to 5, although it is also proposed (but in practice not attempted) to slowly increase the value of a
in
order to improve the approximation. The authors showed that minimizing the above expression can
be achieved by solving a sequence of linear programs of the following form:3

Problem 2

min

v

i

a e

a v
(cid:3)

i (vi  v
(cid:3)
i

)

subject to :

yi (w(cid:1) xi + b) (cid:21) 1, v (cid:20) w (cid:20) v

(cid:3)
where v

is the solution v from the last iteration. Note that each of these iterations nds the
while keeping consistency with the con-
is sufciently large then an optimal solution of Problem 2 is also an

steepest descent direction of the objective (cid:229)
straints. It is proved that if a
3. The algorithm also can trade off the training error with the variable selection criteria, which we have omitted in our

i 1 e
a

jwij

description.

1441

(cid:229)
WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

optimal solution of Problem 1. Despite some experimental evidence that shows that this approxima-
tion is appropriate, we see one main drawback of the approach of Bradley and Mangasarian. When
the dimensionality of the input is large, the optimization problem becomes harder to solve and
involves a larger number of variables. This makes it somewhat restrictive for large problems. Con-
trarily to SVM optimization problems, the dual of Problem 2 does not have sparse constraints and
cannot be solved as easily. To overcome this drawback, a decomposition method has been derived
by Bradley and Mangasarian (2000). Unfortunately, we have not seen its efciency on datasets with
thousands of features and we still argue that even this decomposition requires a signicant amount
of computational resources compared to what is required on the same problem for a SVM.

Another remark about the approach of Bradley and Mangasarian concerns the hyperparameter
. For large a
, Problem 2 resembles Problem 1, has a large number of local minima and is very
difcult to optimize: ultimately, if a
is large enough, the convergence of the procedure is reached at
the rst iteration. On the other hand, if a
is too small then the step function is not well approximated
and the minimization does not provide a vector with a small number of features. Unfortunately, the
sensitivity of the method to a has not been assessed and it is not known whether it depends on the
dimensionality of the problem at hand, since only problems with less than 34 features are considered
by Bradley et al. (1995) and Bradley and Mangasarian (1998). Note however that we applied the
same technique of Bradley and Mangasarian and observed that the value a = 5 led to reasonable
(cid:3)
results when the starting value v

was set to zero.

Mainly because of the computational drawback we have just mentioned, we argue that the
method of Bradley and Mangasarian may not be appropriate when dealing with datasets of high
dimensionality occurring for instance in domains such as bioinformatics or medical science. The
algorithm we introduce in this paper has been designed to be able to handle a large number of
features and to be easily implemented.

There are a number of other works which aim to minimize the zero-norm in a variety of domains.
Fung et al. (2000) use a modication of the algorithm just described to try to use as few kernel
functions (training data points) as possible to construct a decision rule. The resulting classier is
intended to provide fast online decision making for time critical applications. Perez-Cruz et al.
(2002) describe a scheme to (more) directly minimize the training error of SVMs in the case where
the data are linearly inseparable. Usually, in SVMs an approximation of the step function is used,
such as the hinge loss, a piece-wise linear approximation. The authors instead describe an efcient
optimization scheme based on either a sigmoidal or polynomial approximation of the step function.
They showed that this can improve performance in certain situations.

In this work we will also consider the problem of nding a minimal subset of features in spaces
induced by kernels, and applying the zero-norm to feature selection tasks. Hence feature selection
works such as those by Guyon et al. (2001) and Weston et al. (2001b) where the authors propose
algorithms to choose a small number of features for either linear or nonlinear decision rules are also
relevant. Related work by Takimoto and Warmuth (2000) shows how to nd a sparse normal vector
in a feature space induced by a polynomial kernel using a kernel version of winnow.

Outline of the paper The paper is organized as follows: the next section presents the notation we
will use throughout the paper and briey explains some concepts such as feature spaces and kernels.
Section 3b presents the new procedure we propose to minimize the zero-norm of a linear model. It
gives a number of examples on how to apply this technique to two-class as well as multiclass models.
In Section 4, we present some experimental results on different variable and feature selection tasks

1442

a
USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

for articial problems and for real biological problems from Microarray experiments. Section 5 is
related to kernel methods and shows how to minimize the zero-norm of the parameters of a non-
linear model. Finally, Section 6 concludes the work with a discussion.

2. Notation and Preliminaries

Before presenting our work, we recall some basics of SVM and kernel methods. The SVM proce-
dure aims at solving the following problem:

Problem 3

kwk2

2 +C

min
w,b

m(cid:229)
i=1

i

subject to :

yi (w(cid:1) xi + b) (cid:21) 1 x

i, x

i (cid:21) 0

To solve it, it is convenient to consider the dual formulation (Vapnik, 1995, 1998, Scholkopf and

Smola, 2002, provide detailed discussion and derivation):

Problem 4

maxa

i

m(cid:229)
i=1

i  1
2

m(cid:229)
i, j=1

jyiy j(xi (cid:1) x j)

ia

subject to :
where a

iyi = 0, C (cid:21) a

i (cid:21) 0

m(cid:229)
i=1

i are the dual variables related to the constraints yi(w(cid:1) xi + b) (cid:21) 1 x
this problem can then be used to compute the value of w and b. It turns out that

i. The solution of

w(cid:1) x =

m(cid:229)
i=1

iyi(xi (cid:1) x)

This means that the decision function g(x) dened in the introduction can be computed by using dot
products only. This remark allows us to use any kind of functions k(x,x
if k can be
0) corresponds to a dot-product one
understood as a dot-product. To know whether a function k(x,x
can use results from functional analysis, among them Mercers Theorem. This theorem shows that
under certain conditions,4 many functions called kernels satisfy the following

0) instead of x(cid:1) x
0

i(x2) = F (x1)(cid:1) F (x2)

i(x1)f

k(x1,x2) = (cid:229)
i(x), . . .) 2 (cid:96)2. The function F

i

where F (x) = (f 1(x), . . . ,f
embeds the points xi into a space such
that k(x1,x2) can be interpreted as a dot product in that space. This space is called the feature
space. Many existing functions such as Gaussian kernels k(x1,x2) = exp(kx1  x2k2/s 2) and
polynomials k(x1,x2) = (x1 (cid:1) x2 + 1)d are such kernels.

In the following, we will denote by x(cid:3) w the component-wise product between two vectors:

x(cid:3) w = (x1w1, . . . ,xnwn)

4. We do not go into detail on these conditions here, see work by Courant and Hilbert (1953), Vapnik (1998), Scholkopf

and Smola (2002).

1443

x
a
a
a
a
f
WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

3. Minimizing the Zero-Norm

In this section we propose a novel method of minimizing the zero norm. We present different
formulations of the minimization of the zero-norm for different machine learning tasks and an opti-
mization method for each of them. These methods are all based on the same idea that is explained
in the following subsection, and introduced for two-class linear classication tasks.

3.1 For Two-Class Linear Models

We wish to construct an algorithm which minimizes the zero-norm of linear models. Formally, we
would like to solve the following optimization problem:

Problem 5

kwk0

0

min
w

subject to :

yi(w(cid:1) xi + b) (cid:21) 1

That is, we wish to nd the separating hyperplane with the fewest nonzero elements in the vector
of coefcients w (note this may not be unique). Unfortunately, as we discussed in the introduction,
this problem is combinatorially hard ((Amaldi and Kann, 1998)). We thus propose to study the
following approximation:

Problem 6

min
w

n(cid:229)
j=1

ln(e +jw jj)

yi(w(cid:1) xi + b) (cid:21) 1

subject to :
where 0 < e (cid:28) 1 has been introduced in order to avoid problems when one of the w j is zero.
This new problem is still not easy since there are many local minima, but at least it can be dealt with
by using constrained gradient descent. The relation with the minimization of the zero-norm can be
understood as follows:

Let wl(e ), also written wl when the context is clear, be a minimizer of Problem 6, and w0 a

minimizer of Problem 5, then we have:

n(cid:229)
j=1

ln(e +j(wl) jj) (cid:20) n(cid:229)
j=1

ln(e +j(w0) jj).

(1)

The following inequality is equivalent to (1),

ln(e ) + (cid:229)

(wl) j6=0

(wl) j=0

ln(e +j(wl) jj) (cid:20) (cid:229)

(w0) j=0

ln(e ) + (cid:229)

(w0) j6=0

ln(e +j(w0) jj),

i.e.,

(nkwlk0

0)ln(e ) + (cid:229)

(wl) j6=0

ln(e +j(wl) jj) (cid:20) (nkw0k0

0)ln(e ) + (cid:229)

(w0) j6=0

ln(e +j(w0) jj).

1444

(cid:229)
USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

We thus see that

kwlk0

0 (cid:20) kw0k0

0 + (cid:229)

(wl) j6=0

= kw0k0

0 + (cid:229)

ln(e +j(wl) jj)

ln(e +j(wl) jj)

ln(e )

ln(e )

(cid:18)

 (cid:229)
(w0) j6=0
1
lne

+ O

ln(e +j(w0) jj)
(cid:19)

ln(e )

.

(cid:1)
(cid:0)
If wl were independent of the choice of e , then it could be summarized as O
1
lne
we make the additional assumption that all nonzero entries of wl satisfy j(wl) jj (cid:21) d
d > 0 (e.g., setting d

to equal the machine precision), we get

(wl) j6=0

just like w0. If
for some xed

kwlk0

0 (cid:20) kw0k0

0 + O

(cid:19)

,

(cid:18)

1
lne

and thus the zero norm of wl is almost the same as the one of w0.

The approximation to the zero norm given in Problem 6 also has a connection with sparse
Bayesian modelling as shown by Tipping (2001). This latter framework incorporates an implicit
prior over the parameters given by  ln p(w) = (cid:229) n
j=1 lnjw jj, which may be considered to be mini-
mized in some sense, and is clearly closely related to the objective function of Problem 6. While this
prior is never used explicitly (computation is performed instead in the space of hyperparameters),
and the Bayesian framework is not formulated in terms of the zero norm, the models obtained are
nevertheless highly sparse.

In summary, due to the structure of Problem 6 it is always better to set w j to zero when it is
possible. This is due to the form of the logarithm function that decreases quickly to zero compared
to its increase for larger values of w j. Said differently, you can increase signicantly one w j without
changing that much the objective although, by decreasing w j toward zero, the objective function
will decrease a lot. So, it is better to increase one w j while setting to zero another one rather than
doing some compromise between both. From now on, we will assume that e
is equal to the machine
precision. Doing so, we will identify e with zero. To solve Problem 6, we use an iterative method
that performs a gradient step at each iteration known as Franke and Wolfes method (Franke and
Wolfe, 1956). It is proved to converge to a local minimum. For the problem of interest, it takes the
following form (see Appendix A of Weston et al. 2001a for more details about the derivation of this
procedure):

1. Set z = (1, . . . ,1)

2. Solve

Problem 7

min

jw jj

n(cid:229)
j=1

subject to :

yi(w(cid:1) (xi (cid:3) z) + b) (cid:21) 1

3. Let w be the solution of the previous problem. Set z   z(cid:3) w

1445

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

4. Go back to 2 until convergence.

The method we have just dened is thus an Approximation of the zero-norm Minimization (AROM).
It is simply a succession of linear programs combined with a multiplicative update.

Sometimes, it can be advantageous to consider fast approximations of this algorithm. For in-
stance, one could take a suboptimal descent direction by using the (cid:96)2-norm instead of the (cid:96)1-norm in
step 2. The (cid:96)2-norm has the advantage that it leads to a simple dual formulation which then becomes
equivalent to training an SVM:

1. Set z = (1, ..,1)

2. Solve

Problem 8

maxa

i

m(cid:229)
i=1

i  1
2

m(cid:229)
i, j=1

jyiy j (z(cid:3) xi (cid:1) z(cid:3) x j)

ia

subject to :

iyi = 0, a

i (cid:21) 0

m(cid:229)
i=1

3. Let w be the solution of the previous problem. Set z   z(cid:3) w

4. Go back to 2 until convergence.

This may be useful for a number of reasons: rstly, the dual form may be much easier to
solve if the number of features is larger than the number of examples (otherwise the primal form is
preferable). Secondly, there exist several decomposition methods for the dual form of the (cid:96)2-norm in
the SVM literature (Vapnik, 1998, Scholkopf and Smola, 2002) which greatly reduce computational
resources required and speed up training times. Finally, in the dual form (which only employs dot
products) kernel functions can also be used, as we shall show later. Note that the use of the (cid:96)2-norm
in our algorithm can also be replaced with the (cid:96)1-norm (i.e replacing Problem 8 with Problem 7) in
later iterations when the solution is already suitably sparse. This is possible due to the sparsity in z.
This new formulation of Problem 6 allows a different interpretation of the algorithm. Intuitively,
whether using the (cid:96)1 or (cid:96)2-norm, one can understand what the algorithm does in the following way.
On a given iteration suppose one feature is given less weight relative to others (according to the
values in the vector w). On the subsequent iteration the training data are rescaled according to
the previous iterations (stored in the vector z), so this feature is likely to achieve an even smaller
weighting in the next round, the multiplicative nature of the weighting making its scaling factor zi
rapidly decay to zero if it is not necessarily to fulll the separability constraints (to classify the data
correctly). If a features scaling factor zi is forced to zero note that it cannot be increased again. If,
on the other hand, the feature is necessary to describe the data this cannot diminish to zero as its
weight wi cannot be assigned a value zero.

The methods developed here apply for a linearly separable training set. When this is not the
case, we make use of the method by Freund and Schapire (1998), Cortes and Vapnik (1995) which
involves adding a constant to the diagonal of the kernel matrix. This is described in the next section.

1446

a
a
a
USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

3.2 The Linearly Inseparable Case

It is possible to extend the proposed algorithm to also trade off the training error with the number
of features selected, which is necessary in the linearly inseparable case. For simplicity let us again
consider the case of two-class linear models. Introducing slack variables x
i for each training point
(Bennett and Mangasarian, 1992, Cortes and Vapnik, 1995) we can in general solve:

min
w2Rn
subject to:

jjwjjp + l jjx jjq
yi(w(cid:1) xi + b) (cid:21) 1 x

i.

(2)

(3)

Let us consider the case p = 0 and q = 0. Using our approach it is possible to solve this problem by
rewriting the training data xi   (xi, 1
i) j = 1 if i = j and 0 otherwise.
Then minimizing Problem 5 is equivalent to minimizing (2) with the original data. This can be
implemented in the (cid:96)2-AROM method by adding a ridge to the kernel rather than explicitly storing
the extended vectors.

i 2 f0,1gn and (d

i) where d

Note that it is also possible to derive a system to minimize the training error of SVMs, i.e using
p = 2 and q = 0. This is a kind of minimization that researchers have been interested in solving but
have rarely in practice been able to solve, although Fung et al. (2000) and Perez-Cruz et al. (2002)
also propose solutions to this problem.

3.3 For Multi-Class Linear Models

When many classes are involved, one could use a classical trick that consists in decomposing the
multi-class problem into many two-class ones. Often, the one-against-all approach is used: one
vector wk and one bias bk are dened for each class k, and the output is computed as:

g(x) = arg max
k

wk (cid:1) x + bk

(4)

Here, the vector wk is learned by discriminating the class k from all the other classes. This gives
many two-class problems. In this framework, the minimization of the zero-norm is done for each
vector wk independently of the others. However, the actual zero-norm we are interested in is the
following:

(5)
where Q is the number of classes and jwkj stands for the component-wise absolute value of wk.
Thus, applying this kind of decomposition scheme adds a suboptimal process to the overall method.
To perform a zero-norm minimization for the multi-class problems, we use the same scheme as the
(cid:96)2 approximation scheme exposed before, but the original problems we approximate are different
and are stated as:

k=1

0

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Q(cid:229)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)0

jwkj

Problem 9

min
w1,..,wQ

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) Q(cid:229)

k=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)0

0

jwkj

subject to :

Some constraints

1447

l
d
WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

The constraints can take different forms depending on the way the data has to be discriminated.

For instance, it could be related to the one-against-the-rest approach: for all i 2 f1, ..,mg

yki (wk (cid:1) xi + bk) (cid:21) 1, 8k 2 f1, ..,Qg

where yk 2 f(cid:6)1gm is a target vector corresponding to the discrimination of class k from the others,
i.e. yki = 1 iff xi is in class k. Another possibility would be to consider the constraints related to the
multi-class SVM of Weston and Watkins (1999): for all i 2 f1, ..,mg
(wc(i)  w j)(cid:1) xi + bc(i)  b j (cid:21) 1, 8 j 2 f1, ..,Qg,

j 6= c(i)

where c(i) is the target class for point xi.

Considering the logarithmic approximation as we did previously, we can derive the following

problem (see Appendix B of Weston et al. 2001a for calculations),

1. Set z = (1, . . . ,1)

2. Solve:

Problem 10

subject to :

min
wk,bk

jwk jj

Q(cid:229)
k=1

n(cid:229)
j=1

(cid:0)
wc(i)  wk
for k = 1, ..,Q,k 6= c(i)

(cid:1)(cid:1) (xi (cid:3) z) + bc(i)  bk (cid:21) 1

3. Let ( wk, bk) be the solution, set z   z(cid:3) ((cid:229) Q
k=1
4. Go back to step 2 until convergence.

j wkj)

5. Output z

The features that are selected are those for which the components of the output z are non-
zero. As done before in the binary case, for implementation purposes, the objective function of
Problem 10 can be replaced by,

kwkk2

Q(cid:229)
k=1

The solution given by Problem 10 with this new objective function is no more the steepest direction
desired for the gradient descent but only an approximation of it. This approximation can bring many
advantages such as being able to handle input spaces with very high dimensionality.

In the above calculations we have considered only the constraints relative to the multi-class
SVM of Weston and Watkins (1999). Other constraints such as those for the one-against-the-rest
approach could be used. Note however that the objective function minimized here is specic to
the way we deal with multi-class systems and that it corresponds to an overall goal although the
naive one-against-the-rest approach developed for feature selection would be as suggested in the
beginning of this section. The nal algorithm with the one-against-the-rest constraints takes the
form of:

1448

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

1. Set z = (1, . . . ,1)

2. Solve:

Problem 11

subject to :

min
wk,bk

kwkk2

2

Q(cid:229)
k=1

yki (wk (cid:1) xi (cid:3) z + bk) (cid:21) 1
for k = 1, ..,Q

3. Let ( wk, bk) be the solution, set z   z(cid:3) ((cid:229) Q
k=1
4. Go back to step 2 until convergence.

j wkj)

5. Output z

Note that the latter procedure is the preferred one for very large data sets with large number of
classes. In this case indeed, Problem 11 is equivalent to running a one-against-the-rest linear SVM
on the data sets where the latter has been scaled by the vector z.

3.4 Using Nonlinear Functions via Kernels

It is also possible to perform a minimization of the zero-norm with nonlinear classiers. In this
situation one would like to nd a minimal set of features in the high dimensional space induced by
a kernel. In contrast to input feature selection the goal of kernel space feature selection is usually of
improving generalization performance rather than improving running time or attempting to interpret
the decision rule, although it is also possible for these factors to play a role.

Note that this is not the same as the objective of Fung et al. (2000) in which the authors try to use
as few kernel functions as possible to construct a decision rule. Another different but related tech-
nique is explored by Weston et al. (2001b) where the authors implement a nonlinear decision rule
via kernels but use a feature selection algorithm to select useful input features rather than selecting
features directly in feature space. Finally, in other related work by Takimoto and Warmuth (2000),
the authors show how to nd a sparse normal vector in a feature space induced by a polynomial
kernel using a kernel version of winnow.
Minimization of the zero-norm with nonlinear classiers is possible using the (cid:96)2-AROM method.
To do this one must compute k(x,y) = (z(cid:3)f (x))(cid:1) (z(cid:3)f (y)). Note that if this is computed explicitly
to form a kernel matrix it does not slow down the optimization process (there are still only m vari-
ables) and can be fast in later iterations when w is sparse. Essentially, one only needs to compute
the explicit vectors in feature space for the multiplicative update between optimization steps, the
optimization steps still have the same complexity after the kernel matrix is calculated. This could
already be much more efcient than using an algorithm which requires an explicit representation
in the optimization step such as required when using the (cid:96)1 norm. Nevertheless, to avoid explicit
computation in feature space we wish to perform all the steps with kernels. To do this it will be
necessary to compute element-wise multiplication in feature space, i.e., multiplications in feature
space of the form

f (x)(cid:3) f (y)

1449

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

This is possible with polynomial type kernels.5 We now show that in this case, the multiplication
reduces to a multiplication in the input space.
Consider the polynomial kernel of degree d, dened as k(x,y) = (x(cid:1) y)d. We denote its feature
map by f d, hence k(x,y) = f d(x)(cid:1) f d(y). Letting n denote the input dimension, we have

f d(x

0 (cid:3) x)(cid:1) f d(y

!d
0 (cid:3) y))d


0 (cid:3) y) = ((x
0 (cid:3) x)(cid:1) (y
n(cid:229)
0
0
ixiy
x
iyi
i=1
n(cid:229)

=

=

i1,...,id=1

(cid:2)

0
0
i1xi1y
x
i1yi1

0
0
. . . x
id xid y
id yid

=

=

(cid:2)

0
(x
i1

n(cid:229)
i1,...,id=1
f d(x

0
(cid:3)(cid:1)(cid:2)
. . . x
id
0)(cid:3) f d(x)

)(xi1
f d(y

)

0
(cid:3)
(y
. . . xid
i1
0)(cid:3) f d(y)

.

(cid:3)(cid:2)

(cid:3)

)

0
. . . y
id

)(yi1

. . . yid

(6)

Using the same reasoning this can be extended to feature spaces with monomials of degree d or
less (polynomials) by dening f 1:d(x) = hf p(x) : 1 (cid:20) p (cid:20) di (which is similar to the conventional
polynomial kernel map, but with slightly different scaling factors6) and then noticing that f 1:d(x(cid:3)
y) = f 1:d(x)(cid:3) f 1:d(y). This can be calculated with the kernel:

k(x,y) =

(x(cid:1) y)t

d(cid:229)
t=1

Now, to apply it to our algorithm, we need to be able to compute not only z(cid:3) xi but also the
dot product in feature space (z(cid:3) xi)(cid:1) x j which will be used in the next iteration. In Appendix C of
Weston et al. (2001a), we show how to perform such a calculation.

It is then possible to perform an approximation of the minimization of the zero-norm in the
feature space for polynomial kernels. Contrary to the linear case, it is not easy to explicitly look at
the coordinates of the resulting vector w. It is dened in feature space and only a dot product can
be performed easily. Thus, once the algorithm has nished, one can use the resulting classier for
prediction but less easily for interpretation. In the linear case, on the other hand, one can also be
interested in interpretation of the sparse solution found: one may be interested in which features are
used as well as the quality of the predictor.

In the following sections we will discuss applications of the algorithms we have exposed in the

problems of feature selection and feature selection in spaces induced by kernels.

4. Feature Selection

Our method of approximately minimizing the zero-norm chooses a small number of features and
can therefore be used as a feature selection algorithm. We describe this method below.

5. Note that it is not surprising that this is impossible with some other types of kernels, for example with Gaussian

kernels which are innite dimensional

6. It turns out we need these particular scaling factors to make the zero-norm minimization feasible.

1450

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

4.1 Using the Zero-Norm for Feature Selection

Feature selection can be implemented using our method in the following way:

min
w2Rn
subject to:

jjwjjp
yi(w(cid:1) xi + b) (cid:21) 1 and jjwjj0 (cid:20) r

for p = f1,2g and the desired number of features r. This method can be approximated by minimiz-
ing the zero-norm using the (cid:96)2-AROM or (cid:96)1-AROM methods, stopping the step-wise minimization
when the constraint jjwjj0 (cid:20) r is met. One can then re-train a p-norm classier on the features which
are the nonzero elements of w7. In this way one is free to choose the parameter r which dictates how
many features the classier will use. This should thus be differentiated from a zero-norm classier
which would try to use the minimum number of features possible, which is not always the optimum
choice in terms of generalization.

In order to assess the quality of our method, we compare it with using correlation coefcients (a
standard approach) and also with three recently introduced methods, Recursive Feature Elimination
and using the generalization bound R2W 2, which we briey review in the following section, and the
zero-norm minimization algorithm of Bradley et al. (1995).

4.2 Correlation Coefcients (CORR)

Correlation coefcients score the importance of each feature independently of the other features by
comparing that features correlation to the output labels. The score f j of feature j is given by:

( j(+)   j())2
j(+))2 + (s
j())2

,

f j =

(s

where (+) and () are the mean of the feature values for the positive and negative examples
respectively, and s

() are their respective standard deviations.

(+) and s

4.3 Recursive Feature Elimination

Recursive Feature Elimination (RFE) is a recently proposed feature selection algorithm described
by Guyon et al. (2001). The method, given that one wishes to employ only r < n input dimensions
in the nal decision rule, attempts to nd the best subset r. The method operates by trying to choose
the r features which lead to the largest margin of class separation, using an SVM classier. This
combinatorial problem is solved in a greedy fashion at each iteration of training by removing the
input dimension that decreases the margin the least until only r input dimensions remain (this is
known as backward selection).
ia

jyiy jk(xi,x j) is a measure of predictive ability (and is inversely pro-
portionate to the margin). The algorithm is thus to remove features which keep this quantity small.
This can be done with the following iterative procedure:

For SVMs W 2(a ) = (cid:229)

7. This is necessary because the algorithm, in nding the features, scales them differently to their original values (be-
cause the update process scales the data on each iteration). Hence, having found the features, we then use the original
scaling factors again in re-training, which we observed gives a slight performance gain. We thus used this two-stage
process in our experiments. Note one cannot do this in the kernel version (Section 5) as the original features are no
longer accessible.

1451

a
WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

(cid:15) Given solution a

, calculate for each feature p:
ia

(p)(a ) = (cid:229)
W 2

p
jyiy jk(x
i

p
j

,x

)

(where x

p
i means training point i with feature p removed).

(cid:15) Remove the feature with smallest value of jW 2(a )W 2

(p)(a )j.

If the classier is a linear one (of type g(x) = w(cid:1) x + b), this algorithm corresponds to removing the
smallest corresponding value of jwij in each iteration. To speed up computations when the number
of features is large the authors suggest to remove half of the features each iteration. Note that RFE
has been designed for two-class problems although a multi-class version can be derived easily for a
one-against-the-rest approach. The idea is then to remove the features that lead to the smallest value
(a ) is the corresponding margin based value W 2(a k) for
of (cid:229) Q
the machine discriminating class k from all the others. We will consider this implementation of
multi-class RFE in the multi-class experiments.

k,(p)(a k)j where W 2

(a k)W 2

jW 2

k=1

k

k

4.4 Feature Selection via R2W 2

An alternative method of using SVMs for feature selection is described by Weston et al. (2001b).
The idea is the following. Feature selection is performed by scaling the input parameters by a real
valued vector s
i indicate more useful features. Thus the problem is now one of
choosing the best kernel of the form:

. Larger values of s

ks (x,x

0) = k(x(cid:3) s

0 (cid:3) s )

,x

which means nding the parameters s
. This can be achieved by choosing these hyperparameters
via a generalization bound (or a validation set). For SVMs, expectation of the error probability has
the bound

(cid:27)

(cid:26)

,

(7)

EPerr (cid:20) 1
m

E

R2
M2

= 1
m

E

(cid:8)

(cid:9)
R2W 2(a 0)

if the training data of size m belong to a sphere of size R and are separable with margin M (both in
the feature space). Here, the expectation is taken over sets of training data of size m. The values of
s can thus be found by minimizing such a bound by using gradient descent. This method is related
to the Automatic Relevance Determination (ARD) feature-selection methods by MacKay and Neal
(1998).

4.5 Applications

We compared our AROM approach to a standard SVM with no feature selection, and to SVMs
using the feature selection methods described above. Finally, we also compared to the previous
approach of minimizing the zero-norm called FSV by Bradley et al. (1995), which is described in
Section 1 (Problem 2). In order to make our method (and FSV) choose exactly r features we stop
at the last iteration before the constraint jjwjj0 (cid:20) r is satised and choose the r largest elements
of w.
In all methods we then train a linear SVM on the r chosen features. We note that one
could try to improve on all these results by optimizing over the so-called soft margin parameter
C, which we left xed to C = 
. One can nd the datasets used in these experiments at http:
//www.kyb.tuebingen.mpg.de/bs/people/weston/l0.

1452

a
USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

Linear problem We start with an articial problem, where six dimensions out of 100 were rele-
vant. The probability of y = 1 or 1 was equal. The rst three features fx1,x2,x3g were drawn8 as
xi = yN(i,1), and the second three features fx4,x5,x6g were drawn as xi = N(0,1) with a probability
of 0.7, otherwise the rst three were drawn as xi = N(0,1) and the second three as xi = yN(i 3,1).
The remaining features are noise xi = N(0,20), i = 7, . . . ,100. The inputs are then scaled to have
mean zero and standard deviation one.

In this problem the rst six features have redundancy and the rest of the features are irrelevant.
We used linear decision rules and for feature selection we selected the 2 best features. We trained
on 10, 20 and 30 randomly drawn training points, testing on a further 500 points, and averaging test
error over 100 trials. The results are given in Table 1. For each technique the test error and standard
error is given. For 20 or more training points, AROM SVMs outperform RFE, R2W2 and CORR
SVMs whereas conventional SVMs overt. Note that for very small training set sizes (e.g. size 10)
the number of times that two relevant and non-redundant features are chosen (given in brackets in
the gure) is not completely correlated with the test error. This can happen when it is easier for
an algorithm to choose two relevant and redundant features (as in CORR) than to try to nd the
non-redundant ones.

10 points

Method
SVM

33.8%(cid:6)0.66% (0)
23.58%(cid:6)1.29% (9)
CORR SVM
30.1%(cid:6)1.45% (10)
RFE SVM
26.3%(cid:6)1.41% (14)
R2W2 SVM
24.6%(cid:6)1.49% (17)
FSV SVM
(cid:96)2-AROM SVM 26.7%(cid:6)1.46% (15)
(cid:96)1-AROM SVM 25.8%(cid:6)1.49% (20)

20 points

23.2%(cid:6)0.56% (0)
15.8%(cid:6)0.54% (9)
11.6%(cid:6)1.10% (64)
9.8%(cid:6)0.86% (66)
9.1%(cid:6)0.83% (70)
8.8%(cid:6)0.90% (74)
8.9%(cid:6)0.97% (77)

30 points

16.4%(cid:6)0.39% (0)
14.3%(cid:6)0.32% (5)
8.2%(cid:6)0.61% (73)
7.8%(cid:6)0.61% (67)
5.9%(cid:6)0.54% (85)
5.7%(cid:6)0.50% (85)
5.9%(cid:6)0.51% (83)

Table 1: Feature selection on a linear problem. AROM SVMs (using both the (cid:96)1 and (cid:96)2 multi-
plicative updates) outperform RFE, R2W2 and CORR SVMs, other techniques of feature
selection in the case where conventional SVMs overt. For each technique the percentage
test error and standard error is given. In brackets is the number of times that two relevant
and non-redundant features are chosen. See the text for more details.

We measured the signicance of these results using the Wilcoxon signed rank test with a signif-
icance level a = 0.05. The results show that the (cid:96)1-AROM SVM, (cid:96)2-AROM SVM and FSV SVM
are signicantly better than all the other methods for training set sizes 30 with a p-value less than
0.005 but are not signicantly different from each other. For training set size 20, R2W2 SVM is
also not signicantly different to these best performing algorithms (but signicantly outperforms
the others).
We also compared these methods with some naive wrapper feature selection methods using
the (cid:96)1- or (cid:96)2- norm: choose the two largest values of jwij as the features of choice. This is in
effect using only the rst iteration of the AROM (or RFE) algorithm, and as such represents a sanity
check that the iterative procedure does improve the quality of the chosen features. In fact these naive

8. We denote N(,s ) to be a normal distribution with mean  and standard deviation s

.

1453

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

methods perform similarly to the CORR SVM: the (cid:96)2-norm yields for 10, 20 and 30 training points:
26.8%(cid:6)1.39 (3), 16.302%(cid:6)0.77042 (16) and 13.4%(cid:6)0.42 (17). The (cid:96)1-norm yields: 25.9%(cid:6)1.45
(17), 11.008%(cid:6)1.0921 (67) and 12.1%(cid:6)1.35 (66).
Two-class Microarray datasets We then performed experiments on real-life problems. For DNA
microarray data analysis one needs to determine the relevant genes in discrimination as well as
discriminate accurately. We look at a problem of distinguishing between cancerous and normal
tissue in a colon cancer problem examined by Alon et al. (1999) (see also Guyon et al. 2001 for a
treatment of this problem) and in a large B-Cell lymphoma problem described by Alizadeh (2000).
In the colon cancer problem, 62 tissue samples probed by oligonucleotide arrays contain 22
normal and 40 colon cancer tissues that must be discriminated based upon the expression of 2000
genes. Splitting the data into a training set of 50 and a test set of 12 in 500 separate trials we obtained
a test error of 13.89% for standard linear SVMs. We then measured the test error of SVMs trained
with features chosen by ve input selection methods: CORR, RFE, R2W2, FSV and our approach,
(cid:96)2AROM. We chose subsets of 2000, 1000, 500, 250, 100, 50 and 20 genes. The results are shown
in Table 2.

Feats.
2000
1000
500
250
100
50
20

RFE SVM

FSV SVM

R2W2 SVM

(cid:96)2AROM
CORR SVM
13.89%(cid:6)1.6% 13.89%(cid:6)1.6% 13.89%(cid:6)1.6% 13.89%(cid:6)1.6% 13.89%(cid:6)1.6%
14.44%(cid:6)1.6% 13.89%(cid:6)1.6% 14.17%(cid:6)1.5% 16.11%(cid:6)1.5% 14.17%(cid:6)1.6%
16.39%(cid:6)1.9% 13.06%(cid:6)1.4% 12.78%(cid:6)1.6% 15.56%(cid:6)1.8% 14.17%(cid:6)1.3%
15.28%(cid:6)1.7% 12.78%(cid:6)1.4% 12.78%(cid:6)1.6% 14.17%(cid:6)1.7% 13.61%(cid:6)1.6%
16.94%(cid:6)1.7% 13.06%(cid:6)1.7% 12.5%(cid:6)1.7% 14.17%(cid:6)2.2% 11.94%(cid:6)1.9%
21.67%(cid:6)1.6% 12.5%(cid:6)1.5% 11.11%(cid:6)1.7% 16.39%(cid:6)2.1% 11.11%(cid:6)1.7%
32.5%(cid:6)2.6% 14.44%(cid:6)1.7% 14.44%(cid:6)2.1% 16.67%(cid:6)2.2% 14.17%(cid:6)2%

Table 2: Input selection on micro-array data colon cancer vs normal. The average percentage test
error over 30 splits is given for various numbers of genes selected using ve approaches.

RFE SVM

R2W2 SVM

Feats. CORR SVM
4026
3000
2000
1000
500
250
100
50
20

FSV SVM (cid:96)2-AROM SVM
7.87%(cid:6)0.9% 7.87%(cid:6)0.9% 7.87%(cid:6)0.9% 7.87%(cid:6)0.9% 7.87%(cid:6)0.9%
7.69%(cid:6)0.9%
7.87%(cid:6)0.8% 7.87%(cid:6)0.9% 7.69%(cid:6)0.8% 8.98%(cid:6)1%
7.78%(cid:6)0.8% 7.87%(cid:6)0.8% 7.87%(cid:6)0.8% 7.87%(cid:6)1%
7.87%(cid:6)0.8%
8.24%(cid:6)0.8% 7.96%(cid:6)0.9% 6.85%(cid:6)0.7% 6.85%(cid:6)1%
7.41%(cid:6)0.8%
9.91%(cid:6)0.9% 6.94%(cid:6)0.8% 6.76%(cid:6)0.8% 7.78%(cid:6)1%
6.76%(cid:6)0.9%
6.11%(cid:6)0.8%
10.6%(cid:6)1% 6.48%(cid:6)0.7% 5.83%(cid:6)0.8% 9.63%(cid:6)1%
12.7%(cid:6)1.2% 6.57%(cid:6)0.8% 6.2%(cid:6)0.9%
14%(cid:6)0.9%
5.93%(cid:6)0.8%
13.6%(cid:6)1.1% 6.94%(cid:6)0.9% 6.76%(cid:6)0.9% 14%(cid:6)0.9%
6.76%(cid:6)0.9%
14.1%(cid:6)1.3% 9.17%(cid:6)1% 8.15%(cid:6)0.8% 12.7%(cid:6)1.2% 8.43%(cid:6)0.9%

Table 3: Input selection on the lymphoma micro-array data. The average test error over 30 splits is

given for various numbers of genes selected using ve approaches.

1454

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

In this experiment we can no longer use the Wilcoxon signed rank test to assess signicance
because the trials are dependent. We therefore used the corrected resampled t-test statistic of Nadeau
and Bengio (2001) which is a test developed to attempt to take this dependence into account. In
fact, taking a condence level of a = 0.05 the test showed that none of the differences between
algorithms were signicant  apart from between CORR SVM and the other algorithms for feature
size 20. Note that the Wilcoxon test, on the other hand, returns that many of the differences are
signicant. This serves to highlight the difculty of assessing signicance with such small dataset
sizes.

In the lymphoma problem the gene expression of 96 samples is measured with microarrays to
give 4026 features, 61 of the samples are in classes DLCL, FL or CLL (malignant) and 35
are labelled otherwise (usually normal). We followed the same approach as in the colon cancer
problem, splitting the data this time into training sets of size 60 and test sets of size 36 over 30
separate trials. The results are given in Table 3. Similarly to the colon cancer experiment, the
corrected resampled t-test statistic returned that none of the differences between algorithms were
signicant.

To give an idea of the relative training times of the methods we computed the CPU time of
one training run on the lymphoma dataset. We obtained the following times: FSV: 139.34 seconds,
(cid:96)2-AROM SVM: 2.39 seconds, RFE (choosing 20 features): 1.22 seconds, SVM: 0.34 secs, CORR:
0.27 secs. Clearly computational efciency is not accurately measured by computer time if the
algorithms are not optimized. Yet we think this provides an idea of the computational efciency,
e.g. the difference between FSV and (cid:96)2-AROM SVM (which both try to minimize the zero-norm)
is due to the inability of FSV to take advantage of dual optimization and so scales with respect to
the number of features, rather the number of training patterns.

Features AROM M-SVM AROM 1vsR-SVM RFE 1vsR-SVM
6.2%(cid:6) 2.3%
6.2%(cid:6) 2.0%
3.4%(cid:6) 0.89%
4.3%(cid:6) 1.1%

4.3%(cid:6) 1.1%
1.9%(cid:6) 1.4%
2.9%(cid:6) 1.2%
3.8%(cid:6) 1.0%

10
20
40
79

2.4%(cid:6) 1.2%
5.2%(cid:6) 1.6%
3.4%(cid:6) 0.9%
4.3%(cid:6) 1.1%

Table 4: Result of the feature selection preprocessing on the Brown-Yeast dataset: 5 classes, 208
training points of dimension 79. The percentages represent the fraction of errors using 8
fold cross validation, as well as their standard errors. The number of features used by the
methods is also given. M-SVM means multiclass SVM and 1vsR stands for one-against-
the-rest. The AROM is performed with the (cid:96)2 approximation.

Multi-class Microarray dataset We used another Microarray dataset (Brown Yeast dataset) of
208 genes that has to be discriminated into ve classes based on 79 gene expression values corre-
sponding to different experimental conditions. We performed 8-fold cross validation for the differ-
ent methods. The rst algorithm we tested is a classical multi-class SVM described by Weston and
Watkins (1999) without any feature selection method, the second is the same but with a preprocess-
ing step using our multi-class (cid:96)2-AROM procedure to select features. The constraints are chosen to
be the same as for the multi-class implementation of Weston and Watkins (1999). We also applied
a one-against-the-rest SVM with and without feature selection, the latter being performed via the

1455

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

(cid:96)2-AROM procedure. Finally, we also ran a one-against-the-rest SVM combined with RFE. Table
4 presents the result. Note that the performance of the learning system combined with AROM for
feature selection improves generalization performance. Being able to reduce the number of features
while having lower or the same generalization error allows the user to focus on a limited amount of
information and to check whether this information is relevant or not. It is worth noticing also that
the performance of the one-against-the-rest approach is improved when selecting only 40 features
with the RFE method. The number of features is however larger than with the AROM procedure.
Thus both in terms of identifying a small number of useful features and improving generalization
ability, AROM seems to be preferable to RFE. However we expect the number of trials performed
here to be too small to suggest the signicance of these results.

4.6 Discussion

We have shown how the zero-norm minimization can be used for feature selection, and have com-
pared it to some existing feature selection methods. It performs about as well as the best alternative
method compared on some specic microarray problems.

When comparing feature selection algorithms, the generalizaton performance is not the only

reason for choosing a particular method. Some key differences between the methods include:

(cid:15) Computational efciency. Of the algorithms tested correlation scores are the fastest to com-
pute, then methods that use dual optimization (which is possible with the two-norm) such as
SVM, RFE and (cid:96)2-AROM, whereas the slowest are the methods which minimize the one-
norm such as FSV and (cid:96)1-AROM SVM.

(cid:15) Applicability to Nonlinear problems. Of the methods tested, only R2W2 and RFE are appli-

cable for choosing features in input space relevant for nonlinear problems.

(cid:15) Capacity. By searching the space of subsets of features wrapper approaches (e.g. RFE) and the
zero-norm minimization can more effectively minimize the training error than lter methods
such as CORR. Hence in some sense they have a higher capacity. The applicability of these
algorithms thus depends upon the complexity of the problem at hand: lter methods can
undert complex data (e.g. CORR can fail if the data are multivariate or nonlinear) whereas
the other methods can overt the data. Clearly, choosing features via a criterion such as cross
validation error or a generalization bound can bias the estimate of the criterion through its
minimization in just then same way as training error is a biased measure of generalization
ability after minimizing it. As a nal remark this means that even though methods such as
RFE and even the AROM methods end up being forms of backward selection which do not
search through the whole space of possible subsets this is not necessarily a bad thing in that
their capacity is thus not as high as a more complete search of the space (which as well as
overtting, would be computational less tractable anyway).

Several other issues are noteworthy:
(cid:15) Model selection: number of features. We have not addressed the issue of model selection (in
terms of selecting the number of features) in this work, however we believe this is an impor-
tant problem. Of course this hyperparameter can be chosen like any other hyperparameter,
e.g. trying different values and estimating generalization error. However, making this com-
putationally efcient (given that the feature selection algorithm itself with xed value of the

1456

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

parameter can be already expensive to compute) could be difcult without somehow solving
both problems at once. Furthermore, as many approaches, e.g. wrapper approaches have al-
ready used an estimate of generalization error to select features it makes using the same or
related measures more biased and thus less effective for this task.

(cid:15) Model selection: other parameters. For SVMs it would also be nice to be able to select
the other parameters (kernel parameters, soft margin parameter C) at the same times as the
number of features. Of the techniques described only R2W2 provides an obvious mechanism
for doing this.

(cid:15) The goal of feature selection. In this work we have concentrated on the goal of choosing
features such that generalization error is minimized. However, this is not always of interest:
for example the goal may be to know which features would be (the most) relevant features
if the optimal decision function were known. In microarray data, which we have focussed
on, the true goal is often more application specic than what we have addressed. One may be
interested in nding genes which are potential drug targets. Filter methods such as correlation
scores are thus often preferred because they return a ranked list of (possibly redundant) genes
(rather than subsets) which are highly correlated with the labels. The redundancy in this
case can be benecial due to application specic constraints (e.g. usefulness of this gene as a
drug target). However, in the future choosing subsets of genes may become more important,
especially as the amount of available data increases, making such (more difcult) problems
feasible

5. Nonlinear Feature Selection via Kernels

In the nonlinear feature selection problem one would like to select a subset of features in the space
induced by a kernel, usually with the aim of improving the discriminative ability of a classier (see
Section 3.4 for the details of how to apply the zero-norm to this problem).

An example of the use of such a method would be an application where the data require one to
use a nonlinear decision rule (e.g. a polynomial) but the best decision rule is sparse in the param-
eters of the nonlinear rule (e.g. a sparse polynomial). For example, one possible application could
thus be image recognition, where not all polynomial terms are expected to have nonzero weight
(e.g. terms involving pixels far away from each other). However, in this case it might be better to
explicitly implement this prior knowledge into the construction of the kernel. In this section we
only demonstrate the effectiveness of this method on toy examples.

5.1 Experimental Results

We compared the zero-norm minimization to the solution of SVMs on some general nonlinear toy
problems. Note that we do not compare to conventional feature selection methods because if w
is too large in dimension these methods can no longer be easily used. We chose input spaces of
dimension n = 10 and a mapping into feature space of polynomial of degree 2: f (x) = f 1:2(x). The
following noiseless problems (target functions) were chosen: (a) f (x) = x1x2 + x3, (b) f (x) = x1;
and randomly chosen polynomial functions with (c) 95%, (d) 90%, (e) 80% and (f) 0% sparsity of
the target. That is, d% of the coefcients of the polynomial to be learned are zero. The problems
were attempted for training set sizes of 20, 50, 75 and 100 over 30 trials, and test error measured on
a further 500 testing points.

1457

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

f(x) = x1 x2+x3

f(x) = x1

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r

E

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r

E

0

20

40

60

Training pts
(a)

80

100

0

20

40

60

Training pts

80

100

(b)

95% sparse poly degree 2

90% sparse poly degree 2

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r

E

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r

E

0

20

40

60

Training pts
(c)

80

100

0

20

40

80

100

60

Training pts
(d)

80% sparse poly degree 2

0.35

0.3

0.25

0.2

0.15

0.1

0.05

r
o
r
r

E

0% sparse poly degree 2

0.5

0.4

r
o
r
r

E

0.3

0.2

0.1

0

20

40

60

Training pts
(e)

80

100

0

20

40

80

100

60

Training pts
(f)

Figure 1: A comparison of SVMs (dashed lines) and the (cid:96)2-AROM SVM (solid lines) for learning
sparse and non-sparse target functions with polynomials of degree 2 over 10 inputs. The
(cid:96)2-AROM SVM outperforms the SVM solution if the target is sparse, and gives compara-
ble performance otherwise. When the target is a sparse linear target the (cid:96)2-AROM SVM
even outperforms a linear SVM (dotted lines). See the text for details.

We implemented the multiplicative updates by calculating the nonlinear map and using the
method of Section 3.4. Note that comparing to the explicit calculation of w (which is not always

1458

USE OF THE ZERO-NORM WITH LINEAR MODELS AND KERNEL METHODS

possible if w is large) we found the performance was identical. We do not believe this will always
be the case: if the required solution does not exist in the span of the training vectors then Equation
(25) in Appendix C of Weston et al. (2001a) could be a poor approximation. Results are shown in
Figure 1.

In problems (a)-(d) the (cid:96)2-AROM SVM method clearly outperforms SVMs. This is due to
the norm that SVMs use, the (cid:96)2-norm which places a preference on using as many coefcients as
possible in its decision rule. This is costly when the number of features one should use is small.
In problem (b) where the decision rule to be learned is linear (just one feature) the difference is the
largest. On this problem the dotted lines in the plot represent the performance of a linear SVM (as
the target is linear). The linear SVM outperforms the polynomial SVM, as one would expect, but
surprisingly the AROM feature selection method in the space of polynomial coefcients of degree
two still outperforms the linear SVM. This is again because the linear SVM, using the (cid:96)2-norm does
not perform feature selection.

Finally, problems (e) and (f) show an 80% and 0% sparse target respectively. Note that although
our method outperforms SVMs in the case of sparse targets it is not much worse (in this example at
least) when the target is not sparse, as in problem (f). Note that is not the case in the micro-array
analysis of the previous section, the feature selection degrades performance when the number of
features becomes too small even though the data are still linearly separable.

6. Final Discussion and Conclusions

In conclusion we have introduced a simple optimization technique for minimizing the zero-norm and
have studied several applications of the zero-norm in machine learning. The main advantage of our
algorithm over existing ones is its computational advantage when the number of variables exceeds
the number of examples. In particular, we are able to use existing speedup methods and heuristics
that have been designed for SVM and to use them to minimize the zero-norm. It allows one to
handle large data sets with relatively small computer resources as has been shown in the original
papers (e.g. Osuna et al., 1997a,b). It is also simple to adapt our method to different domains, as
shown in the applications. We have shown how it can be useful in terms of feature selection (when
one is interested in discovering features) and pattern recognition (for obtaining good generalization
performance). In related work (Weston et al., 2001a), we also show how to use it for obtaining
sparse representations and for compression (vector quantization) on both toy and real-life problems.
The usefulness of the minimization of the zero-norm in general depends on the type of problem.
In vector quantization and feature selection problems the methods usefulness is clear, since sparsity
is an explicit goal in these applications. In pattern recognition it depends on the (unknown) target.
For example: are your data such that a rule using just a small number of features yields good
performance? Are there many irrelevant features (noise)? In these cases the zero-norm can be
very useful. Constructing toy examples of this type shows one can obtain large performance gains
selecting the appropriate norm, for example in Figure 1. However, the nature of the (cid:96)0-norm as a
regularizer is not completely clear, in our experiments we trained a subsequent (cid:96)2-norm classier
on the chosen features, giving improved performance. Part of the problem is that the (cid:96)0-norm does
not have a unique solution. Trading off the number of variables with badness of t is perhaps not
really enough to produce good classiers and a third (regularization) term is in fact necessary.

Finally, our algorithm for minimizing the zero-norm can be used in many other contexts which
we have not described, some of which are the subject of our next research activities. These include

1459

WESTON, ELISSEEFF, SCH OLKOPF AND TIPPING

multi-label categorisation (where each example may belong to one or many categories), in regres-
sion, and in time series analysis. We plan to further research these kinds of system which are of
particular importance in a growing number of real-world applications, especially in the domain of
bioinformatics.

