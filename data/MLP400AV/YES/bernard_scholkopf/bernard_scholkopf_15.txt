Abstract

The Support Vector (SV) machine is a novel type of learning machine, based on statistical learning theory,
which contains polynomial classi(cid:12)ers, neural networks, and radial basis function (RBF) networks as special
cases. In the RBF case, the SV algorithm automatically determines centers, weights and threshold such
as to minimize an upper bound on the expected test error.
The present study is devoted to an experimental comparison of these machines with a classical approach,
where the centers are determined by k{means clustering and the weights are found using error backprop-
agation. We consider three machines, namely a classical RBF machine, an SV machine with Gaussian
kernel, and a hybrid system with the centers determined by the SV method and the weights trained by
error backpropagation. Our results show that on the US postal service database of handwritten digits,
the SV machine achieves the highest test accuracy, followed by the hybrid approach. The SV approach is
thus not only theoretically well{founded, but also superior in a practical application.

Copyright c(cid:13) Massachusetts Institute of Technology, 		

This report describes research done at the Center for Biological and Computational Learning, the Arti(cid:12)cial Intelligence
Laboratory of the Massachusetts Institute of Technology, and at AT&T Bell Laboratories (now AT&T Research, and Lucent
Technologies Bell Laboratories). Support for the Center is provided in part by a grant from the National Science Foundation
under contract ASC{	. BS thanks the M.I.T. for hospitality during a three{week visit in March 		, where this work
was started. At the time of the study, BS, CB, and VV were with AT&T Bell Laboratories, NJ; KS, FG, PN, and TP were with
the Massachusetts Institute of Technology. KS is now with the Department of Information Systems and Computer Science
at the National University of Singapore, Lower Kent Ridge Road, Singapore ; CB and PN are with Lucent Technologies,
Bell Laboratories, NJ; VV is with AT&T Research, NJ. BS was supported by the Studienstiftung des deutschen Volkes;
CB was supported by ARPA under ONR contract number N-	-C-. We thank A. Smola for useful discussions.
Please direct correspondence to Bernhard Sch(cid:127)olkopf, bs@mpik-tueb.mpg.de, Max{Planck{Institut f(cid:127)ur biologische Kybernetik,
Spemannstr. ,  T(cid:127)ubingen, Germany.

Figure : A simple {dimensional classi(cid:12)cation prob-
lem: (cid:12)nd a decision function separating balls from cir-
cles. The box, as in all following pictures, depicts the
region [(cid:0); ].



Introduction

Consider Fig. . Suppose we want to construct a radial
basis function classi(cid:12)er

D(x) = sgn  
Xi=

wi exp(cid:18)(cid:0)

kx (cid:0) xik

ci



(cid:19) + b!

()

(b and ci being constants, the latter positive) separating
balls from circles, i.e. taking di(cid:11)erent values on balls and
circles. How do we choose the centers xi? Two extreme
cases are conceivable:

The (cid:12)rst approach consists in choosing the centers for
the two classes separately, irrespective of the classi(cid:12)ca-
tion task to be solved. The classical technique of (cid:12)nding
the centers by some clustering technique (before tackling
the classi(cid:12)cation problem) is such an approach. The
weights wi are then usually found by either error back-
propagation (Rumelhart, Hinton, & Williams, 	) or
the pseudo{inverse method (e.g. Poggio & Girosi, 		).
An alternative approach (Fig. ) consists in choosing
as centers points which are critical for the classi(cid:12)cation
task at hand. Recently, the Support Vector Algorithm
was developed (Boser, Guyon & Vapnik 		, Cortes &
Vapnik 		, Vapnik 		) which implements the lat-
ter idea. It is a general algorithm, based on guaranteed
risk bounds of statistical learning theory, which in par-
ticular allows the construction of radial basis function
classi(cid:12)ers. This is done by simply choosing a suitable
kernel function for the SV machine (see Sec. .). The
SV training consists of a quadratic programming prob-
lem which can be solved e(cid:14)ciently and for which we are
guaranteed to (cid:12)nd a global extremum. The algorithm
automatically computes the number and location of the
above centers, the weights wi, and the threshold b, in
the following way: by the use of a suitable kernel func-
tion (in the present case, a Gaussian one), the patterns
are mapped nonlinearly into a high{dimensional space.
There, an optimal separating hyperplane is constructed,
expressed in terms of those examples which are closest

Figure : RBF centers automatically found by the Sup-
port Vector algorithm (indicated by extra circles), using
ci =  for all i (cf. Eq. ). The number of SV centers ac-
cidentally coincides with the number of identi(cid:12)able clus-
ters (indicated by crosses found by k{means clustering
with k =  and k =  for balls and circles, respectively)
but the naive correspondence between clusters and cen-
ters is lost; indeed,  of the SV centers are circles, and
only  of them are balls. Note that the SV centers are
chosen with respect to the classi(cid:12)cation task to be solved.

to the decision boundary (Vapnik 		). These are the
Support Vectors which correspond to the centers in input
space.

The goal of the present study is to compare real{world
results obtained with k{means clustering and classical
RBF training to those obtained with the centers, weights
and threshold automatically chosen by the Support Vec-
tor algorithm. To this end, we decided to undertake a
performance study combining expertise on the Support
Vector algorithm (AT&T Bell Laboratories) and classi-
cal radial basis function networks (Massachusetts Insti-
tute of Technology). We report results obtained on a US
postal service database of handwritten digits.

We have organized the material as follows.

In the
next Section, we describe the algorithms used to train
the di(cid:11)erent types of RBF classi(cid:12)ers used in this paper.
Following that, we present an experimental comparison
of the approaches. We conclude with a discussion of our
(cid:12)ndings.

 Di(cid:11)erent Ways of Constructing a

Radial Basis Function Classi(cid:12)er

We describe three radial basis function systems, trained
in di(cid:11)erent ways. In Sec. ., we discuss the (cid:12)rst sys-
tem trained along more classical lines.
In the follow-
ing section (.), we discuss the Support Vector algo-
rithm, which constructs an RBF network whose param-
eters (centers, weights, threshold) are automatically op-
timized. In Sec. ., (cid:12)nally, we use the Support Vector
algorithm merely to choose the centers of the RBF net-
work and then optimize the weights separately.








. Classical Spherical Gaussian RBFs:

We begin by (cid:12)rst describing the classical Gaussian RBF
system. A d-dimensional spherical Gaussian RBF net-
work with K centers has the mathematical form

g(~x) =

=

K

K

Xi=
Xi=

wiGi(~x) + b

wi



((cid:25))d=(cid:27)d
i

exp((cid:0)

k~x (cid:0) ~cik



(cid:27)
i

) + b

where Gi is the ith Gaussian basis function with center
~ci and variance (cid:27)
i . The weight coe(cid:14)cients wi combine
the Gaussian terms into a single output value and b is
a bias term. In general, building a Gaussian RBF net-
work for a given learning task involves () determining
the total number of Gaussian basis functions to use for
each output class and for the entire system, () locating
the Gaussian basis function centers, () computing the
cluster variance for each Gaussian basis function, and ()
solving for the weight coe(cid:14)cients and bias in the summa-
tion term. One can implement a binary pattern classi(cid:12)er
on input vectors ~x as a Gaussian RBF network by de(cid:12)n-
ing an appropriate output threshold that separates the
two pattern classes.

In this (cid:12)rst system, we implement each individual
digit recognizer as a spherical Gaussian RBF network,
trained with a classical RBF algorithm. Given a spec-
i(cid:12)ed number of Gaussian basis functions for each digit
class, the algorithm separately computes the Gaussian
centers and variances for each of the  digit classes
to form the systems RBF kernels. The algorithm then
solves for an optimal set of weight parameters between
the RBF kernels and each output node to perform the
desired digit recognition task. The training process con-
structs all  digit recognizers in parallel so one can re-
use the same Gaussian basis functions among the  digit
recognizers. To avoid over(cid:12)tting the available training
data with an overly complex RBF classi(cid:12)er connected to
every Gaussian kernel, we use a \bootstrap" like oper-
ation that selectively connects each recognizers output
node to only a \relevant" subset of all basis functions.
The idea is similar to how we choose relevant \near-miss"
clusters for each individual digit recognizer in the origi-
nal system. The training procedure proceeds as follows
(for further details, see Sung, 		):

. The (cid:12)rst training task is to determine an appro-
priate number k of Gaussian kernels for each digit
class. This information is needed to initialize our
clustering procedure for computing Gaussian RBF
kernels. We opted for using the same numbers of
Gaussian kernels as the ones automatically com-
puted by the SV algorithm (see Table ).

. Our next task is to actually compute the Gaussian
kernels for each digit class. We do this by sepa-
rately performing classical k{means clustering (see
e.g. Lloyd, 	) on each digit class in the US postal
service (USPS) training database. Each clustering
operation returns a set of Gaussian centroids and



their respective variances for the given digit class.
Together, the Gaussian clusters from all  digit
classes form the systems RBF kernels.

. For each single-digit recognizer, we build an initial
RBF network using only Gaussian kernels from its
target class, using error backpropagation to train
the weights. We then separately collect all the false
positive mistakes each initial digit recognizer makes
on the USPS training database.

. In the (cid:12)nal training step, we augment each initial
digit recognizer with additional Gaussian kernels
from outside its target class to help reduce mis-
classi(cid:12)cation errors. We determine which Gaus-
sian kernels are \relevant" for each recognizer as
follows: For each false positive mistake the initial
recognizer makes during the previous step, we look
up the misclassi(cid:12)ed patterns actual digit class and
include the nearest Gaussian kernel from its class in
the \relevant" set. The (cid:12)nal RBF network for each
single-digit recognizer thus contains every Gaussian
kernel from its target class, and several \relevant"
kernels from the other 	 digit classes, trained by
error backpropagation. Because our (cid:12)nal digit rec-
ognizers have fewer weight parameters than a naive
system that fully connects all  recognizers to ev-
ery Gaussian kernel, we expect our system to gen-
eralize better on new data.

. The Support Vector Machine

Structural Risk Minimization. For the case of two{
class pattern recognition, the task of learning from ex-
amples can be formulated in the following way: given a
set of functions

ff(cid:11) : (cid:11)  (cid:3)g;

f(cid:11) : R

N

! f(cid:0); +g

and a set of examples

(x; y); : : : ; (x; y); xi  R

N ; yi  f(cid:0); +g;

each one generated from an unknown probability distri-
bution P (x; y); we want to (cid:12)nd a function f(cid:11)(cid:3) which
provides the smallest possible value for the risk

R((cid:11)) =Z jf(cid:11)(x) (cid:0) yj dP (x; y):

The problem is that R((cid:11)) is unknown, since P (x; y) is
unknown. Therefore an induction principle for risk min-
imization is necessary.

The straightforward approach to minimize the empir-

ical risk

Remp((cid:11)) =




jf(cid:11)(xi) (cid:0) yij



Xi=

turns out not to guarantee a small actual risk (i.e. a
small error on the training set does not imply a small
error on a test set), if the number  of training examples
is limited. To make the most out of a limited amount
of data, novel statistical techniques have been developed
during the last  years. The Structural Risk Minimiza-
tion principle (Vapnik, 		) is based on the fact that

for the above learning problem, for any (cid:11)  (cid:3) with a
probability of at least  (cid:0) (cid:17), the bound

R((cid:11)) (cid:20) Remp((cid:11)) + (cid:8)(

h


;

log((cid:17))



)

()

holds, (cid:8) being de(cid:12)ned as

(cid:8)(

h


;

log((cid:17))



) =s h(cid:0)log 

h + (cid:1) (cid:0) log((cid:17)=)



:

The parameter h is called the VC{dimension of a set of
functions. It describes the capacity of a set of functions
implementable by the learning machine. For binary clas-
si(cid:12)cation, h is the maximal number of points k which can
be separated into two classes in all possible k ways by
using functions of the learning machine; i.e.
for each
possible separation there exists a function which takes
the value  on one class and (cid:0) on the other class.

According to (), given a (cid:12)xed number  of train-
ing examples one can control the risk by controlling two
quantities: Remp((cid:11)) and h(ff(cid:11) : (cid:11)  (cid:3)
g); (cid:3) denoting
some subset of the index set (cid:3). The empirical risk de-
pends on the function chosen by the learning machine
(i.e. on (cid:11)), and it can be controlled by picking the right
(cid:11). The VC{dimension h depends on the set of functions
ff(cid:11) : (cid:11)  (cid:3)
g which the learning machine can imple-
ment. To control h, one introduces a structure of nested
subsets Sn := ff(cid:11) : (cid:11)  (cid:3)ng of ff(cid:11) : (cid:11)  (cid:3)g,

S (cid:26) S (cid:26) : : : (cid:26) Sn (cid:26) : : : ;

()

with the corresponding VC{dimensions satisfying

h (cid:20) h (cid:20) : : : (cid:20) hn (cid:20) : : :

For a given set of observations (x; y); :::; (x; y) the
Structural Risk Minimization principle chooses the func-
tion f(cid:11)n
in the subset ff(cid:11) : (cid:11)  (cid:3)ng for which the
guaranteed risk bound (the right hand side of ()) is
minimal.



The remainder of this section follows Sch(cid:127)olkopf,
Burges & Vapnik (		) in brie(cid:13)y reviewing the Sup-
port Vector algorithm. For details, the reader is referred
to (Vapnik, 		).

A Structure on the Set of Hyperplanes. Each par-
ticular choice of a structure () gives rise to a learning
algorithm. The Support Vector algorithm is based on a
structure on the set of hyperplanes. To describe it, (cid:12)rst
note that given a dot product space Z and a set of vectors
x; : : : ; xr  Z; each hyperplane fx  Z : (w (cid:1) x)+ b = g
corresponds to a canonical pair (w; b)  Z (cid:2) R if we ad-
ditionally require

min

i=;:::;r

j(w (cid:1) xi) + bj = :

()

Let Bx ;:::;xr = fx  Z : kx (cid:0) ak < Rg (a  Z) be the
smallest ball containing the points x; : : : ; xr, and

fw;b = sgn ((w (cid:1) x) + b)

()

W ((cid:11)) =

the decision function de(cid:12)ned on these points. The pos-
sibility of introducing a structure on the set of hyper-
planes is based on the result (Vapnik, 		) that the set
ffw;b : kwk (cid:20) Ag has a VC-dimension h satisfying

h (cid:20) RA:

()



Note. Dropping the condition kwk (cid:20) A leads to a set
of functions whose VC{dimension equals N + , where
N is the dimensionality of Z. Due to kwk (cid:20) A, we can
get VC{dimensions which are much smaller than N , en-
abling us to work in very high dimensional spaces.

The Support Vector Algorithm. Now suppose we
want to (cid:12)nd a decision function fw;b with the property
fw;b(xi) = yi; i = ; : : : ; : If this function exists, canon-
icality () implies

yi((w (cid:1) xi) + b) (cid:21) ;

i = ; : : : ; :

()

In many practical applications, a separating hyperplane
does not exist. To allow for the possibility of examples
violating (), Cortes & Vapnik (		) introduce slack
variables

(cid:24)i (cid:21) ;

i = ; : : : ; ;

()

to get

yi((w (cid:1) xi) + b) (cid:21)  (cid:0) (cid:24)i;

i = ; : : : ; :

(	)

The Support Vector approach to minimizing the guaran-
teed risk bound () consists in the following: minimize

(cid:8)(w; (cid:24)) = (w (cid:1) w) + (cid:13)

(cid:24)i



Xi=

()

ing the second term of the bound (). The termP

subject to the constraints () and (	). According to
(), minimizing the (cid:12)rst term amounts to minimizing the
VC{dimension of the learning machine, thereby minimiz-
i= (cid:24)i,
on the other hand, is an upper bound on the number of
misclassi(cid:12)cations on the training set | this controls the
empirical risk term in (). For a suitable positive con-
stant (cid:13), this approach therefore constitutes a practical
implementation of Structural Risk Minimization on the
given set of functions.

Introducing Lagrange multipliers (cid:11)i and using the
Kuhn{Tucker theorem of optimization theory, the solu-
tion can be shown to have an expansion

w =



Xi=

yi(cid:11)ixi;

()

with nonzero coe(cid:14)cients (cid:11)i only for the cases where the
corresponding example (xi; yi) precisely meets the con-
straint (	). These xi are called Support Vectors. All
the remaining examples xi of the training set are irrele-
vant: their constraint (	) is satis(cid:12)ed automatically (with
(cid:24)i = ), and they do not appear in the expansion ().
The coe(cid:14)cients (cid:11)i are found by solving the following
quadratic programming problem: maximize

(cid:11)i (cid:0)



Xi=






Xi;j=

yiyj (cid:11)i(cid:11)j(xi (cid:1) xj)

()

subject to

 (cid:20) (cid:11)i (cid:20) (cid:13);

i = ; : : : ; ; and

(cid:11)iyi = :

()



Xi=

Figure : A simple two{class classi(cid:12)cation problem as
solved by the Support Vector algorithm (ci =  for all
i; cf. Eq. ). Note that the RBF centers (indicated by
extra circles) are closest to the decision boundary.

By linearity of the dot product, the decision function ()
can thus be written as

f (x) = sgn  
Xi=

yi(cid:11)i (cid:1) (x (cid:1) xi) + b! :

So far, we have described linear decision surfaces. To
allow for much more general decision surfaces, one can
(cid:12)rst nonlinearly transform the input vectors into a high{
dimensional feature space by a map (cid:30) and then do a
linear separation there. Maximizing () then requires
the computation of dot products ((cid:30)(x) (cid:1) (cid:30)(xi)) in a high{
dimensional space. In some cases, these expensive calcu-
lations can be reduced signi(cid:12)cantly by using a suitable
function K such that

((cid:30)(x) (cid:1) (cid:30)(xi)) = K(x; xi):

We thus get decision functions of the form

f (x) = sgn  
Xi=

yi(cid:11)i (cid:1) K(x; xi) + b! :

()

In practise, we need not worry about conceiving the map
(cid:30). We will choose a K which is the Kernel of a posi-
tive Hilbert{Schmidt operator, and Mercers theorem of
functional analysis then tells us that K corresponds to
a dot product in some other space (see Boser, Guyon &
Vapnik, 		). Consequently, everything that has been
said above about the linear case also applies to nonlinear
cases obtained by using a suitable kernel K instead of
the Euclidean dot product. We are now in a position to
explain how the Support Vector algorithm can construct
radial basis function classi(cid:12)ers: we simply use

K(x; xi) = exp(cid:0)(cid:0)kx (cid:0) xik

=c(cid:1)

()

(see Aizerman, Braverman & Rozonoer, 	). Other
possible choices of K include

K(x; xi) = (x (cid:1) xi)d;

Figure : Two{class classi(cid:12)cation problem solved by the
Support Vector algorithm (ci =  for all i; cf. Eq. ).

yielding polynomial classi(cid:12)ers (d  N), and

K(x; xi) = tanh((cid:20) (cid:1) (x (cid:1) xi) + (cid:2))

for constructing neural networks.

Interestingly, these di(cid:11)erent types of SV machines use
largely the same Support Vectors; i.e. most of the centers
of an SV machine with Gaussian kernel coincide with
the weights of the polynomial and neural network SV
classi(cid:12)ers (Sch(cid:127)olkopf, Burges & Vapnik 		).

To (cid:12)nd the decision function (), we have to maxi-

mize

W ((cid:11)) =

(cid:11)i (cid:0)



Xi=






Xi;j=

yiyj (cid:11)i(cid:11)jK(xi; xj)

()

under the constraint (). To (cid:12)nd the threshold b, one
takes into account that due to (	), for Support Vectors
xj for which (cid:24)j =  we have

yi(cid:11)i (cid:1) K(xj ; xi) + b = yj :



Xi=

Finally, we note that the Support Vector algorithm
has been empirically shown to exhibit good generaliza-
tion ability (Cortes & Vapnik, 		). This can be fur-
ther improved by incorporating invariances of a problem
at hand, as with the Virtual Support Vector method
of generating arti(cid:12)cial examples from the Support Vec-
tors (Sch(cid:127)olkopf, Burges, & Vapnik, 		). In addition,
the decision rule (), which requires the computation of
dot products between the test example and all Support
Vectors, can be sped up with the reduced set technique
(Burges, 		). These methods have led to substantial
improvements for polynomial Support Vector machines
(Burges & Sch(cid:127)olkopf, 		), and they are directly appli-
cable also to RBF Support Vector machines.

. A Hybrid System: SV Centers Only

The previous section discusses how one can train RBF
like networks using the Support Vector algorithm. This



Digit Class
# of SVs

# of pos. SVs










































	






Table : Numbers of centers (Support Vectors) automatically extracted by the Support Vector
machine. The (cid:12)rst row gives the total number for each binary classi(cid:12)er, including both positive
and negative examples; in the second row, we only counted the positive SVs. The latter number
was used in the initialization of the k{means algorithm, cf. Sec. ..

digit

classical RBF

full SVM

SV centers only



















































Table : Two{class-classi(cid:12)cation: numbers of test errors (out of  test patterns) for the three
systems described in Sections . { ..

involves the choice of an appropriate kernel function K
and solving the optimization problem in the form of
Eq. (). The Support Vector algorithm thus automati-
cally determines the centers (which are the Support Vec-
tors), the weights (given by yi(cid:11)i), and the threshold b
for the RBF machine.

To assess the relative in(cid:13)uence of the automatic SV
center choice and the SV weight optimization, respec-
tively, we built another RBF system, constructed with
centers that are simply the Support Vectors arising from
the SV optimization, and with the weights trained sep-
arately.

 Experimental Results

Toy examples. What are the Support Vectors? They
are elements of the data set that are \important" in sep-
arating the two classes from each other. In general, the
Support Vectors with zero slack variables (see Eq. ) lie
on the boundary of the decision surface, as they precisely
satisfy the inequality (	) in the high{dimensional space.
Figures  and  illustrate that for the used Gaussian
kernel this is also the case in input space.

This raises an interesting question from the point of
view of interpreting the structure of trained RBF net-
works. The traditional view of RBF networks has been
one where the centers were regarded as \templates" or
stereotypical patterns. It is this point of view that leads
to the clustering heuristic for training RBF networks.
In contrast, the Support Vector machine posits an alter-
nate point of view, with the centers being those examples
which are critical for a given classi(cid:12)cation task.

US Postal Service Database. We used the USPS
database of 	 handwritten digits ( for training,
 for testing), collected from mail envelopes in Buf-
falo (cf. LeCun et al., 		). Each digit is a  (cid:2) 
vector with entries between (cid:0) and . Preprocessing
consisted in smoothing with a Gaussian kernel of width
(cid:27) = :. The Support Vector machine results reported
in the following were obtained with (cid:13) =  (cf. ()) and



c = : (cid:1)  (cid:1)  (cf. ()). In all experiments, we used
the Support Vector algorithm with standard quadratic
programming techniques (conjugate gradient descent).

Two{class classi(cid:12)cation. Table  shows the numbers
of Support Vectors, i.e. RBF centers, extracted by the
SV algorithm. Table  gives the results of binary clas-
si(cid:12)ers separating single digits from the rest, for the sys-
tems described in Sections ., ., and ..

Ten{class classi(cid:12)cation. For each test pattern, the
arbitration procedure in all three systems simply re-
turns the digit class whose recognizer gives the strongest
response. Table  shows the -class digit recognition
error rates for our original system and the two RBF-
based systems.

The fully automatic Support Vector machine exhibits
the highest test accuracy. Using the Support Vector
algorithm to choose an appropriate number and corre-
sponding centers for the RBF network is also better than
the baseline procedure of choosing the centers by a clus-
tering heuristic. It can be seen that in contrast to the
k{means cluster centers, the centers chosen by the Sup-
port Vector algorithm allow zero training error rates.

 Summary and Discussion

The Support Vector algorithm provides a principled way
of choosing the number and the locations of RBF cen-
ters. Our experiments on a real{world pattern recogni-
tion problem have shown that compared to a correspond-
ing number of centers chosen by k{means, the centers
chosen by the Support Vector algorithm allowed a train-
ing error of zero, even if the weights were trained by
classical RBF methods. Our interpretation of this (cid:12)nd-
ing is that the Support Vector centers are speci(cid:12)cally

The SV machine is rather insensitive to di(cid:11)erent choices
for all values in :; :; : : : ; :, the performance is

of c:
about the same (in the area of % (cid:0) :%).

In the Support Vector case, we constructed ten two{class
classi(cid:12)ers, each trained to separate a given digit from the
other nine, and combined them by doing the ten{class clas-
si(cid:12)cation according to the maximal output (before applying
the sgn function) among the two{class classi(cid:12)ers.

USPS Database

Clustered Centers

S.V. centers Full S.V.M.

Training (	 patterns)

Test ( patterns)

.%
.%

.%
.	%

.%
.%

Classi(cid:12)cation Error Rate

Table : -class digit recognition error rates for three RBF classi(cid:12)ers constructed with di(cid:11)erent algo-
rithms. The (cid:12)rst system is a more classical one choosing its centers by a clustering heuristic. The other
two are the Gaussian RBF-based systems we trained, one with the Support Vectors were chosen to be
the centers and the second where the entire network was trained using the Support Vector algorithm.

[] Poggio, T., & Girosi, F. 		. Networks for approx-
imation and learning. Proc. IEEE, : { 	.

[	] Rumelhart, D. E., Hinton, G. E., & Williams,
R. J. 	. Learning representations by back{
propagating errors. Nature, : { .

[] Sch(cid:127)olkopf, B.; Burges, C.; and Vapnik, V. 		. Ex-
tracting support data for a given task. In: Fayyad,
U. M., and Uthurusamy, R. (eds.): Proceedings,
First International Conference on Knowledge Dis-
covery and Data Mining, AAAI Press, Menlo Park,
CA.

[] Sch(cid:127)olkopf, B., Burges, C.J.C., Vapnik, V. 		. In-
corporating Invariances in Support Vector Learning
Machines. In C. von der Malsburg, W. von Seelen,
J. C. Vorbr(cid:127)uggen, and B. Sendho(cid:11), editors, Arti(cid:12)-
cial Neural Networks | ICANN	, pages  { ,
Berlin. (Springer Lecture Notes in Computer Sci-
ence, Vol. .)

[] Sung, K. 		. Learning and Example Selection for
Object and Pattern Detection. Ph.D. Thesis, Mas-
sachusetts Institute of Technology.

[] Vapnik, V. 		. Estimation of Dependences Based
on Empirical Data, [in Russian] Nauka, Moscow;
English translation: Springer{Verlag, New York,
	.

[] Vapnik, V. 		. The Nature of Statistical Learning

Theory. Springer Verlag, New York.

chosen for the classi(cid:12)cation task at hand, whereas k{
means does not care about picking those centers which
will make a problem separable.

In addition, the SV centers yielded lower test error
rates than k{means. It is interesting to note that using
SV centers, while sticking to the classical procedure for
training the weights, improved training and test error
rates by approximately the same margin ( per cent).
In view of the guaranteed risk bound (), this can be
understood in the following way: the improvement in
test error (risk) was solely due to the lower value of
the training error (empirical risk); the con(cid:12)dence term
(the second term on the right hand side of ()), depend-
ing on the VC{dimension and thus on the norm of the
weight vector (Eq. ), did not change, as we stuck to the
classical weight training procedure. However, when we
also trained the weights with the Support Vector algo-
rithm, we minimized the norm of the weight vector (see
Eq. ) and thus the con(cid:12)dence term, while still keeping
the training error zero. Thus, consistent with (), the
Support Vector machine achieved the highest test accu-
racy of the three systems.

