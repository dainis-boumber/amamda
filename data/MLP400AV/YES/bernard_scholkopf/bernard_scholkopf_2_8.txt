Abstract

We propose a general framework for learning
from labeled and unlabeled data on a directed
graph in which the structure of the graph in-
cluding the directionality of the edges is con-
sidered. The time complexity of the algo-
rithm derived from this framework is nearly
linear due to recently developed numerical
techniques.
In the absence of labeled in-
stances, this framework can be utilized as
a spectral clustering method for directed
graphs, which generalizes the spectral clus-
tering approach for undirected graphs. We
have applied our framework to real-world web
classication problems and obtained encour-
aging results.

1. Introduction

Given a directed graph, the vertices in a subset of the
graph are labeled. Our problem is to classify the re-
maining unlabeled vertices. Typical examples of this
kind are web page categorization based on hyperlink
structure and document classication based on cita-
tion graphs (Fig. 1). The main issue to be resolved
is to determine how to eectively exploit the structure
of directed graphs.

One may assign a label to an unclassied vertex on the
basis of the most common label present among the
classied neighbors of the vertex. However we want
to exploit the structure of the graph globally rather
than locally such that the classication or clustering
is consistent over the whole graph. Such a point of

Appearing in Proceedings of the 22 nd International Confer-
ence on Machine Learning, Bonn, Germany, 2005. Copy-
right 2005 by the author(s)/owner(s).

(2005).

view has been considered previously in the method of
Zhou et al.
It is motivated by the frame-
work of hubs and authorities (Kleinberg, 1999), which
separates web pages into two categories and uses the
following recursive notion: a hub is a web page with
links to many good authorities, while an authority is
a web page that receives links from many good hubs.
In contrast, the approach that we will present is in-
spired by the ranking algorithm PageRank used by
the Google search engine (Page et al., 1998). Dier-
ent from the the framework of hubs and authorities,
PageRank is based on a direct recursion as follows: an
authoritative web page is one that receives many links
from other authoritative web page. When the under-
lying graph is undirected, the approach that we will
present reduces to the method of Zhou et al. (2004).

There has been a large amount of activity on how to
exploit the link structure of the web for ranking web
pages, detecting web communities, nding web pages
similar to a given web page or web pages of interest
to a given geographical region, and other applications.
We may refer to (Henzinger, 2001) for a comprehensive
survey. Unlike those work, the present work is on how
to classify the unclassied vertices of a directed graph
in which some vertices have been classied by globally
exploiting the structure of the graph. Classifying a -
nite set of objects in which some are labeled is called
transductive inference (Vapnik, 1998). In the absence
of labeled instances, our approach reduces to a spectral
clustering method for directed graphs, which general-
izes the work of Shi and Malik (2000) that may be the
most popular spectral clustering scheme for undirected
graphs. We would like to mention that understanding
how eigenvectors partition a directed graph has been
proposed as one of six algorithmic challenges in web
search engines by Henzinger (2003). The framework
of probabilistic relational models may also be used to

Learning from Labeled and Unlabeled Data on a Directed Graph

for all 0  r  k  1 each edge [u; v] 2 E with u 2 Vr
has v 2 Vr+1; where Vk = V0; and k is maximal, that
is, there is no other such partition V = V

k0 1
with k
> k: When k = 1, we say that the graph is
aperiodic; otherwise we say that the graph is periodic.

0

0

0

0 [[ V

Figure 1. The World Wide Web can be thought of as a
directed graph, in which the vertices represent web pages,
and the directed edges hyperlinks.

deal with structured data like the web (e.g. Getoor
et al. (2002)). In contrast to the spirit of the present
work however, it focuses on modeling the probabilistic
distribution over the attributes of the related entities
in the model.

The structure of the paper is as follows. We rst
introduce some basic notions from graph theory and
Markov chains in Section 2. The framework for learn-
ing from directed graphs is presented in Section 3. In
the absence of labeled instances, as shown in section 4,
this framework can be utilized as a spectral clustering
approach for directed graphs. In Section 5, we develop
discrete analysis for directed graphs, and characterize
this framework in terms of discrete analysis. Experi-
mental results on web classication problems are de-
scribed in Section 6.

2. Preliminaries

A directed graph G = (V; E) consists of a nite set V;
together with a subset E (cid:181) V  V: The elements of
V are the vertices of the graph, and the elements of
E are the edges of the graph. An edge of a directed
graph is an ordered pair [u; v] where u and v are the
vertices of the graph. When u = v the edge is called
a loop. A graph is simple if it has no loop. We say
that the vertex v is adjacent from the vertex u; and
the the vertex u is adjacent to the vertex v; and the
edge [u; v] is incident from the vertex u and incident
to the vertex v:

A path in a directed graph is a tuple of vertices
(v1; v2; : : : ; vp) with the property that [vi; vi+1] 2 E
for 1  i  p  1: We say that a directed graph is
strongly connected when for every pair of vertices u
and v there is a path in which v1 = u and vp = v: For
a strongly connected graph, there is an integer k  1
and a unique partition V = V0[V1[[Vk1 such that

A graph is weighted when there is a function w :
E ! R+ which associates a positive value w([u; v])
with each edge [u; v] 2 E: The function w is called a
weight function. Typically, we can equip a graph with
a canonical weight function dened by w([u; v]) := 1 at
each edge [u; v] 2 E: Given a weighted directed graph
and a vertex v of this graph, the in-degree function
d : V ! R+ and out-degree function d+ : V ! R+
are respectively dened by d(v) := Pu!v w([u; v]);
and d+(v) := Puv w([v; u]); where u ! v denotes
the set of vertices adjacent to the vertex v, and u  v
the set of vertices adjacent from the vertex v:
Let H(V ) denote the space of functions, in which each
one f : V ! R assigns a real value f (v) to each vertex
v: A function in H(V ) can be thought of as a col-
umn vector in RjV j; where jV j denotes the number of
the vertices in V . The function space H(V ) then can
be endowed with the standard inner product in RjV j
as hf; giH(V ) = Pv2V f (v)g(v) for all f; g 2 H(V ):
Similarly, dene the function space H(E) consisting of
the real-valued functions on edges. When the function
space of the inner product is clear in its context, we
omit the subscript H(V ) or H(E):
For a given weighted directed graph, there is a nat-
ural random walk on the graph with the transition
probability function p : V  V ! R+ dened by
p(u; v) = w([u; v])=d+(u) for all [u; v] 2 E; and 0 oth-
erwise. The random walk on a strongly connected and
aperiodic directed graph has a unique stationary dis-
tribution ; i.e. a unique probability distribution satis-

fying the balance equations (v) = Pu!v (u)p(u; v);
for all v 2 V: Moreover, (v) > 0 for all v 2 V:

3. Regularization Framework

Given a directed graph G = (V; E) and a label set Y =
f1;1g; the vertices in a subset S  V is labeled. The
problem is to classify the vertices in the complement of
S: The graph G is assumed to be strongly connected
and aperiodic. Later we will discuss how to dispose
this assumption.
Assume a classication function f 2 H(V ); which as-
signs a label sign f (v) to each vertex v 2 V: On the
one hand, similar vertices should be classied into the
same class. More specically, a pair of vertices linked
by an edge are likely to have the same label. Moreover,
vertices lying on a densely linked subgraph are likely

Learning from Labeled and Unlabeled Data on a Directed Graph

to have the same label. Thus we dene a functional

(f ) :=

1

2 X[u;v]2E

(u)p(u; v) f (u)
p(u) 

f (v)

p(v)!2

;

(1)
which sums the weighted variation of a function on
each edge of the directed graph. On the other hand,
the initial label assignment should be changed as little
as possible. Let y denote the function in H(V ) dened
by y(v) = 1 or 1 if vertex v has been labeled as pos-
itive or negative respectively, and 0 if it is unlabeled.
Thus we may consider the optimization problem

argmin

f 2H(V )'(f ) + kf  yk2 ;

(2)

where  > 0 is the parameter specifying the tradeo
between the two competitive terms.

We will provide the motivations for the functional de-
ned by (1). In the end of this section, this functional
will be compared with another choice which may seem
more natural. The comparison may make us gain an
insight into this functional.
In Section 4, it will be
shown that this functional may be naturally derived
from a combinatorial optimization problem.
In Sec-
tion 5, we will further characterize this functional in
terms of discrete analysis on directed graphs.

For an undirected graph, it is well-known that the sta-
tionary distribution of the natural random walk has a

d(v) denotes the degree of the vertex v: Substituting
the closed form expression into (1), we have

closed form expression (v) = d(v)=Pu2V d(u); where
pd(v)!2

w([u; v]) f (u)
pd(u) 

which is exactly the regularizer of the transductive in-
ference algorithm of Zhou et al. (2004) operating on
undirected graphs.

2 X[u;v]2E

(f ) =

f (v)

1

;

For solving the optimization problem (2), we introduce
an operator  : H(V ) ! H(V ) dened by

(f )(v) =

(u)p(u; v)f (u)

and

1

2(cid:181)Xu!v
+Xuv

(v)p(v; u)f (u)

p(u)(v)
p(v)(u) :

(3)

Let  denote the diagonal matrix with (v; v) = (v)
for all v 2 V: Let P denote the transition probability
matrix and P T the transpose of P: Then

 =

1=2P 1=2 + 1=2P T 1=2

2

:

(4)

Lemma 3.1. Let  = I  ; where I denotes the
identity. Then (f ) = hf; fi:
Proof. The idea is to use summation by parts, a dis-
crete analogue of the more common integration by
parts.

=

=

f (v)

p(v)!2
(cid:190)

(u)p(u; v)

f 2(v)

1

1

f (v)

f (u)

X[u;v]2E
2 Xv2V Xu!v
+Xuv
2 Xv2V Xu!v
2Xu!v
2 Xv2V Xuv
2Xuv

p(v)!2
(u)p(u; v) f (u)
p(u) 
(u)p(u; v) f (u)
p(u) 
p(u)!2
(v)p(v; u) f (v)
p(v) 
p(u; v)f 2(u) + Xu!v
(cid:190)
p(u)(v)
p(v; u)f 2(v) + Xuv
(cid:190)
p(v)(u)

(u)p(u; v)f (u)f (v)

(v)p(v; u)f (v)f (u)

(v)

+

1

(v)p(v; u)

(u)

f 2(u)

The rst term on the right-hand side may be written

X[u;v]2E
= Xu2V Xvu
Xv2V Xu!v

p(u; v)f 2(u) = Xu2V Xvu
p(u; v)! f 2(u) = Xu2V
(v) ! f 2(v) = Xv2V

(u)p(u; v)

and the second term

p(u; v)f 2(u)

f 2(u) = Xv2V

f 2(v);

Similarly, for the fourth and fth terms, we can show
that

Xv2V Xuv

p(v; u)f 2(v) = Xv2V

f 2(v);

(v)p(v; u)

(u)

Xv2V Xuv

respectively. Therefore,

(f ) = Xv2V f 2(v) 

1

f 2(u) = Xv2V
2(cid:181)Xu!v
p(v)(u)

(v)p(v; u)f (v)f (u)

which completes the proof.

+Xuv

(u)p(u; v)f (u)f (v)

p(u)(v)
(cid:190);

f 2(v):

f 2(v):

Learning from Labeled and Unlabeled Data on a Directed Graph

Lemma 3.2. The eigenvalues of the operator  are in
[1; 1]; and the eigenvector with the eigenvalue equal
to 1 is p:

Proof. It is easy to see that  is similar to the operator

 : H(V ) ! H(V ) dened by  =P + 1P T  =2:
Hence  and  have the same set of eigenvalues. As-
sume that f is the eigenvector of  with eigenvalue :
Choose a vertex v such that jf (v)j = maxu2V jf (u)j:
Then we can show that jj  1 by

jjjf (v)j = 

(v; u)f (u)
Xu2V
2 Xuv
= jf (v)j

 Xu2V
p(v; u) + Xu!v

(v; u)jf (v)j
(v) !

(u)p(u; v)

= jf (v)j:

In addition, we can show that p = p by

1

1

1

=



(v)p(v; u)

(u)p(u; v)

(u)p(u; v)p(u)
2(cid:181)Xu!v
p(u)(v)
(v)p(v; u)p(u)
+Xuv
p(v)(u)
2Xu!v
+ Xuv
p(v)
2 1
p(v) Xu!v
= p(v):
Theorem 3.3. The solution of (2) is f  = (1)(I
)1y; where  = 1=(1 + ):

p(v) !
(u)p(u; v) +p(v)Xuv

=

p(v; u)!

Proof. From Lemma 3.1, dierentiating (2) with re-
spect to function f; we have (I  )f  + (f  y) = 0:
Dene  = 1=(1 + ): This system may be written
(I  )f  = (1  )y: From Lemma 3.2, we easily
know that (I  ) is positive denite and thus in-
vertible. This completes the proof.

At the beginning of this section, we assume the graph
to be strongly connected and aperiodic such that the
natural random walk over the graph converges to a
unique and positive stationary distribution. Obviously
this assumption cannot be guaranteed for a general di-
rected graph. To remedy this problem, we may intro-
duce the so-called teleporting random walk (Page et al.,
1998) as the replacement of the natural one. Given

that we are currently at vertex u with d+(u) > 0; the
next step of this random walk proceeds as follows: (1)
with probability 1   jump to a vertex chosen uni-
formly at random over the whole vertex set except u;
and (2) with probability w([u; v])=d+(u) jump to a
vertex v adjacent from u: If we are at vertex u with
d+(u) = 0; just jump to a vertex chosen uniformly at
random over the whole vertex set except u:

Algorithm. Given a directed graph G = (V; E) and
a label set Y = f1;1g; the vertices in a subset S  V
are labeled. Then the remaining unlabeled vertices
may be classied as follows:

1. Dene a random walk over G with a transition
probability matrix P such that it has a unique sta-
tionary distribution, such as the teleporting ran-
dom walk.

2. Let  denote the diagonal matrix with its di-
agonal elements being the stationary distribu-
tion of the random walk. Compute the matrix
 = (1=2P 1=2 + 1=2P T 1=2)=2:

3. Dene a function y on V with y(v) = 1 or 1 if
vertex v is labeled as 1 or 1; and 0 if v is unla-
beled. Compute the function f = (I  )1y;
where  is a parameter in ]0; 1[; and classify each
unlabeled vertex v as sign f (v):

It is worth mentioning that the approach of Zhou
et al.
(2005) can also be derived from this algo-
rithmic framework by dening a two-step random
walk. Assume a directed graph G = (V; E) with
d+(v) > 0 and d(v) > 0 for all v 2 V: Given that
we are currently at vertex u; the next step of this
random walk proceeds as follows: rst jump back-
ward to a vertex h adjacent to u with probability
p(u; h) = w([h; u])=d(u); then jump forward to a
vertex v adjacent from u with probability p+(h; v) =
w([h; v])=d+(h): Thus the transition probability from

to show that the stationary distribution of the random

u to v is p(u; v) = Ph2V p(u; h)p+(h; v): It is easy
walk is (v) = d(v)=Pu2V d(u) for all v 2 V: Sub-

stituting the quantities of p(u; v) and (v) into (1), we
then recover one of the two regularizers proposed by
Zhou et al. (2005). The other one can also be recov-
ered simply by reversing this two-step random walk.

Now we discuss implementation issues. The closed
form solution shown in Theorem 3.3 involves a ma-
trix inverse. Given an n  n invertible matrix A; the
time required to compute the inverse A1 is gener-
ally O(n3) and the representation of the inverse re-
quires (n2) space. Recent progress in numerical anal-

Learning from Labeled and Unlabeled Data on a Directed Graph

ysis (Spielman & Teng, 2003), however, shows that,
for an n  n symmetric positive semi-denite, diag-
onally dominant matrix A with m non-zero entries
and a n-vector b; we can obtain a vector ~x within rel-
ative distance  of the solution to Ax = b in time
O(m1:31 log(nf (A)=)O(1)); where f (A) is the log of
the ratio of the largest to smallest non-zero eigenvalue
of A: It can be shown that our approach can benet
from this numerical technique. From Theorem 3.3,

(cid:181)I  

1=2P 1=2 + 1=2P T 1=2

2

 f  = (1  )y;

which may be transformed into

2

P + P T 

(cid:181)  
Let A =   
is diagonally dominant.

2

P + P T 

 (1=2f ) = (1  )1=2y:

: It is easy to verify that A

For well understanding this regularization framework,
we may compare it with an alternative approach in
which the regularizer is dened by

(f ) = X[u;v]2E

w([u; v]) f (u)

pd+(u) 

f (v)

pd(v)!2

:

(5)
A similar closed form solution can be obtained from
the corresponding optimization problem. Clearly, for
undirected graphs, this functional also reduces to that
in (Zhou et al., 2004). At rst glance, this functional
may look natural, but in the later experiments we will
show that the algorithm based on this functional does
not work as well as the previous one. This is because
the directionality is only slightly taken into account by
this functional via the degree normalization such that
much valuable information for classication conveyed
by the directionality is ignored by the corresponding
algorithm. Once we remove the degree normalization
from this functional, then the resulted functional is
totally insensitive to the directionality.

4. Directed Spectral Clustering

In the absence of labeled instances, this framework can
be utilized in an unsupervised setting as a spectral
clustering method for directed graphs. We rst dene
a combinational partition criterion, which generalizes
the normalized cut criterion for undirected graphs (Shi
& Malik, 2000). Then relaxing the combinational op-
timization problem into a real-valued one leads to the
functional dened in Section 3.

Given a subset S of the vertices of a directed graph G;

dene the volume of S by vol S :=Pv2S (v): Clearly,

Figure 2. A subset S and its complement S c. Note that
there is only one edge in the out-boundary of S:

vol S is the probability with which the random walk
occupies some vertex in S and consequently vol V = 1:
Let Sc denote the complement of S (Fig. 2). The out-
@S of S is dened by @S := f[u; v]ju 2
boundary
S; v 2 Scg: The value vol @S := P[u;v]2@S (u)p(u; v)
is called the volume of @S: Note that vol @S is the
probability with which one sees a jump from S to S c:

Generalizing the normalized cut criterion for undi-
rected graphs is based on a key observation stated by
Proposition 4.1. vol @S = vol @S c:

Proof. It immediately follows from that the probabil-
ity with which the random walk leaves a vertex equals
the probability with which the random walk arrives at
this vertex. Formally, for each vertex v in V; it is easy
to see that

Xu!v

(u)p(u; v)  Xuv

(v)p(v; u) = 0:

Summing the above equation over the vertices of S
(see also Fig. 2), then we have

Xv2SXu!v
= X[u;v]2@S c

(u)p(u; v)  Xu!v
(u)p(u; v)  X[u;v]2@S

(v)p(v; u)!

(u)p(u; v) = 0;

which completes the proof.

From Proposition 4.1, we may partition the vertex set
of a directed graph into two nonempty parts S and S c
by minimizing

Ncut(S) = vol @S(cid:181) 1

vol S

+

1

vol Sc ;

(6)

which is a directed generalization of the normalized
cut criterion for undirected graphs. Clearly, the ra-
tio of vol @S to vol S is the probability with which the

Learning from Labeled and Unlabeled Data on a Directed Graph

random walk leaves S in the next step under the con-
dition that it is in fact in S now. Similarly understand
the ratio of vol @Sc to vol Sc:

In the following, we show that the functional (1) can
be recovered from (6). Dene an indicator function
h 2 H(V ) by h(v) = 1 if v 2 S; and 1 if v 2 S c:
Denote by  the volume of S: Clearly, we have 0 <
 < 1 due to S  G: Then (6) may be written
(u)p(u; v)(h(u)  h(v))2

Ncut(S) = P[u;v]2E

8(1  )

Dene another function g 2 H(V ) by g(v) = 2(1  )
if v 2 S; and 2 if v 2 Sc: We easily know that
sign g(v) = sign h(v) for all v 2 V and h(u)  h(v) =
g(u) g(v) for all u; v 2 V: Moreover, it is not hard to
see that Pv2V (v)g(v) = 0; and Pv2V (v)g2(v) =
4(1  ): Therefore
Ncut(S) = P[u;v]2E

(u)p(u; v)(g(u)  g(v))2
2 Pv2V

Dene another function f = pg: Then the above
equation may be further transformed into

(v)g2(v)

:

:

Ncut(S) = P[u;v]2E

(u)p(u; v) f (u)
p(u) 
2hf; fi

f (v)

p(v)!2

:

If we allow the function f to take arbitrary real values,
then the graph partition problem (6) becomes

argmin
f 2H(V )

(f )

subject to

kfk = 1; hf; pi = 0:

(7)

From Lemma 3.2, it is easy to see that the solution
of (7) is the normalized eigenvector of the operator 
with the second largest eigenvalue.

Algorithm. Given a directed graph G = (V; E); it
may be partitioned into two parts as follows:

1. Dene a random walk over G with a transition
probability matrix P such that it has a unique
stationary distribution.

2. Let  denote the diagonal matrix with its di-
agonal elements being the stationary distribu-
tion of the random walk. Compute the matrix
 = (1=2P 1=2 + 1=2P T 1=2)=2:

3. Compute the eigenvector ' of  corresponding to
the second largest eigenvalue, and then partition
the vertex set V of G into two parts S = fv 2
V j'(v)  0g and Sc = fv 2 V j'(v) < 0g:

It is easy to extend this approach to k-partition. As-
sume a k-partition to be V = V1 [ V2 [[ Vk; where
Vi \ Vj = ; for all 1  i; j  k: Let Pk denote a k-
partition. Then we may obtain a k-partition by mini-
mizing

Ncut(Pk) = X1ik

vol @Vi
vol Vi

:

(8)

It is not hard to show that the solution of the corre-
sponding relaxed optimization problem of (8) can be
any orthonormal basis for the linear space spanned by
the eigenvectors of  pertaining to the k largest eigen-
values.

5. Discrete Analysis

We develop discrete analysis on directed graphs. The
regularization framework in Section 3 is then recon-
structed and generalized using discrete analysis. This
work is the discrete analogue of classic regularization
theory (Tikhonov & Arsenin, 1977; Wahba, 1990).
We dene the graph gradient to be an operator r :
H(V ) ! H(E) which satises

(rf )([u; v]) :=p(u)s p(u; v)

(v)

f (v) s p(u; v)

(u)

f (u)! :

(9)

For an undirected graph, equation (9) reduces to

(rf )([u; v]) =s w([u; v])

d(v)

f (v) s w([u; v])

d(u)

f (u):

We may also dene the graph gradient of function f
at each vertex v as rf (v) := f(rf )([v; u])j[v; u] 2 Eg;
which is often denoted by rvf: Then the norm of the
graph gradient rf at v is dened by

1

krvfk :=Xuv

and the p-Dirichlet form

2

(rf )2([v; u])!

;

(10)

p(f ) :=

1

2 Xv2V

krvfkp; p 2 [1;1[:

(11)

Note that 2(f ) = (f ): Intuitively, the norm of the
graph gradient measures the smoothness of a function
around a vertex, and the p-Dirichlet form the smooth-
ness of a function over the whole graph. In addition,
we dene krf ([v; u])k := krvfk: Note that krfk is
dened in the space H(E) as krfk = hrf;rfi1=2
H(E):

We dene the graph divergence to be an operator div :
H(E) ! H(V ) which satises

hrf; giH(E) = hf; div giH(V )

(12)

Learning from Labeled and Unlabeled Data on a Directed Graph

for any two functions f and g in H(E): Equation (12)
is a discrete analogue of the Stokes theorem 1. It is
not hard to show that

(div g)(v) =

1

p(v)(cid:181)Xuvp(v)p([v; u])g([v; u])
Xu!vp(u)p(u; v)g([u; v]):

(13)

Intuitively, we may think of the graph divergence
(div g)(v) as the net out(cid:176)ow of the function g at
the vertex v: For a function c : E ! R dened by
c([u; v]) =p(u)p(u; v); it follows from equation (13)
that (div c)(v) = 0 at any vertex v in V:
We dene the graph Laplacian to be an operator  :
H(V ) ! H(V ) which satises 2

f := 

1
2

div(rf ):

(14)

We easily know that the graph Laplacian is linear, self-
adjoint and positive semi-denite. Substituting (9)
and (13) into (14), we obtain

(f )(v) = f (v) 
+Xuv

(u)p(u; v)f (u)

1

(v)p(v; u)f (u)

2(cid:181)Xu!v
p(u)(v)
p(v)(u) :

(15)

In matrix notation,  can be written as

 = I 

1=2P 1=2 + 1=2P T 1=2

2

;

(16)

which is just the Laplace matrix for directed graphs
proposed by Chung (to appear). For an undirected
graph, equation (16) clearly reduces to the Laplacian
for undirected graphs (Chung, 1997).

We dene the graph p-Laplacian to be an operator
p : H(V ) ! H(V ) which satises

pf := 

1
2

div(krfkp2rf ):

Clearly, 2 = ; and p(p 6= 2) is nonlinear.
addition, p(f ) = hpf; fi :
Now we consider the optimization problem

argmin

f 2H(V )'p(f ) + kf  yk2 :

1Given a compact Riemannian manifold (M; g) with
a function f 2 C1(M ) and a vector eld X 2 X (M );

it follows from the Stokes theorem that RM hrf; Xi =
RM (div X)f:

2The Laplace-Beltrami operator  : C 1(M ) !
C1(M ) is dene by f =  div(rf ): The additional fac-
tor 1=2 in (14) is due to edges being oriented.

(17)

In

(18)

Let f  denote the solution of (18). It is not hard to
show that

ppf  + 2(f   y) = 0:

(19)
When p = 2; f  + (f   y) = 0; as we have shown
before, which leads to the closed form solution in The-
orem 3.3. When p 6= 2; we are not aware of any closed
form solution.

6. Experiments

We address the web categorization task on the WebKB
dataset (see http://www-2.cs.cmu.edu/~webkb/). We
only consider a subset containing the pages from the
four universities Cornell, Texas, Washington and Wis-
consin, from which we remove the isolated pages, i.e.,
the pages which have no incoming and outgoing links,
resulting in 858, 825, 1195 and 1238 pages respectively,
for a total of 4116. These pages have been manually
classied into the following seven categories: student,
faculty, sta, department, course, project and other.
We may assign a weight to each hyperlink according
to the textual content or the anchor text. However,
here we are only interested in how much we can obtain
from link structure only and hence adopt the canonical
weight function.

We compare the approach in Section 3 with its coun-
terpart based on (5). Moreover, we also compare both
methods with the schemes in (Zhou et al., 2005; Zhou
et al., 2004). For the last approach, we transform a
directed graph into an undirected one by dening a
symmetric weight function as w([u; v]) = 1 if [u; v] or
[v; u] in E: To distinguish among these approaches,
we refer to them as distribution regularization, degree
regularization, second-order regularization and undi-
rected regularization respectively. As we have shown,
both the distribution and degree regularization ap-
proaches are generalizations of the undirected regu-
larization method.

The investigated task is to discriminate the student
pages in a university from the non-student pages in the
same university. We further remove the isolated pages
in each university. Other categories including faculty
and course are considered as well. For all approaches,
the regularization parameter is set to  = 0:1 as in
(Zhou et al., 2005). In the distribution regularization
approach, we adopt the teleporting random walk with
a small jumping probability  = 0:01 for obtaining a
unique and positive stationary distribution. The test-
ing errors are averaged over 50 trials.
In each trial,
it is randomly decided which of the training points
get labeled.
If there is no labeled point existed for
some class, we sample again. The experimental re-

Learning from Labeled and Unlabeled Data on a Directed Graph

distribution regularization
degree regularization
secondorder regularization
undirected regularization

0.6

0.5

r
o
r
r
e

t
s
e
t

0.4

0.3

0.2

0.1

distribution regularization
degree regularization
secondorder regularization
undirected regularization

0.7

0.6

0.5

0.4

0.3

0.2

r
o
r
r
e

t
s
e
t

distribution regularization
degree regularization
secondorder regularization
undirected regularization

2

4

6

10

8
14
# labeled points

12

16

18

20

2

4

6

10

8
14
# labeled points

12

16

18

20

2

4

6

10

8
14
# labeled points

12

16

18

20

(a) Cornell (student)

(b) Texas (student)

(c) Washington (student)

distribution regularization
degree regularization
secondorder regularization
undirected regularization

0.6

0.5

r
o
r
r
e

t
s
e
t

0.4

0.3

0.2

0.1

distribution regularization
degree regularization
secondorder regularization
undirected regularization

r
o
r
r
e

t
s
e
t

0.5

0.4

0.3

0.2

0.1

distribution regularization
degree regularization
secondorder regularization
undirected regularization

r
o
r
r
e

t
s
e
t

r
o
r
r
e

t
s
e
t

0.6

0.5

0.4

0.3

0.2

0.1

0.7

0.6

0.5

0.4

0.3

0.2

2

4

6

10

8
14
# labeled points

12

16

18

20

2

4

6

10

8
14
# labeled points

12

16

18

20

2

4

6

10

8
14
# labeled points

12

16

18

20

(d) Wisconsin (student)

(e) Cornell (faculty)

(f) Cornell (course)

Figure 3. Classication on the WebKB dataset. Fig. (a)-(d) depict the test errors of the regularization approaches on the
classication problem of student vs. non-student in each university. Fig. (e)-(f) illustrate the test errors of these methods
on the classication problems of faculty vs. non-faculty and course vs. non-course in Cornell University.

sults are shown in Fig. 3. The distribution regulariza-
tion approach shows signicantly improved results in
comparison to the degree regularization method. Fur-
thermore, the distribution regularization approach is
comparable with the second-order regularization one.
In contrast, the degree regularization approach shows
similar performance to the undirected regularization
one. Therefore we can conclude that the degree reg-
ularization approach almost does not take the direc-
tionality into account.

