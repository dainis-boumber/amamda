Abstract

Real-world  learning  tasks  may involve  high-dimensional  data sets
with  arbitrary  patterns of missing  data.  In  this  paper  we  present
a  framework  based  on  maximum likelihood  density  estimation for
learning from such  data set.s.  VVe  use  mixture models for  the  den(cid:173)
sity  estimates and  make two  distinct  appeals  to  the  Expectation(cid:173)
Maximization  (EM)  principle  (Dempster  et  al.,  1977)  in  deriving
a  learning  algorithm-EM is  used  both  for  the  estimation of mix(cid:173)
ture  components  and  for  coping  wit.h  missing  dat.a.  The  result(cid:173)
ing  algorithm  is  applicable  t.o  a  wide  range  of supervised  as  well
as  unsupervised  learning  problems.  Result.s  from  a  classification
benchmark-t.he iris  data set-are presented.

1

Introduction

Adaptive  systems  generally  operate  in  environments  t.hat  are fraught  with  imper(cid:173)
fections;  nonet.heless  they  must  cope  with  these  imperfections and learn  to extract
as  much  relevant  information  as  needed  for  their  part.icular  goals.  One  form  of
imperfection is  incomplet.eness in sensing information.  Incompleteness can arise ex(cid:173)
trinsically  from  the  data generation  process  and  intrinsically  from  failures  of the
system's sensors.  For example,  an object  recognition  system  must  be  able to learn
to classify images with occlusions, and  a robotic controller must be able to integrate
multiple sensors  even  when  only  a  fraction  may operate at any given  time.

In this paper we present a. fra.mework-derived from parametric statistics-for learn-

120

Supervised Learning from Incomplete Data via an EM Approach

121

ing from data sets with arbitrary patterns of incompleteness.  Learning in this frame(cid:173)
work  is  a  classical  estimation problem requiring an explicit  probabilistic model and
an  algorithm for  estimating the  parameters of the  model.  A  possible  disadvantage
of parametric methods is  their lack of flexibility  when  compared with  nonparamet(cid:173)
ric  methods.  This  problem,  however,  can  be  largely  circumvented  by  the  use  of
mixture models  (McLachlan  and  Basford,  1988) .  Mixture  models combine much of
the flexibility of nonparametric methods with  certain of the analytic advantages of
parametric methods.

Mixture models have  been  utilized  recently  for  supervised  learning problems in  the
form  of  the  "mixtures  of experts"  architecture  (Jacobs  et  al.,  1991;  Jordan  and
Jacobs,  1994).  This  architecture  is  a  parametric  regression  model  with  a  modular
structure  similar  to  the  nonparametric  decision  tree  and  adaptive  spline  models
(Breiman  et  al.,  1984;  Friedman,  1991).  The approach  presented  here  differs  from
these  regression-based  approaches  in  that  the  goal  of learning  is  to  estimate  the
density of the data.  No distinction is  made between input and output variables; the
joint  density  is  estimated  and  this  estimate  is  then  used  to form  an  input/output
map.  Similar  approaches  have  been  discussed  by  Specht  (1991)  and  Tresp  et  al.
(1993).  To estimate  the  vector  function  y  =  I(x)  the joint density  P(x, y)  is  esti(cid:173)
mated  and,  given  a  particular  input  x,  the  conditional  density  P(ylx)  is  formed.
To obtain  a  single  estimate  of y  rather  than  the  full  conditional  density  one  can
evaluate y =  E(ylx), the expectation of y  given  x.
The  density-based  approach  to  learning  can  be  exploited  in  several  ways .  First,
having  an  estimate  of the  joint  density  allows  for  the  representation  of any  rela(cid:173)
tion  between  the  variables.  From  P(x, y),  we  can  estimate y =  I(x),  the  inverse
x = 1-1 (y),  or  any  other  relation  between  two  subsets  of the elements of the  con(cid:173)
catenated  vector  (x, y).
Second,  this  density-based  approach  is  applicable  both  to supervised  learning and
unsupervised  learning  in  exactly  the  same  way.  The  only  distinction  between  su(cid:173)
pervised  and  unsupervised  learning  in  this  framework  is  whether  some portion  of
the data vector  is  denoted  as  "input"  and  another  portion  as  "target".

Third,  as  we  discuss  in  this  paper, the density-based  approach  deals  naturally with
incomplete  data,  i.e.  missing  values  in  the  data set.  This  is  because  the  problem
of estimating mixture densities  can  itself be  viewed  as  a  missing data problem  (the
"labels" for  the component densities are missing) and an Expectation-Maximization
(EM)  algorithm (Dempster et  al.,  1977)  can  be  developed  to  handle  both  kinds  of
missing data.

2  Density estimation using  EM

This  section  outlines  the  basic  learning  algorithm  for  finding  the  maximum  like(cid:173)
lihood  parameters  of a  mixture  model  (Dempster  et  al.,  1977;  Duda  and  Hart,
1973;  Nowlan,  1991).  \IVe  assume  that.  t.he  data  . .:t'  =  {Xl, ... , XN}  are  generated
independently  from  a  mixture density

1\1

P(Xi) = LP(Xi IWj;(}j)P(Wj),

;=1

(1)

122

Ghahramani and Jordan

where each  component of the  mixture is  denoted Wj  and  parametrized by  (}j.  From
equation  (1)  and  the independence  assumption we see  that the log likelihood of the
parameters given  the  data set  is

N

M

l((}IX) = LlogLP(xilwj;Oj)P(Wj).

(2)

i=1

j=1

By  the  maximum  likelihood  principle  the  best  model of the  data  has  parameters
that maximize l(OIX).  This function,  however,  is  not easily maximized numerically
because  it involves  the log  of a  sum.

Intuitively, there is  a  "credit-assignment"  problem:  it is not clear  which component
of the  mixture generated  a  given  data point  and  thus  which  parameters  to  adjust
to fit  that data point.  The EM  algorithm for  mixture models is an  iterative method
for  solving  this  credit-assignment  problem.  The intuition  is  that  if one had  access
to  a  "hidden"  random  variable  z  that  indicated  which  data  point  was  genera.ted
by  which  component,  then  the  maximization  problem  would  decouple  into  a  set
of simple  maximizations.  Using  the  indicator  variable  z,  a  "complete-data"  log
likelihood function  can  be  written

N  M

lc((}IX, Z) =  L  L  Zij log P(XdZi; O)P(Zi; (}),

(3)

;=1  j=1

which  does  not  involve a  log of a  summation.
Since  Z  is  unknown  lc  cannot  be  utilized  directly,  so  we  instead  work  with  its  ex(cid:173)
pectation, denoted by  Q(OI(}k)'  As  shown by  (Dempster et aI.,  1977), l(OIX)  can  be
maximized  by  iterating the following  two steps:

Estep:  Q(OI(}k)
M  step:

(}k+l

E[lc(OIX,Z)IX,(}k]
argmax  Q((}IOk)'

o

(4)

The E  (Expectation) step  computes the expected  complete data log likelihood  and
the  M  (Maximization)  step  finds  the  parameters  that  maximize  this  likelihood.
These  two  steps  form  the  basis  of the  EM  algorithm;  in  the  next  two  sections  we
will outline how  they  can  be  used  for  real  and  discrete  density  estimation.

2.1  Real-valued data:  Inixture of Gaussians

Real-valued  data can  be  modeled  as  a  mixture of Gaussians.  For  this  model  the

E-step simplifies  to  computing hij = E[Zijlxi,Ok],  the  probability that  Gaussian  j,

as  defined  by  the  parameters estimated  at time step  k,  generated  data point  i.

Itj 1- 1/ 2 exp{ -~ (Xi  -

itj)Tt;l,k(Xi - itj)}

h ..  =

I}

L~1 IEfl-l/2exp{-~(Xi - it7)TE,I,k(Xi - it7)}'

(5)

The  M-step  re-estimates  the
data set  weighted  by  the  hii=

means  and  covariances  of the  Gaussians1  using  the

)  ~ k+l  _  L~l hijXi
a
Li=1 hij

I-Lj

N

-

'

1 Though this derivation  assumes equal priors for  the Gaussians,  if the priors arc viewed

as  mixing  parameters they can  also  be learned  in  the  maximization  step.

Supervised Learning from Incomplete Data via an EM Approach

123

2.2  Discrete-valued data:  Inixture of Bernoullis

D-dimensional binary  data x  =  (Xl, . .. ,Xd, . .. XD),  Xd  E  {O, 1}, can  be  modeled  as
a  mixture of !II  Bernoulli  densities.  That is,

M

P(xIO) = L P(Wj) IT /-ljd(1  - /-ljd)(l-Xd).

D

For  this  model  the  E-step  involves  computing

nD

h ..  -
I)  -

pX,ld (1  _  p.  )(1-Xld)

d=l}d

}d

'Ef'!l nf=l P7J d  (1  - Pld)(1-xld) ,

(7)

(8)

(9)

and  the  M-step  again re-estimates  the parameters by

~ k+l  _ 'E~l hijXi

ttj

-

N

.

'Ei=l hij

More  generally, discrete or categorical  data can  be modeled  as generated  by  a  mix(cid:173)
ture of multinomial densities  and similar derivations for  the  learning algorithm can
be  applied.  Finally, the  extension  to  data with  mixed real,  binary.  and  categorical
dimensions can  be  readily  derived  by  assuming  a joint  density  with  mixed  compo(cid:173)
nents  of the  three  types .

3  Learning from  inco111plete  data

In  the  previous  section  we  presented  one  aspect  of  the  EM  algorithm:  learning
mixture  models.  Another  important  application  of EM  is  to  learning  from  data
sets  with  missing  values  (Little  and  Rubin,  1987;  Dempster  et  aI.,  1977).  This
application  has  been  pursued  in  the  statistics  literature  for  non-mixture  density
estimation problems;  in  this  paper  we  combine this application of EM  with that of
learning  mixture parameters.
We  assume  that.  the  data  set  ,l:'  =  {Xl .. . , XN}  is  divided  into  an  observed  com(cid:173)
ponent ,yo  and  a  missing component ;t'm.  Similarly, each  data vector  Xi  is  divided
into (xi, xi)  where  each  data vector  can  have  different  missing components-this
would be denoted  by superscript  Dli  and  OJ.  but we  have simplified the notation for
the sake  of clarity.

To handle missing data we  rewrite  the  EM  algorithm as  follows

Estep:
M step:

E[ic( fJl,t', ;t'm , Z) I;t'. Ok]
argmax  Q(fJlfJk).

o

(10)

Comparing to equation (4)  we see  that aside from  t.he  indicator variables Z  we  have
added  a  second  form  of incomplete  data,  ;t'm ,  corresponding  to  the  missing  values
in  the  data set.  The E-step  of the  algorithm estimates both  these  forms  of missing
information; in  essence  it uses  the current  estimate of the data density  to complete
the missing values.

124

Ghahramani and Jordan

3.1  Real-valued data:  mixture of Gaussians

We start  by  writing the log  likelihood of the complete data,

ic(OIXO, xm, Z) = L L Zij  log P(xdzj, 0) + L L Zij  log P(zdO).

N  M

N  M

(11)

j

j

We  can  ignore  the second  term since  we  will  only  be  estimating the  parameters of
the  P(XdZi, 0).  Using  equation  (11)  for  the  mixture of Gaussians  we  not.e  that  if
only the indicator variables Zi  are missing,  the E step  can be reduced  to estimating
E[ Zij lXi, 0].  For the case  we  are interested  in,  with  two types of missing data Zi  and
xi, we  expand  equation  (11)  using m  and  0  superscripts  to denote subvectors  and
submatrices  of the  parameters  matching  the  missing  and  observed  components  of
the data,

Ic(OIXO,  xm, Z) = L L Zij[n log27r + ! log IEj 1- !(xi -l1-jf E;l,OO(xi -l1-j)

N  M

. .22

I

J

2
m)  1(  m
- 2 Xi

-

I1-j

I1-j

0

(

o)T~-l,Om(  m
Xi  -

L...j

I1-j

m)T~-l,mm(  m
Xi

L...j

-

I1-j

- Xi  -

m)]

Note  that  after  taking  the  expectation,  the  sufficient  statistics  for  the  parameters
involve  three  unknown  terms,  Zij,  ZijXi,  and  zijxixiT.  Thus  we  must  compute:
E[Zijlx?,Ok]'  E[Zijxilx?,Ok],  and  E[ZijxixinTlx?,Ok].
One intuitive approach  to  dealing with  missing data is  to  use  the  current  estimate
of the  data  density  to  compute  the  expectat.ion  of the  missing data  in  an  E-step,
complete the  data with  these expectations,  and  then  use  this completed data to re(cid:173)
estimate  parameters in  an  M-step.  However,  this  intuition fails  even  when  dealing
with a single two-dimensional Gaussian;  the expectation of the missing data always
lies  along  a  line,  which  biases  the  estimate of the  covariance.  On  the  other  hand,
the approach arising from application of the EM  algorithm specifies  that one should
use  the current  density estimate to compute the expectation of whatever incomplete
terms  appear  in  the  likelihood  maximization.  For  the  mixture of Gaussians  these
incomplete  terms  involve  interactions  between  the  indicator  variable  :;ij  and  the
first  and  second  moments of xi.  Thus,  simply  computing  the  expectation  of the
missing  data Zi  and  xi from  our  model  and  substituting  those  values  into  the  M
step  is  not sufficient  to guarantee an  increase  in  the likelihood of the  parameters.
The above  terms  can  be  computed  as  follows:  E[ Zij lxi, Ok]  is  again  hij,  the  proba(cid:173)
bility as  defined  in  (5)  measured only on  the observed  dimensions of Xi,  and
E[Zijxilxi, Ok]  = hijE[xilzij = 1, xi, Od  = hij(l1-j + EjOEjO-l (xi -Il.'},  (12)
Defining xi] = E[xi IZij  = 1, xi, Ok],  the  regression  of xi on  xi using Gaussian j,
(13)

E[  ..  m  mTI   0  ] _  h .. (~mm  ~mo~oo-l ~moT  ~ m ~ mT)
.

L...j  ~j

+ XijXij

Z'Jxi  Xi

xi'  k  -

'J  L...j

L...j

-

The  M-step  uses  these  expectations  substituted  into  equations  (6)a  and  (6)b  to
re-estimate  the  means  and  covariances.  To  re-estimate  the  mean  vector,  I1-j'  we
substitute  the  values  E[xilzij  =  1, xi, Ok]  for  the  missing  components  of  Xi  in
equation  (6)a.  To  re-estimate  the  covariance  matrix  we  substitute  t.he  values
E[xixiTlzij =  1, xi, Ok]  for  the outer product matrices involving the missing com(cid:173)
ponents of Xi  in equation  (6)b.

Supervised Learning from Incomplete Data via an EM Approach

125

3.2  Discrete-valued data:  Inixture of Bernoullis

For the  Bernoulli mixture the sufficient statistics for  the  M-step  involve  t he  incom(cid:173)
plete terms E[Zij Ix?, Ok]  and E[ Zij xi Ix~, Ok].  The first  is equal to hij calculated over
the  observed  subvector of Xi.  The second,  since we  assume  that  within  a  class  the
individual  dimensions  of the  Bernoulli  variable  are  independent.,  is  simply  hijl-Lj.
The M-step  uses  these expectations substituted  into equation  (9).

4  Supervised learning

If each  vector  Xi  in  the  data set  is  composed  of an  "input"  subvector,  x},  and  a
"target"  or output  subvector,  x?,  then  learning  the joint density  of the  input  and
target  is  a  form of supervised  learning.  In  supervised  learning we  generally  wish  to
predict  the output variables from the input variables.  In this section we  will outline
how  this is  achieved  using  the estimated density.

4.1  Function approximation

For  real-valued  function  approximation  we  have  assumed  that  the  densit.y  is  esti(cid:173)
mated  using  a  mixture of Gaussians.  Given  an  input  vector  x~  we  ext ract  all  the
relevant  information  from  the  density  p(xi, XO)  by  conditionalizing  t.o  p(xOlxD.
For  a  single  Gaussian  this  conditional  densit.y  is  normal,  and,  since  P(x 1 ,  XO)  is  a
mixture  of Gaussians  so  is  P(xolxi ).  In  principle,  this  conditional  density  is  the
final  output  of the  density  estimator.  That  is,  given  a  particular  input  the  net(cid:173)
work  returns  the complete conditional density of t.he  output.  However,  since  many
applications  require  a  single  estimate  of  the  output,  we  note  three  ways  to  ob(cid:173)
tain  estimates  x  of  XO  =  f(x~):  the  least  squares  estimate  (LSE),  which  takes
XO(xi)  =  E(xOlxi);  stochastic  sampling  (STOCH),  which  samples  according  to
the  distribution  xO(xD  ""  P(xOlxi);  single  component  LSE  (SLSE),  which  takes
xO(xD  = E(xOlxLwj)  where  j  = argmaxk P(zklx~).  For  a  given  input,  SLSE  picks
the  Gaussian  with  highest  posterior  and  approximates  the  out.put  with  the  LSE
estimator given  by  that  Gaussian  alone.

The conditional expectation or  LSE  estimator for  a  Gaussian  mixt.ure  is

(14)

which  is  a  convex  sum  of linear  approximations,  where  the  weights  h ij  vary  non(cid:173)
linearly  according  to equation  (14)  over  the  input space.  The  LSE  estimator on  a
Gaussian  mixture has  interesting  relations  to  algorithms such  as  CART  (Breiman
et al.,  1984), MARS (Friedman,  1991), and  mixtures of experts  (Jacobs <.'t  al.,  1991;
Jordan  and  Jacobs,  1994),  in  that  the  mixture  of Gaussians  competit.ively  parti(cid:173)
tions  the input space,  and learns a  linear regression  surface on  each  part-it.ion.  This
similarity has  also  been  noted  by  Tresp  et  al.  (1993)  .

The stochastic estimator (STOCH) and  the single component estimator (SLSE)  are
better suited  than  any least squares  method  for  learning non-convex  ill verse  maps,
where  the  mean of several  solutions  to  an  inverse  might  not  be  a  solut ion.  These

126

Ghahramani and Jordan

Figure  1:  Classification  of the  iris  data
set.  100  data points were  used for  train(cid:173)
ing  and  50  for  testing.  Each  data point
consisted  of 4 real-valued attributes and
one  of  three  class  labels.  The  figure
shows  classification  performance    1
standard  error  (11  = 5)  as  a  function
of  proportion  missing  features  for  the
EM  algorithm  and  for  mean  imputa(cid:173)
tion (MI), a common heuristic where the
missing  values  are  replaced  with  their
unconditional  means.

Classification  with  missing  inputs

100

~" 0-1---~,  !  -t EM
U
;.;::
'" '" !!  60
U ... U
II) .. o 40

\ , 'l,_

\
,
, ,

,

-'-'tI  MI

U
~

20

o

20

40

60

80

100

%  missing features

estimators take advantage of the explicit representat.ion of the input/output density
by  selecting one of the several  solutions to  the inverse.

4.2  Classification

Classification  problems involve  learning  a  mapping from  an  input space  into  a  set
of discrete  class  labels.  The  density  estimat.ion  framework  presented  in  this  paper
lends  itself to solving classification  problems by  estimating the joint density  of the
input  and  class  label  using  a  mixture model.  For example,  if the  inputs have real(cid:173)
valued  attributes and  there  are  D  class  labels,  a  mixture model with Gaussian  and
multinomial components will  be  used:

P(x, e = dlO) = ~ P(Wj) (27r)n/2IEj 11/2  exp{ -"2 (x - I-tj fEj1 (x -

~jd

I-'j n,

(15)

AI
~

1

denoting  the  joint  probability  that  the  data  point.  is  x  and  belongs  to  class  d,
where  the  ~j d  are  the  parameters for  the  multinomial.  Once  this  density  has  been
estimated,  the maximum likelihood  label  for  a  particular input  x  may be obtained
by computing P(C = dlx, 0).  Similarly, the class conditional densities can be derived
by  evaluating P( x Ie  =  d, 0).  Condi tionalizing over  classes  in  this  way  yields  class
conditional  densities  which  are  in  turn  mixtures  of  Gaussians.  Figure  1  shows
the  performance  of the  EM  algorithm  on  an  example  classification  problem  with
varying  proportions of missing features.  We  have  also  applied  these  algorithms to
the  problems of clustering  35-dimensional greyscale  images and  approximating the
kinematics of a  three-joint  planar arm from  incomplete data.

5  Discussion

Densit.y  estimation in high dimensions is  generally considered  to be more difficult(cid:173)
requiring more parameters-than function  approximation.  The density-estimation(cid:173)
based approach to learning, however,  has two advantages.  First, it  permits ready in(cid:173)
corporation of results from  the statistical literature on  missing data to yield flexible
supervised  and  unsupervised  learning architectures.  This is  achieved  by  combining
two  branches of application of the EM  algorithm yielding a set of learning rules for
mixtures under  incomplete sampling.

Supervised Learning from Incomplete Data via an EM Approach

127

Second,  estimating  the  density  explicitly  enables  us  to  represent  any  relation  be(cid:173)
tween the variables.  Density estimation is fundamentally more general than function
approximation  and  this  generality  is  needed  for  a  large  class  of learning  problems
arising from  inverting causal systems (Ghahramani, 1994).  These  problems cannot
be solved easily  by  traditional function  approximation techniques  since  the  data is
not generated  from  noisy samples of a  function,  but  rather of a  relation.

Acknow ledgmuents

Thanks to  D.  M.  Titterington and  David Cohn for  helpful  comments.  This project
was supported  in  part by  grants from  the McDonnell-Pew  Foundation, ATR Audi(cid:173)
tory  and  Visual  Perception  Research  Laboratories,  Siemens  Corporation,  the  N a(cid:173)
tional Science  Foundation, and  the Office  of Naval  Research.  The iris  data set  was
obtained  from  the  VCI  Repository of Machine  Learning  Databases.

