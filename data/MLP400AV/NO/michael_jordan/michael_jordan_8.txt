13  ABSTRACT  (MaxiMw  2Wb word)

W e present  a tree-structured  architecture  for supervised learning.  Ile statistical  model underlying the
architecture  is a hierarchical  mixture model in which both the mixture coefficients  and the mixture
comnponents  are generalized  linear models (GUIM's).  Learning  is treated as a maximum  liklihood
problem; in particular, we present an  Expectation-Maximization  (EM  algorithm for adjusting the
parameters of the architecture.  We also develop an on-line learning algorithm in which the
parameters  are updated incrementally.  Comparative simulation results are presented  in the robot
dy namics domain.

14  SUBJECT TERMS

supervised learning
decision  trees

statistics
neural networks

1.NUMBER OF PAGES

29

1S.  PRICE CODE

17  SECURITY  CLASSIFICATION

OF  REPORT

18. SECURITY  CLASSIFICATION

OF THIS  PAGE

19. SECURITY  CLASSIFICATION

OF ABSTRACT

20.  UIMITATKON  OF

ABSTRACT

U NCLASSIFIED

UNCLASSIFIED

UNCLASSIFIE

UNCLASSFEMD

DTIC  QuALITY  iNSPEC'TED  5

i

MASSACHUSETTS  INSTITUTE  OF  TECHNOLOGY

ARTIFICIAL  INTELLIGENCE  LABORATORY

and

CENTER  FOR  BIOLOGICAL  AND  COMPUTATIONAL  LEARNING

DEPARTMENT  OF  BRAIN  AND  COGNITIVE  SCIENCES

A.I.  Memo  No.  1440
C.B.C.L.  Memo  No.  83

August  6,  1993

Hierarchical  Mixtures  of Experts  and  the  EM

Algorithm

Michael  I.  Jordan  and  Robert  A.  Jacobs

Abstract

We  present  a  tree-structured  architecture  for supervised  learning.  The  statistical  model  underlying  the
architecture  is  a hierarchical  mixture  model in which  both  the mixture  coefficients  and  the mixture  com-
ponents  are  generalized  linear models  (GLIM's).  Learning  is  treated  as  a maximum  likelihood  problem;
in  particular,  we  present  an  Expectation-Maximization  (EM)  algorithm for  adjusting the  parameters  of
the  architecture.  We  also  develop  an  on-line  learning  algorithm  in  which  the  parameters  are  updated
incrementally.  Comparative simulation  results  are  presented  in  the  robot dynamics  domain.

Copyright  @  Massachusetts  Institute  of Technology,  1993

94-07596
IEIIIlilIl

This  report  describes  research  done  at  the  Dept.  of  Brain  and  Cognitive  Sciences,  the Center  for  Biological  and
Computational  Learning,  and  the Artificial  Intelligence  Laboratory  of the Massachusetts  Institute  of Technology.
Support  for  CBCL  is provided  in  part  by  a  grant  from  the  NSF  (ASC-9217041).  Support  for  the  laboratory's
artificial  intelligence  research  is  provided  in  part  by  the  Advanced  Research  Projects  Agency  of the  Dept.  of
Defense.  The  authors  were  supported  by  a  grant  from  the  McDonnell-Pew  Foundation,  by  a  grant  from  ATR
Human  Information  Processing  Research  Laboratories,  by  a  grant  from  Siemens  Corporation,  by  by  grant  IRI-
9013991  from  the  National  Science  Foundation,  by  grant  N00014-90-J-1942  from  the Office  of  Naval  Research,
and  by  NSF  grant  ECS-9216531  to  support  an  Initiative  in  Intelligent  Control  at  MIT.  Michael  1. Jordan  is a
NSF  Presidential  Young  Investigator.

94  3

8  009

Introduction

The  principle  of divide-and-conquer  is  a  principle  with  wide  applicability  throughout  applied
mathematics.  Divide-and-conquer  algorithms  attack  a  complex  problem  by  dividing  it  into
simpler  problems  whose  solutions  can  be  combined  to yield  a  solution  to  the complex  problem.
This approach  can often lead  to simple. elegant  and  efficient  algorithms.  In  this paper  we explore
a  particular  application  of  the  divide-and-conquer  principle  to  the  problem  of  learning  from
examples.  We  describe  a  network  architecture  and  a  learning  algorithm  for  the  architecture.
both  of which  are  inspired  by  the  philosophy  of divide-and-conquer.

In  the  statistical  literature  and  in  the  machine  learning  literature,  divide-and-conquer  ap-
proaches  have  become  increasingly  popular.  The  CART  algorithm  of Breiman,  Friedman.  01-
shen,  and  Stone  (1984),  the  MARS  algorithm  of  Friedman  (1991),  and  the  ID3  algorithm  of
Quinlan  (1986)  are  well-known  examples.  These  algorithms  fit  surfaces  to  data  by  explicitly
dividing  the input  space  into  a  nested  sequence  of regions,  and  by fitting  simple  surfaces  (e.g.,
constant  functions)  within  these  regions.  They  have convergence  times  that  are often  orders  of
magnitude  faster  than  gradient-based  neural  network  algorithms.

Although  divide-and-conquer  algorithms  have  much  to  recommend  them,  one  should  be
concerned  about  the statistical consequences  of dividing  the input  space.  Dividing  the data can
have favorable  consequences  for  the  bias of an estimator,  but  it generally  increases  the variance.
Consider  linear  regression,  for  example,  in  which  the  variance  of the estimates  of the slope  and
intercept  depend  quadratically  on  the  spread  of data  on  the  x-axis.  The  points  that  are  the
most  peripheral  in  the  input  space  are  those  that  have  the  maximal  effect  in  decreasing  the
variance  of the  parameter  estimates.

The  foregoing  considerations  suggest  that  divide-and-conquer  algorithms  generally  tend  to
be  variance-increasing  algorithms.  This  is  indeed  the  case  and  is  particularly  problematic  in
high-dimensional  spaces  where  data  become  exceedingly  sparse  (Scott,  1992).  One  response  to
this  dilemma-that  adopted  by  CART,  MARS,  and  ID3,  and  also  adopted  here-is  to  utilize
piecewise  constant  or  piecewise  linear  functions.  These  functions  minimize  variance  at  a cost
of increased  bias.  We  also make  use  of a  second  variance-decreasing  device;  a  device  familiar  in
the  neural  network  literature.  We  make  use of "soft"  splits  of data (Bridle,  1989;  Nowlan.  1991:
Wahba,  Gu,  Wang,  k  Chappell,  1993),  allowing  data  to lie  simultaneously  in  multiple  regions.
This  approach  allows  the  parameters  in  one  region  to  be  influenced  by  data  in  neighboring
regions.  CART,  MARS,  and  ID3  rely  on  "hard"  splits,  which,  as  we  remarked  above,  have
particularly  severe  effects  on  variance.  By  allowing  soft  splits  the  severe  effects  of lopping  off
distant  data can be ameliorated.  We also attempt  to minimize  the bias that  is incurred  by using
piecewise  linear  functions,  by  allowing  the splits  to be  formed  along  hyperplanes  at  arbitrary
orientations  in  the  input  space.  This  lessens  the  bias  due  to  high-order  interactions  among
the  inputs  and  allows  the  algorithm  to  be  insensitive  to  the  particular  choice  of coordinates
used  to  encode  the  data  (an  improvement  over  methods  such  as  MARS  and  ID3,  which  are
coordinate-dependent).

The  work  that  we  describe  here  makes  contact  with  a.  number  of  branches  of  statistical
theory.  First,  as  in  our  earlier  work  (Jacobs,  Jordan,  Nowlan,  &, Hinton,  1991),  we  formulate
the  learning  problem  as  a  mixture  estimation  problem  (cf.  Cheeseman,  et  al,  1988;  Duda  &K:
Hart.  1973;  Nowlan.  1991;  Redner  &  Walker,  1984;  Titterington,  Smith,  &  Makov,  1985).  We
show  that  the  algorithm  that  is  generally  employed  for  the  unsupervised  learning  of mixture
parameters-the  Expectation-Maximization
(EM)  algorithm  of  Dempster,  Laird  and  Rubin
(1977)-can  also  be  exploited  for  supervised  learning.  Second.  we  utilize  generalized  linear
model  (GLIM)  theory  (McCullagh  &  Nelder,  1983)  to  provide  the  basic  statistical  structure  Oea

0
-1

for  the  components  of the  architecture.  In  particular.  the  "soft  splits"  referred  to  above  are
modeled  as  multinomial logit models-a  specific  form of GLIM.  We  also show that the algorithm
developed  for  fitting  GLIM's-the  iteratively  reweighted  least  squares  (IRLS)  algorithm-can
be  usefully  employed  in  our  model,  in  particular  as  the  M  step  of the  EM  algorithm.  Finally.
we  show  that  these  ideas  can  be  developed  in  a  recursive  manner,  yielding  a  tree-structured
approach  to estimation  that  is  reminiscent  of CART,  MARS.  and  ID3.

The remainder  of the paper proceeds  as follows.  We  first introduce  the hierarchical  mixture-
of-experts architecture  and  present  the likelihood  function  for the architecture.  After  describing
a gradient  descent  algorithm,  we develop  a more powerful  learning algorithm  for the architecture
that  is  a  special  case  of the general  Expectation-Maximization  (EM)  framework  of Dempster.
Laird  and  Rubin  (1977).  We  also  describe  a  least-squares  version  of this  algorithm  that  leads
to  a  particularly  efficient  implementation.  Both  of  the  latter  algorithms  are  batch  learning
algorithms.  In  the  final  section,  we  present  an  on-line  version  of  the  least-squares  algorithm
that  in  practice  appears  to be  the  most  efficient  of the algorithms  that  we  have  studied.

Hierarchical  mixtures  of experts

The  algorithms  that  we discuss  in  this  paper  are supervised  learning  algorithms.  We  explicitly
address  the  case  of regression,  in  which  the  input  vectors  are  elements  of R'  and  the  output
vectors  are  elements  of  Rn.  We  also  consider  classification  models  and  counting  models  in
which  the  outputs  are  integer-valued.  The  data  are  assumed  to form  a countable  set  of paired
observations  A' =  {(x(t),y(t))}.  In  the  case  of the  batch  algorithms  discussed  below,  this  set  is
assumed  to be finite;  in  the  case  of the  on-line  algorithms,  the  set  may  be infinite.

We  propose  to solve nonlinear  supervised  learning  problems  by  dividing  the input  space  into
a  nested  set  of regions  and  fitting  simple  surfaces  to  the  data  that  fall  in  these  regions.  The
regions  have  "soft"  boundaries,  meaning  that  data  points  may  lie  simultaneously  in  multiple
regions.  The boundaries  between  regions  are themselves  simple  parameterized  surfaces  that  are
adjusted  by  the  learning  algorithm.

The  hierarchical  mixture-of-experts  (HME)  architecture  is  shown  in  Figure  1.1  The  ar-
chitecture  is  a  tree  in  which  the  gating  networks  sit  at  the  nonterminals  of  the  tree.  These
networks  receive  the  vector  x as  input  and  produce  scalar  outputs  that are  a  partition  of unity
at  each  point  in  the  input, space.  The  expert  networks  sit  at  the leaves of  the tree.  Each  expert
produces  an output  vector  lij for  each  input  vector.  These  output  vectors  proceed  up  the  tree,
being  blended  by  the  gating  network  outputs.

All  of the  expert  networks  in  the  tree  are  linear  with  a  single  output  nonlinearity.  We
will  refer  to such  a  network  as  "generalized  linear,"  borrowing  the  terminology  from  statistics
(McCullagh  &  Nelder,  1983).  Expert  network  (i,j) produces  its  output  /pi, as  a  generalized
linear  function  of the input  x:

(1)
where  (Tij  is  a  weight  matrix  and  f  is  a  fixed  continuous  nonlinearity.  The  vector  x is  assumed
to include  a  fixed  component  of one  to allow  for  an  intercept  term.

l(cid:127)ij  =  f(Uijx),

For  regression  problems,  f(.)  is  generally  chosen  to  be  the  identity  function  (i.e.,  the  ex-
perts  are  linear).  For  binary  classification  problems,  f(.) is  generally  taken  to  be  the  logistic
function,  in  which  case  the  expert  outputs  are  interpreted  as  the log  odds  of "success"  under  a

'To  simplify  the  presentation,  we  restrict  ourselves  to  a  two-level  hierarchy  throughout  the  paper.  All  of the
algorithms  that  we  describe,  however,  generalize  readily  to  hierarchies  of  arbitrary  depth.  See  Jordan  and  Xu
(1993)  for  a recursive  formalism  that  handles  arbitrary  hierarchies.

2

Network

g'

GatingGating

Newr

"I  92

P ~

91-

g2

1

N.\.

r

P21

2

Expert
iX 91
Network

Expert
91
Network

Expert
92
Network

Epr
2\
Network

Figure  1:  A  two-level  hierarchical  mixture  of  experts.  To  form  a  deeper  tree,  each  expert  is
expanded  recursively  into  a gating  network  and  a set  of sub-experts.

Bernoulli  probability  model  (see  below).  Other  models  (e.g.,  multiway  classification,  counting,
rate  estimation  and  survival  estimation)  are  handled  by  making  other  choices  for  f(-).  These
models  are  smoothed  piecewise  analogs  of the  corresponding  GLIM  models  (cf.  McCullagh  &
Nelder,  1983).

The  gating  networks  are also  generalized  linear.  Define  intermediate  variables  &,  as  follows:

vI  x,

(2)

where vi is a weight  vector.  Then the ith  output of the top-level gating  network is the "softmax"
function  of the & (Bridle,  1989;  McCullagh  & Nelder,  1983):

9i =  (cid:127)k

'

(3)

Note  that  the gi  are  positive  and sum  to one  for each  x.  They  can  be  interpreted  as  providing
a  "soft"  partitioning  of the input  space.

Similarly,  the gating  networks  at  lower  levels  are also  generalized  linear  systems.  Define

,i,

as  follows:

jTX.

3

(4)

Then

9ti  =

f)Ek

(5)

is  the  output  of the  jth  unit  in  the  ith  gating  network  at  the  second  level  of  the  architecture.
Once  again,  the  gjli  are  positive  and  sum  to  one  for  each  x.  They  can  be  interpreted  as
providing  a nested  "soft"  partitioning  of the input  space  within  the partitioning  providing  by
the  higher-level  gating  network.

The  output  vector  at  each  nonterminal  of  the  tree  is  the  weighted  output  of the  experts
below  that  nonterminal.  That  is,  the  output  at  the  ith  nonterminal  in  the  second  layer  of the
two-level  tree is:

and  the  output  at  the top level  of the  tree is:

p  =  E9~i

Note that  both  the g's  and  the  t's depend  on  the input  x, thus the  total  output  is  a nonlinear
function  of the  input.

Regression  surface

Given  the  definitions  of the  expert  networks  and  the  gating  networks,  the  regression  surface
defined  by  the  hierarchy  is  a  piecewise  blend  of the  regression  surfaces  defined  by  the  experts.
The  gating  networks  provide  a  nested,  "soft"  partitioning  of the  input  space  and  the  expert
networks  provide  local  regression  surfaces  within  the partition.  There is overlap  between  neigh-
boring  regions.  To  understand  the  nature  of  the  overlap,  consider  a  one-level  hierarchy  with
two  expert  networks.  In  this  case,  the gating  network  has  two  outputs, g,  and  g2.  The  gating
output  g,  is  given  by:

9l  =

~ef + e2
1

+ 1

e-(VI -V2)TX'

(6)
(7)
7

which  is  a logistic  ridge  function  whose  orientation  is  determined  by  the direction  of the  vector

- v 2. The gating output g2  is equal  to 1 - gi.  For a given x, the  total output  i  is  the convex
V
combination  91,01  +  92P2,  This  is  a  weighted  average  of the  experts.  where  the  weights  are
determined  by  the values  of the ridge  function.  Along  the ridge, g,  = g2  =  , and  both  experts
contribute  equally.  Away  from  the  ridge,  one  expert  or  the  other dominates.  The  amount  of
smoothing  across  the  ridge  is  determined  by  the  magnitude  of the  vector v 2  - v 1 . If v 2  - V1
is  large,  then  the  ridge  function  becomes  a sharp split  and  the  weighted  output  of the  experts
becomes  piecewise  (generalized)  linear.  If v 2  - vI  is  small,  then  each  expert  contributes  to a
significant  degree  on each  side of the ridge.  thereby  smoothing  the piecewise  map.  In  the limit
of a zero  difference  vector,  gi  =  92  =   for  all x, and  the total  output  is  the same  fixed  average
of the  experts on  both  sides  of the fictitious  "split."

In general,  a given  gating network induces  a smoothed  planar partitioning of the input space.
Lower-level  gating  networks  induce  a  partition  within  the  partition  induced  by  higher-level
gating  networks.  The  weights  in  a  given  gating  network  determine  the  amount  of  smoothing

4

across  the  partition  at  that  particular  level  of resolution:
large  weight  vectors  imply  sharp
changes  in  the regression  surface  across  a ridge and  small weights  imply  a smoother  surface.  In
the limit  of zero  weights  in  all gating  networks,  the  entire  hierarchy  reduces  to a fixed  average
(a  linear  system  in  the  case  of regression).

A  probability  model

The  hierarchy  can  be  given  a  probabilistic  interpretation.  We  suppose  that  the  mechanism
by  which  data  are  generated  by  the environment  involves  a  nested  sequence  of decisions  that
terminates  in  a  regressive  process  that  maps  x to y.  The  decisions  are  modeled  as  multinomial
random  variables.  That  is,  for  each  x,  we  interpret  the  values  gi(x, v)  as  the  multinomial
probabilities  associated  with the  first decision  and  the gjli(x,  v%)  as  the  (conditional)  multino-
mial  probabilities  associated  with  the  second  decision,  where  the  superscript  "0"  refers  to the
"true"  values  of the  parameters.  The  decisions  form a decision  tree.  We use a statistical  model
to model  this  decision  tree;  in  particular,  our choice  of parameterization  (cf.  Equations  2,  4,  3
and 5)  corresponds to a multinomial  legit  probability  model  at each  nonterminal  of the tree (see
Appendix  2).  A  multinomial  logit  model  is  a special  case  of a GLIM  that is  commonly  used  for
,.soft"  multiway  classification  (McCullagh  & Nelder,  1983).  Under the multinomial  logit model,
we  interpret  the  gating  networks  as  modeling  the input-dependent,  multinomial  probabilities
associated  with  decisions  at  particular  levels  of resolution  in  a  tree-structured  model  of  the
data.

Once  a  particular  sequence  of  decisions  has  been  made,  resulting  in  a choice  of regressive
process  (ij),  output y is  assumed  to be generated  according  to the following  statistical  model.
First,  a linear  predictor  nij  is formed:

0=  U=.x.

The  expected  value  of y  is obtained  by  passing  the  linear  predictor  through  the  link function
P:2

The  output  y  is  then  chosen  from  a  probability  density  P,  with  mean  (cid:127)A  and  "dispersion"
parameter  09.  We  denote  the density  of y  as:

P(ylx,  #)D

where  the  parameter  vector  09  includes  the  weights  U  and  the  dispersion  parameter  09.:

We  assume  the density  P  to be  a member of the exponential  family  of densities  (McCullagh  &
Nelder,  1983).  The  interpretation  of the dispersion  parameter  depends  on  the particular  choice
of density.  For example,  in  the  case  of the  n-dimensional  Gaussian,  the  dispersion  parameter
is  the covariance  matrix  9.

2 We  utilize  the  neural  network  convention  in  defining  links.  In  GLIM  theory,  the convention  is  that. the  link

function  relates  r  to p;  thus,  q  =  h(p),  where  h  is  equivalent  to  our  f-1.

'Not  all exponential  family  densities  have  a dispersion  parameter;  in particular,  the  Bernoulli density  discussed

below  has  no  dispersion  parameter.

Given  these  assumptions,  the  total probability  of generating  y from  x  is  the mixture of the
probabilities of generating  y from each of the component  densities,  where the mixing proportions
are  multinomial  probabilities:

P(ylx, 00)  =  - gi(x, v)  Egjli(x, v)P(yjx, 0),

I

3

(8)

Note  that  0o includes  the expert  network  parameters  090  as  well  as  the gating  network parame-
ters v9  and  v9.  Note also that we  have explicitly  indicated  the dependence  of the probabilities
gi  and  gjli  on the  input  x  and on  the  parameters.  In  the  remainder  of the  paper  we  drop  the
explicit  reference  to the  input  and  the  parameters  to  simplify  the  notation:

P(ylx,90)  =  -i

R  W,

(9)

i

j

We  also  utilize  Equation  9  without  the  superscripts  to  refer  to  the  probability  model  defined
by  a particular  HME  architecture,  irrespective  of any  reference  to a  "true"  model.

Example  (regression)

In  the case  of regression  the probabilistic  component  of the  model  is  generally  assumed  to
be  Gaussian.  Assuming  identical  covariance  matrices  of  the  form  a2I for  each  of the  experts
yields  the  following  hierarchical  probability  model:

P(l,0  =

I1iI
P(y x, 9)  =  (2r)n/20 n

i

j

gjl e-

Example  (binary classification)

In  binary  classification  problems  the output  y  is  a discrete  random  variable  having  possible
outcomes  of  "failure"  and  "success."  The  probabilistic  component  of  the  model  is  generally
assumed  to be  the  Bernoulli  distribution  (Cox,  1970).  In  this  case,  the  mean  pii  is  the  condi-
tional  probability  of classifying  the input  as  "success."  The  resulting  hierarchical  probability
model  is  a mixture  of Bernoulli  densities:

P(ylx, 0) = F g  , (cid:127)gjliY(cid:127)(1-

i

j

p.ij)l-.

Posterior  probabilities

In  developing  the  learning  algorithms  to  be  presented  in  the  remainder  of the  paper,  it  will
prove  useful  to define  posterior  probabilities  associated  with  the  nodes  of the  tree.  The  terms
"posterior"  and  "prior"  have  meaning  in  this  context  during  the  training  of the  system.  We
refer  to  the  probabilities  gi  and  gili  as  prior  probabilities,  because  they  are  computed  based
only  on  the  input  x,  without  knowledge  of  the  corresponding  target  output  y.  A  posterior
probability  is defined  once  both  the input  and  the target  output are  known.  Using  Bayes'  rule,
we  define  the  posterior  probabilities  at  the  nodes  of the tree  as  follows:

hi=  gi Ej gjliPij(Y)

Ei gi Ej gjliPij()

(10)

d

and

h jli
:

ji~j(Y)

hji=  y.jg31,P(cid:127) 3 (y).
E3 #3~iij3(Y)'

(11 )

We  will  also  find  it  useful  to  define  the  joint  posterior  probability  hij,  the  product  of hi  and
hjil:

=

gigj1iPij(Y)
,

j gj1 iPij(Y)

h  i gi

(12)

This  quantity  is the  probability  that  expert  network  (i, j)  can  be  considered  to have generated
the  data, based  on  knowledge  of both  the  input  and  the  output.  Once  again,  we  emphasize
that  all  of these quantities  are  conditional  on  the input  x.

In  deeper  trees,  the  posterior  probability  associated  with  an  expert  network  is  simply  the
product  of the  conditional  posterior  probabilities  along  the  path  from  the  root  of the  tree  to
that  expert.

The  likelihood and  a  gradient  descent  learning  algorithm

Jordan  and  Jacobs  (1992)  presented  a  gradient  descent  learning  algorithm  for  the  hierarchical
architecture.  The  algorithm  was based  on  earlier  work  by  Jacobs,  Jordan,  Nowlan,  and  Hinton
(1991),  who treated  the problem  of learning  in mixture-of-experts  architectures  as a maximum
likelihood  estimation  problem.  The  log  likelihood  of a data  set  A'  = {(x(t),  y(t))}jN  is  obtained
by  taking  the  log  of  the  product  of  N  densities  of  the  form  of  Equation  9,  which  yields  the
following  log likelihood:

1(9; A')  =

i n'9t) E
i

t

g

j

,9jii(y(t).

(13)

Let  us  assume  that  the  probability  density  P  is  Gaussian  with  an  identity  covariance  matrix
and  that  the link  function  is  the identity.  In  this  case,  by  differentiating  1(e; A')  with  respect
to the  parameters,  we  obtain  the following  gradient  descent  learning  rule  for the  weight  matrix
U1ij:

Allij  =  p L  h~t)h(t)(y(t) -

t(t))x(t)T,

(14)

t

where  p  is  a learning  rate.  The  gradient  descent  learning  rule  for  the  ith  weight  vector  in  the
top-level  gating  network  is  given  by:

Avi =  p  -'(h) - g9t))x(i),

t

(15)

and  the  gradient  descent  rule  for  the jth  weight  vector  in  the  i'h  lower-level  gating  network  is
given  by:

(16)

Avij  =

- g5))x('),

t  0)(0)
t

Updates  can  also  be obtained  for  covariance  matrices  (Jordan  & Jacobs,  1992).

The  algorithm  given  by  Equations  14,  15,  and  16  is  a batch  learning  algorithm.  The  corre-
sponding  on-line  algorithm  is  obtained  by  simply  dropping  the  summation  sign  and  updating
the  parameters  after  each  stimulus  presentation.  Thus,  for example,

is  the  stochastic  update  rule  for  the  weights  in  the  (i,j)th  expert  network  based  on  the  tth
stimulus  pattern.

+ pht)ht)t(y(t) - pi) )xt)T(17)

(

,

_

S

The  EM  algorithm

In  the  following  sections  we  develop  a  learning  algorithm  for  the  HME  architecture  based  on
the  Expectation-Maximization  (EM)  framework  of  Dempster,  Laird,  and  Rubin  (1977).  We
derive  an  EM  algorithm  for  the  architecture  that  consists  of the  iterative  solution  of a  coupled
set  of iteratively-reweighted  least-squares  problems.

The  EM  algorithm  is  a  general  technique  for  maximum  likelihood  estimation.  In  practice
EM  has  been  applied  almost  exclusively  to unsupervised  learning  problems.  This  is  true  of the
neural  network  literature  and  machine  learning  literature,  in  which  EM  has  appeared  in  the
context  of clustering  (Cheeseman,  et  al.  1988;  Nowlan,  1990)  and  density  estimation  (Specht,
1991),  as  well  as  the statistics  literature.  in  which  applications  include  missing  data  problems
(Little &, Rubin,  1987). mixture density estimation  (Redner  & Walker,  1984),  and factor analysis
(Dempster,  Laird.  & Rubin.  1977).  Another  unsupervised  learning  application  is  the learning
problem  for  Hidden  Markov  Models,  for  which  the  Baum-Welch  reestimation  formulas  are  a
special  case  of EM.  There  is  nothing  in  the  EM  framework  that  precludes  its  application  to
regression  or  classification  problems;  however,  such  applications  have  been  few. 4

EM  is  an  iterative  approach  to maximum  likelihood  estimation.  Each  iteration  of an  EM
algorithm  is  composed  of  two  steps:  an  Estimation  (E)  step  and  a  Maximization  (M)  step.
The  M  step  involves  the  maximization  of a  likelihood  function  that  is  redefined  in  each  itera-
tion  by  the  E  step.  If the  algorithm  simply  increases  the  function  during  the  M  step,  rather
than  maximizing  the function,  then  the  algorithm  is  referred  to  as  a  Generalized  EM  (GEM)
algorithm.  The  Boltzmann  learning  algorithm  (Hinton  & Sejnowski,  1986)  is  a  neural  network
example  of a  GEM  algorithm.  GEM  algorithms  are  often  significantly  slower  to converge  than
EM  algorithms.

An  application  of  EM  generally  begins  with  the  observation  that  the  optimization  of  the
likelihood  function  1(0; A')  would  be simplified  if only  a  set  of additional  variables,  called  "miss-
ing"  or  -hidden"  variables,  were  known.  In  this  context,  we refer  to  the  observable  data  A'  as
the  "incomplete  data"  and  posit  a  "complete  data"  set  Y  that  includes  the  missing  variables
Z.  We  specify  a probability  model  that  links  the  fictive  missing  variables  to  the  actual  data:
P(y. zlx ,0).  The  logarithm  of the density  P defines  the  "complete-data  likelihood,"  l,(0; Y).
The original  likelihood,  1(0; A'),  is referred  to in this context  as the  "incomplete-data likelihood."
It  is  the relationship  between  these  two likelihood  functions  that  motivates  the EM  algorithm.
Note  that  the  complete-data  likelihood  is  a random  variable,  because  the  missing  variables  Z
are  in  fact  unknown.  An  EM  algorithm  first  finds  the  expected  value  of the  complete-data
likelihood,  given  the  observed  data  and  the current  model.  This  is  the  E  step:

Q(0, 0(PJ)  =  E[1(O; Y)I X],

where  0(P)  is  the value  of the parameters  at  the  pth  iteration  and  the expectation  is  taken  with
respect  to 0(P).  This step yields  a deterministic  function  Q.  The M  step maximizes  this function
with  respect  to 0 to find  the  new  parameter  estimates  O(P+'):

0(P+l)  =  argmax!Q(O,O(P)).

The  E  step  is  then  repeated  to yield  an  improved  estimate  of the  complete  likelihood  and  the
process  iterates.

"4An  exception  is the  "switching  regression"  model  of Quandt  and  Ramsey  (1972).  For  further discussion  of

switching  regression,  see  Jordan  and  Xu  (1993).

8

An  iterative  step  of EM  chooses  a parameter  value  that  increases  the value of Q,  the  expec-
tation of the complete  likelihood.  What is  the effect  of such a  step on  the incomplete likelihood?
Dempster,  et al.  proved  that  an  increase  in  Q  implies  an  increase  in  the incomplete  likelihood:

1(0(P+I);  X)  _  1(0(p);  Xt).

Equality  obtains  only  at  the stationary  points  of I (Wu,  1983).  Thus  the likelihood  I increases
monotonically  along  the  sequence  of parameter  estimates  generated  by  an  EM  algorithm.  In
practice  this implies  convergence  to a  local  maximum.

Applying  EM  to  the  HME  architecture

To  develop  an  EM  algorithm  for  the  HME  architecture,  we  must  define  appropriate  "missing
data"  so  as  to  simplify  the likelihood  function.  We  define  indicator  variables  zi  and  z. 3i,  such
that  one and  only  one  of the zi  is  equal  to one,  and one  and only  one of the  z~ji  is equal  to one.
These  indicator  variables  have  an  interpretation  as the  labels  that  correspond  to the  decisions
in  the  probability  model.  We  also  define  the  indicator  variable  zij,  which  is  the  product  of
zi  and  zjlj.  This  variable  has  an  interpretation  as  the  label  that  specifies  the  expert  (the
regressive  process)  in  the probability  model.  If the labels  zi,  zjli  and  zij  were  known,  then  the
maximum  likelihood  problem  would decouple  into  a separate  set  of regression  problems  for each
expert  network  and  a separate  set  of multiway  classification  problems  for  the  gating  networks.
These problems  would  be solved independently  of each  other, yielding  a rapid  one-pass  learning
algorithm.  Of  course,  the  missing  variables  are  not  known,  but  we  can  specify  a  probability
model  that  links  them  to the observable  data.  This  probability  model  can  be  written  in  terms
of the  Zj,  as  follows:

p(y(t).zI)Ix(t)O)

=

g

)gp..t(y(t))

J-r  j"g  t)

=11 fl

.t
g  Pij(Y(  )}Zi,

IMp
j1g)i

(cid:127)()

(18)

(19)

using  the fact  that  z 9t) is  an indicator  variable.  Taking the logarithm  of this  probability  model
yields  the  following  complete-data  likelihood:

l,(e; Y)  =

t

-

t

j  Y  z)  nfg~t  ()gPj~y(t))
i

ziV  ln  gijli  ij(cid:127)#i

j

Pnha

!

,,(  + In P,3(y(t)}.

t

j

(20)

(21)

Note the  relationship of the complete-data  likelihood  in  Equation  21  to the incomplete-data
likelihood  in  Equation  13.  The  use  of the  indicator  variables  zij  has  allowed  the  logarithm  to
be  brought  inside  the  summation  signs,  substantially  simplifying  the  maximization  problem.
We  now  define  the E  step of the EM  algorithm  by  taking the expectation  of the complete-data
likelihood:

Q(O, 0())

E

b{l)ng1t  + lng94i  + In pA1(y( t))},

0=

t
where  we  have  used  the fact  that:

i

j

E[z~jIA']  =

=  l-y(t),X(0,u(,))

9

(22)

(23)

p(y(t)0  zA)

-1  _x(t) _(P))P(t

!)  -

ljx(t ),9(P))

P(Y(t)Ix(t), O(P))

p  y( t) x (t )  O  . .(P ) )g y, ( t ),(
_  ,
jli
--  (cid:127)igtt
Ei '

j  ,(Olp  w  oIX~lvt)  O  (P))

m

gli

(cid:127)

,,J

.

i

(Note  also  that  E[zPt)IX]

=  h
0 h4 t)  and  E[z(jIA'] = h(Wt)
I  I

'jlijli

.

be

(25)

(26)

The  M  step  requires  maximizing  Q(0,9(P))  with  respect  to  the expert  network  parameters
and  the  gating  network  parameters.  Examining  Equation  22,  we  see  that  the  expert  nctwork
parameters  influence  the  Q  function  only  through  the  terms  ht ) In  Pij(y(t)),  and  the  gating
network  parameters  influence  the Q  function  only  through the  tefms  0)  In gi)  and  0
Thus  the M  step  reduces  to  the following  separate  maximization  problems:

n  In

arg max E  0)  In Pij(y(t)),

vp+  =  argrmaxiE

0)  In g~t)

t

k

vP+1)  = argm.

0  E  0)  In

t

k

I

(27)

(28)

(29)

and

Each  of  these  maximization  problems  are  themselves  maximum  likelihood  problems.  Equa-
tion  27 is simply  the general  form of a weighted  maximum  likelihood  problem  in the probability
density  P~j.  Given  our parameterization  of Pij,  the  log  likelihood  in  Equation  27  is  a weighted
log  likelihood  for a GLIM.  An efficient  algorithm  known  as iteratively  reweighted  least-squares
(IRLS)  is  available  to solve  the  maximum  likelihood  problem  for  such  models  (McCullagh  &
Nelder,  1983).  We discuss  IRLS  in  Appendix  A.

Equation  28  involves  maximizing  the  cross-entropy  between  the  posterior  probabilities  h()
and  the prior probabilities  g9k.  This  cross-entropy  is the log likelihood  associated  with  a multi-
nomial  logit  probability  model  in  which  the  h(t)  act  as  the output  observations  (see  Appendix
B).  Thus  the maximization  in  Equation  28  is  also  a  maximum  likelihood  problem  for  a GLIM
and  can  be solved  using  IRLS.  The  same  is true  of Equation  29,  which  is  a weighted  maximum
likelihood  problem  with  output  observations  hI)k  and observation  weights  h1k.

In  summary,  the  EM  algorithm  that  we  have  obtained  involves  a  calculation  of posterior
probabilities  in  the  outer loop  (the  E  step),  and  the  solution  of a set  of IRLS  problems  in  the
inner  loop  (the  M  step).  We  summarize  the algorithm  as  follows:

10

Algorithm  1

1.  For  each  data  pair  (x(t),y(t)),  compute  the  posterior  probabilities  hlt)  and  h(')  using  the

jli



current  values  of the  parameters.

2.  For each  expert  (i,j), solve  an  IRLS  problem  with  observations  {(x(t ),y(t ))}N  and obser-

vation  weights  {hi  h1

3.  For each  top-level gating network,  solve an IRLS  problem  with observations  {(xit), h(t).)IN

4.  For  each  lower-level  gating  network,  solve  a  weighted  IRLS  problem  with  observations

{(x(t,

)} I  and  observation  weights  {h(t)}N.

5.  Iterate  using  the updated  parameter  values.

A  least-squares  algorithm

In  the  case  of regression,  in  which  a  Gaussian  probability  model  and  an  identity  link  function
are  used,  the  IRLS  loop  for  the  expert  networks  reduces  to  weighted  least  squares,  which  can
be  solved  (in  one  pass)  by  any  of the  standard  least-squares  algorithms  (Golub  &  van  Loan.
1989).  The  gating  networks  still  require  iterative  processing.  Suppose,  however,  that  we  fit the
parameters  of the gating  uctworks  using  least  squares  rather  than  maximum likelihood.  In  this
case,  we  might  hope  to obtain  an algorithm  in which the gating network parameters  are  fit  by a
one-pass  algorithm.  To motivate  this  approach,  note  that  we can  express  the IRLS  problem  for
the gating  networks  as  follows.  Differentiating  the  cross-entropy  (Equation  28)  with  respect  to
the  parameters  vi  (using  the  fact  that  Ogi/lOj  =  gi(ij  - gj),  where  bij  is  the  Kronecker  delta)
and  setting  the derivatives  to zero  yields  the following  equations:

E(h-t)

- gi(x1t)  vi))x10  =  0,

t

(30)

which  are  a coupled  set  of equations  that  must  be  solved  for each  i.  Similarly,  for each  gating
network  at  the second  level  of the  tree, we  obtain  the following  equations:

"i0)(0) - gjli(x(t),  vij))x(t)  =  0,

(31)

which  must  be solved  for each  i and j.  There  is one aspect  of these equations  that  renders  them
unusual.  Recall  that  if the labels  z
and  P)  were  known,  then  the gating  networks  would  be

essentially  solving  a set  of multiway  classification  problems.  The supervised  errors  (z:t)  - g(cid:127)t))
and  (-(i) -g(it)  would  appear  in  the algorithm  for solving these problems.  Note that these errors
are  differences  between  indicator  variables  and  probabilities.  In  Equations  30  and  31,  on  the
other  hand,  the errors  that  drive  the algorithm  are  the differences  (h0  - g~i))  and  (h(
0)
which  are  differences  between  probabilities.  The  EM  algorithm  effectively  "fills  in"  the  missing
labels  with  estimated  probabilities  hi  and  hjli.  These  estimated  probabilities  can  be  thought
of as  targets  for  the  gi  and  the  gili.  This  suggests  that  we  can  compute  "virtual  targets"  for
the  underlying  linear  predictors  &,  and  Jlj,  by  inverting  the  softmax  function.  (Note  that  this

-

11

option  would  not  be  available  for  the  zi  and  zJli,  even  if  they  were  known,  because  zero  and
one  are  not  in  the  range  of the softmax  function.)  Thus  the  targets for  the i  are  the  values:

In ht)  --  In C,

where  C  =  Ek Ctk  is  the  normalization  constant  in  the  softmax  function.  Note,  however,  that
constants  that  are  common  to  all  of the  i  can  be  omitted,  because  such  constants  disappear
when  i  are  converted  to gi.  Thus  the  values  In h!'  can  be used  as  targets for the  i.  A  similar
argument  shows  that  the  values  In h(')  can  be  used  as  targets  for  the  cij,  with  observation
weights  h~t)k

The  utility  of this  approach  is  that  once  targets  are  available  for  the  linear  predictors  (cid:127)i
and  ij,  the  problem  of finding  the  parameters  vi  and  vii  reduces  to a coupled  set of weighted
least-squares  problems.  Thus  we  obtain  an  algorithm  in  which  all  of  the  parameters  in  the
hierarchy,  both  in  the  expert  networks  and  the  gating  networks,  can  be  obtained  by  solving
least-squares  problems.  This yields  the following  learning  algorithm:

Algorithm  2

1.  For  each  data  pair  (x(O),y(t)),  compute  the posterior  probabilities  h  t)  and  h 9t) using  the

current  values  of the  parameters.

2.  For  each  expert  (ij),

solve  a  weighted

least-squares  problem  with  observations

{(x(t),y(t))}i

and  observation  weights  {h(t  }  N

3.  For  each  top-level  gating  network,  solve  a  least-squares  problem  with  observations

{(x(0, In h('0)})I.

4.  For  each  lower-level  gating  network,  solve  a  weighted  least-squares  problem  with  obser-

vations  f(x(t).  rlnkh)}N  and  observation  weights  {ht)}N

5.  Iterate  using  the  updated  parameter  values.

It is  important  to note  that  this  algorithm  does  not yield  the same  parameter  estimates  as
Algorithm  1; the  gating network  residuals  (ht ) - g~t))  are  being fit  by  least  squares  rather than
maximum  likelihood.  The  algorithm  can  be  thought  of as  an  approximation  to  Algorithm  1,
an  approximation  based  on  the assumption  that  the differences  between  hlt) and  g[t)  are  small.
This  assumption  is  equivalent  to  the  assumption  that  the  architecture  can  fit  the  underlying
regression  surface  (a  consistency  condition)  and  the  assumption  that  the  noise  is  small.
In
practice  we have found  that the least  squares algorithm  works reasonably  well,  even  in  the early
stages  of fitting  when  the  residuals  can  be  large.  The  ability  to  use least  squares  is  certainly
appealing  from  a  computational  point  of view.  One  possible  hybrid  algorithm  involves  using
the  least  squares  algorithm  to  converge  quickly  to  the  neighborhood  of  a  solution  and  then
using  IRLS  to refine  the solution.

Simulation  results

We  tested  Algorithm  1 and  Algorithm  2  on  a  nonlinear  system  identification  problem.  The
data  were  obtained  from  a  simulation  of a  four-joint  robot  arm  moving  in  three-dimensional

12

e

Architecture
linear
backprop
HME  (Algorithm  1)
HME  (Algorithm  2)
CART
CART  (linear)
MARS

Relative  Error  #  Epochs

.31
.09
.10
.12
.17
.13
.16

1

5,500

35
39
NA
NA
NA

Table  1:  Average  values  of relative error  and number  of epochs  required  for  convergence  for the
batch  algorithms.

space  (Fun  & Jordan,  1993).  The network must learn  the forward dynamics of the arm;  a state-
dependent  mapping  from joint  torques  to joint  accelerations.  The  state  of the  arm  is encoded
by  eight  real-valued  variables:  four  positions  (rad)  and  four  angular  velocities  (rad/sec).  The
torque  was  encoded  as  four  real-valued  variables  (N.
im).  Thus  there  were  twelve  inputs  to
the  learning  system.  Given  these  twelve  input  variables,  the  network  must  predict  the  four
accelerations  at  the  joints  (rad/sec2 ).  This  mapping  is  highly  nonlinear  due  to  the  rotating
coordinate  systems  and  the interaction  torques  between  the  links  of the arm.

We  generated  15,000  data  points  for  training  and  5,000  points  for  testing.  For  each  epoch
(i.e., each pass  through the training  set), we computed the relative error on the test set.  Relative
error is  computed  as  a ratio  between  the  mean  squared  error and  the  mean  squared  error  that
would  be obtained  if the learner  were to output  the mean  value  of the  accelerations  for all  data
points.

We compared  the  performance  of a binary  hierarchy  to that of a  backpropagation  network.
The  hierarchy  was  a  four-level  hierarchy  with  16  expert  networks  and  15  gating  networks.
Each  expert  network  had  4  output  units  and  each  gating  network  had  1  output  unit.  The
backpropagation  network had  60 hidden  units,  which  yields  approximately  the same  number  of
parameters  in  the network  as  in  the  hierarchy.

The HME  architecture  was  trained by Algorithms  1 and 2, utilizing  Cholesky decomposition
to  solve  the weighted  least-squares  problems  (Golub  & van  Loan,  1989).  Note  that the  HME
algorithms  have  no free parameters.  The free  parameters  for  the backpropagation  network  (the
learning  rate  and  the  momentum  term)  were  chosen  based  on a coarse  search  of the parameter
space.  (Values  of 0.00001  and  0.15  were  chosen  for  these  parameters.)  There  were  difficulties
with  local  minima  (or plateaus)  using  the  backpropagation  algorithm:  Five  of ten  runs  failed
to converge  to  "reasonable"  error values.  (As  we  report  in  the  next  section,  no such  difficulties
were encountered  in  the case of on-line backpropagation).  We report  average convergence  times
and  average  relative  errors only  for those  runs  that  converged  to  "reasonable"  error  values.  All
ten  runs  for  both  of the  HME  algorithms  converged  to  "reasonable"  error values.

Figure 2 shows the performance of the hierarchy and the backpropagation  network.  The hor-
izontal  axis of the graph  gives  the training  time in  epochs.  The  vertical axis  gives generalization
performance  as  measured  by  the  average  relative  error  on  the  test  set.

Table  1 reports  the  average  relative  errors  for  both  architectures  measured  at  the  minima
of the  relative  error  curves.  (Minima  were  defined  by  a sequence  of three  successive  increases
in  the relative  error.)  We  also report  values  of relative  error  for the  best linear  approximation,
the CART  algorithm,  and  the  MARS  algorithm.  Both  CART  and  MARS  were  run  four  times,

13

I

Backpropagation

.........  HME  (Algorithm 2)

6
0

..

........-....

0

1

II

10

I

1000

100

Epochs

Figure  2:  Relative  error  on  the  test  set  for  a  backpropagation  network  and  a  four-level  HME
architecture  trained  with  batch  algorithms.  The  standard  errors  at  the  minima  of the  curves
are  0.013  for  backprop  and  0.002  for  HME.

once  for each  of the output  variables.  We  combined  the results  from  these  four  computations  to
compute  the  total  relative  error.  Two  versions  of CART  were  run;  one  in  which  the  splits  were
restricted  to  be  parallel  to the axes  and  one in  which  linear  combinations  of the input  variables
were  allowed.

The  MARS  algorithm  requires  choices  to  be  made  for  the  values  of two structural  parame-
ters:  the  maximum  number  of basis  functions  and  the  maximum  number  of interaction  terms.
Each  basis  function  in  MARS  yields  a  linear  surface  defined  over  a  rectangular  region  of the
input  space,  corresponding  roughly  to the function  implemented  by  a  single  expert  in  the  HME
architecture.  Therefore  we  chose  a  maximum  of  16  basis  functions  to  correspond  to  the  16
experts  in  the  four-level  hierarchy.  To  choose  the  maximum  number  of interactions  (mi),  we
compared  the  performance  of MARS  for  mi  =  1,  2,  3,  6,  and  12,  and  chose  the  value  that
yielded  the best  performance  (mi  =  3).

For  the  iterative  algorithms,  we  also  report  the  number  of epochs  required  for  convergence.
Because  the  learning  curves  for  these  algorithms  generally  have  lengthy  tails,  we  defined  con-
vergence  as the first  epoch  at  which  the relative  error drops  within five  percent  of the minimum.
All  of the architectures  that  we  studied  performed  significantly  better  than  the  best  linear
approximation.  As  expected,  the CART  architecture  with linear  combinations  performed  better
than  ('ART  with  axis-parallel  splits.'  The  HME  architecture  yielded  a  modest  improvement

'it  should  be  noted  that  CART is  at an  advantage  relative  to  the other  algorithms  in  this comparison,  because
no  structural  parameters  were  fixed  for  CART.  That  is,  CART  is  allowed  to  find  the  best. tree  of any  size  to  fit
the data.

14

I

Epoch  0

L

Epoch 9

L,,,

LI

I  L
AA  A  A  A
LLLLILILLLLLL

L  L

A  A  A
WLLUI  LL I  WL Lau

Epoch  19

L,..

L

Epoch 29

L

L

LJL

,

..L

LL  L

L
I LJ LA L  Lj

L, Lj

Figure  3:  A  sequence  of histogram  trees  for  the  HME  architecture.  Each  histogram  displays
the  distribution  of posterior  probabilities  across  the training  set  at  each  node in  the  tree.

over  MARS  and  CART.  Backpropagation  produced  the lowest  relative  error  of the  algorithms
tested  (ignoring  the difficulties  with  convergence).

These  differences  in relative error should  be treated  with  some  caution.  The  need to set free
parameters  for some of the architectures  (e.g.,  backpropagation)  and  the need to make structural
choices  (e.g.,  number  of  hidden  units,  number  of  basis  functions,  number  of experts)  makes
it  difficult  to  match  architectures.  The  HME  architecture,  for  example,  involves  parameter
dependencies  that  are  not  present  in  a backpropagation  network.  A  gating  network  at  a high
level  in  the  tree can  "pinch  off"  a  branch  of the tree,  rendering  useless  the  parameters  in  that
branch  of the  tree.  Raw  parameter  count  is  therefore  only  a very  rough  guide  to architecture
capacity;  more  precise  measures  are  needed  (e.g.,  VC  dimension)  before  definitive  quantitative
comparisons  can  be  made.

The  differences  between  backpropagation  and  HME  in  terms of convergence  time are  more
definitive.  Both  HME  algorithms  reliably  converge  more  than  two orders  of magnitude  faster
than  backpropagation.

As  shown  in  Figure  3,  the  HME  architecture  lends  itself  well  to  graphical  investigation.
This  figure  displays  the  time  sequence  of the  distributions  of posterior  probabilities  across  the
training  set  at  each  node of the  tree.  At  Epoch  0,  before  any  learning  has  taken  place,  most  of
the  posterior  probabilities  at  each  node  are  approximately  0.5  across  the  training  set.  As  the
training  proceeds,  the  histograms  flatten  out,  eventually  approaching  bimodal  distributions  in

15

IIII

i'll,

IIII

'l ,

I,'

Ill

I.il  li

..

,_LL,.

__

-.

III  LI  .Ji

,,,.  wLIW

Uh  wI  LI

Figure  4:  A  deviance  tree  for  the  HME  architecture.  Each  plot  displays  the  mean  squared
error  (MSE)  for  the four output  units  of the clipped  tree.  The  plots are on  a log scale  covering
approximately  three  orders of magnitude.

which  the  posterior  probabilities  are  either  one  or  zero  for  most  of the  training  patterns.  This
evolution  is  indicative  of increasingly  sharp  splits  being  fit  by  the gating  networks.  Note  that
there  is a tendency  for  the splits  to  be formed  more  rapidly  at  higher  levels  in  the  tree  than at
lower  levels.

Figure  4  shows  another  graphical  device  that  can  be  useful  for  understanding  the  way  in
which  a  HME  architecture  fits  a  data  set.  This  figure,  which  we  refer  to as  a  "deviance  tree,"
shows  the  deviance  (mean  squared  error)  that  would  be  obtained  at  each  level  of  the tree  if
the  tree  were  clipped  at  that  level.  We  construct  a  clipped  tree  at  a  given  level  by  replacing
each  nonterminal  at  that  level  with  a  matrix  that  is  a weighted  average  of the experts  below
that  nonterminal.  The  weights  are  the  total  prior  probabilities  associated  with  each  expert
across  the training  set.  The error for each  output  unit is  then  calculated  by  passing the  test set
through  the clipped  tree.  As  can  be  seen  in  the figure,  the deviance  is  substantially  smaller  for
deeper  trees (note  that  the  ordinate  of  the  plots  is  on  a log  scale).  The  deviance  in  the  right
branch  of the  tree is  larger  than  in  the left  branch  of the  tree.  Information  such  as  this can  be
useful  for  purposes  of exploratory  data  analysis  and  for  model  selection.

16

An  on-line  algorithm

The  batch  least-squares  algorithm  that  we  have  described  (Algorithm  2)  can  be  converted
into  an  on-line  algorithm  by  noting  that  linear  least  squares  and  weighted  linear  least  squares
problems  can  be  solved  by  recursive  procedures  that  update the  parameter  estimates  with each
successive  data point (Ljung  & S6derstr6m,  1986).  Our application  of these  recursive  algorithms
is  straightforward;  however,  care must  be taken to handle the observation  weights  (the posterior
probabilities)  correctly.  These  weights  change  as  a function  of the  changing  parameter  values.
This  implies  that  the  recursive  least  squares  algorithm  must  include  a, decay  parameter  that
allows  the  system  to  "forget"  older  values  of the  posterior  probabilities.

In  this  section  we  present  the  equations  for the  on-line  algorithm.  These  equations  involve
an update not only  of the parameters in each  of the networks. 6 but  also the storage and updating
of an  inverse  covariance  matrix  for each  network.  Each  matrix has  dimensionality  mxrn,  where
m  is the dimensionality  of the input  vector.  (Note  that the size of these matrices  depends  on the
square  of the number  of input  variables,  not  the square of the number of paramretrs. Note also
that  the update equation  for  the inverse  covariance  matrix updates  the inverse  matrix  directly;
there  is  never  a  need  to invert  matrices.)

The  on-line  update  rule  for the  parameters  of the expert  networks is  given  by  the following

recursive  equation:

0(t+0) =  0()9  + rh(t)h,(t9tv(t) - A (t)X(t)Tgt)

iJ

=

f

l"ji

-

ij  ,

(32)

where  Rij  is the inverse  covariance  matrix for expert  network  (ij).  This  matrix  is  updated  via
the  equation:

R~t)

\  1R(t, 1) - A-'-

R~t.-1)X(t)X(t)T R(t-1)

\[h -t)]-  + x(t)TR  l-1)x(t)0

(33)

where  A is the  decay  parameter.

It  is interesting to note the similarity  between  the parameter update  rule in  Equation  32  and
the  gradient  rule  presented  earlier  (cf.  Equation  14).  These  updates  are  essentially  the  same,
except  that  the  scalar  p  is  replaced  by  the  matrix  R/.) It  can  be  shown,  however,  that  R')

is  an  estimate  of the  inverse  Hessian  of the  least-squares  cost  function  (Ljung  &  S6derstr6m,
1986),  thus  Equation  32  is  in  fact  a  stochastic  approximation  to  a  Newton-Raphson  method
rather  than  a  gradient  method.'

Similar  equations  apply  for  the  updates  of the  gating  networks.  The  update  rule  for  the
parameters  of the  top-level  gating network  is given  by the following  equation  (for  the ith  output
of the gating  network):

where  the inverse  covariance  matrix  Si  is  updated  by:

vlt+i) =  vlt) +  S)(Inhlt)-

(t))x(t)

_
t  =
S(t)  =  A_,S!,_,)  _

_1  St1XtXtT5t1

A + X(t)TS(- 1-)X(t)

(34)

(3.5)

Finally,  the  update  rule  for the  parameters  of the  lower-level  gating  network  are  as  follows:

V(t+l)

tj

(t)  Sh t )h(In
j

lnh(9) - Ct)3x(t)
\

jli .,

="vii  +(

ij

i

,

,(6

'Note  that in  this section  we use the term  "parameters" for  the variables  that are traditionally  called  "weights"

in the  neural  network  literature.  We  reserve  the term  "weights"  for  the observation  weights.

"7This is  true for  fixed values  of the posterior  probabilities.  These posterior  probabilities  are also changing  over
time,  however,  as  required  by  the  EM  algorithm.  The  overall  convergence  rate  of the  algorithm  is  determined
by  the  convergence  rate  of EM,  not  the  convergence  rate  of Newton-Raphson.

17

qV

Backpropagation
HME

00

'

"  ooW(cid:127)

.....  0..

--o--  -- e- o

1

5

10

50

10

Epochs

Figure  5:  Relative error on  the  test set for a backpropagation  network and  a  four-level  hierarchy
trained  with  on-line  algorithms.  The standard  errors  at  the  minima of the  curves  are  0.008  for
backprop  and  0.009  for  HME.

where  the  inverse  covariance  matrix  Si  is updated  by:

,,  =

Sj

-

A

[h0)]-1  + x(t)TSt._)x- )

((t)

Simulation  results

The  on-line  algorithm  was  tested  on  the  robot  dynamics  problem  described  in  the  previous
section.  Preliminary  simulations  convinced  us of the necessity  of the decay  parameter  (A).  We
also  found  that  this  parameter  should  be  slowly  increased  as  training  proceeds--on  the  early
trials  the posterior  probabilities  are changing  rapidly  so that  the covariances  should  be decayed
rapidly,  whereas  on  later  trials  the  posterior  probabilities  have  stabilized  and  the  covariances
should  be  decayed  less  rapidly.  We  used  a simple  fixed  schedule:  A was  initialized  to 0.99 and
increased  a fixed  fraction  (0.6)  of the  remaining  distance  to  1.0 every  1000  time  steps.

The  performance  of  the  on-line  algorithm  was  compared  to  an  on-line  backpropagation
network.  Parameter settings  for the backpropagation  network  were obtained  by a coarse search
through  the  parameter  space,  yielding  a  value  of  0.15  for  the  learning  rate  and  0.20  for  the
momentum.  The  results  for both  architectures  are  shown  in  Figure 5.  As  can  be seen,  the on-
line  algorithm  for backpropagation  is significantly  faster than the corresponding  batch algorithm
(cf.  Figure  2).  This  is  also  true  of  the  on-line  HME  algorithm,  which  has  nearly  converged
within  the first  epoch.

18

Architecture
linear
backprop  (on-line)
HME  (on-line)
HME  (gradient)

Relative  Error [ #  Epochs

.32
.08
.12
.15

1
63
2
104

Table  2:  Average  values  of relative  error and  number of epochs  required  for  convergence  for the
on-line  algorithms.

The  minimum  values  of relative  error  and  the  convergence  times  for  both  architectures  are
provided  in  Table  2.  We also  provide  the  corresponding  values  for  a simulation  of the on-line
gradient  algorithm  for  the HME  architecture  (Equation  17).

We also performed a set of simulations which  tested  a variety of different  HME architectures.
We  compared  a one-level  hierarchy  with  32  experts  to hierarchies  with  five  levels  (32  experts),
and  six  levels  (64  experts).  We  also  simulated  two  three-level  hierarchies,  one  with  branching
factors of 4, 4, and 2 (proceeding  from the top of the tree  to the bottom),  and one with branching
factors  of 2,  4,  and 4.  (Each  three-level  hierarchy  contained  32 experts.)  The results  are  shown
in  Figure  6.  As  can  be  seen,  there  was  a significant  difference  between  the  one-level  hierarchy
and  the other  architectures.  There  were  smaller  differences  among  the  multi-level  hierarchies.
No significant  difference  was  observed  between  the two  different  3-level  architectures.

Model  selection

Utilizing  the HME  approach  requires that choices  be made regarding  the structural  parameters
of the model,  in  particular  the  number  of levels  and  the branching  factor  of the  tree.  As with
other  flexible  estimation  techniques,  it  is  desirable  to allow  these  structural  parameters  to be
chosen  based  at  least  partly  on  the  data.  This  model  selection  problem  can  be  addressed  in  a
variety  of ways.  In  this  paper  we have  utilized  a test  set  approach  to model  selection,  stopping
the training when  the error  on the test  set reaches  a  minimum.  As is  the case with other neural
network  algorithms,  this  procedure  can  be  justified  as  a complexity  control  measure.  As  we
have noted,  when  the parameters  in  the gating  networks  of an  HME  architecture  are small,  the
entire  system  reduces  to  a  single  "averaged"  GLIM  at  the  root  of the  tree.  As  the  training
proceeds,  the  parameters  in  the  gating  networks  begin  to  grow  in  magnitude  and  splits  are
formed.  When  a  split  is  formed  the  parameters  in  the  branches  of the  tree  on  either  side  of
the split  are  decoupled  and  the effective  number  of degrees  of freedom  in  the system  increases.
This  increase  in  complexity  takes  place  gradually  as  the values  of the parameters  increase  and
the splits  sharpen.  By  stopping  the training  of the system  based on  the performance  on a test
set,  we  obtain  control  over  the effective  number  of degrees  of freedom  in  the architecture.

Other approaches  to model  selection  can  also be considered.  One  natural approach  is  to use
ridge regression  in each  of the expert  networks  and the gating networks.  This approach extends
naturally  to  the  on-line  setting  in  the form  of  a  "weight  decay."  It  is  also  worth  considering
Bayesian  techniques  of the kind  considered  in  the decision  tree  literature  by  Buntine  (1991),  as
well  as the  MDL  methods  of Quinlan  and  Rivest  (1989).

19

oU

CD
6

10

-

a  C;
40

o  _

1 -level

....... 3-level  (a)
3-level  (b)
5-level
6-level

--

0

2

4

6

8

10

Epochs

Figure  6:  Relative  error on  the  test set  for  HME  hierarchies  with  different  structures.  "3-level
(a)-  refers  to a 3-level  hierarchy  with  branching  factors  of 4,  4,  and  2,  and  "3-level  (b)"  refers
to  a 3-level  hierarchy  with  branching  factors  of 2,  4,  and  4.  The  standard  errors  for  all  curves
at  their  respective  minima  were  approximately  0.009.

Related  work

There  are  a  variety  of ties that  can  be  made  between  the  HME  architecture  and  related  work
in  statistics,  machine  learning,  and  neural networks.  In  this section  we  briefly  mention  some  of
these  ties  and  make  some  comparative  remarks.

Our architecture  is  not  the only  nonlinear  approximator  to make  substantial  use of GLIM's
and  the  IRLS  algorithm.  IRLS  also figures  prominently  in  a branch of nonparametric  statistics
known  as  generalized  additive  models  (GAM's;  Hastie  & Tibshirani,  1990).  It  is interesting  to
note the complementary  roles of IRLS  in  these two architectures.  In the  GAM  model,  the IRLS
algorithm  appears  in  the outer  loop,  providing  an  adjusted  dependent  variable  that  is fit  by  a
backfitting  procedure  in  the  inner  loop.  In  the  HME  approach,  on  the  other  hand,  the outer
loop  is  the  E step of EM  and  IRLS  is  in  the  inner loop.  This  complementarity  suggests  that  it
might  be of interest  to consider  hybrid  models  in  which  a HME  is  nested  inside  a GAM  or vice
versa.

We  have  already  mentioned  the  close  ties  between  the  HME  approach  and  other  tree-
structured estimators  such  as  CART  and MARS.  Our approach  differs  from  MARS  and  related
architectures-such  as  the  basis-function  trees  of  Sanger  (1990)-by  allowing  splits  that  are
oblique  with  respect  to the  axes.  We  also differ  from  these architectures  by  using  a statistical
model-the  multinomial  logit  model-for  the splits.  We believe  that both of these features  can
play  a  role  in  increasing  predictive  ability-the  use  of oblique  splits  should  tend  to  decrease

20

V

bias,  and  the  use of smooth  multinomial  logit  splits should  generally  decrease  variance.  Oblique
splits  also render  the HME  architecture  insensitive  to the  particular  choice  of coordinates  used
to encode  the  data.  Finally,  it  is  worth  emphasizing  the  difference  in  philosophy  behind  these
architectures.  Whereas  CART  and  MARS  are  entirely  nonparametric,  the  HME  approach  has
a strong  flavor  of parametric  statistics,  via its  use of generalized  linear  models,  mixture  models
and  maximum  likelihood.

Similar  comments can  be made with  respect  to the decision  tree methodology  in the machine
learning  literature.  Algorithms  such  as  ID3  build  trees  that  have  axis-parallel  splits  and  use
heuristic  splitting  algorithms  (Quinlan,  1986).  More  recent  research  has  studied  decision  trees
with  oblique  splits  (Murthy,  Kasif  &  Salzberg,  1993;  Utgoff  & Brodley,  1990).  None  of these
papers,  however,  have  treated  the  problem  of splitting  data  as  a statistical  problem.  nor  have
they  provided  a global  goodness-of-fit  measure  for their  trees.

There  are  a  variety  of  neural  network  architectures  that  are  related  to  the  HME  architec-
ture.  The  multi-resolution  aspect  of  HME  is  reminiscent  of Moody's  (1989)  multi-resolution
CMAC  hierarchy,  differing  in  that Moody's  levels  of resolution  are  handled  explicitly  by sepa-
rate  networks.  The  "neural  tree"  algorithm  (Strbmberg,  Zrida, & Isaksson,  1991)  is  a  decision
tree  with  multi-layer  perceptions  (MLP's)  at  the  non-terminals.  This  architecture  can  form
oblique  (or  curvilinear)  splits,  however  the  MLP's  are  trained  by  a heuristic  that  has  no  clear
relationship  to  overall  classification  performance.  Finally,  Hinton  and  Nowlan  (see  Nowlan,
1991)  have independently  proposed  extending  the  Jacobs  et al.  (1991)  modular  architecture  to
a  tree-structured  system.  They  did  not  develop  a likelihood  approach  to the problem,  however,
proposing  instead  a  heuristic  splitting  scheme.

Conclusions

We  have  presented  a  tree-structured  architecture  for  supervised  learning.  We  have  developed
the  learning  algorithm  for  this  architecture  within  the framework  of maximum  likelihood  esti-
mation,  utilizing  ideas from mixture  model estimation and  generalized  linear  model  theory.  The
maximum  likelihood  framework  allows  standard  tools  from  statistical  theory  to  be  brought  to
bear  in  developing  inference  procedures  and  measures  of uncertainty  for the  architecture  (Cox
&  Hinkley,  1974).  It  also opens  the  door  to the  Bayesian  approaches  that  have  been  found  to
be  useful  in  the  context  of unsupervised  mixture  model  estimation  (Cheeseman,  et  al.,  1988).
Although  we  have  not  emphasized  theoretical  issues  in  this  paper,  there  are  a  number  of
points  that  are  worth  mentioning.  First,  the  set  of  exponentially-smoothed  piecewise  linear
functions  that we  have utilized  are  clearly dense  in the set  of piecewise  linear  functions  on  com-
pact  sets  in  *I,
thus it  is  straightforward  to  show  that  the  hierarchical  architecture  is  dense
in  the  set  of continuous  functions  on  compact  sets  in  Wm.  That  is,  the architecture  is  "univer-
sal"  in  the  sense  of Hornik,  Stinchcombe,  and  White  (1989).  From  this  result  it  would  seem
straightforward  to develop  consistency  results  for  the architecture  (cf.  Geman,  Bienenstock,  &
Doursat,  1992;  Stone,  1977).  We  are  currently  developing  this  line  of argument  and  are study-
ing  the  asymptotic  distributional  properties  of fixed  hierarchies.  Second,  convergence  results
are  available  for  the architecture.  We  have  shown  that  the convergence  rate of the algorithm  is
linear  in  the condition  number  of a matrix  that is  the product  of an  inverse  covariance  matrix
and  the Hessian  of the log likelihood  for the  architecture  (Jordan  & Xu,  1993).

Finally,  it  is  worth  noting  a  number  of possible  extensions  of the work  reported  here.  Our
earlier  work on  hierarchical  mixtures  of experts  utilized  the multilayer  perceptron  as  the  prim-
itive  function  for  the  expert  networks  and  gating  networks  (Jordan  &  Jacobs,  1992).  That

21

option  is  still  available,  although  we lose  the EM  proof of convergence  (cf.  Jordan &  Xu,  1993)
and  we  lose  the  ability  to fit  the  sub-networks  efficiently  with  IRLS.  One  interesting  example
of such  an  application  is  the  case  where  the  experts  are  auto-associators  (Bourlard  & Kamp,
1988).  in  which  case  the  architecture  fits  hierarchically-nested  local  principal  component  de-
compositions.  Another  area  in  unsupervised  learning  worth  exploring  is  the  non-associative
version  of the  hierarchical  architecture.  Such  a model  would  be  a recursive  version  of classical
mixture-likelihood  clustering  and  may  have  interesting  ties  to  hierarchical  clustering  models.
Finally.  it  is  also  of interest  to  note  that  the  recursive  least  squares  algorithm  that  we  utilized
in  obtaining  an  on-line  variant  of Algorithm  2  is  not  the only  possible  on-line  approach.  Any
of  the  fast  filter  algorithms  (Haykin,  1991)  could  also  be  utilized,  giving  rise  to  a  family  of
on-line  algorithms.  Also.  it  is  worth  studying  the  application  of the  recursive  algorithms  to
PRESS-like  cross-validation  calculations  to efficiently  compute  the  changes  in  likelihood  that
arise  from  adding  or  deleting  parameters  or  data points.

Acknowledgements:  We  want  to thank  Geoffrey  Hinton,  Tony  Robinson,  Mitsuo  Kawato, and  Daniel
Wolpert  for  helpful  comments  on the  manuscript.

