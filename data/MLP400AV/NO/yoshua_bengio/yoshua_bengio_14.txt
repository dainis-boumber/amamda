Abstract

While logistic sigmoid neurons are more bi-
ologically plausible than hyperbolic tangent
neurons, the latter work better for train-
ing multi-layer neural networks. This pa-
per shows that rectifying neurons are an
even better model of biological neurons and
yield equal or better performance than hy-
perbolic tangent networks in spite of the
hard non-linearity and non-dierentiability
at zero, creating sparse representations with
true zeros, which seem remarkably suitable
for naturally sparse data. Even though they
can take advantage of semi-supervised setups
with extra-unlabeled data, deep rectier net-
works can reach their best performance with-
out requiring any unsupervised pre-training
on purely supervised tasks with large labeled
datasets. Hence, these results can be seen as
a new milestone in the attempts at under-
standing the diculty in training deep but
purely supervised neural networks, and clos-
ing the performance gap between neural net-
works learnt with and without unsupervised
pre-training.

1 Introduction

Many dierences exist between the neural network
models used by machine learning researchers and those
used by computational neuroscientists. This is in part

Appearing in Proceedings of the 14th International Con-
ference on Articial Intelligence and Statistics (AISTATS)
2011, Fort Lauderdale, FL, USA. Volume 15 of JMLR:
W&CP 15. Copyright 2011 by the authors.

because the objective of the former is to obtain com-
putationally ecient learners, that generalize well to
new examples, whereas the objective of the latter is to
abstract out neuroscientic data while obtaining ex-
planations of the principles involved, providing predic-
tions and guidance for future biological experiments.
Areas where both objectives coincide are therefore
particularly worthy of investigation, pointing towards
computationally motivated principles of operation in
the brain that can also enhance research in articial
intelligence.
In this paper we show that two com-
mon gaps between computational neuroscience models
and machine learning neural network models can be
bridged by using the following linear by part activa-
tion : max(0, x), called the rectier (or hinge) activa-
tion function. Experimental results will show engaging
training behavior of this activation function, especially
for deep architectures (see Bengio (2009) for a review),
i.e., where the number of hidden layers in the neural
network is 3 or more.
Recent theoretical and empirical work in statistical
machine learning has demonstrated the importance of
learning algorithms for deep architectures. This is in
part inspired by observations of the mammalian vi-
sual cortex, which consists of a chain of processing
elements, each of which is associated with a dierent
representation of the raw visual input. This is partic-
ularly clear in the primate visual system (Serre et al.,
2007), with its sequence of processing stages: detection
of edges, primitive shapes, and moving up to gradu-
ally more complex visual shapes. Interestingly, it was
found that the features learned in deep architectures
resemble those observed in the rst two of these stages
(in areas V1 and V2 of visual cortex) (Lee et al., 2008),
and that they become increasingly invariant to factors
of variation (such as camera movement) in higher lay-
ers (Goodfellow et al., 2009).

315Deep Sparse Rectier Neural Networks

Regarding the training of deep networks, something
that can be considered a breakthrough happened
in 2006, with the introduction of Deep Belief Net-
works (Hinton et al., 2006), and more generally the
idea of initializing each layer by unsupervised learn-
ing (Bengio et al., 2007; Ranzato et al., 2007). Some
authors have tried to understand why this unsuper-
vised procedure helps (Erhan et al., 2010) while oth-
ers investigated why the original training procedure for
deep neural networks failed (Bengio and Glorot, 2010).
From the machine learning point of view, this paper
brings additional results in these lines of investigation.
We propose to explore the use of rectifying non-
linearities as alternatives to the hyperbolic tangent
or sigmoid in deep articial neural networks, in ad-
dition to using an L1 regularizer on the activation val-
ues to promote sparsity and prevent potential numer-
ical problems with unbounded activation. Nair and
Hinton (2010) present promising results of the inu-
ence of such units in the context of Restricted Boltz-
mann Machines compared to logistic sigmoid activa-
tions on image classication tasks. Our work extends
this for the case of pre-training using denoising auto-
encoders (Vincent et al., 2008) and provides an exten-
sive empirical comparison of the rectifying activation
function against the hyperbolic tangent on image clas-
sication benchmarks as well as an original derivation
for the text application of sentiment analysis.
Our experiments on image and text data indicate that
training proceeds better when the articial neurons are
either o or operating mostly in a linear regime. Sur-
prisingly, rectifying activation allows deep networks to
achieve their best performance without unsupervised
pre-training. Hence, our work proposes a new contri-
bution to the trend of understanding and merging the
performance gap between deep networks learnt with
and without unsupervised pre-training (Erhan et al.,
2010; Bengio and Glorot, 2010). Still, rectier net-
works can benet from unsupervised pre-training in
the context of semi-supervised learning where large
amounts of unlabeled data are provided. Furthermore,
as rectier units naturally lead to sparse networks and
are closer to biological neurons responses in their main
operating regime, this work also bridges (in part) a
machine learning / neuroscience gap in terms of acti-
vation function and sparsity.
This paper is organized as follows. Section 2 presents
some neuroscience and machine learning background
which inspired this work. Section 3 introduces recti-
er neurons and explains their potential benets and
drawbacks in deep networks. Then we propose an
experimental study with empirical results on image
recognition in Section 4.1 and sentiment analysis in
Section 4.2. Section 5 presents our conclusions.

2 Background

2.1 Neuroscience Observations

For models of biological neurons, the activation func-
tion is the expected ring rate as a function of the
total input currently arising out of incoming signals
at synapses (Dayan and Abott, 2001). An activation
function is termed, respectively antisymmetric or sym-
metric when its response to the opposite of a strongly
excitatory input pattern is respectively a strongly in-
hibitory or excitatory one, and one-sided when this
response is zero. The main gaps that we wish to con-
sider between computational neuroscience models and
machine learning models are the following:

 Studies on brain energy expense suggest that
neurons encode information in a sparse and dis-
tributed way (Attwell and Laughlin, 2001), esti-
mating the percentage of neurons active at the
same time to be between 1 and 4% (Lennie, 2003).
This corresponds to a trade-o between richness
of representation and small action potential en-
ergy expenditure. Without additional regulariza-
tion, such as an L1 penalty, ordinary feedforward
neural nets do not have this property. For ex-
ample, the sigmoid activation has a steady state
2, therefore, after initializing with
regime around 1
small weights, all neurons re at half their satura-
tion regime. This is biologically implausible and
hurts gradient-based optimization (LeCun et al.,
1998; Bengio and Glorot, 2010).

learning models

 Important divergences between biological and
concern non-linear
machine
activation functions.
A common biological
model of neuron, the leaky integrate-and-re (or
LIF ) (Dayan and Abott, 2001), gives the follow-
ing relation between the ring rate and the input
current, illustrated in Figure 1 (left):

(cid:35)1

,

(cid:34)



(cid:16) E+RIVr

(cid:17)

 log

0 ,

+ tref

E+RIVth
if E + RI > Vth
if E + RI  Vth

f(I) =

where tref is the refractory period (minimal time
between two action potentials), I the input cur-
rent, Vr the resting potential and Vth the thresh-
old potential (with Vth > Vr), and R, E, 
the membrane resistance, potential and time con-
stant. The most commonly used activation func-
tions in the deep learning and neural networks lit-
erature are the standard logistic sigmoid and the

316Xavier Glorot, Antoine Bordes, Yoshua Bengio

Figure 1: Left: Common neural activation function motivated by biological data. Right: Commonly
used activation functions in neural networks literature: logistic sigmoid and hyperbolic tangent (tanh).

hyperbolic tangent (see Figure 1, right), which are
equivalent up to a linear transformation. The hy-
perbolic tangent has a steady state at 0, and is
therefore preferred from the optimization stand-
point (LeCun et al., 1998; Bengio and Glorot,
2010), but it forces an antisymmetry around 0
which is absent in biological neurons.

2.2 Advantages of Sparsity

Sparsity has become a concept of interest, not only in
computational neuroscience and machine learning but
also in statistics and signal processing (Candes and
Tao, 2005). It was rst introduced in computational
neuroscience in the context of sparse coding in the vi-
sual system (Olshausen and Field, 1997). It has been
a key element of deep convolutional networks exploit-
ing a variant of auto-encoders (Ranzato et al., 2007,
2008; Mairal et al., 2009) with a sparse distributed
representation, and has also become a key ingredient
in Deep Belief Networks (Lee et al., 2008). A sparsity
penalty has been used in several computational neuro-
science (Olshausen and Field, 1997; Doi et al., 2006)
and machine learning models (Lee et al., 2007; Mairal
et al., 2009), in particular for deep architectures (Lee
et al., 2008; Ranzato et al., 2007, 2008). However, in
the latter, the neurons end up taking small but non-
zero activation or ring probability. We show here that
using a rectifying non-linearity gives rise to real zeros
of activations and thus truly sparse representations.
From a computational point of view, such representa-
tions are appealing for the following reasons:

One of

(Bengio, 2009)

is

 Information disentangling.

of deep learning

the
algo-
claimed objectives
rithms
to disentangle the
factors explaining the variations in the data. A
dense representation is highly entangled because
almost any change in the input modies most of

the entries in the representation vector. Instead,
if a representation is both sparse and robust to
small input changes, the set of non-zero features
is almost always roughly conserved by small
changes of the input.

 Ecient variable-size representation. Dif-
ferent inputs may contain dierent amounts of in-
formation and would be more conveniently repre-
sented using a variable-size data-structure, which
is common in computer representations of infor-
mation. Varying the number of active neurons
allows a model to control the eective dimension-
ality of the representation for a given input and
the required precision.

 Linear separability. Sparse representations are
also more likely to be linearly separable, or more
easily separable with less non-linear machinery,
simply because the information is represented in
a high-dimensional space. Besides, this can reect
the original data format. In text-related applica-
tions for instance, the original raw data is already
very sparse (see Section 4.2).

 Distributed but sparse. Dense distributed rep-
resentations are the richest representations, be-
ing potentially exponentially more ecient than
purely local ones (Bengio, 2009). Sparse repre-
sentations eciency is still exponentially greater,
with the power of the exponent being the number
of non-zero features. They may represent a good
trade-o with respect to the above criteria.

Nevertheless, forcing too much sparsity may hurt pre-
dictive performance for an equal number of neurons,
because it reduces the eective capacity of the model.

317Deep Sparse Rectier Neural Networks

Figure 2: Left: Sparse propagation of activations and gradients in a network of rectier units. The
input selects a subset of active neurons and computation is linear in this subset. Right: Rectier and softplus
activation functions. The second one is a smooth version of the rst.

3 Deep Rectier Networks

3.1 Rectier Neurons

The neuroscience literature (Bush and Sejnowski,
1995; Douglas and al., 2003) indicates that corti-
cal neurons are rarely in their maximum saturation
regime, and suggests that their activation function can
be approximated by a rectier. Most previous stud-
ies of neural networks involving a rectifying activation
function concern recurrent networks (Salinas and Ab-
bott, 1996; Hahnloser, 1998).
The rectier function rectier(x) = max(0, x) is one-
sided and therefore does not enforce a sign symmetry1
or antisymmetry1: instead, the response to the oppo-
site of an excitatory input pattern is 0 (no response).
However, we can obtain symmetry or antisymmetry by
combining two rectier units sharing parameters.

Advantages The rectier activation function allows
a network to easily obtain sparse representations. For
example, after uniform initialization of the weights,
around 50% of hidden units continuous output val-
ues are real zeros, and this fraction can easily increase
with sparsity-inducing regularization. Apart from be-
ing more biologically plausible, sparsity also leads to
mathematical advantages (see previous section).
As illustrated in Figure 2 (left), the only non-linearity
in the network comes from the path selection associ-
ated with individual neurons being active or not. For a
given input only a subset of neurons are active. Com-
putation is linear on this subset: once this subset of
neurons is selected, the output is a linear function of

1The hyperbolic tangent absolute value non-linearity
| tanh(x)| used by Jarrett et al. (2009) enforces sign symme-
try. A tanh(x) non-linearity enforces sign antisymmetry.

the input (although a large enough change can trigger
a discrete change of the active set of neurons). The
function computed by each neuron or by the network
output in terms of the network input is thus linear by
parts. We can see the model as an exponential num-
ber of linear models that share parameters (Nair and
Hinton, 2010). Because of this linearity, gradients ow
well on the active paths of neurons (there is no gra-
dient vanishing eect due to activation non-linearities
of sigmoid or tanh units), and mathematical investi-
gation is easier. Computations are also cheaper: there
is no need for computing the exponential function in
activations, and sparsity can be exploited.

Potential Problems One may hypothesize that the
hard saturation at 0 may hurt optimization by block-
ing gradient back-propagation. To evaluate the poten-
tial impact of this eect we also investigate the soft-
plus activation: softplus(x) = log(1+ex) (Dugas et al.,
2001), a smooth version of the rectifying non-linearity.
We lose the exact sparsity, but may hope to gain eas-
ier training. However, experimental results (see Sec-
tion 4.1) tend to contradict that hypothesis, suggesting
that hard zeros can actually help supervised training.
We hypothesize that the hard non-linearities do not
hurt so long as the gradient can propagate along some
paths, i.e., that some of the hidden units in each layer
are non-zero. With the credit and blame assigned to
these ON units rather than distributed more evenly, we
hypothesize that optimization is easier. Another prob-
lem could arise due to the unbounded behavior of the
activations; one may thus want to use a regularizer to
prevent potential numerical problems. Therefore, we
use the L1 penalty on the activation values, which also
promotes additional sparsity. Also recall that, in or-
der to eciently represent symmetric/antisymmetric
behavior in the data, a rectier network would need

318Xavier Glorot, Antoine Bordes, Yoshua Bengio

rectier networks

twice as many hidden units as a network of symmet-
ric/antisymmetric activation functions.
ill-
Finally,
conditioning of the parametrization.
Biases and
weights can be scaled in dierent (and consistent) ways
while preserving the same overall network function.
More precisely, consider for each layer of depth i of
the network a scalar i, and scaling the parameters as
. The output units values
W

subject

and b

are

to

bi(cid:81)i

(cid:48)
i =

(cid:48)
then change as follow: s

j=1 j
=

(cid:48)
i =

Wi
i

long as(cid:81)n

s(cid:81)n

j=1 j

. Therefore, as

j=1 j is 1, the network function is identical.

3. Use a linear activation function for the reconstruc-
tion layer, along with a quadratic cost. We tried
to use input unit values either before or after the
rectier non-linearity as reconstruction targets.
(For the rst layer, raw inputs are directly used.)

4. Use a rectier activation function for the recon-

struction layer, along with a quadratic cost.

The rst strategy has proven to yield better gener-
alization on image data and the second one on text
data. Consequently, the following experimental study
presents results using those two.

3.2 Unsupervised Pre-training

4 Experimental Study

This paper is particularly inspired by the sparse repre-
sentations learned in the context of auto-encoder vari-
ants, as they have been found to be very useful in
training deep architectures (Bengio, 2009), especially
for unsupervised pre-training of neural networks (Er-
han et al., 2010).
Nonetheless, certain diculties arise when one wants
to introduce rectier activations into stacked denois-
ing auto-encoders (Vincent et al., 2008). First, the
hard saturation below the threshold of the rectier
function is not suited for the reconstruction units. In-
deed, whenever the network happens to reconstruct
a zero in place of a non-zero target, the reconstruc-
tion unit can not backpropagate any gradient.2 Sec-
ond, the unbounded behavior of the rectier activation
also needs to be taken into account.
In the follow-
ing, we denote x the corrupted version of the input x,
() the logistic sigmoid function and  the model pa-
rameters (Wenc, benc, Wdec, bdec), and dene the linear
recontruction function as:

f(x, ) = Wdec max(Wencx + benc, 0) + bdec .

Here are the several strategies we have experimented:

1. Use a softplus activation function for the recon-

struction layer, along with a quadratic cost:
L(x, ) = ||x  log(1 + exp(f(x, )))||2 .

2. Scale the rectier activation values coming from
the previous encoding layer to bound them be-
tween 0 and 1, then use a sigmoid activation func-
tion for the reconstruction layer, along with a
cross-entropy reconstruction cost.
L(x, ) = x log((f(x, )))

(1  x) log(1  (f(x, ))) .

2Why is this not a problem for hidden layers too? we hy-
pothesize that it is because gradients can still ow through
the active (non-zero), possibly helping rather than hurting
the assignment of credit.

This section discusses our empirical evaluation of recti-
er units for deep networks. We rst compare them to
hyperbolic tangent and softplus activations on image
benchmarks with and without pre-training, and then
apply them to the text task of sentiment analysis.

4.1 Image Recognition

Experimental setup We considered the image
datasets detailed below. Each of them has a train-
ing set (for tuning parameters), a validation set (for
tuning hyper-parameters) and a test set (for report-
ing generalization performance). They are presented
according to their number of training/validation/test
examples, their respective image sizes, as well as their
number of classes:

 MNIST (LeCun et al., 1998): 50k/10k/10k, 28 

28 digit images, 10 classes.

 CIFAR10

(Krizhevsky

and Hinton,

2009):
50k/5k/5k, 32  32  3 RGB images, 10 classes.
 NISTP: 81,920k/80k/20k, 32  32 character im-
ages from the NIST database 19, with randomized
distortions (Bengio and al, 2010), 62 classes. This
dataset is much larger and more dicult than the
original NIST (Grother, 1995).

 NORB:

taken

images of

233,172/58,428/58,320,

from
Jittered-Cluttered NORB (LeCun et al., 2004).
Stereo-pair
toys on a cluttered
background, 6 classes. The data has been prepro-
cessed similarly to (Nair and Hinton, 2010): we
subsampled the original 2  108  108 stereo-pair
images to 2  32  32 and scaled linearly the
image in the range [1,1]. We followed the
procedure used by Nair and Hinton (2010) to
create the validation set.

319Deep Sparse Rectier Neural Networks

Table 1: Test error on networks of depth 3. Bold
results represent statistical equivalence between similar ex-
periments, with and without pre-training, under the null
hypothesis of the pairwise test with p = 0.05.

Neuron MNIST CIFAR10 NISTP NORB

With unsupervised pre-training

Rectier
Tanh
Softplus

1.20%
1.16%
1.17%

49.96% 32.86% 16.46%
17.66%
50.79%
49.52% 33.27% 19.19%

35.89%

Without unsupervised pre-training

Rectier
Tanh
Softplus

1.43%
1.57%
1.77%

50.86% 32.64% 16.40%
19.29%
52.62%
53.20%
17.68%

36.46%
35.48%

For all experiments except on the NORB data (Le-
Cun et al., 2004), the models we used are stacked
denoising auto-encoders (Vincent et al., 2008) with
three hidden layers and 1000 units per layer. The ar-
chitecture of Nair and Hinton (2010) has been used
on NORB: two hidden layers with respectively 4000
and 2000 units. We used a cross-entropy reconstruc-
tion cost for tanh networks and a quadratic cost
over a softplus reconstruction layer for the rectier
and softplus networks. We chose masking noise as
the corruption process: each pixel has a probability
of 0.25 of being articially set to 0. The unsuper-
vised learning rate is constant, and the following val-
ues have been explored: {.1, .01, .001, .0001}. We se-
lect the model with the lowest reconstruction error.
For the supervised ne-tuning we chose a constant
learning rate in the same range as the unsupervised
learning rate with respect to the supervised valida-
tion error. The training cost is the negative log likeli-
hood  log P (correct class|input) where the probabil-
ities are obtained from the output layer (which imple-
ments a softmax logistic regression). We used stochas-
tic gradient descent with mini-batches of size 10 for
both unsupervised and supervised training phases.
To take into account the potential problem of rectier
units not being symmetric around 0, we use a vari-
ant of the activation function for which half of the
units output values are multiplied by -1. This serves
to cancel out the mean activation value for each layer
and can be interpreted either as inhibitory neurons or
simply as a way to equalize activations numerically.
Additionally, an L1 penalty on the activations with a
coecient of 0.001 was added to the cost function dur-
ing pre-training and ne-tuning in order to increase the
amount of sparsity in the learned representations.

Main results Table 1 summarizes the results on
networks of 3 hidden layers of 1000 hidden units each,

Figure 3: Inuence of nal sparsity on accu-
racy.
200 randomly initialized deep rectier networks
were trained on MNIST with various L1 penalties (from
0 to 0.01) to obtain dierent sparsity levels. Results show
that enforcing sparsity of the activation does not hurt nal
performance until around 85% of true zeros.

comparing all the neuron types3 on all the datasets,
with or without unsupervised pre-training. In the lat-
ter case, the supervised training phase has been carried
out using the same experimental setup as the one de-
scribed above for ne-tuning. The main observations
we make are the following:

 Despite the hard threshold at 0, networks trained
with the rectier activation function can nd lo-
cal minima of greater or equal quality than those
obtained with its smooth counterpart, the soft-
plus. On NORB, we tested a rescaled version
of the softplus dened by 1
 sof tplus(x), which
allows to interpolate in a smooth manner be-
tween the softplus ( = 1) and the rectier ( =
). We obtained the following /test error cou-
ples: 1/17.68%, 1.3/17.53%, 2/16.9%, 3/16.66%,
6/16.54%, /16.40%. There is no trade-o be-
tween those activation functions. Rectiers are
not only biologically plausible, they are also com-
putationally ecient.

 There is almost no improvement when using un-
supervised pre-training with rectier activations,
contrary to what is experienced using tanh or soft-
plus. Purely supervised rectier networks remain
competitive on all 4 datasets, even against the
pretrained tanh or softplus models.

3We also tested a rescaled version of the LIF and
max(tanh(x), 0) as activation functions. We obtained
worse generalization performance than those of Table 1,
and chose not to report them.

320Xavier Glorot, Antoine Bordes, Yoshua Bengio

 Rectier networks are truly deep sparse networks.
There is an average exact sparsity (fraction of ze-
ros) of the hidden layers of 83.4% on MNIST,
72.0% on CIFAR10, 68.0% on NISTP and 73.8%
on NORB. Figure 3 provides a better understand-
ing of the inuence of sparsity.
It displays the
MNIST test error of deep rectier networks (with-
out pre-training) according to dierent average
sparsity obtained by varying the L1 penalty on
the activations. Networks appear to be quite ro-
bust to it as models with 70% to almost 85% of
true zeros can achieve similar performances.

With labeled data, deep rectier networks appear to
be attractive models. They are biologically credible,
and, compared to their standard counterparts, do not
seem to depend as much on unsupervised pre-training,
while ultimately yielding sparse representations.
This last conclusion is slightly dierent from those re-
ported in (Nair and Hinton, 2010) in which is demon-
strated that unsupervised pre-training with Restricted
Boltzmann Machines and using rectier units is ben-
ecial.
In particular, the paper reports that pre-
trained rectied Deep Belief Networks can achieve a
test error on NORB below 16%. However, we be-
lieve that our results are compatible with those: we
extend the experimental framework to a dierent kind
of models (stacked denoising auto-encoders) and dif-
ferent datasets (on which conclusions seem to be dier-
ent). Furthermore, note that our rectied model with-
out pre-training on NORB is very competitive (16.4%
error) and outperforms the 17.6% error of the non-
pretrained model from Nair and Hinton (2010), which
is basically what we nd with the non-pretrained soft-
plus units (17.68% error).

Semi-supervised setting Figure 4 presents re-
sults of semi-supervised experiments conducted on the
NORB dataset. We vary the percentage of the orig-
inal labeled training set which is used for the super-
vised training phase of the rectier and hyperbolic tan-
gent networks and evaluate the eect of the unsuper-
vised pre-training (using the whole training set, unla-
beled). Conrming conclusions of Erhan et al. (2010),
the network with hyperbolic tangent activations im-
proves with unsupervised pre-training for any labeled
set size (even when all the training set is labeled).
However, the picture changes with rectifying activa-
tions.
In semi-supervised setups (with few labeled
data), the pre-training is highly benecial. But the
more the labeled set grows, the closer the models with
and without pre-training. Eventually, when all avail-
able data is labeled, the two models achieve identical
performance. Rectier networks can maximally ex-
ploit labeled and unlabeled information.

Figure 4: Eect of unsupervised pre-training. On
NORB, we compare hyperbolic tangent and rectier net-
works, with or without unsupervised pre-training, and ne-
tune only on subsets of increasing size of the training set.

4.2 Sentiment Analysis
Nair and Hinton (2010) also demonstrated that recti-
er units were ecient for image-related tasks. They
mentioned the intensity equivariance property (i.e.
without bias parameters the network function is lin-
early variant to intensity changes in the input) as ar-
gument to explain this observation. This would sug-
gest that rectifying activation is mostly useful to im-
age data. In this section, we investigate on a dierent
modality to cast a fresh light on rectier units.
A recent study (Zhou et al., 2010) shows that Deep Be-
lief Networks with binary units are competitive with
the state-of-the-art methods for sentiment analysis.
This indicates that deep learning is appropriate to this
text task which seems therefore ideal to observe the
behavior of rectier units on a dierent modality, and
provide a data point towards the hypothesis that rec-
tier nets are particarly appropriate for sparse input
vectors, such as found in NLP. Sentiment analysis is
a text mining area which aims to determine the judg-
ment of a writer with respect to a given topic (see
(Pang and Lee, 2008) for a review). The basic task
consists in classifying the polarity of reviews either by
predicting whether the expressed opinions are positive
or negative, or by assigning them star ratings on either
3, 4 or 5 star scales.
Following a task originally proposed by Snyder and
Barzilay (2007), our data consists of restaurant reviews
which have been extracted from the restaurant review
site www.opentable.com. We have access to 10,000
labeled and 300,000 unlabeled training reviews, while
the test set contains 10,000 examples. The goal is to
predict the rating on a 5 star scale and performance is
evaluated using Root Mean Squared Error (RMSE).4

4Even though our tasks are identical, our database is

321Deep Sparse Rectier Neural Networks

Customers review:
Overpriced, food small portions, not well described on menu.
Food quality was good, but way too many avors and textures going on in every single dish. Didnt quite all go together.
Calameri was lightly fried and not oilygood jobthey need to learn how to make desserts better as ours was frozen.
The food was wonderful, the service was excellent and it was a very vibrant scene. Only complaint would be that it was a bit noisy.
We had a great time there for Mothers Day. Our server was great! Attentive, funny and really took care of us!

Rating

(cid:63)
(cid:63)(cid:63)
(cid:63) (cid:63) (cid:63)
(cid:63) (cid:63) (cid:63)(cid:63)
(cid:63) (cid:63) (cid:63) (cid:63) (cid:63)

Figure 5: Examples of restaurant reviews from www.opentable.com dataset. The learner must predict the
related rating on a 5 star scale (right column).
Figure 5 displays some samples of the dataset. The re-
view text is treated as a bag of words and transformed
into binary vectors encoding the presence/absence of
terms. For computational reasons, only the 5000 most
frequent terms of the vocabulary are kept in the fea-
ture set.5 The resulting preprocessed data is very
sparse: 0.6% of non-zero features on average. Un-
supervised pre-training of the networks employs both
labeled and unlabeled training reviews while the su-
pervised ne-tuning phase is carried out by 10-fold
cross-validation on the labeled training examples.
The model are stacked denoising auto-encoders, with
1 or 3 hidden layers of 5000 hidden units and rectier
or tanh activation, which are trained in a greedy layer-
wise fashion. Predicted ratings are dened by the ex-
pected star value computed using multiclass (multino-
mial, softmax) logistic regression output probabilities.
For rectier networks, when a new layer is stacked, ac-
tivation values of the previous layer are scaled within
the interval [0,1] and a sigmoid reconstruction layer
with a cross-entropy cost is used. We also add an L1
penalty to the cost during pre-training and ne-tuning.
Because of the binary input, we use a salt and pepper
noise (i.e. masking some inputs by zeros and others
by ones) for unsupervised training of the rst layer. A
zero masking (as in (Vincent et al., 2008)) is used for
the higher layers. We selected the noise level based on
the classication performance, other hyperparameters
are selected according to the reconstruction error.

tain a RMSE lower than 0.833. Additionally, although
we can not replicate the original very high degree of
sparsity of the training data, the 3-layers network can
still attain an overall sparsity of more than 50%. Fi-
nally, on data with these particular properties (binary,
high sparsity), the 3-layers network with tanh activa-
tion function (which has been learnt with the exact
same pre-training+ne-tuning setup) is clearly outper-
formed. The sparse behavior of the deep rectier net-
work seems particularly suitable in this case, because
the raw input is very sparse and varies in its number of
non-zeros. The latter can also be achieved with sparse
internal representations, not with dense ones.
Since no result has ever been published on the
OpenTable data, we applied our model on the Amazon
sentiment analysis benchmark (Blitzer et al., 2007) in
order to assess the quality of our network with respect
to literature methods. This dataset proposes reviews
of 4 kinds of Amazon products, for which the polarity
(positive or negative) must be predicted. We followed
the experimental setup dened by Zhou et al. (2010).
In their paper, the best model achieves a test accuracy
of 73.72% (on average over the 4 kinds of products)
where our 3-layers rectier network obtains 78.95%.

Table 2: Test RMSE and sparsity level obtained
by 10-fold cross-validation on OpenTable data.

Network
No hidden layer
Rectier (1-layer)
Rectier (3-layers)
Tanh (3-layers)

RMSE

0.885  0.006
0.807  0.004
0.746  0.004
0.774  0.008

Sparsity
99.4%  0.0
28.9%  0.2
53.9%  0.7
00.0%  0.0

Results are displayed in Table 2.
Interestingly, the
RMSE signicantly decreases as we add hidden layers
to the rectier neural net. These experiments con-
rm that rectier networks improve after an unsuper-
vised pre-training phase in a semi-supervised setting:
with no pre-training, the 3-layers model can not ob-

much larger than the one of (Snyder and Barzilay, 2007).
5Preliminary experiments suggested that larger vocab-

ulary sizes did not markedly change results.

5 Conclusion
Sparsity and neurons operating mostly in a linear
regime can be brought together in more biologically
plausible deep neural networks. Rectier units help
to bridge the gap between unsupervised pre-training
and no pre-training, which suggests that they may
help in nding better minima during training. This
nding has been veried for four image classication
datasets of dierent scales and all this in spite of their
inherent problems, such as zeros in the gradient, or
ill-conditioning of the parametrization. Rather sparse
networks are obtained (from 50 to 80% sparsity for
the best generalizing models, whereas the brain is hy-
pothesized to have 95% to 99% sparsity), which may
explain some of the benet of using rectiers.
Furthermore, rectier activation functions have shown
to be remarkably adapted to sentiment analysis, a
text-based task with a very large degree of data spar-
sity. This promising result tends to indicate that deep
sparse rectier networks are not only benecial to im-
age classication tasks and might yield powerful text
mining tools in the future.

322Xavier Glorot, Antoine Bordes, Yoshua Bengio

