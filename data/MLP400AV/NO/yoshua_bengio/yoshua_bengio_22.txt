Abstract

Several unsupervised learning algorithms based on an eigendecompo-
sition provide either an embedding or a clustering only for given train-
ing points, with no straightforward extension for out-of-sample examples
short of recomputing eigenvectors. This paper provides a unied frame-
work for extending Local Linear Embedding (LLE), Isomap, Laplacian
Eigenmaps, Multi-Dimensional Scaling (for dimensionality reduction)
as well as for Spectral Clustering. This framework is based on seeing
these algorithms as learning eigenfunctions of a data-dependent kernel.
Numerical experiments show that the generalizations performed have a
level of error comparable to the variability of the embedding algorithms
due to the choice of training data.

1 Introduction
Many unsupervised learning algorithms have been recently proposed, all using an eigen-
decomposition for obtaining a lower-dimensional embedding of data lying on a non-linear
manifold: Local Linear Embedding (LLE) (Roweis and Saul, 2000), Isomap (Tenenbaum,
de Silva and Langford, 2000) and Laplacian Eigenmaps (Belkin and Niyogi, 2003). There
are also many variants of Spectral Clustering (Weiss, 1999; Ng, Jordan and Weiss, 2002), in
which such an embedding is an intermediate step before obtaining a clustering of the data
that can capture at, elongated and even curved clusters. The two tasks (manifold learning
and clustering) are linked because the clusters found by spectral clustering can be arbitrary
curved manifolds (as long as there is enough data to locally capture their curvature).

2 Common Framework
In this paper we consider ve types of unsupervised learning algorithms that can be cast
in the same framework, based on the computation of an embedding for the training points
obtained from the principal eigenvectors of a symmetric matrix.
Algorithm 1
1. Start from a data set D = fx1; : : : ; xng with n points in Rd. Construct a n (cid:2) n
neighborhoodor similarity matrix M. Let us denote KD((cid:1);(cid:1)) (or K for shorthand)the
data-dependentfunctionwhichproducesM byMij = KD(xi; xj).
2. Optionally transform M, yielding a normalized matrix ~M. Equivalently, this corre-
spondstogenerating ~M froma ~KD by ~Mij = ~KD(xi; xj).

3. Computethem largestpositiveeigenvalues(cid:21)k andeigenvectorsvk of ~M.
4. Theembeddingofeachexample xi isthevector yi with yik the i-thelementofthe k-th
principaleigenvectorvk of ~M. Alternatively(MDSandIsomap),theembeddingisei,with
eik = p(cid:21)kyik. Iftherst m eigenvaluesarepositive,thenei (cid:1) ej isthebestapproximation
of ~Mij usingonlym coordinates,inthesquarederrorsense.
In the following, we consider the specializations of Algorithm 1 for different unsupervised
learning algorithms. Let Si be the i-th row sum of the afnity matrix M:

Mij:

(1)

Si =Xj

We say that two points (a; b) are k-nearest-neighbors of each other if a is among the k
nearest neighbors of b in D [ fag or vice-versa. We denote by xij the j-th coordinate of
the vector xi.

1

1
n

Sk! :

2.1 Multi-Dimensional Scaling
Multi-Dimensional Scaling (MDS) starts from a notion of distance or afnity K that is
computed between each pair of training examples. We consider here metric MDS (Cox
and Cox, 1994). For the normalization step 2 in Algorithm 1, these distances are converted
to equivalent dot products using the double-centeringformula:

1

2 Mij (cid:0)

1
n

(2)

~Mij = (cid:0)

n2 Xk

Si (cid:0)

Sj +
The embedding eik of example xi is given by p(cid:21)kvki.
2.2 Spectral Clustering
Spectral clustering (Weiss, 1999) can yield impressively good results where traditional
clustering looking for roundblobs in the data, such as K-means, would fail miserably. It
is based on two main steps: rst embedding the data points in a space in which clusters are
more obvious (using the eigenvectors of a Gram matrix), and then applying a classical
clustering algorithm such as K-means, e.g. as in (Ng, Jordan and Weiss, 2002). The afnity
matrix M is formed using a kernel such as the Gaussian kernel. Several normalization steps
have been proposed. Among the most successful ones, as advocated in (Weiss, 1999; Ng,
Jordan and Weiss, 2002), is the following:

~Mij =

:

(3)

Mij

pSiSj

To obtain m clusters, the rst m principal eigenvectors of ~M are computed and K-means
is applied on the unit-norm coordinates, obtained from the embedding yik = vki.
2.3 Laplacian Eigenmaps
Laplacian Eigenmaps is a recently proposed dimensionality reduction procedure (Belkin
and Niyogi, 2003) that has been proposed for semi-supervised learning. The authors use
an approximation of the Laplacian operator such as the Gaussian kernel or the matrix whose
element (i; j) is 1 if xi and xj are k-nearest-neighbors and 0 otherwise. Instead of solving
an ordinary eigenproblem, the following generalized eigenproblem is solved:

(S (cid:0) M )vj = (cid:21)jSvj

(4)
with eigenvalues (cid:21)j, eigenvectors vj and S the diagonal matrix with entries given by eq. (1).
The smallest eigenvalue is left out and the eigenvectors corresponding to the other small
eigenvalues are used for the embedding. This is the same embedding that is computed
with the spectral clustering algorithm from (Shi and Malik, 1997). As noted in (Weiss,
1999) (Normalization Lemma 1), an equivalent result (up to a componentwise scaling of
the embedding) can be obtained by considering the principal eigenvectors of the normalized
matrix dened in eq. (3).

2.4 Isomap
Isomap (Tenenbaum, de Silva and Langford, 2000) generalizes MDS to non-linear mani-
folds. It is based on replacing the Euclidean distance by an approximation of the geodesic
distance on the manifold. We dene the geodesic distance with respect to a data set D, a
distance d(u; v) and a neighborhood k as follows:

~D(a; b) = min

d(pi; pi+1)

(5)

p Xi

where p is a sequence of points of length l (cid:21) 2 with p1 = a, pl = b, pi 2 D 8i 2
f2; : : : ; l (cid:0) 1g and (pi,pi+1) are k-nearest-neighbors. The length l is free in the minimiza-
tion. The Isomap algorithm obtains the normalized matrix ~M from which the embedding
is derived by transforming the raw pairwise distances matrix as follows: rst compute the
matrix Mij = ~D2(xi; xj) of squared geodesic distances with respect to the data D, then
apply to this matrix the distance-to-dot-product transformation (eq. (2)), as for MDS. As in
MDS, the embedding is eik = p(cid:21)kvki rather than yik = vki.
2.5 LLE
The Local Linear Embedding (LLE) algorithm (Roweis and Saul, 2000) looks for an em-
bedding that preserves the local geometry in the neighborhood of each data point. First, a

M = (I (cid:0) W )0(I (cid:0) W )

sparse matrix of local predictive weights Wij is computed, such thatPj Wij = 1, Wij = 0
if xj is not a k-nearest-neighbor of xi and (Pj Wijxj(cid:0)xi)2 is minimized. Then the matrix

(6)
is formed. The embedding is obtained from the lowest eigenvectors of M, except for the
smallest eigenvector which is uninteresting because it is (1; 1; : : : 1), with eigenvalue 0.
Note that the lowest eigenvectors of M are the largest eigenvectors of ~M(cid:22) = (cid:22)I (cid:0) M to
t Algorithm 1 (the use of (cid:22) > 0 will be discussed in section 4.4). The embedding is given
by yik = vki, and is constant with respect to (cid:22).
3 From Eigenvectors to Eigenfunctions
To obtain an embedding for a new data point, we propose to use the Nystrom formula (eq. 9)
(Baker, 1977), which has been used successfully to speed-up kernel methods computations
by focussing the heavier computations (the eigendecomposition) on a subset of examples.
The use of this formula can be justied by considering the convergence of eigenvectors
and eigenvalues, as the number of examples increases (Baker, 1977; Williams and Seeger,
2000; Koltchinskii and Gine, 2000; Shawe-Taylor and Williams, 2003). Intuitively, the
extensions to obtain the embedding for a new example require specifying a new column of
the Gram matrix ~M, through a training-set dependent kernel function ~KD, in which one of
the arguments may be required to be in the training set.

If we start from a data set D, obtain an embedding for its elements, and add more and
more data, the embedding for the points in D converges (for eigenvalues that are unique).
(Shawe-Taylor and Williams, 2003) give bounds on the convergence error (in the case of
kernel PCA). In the limit, we expect each eigenvector to converge to an eigenfunction for
the linear operator dened below, in the sense that the i-th element of the k-th eigenvector
converges to the application of the k-th eigenfunction to xi (up to a normalization factor).

Consider a Hilbert space Hp of functions with inner product hf; gip =R f (x)g(x)p(x)dx;
with a density function p(x). Associate with kernel K a linear operator Kp in Hp:

(Kpf )(x) =Z K(x; y)f (y)p(y)dy:

(7)

We dont know the true density p but we can approximate the above inner product and
linear operator (and its eigenfunctions) using the empirical distribution ^p. An empirical
Hilbert space H ^p is thus dened using ^p instead of p. Note that the proposition below can be

applied even if the kernel is not positive semi-denite, although the embedding algorithms
we have studied are restricted to using the principal coordinates associated with positive
eigenvalues. For a more rigorous mathematical analysis, see (Bengio et al., 2003).
Proposition 1
Let ~K(a; b) be a kernel function, not necessarily positive semi-denite, that gives rise to
a symmetric matrix ~M with entries ~Mij = ~K(xi; xj) upon a dataset D = fx1; : : : ; xng.
Let (vk; (cid:21)k) be an (eigenvector,eigenvalue) pair that solves ~M vk = (cid:21)kvk. Let (fk; (cid:21)0k)
be an (eigenfunction,eigenvalue) pair that solves ( ~K ^pfk)(x) = (cid:21)0kfk(x) for any x, with ^p
the empirical distribution over D. Let ek(x) = yk(x)p(cid:21)k or yk(x) denote the embedding
associated with a new point x. Then

(cid:21)0k =

1
(cid:21)k
n
pn
(cid:21)k

n

Xi=1
fk(x) =
fk(xi) = pnvki
fk(x)
pn
yk(xi) = yik;

yk(x) =

vki ~K(x; xi)

vki ~K(x; xi)

=

1
(cid:21)k

n

Xi=1

ek(xi) = eik

(8)

(9)

(10)

(11)

(12)

See (Bengio et al., 2003) for a proof and further justications of the above formulae. The
generalized embedding for Isomap and MDS is ek(x) = p(cid:21)kyk(x) whereas the one for
spectral clustering, Laplacian eigenmaps and LLE is yk(x).
Proposition 2
In addition, if the data-dependent kernel ~KD is positive semi-denite, then

fk(x) =r n

(cid:21)k

(cid:25)k(x)

where (cid:25)k(x) is the k-th component of the kernel PCA projection of x obtained from the
kernel ~KD (up to centering).
This relation with kernel PCA (Scholkopf, Smola and Muller, 1998), already pointed out
in (Williams and Seeger, 2000), is further discussed in (Bengio et al., 2003).

4 Extending to new Points
Using Proposition 1, one obtains a natural extension of all the unsupervised learning algo-
rithms mapped to Algorithm 1, provided we can write down a kernel function ~K that gives
rise to the matrix ~M on D, and can be used in eq. (11) to generalize the embedding. We
consider each of them in turn below. In addition to the convergence properties discussed in
section 3, another justication for using equation (9) is given by the following proposition:
Proposition 3
If we dene the fk(xi) by eq. (10) and take a new point x, the value of fk(x) that minimizes

m

n

Xi=1  ~K(x; xi) (cid:0)

Xt=1
is given by eq. (9), for m (cid:21) 1 and any k (cid:20) m.
The proof is a direct consequence of the orthogonality of the eigenvectors vk. This proposi-
tion links equations (9) and (10). Indeed, we can obtain eq. (10) when trying to approximate

(13)

(cid:21)0tft(x)ft(xi)!2

~K at the data points by minimizing the cost

n

Xi;j=1  ~K(xi; xj) (cid:0)

m

Xt=1

(cid:21)0tft(xi)ft(xj)!2

for m = 1; 2; : : : When we add a new point x, it is thus natural to use the same cost to
approximate the ~K(x; xi), which yields (13). Note that by doing so, we do not seek to
approximate ~K(x; x). Future work should investigate embeddings which minimize the
empirical reconstruction error of ~K but ignore the diagonal contributions.
4.1 Extending MDS
For MDS, a normalized kernel can be dened as follows, using a continuous version of the
double-centering eq. (2):

~K(a; b) = (cid:0)

1
2

(d2(a; b) (cid:0) Ex[d2(x; b)] (cid:0) Ex0 [d2(a; x0)] + Ex;x0 [d2(x; x0)])

(14)

where d(a; b) is the original distance and the expectations are taken over the empirical data
D. An extension of metric MDS to new points has already been proposed in (Gower, 1968),
solving exactly for the embedding of x to be consistent with its distances to training points,
which in general requires adding a new dimension.
4.2 Extending Spectral Clustering and Laplacian Eigenmaps
Both the version of Spectral Clustering and Laplacian Eigenmaps described above are
based on an initial kernel K, such as the Gaussian or nearest-neighbor kernel. An equiva-
lent normalized kernel is:

~K(a; b) =

1
n

K(a; b)

pEx[K(a; x)]Ex0 [K(b; x0)]

where the expectations are taken over the empirical data D.
4.3 Extending Isomap
To extend Isomap, the test point is not used in computing the geodesic distance between
training points, otherwise we would have to recompute all the geodesic distances. A rea-
sonable solution is to use the denition of ~D(a; b) in eq. (5), which only uses the training
points in the intermediate points on the path from a to b. We obtain a normalized kernel by
applying the continuous double-centering of eq. (14) with d = ~D.
A formula has already been proposed (de Silva and Tenenbaum, 2003) to approximate
Isomap using only a subset of the examples (the landmarkpoints) to compute the eigen-
vectors. Using our notations, this formula is

e0k(x) =

1

2p(cid:21)k Xi

vki(Ex0 [ ~D2(x0; xi)] (cid:0) ~D2(xi; x)):

(15)

where Ex0 is an average over the data set. The formula is applied to obtain an embedding
for the non-landmark examples.
Corollary 1

The embedding proposed in Proposition 1 for Isomap (ek(x)) is equal to formula 15 (Land-
mark Isomap) when ~K(x; y) is dened as in eq. (14) with d = ~D.

struction. Therefore (1; 1; : : : 1) is an eigenvector with eigenvalue 0, and all the other eigen-

Proof: the proof relies on a property of the Gram matrix for Isomap: Pi Mij = 0, by con-
vectors vk have the property Pi vki = 0 because of the orthogonality with (1; 1; : : : 1).
Writing (Ex0 [ ~D2(x0; xi)](cid:0) ~D2(x; xi)) = 2 ~K(x; xi)+Ex0;x00 [ ~D2(x0; x00)](cid:0)Ex0 [ ~D2(x; x0)]
2p(cid:21)k Pi vki ~K(x; xi) + (Ex0;x00 [ ~D2(x0; x00)] (cid:0) Ex0 [ ~D2(x; x0)])Pi vki =
yields e0k(x) = 2
ek(x), since the last sum is 0.

4.4 Extending LLE
The extension of LLE is the most challenging one because it does not t as well the frame-
work of Algorithm 1: the M matrix for LLE does not have a clear interpretation in terms
of distance or dot product. An extension has been proposed in (Saul and Roweis, 2002),
but unfortunately it cannot be cast directly into the framework of Proposition 1. Their
embedding of a new point x is given by

n

yk(x) =

yk(xi)w(x; xi)

(16)

Xi=1

where w(x; xi) is the weight of xi in the reconstruction of x by its k-nearest-neighbors in
the training set (if x = xj 2 D, w(x; xi) = (cid:14)ij). This is very close to eq. (11), but lacks the
normalization by (cid:21)k. However, we can see this embedding as a limit case of Proposition 1,
as shown below.
We rst need to dene a kernel ~K(cid:22) such that

~K(cid:22)(xi; xj) = ~M(cid:22);ij = ((cid:22) (cid:0) 1)(cid:14)ij + Wij + Wji (cid:0)Xk

WkiWkj

(17)

for xi; xj 2 D. Let us dene a kernel ~K0 by

~K0(xi; x) = ~K0(x; xi) = w(x; xi)

and ~K0(x; y) = 0 when neither x nor y is in the training set D. Let ~K00 be dened by

~K00(xi; xj) = Wij + Wji (cid:0)Xk

WkiWkj

and ~K00(x; y) = 0 when either x or y isnt in D. Then, by construction, the kernel ~K(cid:22) =
((cid:22) (cid:0) 1) ~K0 + ~K00 veries eq. (17). Thus, we can apply eq. (11) to obtain an embedding of
a new point x, which yields

y(cid:22);k(x) =

yik(cid:16)((cid:22) (cid:0) 1) ~K0(x; xi) + ~K00(x; xi)(cid:17)

1

(cid:21)k Xi
(cid:22) (cid:0) 1
(cid:22) (cid:0) ^(cid:21)k Xi

with (cid:21)k = ((cid:22) (cid:0) ^(cid:21)k), and ^(cid:21)k being the k-th lowest eigenvalue of M. This rewrites into

y(cid:22);k(x) =

yikw(x; xi) +

1

(cid:22) (cid:0) ^(cid:21)k Xi

yik ~K00(x; xi):

Then when (cid:22) ! 1, y(cid:22);k(x) ! yk(x) dened by eq. (16).
Since the choice of (cid:22) is free, we can thus consider eq. (16) as approximating the use of the
kernel ~K(cid:22) with a large (cid:22) in Proposition 1. This is what we have done in the experiments
described in the next section. Note however that we can nd smoother kernels ~K(cid:22) verifying
eq. (17), giving other extensions of LLE from Proposition 1. It is out of the scope of this
paper to study which kernel is best for generalization, but it seems desirable to use a smooth
kernel that would take into account not only the reconstruction of x by its neighbors xi, but
also the reconstruction of the xi by their neighbors including the new point x.
5 Experiments
We want to evaluate whether the precision of the generalizations suggested in the pre-
vious section is comparable to the intrinsic perturbations of the embedding algorithms.
The perturbation analysis will be achieved by considering splits of the data in three sets,
D = F [ R1 [ R2 and training either with F [ R1 or F [ R2, comparing the embeddings
on F . For each algorithm described in section 2, we apply the following procedure:

x 10  4

10

8

6

4

2

0

-2

-4

7

6

5

4

3

2

1

0

0

x 10  3

0.05

0.1

0.15

0.2

0.25

-1

-2

-3

0

x 10  4

20

15

10

5

0

-5

0

1

0.8

0.6

0.4

0.2

0

-0. 2

0.05

0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.02

0.04

0.06

0.08

0.1

0.12

0.14

-0. 4

0

0.05

0.1

0.15

0.2

0.25

Figure 1: Training set variability minus out-of-sample error, wrt the proportion of training
samples substituted. Top left: MDS. Top right: spectral clustering or Laplacian eigenmaps.
Bottom left: Isomap. Bottom right: LLE. Error bars are 95% condence intervals.

1. We choose F (cid:26) D with m = jFj samples. The remaining n(cid:0) m samples in D=F
are split into two equal size subsets R1 and R2. We train (obtain the eigenvectors)
over F [ R1 and F [ R2. When eigenvalues are close, the estimated eigenvectors
are unstable and can rotate in the subspace they span. Thus we estimate an afne
alignment between the two embeddings using the points in F , and we calculate
the Euclidean distance between the aligned embeddings obtained for each si 2 F .
2. For each sample si 2 F , we also train over fF [ R1g=fsig. We apply the exten-
sion to out-of-sample points to nd the predicted embedding of si and calculate
the Euclidean distance between this embedding and the one obtained when train-
ing with F [ R1, i.e. with si in the training set.
3. We calculate the mean difference (and its standard error, shown in the gure)
between the distance obtained in step 1 and the one obtained in step 2 for each
sample si 2 F , and we repeat this experiment for various sizes of F .

The results obtained for MDS, Isomap, spectral clustering and LLE are shown in gure 1
for different values of m. Experiments are done over a database of 698 synthetic face im-
ages described by 4096 components that is available at http://isomap.stanford.edu.
Qualitatively similar
such as
Ionosphere (http://www.ics.uci.edu/~mlearn/MLSummary.html) and swissroll
(http://www.cs.toronto.edu/~roweis/lle/). Each algorithm generates a two-
dimensional embedding of the images, following the experiments reported for Isomap.
The number of neighbors is 10 for Isomap and LLE, and a Gaussian kernel with a standard
deviation of 0.01 is used for spectral clustering / Laplacian eigenmaps. 95% condence

results have been obtained over other databases

intervals are drawn beside each mean difference of error on the gure.

As expected, the mean difference between the two distances is almost monotonically in-
creasing as the fraction of substituted examples grows (x-axis in the gure). In most cases,
the out-of-sample error is less than or comparable to the training set embedding stability:
it corresponds to substituting a fraction of between 1 and 4% of the training examples.
6 Conclusions
In this paper we have presented an extension to ve unsupervised learning algorithms
based on a spectral embedding of the data: MDS, spectral clustering, Laplacian eigen-
maps, Isomap and LLE. This extension allows one to apply a trained model to out-of-
sample points without having to recompute eigenvectors. It introduces a notion of function
induction and generalization error for these algorithms. The experiments on real high-
dimensional data show that the average distance between the out-of-sample and in-sample
embeddings is comparable or lower than the variation in in-sample embedding due to re-
placing a few points in the training set.
