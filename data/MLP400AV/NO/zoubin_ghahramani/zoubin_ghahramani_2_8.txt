Abstract

We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A
threshold model that generalizes the probit function is used as the likelihood function for ordinal
variables. Two inference techniques, based on the Laplace approximation and the expectation prop-
agation algorithm respectively, are derived for hyperparameter learning and model selection. We
compare these two Gaussian process approaches with a previous ordinal regression method based
on support vector machines on some benchmark and real-world data sets, including applications of
ordinal regression to collaborative ltering and gene expression analysis. Experimental results on
these data sets verify the usefulness of our approach.

Keywords: Gaussian processes, ordinal regression, approximate Bayesian inference, collaborative
ltering, gene expression analysis, feature selection

1. Introduction

Practical applications of supervised learning frequently involve situations exhibiting an order among
the different categories, e.g. a teacher always rates his/her students by giving grades on their overall
performance. In contrast to metric regression problems, the grades are usually discrete and nite.
These grades are also different from the class labels in classication problems due to the existence
of ranking information. For example, grade labels have the ordering F < D < C < B < A. This is
a learning task of predicting variables of ordinal scale, a setting bridging between metric regression
and classication referred to as ranking learning or ordinal regression.

There is some literature about ordinal regression in the domain of machine learning. Kramer
et al. (2001) investigated the use of a regression tree learner by mapping the ordinal variables into
numeric values. However there might be no principled way of devising an appropriate mapping
function. Frank and Hall (2001) converted an ordinal regression problem into nested binary clas-
sication problems that encode the ordering of the original ranks, and then the results of standard
binary classiers can be organized for prediction. Har-Peled et al. (2003) proposed a constraint
classication approach for ranking problems based on binary classiers. Cohen et al. (1999) con-
sidered general ranking problems in the form of preference judgements. Herbrich et al. (2000)
applied the principle of Structural Risk Minimization (Vapnik, 1995) to ordinal regression lead-
ing to a new distribution-independent learning algorithm based on a loss function between pairs of
ranks. Shashua and Levin (2003) generalized the formulation of support vector machines to or-

c(cid:13)2005 Wei Chu and Zoubin Ghahramani.

CHU AND GHAHRAMANI

dinal regression and the numerical results they presented shows a signicant improvement on the
performance compared with the on-line algorithm proposed by Crammer and Singer (2002).

In the statistics literature, most of the approaches are based on generalized linear models (Mc-
Cullagh and Nelder, 1983). The cumulative model (McCullagh, 1980) is well-known in classical
statistical approaches for ordinal regression, in which they rely on a specic distributional assump-
tion on the unobservable latent variables and a stochastic ordering of the input space. Johnson and
Albert (1999) described Bayesian inference on parametric models for ordinal data using sampling
techniques. Tutz (2003) presented a general framework for semiparametric models that extends
generalized additive models (Hastie and Tibshirani, 1990) by incorporating nonparametric parts.
The nonparametric components of the regression model are tted by maximizing penalized log
likelihood, and model selection is carried out using AIC.

Gaussian processes (OHagan, 1978; Neal, 1997) have provided a promising non-parametric
Bayesian approach to metric regression (Williams and Rasmussen, 1996) and classication prob-
lems (Williams and Barber, 1998). The important advantage of Gaussian process models (GPs) over
other non-Bayesian models is the explicit probabilistic formulation. This not only provides prob-
abilistic predictions but also gives the ability to infer model parameters such as those that control
the kernel shape and the noise level. The GPs are also different from the semiparametric approach
of Tutz (2003) in several ways. First, the additive models (Fahrmeir and Tutz, 2001) are dened by
functions in each input dimension, whereas the GPs can have more general non-additive covariance
functions; second, the kernel trick allows to use innite basis function expansions; third, the GPs
perform Bayesian inference in the space of the latent functions.

In this paper, we present a probabilistic kernel approach to ordinal regression in Gaussian pro-
cesses. We impose a Gaussian process prior distribution on the latent functions, and employ an
appropriate likelihood function for ordinal variables which can be regarded as a generalization of
the probit function. Two Bayesian inference techniques are applied to implement model adapta-
tion by using the Laplace approximation (MacKay, 1992) and the expectation propagation (Minka,
2001) respectively. Comparisons of the generalization performance against the support vector ap-
proach (Shashua and Levin, 2003) on some benchmark and real-world data sets, such as movie
ranking and gene expression analysis, verify the usefulness of this approach.

The paper is organized as follows: in Section 2, we describe the Bayesian framework in Gaus-
sian processes for ordinal regression; in Section 3, we discuss the Bayesian techniques for hyperpa-
rameter inference; in Section 4, we present the predictive distribution for probabilistic prediction; in
Section 5, we give some extensive discussion on these techniques; in Section 6, we report the results
of numerical experiments on some benchmark and real-world data sets; we conclude this paper in
Section 7.

2. Bayesian Framework
Consider a data set composed of n samples. Each of the samples is a pair of input vector xi  R d
and the corresponding target yi  Y where Y is a nite set of r ordered categories. Without loss
of generality, these categories are denoted as consecutive integers Y = {1,2, . . . , r} that keep the
known ordering information. The main idea is to assume an unobservable latent function f (xi)  R
associated with xi in a Gaussian process, and the ordinal variable yi dependent on the latent function
f (xi) by modelling the ranks as intervals on the real line. A Bayesian framework is described with
more details in the following.

1020

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

2.1 Gaussian Process Prior
The latent functions { f (xi)} are usually assumed as the realizations of random variables indexed
by their input vectors in a zero-mean Gaussian process. The Gaussian process can then be fully
specied by giving the covariance matrix for any nite set of zero-mean random variables { f (xi)}.
The covariance between the functions corresponding to the inputs xi and x j can be dened by Mercer
kernel functions (Wahba, 1990; Scholkopf and Smola, 2001), e.g. Gaussian kernel which is dened
as

Cov[ f (xi), f (x j)] = K (xi, x j) = exp 
i denotes the V -th element of xi.1 Thus, the prior probability of these latent

j)2!

(x
i  x

d(cid:229)
V =1

(1)

2

where k > 0 and x
functions { f (xi)} is a multivariate Gaussian
1
Z f

P ( f ) =

where f = [ f (x1), f (x2), . . . , f (xn)]T , Z f = (2p )
i j-th element is dened as in (1).

exp(cid:18)
2|S
|

n

1
2

f T S 1 f(cid:19)
2 , and S

1

is the n n covariance matrix whose

(2)

(3)

(4)

2.2 Likelihood for Ordinal Variables

The likelihood is the joint probability of observing the ordinal variables given the latent functions,
denoted as P (D| f ) where D denotes the target set {yi}. Generally, the likelihood can be evaluated
as a product of the likelihood function on individual observation:

P (D| f ) =

n(cid:213)

i=1

P (yi| f (xi))

where the likelihood function P (yi| f (xi)) could be intuitively dened as
if byi1 < f (xi)  byi,

Pideal(yi| f (xi)) =(cid:26) 1

0

otherwise

j
i =2

and br = +

i with positive padding variables D

where b0 = 
are dened subsidiarily, b1  R and the other threshold variables can
i and i = 2, . . . , r 1. The
be further dened as b j = b1 + (cid:229)
role of b1 < b2 < . . . < br1 is to divide the real line into r contiguous intervals; these intervals map
the real function value f (xi) into the discrete variable yi while enforcing the ordinal constraints.
The likelihood function (4) is used for ideally noise-free cases.
In the presence of noise from
inputs or targets, we may explicitly assume that the latent functions are contaminated by a Gaussian
noise with zero mean and unknown variance s 2.2 N (d ; ,s 2) is used to denote a Gaussian random
variable d with mean  and variance s 2 henceforth. Then the ordinal likelihood function becomes

P (yi| f (xi)) = Z Pideal(yi| f (xi) + d

i)N (d

i;0,s 2)dd

i = F (cid:0)zi

1(cid:1) F (cid:0)zi
2(cid:1)

(5)

1. Other Mercer kernel functions, such as polynomial kernels and spline kernels etc., can also be used in the covariance

function.

2. In principle, any distribution rather than a Gaussian can be assumed for the noise on the latent functions.

1021

k
V
V
V
D
CHU AND GHAHRAMANI

Ordinal Likelihood Function P(y|f(x))

d -ln P(y|f(x)) / df(x)

d2 -ln P(y|f(x)) / d 2f(x)

1

0.8

0.6

0.4

0.2

0
-9

y=1

y=2

y=3

-6

b
=-3
1

0
f(x)

=3
b

2

6

9

15

10

5

0

-5

-10

-15

-9

y=1

y=2

y=3

-6

b
=-3
1

0
f(x)

b
=3
2

6

9

1

0.8

0.6

0.4

0.2

0
-9

y=1

y=2

y=3

-6

b
=-3
1

0
f(x)

b
=3
2

6

9

Figure 1: The graph of the likelihood function for an ordinal regression problem with r = 3, along
with the rst and second order derivatives of the loss function (negative logarithm of the
likelihood function), where the noise variance s 2 = 1, and the two thresholds are b1 = 3
and b2 = +3.

, zi

1 =

2 =

byi f (xi)

byi1 f (xi)

, and F (z) = R z

 N (V ;0,1)dV . Note that binary classication
where zi
is a special case of ordinal regression when r = 2, and in this case the likelihood function (5) be-
comes the probit function. The quantity ln P (yi| f (xi)) is usually referred to as the loss function
`(yi, f (xi)). The derivatives of the loss function with respect to f (xi) are needed in some approx-
imate Bayesian inference methods. The rst order derivative of the loss function can be written
as

We present graphs of the ordinal likelihood function (5) and the derivatives of the loss function
in Figure 1 as an illustration. Note that the rst order derivative (6) is a monotonically increasing
function of f (xi), and the second order derivative (7) is always a positive value between 0 and 1
s 2 .
i;0,s 2) is also log-
Given the facts that Pideal(yi| f (xi) + d
concave, as pointed out by Pratt (1981), the convexity of the loss function follows, because the
integral of a log-concave function with respect to some of its arguments is a log-concave function
of its remaining arguments (Brascamp and Lieb, 1976, Cor. 3.5).

i) is log-concave in ( f (xi),d

i) and N (d

2.3 Posterior Probability

Based on Bayes theorem, the posterior probability can then be written as

P ( f|D) =

1

P (D)

n(cid:213)

i=1

P (yi| f (xi)) P ( f )

(8)

where the prior probability P ( f ) is dened as in (2), the likelihood function P (yi| f (xi)) is dened
as in (5), and P (D) = R P (D| f )P ( f )d f .

1022

 `(yi, f (xi))

f (xi)

1

N (zi

=

1;0,1) N (zi
2;0,1)
F (zi
1) F (zi
2)

and the second order derivative can be given as
1;0,1) N (zi
2;0,1)
F (zi
1) F (zi
2)

s 2(cid:18) N (zi

 2`(yi, f (xi))

 2 f (xi)

=

1

2

(cid:19)

zi
1N (zi

1
s 2

+

1;0,1) zi
F (zi

2N (zi
1) F (zi
2)

2;0,1)

(6)

.

(7)

s
s

s
GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

The Bayesian framework we described above is conditional on the model parameters including
in the covariance function (1) that control the kernel shape, the threshold
in the likelihood function (5). All these param-
, which is the hyperparameter vector. The normalization factor P (D)
, a yardstick for model selection. In the

the kernel parameters k
parameters {b1,D 2, . . . ,D
eters can be collected into q
in (8), more exactly P (D|q ), is known as the evidence for q

r1} and the noise level s

next section, we discuss techniques for hyperparameter learning.

3. Model Adaptation
In a full Bayesian treatment, the hyperparameters q must be integrated over the q -space. Monte
Carlo methods (Neal, 1997) can be adopted here to approximate the integral effectively. However
these might be prohibitively expensive to use in practice. Alternatively, we consider model se-
lection by determining an optimal setting for q
. The optimal values of hyperparameters q can be
simply inferred by maximizing the posterior probability P (q |D), where P (q |D) (cid:181) P (D|q )P (q ).
The prior distribution on the hyperparameters P (q ) can be specied by domain knowledge, or al-
ternatively some vague uninformative distribution. The evidence is given by a high dimensional
integral, P (D|q ) = R P (D| f )P ( f ) d f . A popular idea for computing the evidence is to approxi-
mate the posterior distribution P ( f|D) as a Gaussian, and then the evidence can be calculated by an
explicit formula (MacKay, 1992; Csato et al., 2000; Minka, 2001). In this section, we describe two
Bayesian techniques for model adaptation by using the Laplace approximation and the expectation
propagation respectively.

3.1 MAP Approach with Laplace Approximation

The evidence can be calculated analytically after applying the Laplace approximation at the max-
imum a posteriori (MAP) estimate, and gradient-based optimization methods can then be used to
infer the optimal hyperparameters by maximizing the evidence. The MAP estimate on the latent
functions is referred to f MAP = argmax f P ( f|D), which is equivalent to the minimizer of negative
logarithm of P ( f|D), i.e.

S ( f ) =

`(yi, f (xi)) +

n(cid:229)

i=1

f T S 1 f

1
2

(9)

(10)

where `(yi, f (xi)) = ln P (yi| f (xi)) is known as the loss function. Note that
positive denite matrix, where L
given as in (7).
Thus, this is a convex programming problem with a unique solution.3 The Laplace approximation
of S ( f ) refers to carrying out the Taylor expansion at the MAP point and retaining the terms up
to the second order (MacKay, 1992). Since the rst order derivative with respect to f vanishes at
f MAP, S ( f ) can also be written as

is a diagonal matrix whose ii-th entry is

 2`(yi, f (xi))

 2 f (xi)

is a

 2S ( f )
f 

f T = S 1 + L

S ( f )  S ( f MAP) +

1
2

( f  f MAP)T(cid:0)S 1 + L MAP(cid:1) ( f  f MAP)

where L MAP denotes the matrix L
at the MAP estimate. This is equivalent to approximating the pos-
terior distribution P ( f|D) as a Gaussian distribution centered on f MAP with the covariance matrix

3. The Newton-Raphson formula can be used to nd the solution for simple cases.

1023


CHU AND GHAHRAMANI

(S 1 + L MAP)1, i.e. P ( f|D)  N ( f ; f MAP, (S 1 + L MAP)1). Using the Laplace approximation
(10) and Z f dened as in (2), the evidence can be computed analytically as follows

P (D|q ) =

1

Z f Z exp(S ( f )) d f  exp(S ( f MAP))|I + S

L MAP| 1

2

(11)

where I is an n n identity matrix. The gradients of the logarithm of the evidence (11) with respect
to the hyperparameters q can be derived analytically. Then gradient-based optimization methods
can be employed to search for the maximizer of the evidence. Refer to Appendix A for the detailed
gradient formulae and the outline of our algorithm for model adaptation.

3.2 Expectation Propagation with Variational Methods

The expectation propagation algorithm (EP) is an approximate Bayesian inference method (Minka,
2001), which can be regarded as an extension of assumed-density-lter (ADF). The EP algorithm
has been applied in Gaussian process classication along with variational methods for model selec-
tion (Seeger, 2002; Kim and Ghahramani, 2003). In the setting of Gaussian processes, EP attempts
to approximate P ( f|D) as a product distribution in the form of Q( f ) = (cid:213)
n
i=1 ti( f (xi))P ( f ) where
ti( f (xi)) = si exp( 1
2 pi( f (xi) mi)2). The parameters {si, mi, pi} in {ti} are successively optimized
by minimizing the following Kullback-Leibler divergence,

tnew
i = argmin
ti

KL(cid:18)Q( f )

told
i

Q( f )
told
i

ti(cid:19) .

(12)

P (yi| f (xi))(cid:13)(cid:13)(cid:13)(cid:13)

Since Q( f ) is in the exponential family, this minimization can be simply solved by moment match-
ing up to the second order. A detailed updating scheme can be found in Appendix B. At the
equilibrium of Q( f ), we obtain an approximate posterior distribution as P ( f|D)  N ( f ; (S 1 +
P )1P m, (S 1 +P )1) where P
is a diagonal matrix whose ii-th entry is pi and m = [m1, m2, . . . , mn]T .
Variational methods can be used to optimize the hyperparameters q by maximizing the lower

bound on the logarithm of the evidence. By applying Jensens inequality, we have

log P (D|q ) = logR P (D| f )P ( f )
= R Q( f )log P (D| f )d f +R Q( f )log P ( f )d f R Q( f )logQ( f )d f = F (q ).

Q( f ) Q( f )d f  R Q( f )log P (D| f )P ( f )

Q( f )

d f

(13)

The lower bound F (q ) can be written as an explicit expression at the equilibrium of Q( f ), and then
the gradients with respect to q can be derived by neglecting the possible dependency of Q( f ) on q
.
The detailed formulation can be found in Appendix C.

4. Prediction

We have described two techniques, the MAP approach and the EP approach, to infer the optimal
model. At the optimal hyperparameters we inferred, denoted as q , let us take a test case x for
which the target yx is unknown. The latent variable f (x) and the column vector f containing the n
zero-mean random variables { f (xi)}n
i=1 have the prior joint multivariate Gaussian distribution, i.e.

f

f (x) (cid:21)  N (cid:20)(cid:18) 0

0 (cid:19) ,(cid:18) S

k

kT K (x, x) (cid:19)(cid:21)

(cid:20)

1024

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

where k = [K (x, x1), K (x, x2), . . . , K (x, xn)]T . The conditional distribution of f (x) given f is a
Gaussian too, denoted as P ( f (x)| f ,q ) with mean f T S 1k and variance K (x, x) kT S 1k. The
predictive distribution of P ( f (x)|D,q ) can be computed as an integral over f -space, which can be

written as

P ( f (x)|D,q ) = Z P ( f (x)| f ,q )P ( f |D,q ) d f .

(14)

The posterior distribution P ( f|D,q ) can be approximated as a Gaussian by the MAP approach or
the EP approach (refer to Section 3). The predictive distribution (14) can then be simplied as a
Gaussian N ( f (x); x,s 2

x) with mean x and variance s 2

x. In the MAP approach, we reach

x = kT S 1 f MAP

and

x = K (x, x) kT (S + L 1
s 2

MAP)1k.

While in the EP approach, we get

x = kT (S + P 1)1m and

x = K (x, x) kT (S + P 1)1k.
s 2

The predictive distribution over ordinal targets yx is

(15)

(16)

P (yx|x, D,q ) = R P (yx| f (x),q )P ( f (x)|D,q ) d f (x)

The predictive ordinal scale can be decided as argmax

= F (cid:18) byxxs 2+s 2

x(cid:19) F (cid:18) byx1x
x(cid:19) .
s 2+s 2
P (yx = i|x, D,q ).

i

5. Discussion

In the MAP approach, the mean of the predictive distribution depends on the MAP estimate f MAP,
which is unique and can be found by solving a convex programming problem. Evidence maximiza-
tion is useful if the Laplace approximation around the mode point f MAP gives a good summary of
the posterior distribution P ( f|D). While in the approach of expectation propagation, the mean of
the predictive distribution depends on the approximate mean of the posterior distribution. When
the true shape of P ( f|D) is far from a Gaussian centered on the mode, the EP approach can have
a great advantage over the Laplace approximation. However the EP algorithm cannot guarantee
convergence, though it usually works well in practice.

, the inversion of the matrix S

The gradient-based optimization method usually requests evidence evaluation at tens of different
settings of q before the minimum is found. For each q
is required that
costs time at O(n3), where n is the number of training samples. Recently, Csato and Opper (2002)
proposed a fast training algorithm for Gaussian processes in which the set of basis vectors are
determined on-line for sparse representation. Lawrence et al. (2003) proposed a greedy selection
with criteria based on information-theoretic principles for sparse Gaussian processes (Seeger, 2003).
Tresp (2000) proposed the Bayesian committee machines to divide and conquer large data sets,
while using innite mixtures of Gaussian Processes (Rasmussen and Ghahramani, 2002) is another
promising technique. These algorithms can be applied directly in the settings of ordinal regression
for speedup.

Feature selection is an essential part in modelling. In Gaussian processes, the automatic rele-
vance determination (ARD) method proposed by MacKay (1994) and Neal (1996) can be embedded

1025

CHU AND GHAHRAMANI

into the covariance function (1) as follows:

Cov[ f (xi), f (x j)] = K (xi, x j) = exp 

1
2

d(cid:229)
V =1

V (x

i  x

j)2!

(17)

where k
V > 0 is the ARD parameter.4 The gradients with respect to the variables {lnk
V } can also
be derived analytically for model adaptation. The optimal value of the ARD parameter k
indicates
the relevance of the V -th input feature to the target. The form of feature selection we use here
results in a type of feature weighting. Furthermore, the linear combination of heterogeneous kernels
with positive coefcients is still a valid covariance function. Lanckriet et al. (2004) suggest to
learn the kernel matrix with semidenite programming. In the Bayesian framework, these positive
coefcients for kernels could be treated as hyperparameters, and optimized using the evidence as a
criterion for optimization.

Note that binary classication is a special case of ordinal regression with r = 2, and the like-
lihood function (5) becomes the probit function when r = 2. Both of the probit function and the
logistic function can be used as the likelihood function in binary classication, while they have
different origins. Due to the dichotomous nature in the classes of multi-classication, discriminant
functions are constructed for each class and then compete again others via the softmax function to
determine the likelihood. The logistic function, as a special case of the softmax function, comes
from general classication problems.

In metric regression, warped Gaussian processes (Snelson et al., 2004) assume that there is
a nonlinear, monotonic, and continuous warping function relating the observed targets and some
latent variables in a Gaussian process. The warping function, which is learned from the data, can be
thought of as a pre-processing transformation applied before modelling with a Gaussian process. A
different (and very common) approach to dealing with this preprocessing is to discretize the target
values into r different bins. These discrete values are clearly ordinal, and applying ordinal regression
to these discrete values seems the natural choice. Interestingly, as the number of discretization bins
r is increased, the ordinal regression model becomes very similar to the warped Gaussian processes
model. In particular, by varying the thresholds in our ordinal regression model, it can approximate
any continuous warping function.

6. Numerical Experiments

We start this section with a simple synthetic data set to visualize the behavior of these algorithms,
and report the experimental results on sixteen benchmark data sets.5 Then we perform experiments
on a collaborative ltering problem using the EachMovie data, and on Gleason score prediction
from gene microarray data related to prostate cancer. Shashua and Levin (2003) generalized the sup-
port vector formulation by nding multiple thresholds to dene parallel discriminant hyperplanes
for ordinal scales, and reported that the performance of the support vector approach is better than
that of the on-line algorithm (Crammer and Singer, 2002). The problem size in the large-margin
ranking algorithm of Herbrich et al. (2000) is a quadratic function of the training data size making
the algorithmic complexity O(n4)O(n6). This makes the experiments on large data sets computa-
tionally difcult. Thus, we decide to limit our comparisons to the support vector approach (SVM)

4. These ARD parameters control the covariance length-scale of the Gaussian process along each input dimension.
5. These data sets are publicly available at http://www.liacc.up.pt/ltorgo/Regression/DataSets.html.

1026

k
V
V
V
GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

of Shashua and Levin (2003) and the two versions of our approach, the MAP approach with Laplace
approximation (MAP) and the EP algorithm with variational methods (EP). In our implementation,6
we used the routine L-BFGS-B (Byrd et al., 1995) as the gradient-based optimization package, and
started from the initial values of hyperparameters to infer the optimal values in the criterion of the
approximate evidence (11) for MAP or the variational lower bound (13) for EP respectively.7 The
improved SMO algorithm (Keerthi et al., 2001) was adapted to implement the SVM approach (refer
to Chu and Keerthi (2005) for detailed description and extensive discussion),8 and 5-fold cross vali-
dation was used to determine the optimal values of model parameters (the kernel parameter k and the
regularization factor C) involved in the problem formulations. The initial search was done on a 77
k  3},
coarse grid linearly spaced in the region {(log10C,log10
k )
followed by a ne search on a 9  9 uniform grid linearly spaced by 0.2 in the (log10C,log10
space. We have utilized two evaluation metrics which quantify the accuracy of predictive ordinal
scales { y1, . . . , yt} with respect to true targets {y1, . . . , yt}:

k )| 3  log10C  3,3  log10

 Mean absolute error is the average deviation of the prediction from the true target, i.e.

i=1| yi  yi|, in which we treat the ordinal scales as consecutive integers;

t

1
t

 Mean zero-one error gives an error of 1 to every incorrect prediction that is the fraction of

incorrect predictions.

6.1 Articial Data

Figure 2 presents the behavior of the three algorithms using the Gaussian kernel (1) on a synthetic
2D data with three ordinal scales.
In the support vector approach, the optimal thresholds were
determined by the SMO algorithm and 5-fold cross validation was used to decide the optimal values
of the kernel parameter and the regularization factor. As for the Gaussian process algorithms, model
adaptation (see Section 3) was used to determine the optimal values of the kernel parameter, the
noise level and the thresholds automatically. The gure shows that all the algorithms are working
reasonably well on this task.

6.2 Benchmark Data

We collected nine benchmark data sets (Set I in Table 1) that were used for metric regression prob-
lems. The target values were discretized into ordinal quantities using equal-length binning. These
bins divide the range of target values into a given number of intervals that are of same length. The
resulting rank values are ordered, representing these intervals of the original metric quantities. For
each data set, we generated two versions by discretizing the target values into ve and ten intervals
respectively. We randomly partitioned each data set into training/test splits as specied in Table 1.
The partition was repeated 20 times independently. The Gaussian kernel (1) was used in these three
algorithms. The test results are recorded in Tables 2 and 3. The performance of the MAP and EP
approaches are closely matching. Our Gaussian process algorithms often yield better results than

6. The two versions of our proposed approach were implemented in ANSI C, and the source code is accessible at

http://www.gatsby.ucl.ac.uk/chuwei/code/gpor.tar.
7. In numerical experiments, the initial values of the hyperparameters were usually chosen as s 2 = 1, k = 1/d for
Gaussian kernel, the threshold b1 = 1 and D
i = 2/r. We suggest to try several starting points in practice, and then
8. The source code in ANSI C is available at http://www.gatsby.ucl.ac.uk/chuwei/code/svorim.tar.

choose the best model by the objective functional.

1027

(cid:229)
CHU AND GHAHRAMANI

The SVM Approach

The MAP Approach

The EP Approach

40.37

26.79

1

2

3

4

The SVM Approach

27.83

22.96

1

2

3

4

3

2

1

0

1

2

3
0

3

2

1

0

1

2

3
0

0.46

1.01

1

2

3

4

The MAP Approach

0.44

1.56

1

2

3

4

3

2

1

0

1

2

3
0

3

2

1

0

1

2

3
0

0.96

2.33

1

2

3

4

The EP Approach

1.26

3.22

1

2

3

4

i

e
s
o
N


r
e
w
o
L

i

e
s
o
N


r
e
h
g
H

i

3

2

1

0

1

2

3
0

3

2

1

0

1

2

3
0

Figure 2: The performance of the three algorithms on a synthetic three-rank ordinal regression
problem. The discriminant function values of the SVM approach, and the predictive
mean values of the two Gaussian process approaches are presented as contour graphs in-
dexed by the two thresholds. The upper graphs are for the case of lower noise level, while
the lower graphs are for the case of higher noise level. The training samples we used are
presented in these graphs. The dots denote the training samples of rank 1, the crosses
denote the training samples of rank 2 and the circles denote the training samples of rank
3.

the support vector approach on the average value, especially when the number of training samples
is small.

In the next experiment, we selected seven very large metric regression data sets (Set II in Table
1). The input vectors were normalized to zero mean and unit variance coordinate-wise. The target
values of these data sets were discretized into 10 ordinal quantities using equal-frequency binning.
For each data set, a small subset was randomly selected for training and then tested on the remaining
samples, as specied in Table 1. The partition was repeated 100 times independently. To show the
advantage of explicitly modelling the ordinal nature of the targets, we also employed the standard
Gaussian process algorithm (Williams and Rasmussen, 1996) for metric regression (GPR)9 to tackle
these ordinal regression tasks, where the ordinal targets were naively treated as continuous values
and the predictions for test cases were rounded to the nearest ordinal scale. The Gaussian kernel
(1) was used in the four algorithms. From the test results in Table 4, the ordinal regression algo-

9. In the GPR, the type-II maximum likelihood was used for model selection.

1028

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

Data Sets
Diabetes
Pyrimidines
Triazines
Wisconsin Breast Cancer
Machine CPU
Auto MPG
Boston Housing
Stocks Domain
Abalone
Bank Domains(1)
Bank Domains(2)
Computer Activity(1)
Computer Activity(2)
California Housing
Census Domains(1)
Census Domains(2)

Set I

Set II

Attributes(Numeric,Nominal)

Training Instances

Instances for Test

2(2,0)
27(27,0)
60(60,0)
32(32,0)
6(6,0)
7(4,3)
13(12,1)
9(9,0)
8(7,1)
8(8,0)
32(32,0)
12(12,0)
21(21,0)
8(8,0)
8(8,0)
16(16,0)

30
50
100
130
150
200
300
600
1000
50
75
100
125
150
175
200

13
24
86
64
59
192
206
350
3177
8142
8117
8092
8067
15490
16609
16584

Table 1: Data sets and their characteristics. Attributes state the number of numerical and nominal
attributes. Training Instances and Instances for Test specify the size of training/test
partition. The partitions we generated and the test results on individual partitions can be
accessed at http://www.gatsby.ucl.ac.uk/chuwei/ordinalregression.html.

Mean zero-one error

Mean absolute error

SVM

MAP

EP

SVM

Data
57.3112.09% 54.2313.78% 54.2313.78% 0.74620.1414 0.66150.1376 0.66540.1373
Diabetes
41.468.49% 39.797.21% 36.466.47% 0.45000.1136 0.42710.0906 0.39170.0745
Pyrimidines
54.191.48% 52.912.15% 52.622.66% 0.69770.0259 0.68720.0229 0.68780.0295
Triazines
?70.783.73% 65.004.71% 65.164.65% 1.00310.0727 1.01020.0937 1.01410.0932
Wisconsin
17.373.56% 16.533.56% 16.783.88% 0.19150.0423 0.18470.0404 0.18560.0424
Machine
Auto MPG ?25.732.24% 23.781.85% 23.751.74% 0.25960.0230 0.24110.0189 0.24110.0186
25.561.98% 24.882.02% 24.491.85% 0.26720.0190 0.26040.0206 0.25850.0200
Boston
10.811.70% 11.992.34% 12.002.06% 0.10810.0170 0.11990.0234 0.12000.0206
Stocks
21.580.32% 21.500.22% 21.560.36% 0.22930.0038 0.23220.0025 ?0.23370.0072
Abalone

MAP

EP

Table 2: Test results of the three algorithms using a Gaussian kernel. The targets of these bench-
mark data sets were discretized by 5 equal-length bins. The results are the averages over
20 trials, along with the standard deviation. We use the bold face to indicate the cases in
which the average value is the lowest in the results of the three algorithms. The symbols
? are used to indicate the cases in which the indicated entry is signicantly worse than the
winning entry; A p-value threshold of 0.01 in Wilcoxon rank sum test was used to decide
statistical signicance.

rithms are clearly superior to the naive approach of applying standard metric regression. We also
observed that the performance of Gaussian process algorithms are signicantly better than that of
the support vector approach on six of the seven data sets. This veries our judgement in the previous
experiment that our Gaussian process algorithms yield better performance than the support vector
approach on small data sets. Although the EP approach often yields better results of mean zero-one
error than the MAP approach on these tasks, we have not detected any statistically signicant dif-
ference on their performance. In Table 4 we also report their negative logarithm of the likelihood in
prediction (NLL). The performance of the MAP and EP approaches are closely matching too with
no statistically signicant difference.

1029

CHU AND GHAHRAMANI

Mean zero-one error

Mean absolute error

EP

SVM

SVM

MAP

Data
?90.387.00% 83.465.73% 83.085.91% 2.45770.4369 2.13850.3317
Diabetes
2.14230.3314
Pyrimidines 59.377.63% 55.428.01% 54.387.70% 0.91870.1895 0.87710.1749
0.82920.1338
?67.913.63% 63.724.34% 64.013.78% 1.23080.0874 1.19940.0671
Triazines
1.20120.0680
?85.863.78% 78.523.58% 78.523.51% 2.12500.1500 2.13910.1797
2.14370.1790
Wisconsin
32.633.84% 33.813.91% 33.733.64% 0.43980.0688 0.47460.0727
0.46860.0763
Machine
Auto MPG 44.012.30% 43.962.81% 43.882.60% 0.50810.0263 0.49900.0352
0.49790.0340
42.062.49% 41.532.77% 41.262.86% 0.49710.0305 0.49200.0330
0.48960.0346
Boston
17.742.15% ?19.901.72% ?19.441.91% 0.18040.0213 ?0.20060.0166 ?0.19600.0184
Stocks
0.51130.0053
42.840.86% 42.600.91% 42.270.46% 0.51600.0087 0.51400.0075
Abalone

MAP

EP

Table 3: Test results of the three algorithms using a Gaussian kernel. The targets of these bench-
mark data sets were discretized by 10 equal-length bins. The results are the averages over
20 trials, along with the standard deviation. We use the bold face to indicate the cases in
which the average value is the lowest in the results of the three algorithms. The symbols
? are used to indicate the cases in which the indicated entry is signicantly worse than the
winning entry; A p-value threshold of 0.01 in Wilcoxon rank sum test was used to decide
statistical signicance.

Mean zero-one error
SVM

NLL

GPR

MAP

Data
?59.43  2.80 % 49.07  2.69 % 48.65  1.93 % 48.35  1.91 % 1.14  0.07 1.14  0.07
Bank(1)
?86.37  1.49 % ?82.26  2.06 % 80.96  1.51 % 80.89  1.52 % 2.20  0.09 2.20  0.09
Bank(2)
CompAct(1) ?65.52  2.31 % ?59.87  2.25 % 58.52  1.73 % 58.51  1.53 % 1.65  0.16 1.64  0.14
CompAct(2) ?59.30  2.27 % ?54.79  2.10 % 53.80  1.84 % 53.92  1.68 % 1.49  0.11 1.48  0.09
?76.13  1.27 % ?70.63  1.40 % 69.60  1.12 % 69.58  1.11 % 1.89  0.08 1.89  0.09
California
?78.06  0.81 % ?74.69  0.94 % 73.71  0.77 % 73.71  0.77 % 2.04  0.08 2.05  0.08
Census(1)
?78.02  0.85 % ?76.01  1.03 % 74.53  0.81 % 74.48  0.84 % 2.03  0.06 2.03  0.07
Census(2)

MAP

EP

EP

Table 4: Test results of the four algorithms using a Gaussian kernel. The targets of these bench-
mark data sets were discretized by 10 equal-frequency bins. The results are the average
over 100 trials, along with the standard deviation. GPR denotes the standard algorithm
of Gaussian process metric regression that treats the ordinal scales as continuous values.
NLL denotes the negative logarithm of the likelihood in prediction. We use the bold face
to indicate the cases in which the average value is the lowest mean zero-one error of the
four algorithms. The symbols ? are used to indicate the cases in which the indicated entry
is signicantly worse than the winning entry; A p-value threshold of 0.01 in Wilcoxon
rank sum test was used to decide statistical signicance.

For these data sets, the overall training time of MAP and EP approaches was substantially less
than that of the SVM approach. This is because the MAP and EP approaches can tune the model
parameters by gradient descent that usually required evidence evaluations at tens of different settings
of q
, whereas k-fold cross validation for the SVM approach required evaluations at 130 different
nodes of q on the grid for every fold. For larger data sets, the SVM approach may still have an
advantage on training time due to the sparseness property in its computation.

1030

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

6.3 Collaborative Filtering

Collaborative ltering exploits correlations between ratings across a population of users. The goal
is to predict a persons rating on new items given the persons past ratings on similar items and the
ratings of other people on all the items (including the new item). The ratings are ordered, making
collaborative ltering an ordinal regression problem. We carried out ordinal regression on a subset
of the EachMovie data (Compaq, 2001).10 The rates given by the user with ID number 52647
on 449 movies were used as the targets, in which the numbers of zero-to-ve star are 40, 20, 57,
113, 145 and 74 respectively. We selected 1500 users who contributed the most ratings on these
449 movies as the input features. The ratings given by the 1500 users on each movie were used as
the input vector accordingly. In the 449 1500 input matrix, about 40% elements were observed.
We randomly selected a subset with size {50,100, . . . ,300} of the 449 movies for training, and
then tested on the remaining movies. At each size, the random selection was carried out 20 times
independently.

Pearson correlation coefcient is the most popular correlation measure (Basilico and Hofmann,
2004), which corresponds to a dot product between normalized rating vectors. For instance, if
applied to the movies, we can dene the so-called z-scores as
r(v, u) (v)

z(v, u) =

s (v)

where u indexes users, v indexes movies, and r(v, u) is the rating on the movie v given by the user
u. (v) and s (v) are the movie-specic mean and standard deviation respectively. This correlation
coefcient, dened as

K (v, v0) = (cid:229)

z(v, u)z(v0, u)

u

where (cid:229) u denotes summing over all the users, was used as the covariance/kernel function in our
experiments for the three algorithms. As not all ratings are observed in the input vectors, we con-
sider two ad hoc strategies to deal with missing values: mean imputation and weighted low-rank
approximation. In the rst case, unobserved values are identied with the mean value, that means
their corresponding z-score is zero. In the second case, we applied the EM procedure described
by Srebro and Jaakkola (2003) to ll in the missing data with the estimate. In the input matrix,
observed elements were weighted by one and missing data were given weight zero. The low rank
was xed at 2. In Figure 3, we present the test results of the two cases at different training data
size. Using mean imputation, SVM produced a bit more accurate results than Gaussian processes
on mean absolute error. In the cases with low rank approximation as preprocessing, the performance
of the three algorithms are highly competitive, and more interestingly, we observed about 0.08 im-
provement on mean absolute error for all the three algorithms. A serious treatment on the missing
data could be an interesting research topic for future work.

6.4 Gene Expression Analysis

Singh et al. (2002) carried out microarray expression analysis on 12600 genes to identify genes
that might anticipate the clinical behavior of prostate cancer. Fifty-two samples of prostate tumor
were investigated. For each sample, the Gleason score ranging from 6 to 10, was given by the

10. The Compaq System Research Center ran the EachMovie service for 18 months. 72916 users entered a total of

2811983 numeric ratings on 1628 movies, i.e. about 2.4% are rated by zero-to-ve star.

1031

CHU AND GHAHRAMANI

with Mean Imputation

with Weighted Lowrank Approximation

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.7

0.65

0.6

0.55

0.5

l

r
o
r
r
e

e
t
u
o
s
b
a

n
a
e
M

r
o
r
r
e



e
n
o

o
r
e
z


n
a
e
M

50

100

50

100

150

200

Training data size

150

200

Training data size

250

300

50

100

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.7

0.65

0.6

0.55

0.5

250

300

50

100

150

200

Training data size

150

200

Training data size

250

300

250

300

Figure 3: The test results of the three algorithms on the subset of EachMovie data over 20 trials.
The grouped boxes represent the results of SVM (left), MAP (middle) and EP (right)
respectively at different training data size. The notched-boxes have lines at the lower
quartile, median, and upper quartile values. The whiskers are lines extending from each
end of the box to the most extreme data value within 1.5IQR(Interquartile Range) of the
box. Outliers are data with values beyond the ends of the whiskers, which are displayed by
dots. The higher graphs are for the results of mean absolute error and the lower graphs are
for mean zero-one error. The cases of mean imputation are presented in the left graphs,
and the cases with weighted low-rank approximation as preprocessing are presented in
the right graphs.

1032

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

The SVM Approach

The MAP Approach

The EP Approach

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

1 3 5 10 40 200 1000 12600

Number of selected genes

The SVM Approach

1 3 5 10 40 200 1000 12600

Number of selected genes

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

1 3 5 10 40 200 1000 12600

Number of selected genes

The MAP Approach

1 3 5 10 40 200 1000 12600

Number of selected genes

0.6

0.5

0.4

0.3

0.2

0.1

0

0.6

0.5

0.4

0.3

0.2

0.1

0

1 3 5 10 40 200 1000 12600

Number of selected genes

The EP Approach

1 3 5 10 40 200 1000 12600

Number of selected genes

r
o
r
r
e



e
n
o

o
r
e
z


n
a
e
M

r
o
r
r
e



t

l

e
u
o
s
b
a
n
a
e
M



Figure 4: The test results of the three algorithms using a linear kernel on the prostate cancer data of
selected genes. The horizonal axes are indexed on log2 scale. The rungs in these boxes
indicate the mean values, and the heights of these vertical boxes indicate the standard
deviations over the 20 trials.

1033

CHU AND GHAHRAMANI

V x

i x

pathologist reecting the level of differentiation of the glands in the prostate tumor. Predicting the
Gleason score from the gene expression data is thus a typical ordinal regression problem. Since
only 6 samples had a score greater than 7, we merged them as the top level, leading to three levels
{= 6, = 7, 8} with 26, 20 and 6 samples respectively. We randomly partitioned the data into 2
folds for training and test and repeated this partitioning 20 times independently. An ARD linear
kernel K (xi, x j) = (cid:229) dV =1
j was used to evaluate feature relevance. These ARD parameters
{k
V } were optimized by evidence maximization. According to the optimal values of these ARD
parameters, the genes were ranked from irrelevant to relevant. We then removed the irrelevant
genes gradually based on the rank list. The gene number was reduced from 12600 to 1. At each
number of selected genes, a linear kernel K (xi, x j) = (cid:229) dV =1 x
j was used in the three algorithms for
i x
a fair comparison. Figure 4 presents the test results of the three algorithms for different numbers of
selected genes. We observed great and steady improvement using the subset of genes selected by
the ARD technique. The best validation output is achieved around 40 top-ranked features. In this
case, with only 26 training samples, the Bayesian approaches perform much better than the SVM,
and the EP approach is generally better than the MAP approach but the difference is not statistically
signicant.

7. Conclusion

Ordinal regression is an important supervised learning problem with properties of both metric re-
gression and classication. In this paper, we proposed a simple yet novel nonparametric Bayesian
approach to ordinal regression based on a generalization of the probit likelihood function for Gaus-
sian processes. Two approximate inference procedures were derived in detail for evidence evalua-
tion and model adaptation. The approach intrinsically incorporates ARD feature selection and pro-
vides probabilistic prediction. The existent fast algorithms for Gaussian processes can be adapted
directly to tackle relatively large data sets. Experiments on benchmark and real-world data sets show
that the generalization performance is competitive and often better than support vector methods.

Acknowledgments

The main part of this work was carried out at Institute for Pure and Applied Mathematics (IPAM) of
UCLA. We thank David L. Wild for stimulating this work and for many discussions. We also thank
David J. C. MacKay for valuable comments. Wei Chu was supported by the National Institutes of
Health and its National Institute of General Medical Sciences division under Grant Number 1 P01
GM63208. Zoubin Ghahramani was partially supported from CMU by DARPA under the CALO
project. The reviewers thoughtful comments are gratefully appreciated.

Appendix A. Gradient Formulae for Evidence Maximization

Evidence maximization is equivalent to nding the minimizer of the negative logarithm of the evi-
dence which can be written in an explicit expression as follows

ln P (D|q ) 

n(cid:229)

i=1

`(yi, fMAP(xi)) +

1
2

MAPS 1 f MAP +
f T

1
2

ln|I + S

L MAP|.

1034

k
V
V
V
V
GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

Initialization choose a favorite gradient-descent optimization package

select the starting point q

for the optimization package

Looping while the optimization package requests evidence/gradient evaluation at q

1. nd the MAP estimate by solving the convex programming problem (9)
2. evaluate the negative logarithm of the evidence (18) at the MAP
3. calculate the gradients with respect to q
4. feed the evidence and gradients to the optimization package

(18)(18)

Exit Return the optimal q

found by the optimization package

Table 5: The outline of our algorithm for model adaptation using the MAP approach with Laplace

approximation.

We usually collect {lnk ,lns

r1} as the set of variables to tune. This denition of
tunable variables is helpful to convert the constrained optimization problem into an unconstrained
optimization problem. The outline of our algorithm for model adaptation is described in Table 5.

, b1,lnD 2, . . . ,lnD

The derivatives of ln P (D|q ) with respect to these variables can be derived as follows:

Note that at the MAP estimate S 1 f MAP = (cid:229) n
(zi
1)
F (zi

sr =

i=1

and

(zi
1)

vr =

N (zi

runs from 0 to 3, zi

where r
is denoted as L
given in the following:

2 =
ii, which is dened as in (7), i.e. L

1 =

and zi

byi f (xi)




ii

k =

ii
f T

f

.

ii

f (xi) = 1

s 3 (2(v0)3 + 3v0v1 + v2  v0).

1035

  ln P (D|q )

lnk

lns

  ln P (D|q )
= s
  ln P (D|q )
  ln P (D|q )

 b1

= D

=

lnD

2

trace(cid:20)(L 1
trace(cid:20)L 1
 `(yi, fMAP(xi))

+

2

=

n(cid:229)

i=1

 `(yi, fMAP(xi))

 b1

+

1
2

n(cid:229)

i=1

 `(yi, fMAP(xi))

+

n(cid:229)

i=1

MAP + S )1
MAP(L 1

(cid:21)
2
MAP + S )1S

S 1 f MAP

MAPS 1
f T
L MAP
(cid:21);

+

2

MAP(L 1

MAP + S )1S

L MAP

(cid:21);
L MAP
 b1 (cid:21);
L MAP
(cid:21) .

MAP(L 1

MAP + S )1S

MAP(L 1

MAP + S )1S

trace(cid:20)L 1
trace(cid:20)L 1
trace(cid:20)L 1
(cid:12)(cid:12)(cid:12) f = f MAP

 `(yi, f (xi))

f

2

. For more details, let us dene

N (zi
1;0,1)
1) F (zi
2)
1;0,1) (zi
N (zi
2)
F (zi
1) F (zi
2)
. The ii-th entry of the diagonal matrix L
byi1 f (xi)
ii = 1
s 2 v1. The detailed derivatives are

s 2 (v0)2 + 1

2;0,1)


k

S

k
k

S

k
k


k


s
s


s


i
i

D
i
D
i


D
i

r
r
r
s
s

L


L



k

L

CHU AND GHAHRAMANI

S 1 f .

f

k = L 1(L 1 + S )1 
 `(yi, f (xi))

= v1s

.

ii

ii + 1

s =  2
s = L 1(L 1 +S )1S
v2).

f

ii

ii
f T

f (xi) +

f
= 
 b1
= L 1(L 1 + S )1S

ii
 b1

f
 b1

s 3 (2v0v2 + 2(v0)2v1  v1 + (v1)2 + v3) +

f

.

ii
f T

s , where y

is a column vector whose i-th element is 1

s 2 (v0 v0v1

.

b, where y

b is a column vector whose i-th element is L

ii.

=




 v0s
 s0s
0

if yi > i ;
if yi = i ;
otherwise.

f

ii
f T
f T

if yi > i ;
if yi = i ;
otherwise.

ii
f

f T

ii

f (xi) +
i +

ii
f













 `(yi, f (xi))

ii

i =


ii

iD =


s 3 (s0  2v0s1  2(v0)2s0  s2  v1s0).

 j


i = 1

i =
i = L 1(L 1 + S )1S

f

ii i.e. 1

s 2 ((v0)2 + v1)

1
s 2 (v0s0 + s1)

0

if yi > i ;
if yi = i ;
otherwise.

D , where y

is a column vector whose i-th element is dened as

Appendix B. Approximate Posterior Distribution by EP
The expectation propagation algorithm attempts to approximate P ( f|D) in form of a product of
Gaussian distributions Q( f ) = (cid:213)
2 pi( f (xi) mi)2). The
updating scheme is given as follows.

i=1 t( f (xi))P ( f ) where t( f (xi)) = si exp( 1

n

The initial states:

 individual mean mi = 0 i ;
 individual inverse variance pi = 0 i ;
 individual amplitude si = 1 i ;
 posterior covariance A = (S 1 + P )1, where P = diag(p1, p2, . . . , pn) ;
 posterior mean h = AP m, where m = [m1, m2, . . . , mn]T .

Looping i from 1 to n until there is no signicant change in {mi, pi, si}n

i=1:

 t( f (xi)) is removed from Q( f ) to get a leave-one-out posterior distribution Q\i( f ) having

1036



S

k

s

L

s
L

L



s


y
s

L

L


L



y

D
i

L

D

L


L



D
i
j

L



D
i

L



D
i

L

D


D
y
D
y
L
 t( f (xi)) in Q( f ) is updated by incorporating the message P (yi| f (xi)) into Q\i( f ):

GAUSSIAN PROCESSES FOR ORDINAL REGRESSION

 variance of f (xi): l \i
 mean of f (xi): h\i
 others with j 6= i: l \i

;

i = Aii
1Aii pi
i = hi + l \i

i pi(hi  mi) ;
j = h j .

j = A j j and h\i

i

i

i ,l \i
i )d f (xi) = F (z1) F (z2)
 Zi = R P (yi| f (xi))N ( f (xi); h\i
byi1h\i
ql \i
i +s 2

where z1 =

and z2 =

byih\i
ql \i
i +s 2
(cid:17) .
i +s 2)(cid:16) z1N (z1;0,1)z2N (z2;0,1)
=  1
F (z1)F (z2)
2(l \i
(cid:17) .
i +s 2(cid:16) N (z1;0,1)N (z2;0,1)
=  1
F (z1)F (z2)
ql \i

log Zi
 h\i
i

log Zi
\i
i

i =

i =



.

(18)

i < 1.

 u
 hnew
 pnew

i

i

i  2b
i = g 2
i .
i + l \i
i = h\i
.
i =
1l \i
i = h\i
i +
i = Ziql \i

i
g i
i

i

 mnew

 snew

i .

.

 Note that pnew
 if pnew

i pnew

i + 1exp(cid:16) g 2

2u

i

i(cid:17) .

i > 0 all the time, because 0 < u

i < 1
l \i

i +s 2 and then l \i

i

mean h and covariance A as follows:
i where r =

i  pi, skip this sample and this updating; otherwise update {pi, mi, si}, the posterior
 A new = A  r aiaT
 hnew = h + h ai where h =

pnew
i pi
1+(pnew
i pi)Aii
g i+pi(himi)
and g

and ai is the i-th column of A.

i is dened as in (18).

1Aii pi

As a byproduct, we can get the approximate evidence P (D|q ) at the EP solution, which can be

written as

where B = (cid:229)

i j Ai j(mi pi)(m j p j) (cid:229)

2 (P 1)
det 1
2 (S + P 1)

n(cid:213)

si

det 1
i=1
i pim2
i .

exp(cid:18)B
2(cid:19)

Appendix C. Gradient Formulae for Variational Bound
At the equilibrium of Q( f ), the variational bound F (q ) can be analytically calculated as follows:

F (q ) =

n(cid:229)

Z N ( f (xi); hi, Aii)ln(P (yi| f (xi)))d f (xi)
i=1
1
trace((I + S
2

|
mT (S + P 1)1S (S + P 1)1m +

ln|I + S

)1)



1
2

n
2

.

1
2

1037

b


l
g

g
u
u
u
u
P
P
CHU AND GHAHRAMANI

Note that (S + P 1)1m can be directly obtained by {g
with respect to the variables {lnk ,lns
, b1,lnD 2, . . . ,lnD
log P ( f )

 F (q )
lnk = k Z Q( f )

d f

i} dened as in (18). The gradient of F (q )
r1} can be given in the following:

2

2

2

s 2+Aii

 F (q )
 b1

hT S 1
k (cid:19) +

= 
= 

k (cid:19) +
trace(cid:18)S 1
trace(cid:18)(P 1 + S )1
 F (q )
lns = s
(cid:229) n
i=1R N ( f (xi); hi, Aii)
{1yi<r}R N (cid:16) f (xi); his 2+Aiibyi
= (cid:229)
{1<yir}R N (cid:16) f (xi); his 2+Aiibyi1
+(cid:229)
i=1R N ( f (xi); hi, Aii)
{1yi<r}R N ( f (xi); his 2+Aiibyi
{1<yir}R N ( f (xi); his 2+Aiibyi1
(cid:229) n
i=1R N ( f (xi); hi, Aii)
{i yi<r}R N ( f (xi); his 2+Aiibyi
{i <yir}R N ( f (xi); his 2+Aiibyi1

(cid:229)
i = D

= (cid:229) n

s 2+Aii

s 2+Aii

,

s 2+Aii
ln P (yi| f (xi))

 b1

s 2+Aii

s 2+Aii

 F (q )
lnD

= D

= (cid:229)

D
{i <yir}

S 1h +

2

trace(cid:18)S 1

S 1A(cid:19)
(P 1 + S )1m ,

mT (P 1 + S )1

2

ln P (yi| f (xi))

d f (xi)

byi f (xi)
2p (s 2+Aii)

byi1 f (xi)
2p (s 2+Aii)

(hibyi )2
2(s 2+Aii)(cid:19)
(hibyi1)2
2(s 2+Aii) (cid:19)

exp(cid:18)
P (yi| f (xi))
exp(cid:18)
P (yi| f (xi))

d f (xi)

d f (xi) ,

,

s 2Aii

s 2+Aii(cid:17)
s 2+Aii(cid:17)

s 2Aii

d f (xi)

s 2Aii
s 2+Aii

)

,

s 2Aii
s 2+Aii

)

,

12p (s 2+Aii)

12p (s 2+Aii)

d f (xi)

(hibyi )2
2(s 2+Aii)(cid:19)
(hibyi1)2
2(s 2+Aii) (cid:19)

exp(cid:18)
P (yi| f (xi)
exp(cid:18)
P (yi| f (xi))

d f (xi) ,

ln P (yi| f (xi))

d f (xi)

s 2Aii
s 2+Aii

)

,

s 2Aii
s 2+Aii

)

,

12p (s 2+Aii)

12p (s 2+Aii)

(hibyi )2
2(s 2+Aii)(cid:19)
(hibyi1)2
2(s 2+Aii) (cid:19)

exp(cid:18)
P (yi| f (xi)
exp(cid:18)
P (yi| f (xi))

d f (xi)

d f (xi) ,

where (cid:229)
dimensional integrals can be approximated using Gaussian quadrature or calculated by Romberg
integration at some appropriate accuracy.

means summing over all the samples whose targets satisfy i < yi  r, and these one-

