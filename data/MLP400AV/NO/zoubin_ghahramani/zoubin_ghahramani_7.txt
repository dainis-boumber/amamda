Abstract

We present a framework for learning in hidden Markov models with distributed state
representations. Within this framework, we derive a learning algorithm based on the
Expectation{Maximization (EM) procedure for maximum likelihood estimation. Anal-
ogous to the standard Baum-Welch update rules, the M-step of our algorithm is exact
and can be solved analytically. However, due to the combinatorial nature of the hidden
state representation, the exact E-step is intractable. A simple and tractable mean (cid:12)eld
approximation is derived. Empirical results on a set of problems suggest that both the
mean (cid:12)eld approximation and Gibbs sampling are viable alternatives to the computa-
tionally expensive exact algorithm.

Copyright c(cid:13) Massachusetts Institute of Technology, 		

This report describes research done at the Center for Biological and Computational Learning and the Arti(cid:12)cial
Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Center is provided in part
by a grant from the National Science Foundation under contract ASC{	. This project was supported in
part by a grant from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research
Laboratories, by a grant from Siemens Corporation, and by grant N-	-- from the O(cid:14)ce of Naval Research.



Introduction

A problem of fundamental interest to machine learning is time series modeling. Due to the sim-
plicity and e(cid:14)ciency of its parameter estimation algorithm, the hidden Markov model (HMM) has
emerged as one of the basic statistical tools for modeling discrete time series, (cid:12)nding widespread
application in the areas of speech recognition (Rabiner and Juang, 	) and computational molec-
ular biology (Baldi et al., 		). An HMM is essentially a mixture model, encoding information
about the history of a time series in the value of a single multinomial variable (the hidden state).
This multinomial assumption allows an e(cid:14)cient parameter estimation algorithm to be derived (the
Baum-Welch algorithm). However, it also severely limits the representational capacity of HMMs.
For example, to represent  bits of information about the history of a time sequence, an HMM
would need  distinct states. On the other hand an HMM with a distributed state representa-
tion could achieve the same task with  binary units (Williams and Hinton, 		). This paper
addresses the problem of deriving e(cid:14)cient learning algorithms for hidden Markov models with
distributed state representations.

The need for distributed state representations in HMMs can be motivated in two ways. First, such
representations allow the state space to be decomposed into features that naturally decouple the
dynamics of a single process generating the time series. Second, distributed state representations
simplify the task of modeling time series generated by the interaction of multiple independent
processes. For example, a speech signal generated by the superposition of multiple simultaneous
speakers can be potentially modeled with such an architecture.

Williams and Hinton (		) (cid:12)rst formulated the problem of learning in HMMs with distributed
state representation and proposed a solution based on deterministic Boltzmann learning. The ap-
proach presented in this paper is similar to Williams and Hintons in that it is also based on a
statistical mechanical formulation of hidden Markov models. However, our learning algorithm is
quite di(cid:11)erent in that it makes use of the special structure of HMMs with distributed state rep-
resentation, resulting in a more e(cid:14)cient learning procedure. Anticipating the results in section ,
this learning algorithm both obviates the need for the two-phase procedure of Boltzmann machines,
and has an exact M-step. A di(cid:11)erent approach comes from Saul and Jordan (		), who derived
a set of rules for computing the gradients required for learning in HMMs with distributed state
spaces. However, their methods can only be applied to a limited class of architectures.

 Factorial hidden Markov models

Hidden Markov models are a generalization of mixture models. At any time step, the probability
density over the observables de(cid:12)ned by an HMM is a mixture of the densities de(cid:12)ned by each state
in the underlying Markov model. Temporal dependencies are introduced by specifying that the
prior probability of the state at time t depends on the state at time t (cid:0)  through a transition
matrix, P (Figure a).

Another generalization of mixture models, the cooperative vector quantizer (CVQ; Hinton and
Zemel, 		 ), provides a natural formalism for distributed state representations in HMMs. Whereas
in simple mixture models each data point must be accounted for by a single mixture component,
in CVQs each data point is accounted for by the combination of contributions from many mixture
components, one from each separate vector quantizer. The total probability density modeled by a
CVQ is also a mixture model; however this mixture density is assumed to factorize into a product
of densities, each density associated with one of the vector quantizers. Thus, the CVQ is a mixture



model with distributed representations for the mixture components.

Factorial hidden Markov models combine the state transition structure of HMMs with the dis-
tributed representations of CVQs (Figure b). Each of the d underlying Markov models has a
t
discrete state s
i at time t and transition probability matrix Pi. As in the CVQ, the states are mu-
tually exclusive within each vector quantizer and we assume real-valued outputs. The sequence of
observable output vectors is generated from a normal distribution with mean given by the weighted
combination of the states of the underlying Markov models:

t

y

(cid:24) N   d
Xi=

Wis

t

i; C! ;

where C is a common covariance matrix. The k-valued states si are represented as discrete column
vectors with a  in one position and  everywhere else; the mean of the observable is therefore a
combination of columns from each of the Wi matrices.

a)

y

b)

W1

1s

W

s

y

W2
Wd
...
2s

sd

P

P1

P2

Pd

Figure . a) Hidden Markov model. b) Factorial hidden Markov model.

We capture the above probability model by de(cid:12)ning the energy of a sequence of T states and

observations, f(s

t; y

t)g

T

t=, which we abbreviate to fs; yg, as:

H(fs; yg) =

t

i#

C (cid:0)"y

t

(cid:0)

t
Wis

i# (cid:0)

d

Xi=

T

d

Xt=

Xi=

t
t(cid:0)
i Ais
i

;

s

()

T

d

t

(cid:0)

Wis




Xi=

Xt="y
il ) such that Pk

where [Ai]jl = log P (st
for the initial state, s
probability model is de(cid:12)ned from this energy by the Boltzmann distribution

, are introduced by setting the second term in () to (cid:0)Pd

j= e[Ai]jl = , and  denotes matrix transpose. Priors

i log (cid:25)i. The

i= s

ij jst(cid:0)

P (fs; yg) =



Z

expf(cid:0)H(fs; yg)g:

()

We refer to HMMs with distributed state as factorial HMMs as the features of the distributed state factorize the

total state representation.



Note that like in the CVQ (Ghahramani, 		), the unclamped partition function

Z = Z dfygXfsg

expf(cid:0)H(fs; yg)g;

evaluates to a constant, independent of the parameters. This can be shown by (cid:12)rst integrating the
Gaussian variables, removing all dependency on fyg, and then summing over the states using the
constraint on e[Ai]jl.

The EM algorithm for Factorial HMMs

As in HMMs, the parameters of a factorial HMM can be estimated via the EM (Baum-Welch)
algorithm. This procedure iterates between assuming the current parameters to compute proba-
bilities over the hidden states (E-step), and using these probabilities to maximize the expected log
likelihood of the parameters (M-step).

Using the likelihood (), the expected log likelihood of the parameters is

Q((cid:30)new

j(cid:30)) = h(cid:0)H(fs; yg) (cid:0) log Z ic;

()

d

where (cid:30) = fWi; Pi; C g
i= denotes the current parameters, and h(cid:1)ic denotes expectation given the
clamped observation sequence and (cid:30). Given the observation sequence, the only random variables are
the hidden states. Expanding equation () and limiting the expectation to these random variables
t
we (cid:12)nd that the statistics that need to be computed for the E-step are hs
ic.
i ic, hs
t
Note that in standard HMM notation (Rabiner and Juang, 	), hs
i ic corresponds to (cid:13)t and
t
j ic has no analogue when there is only a single underlying
hs
Markov model. The M-step uses these expectations to maximize Q with respect to the parameters.
The constant partition function allowed us to drop the second term in (). Therefore, unlike
the Boltzmann machine, the expected log likelihood does not depend on statistics collected in an
unclamped phase of learning, resulting in much faster learning than the traditional Boltzmann
machine (Neal, 		).

t
ic corresponds to (cid:24)t, whereas hs
is

t(cid:0)
t
t
j ic, and hs
i s
i

t(cid:0)
i

t
is

t
is

M-step

Setting the derivatives of Q with respect to the output weights to zero, we obtain a linear system
of equations for W :

W new = 
XN;t



hss

y
ic
XN;t


hsic y


 ;

summation over a data set of N sequences, and y is the Moore-Penrose pseudo-inverse. To estimate

where s and W are the vector and matrix of concatenated si and Wi, respectively,PN denotes
the log transition probabilities we solve @Q=@[Ai]jl =  subject to the constraint Pj e[Ai]jl = ,

obtaining

()

The covariance matrix can be similarly estimated:

ij st(cid:0)
il ic
ij st(cid:0)

il ic! :

[Ai]new

jl = log  PN;thst
PN;t;j hst
(cid:0)XN;t

yy



C new = XN;t


chss
yhsi



y
c hsicy

:

i

The M-step equations can therefore be solved analytically; furthermore, for a single underlying
Markov chain, they reduce to the traditional Baum-Welch re-estimation equations.



E-step

Unfortunately, as in the simpler CVQ, the exact E-step for factorial HMMs is computationally
intractable. For example, the expectation of the j th unit in vector i at time step t, given fyg, is:

hst

ij ic = P (st

ij = jfyg; (cid:30))

k

=

Xj;:::;jh=i ;:::;jd

P (st

j= ;: : : ;st

ij = ; : : : ; st

d;jd= jfyg; (cid:30))

Although the Markov property can be used to obtain a forward-backward{like factorization of this
expectation across time steps, the sum over all possible con(cid:12)gurations of the other hidden units
within each time step is unavoidable. For a data set of N sequences of length T , the full E-step
calculated through the forward-backward procedure has time complexity O(N T kd). Although
more careful bookkeeping can reduce the complexity to O(N T dkd+), the exponential time cannot
be avoided. This intractability of the exact E-step is due inherently to the cooperative nature of
the model|the setting of one vector only determines the mean of the observable if all the other
vectors are (cid:12)xed.

Rather than summing over all possible hidden state patterns to compute the exact expectations,
a natural approach is to approximate them through a Monte Carlo method such as Gibbs sampling.
The procedure starts with a clamped observable sequence fyg and a random setting of the hidden
t
j g. At each time step, each state vector is updated stochastically according to its probability
states fs
(cid:28)
t
t
distribution conditioned on the setting of all the other state vectors: s
: j =
i (cid:24) P (s
i jfyg; fs
j
i or (cid:28) = tg; (cid:30)): These conditional distributions are straightforward to compute and a full pass
of Gibbs sampling requires O(N T kd) operations. The (cid:12)rst and second-order statistics needed
to estimate hs
ijs visited and the probabilities
estimated during this sampling process.

ic are collected using the st

t
j ic and hs

i ic, hs

t(cid:0)
i

t
is

t
is

t

Mean (cid:12)eld approximation

A di(cid:11)erent approach to computing the expectations in an intractable system is given by mean (cid:12)eld
theory. A mean (cid:12)eld approximation for factorial HMMs can be obtained by de(cid:12)ning the energy
function

~
H(fs; yg) =



 Xt hy

t

(cid:0) (cid:22)

ti

t

C (cid:0)hy

(cid:0) (cid:22)

ti (cid:0)Xt;i

t
i log m
s

t
i:

which results in a completely factorized approximation to probability density ():

~P (fs; yg) / Yt

expf(cid:0)

t



 hy

(cid:0) (cid:22)

ti

t

C (cid:0)hy

(cid:0) (cid:22)

(m

tigYt;i;j

t

ij)st

ij

()

t and
In this approximation, the observables are independently Gaussian distributed with mean (cid:22)
t
each hidden state vector is multinomially distributed with mean m
i. This approximation is made as
t
tight as possible by chosing the mean (cid:12)eld parameters (cid:22)
i that minimize the Kullback-Liebler
divergence

t and m

KL( ~P kP ) (cid:17) hlog P i ~P (cid:0) hlog ~P i ~P

where h(cid:1)i ~P denotes expectation over the mean (cid:12)eld distribution (). With the observables clamped,
t. Minimizing KL( ~P kP ) with respect to the mean (cid:12)eld
(cid:22)

t can be set equal to the observable y



parameters for the states results in a (cid:12)xed-point equation which can be iterated until convergence:

m

t new
i

= (cid:27)fW 

i C (cid:0)hy

+Aim

t

(cid:0) ^y

ti + W 

t(cid:0)
i + A

im

t+
i g

i C (cid:0)Wim

t
i (cid:0)




diagfW 

i C (cid:0)Wig (cid:0) 

()

t

(cid:17) Pi Wim

t
where ^y
i and (cid:27)f(cid:1)g is the softmax exponential, normalized over each hidden state vector.
The (cid:12)rst term is the projection of the error in the observable onto the weights of state vector i|the
more a hidden unit can reduce this error, the larger its mean (cid:12)eld parameter. The next three
terms arise from the fact that hs
ij. The last two terms introduce
dependencies forward and backward in time. Each state vector is asynchronously updated using
(), at a time cost of O(N T kd) per iteration. Convergence is diagnosed by monitoring the KL
divergence in the mean (cid:12)eld distribution between successive time steps; in practice convergence is
very rapid (about  to  iterations of ()).

ij i ~P is equal to mij and not m

 Empirical Results

We compared three EM algorithms for learning in factorial HMMs|using Gibbs sampling, mean
(cid:12)eld approximation, and the exact (exponential) E step|on the basis of performance and speed
on randomly generated problems. Problems were generated from a factorial HMM structure, the
parameters of which were sampled from a uniform [; ] distribution, and appropriately normalized
to satisfy the sum-to-one constraints of the transition matrices and priors. Also included in the
comparison was a traditional HMM with as many states (kd) as the factorial HMM.

Table  summarizes the results. Even for moderately large state spaces (d (cid:21)  and k (cid:21) )
the standard HMM with kd states su(cid:11)ers from severe over(cid:12)tting. Furthermore, both the standard
HMM and the exact E-step factorial HMM are extremely slow on the larger problems. The Gibbs
sampling and mean (cid:12)eld approximations o(cid:11)er roughly comparable performance at a great increase
in speed.

 Discussion

The basic contribution of this paper is a learning algorithm for hidden Markov models with dis-
tributed state representations. The standard Baum-Welch procedure is intractable for such archi-
tectures as the size of the state space generated from the cross product of d k-valued features is
O(kd), and the time complexity of Baum-Welch is quadratic in this size. More importantly, unless
special constraints are applied to this cross-product HMM architecture, the number of parameters
also grows as O(kd), which can result in severe over(cid:12)tting.

The architecture for factorial HMMs presented in this paper did not include any coupling between
the underlying Markov chains. It is possible to extend the algorithm presented to architectures which
incorporate such couplings. However, these couplings must be introduced with caution as they may
result either in an exponential growth in parameters or in a loss of the constant partition function
property.

The learning algorithm derived in this paper assumed real-valued observables. The algorithm can
also be derived for HMMs with discrete observables, an architecture closely related to sigmoid belief
networks (Neal, 		). However, the nonlinearities induced by discrete observables make both the
E-step and M-step of the algorithm more di(cid:14)cult.



Table : Comparison of factorial HMM on four problems of varying size

d


Alg #
k
 HMM 

Exact
Gibbs

MF



 HMM 

Exact
Gibbs

MF



 HMM 

Exact
Gibbs

MF



 HMM 

Exact
Gibbs

MF

Train
	 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
	 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
	 (cid:6) 
,,	

-,-,-	
-,-,-	
-,-,-	

Test

 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
- (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 

- (cid:6) 

	 (cid:6) 
 (cid:6) 	

 (cid:6) 
-,-,-

Cycles
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
 (cid:6) 
,,

-,-,-
-,-,-
-,-,-

,,
,,
,,

Time/Cycle

. s
. s
. s
. s
. s
. s
. s
. s
. s
.	 s
. s
. s
. s
. s
. s
. s

Table . Data was generated from a factorial HMM with d underlying Markov
models of k states each. The training set was  sequences of length  where the
observable was a -dimensional vector; the test set was  such sequences. HMM
indicates a hidden Markov model with kd states; the other algorithms are factorial
HMMs with d underlying k-state models. Gibbs sampling used  samples of each
state. The algorithms were run until convergence, as monitored by relative change
in the likelihood, or a maximum of  cycles. The # column indicates number of
runs. The Train and Test columns show the log likelihood (cid:6) one standard deviation
on the two data sets. The last column indicates approximate time per cycle on a
Silicon Graphics R processor running Matlab.



In conclusion, we have presented Gibbs sampling and mean (cid:12)eld learning algorithms for factorial
hidden Markov models. Such models incorporate the time series modeling capabilities of hidden
Markov models and the advantages of distributed representations for the state space. Future work
will concentrate on a more e(cid:14)cient mean (cid:12)eld approximation in which the forward-backward algo-
rithm is used to compute the E-step exactly within each Markov chain, and mean (cid:12)eld theory is
used to handle interactions between chains (Saul and Jordan, 		).

