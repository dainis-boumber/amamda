1 Introduction

Support vector (SV) machines comprise a new class of learning algorithms,
motivated by results of statistical learning theory (Vapnik, 1995). Originally
developed for pattern recognition (Vapnik & Chervonenkis, 1974; Boser,
Guyon, & Vapnik, 1992), they represent the decision boundary in terms of
a typically small subset (Scholkopf, Burges, & Vapnik, 1995) of all training
examples, called the support vectors. In order for this sparseness property
to carry over to the case of SV Regression, Vapnik devised the so-called
"-insensitive loss function,

jy  f .x/j" D maxf0;jy  f .x/j  "g;

(1.1)

which does not penalize errors below some " > 0, chosen a priori. His
algorithm, which we will henceforth call "-SVR, seeks to estimate functions,

f .x/ D .w  x/ C b; w; x 2 RN; b 2 R;

(1.2)



Present address: Microsoft Research, 1 Guildhall Street, Cambridge, U.K.

Neural Computation 12, 12071245 (2000)

c(cid:176) 2000 Massachusetts Institute of Technology

1208

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

based on independent and identically distributed (i.i.d.) data,

.x1; y1/; : : : ; .x; y/ 2 RN  R:

(1.3)

Here, RN is the space in which the input patterns live but most of the fol-
lowing also applies for inputs from a set X . The goal of the learning process
is to nd a function f with a small risk (or test error),

X l. f; x; y/ dP.x; y/;

(1.4)

Z

R[ f ] D

where P is the probability measure, which is assumed to be responsible for
the generation of the observations (see equation 1.3) and l is a loss func-
tion, for example, l. f; x; y/ D . f .x/  y/2, or many other choices (Smola &
Scholkopf, 1998). The particular loss function for which we would like to
minimize equation 1.4 depends on the specic regression estimation prob-
lem at hand. This does not necessarily have to coincide with the loss function
used in our learning algorithm. First, there might be additional constraints
that we would like our regression estimation to satisfy, for instance, that it
have a sparse representation in terms of the training data. In the SV case, this
is achieved through the insensitive zone in equation 1.1. Second, we cannot
minimize equation 1.4 directly in the rst place, since we do not know P.
Instead, we are given the sample, equation 1.3, and we try to obtain a small
risk by minimizing the regularized risk functional,

Here, kwk2 is a term that characterizes the model complexity,

emp[ f ]:

1
2

kwk2 C C  R"
X

emp[ f ] :D 1
R"



jyi  f .xi/j";

iD1

(1.5)

(1.6)

measures the "-insensitive training error, and C is a constant determining
the trade-off. In short, minimizing equation 1.5 captures the main insight
of statistical learning theory, stating that in order to obtain a small risk, one
needs to control both training error and model complexitythat is, explain
the data with a simple model.

The minimization of equation 1.5 is equivalent to the following con-

strained optimization problem (see Figure 1):

minimize

 .w; .// D 1
2

kwk2 C C  1



.i C 

i

/;

(1.7)

X

iD1

New Support Vector Algorithms

1209

Figure 1: In SV regression, a desired accuracy " is specied a priori. It is then
attempted to t a tube with radius " to the data. The trade-off between model
complexity and points lying outside the tube (with positive slack variables ) is
determined by minimizing the expression 1.5.

subject to ..w  xi/ C b/  yi  " C i
yi  ..w  xi/ C b/  " C 

i

(1.8)

(1.9)

 ./
i

 0:

(1.10)
Here and below, it is understood that i D 1; : : : ; , and that boldface Greek
letters denote -dimensional vectors of the corresponding variables; ./ is a
shorthand implying both the variables with and without asterisks.

By using Lagrange multiplier techniques, one can show (Vapnik, 1995)

that this leads to the following dual optimization problem. Maximize

W.; 

/ D "

.
i

 i/yi

X
X

.
i

C i/ C X

iD1
 i/.

j

.
i

iD1
 1
2

i;jD1

subject to

X
.i  
h
2

iD1
./
i

i

0; C



/ D 0
i

:

 j/.xi  xj/

(1.11)

(1.12)

(1.13)

The resulting regression estimates are linear; however, the setting can be
generalized to a nonlinear one by using the kernel method. As we will use
precisely this method in the next section, we shall omit its exposition at this
point.

1210

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

To motivate the new algorithm that we shall propose, note that the pa-
rameter " can be useful if the desired accuracy of the approximation can be
specied beforehand. In some cases, however, we want the estimate to be as
accurate as possible without having to commit ourselves to a specic level
of accuracy a priori. In this work, we rst describe a modication of the "-
SVR algorithm, called -SVR, which automatically minimizes ". Following
this, we present two theoretical results on -SVR concerning the connection
to robust estimators (section 3) and the asymptotically optimal choice of the
parameter  (section 4). Next, we extend the algorithm to handle parametric
insensitivity models that allow taking into account prior knowledge about
heteroscedasticity of the noise. As a bridge connecting this rst theoreti-
cal part of the article to the second one, we then present a denition of a
margin that both SV classication and SV regression algorithms maximize
(section 6). In view of this close connection between both algorithms, it is
not surprising that it is possible to formulate also a -SV classication al-
gorithm. This is done, including some theoretical analysis, in section 7. We
conclude with experiments and a discussion.

2 -SV Regression

To estimate functions (see equation 1.2) from empirical data (see equa-
tion 1.3) we proceed as follows (Scholkopf, Bartlett, Smola, & Williamson,
1998). At each point xi, we allow an error of ". Everything above " is cap-
tured in slack variables  ./
, which are penalized in the objective function
via a regularization constant C, chosen a priori (Vapnik, 1995). The size of
" is traded off against model complexity and slack variables via a constant
  0:

i



" C 1


!

X
.i C 

i

iD1

/

(2.1)

(2.2)

(2.3)

(2.4)
;   0, and obtain the

minimize

 .w; ./; "/ D 1
2

kwk2 C C 

subject to ..w  xi/ C b/  yi  " C i
yi  ..w  xi/ C b/  " C 

i

 ./
i

 0; "  0:

For the constraints, we introduce multipliers ./
Lagrangian

i

; ./

i

X
L.w; b; ./; ; ./; "; .//
kwk2 C C" C C
.i C 

i

/  "  X

D 1
2



iD1

.ii C 

i


i

/

iD1

New Support Vector Algorithms

 X
 X

iD1

iD1

i.i C yi  .w  xi/  b C "/


i

.
i

C .w  xi/ C b  yi C "/:

1211

(2.5)

(2.7)

(2.8)

(2.9)

; ; ./

To minimize the expression 2.1, we have to nd the saddle point of Lthat
is, minimize over the primal variables w; "; b;  ./
and maximize over the
dual variables ./
. Setting the derivatives with respect to the primal
X
variables equal to zero yields four equations:
X

.
i

(2.6)

i

i

i

 i/xi
.i C 

i

/   D 0

i

w D
C   
X
.i  
iD1
 ./
C
i


i

/ D 0
i
 ./
i

D 0:

In the SV expansion, equation 2.6, only those ./
i will be nonzero that cor-
respond to a constraint, equations 2.2 or 2.3, which is precisely met; the
corresponding patterns are called support vectors. This is due to the Karush-
Kuhn-Tucker (KKT) conditions that apply to convex constrained optimiza-
tion problems (Bertsekas, 1995). If we write the constraints as g.xi; yi/ 
0, with corresponding Lagrange multipliers i, then the solution satises
i  g.xi; yi/ D 0 for all i.

Substituting the above four conditions into L leads to another optimiza-
tion problem, called the Wolfe dual. Before stating it explicitly, we carry
out one further modication. Following Boser et al. (1992), we substitute a
kernel k for the dot product, corresponding to a dot product in some feature
space related to input space via a nonlinear map 8,

k.x; y/ D .8.x/  8.y//:

(2.10)

By using k, we implicitly carry out all computations in the feature space that
8 maps into, which can have a very high dimensionality. The feature space
has the structure of a reproducing kernel Hilbert space (Wahba, 1999; Girosi,
1998; Scholkopf, 1997) and hence minimization ofkwk2 can be understood in
the context of regularization operators (Smola, Scholkopf, & M uller, 1998).
The method is applicable whenever an algorithm can be cast in terms of
dot products (Aizerman, Braverman, & Rozonoer, 1964; Boser et al., 1992;
Scholkopf, Smola, & M uller, 1998). The choice of k is a research topic in its

 j/k.xi; xj/

(2.11)

(2.12)

(2.13)

(2.14)

(2.15)

subject to

i

X
.i  
h
2
X
.i C 

iD1
./
i

0; C



i

iD1

/ D 0
i

/  C  :

f .x/ D X

iD1

.
i

 i/k.xi; x/ C b;

P

1212

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

own right that we shall not touch here (Williamson, Smola, & Scholkopf,
1998; Scholkopf, Shawe-Taylor, Smola, & Williamson, 1999); typical choices
include gaussian kernels, k.x; y/ D exp.kx  yk2=.2(cid:190) 2// and polynomial
kernels, k.x; y/ D .x  y/d .(cid:190) > 0; d 2 N/.
i  0 do not appear in the
dual, we arrive at the -SVR optimization problem: for   0; C > 0,

Rewriting the constraints, noting that ; ./

maximize W..// D X

.
i

 i/yi
X

iD1
 1
2

.
i

 i/.

j

i;jD1

The regression estimate then takes the form (cf. equations 1.2, 2.6, and 2.10),

j

.
j

where b (and ") can be computed by taking into account that equations 2.2
 j/k.xj; x/ for .w  x/ is understood; cf.
and 2.3 (substitution of
equations 2.6 and 2.10) become equalities with  ./
i D 0 for points with
0 < ./

i < C=, respectively, due to the KKT conditions.

Before we give theoretical results explaining the signicance of the pa-
rameter , the following observation concerning " is helpful. If  > 1, then
" D 0, since it does not pay to increase " (cf. equation 2.1). If   1, it can
still happen that " D 0for example, if the data are noise free and can be
perfectly interpolated with a low-capacity model. The case " D 0, however,
is not what we are interested in; it corresponds to plain L1-loss regression.
We will use the term errors to refer to training points lying outside the
tube1 and the term fraction of errors or SVs to denote the relative numbers of

1 For N > 1, the tube should actually be called a slabthe region between two

parallel hyperplanes.

New Support Vector Algorithms

P

errors or SVs (i.e., divided by ). In this proposition, we dene the modulus
j f .bi/ 
of absolute continuity of a function f as the function ./ D sup
f .ai/j, where the supremum is taken over all disjoint intervals .ai; bi/ with
.bi  ai/ < . Loosely speaking, the condition on the
ai < bi satisfying
conditional density of y given x asks that it be absolutely continuous on
average.

i

i

1213

P

Proposition 1. Suppose -SVR is applied to some data set, and the resulting "
is nonzero. The following statements hold:

i.  is an upper bound on the fraction of errors.
ii.  is a lower bound on the fraction of SVs.
iii. Suppose the data (see equation 1.3) were generated i.i.d. from a distribution
P.x; y/ D P.x/P.yjx/ with P.yjx/ continuous and the expectation of the
modulus of absolute continuity of its density satisfying lim!0 E./ D 0.
With probability 1, asymptotically,  equals both the fraction of SVs and the
fraction of errors.

i D C=. All examples with  ./
i D C= (if not, ./

Proof. Ad (i). The constraints, equations 2.13 and 2.14, imply that at most
a fraction  of all examples can have ./
i > 0
(i.e., those outside the tube) certainly satisfy ./
could
grow further to reduce  ./
Ad (ii). By the KKT conditions, " > 0 implies  D 0. Hence, equation 2.14
becomes an equality (cf. equation 2.7).2 Since SVs are those examples for
which 0 < ./
D 0 for all i; Vapnik,
1995).

i  C=, the result follows (using i  

).

i

i

i

Ad (iii). The strategy of proof is to show that asymptotically, the proba-
bility of a point is lying on the edge of the tube vanishes. The condition on
P.yjx/ means that

j f .x/ C t  yj < (cid:176)



 x

< .(cid:176) /

(2.16)

EP

sup
f;t



Pr

sup

f

for some function .(cid:176) / that approaches zero as (cid:176) ! 0. Since the class of
SV regression estimates f has well-behaved covering numbers, we have
(Anthony & Bartlett, 1999, chap. 21) that for all t,


 OP.j f .x/Ct  yj < (cid:176) =2/ <P.j f .x/Ctyj < (cid:176) /

!

> 


<c1c
2

;

2 In practice, one can alternatively work with equation 2.14 as an equality constraint.

1214
B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett
where OP is the sample-based estimate of P (that is, the proportion of points
that satisfyj f .x/yCtj < (cid:176) ), and c1; c2 may depend on (cid:176) and . Discretizing
the values of t, taking the union bound, and applying equation 2.16 shows
that the supremum over f and t of OP. f .x/  y C t D 0/ converges to zero in
probability. Thus, the fraction of points on the edge of the tube almost surely
converges to 0. Hence the fraction of SVs equals that of errors. Combining
statements i and ii then shows that both fractions converge almost surely
to .

P

Hence, 0    1 can be used to control the number of errors (note that for
D 0 for all i (Vapnik, 1995)).
  1, equation 2.13 implies 2.14, since i  
Moreover, since the constraint, equation 2.12, implies that equation 2.14 is
./
i  C=2, we conclude that proposition 1 actually holds
equivalent to
for the upper and the lower edges of the tube separately, with =2 each.
(Note that by the same argument, the number of SVs at the two edges of the
standard "-SVR tube asymptotically agree.)

i

i

emp

D 0, hence  D .@=@"/R"

A more intuitive, albeit somewhat informal, explanation can be given
in terms of the primal objective function (see equation 2.1). At the point
of the solution, note that if " > 0, we must have .@=@"/ .w; "/ D 0, that
is,  C .@=@"/R"
emp. This is greater than or
equal to the fraction of errors, since the points outside the tube certainly
do contribute to a change in Remp when " is changed. Points at the edge of
the tube possibly might also contribute. This is where the inequality comes
from.
case, " D 0, since it does not pay to increase " (cf. equation 2.1).

Note that this does not contradict our freedom to choose  > 1. In that

P

Let us briey discuss how -SVR relates to "-SVR (see section 1). Both al-
gorithms use the "-insensitive loss function, but -SVR automatically com-
putes ". In a Bayesian perspective, this automatic adaptation of the loss
function could be interpreted as adapting the error model, controlled by the
hyperparameter . Comparing equation 1.11 (substitution of a kernel for the
dot product is understood) and equation 2.11, we note that "-SVR requires
C i/, which, for xed " > 0, encourages
an additional term, "
that some of the ./
i will turn out to be 0. Accordingly, the constraint (see
equation 2.14), which appears in -SVR, is not needed. The primal problems,
equations 1.7 and 2.1, differ in the term ". If  D 0, then the optimization
can grow " arbitrarily large; hence zero empirical risk can be obtained even
when all s are zero.
case, using kernels, Nw is a vector in feature space.

In the following sense, -SVR includes "-SVR. Note that in the general

.
i

iD1

Proposition 2.
priori to N", and the same value of C, has the solution Nw; Nb.

If -SVR leads to the solution N"; Nw; Nb, then "-SVR with " set a

New Support Vector Algorithms

1215

Proof.
remaining variables. The solution does not change.

If we minimize equation 2.1, then x " and minimize only over the

3 The Connection to Robust Estimators

Using the "-insensitive loss function, only the patterns outside the "-tube
enter the empirical risk term, whereas the patterns closest to the actual
regression have zero loss. This, however, does not mean that it is only the
outliers that determine the regression. In fact, the contrary is the case.

Proposition 3 (resistance of SV regression). Using support vector regression
with the "-insensitive loss function (see equation 1.1), local movements of target
values of points outside the tube do not inuence the regression.

Proof. Shifting yi locally does not change the status of .xi; yi/ as being a
point outside the tube. Then the dual solution ./ is still feasible; it satises
the constraints (the point still has ./
i D C=). Moreover, the primal solution,
with i transformed according to the movement of yi, is also feasible. Finally,
the KKT conditions are still satised, as still ./
i D C=. Thus (Bertsekas,
1995), ./ is still the optimal solution.

The proof relies on the fact that everywhere outside the tube, the upper
bound on the ./
is the same. This is precisely the case if the loss func-
tion increases linearly ouside the "-tube (cf. Huber, 1981, for requirements
on robust cost functions). Inside, we could use various functions, with a
derivative smaller than the one of the linear part.

i

For the case of the "-insensitive loss, proposition 3 implies that essentially,
the regression is a generalization of an estimator for the mean of a random
variable that does the following:

 Throws away the largest and smallest examples (a fraction =2 of either
category; in section 2, it is shown that the sum constraint, equation 2.12,
implies that proposition 1 can be applied separately for the two sides,
using =2).
 Estimates the mean by taking the average of the two extremal ones of

the remaining examples.

This resistance concerning outliers is close in spirit to robust estima-
tors like the trimmed mean. In fact, we could get closer to the idea of the
trimmed mean, which rst throws away the largest and smallest points
and then computes the mean of the remaining points, by using a quadratic
loss inside the "-tube. This would leave us with Hubers robust loss func-
tion.

1216

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Note, moreover, that the parameter  is related to the breakdown point
of the corresponding robust estimator (Huber, 1981). Because it species
the fraction of points that may be arbitrarily bad outliers,  is related to the
fraction of some arbitrary distribution that may be added to a known noise
model without the estimator failing.
Finally, we add that by a simple modication of the loss function (White,
1994)weighting the slack variables ./ above and below the tube in the tar-
get function, equation 2.1, by 2 and 2.1/, with  2 [0; 1]respectively
one can estimate generalized quantiles. The argument proceeds as follows.
Asymptotically, all patterns have multipliers at bound (cf. proposition 1).
The , however, changes the upper bounds in the box constraints applying
to the two different types of slack variables to 2C= and 2C.1  /=, re-
spectively. The equality constraint, equation 2.8, then implies that .1  /
and  give the fractions of points (out of those which are outside the tube)
that lie on the two sides of the tube, respectively.

4 Asymptotically Optimal Choice of 

Using an analysis employing tools of information geometry (Murata,
Yoshizawa, & Amari, 1994; Smola, Murata, Scholkopf, & M uller, 1998), we
can derive the asymptotically optimal  for a given class of noise models in
the sense of maximizing the statistical efciency.3

Remark. Denote p a density with unit variance,4 and P a family of noise
models generated from P by P :D fpjp D 1
/; (cid:190) > 0g. Moreover
assume that the data were generated i.i.d. from a distribution p.x; y/ D
p.x/p.y  f .x// with p.y  f .x// continuous. Under the assumption that SV
regression produces an estimate Of converging to the underlying functional
dependency f , the asymptotically optimal , for the estimation-of-location-
parameter model of SV regression described in Smola, Murata, Scholkopf,
& M uller (1998), is

(cid:190) P. y

(cid:190)

Z "

 D 1 

"
" :D argmin

P.t/ dt where

1

.P. / C P. //2



(cid:181)
1 

Z 



P.t/ dt



(4.1)

3 This section assumes familiarity with some concepts of information geometry. A more
complete explanation of the model underlying the argument is given in Smola, Murata,
Scholkopf, & M uller (1998) and can be downloaded from http://svm.rst.gmd.de.

4 p is a prototype generating the class of densities P. Normalization assumptions are

made for ease of notation.

New Support Vector Algorithms

1217

To see this, note that under the assumptions stated above, the probability

of a deviation larger than ", Prfjy  Of .x/j > "g, will converge to

'jy  f .x/j > "

 D

Pr

Z
Z "
XfRn[";"]g p.x/p. / dx d

D 1 

"

p. / d:

(4.2)

This is also the fraction of samples that will (asymptotically) become SVs
(proposition 1, iii). Therefore an algorithm generating a fraction  D 1 

R "" p. / d SVs will correspond to an algorithm with a tube of size ". The

consequence is that given a noise model p. /, one can compute the optimal
" for it, and then, by using equation 4.2, compute the corresponding optimal
value .

To this end, one exploits the linear scaling behavior between the standard
deviation (cid:190) of a distribution p and the optimal ". This result, established
in Smola, Murata, Scholkopf, & M uller (1998) and Smola (1998), cannot be
proved here; instead, we shall merely try to give a avor of the argument.
The basic idea is to consider the estimation of a location parameter using the
"-insensitive loss, with the goal of maximizing the statistical efciency. Us-
ing the Cramer-Rao bound and a result of Murata et al. (1994), the efciency
is found to be



 "

(cid:190)

e

D Q2
GI

:

(4.3)

Here, I is the Fisher information, while Q and G are information geometrical
quantities computed from the loss function and the noise model.

This means that one only has to consider distributions of unit variance,
say, P, to compute an optimal value of  that holds for the whole class of
distributions P. Using equation 4.3, one arrives at

1
e."/

/ G
Q2

D

1

.P."/ C P."//2

P.t/ dt

:

"

(4.4)

Z "

(cid:181)
1 

The minimum of equation 4.4 yields the optimal choice of ", which allows
computation of the corresponding  and thus leads to equation 4.1.
jjp)

Consider now an example: arbitrary polynomial noise models (/ e

with unit variance can be written as

r



r



 exp



P. / D 1
2

0.3=p/
0.1=p/

0

p
1=p

jj

0.3=p/
0.1=p/

(4.5)

where 0 denotes the gamma function. Table 1 shows the optimal value of
 for different polynomial degrees. Observe that the more lighter-tailed



!

!

p

1218

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Table 1: Optimal  for Various Degrees of Polynomial Additive Noise.

Polynomial degree p

1

2

3

4

5

6

7

8

Optimal 

1.00

0.54

0.29

0.19

0.14

0.11

0.09

0.07

the distribution becomes, the smaller  are optimalthat is, the tube width
increases. This is reasonable as only for very long tails of the distribution
(data with many outliers) it appears reasonable to use an early cutoff on
the inuence of the data (by basically giving all data equal inuence via
i D C=). The extreme case of Laplacian noise ( D 1) leads to a tube width
of 0, that is, to L1 regression.

We conclude this section with three caveats: rst, we have only made
an asymptotic statement; second, for nonzero ", the SV regression need
not necessarily converge to the target f : measured using j:j", many other
functions are just as good as f itself; third, the proportionality between "
and (cid:190) has only been established in the estimation-of-location-parameter
context, which is not quite SV regression.

5 Parametric Insensitivity Models

We now return to the algorithm described in section 2. We generalized "-SVR
by estimating the width of the tube rather than taking it as given a priori.
What we have so far retained is the assumption that the "-insensitive zone
has a tube (or slab) shape. We now go one step further and use parametric
models of arbitrary shape. This can be useful in situations where the noise
is heteroscedastic, that is, where it depends on x.
g (here and below, q D 1; : : : ; p is understood) be a set of 2p
positive functions on the input space X . Consider the following quadratic
program: for given ./

p  0, minimize

Let f ./

; : : : ; ./

q

1

 .w; ./; ".// D kwk2=2
C C 

0@ pX

qD1

X

iD1

"
q

/ C 1


.i C 

i

/

1A

(5.1)

(5.2)

(5.3)

(5.4)

q

.q"q C 
X
X

q

subject to ..w  xi/ C b/  yi 
yi  ..w  xi/ C b/ 

"qq.xi/ C i
.xi/ C 
 
"
q
q

i

q

 ./
i

 0; "./

q

 0:

New Support Vector Algorithms

1219

A calculation analogous to that in section 2 shows that the Wolfe dual con-
sists of maximizing the expression 2.11 subject to the constraints 2.12 and
2.13, and, instead of 2.14, the modied constraints, still linear in ./,

./
i

 ./
q

.xi/  C  ./

q

:

(5.5)

X

iD1

In the experiments in section 8, we use a simplied version of this opti-
"
mization problem, where we drop the term 
q from the objective function,
equation 5.1, and use "q and q in equation 5.3. By this, we render the prob-
lem symmetric with respect to the two edges of the tube. In addition, we
use p D 1. This leads to the same Wolfe dual, except for the last constraint,
which becomes (cf. equation 2.14),

q

X
.i C 

i

iD1

/ .xi/  C  :

(5.6)

Note that the optimization problem of section 2 can be recovered by using
the constant function   1.5

p

, evaluated on the xi with 0 < ./

The advantage of this setting is that since the same  is used for both
sides of the tube, the computation of "; b is straightforward: for instance, by
solving a linear system, using two conditions as those described following
equation 2.15. Otherwise, general statements are harder to make; the linear
system can have a zero determinant, depending on whether the functions
 ./
i < C=, are linearly dependent. The
latter occurs, for instance, if we use constant functions  ./  1. In this
case, it is pointless to use two different values ; 
; for the constraint (see
./
iD1
i will be bounded by
equation 2.12) then implies that both sums
C  minf; g. We conclude this section by giving, without proof, a gener-
alization of proposition 1 to the optimization problem with constraint (see
equation 5.6):

P

Proposition 4. Suppose we run the above algorithm on a data set with the result
that " > 0. Then

P
P

i

i

i.

ii.

 .xi/ is an upper bound on the fraction of errors.
 .xi/ is an upper bound on the fraction of SVs.

5 Observe the similarity to semiparametric SV models (Smola, Frie, & Scholkopf,
1999) where a modication of the expansion of f led to similar additional constraints. The
important difference in the present setting is that the Lagrange multipliers i and 
i are
treated equally and not with different signs, as in semiparametric modeling.

1220

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

iii. Suppose the data in equation 1.3 were generated i.i.d. from a distribution
R
P.x; y/ D P.x/P.yjx/ with P.yjx/ continuous and the expectation of its
modulus of continuity satisfying lim!0 E./ D 0. With probability 1,
 .x/ d QP.x//1,
asymptotically, the fractions of SVs and errors equal  .
where QP is the asymptotic distribution of SVs over x.

6 Margins in Regression and Classication

The SV algorithm was rst proposed for the case of pattern recognition
(Boser et al., 1992), and then generalized to regression (Vapnik, 1995). Con-
ceptually, however, one can take the view that the latter case is actually the
simpler one, providing a posterior justication as to why we started this
article with the regression case. To explain this, we will introduce a suitable
denition of a margin that is maximized in both cases.

At rst glance, the two variants of the algorithm seem conceptually dif-
ferent. In the case of pattern recognition, a margin of separation between
two pattern classes is maximized, and the SVs are those examples that lie
closest to this margin. In the simplest case, where the training error is xed
to 0, this is done by minimizing kwk2 subject to yi  ..w  xi/ C b/  1 (note
that in pattern recognition, the targets yi are in f1g).

In regression estimation, on the other hand, a tube of radius " is tted to
the data, in the space of the target values, with the property that it corre-
sponds to the attest function in feature space. Here, the SVs lie at the edge
of the tube. The parameter " does not occur in the pattern recognition case.
We will show how these seemingly different problems are identical (cf.
also Vapnik, 1995; Pontil, Rifkin, & Evgeniou, 1999), how this naturally leads
to the concept of canonical hyperplanes (Vapnik, 1995), and how it suggests
different generalizations to the estimation of vector-valued functions.
Denition 1 ("-margin). Let .E;k:kE/, .F;k:kF/ be normed spaces, andX  E.
We dene the "-margin of a function f : X ! F as

m". f / :D inffkx  ykE: x; y 2 X ;k f .x/  f .y/kF  2"g:

(6.1)
m". f / can be zero, even for continuous functions, an example being f .x/ D
1=x on X D R
C

. There, m". f / D 0 for all " > 0.

Note that the "-margin is related (albeit not identical) to the traditional
modulus of continuity of a function: given  > 0, the latter measures the
largest difference in function values that can be obtained using points within
a distance  in E.

The following observations characterize the functions for which the mar-

gin is strictly positive.

Lemma 1 (uniformly continuous functions). With the
m". f / is positive for all " > 0 if and only if f is uniformly continuous.

above notations,

New Support Vector Algorithms

Proof. By denition of m", we have


k f .x/  f .y/kF  2" H) kx  ykE  m". f /
()kx  ykE < m". f / H) k f .x/  f .y/kF < 2"



;

1221

(6.2)

(6.3)

that is, if m". f / > 0, then f is uniformly continuous. Similarly, if f is uni-
formly continuous, then for each " > 0, we can nd a  > 0 such that
k f .x/  f .y/kF  2" implies kx  ykE  . Since the latter holds uniformly,
we can take the inmum to get m". f /   > 0.

We next specialize to a particular set of uniformly continuous functions.

Lemma 2 (Lipschitz-continuous functions).
that for all x; y 2 X , k f .x/  f .y/kF  L  kx  ykE, then m"  2"
Proof. Take the inmum over kx  ykE  k f .x/ f .y/kF

L

:

If there exists some L > 0 such

 2"
L .

L

Example 1 (SV regression estimation). Suppose that E is endowed with a dot
product .:  :/ (generating the norm k:kE). For linear functions (see equation 1.2)
the margin takes the form m". f / D 2"kwk : To see this, note that since j f .x/ f .y/j D
j.w  .x  y//j, the distance kx  yk will be smallest given j.w  .x  y//j D 2",
when x y is parallel to w (due to Cauchy-Schwartz), i.e. if x y D 2"w=kwk2.
In that case, kx  yk D 2"=kwk. For xed " > 0, maximizing the margin hence
amounts to minimizing kwk, as done in SV regression: in the simplest form (cf.
equation 1.7 without slack variables i) the training on data (equation 1.3) consists
of minimizing kwk2 subject to

j f .xi/  yij  ":

(6.4)

Example 2 (SV pattern recognition; see Figure 2). We specialize the setting
of example 1 to the case where X D fx1; : : : ; xg. Then m1. f / D 2kwk is equal to
the margin dened for Vapniks canonical hyperplane (Vapnik, 1995). The latter is
a way in which, given the data set X , an oriented hyperplane in E can be uniquely
expressed by a linear function (see equation 1.2) requiring that

minfj f .x/j: x 2 Xg D 1:

(6.5)

Vapnik gives a bound on the VC-dimension of canonical hyperplanes in terms of
kwk. An optimal margin SV machine for pattern recognition can be constructed
from data,

.x1; y1/; : : : ; .x; y/ 2 X  f1g

(6.6)

1222

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Figure 2: 1D toy problem. Separate x from o. The SV classication algorithm
constructs a linear function f .x/ D w  x C b satisfying equation 6.5 (" D 1). To
maximize the margin m". f /, one has to minimize jwj.

as follows (Boser et al., 1992):

minimize kwk2 subject to yi  f .xi/  1:

The decision function used for classication takes the form

.x/ D sgn..w  x/ C b/:

f

(6.7)

(6.8)

The parameter " is superuous in pattern recognition, as the resulting decision
function,

.x/ D sgn..w  x/ C b/;

f

will not change if we minimize kwk2 subject to

yi  f .xi/  ":

(6.9)

(6.10)

Finally, to understand why the constraint (see equation 6.7) looks different from
equation 6.4 (e.g., one is multiplicative, the other one additive), note that in re-
gression, the points .xi; yi/ are required to lie within a tube of radius ", whereas in
pattern recognition, they are required to lie outside the tube (see Figure 2), and on
the correct side. For the points on the tube, we have 1 D yi f .xi/ D 1j f .xi/yij.

So far, we have interpreted known algorithms only in terms of maxi-
mizing m". Next, we consider whether we can use the latter as a guide for
constructing more general algorithms.
Example 3 (SV regression for vector-valued functions). Assume E D RN.
For linear functions f .x/ D WxC b; with W being an N N matrix, and b 2 RN,

New Support Vector Algorithms

1223

we have, as a consequence of lemma 1,

m". f /  2"
kWk ;

(6.11)
where kWk is any matrix norm of W that is compatible (Horn & Johnson, 1985)
with k:kE. If the matrix norm is the one induced by k:kE, that is, there exists
a unit vector z 2 E such that kWzkE D kWk, then equality holds in 6.11.
To see the latter, we use the same argument as in example 1, setting x  y D
2"z=kWk.
ij, which is compatible with
the vector normk:k2, the problem of minimizingkWk subject to separate constraints
for each output dimension separates into N regression problems.

For the Hilbert-Schmidt norm kWk2 D

qP

N
i;jD1 W2

In Smola, Williamson, Mika, & Scholkopf (1999), it is shown that one can
specify invariance requirements, which imply that the regularizers act on the output
dimensions separately and identically (i.e., in a scalar fashion). In particular, it
turns out that under the assumption of quadratic homogeneity and permutation
symmetry, the Hilbert-Schmidt norm is the only admissible one.

7 -SV Classication

We saw that -SVR differs from "-SVR in that it uses the parameters  and
C instead of " and C. In many cases, this is a useful reparameterization of
the original algorithm, and thus it is worthwhile to ask whether a similar
change could be incorporated in the original SV classication algorithm
(for brevity, we call it C-SVC). There, the primal optimization problem is to
minimize (Cortes & Vapnik, 1995)

 .w; / D 1
2

kwk2 C C



X

i

i

(7.1)

(7.2)

subject to

yi  ..xi  w/ C b/  1  i;

i  0:

(see equation 6.9)
The goal of the learning process is to estimate a function f
such that the probability of misclassication on an independent test set, the
risk R[ f

], is small.6





Here, the only parameter that we can dispose of is the regularization
constant C. To substitute it by a parameter similar to the  used in the
regression case, we proceed as follows. As a primal problem for -SVC, we

6 Implicitly we make use of thef0; 1g loss function; hence the risk equals the probability

of misclassication.

1224

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

consider the minimization of

 .w; ; / D 1
2

kwk2   C 1



X

i

i

subject to (cf. equation 6.10)

yi  ..xi  w/ C b/    i;
i  0;

  0:

(7.3)

(7.4)

(7.5)

For reasons we shall expain, no constant C appears in this formulation.
To understand the role of , note that for  D 0, the constraint (see 7.4)
simply states that the two classes are separated by the margin 2=kwk (cf.
example 2).

To derive the dual, we consider the Lagrangian

X

X
kwk2   C 1
L.w; ; b; ; ; ; / D 1
2
.i.yi..xi  w/ C b/   C i/ C ii/


i



i

i
 ;

(7.6)
using multipliers i; i;   0. This function has to be mimimized with re-
spect to the primal variables w; ; b;  and maximized with respect to the
dual variables ; ; . To eliminate the former, we compute the correspond-
ing partial derivatives and set them to 0, obtaining the following conditions:

X

i

iyi;

X

i

i   D :

(7.7)

(7.8)

X

w D

iyixi

i

i C i D 1=;

0 D

X

W./ D  1
2

In the SV expansion (see equation 7.7), only those i can be nonzero that
correspond to a constraint (see 7.4) that is precisely met (KKT conditions;
cf. Vapnik, 1995).
Substituting equations 7.7 and 7.8 into L, using i; i;   0, and incor-
porating kernels for dot products leaves us with the following quadratic
optimization problem: maximize

ijyiyjk.xi; xj/

ij

(7.9)

New Support Vector Algorithms

subject to

X
0  i  1=
0 D
X

iyi

i

i  :

i

The resulting decision function can be shown to take the form

X

!
iyik.x; xi/ C b

:

i

.x/ D sgn

f

P

i

Compared to the original dual (Boser et al., 1992; Vapnik, 1995), there are
two differences. First, there is an additional constraint, 7.12, similar to the
i of Boser et al. (1992) no
regression case, 2.14. Second, the linear term
longer appears in the objective function 7.9. This has an interesting conse-
quence: 7.9 is now quadratically homogeneous in . It is straightforward
to verify that one obtains exactly the same objective function if one starts
with the primal function  .w; ; / D kwk2=2 C C  . C .1=/
i/ (i.e.,
if one does use C), the only difference being that the constraints, 7.10 and
7.12 would have an extra factor C on the right-hand side. In that case, due to
the homogeneity, the solution of the dual would be scaled by C; however, it
is straightforward to see that the corresponding decision function will not
change. Hence we may set C D 1.
To compute b and , we consider two sets S, of identical size s > 0,
containing SVs xi with 0 < i < 1 and yi D 1, respectively. Then, due to
the KKT conditions, 7.4 becomes an equality with i D 0. Hence, in terms of
kernels,

P

i

1225

(7.10)

(7.11)

(7.12)

(7.13)

X
X
0@X
X

x2SC[S

j

x2SC

j

b D  1
2s

 D 1
2s

jyjk.x; xj/;

X

X

jyjk.x; xj/ 

jyjk.x; xj/

x2S

j

1A :

(7.14)

(7.15)

As in the regression case, the  parameter has a more natural interpreta-
tion than the one we removed, C. To formulate it, let us rst dene the term
margin error. By this, we denote points with i > 0that is, that are either
errors or lie within the margin. Formally, the fraction of margin errors is

emp[ f ] :D 1
R



jfi: yi  f .xi/ < gj:

(7.16)

1226

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Here, f is used to denote the argument of the sgn in the decision function,
equation 7.13, that is, f

 D sgn f .

We are now in a position to modify proposition 1 for the case of pattern

recognition:

Proposition 5. Suppose k is a real analytic kernel function, and we run -SVC
with k on some data with the result that  > 0. Then

i.  is an upper bound on the fraction of margin errors.

ii.  is a lower bound on the fraction of SVs.

iii. Suppose the data (see equation 6.6) were generated i.i.d. from a distribution
P.x; y/ D P.x/P.yjx/ such that neither P.x; y D 1/ nor P.x; y D 1/
contains any discrete component. Suppose, moreover, that the kernel is an-
alytic and non-constant. With probability 1, asymptotically,  equals both
the fraction of SVs and the fraction of errors.

Proof. Ad (i). By the KKT conditions,  > 0 implies  D 0. Hence, inequal-
ity 7.12 becomes an equality (cf. equations 7.8). Thus, at most a fraction  of
all examples can have i D 1=. All examples with i > 0 do satisfy i D 1=
(if not, i could grow further to reduce i).

Ad (ii). SVs can contribute at most 1= to the left-hand side of 7.12; hence

there must be at least  of them.

Ad (iii). It follows from the condition on P.x; y/ that apart from some
set of measure zero (arising from possible singular components), the two
class distributions are absolutely continuous and can be written as inte-
grals over distribution functions. Because the kernel is analytic and non-
constant, it cannot be constant in any open set; otherwise it would be con-
stant everywhere. Therefore, functions f constituting the argument of the
sgn in the SV decision function (see equation 7.13) essentially functions
in the class of SV regression functions) transform the distribution over
x into distributions such that for all f , and all t 2 R, lim(cid:176)!0 P.j f .x/ C
tj < (cid:176) / D 0. At the same time, we know that the class of these func-
tions has well-behaved covering numbers; hence we get uniform conver-
jP.j f .x/ C tj < (cid:176) /  OP.j f .x/ C tj < (cid:176) /j con-
gence: for all (cid:176) > 0, supf
verges to zero in probability, where OP is the sample-based estimate of P
(that is, the proportion of points that satisfy j f .x/ C tj < (cid:176) ). But then for
OP.j f .x/ C tj < (cid:176) / > / D 0. Hence,
all  > 0, lim(cid:176)!0 lim!1 P.supf
OP.j f .x/ C tj D 0/ converges to zero in probability. Using t D 
supf
thus shows that almost surely the fraction of points exactly on the mar-
gin tends to zero; hence the fraction of SVs equals that of margin errors.

New Support Vector Algorithms

1227

Combining (i) and (ii) shows that both fractions converge almost surely
to .

Moreover, since equation 7.11 means that the sums over the coefcients of
positive and negative SVs respectively are equal, we conclude that proposi-
tion 5 actually holds for both classes separately, with =2. (Note that by the
same argument, the number of SVs at the two sides of the margin asymp-
totically agree.)

A connection to standard SV classication, and a somewhat surprising
interpretation of the regularization parameter C, is described by the follow-
ing result:

Proposition 6.
with C set a priori to 1=, leads to the same decision function.

If -SV classication leads to  > 0, then C-SV classication,

If one minimizes the function 7.3 and then xes  to minimize
Proof.
only over the remaining variables, nothing will change. Hence the obtained
solution w0; b0; 0 minimizes the function 7.1 for C D 1, subject to the con-
straint 7.4. To recover the constraint 7.2, we rescale to the set of variables
0 D =. This leaves us, up to a constant scaling factor
0 D w=; b
w
2, with the objective function 7.1 using C D 1=.

0 D b=; 

As in the case of regression estimation (see proposition 3), linearity of the
target function in the slack variables ./ leads to outlier resistance of the
estimator in pattern recognition. The exact statement, however, differs from
the one in regression in two respects. First, the perturbation of the point is
carried out in feature space. What it precisely corresponds to in input space
therefore depends on the specic kernel chosen. Second, instead of referring
to points outside the "-tube, it refers to margin error pointspoints that are
misclassied or fall into the margin. Below, we use the shorthand zi for
8.xi/.

X

i

Proposition 7 (resistance of SV classication). Suppose w can be expressed
in terms of the SVs that are not at bound, that is,

(cid:176)izi;

w D
(7.17)
6D 0 only if i 2 .0; 1=/ (where the i are the coefcients of the dual
with (cid:176)i
solution). Then local movements of any margin error zm parallel to w do not
change the hyperplane.

Proof. Since the slack variable of zm satises m > 0, the KKT conditions
(e.g., Bertsekas, 1995) imply m D 1=. If  is sufciently small, then trans-
m :D zm C   w results in a slack that is still nonzero,
0
forming the point into z

1228

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett
D 1= D m. Updating the m and keeping
that is, 0
all other primal variables unchanged, we obtain a modied set of primal
variables that is still feasible.

> 0; hence we have 0

m

m

We next show how to obtain a corresponding set of feasible dual vari-

ables. To keep w unchanged, we need to satisfy

iyizi D

iyizi C mymz
0

0
m

:

X

i

X

i6Dm

Substituting z
condition for this to hold is that for all i 6D m,

0
m

D zm C   w and equation 7.17, we note that a sufcient

0
i

D i  (cid:176)iyimym:

Since by assumption (cid:176)i is nonzero only if i 2 .0; 1=/, 0
i will be in .0; 1=/ if
i is, provided  is sufciently small, and it will equal 1= if i does. In both
0
, and the KKT conditions are still
cases, we end up with a feasible solution 
satised. Thus (Bertsekas, 1995), .w; b/ are still the hyperplane parameters
of the solution.

the SV expansion of the solution, w DP

Note that the assumption (7.17) is not as restrictive as it may seem. Although
iyizi, often contains many mul-
tipliers i that are at bound, it is nevertheless conceivable that, especially
when discarding the requirement that the coefcients be bounded, we can
obtain an expansion (see equation 7.17) in terms of a subset of the original
vectors. For instance, if we have a 2D problem that we solve directly in in-
put space, with k.x; y/ D .x  y/, then it already sufces to have two linearly
independent SVs that are not at bound in order to express w. This holds
for any overlap of the two classeseven if there are many SVs at the upper
bound.

i

For the selection of C, several methods have been proposed that could
probably be adapted for  (Scholkopf, 1997; Shawe-Taylor & Cristianini,
1999). In practice, most researchers have so far used cross validation. Clearly,
this could be done also for -SVC. Nevertheless, we shall propose a method
that takes into account specic properties of -SVC.

The parameter  lets us control the number of margin errors, the crucial
quantity in a class of bounds on the generalization error of classiers using
covering numbers to measure the classier capacity. We can use this con-
nection to give a generalization error bound for -SVC in terms of . There
are a number of complications in doing this the best possible way, and so
here we will indicate the simplest one. It is based on the following result:

Proposition 8 (Bartlett, 1998). Suppose  > 0; 0 <  < 1
2 , P is a probability
distribution on X  f1; 1g from which the training set, equation 6.6, is drawn.
Then with probability at least 1   for every f in some function class F, the

New Support Vector Algorithms

probability of error of the classication function f
test set is bounded according to

q


lnN .F ; l21; =2/ C ln.2=/



1229
 D sgn f on an independent



]  R

emp[ f ] C

2


R[ f

(7.18)
where N .F ; l1; / D supXDx1;:::;x
N .FjX; l1; /, FjX D f. f .x1/; : : : ; f .x//:
f 2 Fg, N .FX; l1; / is the -covering number of FX with respect to l1, the
usual l1 metric on a set of vectors.

;

To obtain the generalization bound for -SVC, we simply substitute the
emp[ f ]   (proposition 5, i) and some estimate of the covering
bound R
numbers in terms of the margin. The best available bounds are stated in
terms of the functional inverse of N , hence the slightly complicated expres-
sions in the following.

Proposition 9 (Williamson et al., 1998). Denote BR the ball of radius R around
the origin in some Hilbert space F. Then the covering number N of the class of
functions

F D fx 7! .w  x/: kwk  1; x 2 BRg

at scale  satises

N .F ; l1; /  inf

log2

n

n

 c2R2

2

o


1 C 

n

  1

 1;

1
n log2

(7.19)

(7.20)

where c < 103 is a constant.
This is a consequence of a fundamental theorem due to Maurey. For   2
one thus obtains

N .F ; l1; /  c2R2

2

  1:

log2

log2

(7.21)

To apply these results to -SVC, we rescale w to length 1, thus obtaining
a margin =kwk (cf. equation 7.4). Moreover, we have to combine propo-
sitions 8 and 9. Using =2 instead of  in the latter yields the following
result.
Proposition 10. Suppose -SVC is used with a kernel of the form k.x; y/ D
k.kx  yk/ with k.0/ D 1. Then all the data points 8.xi/ in feature space live in a
ball of radius 1 centered at the origin. Consequently with probability at least 1  
 D sgn f ,
over the training set (see equation 6.6), the -SVC decision function f

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

iyik.x; xi/ (cf. equation 7.13), has a probability of test error

1230

with f .x/ D P

i

bounded according to



R[ f

]  R

s
emp[ f ] C
(cid:181)

s

(cid:181)

4c2kwk2

4c2kwk2

2

log2

2


.2/  1 C ln.2=/





2

2


log2

  C

.2/  1 C ln.2=/
Notice that in general, kwk is a vector in feature space.
Note that the set of functions in the proposition differs from support
vector decision functions (see equation 7.13) in that it comes without the Cb
term. This leads to a minor modication (for details, see Williamson et al.,
1998).

:

Better bounds can be obtained by estimating the radius or even opti-
mizing the choice of the center of the ball (cf. the procedure described by
Scholkopf et al. 1995; Burges, 1998). However, in order to get a theorem of
the above form in that case, a more complex argument is necessary (see
Shawe-Taylor, Bartlet, Williamson, & Anthony, 1998, sec. VI for an indica-
tion).

We conclude this section by noting that a straightforward extension of the
-SVC algorithm is to include parametric models k.x/ for the margin, and
qq.xi/ instead of  in the constraint (see equation 7.4)in
thus to use
complete analogy to the regression case discussed in section 5.

q

P

8 Experiments

8.1 Regression Estimation. In the experiments, we used the optimizer
LOQO.7 This has the serendipitous advantage that the primal variables b
and " can be recovered as the dual variables of the Wolfe dual (see equa-
tion 2.11) (i.e., the double dual variables) fed into the optimizer.

8.1.1 Toy Examples. The rst task was to estimate a noisy sinc function,
given  examples .xi; yi/, with xi drawn uniformly from [3; 3], and yi D
sin.xi/=.xi/ C (cid:192)i, where the (cid:192)i were drawn from a gaussian with zero
mean and variance (cid:190) 2. Unless stated otherwise, we used the radial basis
0j2/,  D 50; C D 100;  D 0:2,
function (RBF) kernel k.x; x
and (cid:190) D 0:2. Whenever standard deviation error bars are given, the results
were obtained from 100 trials. Finally, the risk (or test error) of a regression
estimate f was computed with respect to the sinc function without noise,
j f .x/  sin.x/=.x/j dx. Results are given in Table 2 and Figures 3
as 1
6
through 9.

0/ D exp.jx  x

R

33

7 Available online at http://www.princeton.edu/rvdb/.

New Support Vector Algorithms

1231

Figure 3: -SV regression with  D 0:2 (top) and  D 0:8 (bottom). The larger 
allows more points to lie outside the tube (see section 2). The algorithm auto-
matically adjusts " to 0.22 (top) and 0.04 (bottom). Shown are the sinc function
(dotted), the regression f , and the tube f  ".

1232

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Figure 4: -SV regression on data with noise (cid:190) D 0 (top) and (cid:190) D 1; (bottom).
In both cases,  D 0:2. The tube width automatically adjusts to the noise (top:
" D 0; bottom: " D 1:19).

New Support Vector Algorithms

1233

Figure 5: "-SV regression (Vapnik, 1995) on data with noise (cid:190) D 0 (top) and
(cid:190) D 1 (bottom). In both cases, " D 0:2. This choice, which has to be specied
a priori, is ideal for neither case. In the upper gure, the regression estimate is
biased; in the lower gure, " does not match the external noise (Smola, Murata,
Scholkopf, & M uller, 1998).

1234

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Figure 6: -SVR for different values of the error constant . Notice how " de-
creases when more errors are allowed (large ), and that over a large range of ,
the test error (risk) is insensitive toward changes in .

New Support Vector Algorithms

1235

Figure 7: -SVR for different values of the noise (cid:190) . The tube radius " increases
linearly with (cid:190) (largely due to the fact that both " and the  ./
enter the cost
function linearly). Due to the automatic adaptation of ", the number of SVs and
points outside the tube (errors) are, except for the noise-free case (cid:190) D 0, largely
independent of (cid:190) .

i

1236

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Figure 8: -SVR for different values of the constant C. (Top) " decreases when the
regularization is decreased (large C). Only very little, if any, overtting occurs.
(Bottom)  upper bounds the fraction of errors, and lower bounds the fraction
of SVs (cf. proposition 1). The bound gets looser as C increases; this corresponds
to a smaller number of examples  relative to C (cf. Table 2).

New Support Vector Algorithms

1237

Table 2: Asymptotic Behavior of the Fraction of Errors and SVs.



"

Fraction of errors

Fraction of SVs

10

0.27

0.00

0.40

50

0.22

0.10

0.28

100

0.23

0.14

0.24

200

0.25

0.18

0.23

500

0.26

0.19

0.21

1000

1500

2000

0.26

0.20

0.21

0.26

0.20

0.20

0.26

0.20

0.20

Notes: The " found by -SV regression is largely independent of the sample size . The frac-
tion of SVs and the fraction of errors approach  D 0:2 from above and below, respectively,
as the number of training examples  increases (cf. proposition 1).

R

Figure 10 gives an illustration of how one can make use of parametric
insensitivity models as proposed in section 5. Using the proper model, the
estimate gets much better. In the parametric case, we used  D 0:1 and
 .x/ dP.x/ D 1=2, corresponds to our
 .x/ D sin2..2=3/x/, which, due to
standard choice  D 0:2 in -SVR (cf. proposition 4). Although this relies
on the assumption that the SVs are uniformly distributed, the experimental
ndings are consistent with the asymptotics predicted theoretically: for  D
200, we got 0:24 and 0:19 for the fraction of SVs and errors, respectively.

8.1.2 Boston Housing Benchmark. Empirical studies using "-SVR have
reported excellent performance on the widely used Boston housing regres-
sion benchmark set (Stitson et al., 1999). Due to proposition 2, the only
difference between -SVR and standard "-SVR lies in the fact that different
parameters, " versus , have to be specied a priori. Accordingly, the goal of
the following experiment was not to show that -SVR is better than "-SVR,
but that  is a useful parameter to select. Consequently, we are interested
only in  and ", and hence kept the remaining parameters xed. We adjusted
C and the width 2(cid:190) 2 in k.x; y/ D exp.kx  yk2=.2(cid:190) 2// as in Scholkopf et
al. (1997). We used 2(cid:190) 2 D 0:3  N, where N D 13 is the input dimensionality,
and C= D 10  50 (i.e., the original value of 10 was corrected since in the
present case, the maximal y-value is 50 rather than 1). We performed 100
runs, where each time the overall set of 506 examples was randomly split
into a training set of  D 481 examples and a test set of 25 examples (cf.
Stitson et al., 1999). Table 3 shows that over a wide range of  (note that only
0    1 makes sense), we obtained performances that are close to the best
performances that can be achieved by selecting " a priori by looking at the
test set. Finally, although we did not use validation techniques to select the
optimal values for C and 2(cid:190) 2, the performances are state of the art (Stitson et
al., 1999, report an MSE of 7:6 for "-SVR using ANOVA kernels, and 11:7 for
Bagging regression trees). Table 3, moreover, shows that in this real-world
application,  can be used to control the fraction of SVs/errors.

1238

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Table 3: Results for the Boston Housing Benchmark (top: -SVR; bottom: "-SVR).



automatic "
MSE
STD
Errors
SVs

0.1

2.6
9.4
6.4
0.0
0.3

0.2

1.7
8.7
6.8
0.1
0.4

0.3

1.2
9.3
7.6
0.2
0.6

0.4

0.8
9.5
7.9
0.2
0.7

0.5

0.6
10.0
8.4
0.3
0.8

0.6

0.3
10.6
9.0
0.4
0.9

0.7

0.0
11.3
9.6
0.5
1.0

0.8

0.0
11.3
9.5
0.5
1.0

0.9

0.0
11.3
9.5
0.5
1.0

1.0

0.0
11.3
9.5
0.5
1.0

"

MSE
STD
Errors
SVs

0

11.3
9.5
0.5
1.0

1

9.5
7.7
0.2
0.6

2

8.8
6.8
0.1
0.4

3

9.7
6.2
0.0
0.3

4

11.2
6.3
0.0
0.2

5

13.1
6.0
0.0
0.1

6

15.6
6.1
0.0
0.1

7

18.2
6.2
0.0
0.1

8

22.1
6.6
0.0
0.1

9

27.0
7.3
0.0
0.1

10

34.3
8.4
0.0
0.1

Note: MSE: Mean squared errors; STD: standard deviations thereof (100 trials); errors:
fraction of training points outside the tube; SVs: fraction of training points that are SVs.

8.2 Classication. As in the regression case, the difference between C-
SVC and -SVC lies in the fact that we have to select a different parameter
a priori. If we are able to do this well, we obtain identical performances.
In other words, -SVC could be used to reproduce the excellent results ob-
tained on various data sets using C-SVC (for an overview; see Scholkopf,
Burges, & Smola, 1999). This would certainly be a worthwhile project; how-
ever,we restrict ourselves here to showing some toy examples illustrating
the inuence of  (see Figure 11). The corresponding fractions of SVs and
margin errors are listed in Table 4.

9 Discussion

We have presented a new class of SV algorithms, which are parameterized
by a quantity  that lets one control the number of SVs and errors. We de-
scribed -SVR, a new regression algorithm that has been shown to be rather

0/ D exp.jx  x

Figure 9: Facing page. -SVR for different values of the gaussian kernel width 2s2,
0j2=.2s2//. Using a kernel that is too wide results in
using k.x; x
undertting; moreover, since the tube becomes too rigid as 2s2 gets larger than
1, the " needed to accomodate a fraction .1  / of the points, increases signif-
icantly. In the bottom gure, it can again be seen that the speed of the uniform
convergence responsible for the asymptotic statement given in proposition 1
depends on the capacity of the underlying model. Increasing the kernel width
leads to smaller covering numbers (Williamson et al., 1998) and therefore faster
convergence.

New Support Vector Algorithms

1239

useful in practice. We gave theoretical results concerning the meaning and
the choice of the parameter . Moreover, we have applied the idea under-
lying -SV regression to develop a -SV classication algorithm. Just like
its regression counterpart, the algorithm is interesting from both a prac-

1240

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

Figure 10: Toy example, using prior knowledge about an x-dependence of the
noise. Additive noise ((cid:190) D 1) was multiplied by the function sin2..2=3/x/.
(Top) The same function was used as  as a parametric insensitivity tube (sec-
tion 5). (Bottom) -SVR with standard tube.

New Support Vector Algorithms

1241

Table 4: Fractions of Errors and SVs, Along with the Margins of Class Separation,
for the Toy Example Depicted in Figure 11.



0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.00
0.29
0.009

0.07
0.36
0.035

Fraction of errors
Fraction of SVs
Margin 2=kwk
Note:  upper bounds the fraction of errors and lower bounds the fraction of SVs, and
that increasing , i.e. allowing more errors, increases the margin.

0.25
0.43
0.229

0.50
0.68
0.837

0.32
0.46
0.312

0.39
0.57
0.727

0.61
0.79
0.922

0.71
0.86
1.092

tical and a theoretical point of view. Controlling the number of SVs has
consequences for (1) run-time complexity, since the evaluation time of the
estimated function scales linearly with the number of SVs (Burges, 1998);
(2) training time, e.g., when using a chunking algorithm (Vapnik, 1979)
whose complexity increases with the number of SVs; (3) possible data com-
pression applications characterizes the compression ratio: it sufces to
train the algorithm only on the SVs, leading to the same solution (Scholkopf
et al., 1995); and (4) generalization error bounds: the algorithm directly op-
timizes a quantity using which one can give generalization bounds. These,
in turn, could be used to perform structural risk minimization over . More-
over, asymptotically,  directly controls the number of support vectors, and
the latter can be used to give a leave-one-out generalization bound (Vapnik,
1995).

Figure 11: Toy problem (task: separate circles from disks) solved using -SV
classication, using parameter values ranging from  D 0:1 (top left) to  D
0:8 (bottom right). The larger we select , the more points are allowed to lie
inside the margin (depicted by dotted lines). As a kernel, we used the gaussian
k.x; y/ D exp.kx  yk2/.

1242

B. Scholkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett

In both the regression and the pattern recognition case, the introduction
of  has enabled us to dispose of another parameter. In the regression case,
this was the accuracy parameter "; in pattern recognition, it was the reg-
ularization constant C. Whether we could have as well abolished C in the
regression case is an open problem.

Note that the algorithms are not fundamentally different from previous
SV algorithms; in fact, we showed that for certain parameter settings, the
results coincide. Nevertheless, we believe there are practical applications
where it is more convenient to specify a fraction of points that is allowed to
become errors, rather than quantities that are either hard to adjust a priori
(such as the accuracy ") or do not have an intuitive interpretation (such
as C). On the other hand, desirable properties of previous SV algorithms,
including the formulation as a denite quadratic program, and the sparse
SV representation of the solution, are retained. We are optimistic that in
many applications, the new algorithms will prove to be quite robust. Among
these should be the reduced set algorithm of Osuna and Girosi (1999), which
approximates the SV pattern recognition decision surface by "-SVR. Here,
-SVR should give a direct handle on the desired speed-up.

Future work includes the experimental test of the asymptotic predictions
of section 4 and an experimental evaluation of -SV classication on real-
world problems. Moreover, the formulation of efcient chunking algorithms
for the -SV case should be studied (cf. Platt, 1999). Finally, the additional
freedom to use parametric error models has not been exploited yet. We
expect that this new capability of the algorithms could be very useful in
situations where the noise is heteroscedastic, such as in many problems of
nancial data analysis, and general time-series analysis applications (M uller
et al., 1999; Mattera & Haykin, 1999). If a priori knowledge about the noise
is available, it can be incorporated into an error model  ; if not, we can try to
estimate the model directly from the data, for example, by using a variance
estimator (e.g., Seifert, Gasser, & Wolf, 1993) or quantile estimator (section 3).

Acknowledgments

This work was supported in part by grants of the Australian Research Coun-
cil and the DFG (Ja 379/7-1 and Ja 379/9-1). Thanks to S. Ben-David, A. Elis-
seeff, T. Jaakkola, K. M uller, J. Platt, R. von Sachs, and V. Vapnik for discus-
sions and to L. Almeida for pointing us to Whites work. Jason Weston has
independently performed experiments using a sum inequality constraint
on the Lagrange multipliers, but declined an offer of coauthorship.

