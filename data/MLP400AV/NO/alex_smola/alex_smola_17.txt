Abstract

Learning from structured data is becom-
ing increasingly important. However, most
prior work on kernel methods has focused
on learning from attribute-value data. Only
recently, research started investigating ker-
nels for structured data. This paper consid-
ers kernels for multi-instance problems - a
class of concepts on individuals represented
by sets. The main result of this paper is a ker-
nel on multi-instance data that can be shown
to separate positive and negative sets under
natural assumptions. This kernel compares
favorably with state of the art multi-instance
learning algorithms in an empirical study. Fi-
nally, we give some concluding remarks and
propose future work that might further im-
prove the results.

1. Introduction

Support vector machines (SVM) and other kernel
methods (Boser et al., 1992; Scholkopf & Smola, 2002)
have successfully been applied to various tasks in
attribute-value learning. Most real-world data, how-
ever, has no natural representation as a tuple of con-
stants. Dening kernels on individuals that can not
easily be described by a single feature vector means
crossing the boundary between attribute-value and re-
lational learning. It allows kernel methods to be ap-
plied more easily to complex representation spaces.
Multi-instance (MI) learning problems (Dietterich
et al., 1997) occur whenever example objects, indi-

viduals, can not be described by a single characteristic
feature vector, but by a bag of vectors. Any of these
vectors could be responsible for the classication of the
bag. The inherent diculty of MI problems is to iden-
tify the characteristic element of each bag. Extending
kernel methods to MI problems is one step towards
crossing the boundary between attribute-value and re-
lational learning.
The main result of this paper is a kernel on MI data
that can be shown to separate positive and negative
sets under natural assumptions. Empirical studies
compare this kernel to other MI learning algorithms.
On one hand, an SVM using this kernel is compared
to state of the art MI algorithms. On the other hand,
it is compared to other methods that can be used to
apply SVMs to MI data. In both studies the kernel
proposed in this paper compares favorably.
Section 2 introduces the MI setting under various as-
pects. Section 3 summarizes prior work on kernels
for discrete spaces, in particular the work of Haus-
sler (1999) and Gartner (2000). Section 4 gives an
account of the separability of MI problems in kernel
feature space and devises a suitable kernel. Section 5
discusses an alternative way to apply attribute-value
learning algorithms to MI problems. After that we
empirically evaluate our kernel on the musk-dataset,
and conclude with some nal remarks.

2. The Multi-Instance Setting

MI problems have been introduced under this name
by Dietterich et al. (1997). However, similar problems
and algorithms have been considered earlier, for exam-

ple in pattern recognition (Keeler et al., 1991). Within
the last couple of years, several approaches have been
made to upgrade attribute-value learning algorithms
to tackle MI problems. Other approaches focused on
new algorithms specically designed for MI learning.
Formally, concepts are functions I : X  , where
X is often referred to as the instance space or prob-
lem domain (examples are elements of this domain)
and  = {(cid:62),} are the labels (in what follows  al-
ways means {(cid:62),}). There are 2|X| concepts on the
instance space X . A function f : X  R is said to
separate the concept if f(x) > 0  I(x).
If examples are represented by subsets of some domain
X concepts are functions set : 2X  . There are
dierent concepts on sets (in other words: set 
22
1C, C  X ). Such concepts are sometimes referred to
as multi-part concepts. MI concepts are a specic kind
of these concepts.

|X|

Denition 2.1 An MI concept is a function MI
2X  . It is dened as:

:

MI(X)   x  X : I(x)

where I is a concept over an instance space (referred
to as the underlying concept), and X  X is a set.
There are 2|X| dierent MI concepts. The diculty in
this task is not just to generalize beyond examples, but
also identifying the characteristic element of each bag.
Any learning algorithm that sees a positive bag (a bag
with label (cid:62)) cannot infer much about the elements
of the bag, except that one of its elements is positive
in the underlying concept. With large bag sizes this
information is of limited use.
A popular real-world example of an MI problem is
the prediction of drug activity, described by Dietterich
et al. (1997). A drug is active if it binds well to en-
zymes or cell-surface receptors. The binding strength
is determined by the shape of the drug molecule. How-
ever, most molecules can change their shape by rotat-
ing some of their internal bonds. The possible shapes
of a molecule, i.e., a combination of the angles of the
rotatable bonds of the molecule, are known as confor-
mations. A drug binds well to enzymes or cell-surface
receptors if one of its conformations binds well. Thus
the drug activity prediction problem is an MI problem.
Apart from approaches that ignore the MI setting dur-
ing training, two major categories of approaches can
be distinguished: Upgrading existing attribute-value
learning algorithms, and designing a new learning al-
gorithm specically for MI problems. Algorithms that
have specically been designed for MI problems are:

The axis-parallel rectangles (APR) algorithm and vari-
ants (Dietterich et al., 1997), an algorithm based on
simple statistics of the bags (Auer, 1997), and algo-
rithms based on the diverse density approach (Maron
& Lozano-Perez, 1998; Zhang & Goldman, 2002). Al-
gorithms that have been upgraded until now are: the
lazy learning algorithms Bayesian-kNN and Citation-
kNN (Wang & Zucker, 2000), the neural network MI-
NN (Ramon & De Raedt, 2000), the decision tree
learner RELIC (Ruo, 2001), and the rule learner
(Naive)RipperMI (Chevaleyre & Zucker, 2001).
In-
ductive logic programming algorithms have also been
used, for instance, the rst-order decision tree learner
TILDE (Blockeel & De Raedt, 1998).
Of the aforementioned approaches, the APR algorithm
is the classical approach for MI learning. It assumes
that the features of the conformations are independent
and thus orthogonal. We then seek an axis-aligned
(hyper-) rectangle covering at least one element of each
positive bag and none of a negative bag.

3. Kernels on Discrete Spaces

In SVMs and other kernel methods the only informa-
tion needed about instances is an inner product in
some feature space - a positive denite Mercer ker-
nel k(., .) = (cid:104)(.), (.)(cid:105), where (cid:104)., .(cid:105) denotes the inner
product and  is a feature transformation. Below we
summarize prior work on kernels for discrete spaces
that is most relevant in our context. For brevity, be-
low a kernel always means a Mercer kernel.

Convolution Kernels The best known kernel for
representation spaces that are not mere attribute value
tuples, is the convolution kernel proposed by Haussler
(1999). It is dened as:

kconv(x, y) =

kd(xd, yd)

(cid:126)xR1(x),(cid:126)yR1(y)

(cid:88)

D(cid:89)

d=1

where R is a relation between instances z and their
1 decomposes an instance into a set of
parts (cid:126)z, i.e., R
D-tuples. zd denotes the d-th component of (cid:126)z, and kd
is a valid kernel function on this component.
The term convolution kernel refers to the class of ker-
nels given by the above denition. The advantage of
convolution kernels is that they are very general and
can be applied in many dierent problems. However,
because of that generality they require a signicant
amount of work to adapt them to a specic problem,
which makes choosing R a non-trivial task.
More specic kernels on discrete spaces can be found
in Gartner (2000). There, kernels for elementary sym-

bols (k - the matching kernel), sets and multi-sets of
elementary symbols (kset and kmultiset), and Boolean
domains (k - and a polynomial variant) are discussed
along with concept classes that can be separated by
linear classiers using one of these kernels. The spe-
cic kernels are described in more detail below.

3.1 Set Kernels

A kernel on sets can easily be derived form the de-
nition of convolution kernels by letting R be the set-
membership function, i.e., with (cid:126)z  R
1(Z)  (cid:126)z 
Z, D = 1 and for notational convenience z1 = z, k1 = 
we obtain:

kset(X, X

(cid:48)) := (cid:88)
xX,x(cid:48)X(cid:48)
where  is a kernel on X , and X, X
lar, Haussler proved the following theorem:

(cid:48))
kX (x, x

(cid:48)  X . In particu-

(1)

Proposition 3.1 A function k on sets dened by (1)
is a kernel if and only if  itself is a kernel.
For instance, if x, y  X are elementary symbols (con-
stants - no internal structure) a natural choice for a
kernel is the matching kernel, dened as:

k(x, y) = x,y

Since this kernel induces a Hilbert space on (cid:96)2(X ),
assuming X is countable, all possible concepts
(x)  x  C,C  X on this representation space can
be separated using the matching kernel. Clearly for
 = k

kset(X, Y ) = (cid:88)

xX,yY

k(x, y) = |X  Y |.

3.2 Cosets

Next we extend the summation operation over sets to
more general structures on X by using the notion of
cosets. Assume that there exists a set Y (the coset) as-
sociated with X . Then we can represent sets and their
generalizations as mappings X  Y. The following
examples explain the use of this denition:
Sets: here Y = {0, 1} and the map X : x  y indi-
cates whether x  X (in which case y = 1) or not.
Multisets: setting Y = N  {0} allows us to extend
the notion of sets to multisets, i.e., sets X which may
contain elements several times. Here y = X(x) de-
notes the number of elements x contained in X, and,
as before, y = 0 indicates that x (cid:54) X.
Measures: we may extend the notion of cosets to
general measures on X by letting Y = [0,). Such

a denition would also allow us to cater for situations
where class membership is described in more vague
terms, as common in Fuzzy sets (here Y = [0, 1]).
Now that we extended our denition of sets, we need
to dene a kernel on it.
In general, any kernel on
the space of functions X : X  Y will satisfy the
formal requirements. For practical purposes, however,
we consider a class of kernels given by

kX (x, x

(cid:48))kY(X(x), X

(cid:48)(x

(cid:48))).

kset(X, X

(cid:48)) = (cid:88)

xX ,x(cid:48)X

is

(cid:48)(x

the
case
(cid:48))) = X(x)X
(cid:48)(x

(2)
where
A
special
kY(X(x), X
It is easy to
check that (2) reduces to the kernel dened in (1) in
the case of sets and in the case of multisets to a kernel
where the summation ranges over all elements of the
multiset.

situation

(cid:48)).

3.3 Normalization

It is useful to note that (1) corresponds to an averag-
ing process which is carried out in feature space, or in
other words, given a kernel on all elements xij  Xi
on all sets Xi, the kernel matrix on Xi is obtained
by summing up corresponding rows and columns of a
larger matrix.
Such a procedure indicates that if the cardinalities
of Xi vary considerably, sets with a large cardinality
will dominate the solution of the estimation problem
(as can be observed in the experiments on the Musk
dataset). This is not always desirable, which leads us
to the issue of normalization. A natural denition in
this regard is

k(X, X

(cid:48)) :=

(cid:48))

kset(X, X

fnorm(X)fnorm(X(cid:48))

(3)

where fnorm(X) is a suitable normalization function
(cid:48)) (cid:54)= 0. Be-
which is nonnegative for any kset(X, X
low we see that various choices of fnorm(X) will lead
to feature-space normalization, sample normalization,
variance rescaling, etc.
Feature-space normalization: We simply set

fnorm(X) :=(cid:112)kset(X, X).

(4)

Thus we recover the normalization proposed by (Her-
brich, 2002), which proves to be very useful in MI
learning.
Averaging: In this case we need to compute the gen-
eralized cardinality of the set X, which can be achieved
via

X(x).

(5)

fnorm(X) := (cid:88)

xX

For simple sets this just means that we average the
instances mapped into feature space. A minor mod-

ication is to use (cid:112)fnorm(X) instead, which will be

more useful if most of the averages are close to 0.

4. Separating MI Problems

In this section we dene a kernel function on sets of
instances that separates MI problems under natural
assumptions. Using this kernel function, SVMs and
other kernel methods can easily be applied to MI prob-
lems. We begin by introducing the notion of separa-
bility.
Separability A concept I : X   is called lin-
early separable in input space if

 x  X : (cid:104)x, c(cid:105)    I(x)

holds for some constant c and threshold . Likewise,
a concept is called linearly separable with respect to a
feature map  if

 x  X : (cid:104)(x), c(cid:105)    I(x)

holds for some c  span{(x)|x  X}. In what fol-
lows we often use the term separable to refer to con-
cepts that are linearly separable with respect to a fea-
ture transformation given implicitly by a kernel.
For our purposes we further need the notion of a con-
cept that is separable with non-zero margin and some
lower bound.

Denition 4.1 We call a concept separable with mar-
gin  > 0 with respect to the feature map , if there
exist c, b such that for every x  X :

I(x) 
I(x)  0  (cid:104)(x), c(cid:105) + b  1  .

(cid:104)(x), c(cid:105) + b  1

(6)

Note that we can, without loss of generality, assume
that b = 0, since for given , c, b the map x 
((x), 1) together with (c, b) will satisfy the above
condition without a constant oset. Furthermore, any
separable concept on a nite domain (or example set)
is separable with non-zero margin and some lower
bound on this domain (or example set).
We will now dene a kernel kMI that separates MI con-
cepts. This kernel is a variant of the set kernel, as de-
ned in (1). It will be useful in proving necessary and
sucient conditions for separability of MI problems.

kMI(X, Y ) = (cid:88)

xX,yY

kp
I (x, y)

(7)

where p  N is a constant. Since products of kernels
are kernels, also kp
I (x, y) is a kernel, and consequently
also kMI(X, Y ) is a kernel.
In order to show that MI concepts are separable if and
only if the underlying concept is separable, we need
the following two lemmas.

Lemma 4.2 An MI concept MI is separable with the
kernel kMI, as dened in (7), for suciently large p, if
the underlying concept I is separable with the kernel
kI and some , according to (6).

Proof: Consider rst the case that I = 1X , i.e., each
instance satises the concept. Then it follows trivially
from the denition of MI concepts that MI = 12X . For
the remainder of the proof we can thus assume that at
least one instance does not satisfy the concept.
Let now p > 0 if  = 1, and p >  log m
log(1) otherwise.
Here m is a bound on the cardinality of the bags X.
Since the concept is separable, there exists a c satis-
fying (6). Now consider the function
(cid:104)(x), c(cid:105)p.

(8)
One can see that f(x) < 1 if and only if no x  X
satises I: in this case, we have

f(X) = (cid:88)

xX

f(X)  m(1  )p < m(1  )

 log m

log(1) = 1

The nal step consists of showing that f can be writ-
ten as a dot product in the feature space induced by
kMI. Clearly f({x}) is a function in the space of dot
(cid:48))p. Next note that kMI is
products induced by k(x, x
the space of linear combinations of such functions on
X, and clearly f, being a monomial, is one of such
(cid:3)
linear combinations.
Example 4.1 Let X = 4 = {(cid:62),}4, x  X , x =
(x1, x2, x3, x4), I(x)  x2  x4, and
kI(x, y) = k(x, y) = (cid:88)

1

xi=yi=(cid:62)

Then m = 16, as maxi |Xi|  |X| = 24 = 16, and
 = 1/2. Here we have p = 5 > log 16/log2.

The following example illustrates that the simple set-
kernel separates MI concepts on discrete sets.
Example 4.2 Let X = {a, b, c, d}, C = {a, c},
I(x)  x  C, and kI(x, y) = k(x, y). Then  = 1,
m = 4 as maxi |Xi|  |X| = 4), and p = 1 > 0. It
follows that:
kMI (X, Y ) = (cid:88)

k(x, y) = |X  Y | = kset(X, Y )

xX,yY

It follows directly from the lemma above and from the
denition of convolution kernels (Haussler, 1999) that
(for nite example sets) MI concepts can be separated
with convolved Gaussian RBF kernels if the underlying
concept can be separated with Gaussian RBF kernels,
since in this case kp
is a Gaussian RBF kernel itself,
I
albeit with width 2/p instead of 2. In this case the
MI kernel does not require an additional parameter to
be chosen. Also note that for RBF kernels (6) always
holds with b = 0. We further need

Lemma 4.3 If an MI concept MI is separable then
the underlying concept I is separable.

Proof: Say fMI(Z) is a function that separates posi-
tive and negative bags (fMI(Z) >   MI(Z)). Then
fI(z) = fMI({z}) is a function that separates the un-
(cid:3)
derlying concept (with fI(z) >   I(z)).
It is now easy to show:

Theorem 4.4 An MI concept MI is separable by kMI
with a non-zero margin if and only if the underlying
concept I is separable by the kernel kI with a non-zero
margin.

Proof: In Lemma 4.2 the margin of the MI problem
is MI = 1  m(1  )p > 0. Furthermore, assuming
a margin  on fMI in Lemma 4.3, a margin of  can
be found on fI. The lower bounds are maintained
(cid:3)
similarly.
Note that the MI kernel can also be used in distance-
based algorithms such as k-nearest neighbors. The
distance metric is then dened on the kernel in the

standard manner d(x, y) =(cid:112)(cid:104)x, x(cid:105)  2(cid:104)x, y(cid:105) + (cid:104)y, y(cid:105).
Example 4.3 Consider the kernel kMI (X, Y ) = |X 
Y | for solving MI problems on discrete sets based on
the matching kernel (see Example 4.2). In such a case,
the corresponding distance metric can be derived easily
as the symmetric dierence d(X, Y ) = |X (cid:52) Y |.

5. Learning Ray Concepts

Having shown in the previous section that MI prob-
lems can be separated with our MI kernel, we will now
describe a simple approach that can be used to apply
any propositional learning algorithm to MI problems.
The motivation is based on some observations in the
drug activity prediction domain. The advantage of this
approach is the eciency - in real world drug activity
prediction problems bags can be huge which renders
the computation of kMI too expensive. In the empir-
ical evaluation of this method (see section 6) we will
show that - in spite of the simplicity of this approach -

an SVM using this kernel can outperform several other
MI learning algorithms.

If we can make further assumptions on
Statistics
the properties of X , such as being generated by a nor-
mal distribution, by a mixture thereof, or other prop-
erties that can be summarized in a compact fashion
(cid:48)  X can be useful
then computing statistics on X, X
in dening kernels on sets.
Denition 5.1 Denote by s : X  s(X) a map com-
puting statistics on X  X . Then we call
(cid:48)) := k(s(X), s(X
(cid:48)))

kstat(X, X

(9)

the statistic kernel.

Here s(X) is a collection of properties of the set, say
the mean, median, maximum, minimum, etc. Typi-
cally, s(X) will be a vector of real numbers.
A similar approach has been used in the context of in-
ductive logic programming (Krogel & Wrobel, 2001),
where relational aggregations are used to compute
statistics over a database.
It is not uncommon in drug activity prediction to rep-
resent a molecule by a bag of descriptions of its dier-
ent conformations. Each conformation is in turn de-
scribed by a feature vector such that each component
of the vector corresponds to one ray emanating from
the origin and measuring the distance to the molecule
surface (Dietterich et al., 1997).
It is often believed
that the concept space of drug activity prediction can
be described by putting upper and lower bounds along
each ray.
Consider MI rays, i.e., concepts on sets of real numbers
such that rM I(X)   x  X : x  . These con-
cepts are the complement (negation) of upper bounds.
Motivated by the observation that rM I can be
learned using only the maximal element of the set,
we nd the following statistics kernel particularly in-
teresting for drug activity prediction:

Example 5.1 (Minimax Kernel) Dene s to be
the vector of the coordinate-wise maxima and minima
of X, i.e.,

s(X) = (min
xX

x1, . . . , min
xX

xm, max
xX

x1, . . . , max
xX

xm).

In our experiments we used s(X) combined with poly-
(cid:48))(cid:105) + 1)p.
nomial kernels, i.e., k(X, X
The minimax kernel is related to the MI kernel as kMI
can be seen as a soft max function, while minimax cor-
responds to a component-wise min and max function.

(cid:48)) = ((cid:104)s(X), s(X

6. Drug Activity Prediction

Often drug activity prediction problems are used to
asses MI learning algorithms, most prominently the
Musk data set (Dietterich et al., 1997). The problem
consists of predicting the strength of synthetic musk
molecules. The class labels have been found by human
domain experts. Two overlapping data sets are avail-
able. Musk1 contains 47 molecules labeled as Musk
(if the molecule is known to smell musky) and 45 la-
beled as Non-Musk. The 92 molecules are altogether
described by 476 conformations. Musk2 contains 39
Musk molecules and 45 Non-Musk molecules, de-
scribed by 6598 conformations altogether.
Several empirical results on these two data sets have
been achieved and reported in literature. The results
in Table 1 are in alphabetical order. They have either
been obtained by multiple tenfold cross-validation runs
(10CV) or by leave-one-out estimation (LOO). The
best classication results from each section are marked
in boldface. The table is organized as follows: The rst
section contains algorithms specically designed for MI
learning. The second one contains algorithms that are
designed as general purpose learning algorithms, but
have been adapted to learn MI problems. The third
section contains algorithms that have been run on the
musk data using the minimax feature space described
above, and the forth section contains results achieved
by ignoring the MI setting while learning but obeying
it when testing. For the SVM, an RBF kernel was used
with  = 106. Boosted NB (DT) refers to a boosted
naive Bayes (decision tree) classier.
The fth section of the table contains results from us-
ing SVMs with a polynomial version of the minimax
kernel. The sixth section contains results for SVMs
using MI kernels. Due to the extremely small sam-
ple size (MUSK1 contains only 96 bags and Musk2
only 104) which is furthermore not divisible by 10,
10-fold cross-validation is a very noisy and unreliable
process. To address this problem, we opted to com-
pute an error estimate by averaging over 1000 trials of
randomly leaving out 10 instances. The advantage of
this approach is that in addition to the error estimates
we also obtain condence intervals, which allows us to
compare our results with the ones obtained in the lit-
erature. Finally, for the sake of comparability, also the
leave-one-out error was computed.
We chose a Gaussian RBF kernel for the dot prod-
uct on the elements of the bag, using the rule of
thumb that  should be in the order of magnitude
2d2  105 or lower, where d is the dimensionality
of

1

Table 1. Classication errors (in %) on Musk1 and Musk2

Algorithm

Musk1 Musk2 Eval.

EM-DD
Gfs kde Apr
Iterative Apr
MaxDD
Multinst
Bayesian-kNN
Citation-kNN
Mi-nn
NaiveRipperMI
Relic
RipperMI
Tilde

3.2
8.7
7.6
11.1
23.3
9.8
7.6
12.0
12.0
16.3
12.0
13.0

4.0
19.6
10.8
17.5
16.0
17.6
13.7
18.0
23.0
12.7
23.0
20.6

10Cv
10Cv
10Cv
10Cv
10Cv
LOO
LOO

?
?

10Cv

?

10Cv

Algorithm

Musk1

Musk2

LOO 10-out LOO 10-out

18.5
12.0
12.0

19.6
14.1
14.1

Minimax Feature Space
Decision Tree
Boosted DT
Boosted NB
Ignoring the MI Setting while Training
Decision Tree
Boosted DT
Boosted NB
SVM

28.3
18.5
20.3
13.0

19.6
20.6
16.7

17.6
20.6
16.7

43.1
28.4
36.7
18.6

Kernel

Musk1

Musk2

LOO

10-out LOO
Polynomial Minimax kernel
8.4  0.7
p = 5
15.5  2.2
LOO
17.5  2.1
LOO, normalized
(cid:48)(cid:107)2))
) = exp((cid:107)x  x
MI kernel (kI (x, x
13.6  1.1
13.0
7.8
 = 10
15.0  1.9
LOO
19.0  2.7
LOO, normalized

5.5

13.7

10-out
13.7  1.2
17.5  2.1
18.5  1.9
12.0  1.0
19.0  2.2
14.5  2.4

7.6

(cid:48)

of the data1. This led to an almost optimal choice of
parameters (the optimum lay within an order of mag-
nitude of the initial estimate). As for preprocessing,
the data was rescaled to zero mean and unit variance
on a per coordinate basis. In the ray-kernel case, we
simply used a polynomial kernel on s(X).
In order
to avoid adjusting too many parameters, we chose the
-parameterization (Scholkopf et al., 2000), with  set
to 0.075. The latter corresponds to an error level com-
parable to the ones in the published literature.
Normalization proved to be critical: the un-normalized

sets, and the sets normalized by(cid:112)fnorm(X) performed

worst, whereas there was no signicant dierence in
performance between the kernels which employed nor-
malization in feature space and those with averaging in
feature space. This may be due to the fact that the av-
erage length of the sum of features of equal length may
1Note that the parameter p of MI kernels is chosen im-
plicitly when choosing ; as for Gaussian RBF kernels, kp
I
is a Gaussian RBF kernel itself.

be more strongly concentrated, which makes both ap-
proaches qualitatively equivalent. We only report re-
sults on the kernel with normalization in feature space.
To assess the true performance of the estimation pro-
cedure, one must not, however, x a set of parameters
and only then use a cross-validation step to assess the
error for the now xed set of parameters. We therefore
adjusted the parameters, such as kernel width and the
value of  and  for each leave-10-out sample sepa-
rately and computed the CV error for this procedure.
20 random leave-10-out samples were drawn. The cor-
responding results are reported in the fth block of
Table 1, denoted by LOO and LOO norm (the latter
refers to kernels with normalization in feature space).
It is rather obvious that the method degrades dramat-
ically due to the small sample size eects (only 100 ob-
servations). Also, the choice of suitable normalization
in feature space yields only diminishing returns, given
the high variance of the model selection procedure. It
is well known (Cherkassky & Mulier, 1998) that the
choice of model selection rules has a signicant inu-
ence on the performance of the overall estimator, quite
often more than the choice of the estimator itself.
The fact that polynomial minimax kernels outper-
formed the MI kernel (Gaussian RBF) on Musk1 can
be explained by the fact that Musk1 contains much
fewer conformations per bag than Musk2, hence the
min and max statistic s(X) is a quite adequate descrip-
tion of each bag. A small bag size is a major shortcom-
ing also of other (synthetically generated) datasets,
making them very unrealistic - as to our understand-
ing real-world drug activity prediction problems usu-
ally involve big bags.
While EM-DD (Zhang & Goldman, 2002) is superior
on both datasets, among the algorithms that are based
on general purpose learning methods, SVMs with MI
kernels outperform all other methods. We conjecture
that replacing the averaging process over the data in
feature space by a weighted average taking an estimate
of the class of each element in the bag into account
would signicantly improve generalization.
We consider now another drug activity prediction
problem - predicting the mutagenicity of molecules.
The original dataset (Srinivasan et al., 1996), de-
scribed by a set of prolog predicates, has widely
been used in the inductive logic programming com-
munity. The only time this dataset has been used
with MI learning algorithms is described in (Cheva-
leyre & Zucker, 2001). While the basic dataset is de-
scribed by two relations, the atoms of the molecule
and the bonds between the atoms, other representa-
tions include global molecular descriptors. Two sets of

instances are frequently used, the so called friendly
dataset containing 188 instances, and the unfriendly
dataset containing 42 instances. We consider only ex-
periments without global molecular features and rep-
resent each molecule by set of bonds together with the
two adjacent atoms. This setup is similar to the one
described in (Chevaleyre & Zucker, 2001) where error
rates between 18.0% (RipperMI) and 39.0% (Foil)
are reported on the friendly dataset. So far, no re-
sults on the unfriendly dataset have been reported
using MI algorithms. Using our MI kernel and the
data description without any global molecular descrip-
tors we are able to achieve error rates of 7.0% on the
friendly dataset and 25% on the unfriendly dataset.
As before, we converted the data to zero mean and unit
variance. Symbolic values were converted to orthonor-
mal vectors. Model selection ( and ) was carried out
by LOO cross-validation, and the results are reported
for an average over 20 random leave-10-out subsets.

7. Conclusions and Future Work

In this paper we demonstrated a successful approach
to extend the applicability of SVMs to data other than
mere attribute-value tuples. It has been mentioned in
literature that MI problems capture most of the com-
plexity of relational learning problems. Therefore, MI
kernels are an important step in crossing the boundary
between successful attribute-value learning and rela-
tional learning.
We dened a general kernel for MI problems, proved
that it separates MI concepts under natural assump-
tions, and showed its performance on benchmark data.
Favorable for our approaches are the high accuracy and
the simplicity with which other kernel methods can
now be extended to MI problems. For example, by
simply plugging our kernel into SVM regression, the
only very recently formulated problem of MI regres-
sion can be tackled (Amar et al., 2001; Ray & Page,
2001). Clustering and feature extraction tasks have to
the best of our knowledge not yet been investigated for
MI data. Support vector clustering and kernel princi-
pal component analysis algorithms can now easily be
applied to MI data by using our kernel function. Other
distance-based algorithms can be applied to MI prob-
lems by dening a distance on the kernel in the stan-
dard manner. These are promising topics for future
research.
Finally, the idea of labeling the elements of the bags,
as done in EM-DD indicates further space for improve-
ment, e.g., by modifying the averaging process, that is

currently carried out within the bags ((cid:80)xX (x)) into

a weighted average, where preference is given to ele-

ments which are close to elements of other bags with
the same class label. The kernel function would be a
simple extension of the coset kernel and our MI kernel.

Acknowledgments

Research supported in part by the Esprit V project
(IST-1999-11495) Data Mining and Decision Support
for Business Competitiveness: Solomon Virtual En-
terprise and a grant of the ARC. The authors thank
Tamas Horvath, Thorsten Joachims, John Lloyd, Bob
Williamson, and Stefan Wrobel for valuable discus-
sions and/or comments.

