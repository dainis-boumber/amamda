Abstract

We describe a Markov chain method for sampling from the distribution
of the hidden state sequence in a non-linear dynamical system, given a
sequence of observations. This method updates all states in the sequence
simultaneously using an embedded Hidden Markov Model (HMM). An
update begins with the creation of pools of candidate states at each
time. We then dene an embedded HMM whose states are indexes within
these pools. Using a forward-backward dynamic programming algo-
rithm, we can efciently choose a state sequence with the appropriate
probabilities from the exponentially large number of state sequences that
pass through states in these pools. We illustrate the method in a simple
one-dimensional example, and in an example showing how an embed-
ded HMM can be used to in effect discretize the state space without any
discretization error. We also compare the embedded HMM to a particle
smoother on a more substantial problem of inferring human motion from
2D traces of markers.

1 Introduction

Consider a dynamical model in which a sequence of hidden states, x = (x0; : : : ; xn(cid:0)1), is
generated according to some stochastic transition model. We observe y = (y0; : : : ; yn(cid:0)1),
with each yt being generated from the corresponding xt according to some stochastic ob-
servation process. Both the xt and the yt could be multidimensional. We wish to randomly
sample hidden state sequences from the conditional distribution for the state sequence given
the observations, which we can then use to make Monte Carlo inferences about this poste-
rior distribution for the state sequence. We suppose in this paper that we know the dynamics
of hidden states and the observation process, but if these aspects of the model are unknown,
the method we describe will be useful as part of a maximum likelihood learning algorithm
such as EM, or a Bayesian learning algorithm using Markov chain Monte Carlo.

If the state space is nite, of size K, so that this is a Hidden Markov Model (HMM), a
hidden state sequence can be sampled by a forward-backwards dynamic programming al-
gorithm in time proportional to nK 2 (see [5] for a review of this and related algorithms).
If the state space is <p and the dynamics and observation process are linear, with Gaussian
noise, an analogous adaptation of the Kalman lter can be used. For more general models,

or for nite state space models in which K is large, one might use Markov chain sampling
(see [3] for a review). For instance, one could perform Gibbs sampling or Metropolis up-
dates for each xt in turn. Such simple Markov chain updates may be very slow to converge,
however, if the states at nearby times are highly dependent. A popular recent approach is
to use a particle smoother, such as the one described by Doucet, Godsill, and West [2], but
this approach can fail when the set of particles doesnt adequately cover the space, or when
particles are eliminated prematurely.

In this paper, we present a Markov chain sampling method for a model with an arbitrary
state space, X , in which efcient sampling is facilitated by using updates that are based
on temporarily embedding an HMM whose nite state space is a subset of X , and then
applying the efcient HMM sampling procedure. We illustrate the method on a simple
one-dimensional example. We also show how it can be used to in effect discretize the state
space without producing any discretization error. Finally, we demonstrate the embedded
HMM on a problem of tracking human motion in 3D based on the 2D projections of marker
positions, and compare it with a particle smoother.

2 The Embedded HMM Algorithm

In our description of the algorithm, model probabilities will be denoted by P , which
will denote probabilities or probability densities without distinction, as appropriate for
the state space, X , and observation space, Y. The models initial state distribution is
given by P (x0), transition probabilities are given by P (xt j xt(cid:0)1), and observation prob-
abilities are given by P (yt j xt). Our goal is to sample from the conditional distribution
P (x0; : : : ; xn(cid:0)1 j y0; : : : ; yn(cid:0)1), which we will abbreviate to (cid:25)(x0; : : : ; xn(cid:0)1), or (cid:25)(x).
To accomplish this, we will simulate a Markov chain whose state space is X n  i.e., a
state of this chain is an entire sequence of hidden states. We will arrange for the equilib-
rium distribution of this Markov chain to be (cid:25)(x0; : : : ; xn(cid:0)1), so that simulating the chain
for a suitably long time will produce a state sequence from the desired distribution. The
state at iteration i of this chain will be written as x
n(cid:0)1). The transition
probabilities for this Markov chain will be denoted using Q. In particular, we will use some
initial distribution for the state of the chain, Q(x
(0)), and will simulate the chain according
to the transition probabilities Q(x
(i(cid:0)1)). For validity of the sampling method, we
need these transitions to leave (cid:25) invariant:

0 ; : : : ; x(i)

(i) = (x(i)

(i) j x

(cid:25)(x

0) = X
x 2 X n

(cid:25)(x)Q(x

0 j x);

for all x

0 in X n

(1)

(If X is continuous, the sum is replaced by an integral.) This is implied by the detailed
balance condition:

(cid:25)(x)Q(x

0 j x) = (cid:25)(x

0)Q(x j x

0);

for all x and x

0 in X n

(2)

(i) j x

The transition Q(x
(i(cid:0)1)) is dened in terms of pools of states for each time. The
current state at time t is always part of the pool for time t. Other states in the pool are
produced using a pool distribution, (cid:26)t, which is designed so that points drawn from (cid:26)t are
plausible alternatives to the current state at time t. The simplest way to generate these
additional pool states is to draw points independently from (cid:26)t. This may not be feasible,
however, or may not be desirable, in which case we can instead simulate an inner Markov
chain dened by transition probabilities written as Rt((cid:1) j (cid:1)), which leave the pool distribu-
tion, (cid:26)t, invariant. The transitions for the reversal of this chain with respect to (cid:26)t will be
denoted by ~Rt((cid:1) j (cid:1)), and are dened so as to satisfy the following condition:

(cid:26)t(xt)Rt(x0

t j xt) = (cid:26)t(x0

t) ~Rt(xt j x0

t);

for all xt and x0

t in X

(3)

If the transitions Rt satisfy detailed balance with respect to (cid:26)t, ~Rt will be the same as
Rt. To generate pool states by drawing from (cid:26)t independently, we can let Rt(x0jx) =
~Rt(x0jx) = (cid:26)t(x0). For the proof of correctness below, we must not choose (cid:26)t or Rt based
on the current state, x

(i), but we may choose them based on the observations, y.

To perform a transition Q to a new state sequence, we begin by at each time, t, producing
a pool of K states, Ct. One of the states in Ct is the current state, x(i(cid:0)1)
; the others are
produced using Rt and ~Rt. The new state sequence, x
(i), is then randomly selected from
among all sequences whose states at each time t are in Ct, using a form of the forward-
backward procedure.

t

In detail, the pool of candidate states for time t is found as follows:

1) Pick an integer Jt uniformly from f0; : : : ; K (cid:0)1g.
2) Let x[0]
3) For j from 1 to Jt, randomly pick x[j]

t = x(i(cid:0)1)

t

. (So the current state is always in the pool.)

t

according to the transition probabilities

Rt(x[j]

t

j x[j(cid:0)1]

t

).

4) For j from (cid:0)1 down to (cid:0)K + Jt + 1, randomly pick x[j]

t according to the reversed

transition probabilities, ~Rt(x[j]

t

5) Let Ct be the pool consisting of x[j]

t

).

j x[j+1]
t , for j 2 f(cid:0)K+Jt+1; : : : ; 0; : : : ; Jtg. If some

of the x[j]

t are the same, they will be present in the pool more than once.

Once the pools of candidate states have been found, a new state sequence, x
(i), is picked
from among all sequences, x, for which every xt is in Ct. The probability of picking
(i) = x is proportional to (cid:25)(x)=Qn(cid:0)1
x

t=0 (cid:26)t(xt), which is proportional to

The division by Qn(cid:0)1
t=0 (cid:26)t(xt) is needed to compensate for the pool states having been drawn
from the (cid:26)t distributions. If duplicate states occur in some of the pools, they are treated
as if they were distinct when picking a sequence in this way. In effect, we pick indexes of
states in these pools, with probabilities as above, rather than states themselves.

The distribution of these sequences of indexes can be regarded as the posterior distribu-
tion for a hidden Markov model, with the transition probability from state j at time t(cid:0)1
to state k at time t being proportional to P (x[k]
t(cid:0)1), and the probabilities of the hypo-
thetical observed symbols being proportional to P (yt j x[k]
t ). Crucially, using the
forward-backward technique, it is possible to randomly pick a new state sequence from this
distribution in time growing linearly with n, even though the number of possible sequences
grows as K n. After the above procedure has been used to produce the pool states, x[j]
for
t
t = 0 to n(cid:0)1 and j = (cid:0)K +Jt + 1 to Jt, this algorithm operates as follows (see [5]):

t )=(cid:26)t(x[k]

j x[j]

t

1) For t = 0 to n(cid:0)1 and for j = (cid:0)K +Jt +1 to Jt, let ut;j = P (yt j x[j]
2) For j = (cid:0)K +J0 +1 to J0, let w0;j = u0;j P (X0 = x[j]
3) For t = 1 to n(cid:0)1 and for j = (cid:0)K +Jt + 1 to Jt, let
wt(cid:0)1;k P (x[j]

j x[k]

0 ).

t(cid:0)1)

t

wt;j = ut;j Pk

t )=(cid:26)t(x[j]
t ).

4) Randomly pick sn(cid:0)1 from f(cid:0)K +Jn(cid:0)1 +1; : : : ; Jn(cid:0)1g, picking the value j with

probability proportional to wn(cid:0)1;j.

P (x0)Qn(cid:0)1

t=1 P (xt j xt(cid:0)1)Qn(cid:0)1

t=0 P (yt j xt)

Qn(cid:0)1
t=0 (cid:26)t(xt)

(4)

5) For t = n(cid:0)1 down to 1, randomly pick st(cid:0)1 from f(cid:0)K +Jt(cid:0)1 +1; : : : ; Jt(cid:0)1g,

picking the value j with probability proportional to wt(cid:0)1;j P (x[st]

t

j x[j]

t(cid:0)1).

Note that when implementing this algorithm, one must take some measure to avoid oating-
point underow, such as representing the wt;j by their logarithms.

Finally, the embedded HMM transition is completed by letting the new state sequence, x
be equal to (x[s0]

; x[s1]

; : : : ; x[sn(cid:0)1]
n(cid:0)1 )

0

1

(i),

3 Proof of Correctness

To show that a Markov chain with these transitions will converge to (cid:25), we need to show that
it leaves (cid:25) invariant, and that the chain is ergodic. Ergodicity need not always hold, and
proving that it does hold may require considering the particulars of the model. However,
it is easy to see that the chain will be ergodic if all possible state sequences have non-zero
probability density under (cid:25), the pool distributions, (cid:26)t, have non-zero density everywhere,
and the transitions Rt are ergodic. This probably covers most problems that arise in prac-
tice.

To show that the transitions Q((cid:1) j (cid:1)) leave (cid:25) invariant, it sufces to show that they satisfy
detailed balance with respect to (cid:25). This will follow from the stronger condition that the
probability of moving from x to x
0 (starting from a state picked from (cid:25)) with given values
for the Jt and given pools of candidate states, Ct, is the same as the corresponding proba-
bility of moving from x
0 to x with the same pools of candidate states and with values J 0
t
t in the
dened by J 0
candidate pool.

t = Jt (cid:0) ht, where ht is the index (from (cid:0)K + Jt + 1 to Jt) of x0

The probability of such a move from x to x
0 is the product of several factors. First, there is
the probability of starting from x under (cid:25), which is (cid:25)(x). Then, for each time t, there is the
probability of picking Jt, which is 1=K, and of then producing the states in the candidate
pool using the transitions Rt and ~Rt, which is

Jt

Y

j=1

Rt(x[j]

t

j x[j(cid:0)1]

t

) (cid:2)

(cid:0)1

Y

j=(cid:0)K+Jt+1

~Rt(x[j]

t

j x[j+1]

t

)

(cid:0)1

=

Jt(cid:0)1

Y

j=0

Rt(x[j+1]

t

j x[j]

t ) (cid:2)

Rt(x[j+1]

t

j x[j]
t )

Y

j=(cid:0)K+Jt+1

(cid:26)t(x[j]
t )
(cid:26)t(x[j+1]

t

)

(5)

(6)

Rt(x[j+1]

t

j x[j]
t )

(cid:26)t(x[(cid:0)K+Jt+1]

)

Jt(cid:0)1

t

=

Y
Finally, there is the probability of picking x
the pools, Ct, which is proportional to (cid:25)(x

(cid:26)t(x[0]
t )

j=(cid:0)K+Jt+1

(cid:25)(x) (cid:2)

1
K n (cid:2)

n(cid:0)1

Y

t=0

2
4

(cid:26)t(x[(cid:0)K+Jt+1]

t

)

(cid:26)t(x[0]
t )

0 from among all the sequences with states from
t). The product of all these factors is
0)=Q (cid:26)t(x0
Y

Rt(x[j+1]

j x[j]
t )

Jt(cid:0)1

(cid:2)

0)

t

(cid:25)(x
Qn(cid:0)1
t=0 (cid:26)t(x0
t)

3
5

j=(cid:0)K+Jt+1

=

1
K n

0)

(cid:25)(x)(cid:25)(x
Qn(cid:0)1
t)
t=0 (cid:26)(xt)(cid:26)(x0

n(cid:0)1

Y

t=0

2
4(cid:26)t(x[(cid:0)K+Jt+1]

t

Jt(cid:0)1

Y

)
j=(cid:0)K+Jt+1

Rt(x[j+1]

t

j x[j]
t )

3
5

(7)

We can now see that the corresponding expression for a move from x
apart from a relabelling of candidate state x[j]

t as x[j(cid:0)ht]

.

t

0 to x is identical,

4 A simple demonstration

The following simple example illustrates the operation of the embedded HMM. The state
space X and the observation space, Y, are both <, and each observation is simply the state
plus Gaussian noise of standard deviation (cid:27)  i.e., P (yt j xt) = N (yt j xt; (cid:27)2). The state
transitions are dened by P (xt j xt(cid:0)1) = N (xt j tanh((cid:17)xt(cid:0)1); (cid:28) 2), for some constant
expansion factor (cid:17) and transition noise standard deviation (cid:28) .
Figure 1 shows a hidden state sequence, x0; : : : ; xn(cid:0)1, and observation sequence,
y0; : : : ; yn(cid:0)1, generated by this model using (cid:27) = 2:5, (cid:17) = 2:5, and (cid:28) = 0:4, with
n = 1000. The state sequence stays in the vicinity of +1 or (cid:0)1 for long periods, with
rare switches between these regions. Because of the large observation noise, there is con-
siderable uncertainty regarding the state sequence given the observation sequence, with the
posterior distribution assigning fairly high probability to sequences that contain short-term
switches between the +1 and (cid:0)1 regions that are not present in the actual state sequence,
or that lack some of the short-term switches that are actually present.

We sampled from this distribution over state sequences using an embedded HMM in which
the pool distributions, (cid:26)t, were normal with mean zero and standard deviation one, and the
pool transitions simply sampled independently from this distribution (ignoring the current
pool state). Figure 2 shows that after only two updates using pools of ten states, embedded
HMM sampling produces a state sequence with roughly the correct characteristics. Figure 3
demonstrates how a single embedded HMM update can make a large change to the state
sequence. It shows a portion of the state sequence after 99 updates, the pools of states
produced for the next update, and the state sequence found by the embedded HMM using
these pools. A large change is made to the state sequence in the region from time 840 to
870, with states in this region switching from the vicinity of (cid:0)1 to the vicinity of +1.
This example is explored in more detail in [4], where it is shown that the embedded HMM
is superior to simple Metropolis methods that update one hidden state at a time.

5 Discretization without discretization error

A simple way to handle a model with a continuous state space is to discretize the space
by laying down a regular grid, after transforming to make the space bounded if necessary.
An HMM with grid points as states can then be built that approximates the original model.
Inference using this HMM is only approximate, however, due to the discretization error
involved in replacing the continuous space by a grid of points.

The embedded HMM can use a similar grid as a deterministic method of creating pools of
states, aligning the grid so that the current state lies on a grid point. This is a special case of
the general procedure for creating pools, in which (cid:26)t is uniform, Rt moves to the next grid
point and ~Rt moves to the previous grid point, with both wrapping around when the rst or
last grid point is reached. If the number of pool states is set equal to the number of points
in a grid, every pool will consist of a complete grid aligned to include the current state.

On their own, such embedded HMM updates will never change the alignments of the grids.
However, we can alternately apply such an embedded HMM update and some other MCMC
update (eg, Metropolis) which is capable of making small changes to the state. These small
changes will change the alignment of the new grids, since each grid is aligned to include the
current state. The combined chain will be ergodic, and sample (asymptotically) from the
correct distribution. This method uses a grid, but nevertheless has no discretization error.

We have tried this method on the example described above, laying the grid over the trans-
formed state tanh(xt), with suitably transformed transition densities. With K = 10, the
grid method samples more efciently than when using N (0; 1) pool distributions, as above.

5

0

5


0

200

400

600

800

1000

Figure 1: A state sequence (black dots) and observation sequence (gray dots) of length
1000 produced by the model with (cid:27) = 2:5, (cid:17) = 2:5, and (cid:28) = 0:4.

5

0

5


0

200

400

600

800

1000

Figure 2: The state sequence (black dots) produced after two embedded HMM updates,
starting with the states set equal to the data points (gray dots), as in the gure above.

6

4

2

0

2


4


6


820

840

860

880

900

920

940

Figure 3: Closeup of an embedded HMM update. The true state sequence is shown by
black dots and the observation sequence by gray dots. The current state sequence is shown
by the dark line. The pools of ten states at each time used for the update are shown as small
dots, and the new state sequence picked by the embedded HMM by the light line.

Figure 4: The four-second motion se-
quence used for the experiment, shown
in three snapshots with streamers show-
ing earlier motion. The left plot shows
frames 1-59, the middle plot frames 59-
91, and the right plot frames 91-121.
There were 30 frames per second. The
orthographic projection in these plots
is the one seen by the model. (These
plots were produced using Hertzmann
and Brands mosey program.)

6 Tracking human motion

We have applied the embedded HMM to the more challenging problem of tracking 3D
human motion from 2D observations of markers attached to certain body points. We con-
structed this example using real motion-capture data, consisting of the 3D positions at each
time frame of a set of identied markers. We chose one subject, and selected six markers
(on left and right feet, left and right hands, lower back, and neck). These markers were
projected to a 2D viewing plane, with the viewing direction being known to the model.
Figure 4 shows the four-second sequence used for the experiment.1
Our goal was to recover the 3D motion of the six markers, by using the embedded HMM
to generate samples from the posterior distribution over 3D positions at each time (the
hidden states of the model), given the 2D observations. To do this, we need some model
of human dynamics. As a crude approximation, we used Langevin dynamics with respect
to a simple hand-designed energy function that penalizes unrealistic body positions. In
Langevin dynamics, a gradient descent step in the energy is followed by the addition of
Gaussian noise, with variance related to the step size. The equilibrium distribution for this
dynamics is the Boltzmann distribution for the energy function. The energy function we
used contains terms pertaining to the pairwise distances between the six markers and to the
heights of the markers above the plane of the oor, as well as a term that penalizes bending
the torso far backwards while the legs are vertical. We chose the step size for the Langevin
dynamics to roughly match the characteristics of the actual data.

The embedded HMM was initialized by setting the state at all times to a single frame of
the subject in a typical stance, taken from a different trial. As the pool distribution at time
t, we used the posterior distribution when using the Boltzmann distribution for the energy
as the prior and the single observation at time t. The pool transitions used were Langevin
updates with respect to this pool distribution.

For comparison, we also tried solving this problem with the particle smoother of [2], in
which a particle lter is applied to the data in time order, after which a state sequence is
selected at random in a backwards pass. We used a stratied resampling method to reduce
variance. The initial particle set was created by drawing frames randomly from sequences
other than the sequence being tested, and translating the markers in each frame so that their
centre of mass was at the same point as the centre of mass in the test sequence.

Both programs were implemented in MATLAB. The particle smoother was run with 5000
particles, taking 7 hours of compute time. The resulting sampled trajectories roughly t the
2D observations, but were rather unrealistic  for instance, the subjects feet often oated
above the oor. We ran the embedded HMM using ve pool states for 300 iterations,
taking 1.7 hours of compute time. The resulting sampled trajectories were more realistic

1Data from the graphics lab of Jessica Hodgins, at http://mocap.cs.cmu.edu. We chose
markers 167, 72, 62, 63, 31, 38, downsampled to 30 frames per second. The experiments reported
here use frames 400-520 of trial 20 for subject 14. The elevation of the view direction was 45 degrees,
and the azimuth was 45 degrees away from a front view of the person in the rst frame.

than those produced by the particle smoother, and were quantitatively better with respect to
likelihood and dynamical transition probabilities. However, the distribution of trajectories
found did not overlap the true trajectory. The embedded HMM updates appeared to be
sampling from the correct posterior distribution, but moving rather slowly among those
trajectories that are plausible given the observations.

7 Conclusions
We have shown that the embedded HMM can work very well for a non-linear model with
a low-dimensional state. For the higher-dimensional motion tracking example, the embed-
ded HMM has some difculties exploring the full posterior distribution, due, we think,
to the difculty of creating pool distributions with a dense enough sampling of states to
allow linking of new states at adjacent times. However, the particle smoother was even
more severely affected by the high dimensionality of this problem. The embedded HMM
therefore appears to be a promising alternative to particle smoothers in such contexts.

The idea behind the embedded HMM should also be applicable to more general tree-
structured graphical models. A pool of values would be created for each variable in the
tree (which would include the current value for the variable). The fast sampling algorithm
possible for such an embedded tree (a generalization of the sampling algorithm used for
the embedded HMM) would then be used to sample a new set of values for all variables,
choosing from all combinations of values from the pools.

Finally, while much of the elaboration in this paper is designed to create a Markov chain
whose equilibrium distribution is exactly the correct posterior, (cid:25)(x), the embedded HMM
idea can be also used as a simple search technique, to nd a state sequence, x, which
maximizes (cid:25)(x). For this application, any method is acceptable for proposing pool states
(though some proposals will be more useful than others), and the selection of a new state
sequence from the resulting embedded HMM is done using a Viterbi-style dynamic pro-
gramming algorithm that selects the trajectory through pool states that maximizes (cid:25)(x). If
the current state at each time is always included in the pool, this Viterbi procedure will al-
ways either nd a new x that increases (cid:25)(x), or return the current x again. This embedded
HMM optimizer has been successfully used to infer segment boundaries in a segmental
model for voicing detection and pitch tracking in speech signals [1], as well as in other
applications such as robot localization from sensor logs.
Acknowledgments. This research was supported by grants from the Natural Sciences and
Engineering Research Council of Canada, and by an Ontario Premiers Research Excel-
lence Award. Computing resources were provided by a CFI grant to Geoffrey Hinton.

