Abstract.
distribution with density (x) is done using updates operating on an ensemble of states. The current
state x is rst stochastically mapped to an ensemble, (x(1), . . . , x(K)). This ensemble is then updated
using MCMC updates that leave invariant a suitable ensemble density, (x(1), . . . , x(K)), dened in
terms of (x(i)) for i = 1, . . . , K. Finally a single state is stochastically selected from the ensemble
after these updates. Such ensemble MCMC updates can be useful when characteristics of  and the
ensemble permit (x(i)) for all i  {1, . . . , K}, to be computed in less than K times the amount of
computation time needed to compute (x) for a single x. One common situation of this type is when
changes to some fast variables allow for quick re-computation of the density, whereas changes to other
slow variables do not. Gaussian process regression models are an example of this sort of problem,
with an overall scaling factor for covariances and the noise variance being fast variables. I show that
ensemble MCMC for Gaussian process regression models can indeed substantially improve sampling
performance. Finally, I discuss other possible applications of ensemble MCMC, and its relationship to
the multiple-try Metropolis method of Liu, Liang, and Wong and the multiset sampler of Leman,
Chen, and Lavine.

Introduction

In this paper, I introduce a class of Markov chain Monte Carlo methods that utilize a state space that
is the K-fold Cartesian product of the space of interest  ie, although our interest is in sampling for
x in the space X , we use MCMC updates that operate on (x(1), . . . , x(K)) in the space Y = X K .

Several such methods have previously been proposed  for example, Adaptive Directive Sampling

1

(Gilks, Roberts, and George, 1994) and Parallel Tempering (Geyer, 1991; Earl and Deem, 2005). The
ensemble MCMC methods I introduce here dier from these methods in two fundamental respects.
First, use of the space Y = X K is temporary  after some number of updates on Y, one can switch
back to the space X , perform updates on that space, then switch back to Y, etc. (Of course, one
might choose to always remain in the Y space, but the option of switching back and forth exists.)
Secondly, the invariant density for an ensemble state (x(1), . . . , x(K)) in the Y space is proportional
to the sum of the probabilities of the individual x(k) (or more generally a weighted sum), whereas
in the previous methods mentioned above, the density is a product of densities for each x(k). As a
consequence, the properties of the ensemble methods described here are quite dierent from those of
the methods mentioned above.

I expect that use of an ensemble MCMC method will be advantageous only when, for the particular
distributions being sampled from, and the particular ensembles chosen, a computational short-cut
exists that allows computation of the density for all of x(1), . . . , x(K) with less than K times the eort
needed to compute the density for a single state in X . Without this computational advantage, it is
hard to see how performing updates on X K could be benecial.

In this paper, I mostly discuss one particular context where such a computational short-cut exists
 when the distribution being sampled has the characteristic that after changes to only a subset of
fast variables it is possible to re-compute the density in much less time than is needed when other
slow variables change. By using ensembles of states in which the slow variables have the same values
in all ensemble members, the ensemble density can be quickly computed. In the limit as the size of
the ensemble grows, it is possible using such a method to approach the ideal of updating the slow
variables based on their marginal distribution, integrating over the fast variables.

I apply these fast-slow ensemble MCMC methods to Gaussian process regression models (Rasmussen
and Williams 2006; Neal 1999) that have a covariance function with unknown parameters, which in a
fully Bayesian treatment need to be sampled using MCMC. The computations for such models require
operations on the covariance matrix whose time grows in proportion to n3, where n is the number
of observations. If these computations are done using the Cholesky decomposition of the covariance
matrix, a change to only the overall scale of the covariance function does not require recomputation of
the Cholesky decomposition; hence this scale factor can be a fast variable. If computations are done
by nding the eigenvectors and eigenvalues of the covariance function, both an overall scale factor and
the noise variance can be fast variables. I show that ensemble MCMC with either of these approaches
can improve over MCMC on the original state space.

I conclude by discussing other possible applications of ensemble MCMC, and its relationship to the
multiple-try Metropolis method of Liu, Liang, and Wong (2000), and to the multiset sampler of
Leman, Chen, and Lavine (2009).

MCMC with an ensemble of states

Here, I introduce the idea of using an ensemble of states in general, using the device of stochastically
mapping from the space of interest to another space. I also discuss several ways of dening an ensemble.

2

I will suppose that we wish to sample from a distribution on X with probability density or mass
function (x). The MCMC approach is to dene a Markov chain that has  as an invariant distribution.
Provided this chain is ergodic, simulating the chain from any initial state for a suitably large number
of transitions will produce a state whose distribution is close to . To implement this strategy, we
need to dene a transition probability, T (x|x), for the chain to move to state x if it is currently in
state x. Updates made according to this transition probability must leave  invariant:

Z (x) T (x|x) dx = (x)

(1)

Stochastically mapping to an ensemble and back. An MCMC update that uses an ensemble
of K states can be viewed as probabilistically mapping from the original space, X , to a new space,
Y = X K , performing updates on this new space, and then mapping back to the original space. We can
formalize this temporary mapping strategy, which has many other applications (Neal 2006; Neal
2010, Section 5.4), as follows. We dene a transition distribution, T (x|x), on the space X as the
composition of three other stochastic mappings, T , T , and T :

x

T
 y

T
 y

T
 x

(2)

That is, starting from the current state, x, we obtain a value in the temporary space by sampling from
T (y|x). The target distribution for y  Y has probability/density function (y). We require that

Z (x) T (y|x) dx = (y)

T (y|y) denes a transition on the temporary space that leaves  invariant:

Finally, T gets us back to the original space. It must satisfy

Z (y) T (y|y) dy = (y)

Z (y) T (x|y) dy = (x)

(3)

(4)

(5)

The above conditions imply that the overall transition, T (x|x), will leave  invariant, and so can be
used for Markov sampling of .

In ensemble MCMC, where Y = X K , with the ensemble state written as y = (x(1), . . . , x(K)), the
stochastic mapping T , from X to Y, is dened in terms of an ensemble base measure, a distribution
with probability density or mass function (x(1), . . . , x(K)), as follows:

T (x(1), . . . , x(K) | x) =

1
K

K

Xk=1

k|k(x(k) | x) x(x(k))

(6)

Here, x() is the distribution with mass concentrated at x. The notation x(k) refers to all compo-
nents of the ensemble state other than the kth. The probability density or mass function for the

3

conditional distribution of components other than the kth given a value for the kth component is
k|k(x(k)|x(k)) = (x(1), . . . , x(K)) / k(x(k)), where k is the marginal density or mass function for
the kth component, derived from the joint distribution . We assume that k(x) is non-zero if (x)
is non-zero.

Algorithmically, T creates a state in Y by choosing an index, k, from 1 to K, uniformly at random,
setting x(k) to the current state, x, and nally randomly generating all x(j) for j 6= k from their
conditional distribution under  given that x(k) = x. Note that T depend only on , not on .

A simple example: As a simple (but not useful) illustration, let X = [0, 2) and set K = 2. Choose
the ensemble base measure, , to be uniform on pairs of points with x(2) = x(1) +  (mod 2), for
some constant   [0, 2). (Ie, the ensemble base measure is uniform over pairs of points on a unit
circle with the second point at an angle  counterclockwise from the rst.) Then 1|1(  |x(1)) =
x(1)+ (mod 2)(  ) and 2|2(  |x(2)) = x(1) (mod 2)(  ). The algorithm for the map T is

1) pick k uniformly from {1, 2}
2) set x(k) = x
3) if k = 1, set x(2) = x +  (mod 2);

if k = 2, set x(1) = x   (mod 2).

Having dened T as in equation (6), the density function, , for the ensemble distribution that will

make condition (3) hold can be derived as follows:

(x(1), . . . , x(K)) = Z (x) T (x(1), . . . , x(K) | x) dx

=

=

1
K

1
K

K

Xk=1
Xk=1

K

k|k(x(k) | x(k)) (x(k))

(x(1), . . . , x(K))

k(x(k))

(x(k))

= (x(1), . . . , x(K))

1
K

K

Xk=1

(x(k))
k(x(k))

(7)

(8)

(9)

(10)

So we see that the density function of  with respect to the base measure  is sometimes simply
proportional to the sum of  for all the ensemble members (when the k are all the same uniform
distribution), and more generally is proportional to the sum of ratios of  and k.

The simple example continued: 1 and 2 are both uniform over [0, 2). For pairs in X 2 satisfying the
constraint that x(2) = x(1) +  (mod2), the ensemble density follows the proportionality:

(x(1), x(2))  (x(1)) + (x(2))

(11)

(The probability under  of a pair that violates the constraint is of course zero.)

4

We can let T be any sequence of Markov updates for y = (x(1), . . . , x(K)) that leave  invariant

(condition (4)). We denote the result of applying T to y by y.

The simple example continued: We might dene T to be a sequence of some pre-dened number
of Metropolis updates (Metropolis, et al 1953), using a random-walk proposal, which from state
(x(1), x(2)) is (x(1)
 ) = (x(1) +  (mod2), x(2) +  (mod2)), with  being drawn from a
distribution symmetrical around zero, such as uniform on (/10, +/10). Such a proposal is accepted
with probability min[1, ((x(1)

 )) / ((x(1)) + (x(2))].

 ) + (x(2)

 , x(2)

To return to a single state, x, from the ensemble state y = (x(1), . . . , x(K)), we set x to one of the
x(k), which we select randomly with probabilities proportional to (x(k)) / k(x(k)). We can see that
this T satises condition (5) as follows:

Z (y) T (x|y) dy

= Z "(x(1), . . . , x(K))

1
K

K

j(x(j))

K

Xk=1
Xj=1Z x(j) (x) (x(j))
Xj=1Z (x) j|j(x(j)|x) dx(j)
Xj=1

(x) = (x)

K

K

=

=

=

1
K

1
K

1
K

(x(k))

k(x(k))#


x(j) (x)

K

Xj=1

(x(j))

j(x(j)).

K

Xk=1

(x(k))

k(x(k))


dx(1)    dx(K) (12)

(x(1), . . . , x(K))

dx(1)    dx(K)

(13)

(14)

(15)

The simple example continued: Since 1 and 2 are uniform over [0, 2), T simply picks either x(1) or
x(2) with probabilities proportional to (x(1)) and (x(2)).

Some possible ensembles. The ensemble base measure, , can be dened in many ways, some
of which I will discuss here. Note that the usefulness of these ensembles depends on whether they
produce any computational advantage for the distribution being sampled. This will be discussed in
the next section for problems with fast and slow variables, where variations on some of the ensembles
discussed here will be used.

One possibility is that x(1), . . . , x(K) are independent and identically distributed under , so that
(x(1), . . . , x(K)) = (x(1))    (x(K)), where (x) is the marginal distribution of all the x(k).
In
the conditional distribution k|k, the components other than x(k) will also be independent, with
distribution (x(k)) the same as their marginal distribution. With this ensemble base measure, the

5

ordering of states in the ensemble is irrelevant, and the mapping, T , from a single state to an ensemble
consists of combining the current state with K  1 states sampled from the marginal . The mapping,
T , from the ensemble to a single state consists of randomly selecting a state from x(1), . . . , x(k) with
probabilities proportional to (x(k))/(x(k)).

Rather than the x(k) being independent in the ensemble base measure, they might just be exchange-
able. This can be expressed as the x(i) being independent given some paramater , which has some
prior, () (which is unrelated to the real prior for a Bayesian inference problem). In other words,

(x(1), . . . , x(K)) = Z ()

K

Yk=1

(x(k)|) d

(16)

The mapping T can be implemented by sampling  from the posterior density ( | x(k) = x) 
() (x(k) = x | ), for k randomly chosen from 1, . . . , K (though due to exchangeability, this random
choice of k isnt really necessary) and then sampling x(j) for j 6= k independently from (x(j)|) (which
is the same for all j). The marginal distributions for the x(k) in the ensemble base measure, needed
to compute  and to map from an ensemble to a single state, are of course all the same when this
measure is exchangeable.

Another possibility is for the ensemble base measure on x(1), . . . , x(K) to be dened by a stationary
Markov chain, which again leads to the x(k) all having the same marginal distribution, (x). For
example, if X is the reals, we might let (x) = N (x; 0,  2), where N (x; ,  2) is the normal density

function, and k+1|k(x(k+1)|x(k)) = N (x(k+1); x(k)p1  1/ 2, 1) for k = 1, . . . , K 1. Going from a

single state to an ensemble is done by randomly selecting a position, k, for the current state, and then
simulating the Markov chain forward from x(k) to x(K) and backward from x(k) to x(1) (which will be
the same as forward simulation when the chain is reversible, as in the example here). The ensemble
density is given by equation (10), with all the marginal densities k(x) equal to (x). We return
from an ensemble to a single state by selecting from x(1), . . . , x(k) with probabilities proportional to
(x(k))/(x(k)).

Ensemble base measures can also be dened using constellations of points. Let x(1)

be
some set of K points in X . We let  be the distribution obtained by shifting all these points by an
amount, , chosen from some distribution on X , with density () that is nowhere zero, so that

 , . . . , x(K)



(x(1), . . . , x(K)) = Z ()

K

Yk=1

x(k)

 + (x(k)) d

(17)

The conditional distributions k|k do not depend on ()  they are degenerate distributions in
which the other constellation points are determined by x(k). We therefore move from a single point, x,
to a constellation of points by selecting k at random from 1, . . . , K, setting x(k) to x, and setting x(j)
for j 6= k to x(j)
 . The ensemble density, (x(1), . . . , x(K)), will also not depend on (), as
can be seen from equation (8))  it will be proportional to the sum of (x(k)) for ensembles that have
the shape of the constellation, and be zero for those that do not. The marginal densities, k(x(k)), will
be the same for all constellation points, since  = x(k)  x(k)
 will be the same for all k. (Note that this
is not the same as the marginal density functions being the same for all k.) Hence moving to a single

 + x  x(k)

6

point from a constellation is done by choosing a point, x(k), from the constellation with probabilities
proportional to (x(k)).

The simple example used in the previous section can be seen as a constellation of two points with
x(1)
 = 0, x(2)

 = , () uniform over [0, 2), and addition done modulo 2.

In the presentation above, the ensemble base measure  is assumed to be a proper probability
distribution, but  can instead be an improper limit of proper distributions, as long as the conditional
distributions and ratios of marginal distributions that are needed have suitable limits. In particular,
the conditional distributions k|k used to dene T in equation (6) must reach limits that are proper
distributions; this will also ensure that  is well dened (see equation (8)). The ratios j(x(j)) / k(x(k))
must also reach limits, so that T will be well dened.

We can obviously let () become improper when dening a constellation ensemble, since we have
seen that the choice of this density has no eect. For exchangeable ensembles, we can let ()
be improper as long as the posterior, ( | x(k) = x), is proper. We can also let  go to inn-
ity in the Markov ensemble base measure dened above. We will then have k+1|k(x(k+1)|x(k)) =
N (x(k+1); x(k), 1), so the conditional distributions k|k are simple random walks in both directions
from x(k). Since the marginal distributions for the x(k) are all normal with mean zero and variance  2,
for any x(j) and x(k), the ratio (x(j)) / (x(k)) approaches one as  goes to innity. (Note also that
since the limit of k|k is proper, the relevant values of x(k) will not diverge, so uniform convergence
of these ratios is not necessary.)

Ensemble MCMC for problems with fast and slow variables

Suppose that we wish to sample from a distribution on a space that can be written as a Cartesian
product, X = X1  X2, with elements written as x = (x1, x2). Suppose also that the probability
density or mass function, (x1, x2), is such that re-computing the density after a change to x2 is much
faster than recomputing the density after a change to x1. That is, if we have computed (x1, x2), and
saved suitable intermediate results from this computation, we can quickly compute (x1, x
2) for any
x
2, but there is no short-cut for computing (x
1 6= x1. In general, x1 and x2 might both
be multidimensional. I refer to x1 as the slow variables, and x2 as the fast variables.

2) for x

1, x

I rst encountered this class of problems in connection with models for the Cosmic Microwave
Background (CMB) radiation (Lewis and Bridle, 2002). This is an example of data modelled using a
complex simulation, in this case, of the early universe. In such problems, the slow variables, x1, are
the unknown parameters of the simulation, which are being t to observed data. When new values for
these parameters are considered, as for a Metropolis proposal, computing  with these new values will
require re-running the simulation, which we assume takes a large amount of computation time. Some
fairly simple model relates the output of the simulation to the observed data, with parameters x2 (for
example, the noise level of the observations). When a new value for x2 is considered in conjunction
with a value for x1 that was previously considered, the simulation does not need to be re-run, assuming
the output from the previous run was saved. Instead, computing  with this new x2 and the old x1
requires only some much faster calculation involving the observation model.

7

Fast and slow variables arise in many other contexts as well. For example, in many Bayesian models,
the posterior density for a set of low-level parameters involves a large number of data items, which
cannot be summarized in a few sucient statistics, while a small number of hyperparameters have
densities that depend (directly) only on the low-level parameters. The low-level parameters in such
a situation will be slow, since when they are changed, all the data items must be looked at again,
but the hyperparameters may be fast, since when they change, but the low-level parameters stay
the same, there is no need to look at the data.

Later in this paper, I will consider applying methods for fast and slow variables to the more spe-
cialized problem of sampling from the posterior distribution of the parameters of a Gaussian process
regression model, in which an overall scale factor for the covariance function and the noise variance
can be regarded as fast variables.

Previous methods for problems with fast and slow variables. A simple way of exploiting
fast variables, used by Lewis and Bridle (2002), is to perform extra Metropolis updates that change
only the fast updates, along with less frequent updates that change both fast and slow variables.
Lewis and Bridle found that this is an improvement over always performing updates for all variables.
Similarly, when using a Metropolis method that updates only one variable at a time, one could perform
more updates for the fast variables than for the slow variables. I will refer to these as extra update
Metropolis methods.

Another simple fast-slow method I devised (that has been found useful for the CMB modeling
problem) is to randomly choose a displacement in the slow space, 1  X1, and then perform some
pre-determined number of Metropolis updates in which the proposal from state (x1, x2) is (x1  1, x
2),
where the sign before 1 is chosen randomly, and x
2 is chosen from some suitable proposal distribution,
perhaps depending on x1, x2, and the chosen sign for 1. During such a sequence of random grid
Metropolis updates, all proposed and accepted states will have slow variables of the form x1 + i1,
where x1 is the initial value, and i is an integer.
If intermediate results of computations with all
these values for the slow variables are saved, the number of slow computations may be much less than
the number of Metropolis proposals, since the same slow variables may be proposed many times in
conjunction with dierent values for the fast variables.

If recomputation after a change to the fast variables is very much faster than after a change to
the slow variables, we might hope to nd an MCMC method that samples nearly as eciently as
would be possible if we could eciently compute the marginal distribution for just the slow variables,
I have
devised a dragging Metropolis method (Neal, 2004) that can be seen as attempting to achieve this.
The ensemble MCMC methods I describe next can also be viewed in this way.

(x1) = R (x1, x2) dx2, since we could approximate this integral using many values of x2.

Applying ensemble MCMC to problems with fast and slow variables. Ensemble MCMC can
be applied to this problem using ensembles in which all states have the same value for the slow variables,
x1. Computation of the ensemble probability density (equation (10)) can then take advantage of quick
computation for multiple values of the fast variables. Many sampling schemes of this sort are possible,

8

distinguished by the ensemble base measure used, and by the updates that are performed on the
ensemble (ie, by the transition T ).

For all the schemes I have looked at, the fast and slow variables are independent in the base
ensemble measure used, and the slow variables have the same value in all members of the ensemble.
The ensemble base measure therefore has the following form:

(x(1), . . . , x(K)) = h(x(1)

1 )

K

Yk=2

x(1)

1

(x(k)

1 )i (x(1)

2 , . . . , x(K)

2

)

(18)

Here, (x1) is some density for the slow variables  the choice of  will turn out not to matter, as
long as it is nowhere zero. That the slow variables have the same values in all ensemble members is
expressed by the factors of x(1)
1 ). The joint distribution of the fast variables in the K members of
the ensemble is given by the density function . If we let k be the marginal distribution for the kth
member of the ensemble under , we can write the ensemble density from equation (10) as follows:

(x(k)

1

(x(1), . . . , x(K)) = (x(1), . . . , x(K))

= h

K

Yk=2

x(1)

1

(x(k)

K

1
K

(x(k))
k(x(k))

Xk=1
1 )i (x(1)
2 , . . . , x(K)

2

(19)

(20)

)

1
K

K

Xk=1

(x(k))
k(x(k)
2 )

Possible choices for  include those analogous to some of the general choices for  discussed in the

previous section, in particular the following:

independent

2 , . . . , x(K)

Let x(1)
be independent under , with all having the same marginal distribution. For
example, if the fast variables are parameters in a Bayesian model, we might use their prior
distribution.

2

exchangeable

2 , . . . , x(K)

Let x(1)
2 |  N (,  2) and
  N (0, 2). We could then let  go to innity, so that  is improper, and all k are the same.

be exchangeable under . If x2 is scalar, we might let x(k)

2

grid points

2 = x(k)

Let x(k)
 +  for k = 1, . . . , K. Here, if x2 has dimension D,  is a D-dimensional oset
that is applied to the set of points x(k)
 . This is an example of a constellation, as discussed in the
previous section. One possibility is a rectangular grid, with K = mD for some integer m, and
with grid points spaced a distance dj in dimension j, so that the extent of the grid in dimension
j is (m1)dj.

The ensemble update, T , could be done in many ways, as long as the update leaves  invariant. A
simple and general option is to perform some number of Metropolis-Hastings updates (Metropolis, et al

9

1953; Hastings 1970), in which we propose to change the current ensemble state, y = (x(1), . . . , x(K)), to
a proposed state, y, drawn according to some proposal density g(y|y). We accept the proposed y as
the new state with probability min[ 1, (y) g(y|y) / (y) g(y|y) ], where  is dened by equation (20).
A technical issue arises when X is continuous  since x(k)
1 must be the same for all members of the
ensemble, the probability densities will be innite (eg, due to the delta functions in equation (20)). We
can avoid this by looking at densities for only x(1)
are just equal
to x(1)
1 . (This assumes that we never propose states violating this constraint.) A similar issue arises
when using an ensemble of grid points, as dened above, in which the x(k)
are constrained to a grid
2
 here also, we can simply leave out the innite factors in the density that enforce the deterministic
constraints, assuming that our proposals never violate them.

1 , . . . , x(K)

2 , . . . , x(K)

1 and x(1)

, since x(2)

2

1

I will consider two classes of Metropolis updates for the ensemble state, in which symmetry of the

proposal makes g(y|y)/g(y|y) equal to one, so the acceptance probability is min[ 1, (y)/(y) ]:

fast variables xed

Propose a new value for x1 in all ensemble members, by adding a random oset drawn from some
symmetrical distribution to the current value of x1. Keep the values of x(1)
unchanged
in the proposed state.

2 , . . . , x(K)

2

fast variables shifted

Propose a new value for x1 in all ensemble members as above, together with new values for
x(1)
2 , . . . , x(K)
that is randomly drawn from
some symmetrical distribution.

found by adding an oset to all of x(1)

2 , . . . , x(K)

2

2

These two schemes are illustrated in Figure 1. Either scheme could be combined with any of the three

Figure 1: Illustration of ensemble updates with with one slow variable (horizontal) and one fast variable
(vertical). The distribution is uniform over the outlined region. A grid ensemble with K = 10 is used.
On the left, a move is proposed by changing only the slow variable; it will be accepted with probability
1/4, since four members of the current ensemble have non-zero probability, compared with only one
in the proposed ensemble. On the right, a move is proposed by changing the slow variable and also
shifting the grid ensemble; it will be accepted with probability 2/4. (Of course, a less favourable shift
in the grid ensemble might have led to zero probability of acceptance.)

10

types of ensembles above, but shifting the fast variables when they are sampled independently from
some distribution seems unlikely to work well, since after shifting, the values for the fast variables
would not be typical of the distribution.

One could perform just one Metropolis update on the ensemble state before returning to a single
point in X , or many such updates could be performed. Moving back to a single point and then
regenerating an ensemble before performing the next ensemble update will requires some computation
time, but may improve sampling. Updates might also be performed on X (perhaps changing only fast
variables) before moving back to an ensemble.

Fast and slow variables in Gaussian process regression models

Gaussian process regression (Neal 1999; Rasmussen and Williams, 2006) is one context where ensemble
MCMC can be applied to facilitate sampling when there are fast and slow variables.

A brief introduction to Gaussian process regression. Suppose that we observe pairs (z(i), y(i))
for i = 1, . . . , n, where z(i) is a vector of p covariates, whose distribution is not modeled, and y(i) the
corresponding real-valued response, which we model as y(i) = f (z(i)) + e(i), where f is an unknown
function, and e(i) is independent Gaussian noise with mean zero and variance 2. We can give f a
Gaussian process prior, with mean zero and some covariance function C(z, z) = E[f (z)f (z)]. We
then base prediction for a future observed response, y, associated with covariate vector z, on the
conditional distribution of y given y = (y(1), . . . , y(n)). This conditional distribution is Gaussian, with
mean and variance that can be computed as follows:

E(y|y) = kT 1y, Var(y|y) = v  kT 1k

(21)

Here,  = K + 2I is the covariance matrix of y, with Kij = C(z(i), z(j)). Covariances between y and
y are given by the vector k, with ki = C(z, z(i)). The marginal variance of y is v = C(z, z) + 2.

If the covariance function C and the noise variance 2 are known, the matrix operations above are
all that are needed for inference and prediction. However, in practice, the noise variance is not known,
and the covariance function will also have unknown parameters. For example, one commonly-used
covariance function has the form

C(z, z) = 2(cid:16)a2 + exp(cid:16)

p

Xh=1(cid:16)h (zh  z

h)(cid:17)2 (cid:17)(cid:17)

(22)

Here, the a2 term allows for the overall level of the function to be shifted upwards or downwards
from zero; it might be xed a priori. However, we typically do not know what are suitable values for
the parameter , which expresses the magnitude of variation in the function, or for the parameters
1, . . . , p, which express how relevant each covariate is to predicting the response (with h = 0
indicating that zh has no eect on predictions for y).

In a fully Bayesian treatment of this model, , , and  are given prior distributions. Independent
Gaussian prior distributions for the log of each parameter may be suitable, with means and variances

11

that are xed a priori.
In the models I t below, the logs of the components of  are correlated,
which is appropriate when learning that one covariate is highly relevant to predicting y increases our
belief that other covariates might also be relevant. After sampling the posterior distribution of the
parameters with some MCMC method, predictions for future observations can be made by averaging
the Gaussian distributions given by (21) over values of the parameters taken from the MCMC run.

Sampling from the posterior distribution requires the likelihood for the parameters given the data,
which is simply the Gaussian probability density for y with these parameters. The log likelihood,
omitting constant terms, can therefore be written as

log L(, , ) = (1/2) log det (, , )  (1/2)yT (, , )1y

(23)

This log likelihood is usually computed by nding the Cholesky decomposition of , which is the
lower-triangular matrix L for which LLT = . From L, both terms above are easily calculated. In
the rst term, the determinant of  can be found as (det L)2, with det L being just the product of
the diagonal entries of L. In the second term, yT 1y = yT (LLT )1y = uT u, where u = L1y can be
found by forward substitution.

Computing the Cholesky decomposition of  requires time proportional to n3, with a rather small
constant of proportionality. Computation of  requires time proportional to n2p, with a somewhat
larger constant of proportionality (which may be larger still when covariance functions more complex
than equation (22) are used). If p is xed, the time for the Cholesky decomposition will dominate for
large n, but for moderate n (eg, n = 100, p = 10), the time to compute  may be greater.

I will show here how the likelihood
Expressing computations in a form with fast variables.
for a Gaussian process regression model can be rewritten so that some of the unknown parameters
are fast. Specically, if computations are done using the Cholesky decomposition,  can be a fast
parameter, and if computations are instead done by nding the eigenvectors and eigenvalues of the
covariance matrix, both  and  can be fast parameters. Note that the MCMC methods used work
better when applied to the logs of the parameters, so the actual fast parameters will be log() and
log(), though I sometimes omit the logs when referring to them here.

The form of covariance function shown in equation (22) has already been designed so that  can
be a fast parameter. Previously, I (and others) have used covariance functions in which the constant
term a is not multiplied by the scale factor 2, but writing it as in equation (22) will be essential for
 to be a fast parameter. As an expression of prior beliefs, it makes little dierence whether or not
a2 is multiplied by 2, since a is typically chosen to be large enough that any reasonable shift of the
function is possible, even if a is multiplied by a value of  at the low end of its prior range. Indeed,
one could let a go to innity  analogous to letting the prior for the intercept term in an ordinary
linear regression model be improper  without causing any signicant statistical problem, although in
practice, to avoid numerical problems from near-singularity of , excessively large values of a should
be avoided. A large value for a will be usually be unnecessary if the responses, y, are centred to have
sample mean zero.

To make  a fast variable when using Cholesky computations, we also need to rewrite the contri-

12

bution of the noise variance to  as 2 times 2/2 = 2. We will then do MCMC with log() as
a fast variable and log() and log(h) for h = 1, . . . , p as slow variables. (Note that the Jacobian for
the transformation from (log(), log()) to (log(), log()) has determinant one, so no adjustment of
posterior densities is required with this transformation.)

Fast recomputation of the likelihood after  changes can be done if we write  = 2( + 2I),

where

p

(24)

ij = a2 + exp(cid:16)

Xh=1(cid:16)h (z(i)

h  z(j)

h )(cid:17)2(cid:17)

 + 2I is not a function of log(), but only of log() and the log(h). Given values for these slow
variables, we can compute the Cholesky decomposition of  + 2I, and from that det( + 2I) and
yT ( + 2I)y, as described earlier. If these results are saved, for any value of the fast variable, log(),
we can quickly compute det  = 2n det( + 2) and yT 1y = yT ( + 2I)1y / 2, which suce
for computing the likelihood.

Rather than use the Cholesky decomposition, we could instead compute the likelihood for a Gaus-
sian process model by nding the eigenvectors and eigenvalues of . Let E be the matrix with the
eigenvectors (of unit length) as columns, and let 1, . . . , n be the corresponding eigenvalues. The
projections of the data on the eigenvectors are given by u = E T y. The log likelihood of (23) can then
be computed as follows:

log L = 

1
2

n

Xi=1

log i 

u2
i
i

1
2

n

Xi=1

(25)

Both nding the Cholesky decomposition of an nn matrix and nding its eigenvectors and eigenvalues
take time proportional to n3, but the constant factor for nding the Cholesky decomposition is smaller
than for nding the eigenvectors (by about a factor of fteen), hence the usual preference for using
the Cholesky decomposition.

However, if computations are done using eigenvectors and eigenvalues, both  and  can be made
fast variables. To do this, we write  = 2 + 2I. Given values for the slow variables, log(h) for
h = 1, . . . , p, we can compute , and then nd its eigenvalues, 1, . . . , n, and the projections of y on
each of its eigenvectors, which will be denoted ui for i = 1, . . . , n. If we save these i and ui, we can
quickly compute L for any values of  and . The eigenvectors of  are the same as those of , and
the eigenvalues of  are 2i + 2 for i = 1, . . . , n. The log likelihood for the log(h) values used to
compute  along with values for the fast variables log() and log() can therefore be found as

log L = 

1
2

n

Xi=1

log (2i + 2) 

u2
i

2i + 2

1
2

n

Xi=1

(26)

This procedure must be modied to avoid numerical diculties when  is nearly singular, as can
easily happen in practice. The x is to add a small amount to the diagonal of , giving  =  + r2I,
where r2 is a small amount of jitter, and then using  in the computations described above. The
statistical eect of this is that the noise variance is changed to 2r2 + 2. By xing r to a suitably
small value, the dierence of this from 2 can be made negligible. For consistency, I also use  for

13

the Cholesky method (so the Cholesky decomposition is of  + 2I), even though adding jitter is
necessary in that context only when 2 might be very small.

To summarize, if we use the Cholesky decomposition, we can compute the likelihood for all members
of an ensemble diering only in  using only slightly more time than is needed to compute the likeli-
hood for one point in the usual way. Only a small amount of additional time, independent of n and p,
is needed for each member of the ensemble. Computation of eigenvectors is about fteen times slower
than the Cholesky decomposition, and computing the likelihood for each ensemble member using these
eigenvectors is also slower, taking time proportional to n. However, when using eigenvector compu-
tations, both  and  can be fast variables. Which method will be better in practice is unclear, and
likely depends on the values of n and p  for moderate n and large p, time to compute the covariance
matrix, which is the same for both methods, will dominate, favouring eigenvector computations that
let  be a fast variable, whereas for large n and small p, the smaller time to compute the Cholesky
decomposition may be the dominant consideration.

If we wish to use a small ensemble over both  and , it may sometimes be desirable to do the
computations using the Cholesky decomposition, recomputing it for every value of the fast variables.
This would make sense only if the time to compute the covariances (apart from a nal scaling by 2
and addition of 2I) dominates the time for these multiple Cholesky computations (otherwise using
the ensemble will not be benecial), and the ensemble has less than about fteen members (otherwise
a single eigenvector computation would be faster). However, I do not consider this possibility in the
demonstraton below, where larger ensembles are used.

A demonstration

Here, I demonstrate and compare standard Metropolis and ensemble MCMC on the problem of sam-
pling from the posterior distribution for a Gaussian process regression model, using the fast-slow
methods described above. The model and synthetic test data that I use were chosen to produce
an interesting posterior distribution, having several modes that correspond to dierent degress of
predictability of the response (and hence dierent values for ).

The MCMC programs were written in R, and run on two machines, with two versions of R  version
2.8.1 on a Solaris/SPARC machine, and version 2.12.0 on a Linux/Intel machine. The programs used
are available at my web site.1

In the generated data, the true relationship of the response, y, to the vector
The synthetic data.
of covariates, z, was y = f (z) + e, with e being independent Gaussian noise with standard deviation
0.4, and the regression function being

f (z) = 0.7z2

1 + 0.8 sin(0.3 + (4.5 + 0.5z1) z2) + 0.85 cos(0.1 + 5z3 + 0.1z2
2)

(27)

The number of covariates was p = 12, though as seen above, f (z) depends only on z1, z2, and z3.
The covariate values were randomly drawn from a multivariate Gaussian distribution in which all the

1 http://www.cs.utoronto.ca/radford/

14

zh had mean zero and variance one. There were weak correlations among z1, z2, and z3. There were
strong (0.99) correlations between z1 and z4, z2 and z5, and z3 and z6, and moderate (0.9) correlatons
between z1 and z7, z2 and z8, and z3 and z9. Covariates z10, z11, and z12 were independent of each
other and the other covariates.2

I generated n = 100 independent pairs of covariate vectors and response values in this way.

The Gaussian process model. This synthetic data was t with a Gaussian process regression
model of the sort described in the previous section, in which the prior covariance between responses
y(i) and y(j) was

Cov(y(i), y(j)) = 2(cid:16)a2 + exp(cid:16)

p

Xh=1(cid:16)h (z(i)

h  z(j)

h )(cid:17)2 (cid:17) + r2ij(cid:17) + 2ij

(28)

where ij is zero if i 6= j and one if i = j. I xed a = 1 and r = 0.01.

The priors for , , and  were independent. The prior for log() was Gaussian with mean log(0.5)
and standard deviation 1.5; that for log() was Gaussian with mean of log(1) and standard deviation
1.5. The prior for  was multivariate Gaussian with each log(h) having mean log(0.5) and standard
deviation 1.8, and with the correlation of h and h for h 6= h being 0.69. Having a non-zero
prior correlation between the h is equivalent to using a prior expressed in terms of a higher-level
hyperparameter, 0, conditional on which the h are independent with mean 0.

I tried sampling from the posterior distribution
Performance of standard Metropolis sampling.
for this model and data using standard random-walk Metropolis methods, with the state variables
being the logs of the parameters of the covariance function (the twelve h parameters, , and ).

I rst tried using a multivariate Gaussian Metropolis proposal distribution, centred at the current
state, with independent changes for each variable, with the same standard deviation  ie, the proposal
distribution was N (x, s2I), where x is the current state, and s is the proposal standard deviation.

Figure 2 shows results with s = 0.25 (top) and s = 0.35 (bottom). The plots show three quantities,
on a logarithmic scale, for every 300th iteration from a total of 300,000 Metropolis updates (so that
1000 points are plotted). The quantity shown in red is the average of the h parameters (giving
relevances of the covariates), that shown in green is the  parameter (the overall scale), and that
shown in blue is the  parameter (noise standard deviation).

When s = 0.25, the rejection rate is 87%. From the top plot, it appears that the chain has converged,
and is sampling from two modes, characterized by values of  around 0.5, or much smaller values.
Movement between these modes occurs fairly infrequently, but an adequate sample appears to have
been obtained in 300,000 iterations.

2 In detail, the covariates were generated by letting wh for h = 1, . . . , 12 be independent standard normals, and then
letting z1 = w1, z2 = 0.25z1 + w21  0.252, z3 = 0.25z2 + w31  0.252, z4 = 0.99z1 + w41  0.992, z5 = 0.99z2 +
w51  0.992, z6 = 0.99z3 +w61  0.992, z7 = 0.9z1 +w71  0.92, z8 = 0.9z2 +w81  0.92, z9 = 0.9z3 +w91  0.92,

z10 = w10, z11 = w11, and z12 = w12.

15

average nu

eta

sigma

0

50000

100000

150000

200000

250000

300000

average nu

eta

sigma

s = 0.25:

s = 0.35:

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

0

50000

100000

150000

200000

250000

300000

Figure 2: Runs using Metropolis updates of all parameters at once, with two values for the proposal
standard deviation, s.

However, the bottom plot shows that this is not the case. The rejection rate here, with s = 0.35,
is high, at 94%, but this chain explores additional modes that were missed in the run with s = 0.25.
The sample from 300,000 iterations is nevertheless far from adequate. Some modes were moved to and
from only once in the run, so an accurate estimate of the probality of each mode cannot be obtained.

Metropolis updates that change only one parameter at a time, systematically updating all pa-
rameters once per MCMC iteration, perform better. (This is presumably because in the posterior
distribution there is fairly low dependence of some parameters on others, so that fairly large changes
can sometimes be made to a single parameter.) Updating a h parameter is slow, requiring about the
same computation time as an update changing all parameters. However, updates that change only ,
or (if computations are done using eigenvectors) only , will be fast. To allow a fair comparison, the
number of iterations done was set so that the number of slow evaluations was 300,000, the same as
for the runs with Metropolis updates of all variables at once. To make the comparison as favourable

16

average nu

eta

sigma

0

5000

10000

15000

20000

25000

average nu

eta

sigma

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

Extra:

0

5000

10000

15000

20000

25000

Figure 3: Runs using Metropolis updates of one parameter at a time, with extra updates of fast
parameters for the lower plot.

as possible for this method, I assumed that computations are done using eigenvectors, so that both 
and  are fast variables.

The top plot in Figure 3 shows results using such single-variable Metropolis updates (selected as
among the best of runs with various settings that were tried). Here, the proposal standard deviation
was 2 for the h parameters, and 0.6 for the  and  parameters. (Recall that all parameters are
represented by their logs.) The rejection rates for the updates of the variables ranged from 53% to
77%. One can see that sampling is signicantly improved, but that some modes are still visited only a
few times during the run. One way to try to improve sampling further is to perform additional updates
of the fast variables. The bottom plot in Figure 3 shows the results when 49 additional Metropolis
updates of the fast variables ( and ) are done each iteration. This seems to produce little or no
improvement on this problem. (However, on some other problems I have tried, extra updates of fast
variables do improve sampling.)

17

The time required for a slow evaluation using computations based on the Cholesky decomposition
was 0.0081 seconds on the Intel machine, and 0.0661 seconds on the SPARC machine. When using
computations based on eigenvectors, a slow evaluation took 0.0128 seconds on the Intel machine,
and 0.0886 seconds on the SPARC machine. The ratio of computation time using eigenvectors to
computation time using the Cholesky decomposition was therefore 1.58 for the Intel machine and 1.34
for the SPARC machine. Note that this ratio is much smaller than the ratio of times to compute
eigenvectors versus the Cholesky decomposition, since times for other operations, such as computing
the covariances, are the same for the two methods, and are not negligible in comparison when n = 100
and p = 12.

I now present the results of using ensemble MCMC methods
Performance of ensemble MCMC.
on this problem. The Metropolis method was used to update the ensemble, so that a direct comparison
to the results above can be made, but of course other types of MCMC updates for the ensemble are
quite possible. I tried Metropolis updates where the proposals both changed the slow variables and
shifted the ensemble of values for fast variables, but keeping the ensemble of fast variables xed seemed
to work as well or better for this problem. Updating only one slow variable at a time worked better
than updating all at once.

Accordingly, the results below are for ensemble updates consisting of a sequence of single-variable
Metropolis updates for each slow variable in turn, using Gaussian proposals centred at the current
value for the variable being updated. After each such sequence of updates, the ensemble was mapped
to a single state, and a new ensemble was then generated. (One could keep the same ensemble for many
updates, but for this problem that seems undesirable, considering that regenerating the ensemble is
relatively cheap.)

I will rst show the results when using computations based on eigenvectors, for which both  and
 are fast variables. Three ensembles for fast variables were tried  an independent ensemble, with
the distribution being the Gaussian prior, an exchangeable ensemble, with the ensemble distribution
being Gaussian with standard deviations 0.4 times the prior standard deviations, and a 7  7 grid
ensemble, with grid extent chosen uniformly between the prior standard deviation and 1.1 times the
prior standard deviation. All ensembles had K = 49 members. The number of iterations was set so
that 300,000 slow evaluations were done, to match the runs above using standard Metropolis updates.

Figure 4 shows the results. Comparing with the results of standard Metropolis in Figure 3, one
can see that there is much better movement among the modes, for all three choices of ensemble.
The exchangeable and grid ensembles perform slightly better than the independent ensemble. I tried
increasing the size of the ensemble to 400, and performing extra updates of the fast variables after
mapping back to a single state, but this only slightly improves the sampling (mostly for the independent
ensemble).

The time per iteration for these ensemble methods (with K = 49) was 0.0139 seconds on the Intel
machine and 0.0936 seconds on the SPARC machine (very close for all three ensembles). This is slower
than standard Metropolis using Cholesky computations by a factor of 1.7 for the Intel machine and
1.4 for the SPARC machine. The gain in sampling eciency is clearly much larger than this.

18

average nu

eta

sigma

0

5000

10000

15000

20000

25000

average nu

eta

sigma

0

5000

10000

15000

20000

25000

average nu

eta

sigma

Independent:

Exchangeable:

Grid (7  7):

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a
e

t


/




u
n
e
g
a
r
e
v
a

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

0

5000

10000

15000

20000

25000

Figure 4: Runs using ensemble MCMC with computations based on eigenvalues (both  and  fast),
for three ensembles.

19

Figure 5 shows the results when computations are based on the Cholesky decomposition, so that
only  can be a fast variable. Sampling is much better than with standard Metropolis, but not as good
as when both  and  are fast variables. However, the computation time is lower  0.0082 seconds per
iteration for on the Intel machine, and 0.0666 seconds per iteration on the SPARC machine. These
times are only about 1% higher than for standard Metropolis, so use of an ensemble for  only is
virtually free.

Discussion

I will conclude by discussing other possible applications of ensemble MCMC, and its relationship with
two previous MCMC methods.

Some other applications. Bayesian inference problems with fast and slow variables arise in many
contexts other than Gaussian process regression models. As mentioned early, many Bayesian models
have hyperparameters whose conditional distributions depend only on a fairly small number of pa-
rameters, and which are therefore fast compared to parameters that depend on a large set of data
points.

For example, in preliminary experiments, I have found a modest benet from ensemble MCMC
for logistic regression models in which the prior for regression coecients is a t distribution, whose
width parameter and degrees of freedom are hyperparameters. When there are n observations and
p covariates, recomputing the posterior density after a change only to these hyperparameters takes
time proportional to p, whereas recomputing the posterior density after the regression coecients
change takes time proportional to np (or to n, if only one regression coecient changes, and suitable
intermediate results were retained). The hyperparameters may therefore be seen as fast variables.

In Gaussian process models with latent variables, such as logistic classication models (Neal 1999),
the latent variables are all fast compared to the parameters of the covariance function, since recom-
puting the posterior density for the n latent variables, given the parameters of the covariance function,
takes time proportional n2, with a small constant factor, once the Cholesky decomposition of their
covariance matrix has been found in n3 time. Looking at ensemble MCMC for this problem would be
interesting, though the high dimensionality of the fast variables may raise additional issues.

MCMC for state-space time series models using embedded Hidden Markov models (Neal, Beal, and
Roweis, 2004) can be interpreted as an ensemble MCMC method that maps to the ensemble and
then immediately maps back. Looked at this way, one might consider updates to the ensemble before
mapping back. Perhaps more promising, though, is to use a huge ensemble of paths dened using an
embedded Hidden Markov model when updating the parameters that dene the state dynamics.

Relationship to the multiple-try Metropolis method. A Metropolis update on an ensemble
of points as dened in this paper bears some resemblence to a multiple-try Metropolis update as
dened by Liu, Liang, and Wong (2000)  for both methods, the update is accepted or rejected based
on the ratio of two sums of terms over K points, and in certain cases, these sums are of (x(i)) for the

20

average nu

eta

sigma

0

5000

10000

15000

20000

average nu

eta

sigma

0

5000

10000

15000

20000

average nu

eta

sigma

Independent:

Exchangeable:

Grid (7  7):

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a

t

e


/


u
n



e
g
a
r
e
v
a

a
m
g
s

/


i

a
e

t


/




u
n
e
g
a
r
e
v
a

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

0
0
+
e
5

1
0

e
1

3
0

e
5

0

5000

10000

15000

20000

Figure 5: Runs using ensemble MCMC with computations based on Cholesky decomposition (only 
fast), for three ensembles.

21

K points, x(1), . . . , x(K). In a multiple-try Metropolis update, K points are sampled independently
from some proposal distribution conditional on the current point, one of these K proposed points is
then selected to be the new point if the proposal is accepted, and a set of K points is then produced by
combining the current point with K 1 points sampled independently from the proposal distribution
conditional on this selected point. Finally, whether to accept the selected point, or reject it (retaining
the current point for another iteration), is decided by a criterion using a ratio of sums of terms for
these two sets of K points.

In one simple situation, multiple-try Metropolis is equivalent to mapping from a single point to an
ensemble of K points, doing a Metropolis update on this ensemble, and then mapping back to a single
point. This equivalence arises when the proposal distribution for multiple-try Metropolis does not
actually depend on the current state, in which case it can also be used as an independent ensemble
base distribution, and as a proposal distribution for an ensemble update in which new values for all
ensemble members are proposed independently.3 This method will generally not be useful, however,
since there is no apparent short-cut for evaluating (x) at the K ensemble points (or K proposed
points) in less than K times the computational cost of evaluating (x) at one point.

Applying multiple-try Metropolis usefully to problems with fast and slow variables, as done here for
ensemble MCMC, would require that the K proposals conditional on the current state be dependent,
since for fast computation they need to all have the same values for the slow variables. Liu, et al.
mention the possibility of dependent proposals, but provide details only for a special kind of dependence
that is not useful in this context.

In this paper I have emphasized the need for a computational short-cut if ensemble MCMC is to
provide a benet, since otherwise it is hard to see how an ensemble update taking a factor of K more
computation time can outperform K ordinary updates. The analogous point regarding multiple-try
Metropolis was apparently not appreciated by Liu, et al., as none of the examples in their paper make
use of such a short-cut. Accordingly, in all their examples one would expect a multiply-try Metropolis
method with K trials to be inferior to simply performing K ordinary Metropolis updates with the
same proposal distribution, a comparison which is not presented in their paper.4

Relationship to the multiset sampler. Leman, Chen, and Lavine (2009) proposed a multiset
sampler which can be seen as a particular example of sampling from an ensemble distribution as in
this paper. In their notation, they sample from a distribution for a value x and a multiset of k values
for y, written as s = {y1, . . . , yk}. The y values are conned to some bounded region, Y, which allows
a joint density for x and s to be dened as follow:

(x, s) 

k

Xj=1

(x, yj) = (x)

(yj|x)

k

Xj=1

(29)

3In detail, using the notation of Liu, et al. (2000), we obtain this equivalence by letting T (x, y) = (y) and (x, y) =

1/[(x)(y)], so that w(x, y) = (x)T (x, y)(x, y) = (x)/(x).

4For their CGMC method, the K ordinary Metropolis updates should use the same reference point. This is valid,

since the reference point is determined by a part of the state that remains unchanged during these K updates.

22

where (x, y) is the distribution of interest.
In some examples, they focus on the marginal (x).
Integrating the density above over y1, . . . , yk shows that the marginal (x) is the same as (x), so
that sampling x and s from  will produces a sample of x values from . Leman, et al. sample from
 by alternating a Metropolis update for just x with with a Metropolis update for yj with j selected
randomly from {1, . . . , k}.

This  distribution for the multiset sampler is the same as the ensemble distribution that would
be obtained using the methods for fast and slow variables in this paper, if x is regarded as slow (and
hence written as x1 in the notation of this paper), y is regarded as fast (and hence written as x2), and
in the  density used in dening the ensemble base measure, the fast variables in the ensemble are
independent, with uniform density over Y. The density  dened by equation (20) then corresponds
to the multiset density above.

Leman, et al. do not distinguish between fast and slow variables, however, and hence do not argue
that the multiset sampler would be benecial in such a context. Nor do they assume that there is
any other computational short-cut allowing  to sometimes be evaluated quickly, thereby reducing the
amount of computation needed to evaluate . They instead justify their multiset sampler as allowing
k  1 of the y values in the multiset to freely explore the region Y, since the one remaining y value
can provide a reasonably high (x, y) and hence also a reasonably high value for .

The development in this paper conrms this picture. With the  distribution corresponding to
multiset sampling, the mapping T will (in the notation of Leman, et al.) go from a single (x, y) pair
drawn from (x, y) to an ensemble with k values for y, one of which is the original y, and the other
k  1 of which are drawn independently and uniformly from Y. This is therefore the equilibrium
distribuiton of the multiset samplier. The development here also shows that (x, y) can be recovered
by applying the T mapping, which will select a y from the multiset with probabilities proportional to
(x, y). This makes the complex calculations in Section 6 of (Leman, et al., 2002) unnecessary.

Unfortunately, it also seems that any benet of the multiset sampler can be obtained much more
simply and eciently with a Markov chain sampler that randomly chooses between two Metropolis-
Hastings updates on the original  distribution  one that proposes a new value for x (from some
suitable proposal distribution) with y unchanged, and another that proposes a new value for x along
with a new value for y that is drawn uniformly from Y. As is the case as well for ensemble MCMC
and multiple-try Metropolis, one should expect that looking at K points at once will produce a benet
only when a short-cut allows  for all these points to be computed in less than K times the cost of
computing  for one point.

Acknowledgements

This research was supported by Natural Sciences and Engineering Research Council of Canada. The
author holds a Canada Research Chair in Statistics and Machine Learning.

23

