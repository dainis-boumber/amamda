Abstract

Recent research in machine learning has focused
on breaking audio spectrograms into separate
sources of sound using latent variable decom-
positions. These methods require that the num-
ber of sources be specied in advance, which is
not always possible. To address this problem,
we develop Gamma Process Nonnegative Matrix
Factorization (GaP-NMF), a Bayesian nonpara-
metric approach to decomposing spectrograms.
The assumptions behind GaP-NMF are based
on research in signal processing regarding the
expected distributions of spectrogram data, and
GaP-NMF automatically discovers the number of
latent sources. We derive a mean-eld variational
inference algorithm and evaluate GaP-NMF on
both synthetic data and recorded music.

1. Introduction
Recent research in machine learning has focused on break-
ing audio spectrograms into separate sources of sound us-
ing latent variable decompositions. Such decompositions
have been applied to identifying individual instruments and
notes, e.g., for music transcription (Smaragdis & Brown,
2003), to predicting hidden or distorted signals (Bansal
et al., 2005), and to source separation (Fevotte et al., 2009).
A problem with these methods is that the number of sources
must be specied in advance, or found with expensive tech-
niques such as cross-validation. This problem is particu-
larly relevant when analyzing music. We want the discov-
ered latent components to correspond to real-world sound
sources, and we cannot expect the same number of sources
to be present in every recording.
In this article, we develop Gamma Process Nonnegative
Matrix Factorization (GaP-NMF), a Bayesian nonparamet-

Appearing in Proceedings of the 27 th International Conference
on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the
author(s)/owner(s).

ric (BNP) approach to decomposing spectrograms. We
posit a generative probabilistic model of spectrogram data
where, given an observed audio signal, posterior inference
reveals both the latent sources and their number.
The central computational challenge posed by our model
is posterior inference. Unlike other BNP factorization
methods, our model is not composed of conjugate pairs of
distributionswe chose our distributions to be appropriate
for spectrogram data, not for computational convenience.
We use variational inference to approximate the posterior,
and develop a novel variational approach to inference in
nonconjugate models. Variational inference approximates
the posterior with a simpler distribution, whose parameters
are optimized to be close to the true posterior (Jordan et al.,
1999). In mean-eld variational inference, each variable is
given an independent distribution, usually of the same fam-
ily as its prior. Where the model is conjugate, optimization
proceeds by an elegant coordinate ascent algorithm. Re-
searchers usually appeal to less efcient scalar optimization
where conjugacy is absent. We instead use a bigger varia-
tional family than the model initially asserts. We show that
this gives an analytic coordinate ascent algorithm, of the
kind usually limited to conjugate models.
We evaluated GaP-NMF on several problemsextracting
the sources from music audio, predicting the signal in miss-
ing entries of the spectrogram, and classical measures of
Bayesian model t. Our model performs as well as or better
than the current state-of-the-art. It nds simpler representa-
tions of the data with equal statistical power, without need-
ing to explore many ts over many numbers of sources, and
thus with much less computation.

2. GaP-NMF Model
We model the Fourier power spectrogram X of an audio
signal. The spectrogram X is an M by N matrix of non-
negative reals; the cell Xmn is the power of our input au-
dio signal at time window n and frequency bin m. Each
column of the power spectrogram is obtained as follows.
First, take the discrete Fourier transform of a window of

Bayesian Nonparametric Matrix Factorization for Recorded Music

2(M  1) samples. Next, compute the squared magnitude
of the complex value in each frequency bin. Finally, keep
only the rst M bins, since the remaining bins contain only
redundant information.
We assume the audio signal is composed of K static sound
sources. As a consequence, we can model the observed
spectrogram X with the product of two non-negative ma-
trices: an M by K matrix W describing these sources
and a K by N matrix H controlling how the amplitude
of each source changes over time (Smaragdis & Brown,
2003). Each column of W is the average power spectrum
of an audio source; cell Wmk is the average amount of en-
ergy source k exhibits at frequency m. Each row of H is
the time-varying gain of a source; cell Hkn is the gain of
source k at time n. These matrices are unobserved.
Abdallah & Plumbley (2004) and Fevotte et al. (2009)
show that (under certain assumptions) mixing K sound
sources in the time domain, with average power spectra de-
ned by the columns of W and gains specied by the rows
of H, yields a mixture whose spectrogram X is distributed

Xmn  Exponential((cid:80)

k WmkHkn).

(1)

Previous spectrogram decompositions assume the number
of components K is known. In practice, this is rarely true.
Our goal is to develop a method that infers both the char-
acters and number of latent audio sources from data. We
develop a Bayesian nonparametric model with an innite
number of latent components, a nite number of which are
active when conditioned on observed data.
We now describe the Gamma Process Nonnegative Matrix
Factorization model (GaP-NMF). As in previous matrix de-
composition models, the spectrogram X arises from hid-
den matrices W and H. In addition, the model includes
a hidden vector of non-negative values , where each ele-
ment l is the overall gain of the corresponding source l.
The key idea is that we allow for the possibility of a large
number of sources L, but place a sparse prior on . During
posterior inference, this prior biases the model to use no
more sources than it needs.
Specically, GaP-NMF assumes that X is drawn according
to the following generative process:
Wml  Gamma(a, a)
Hln  Gamma(b, b)
l  Gamma(/L, c)

Xmn  Exponential((cid:80)

l lWmlHln).

(2)

As the truncation level L increases towards innity, the
vector  approximates an innite sequence drawn from a
gamma process with shape parameter  and inverse-scale
parameter c (Kingman, 1993). A property of this se-

quence is that the number of elements K greater than some
number  > 0 is nite almost surely. Specically:

K  Poisson

x1excdx

.

(3)

(cid:18)1

(cid:90) 

c



(cid:19)

For truncation levels L that are sufciently large relative to
the shape parameter , we likewise expect that only a few
of the L elements of  will be substantially greater than 0.
During posterior inference, this property leads to a prefer-
ence for explanations that use relatively few components.
Note that the expected value of Xmn under this model is
constant with respect to L, , a, and b:

Ep[Xmn] =(cid:80)

Ep[l]Ep[Wml]Ep[Hln] = 1
c .

(4)

l

This equation suggests the heuristic of setting the expected
mean of the spectrogram X under the prior equal to its
empirical mean X by setting c = 1/ X.

3. Variational Inference
Posterior inference is the central computational problem
for analyzing data with the GaP-NMF model. Given an ob-
served spectrogram X, we want to compute the posterior
distribution p(, W , H|X, , a, b, c). Exact Bayesian in-
ference is intractable. We appeal to mean-eld variational
inference (Jordan et al., 1999).
Variational
inference is a deterministic alternative to
Markov Chain Monte Carlo (MCMC) methods that re-
places sampling with optimization. It has permitted ef-
cient large-scale inference for several Bayesian nonpara-
metric models (e.g. Blei & Jordan, 2004; Doshi-Velez et al.,
2009; Paisley & Carin, 2009). Variational inference algo-
rithms approximate the true posterior distribution with a
simpler variational distribution controlled by free param-
eters. These parameters are optimized to make the vari-
ational distribution close (in Kullback-Leibler divergence)
to the true posterior of interest. Mean-eld variational in-
ference uses a fully factorized variational distributioni.e.,
under the variational distribution all variables are indepen-
dent.
In conjugate models this permits easy coordinate
ascent updates using variational distributions of the same
families as the prior distributions.
Less frequently, variational methods are applied to non-
conjugate models, which allow increased model expres-
sivity at the price of greater algorithmic challenges. Our
model is such a model. The usual strategy is to use a fac-
torized variational distribution with the same families as the
priors, bound or approximate the objective function, and
use numerical techniques to optimize difcult parameters
(Blei & Lafferty, 2006; Braun & McAuliffe, 2008).
We use a different strategy. We adopt an expanded fam-
ily for our variational distributions, one that generalizes the

Bayesian Nonparametric Matrix Factorization for Recorded Music

priors family. This allows us to derive analytic coordinate
ascent updates for the variational parameters, eliminating
the need for numerical optimization.

3.1. Variational Objective Function

It is standard in mean-eld variational inference to give
each variable a variational distribution from the same fam-
ily as its prior distribution (Jordan et al., 1999). We instead
use the more exible Generalized Inverse-Gaussian (GIG)
family (Jrgenson, 1982):

q(Wml) = GIG((W )
ml
q(Hln) = GIG((H)
q(l) = GIG(()

, (W )
ml
ln , (H)
, ()

,  (W )
ml )
ln ,  (H)
ln )
).

,  ()

l

l

l

(5)

The GIG distribution is an exponential family distribution
with sufcient statistics x, 1/x, and log x, and its PDF (in
canonical exponential family form) is

GIG(y; , , ) =

exp{(  1) log y  y   /y}/2

2 /2K(2



)

,

(6)
for x  0,   0, and   0. (K(x) denotes a modied
Bessel function of the second kind.)
Note that the GIG familys sufcient statistics (y, 1/y, and
log y) are a superset of those of the gamma family (y and
log y), and so the gamma family is a special case of the
GIG family where  > 0,   0.
To compute the bound in equation 8, we will need the ex-
pected values of each Wml, Hln, and l and of their recip-
rocals under our variational GIG distributions. For a vari-
able y  GIG(, , ) these expectations are

E[y] =



K+1(2
)


K(2
)




; E



K1(2
)


K(2
)





.

=

(cid:20) 1

(cid:21)

y

(7)
Having chosen a fully factorized variational family, we can
lower bound the marginal likelihood of the input spectro-
gram under the GaP-NMF model (Jordan et al., 1999):

log p(X|, a, b, c)  Eq[log p(X|W , H, )]
+ Eq[log p(W|a)]  Eq[log q(W )]
+ Eq[log p(H|b)]  Eq[log q(H)]
+ Eq[log p(|, c)]  Eq[log q()].

(8)

The difference between the left and right sides of equation
8 is the Kullback-Leibler (KL) divergence between the true
posterior and the variational distribution q. Thus, maximiz-
ing this bound with respect to q minimizes the KL diver-
gence between q and our posterior distribution of interest.
The second, third, and fourth lines of equation 8 can be
computed using the expectations in equation 7.

The likelihood term in equation 8 expands to

Eq[log p(X|W , H, )] =

(cid:20)

(cid:80)

(cid:88)

m,n

Eq

(cid:21)

Xmn
l lWmlHln

 Eq

(cid:34)

log(cid:88)

l

(cid:35)

lWmlHln

.

(9)

We cannot compute either of the expectations on the right.
However, we can compute lower bounds on both of them.
says that for any vector  such that l  0 and(cid:80)
First, the function x1 is concave. Jensens inequality
= (cid:80)
l l = 1
 1P
(10)
(cid:20)
(cid:20) Xmn

We use this inequality to derive a bound on the rst expec-
tation in equation 9:

 (cid:80)
(cid:88)

=  1P

l 2
l

(cid:21)

(cid:21)

l l

1
xl

.

(11)

1
xl
l

l xl

l l

xl
l

2

lmn

Eq

Eq

(cid:80)

Xmn
l lWmlHln

lWmlHln

l

Second, the function  log x is convex. We can therefore
bound the second expectation in equation 9 using a rst-
order Taylor approximation about an arbitrary (positive)
point mn as in (Blei & Lafferty, 2006) 1:

(cid:34)

log(cid:88)

Eq

(cid:35)



(cid:20)

lWmlHln

l

 log(mn) + 1  1
mn

(12)

Eq [lWmlHln] .

(cid:88)

l

We use equations 11 and 12 to bound equation 9:
Eq[log p(X|W , H, )] 

(cid:88)

(cid:88)

 Xmn

2

lmn

Eq

m,n

l

lWmlHln

(cid:21)

(13)

 log(mn) + 1  1
mn

Eq [lWmlHln] .

1

(cid:88)

l

Note that this bound involves the expectations both of the
model parameters and of their reciprocals under the vari-
ational distribution q. Since both y and 1/y are sufcient
statistics of GIG(y; , , ), this will not pose a problem
during inference, as it would if we were to use variational
distributions from the gamma family.
We denote as L the sum of the likelihood bound in equa-
tion 13 and the second, third, and fourth lines of equation

1Braun & McAuliffe (2008) observe that this bound is max-
imized when the Taylor approximation is taken around the ex-
pected value of the argument to the logarithm function, which
corresponds to the 0th-order delta method. However, retaining
the redundant parameter mn permits faster and simpler up-
dates for our other parameters.

Bayesian Nonparametric Matrix Factorization for Recorded Music

8. L lower bounds the likelihood p(X|, a, b, c). Our vari-
ational inference algorithm maximizes this bound over the
free parameters, yielding an approximation q(W , H, ) to
the true posterior p(W , H, |X, , a, b, c).

We iterate between updating bound parameters and vari-
ational parameters according to equations 14, 15, 16, 17,
and 18. Each update tightens the variational bound on
log p(X|, a, b, c), ultimately reaching a local optimum.

3.2. Coordinate Ascent Optimization
We maximize the bound L using coordinate ascent, iter-
atively optimizing each parameter while holding all other
parameters xed. There are two sets of parameters to opti-
mize: those used to bound the likelihood term in equation
9 and those that control the variational distribution q.

3.2.1. TIGHTENING THE LIKELIHOOD BOUND

In equations 11 and 12, we derived bounds on the in-
tractable expectations in equation 9. After updating the
variational distributions on each set of parameters W , H,
and , we update  and  to re-tighten these bounds.
Using Lagrange multipliers, we nd that the optimal  is

(cid:20)

(cid:21)1

lmn  Eq

1

lWmlHln

.

(14)

The bound in equation 12 is tightest when
Eq [lWmlHln] .

mn =(cid:80)

l

(15)

I.e., this bound is tightest when we take the Taylor approxi-
mation about the expected value of the functions argument.

3.2.2. OPTIMIZING THE VARIATIONAL DISTRIBUTIONS
The derivative of L with respect to any of (W )
 (W )
ml

equals 0 when

, (W )
ml

ml

, or

ml = a + Eq[l](cid:80)
(cid:105)(cid:80)
(cid:104) 1

(W )

n Xmn2

lmn

Eq

l

Eq[Hln]
mn

;

(cid:104) 1

n

(cid:105)

Hln

.

(16)

(W )
ml = a;
ml = Eq
 (W )

Simultaneously updating the parameters (W ), (W ), and
 (W ) according to equation 16 will maximize L with re-
spect to those parameters.
Similarly, the derivative of L with respect to any of (H)
ln ,
ln , or  (H)
(H)

ln

equals 0 and L is maximized when
Eq[Wml]

ln = b + Eq[l](cid:80)
(cid:105)(cid:80)
(cid:104) 1

(cid:104) 1

(H)

(cid:105)

mn

m

m Xmn2

lmn

Eq

Wml

l

;

(H)
ln = b;
ln = Eq
 (H)

.

(17)

Finally, the derivative of L with respect to any of ()
or  ()

equals 0 and L is maximized when

l

, ()

l

,

l

l = c +(cid:80)
(cid:80)

n Xmn2

()

m

lmn

m

(cid:80)
(cid:104)

n

Eq

l =(cid:80)

()
l = 
L;
 ()

Eq[WmlHln]

mn

;

(cid:105)

1

WmlHln

.

(18)

(cid:80)

3.3. Accelerating Inference
Paisley & Carin (2009) observed that if Eq[l] becomes
small for some component l, then we can safely skip the
updates for the variational parameters associated with that
(In our experiments we used 60 dB below
component.
Eq[l] as a threshold.) This heuristic allows the use of
large truncation levels L (yielding a better approximation to
an innite gamma process) without incurring too severe a
performance penalty. The rst few iterations will be expen-
sive, but the algorithm will require less time per iteration as
it becomes clear that only a small number of components
(relative to L) are needed to explain the data.

l

4. Evaluation
We conducted several experiments to assess the decompo-
sitions provided by the GaP-NMF model. We tested GaP-
NMFs ability to recover the true parameters used to gener-
ate a synthetic spectrogram, compared the marginal likeli-
hoods of real songs under GaP-NMF to the marginal likeli-
hoods of those songs under a simpler version of the model,
evaluated GaP-NMFs ability to predict held-out data with
a bandwidth expansion task, and evaluated GaP-NMFs
ability to separate individual notes from mixed recordings.
We compared GaP-NMF to two variations on the same
model:
Finite Bayesian model. This is a nite version of the GaP-
NMF model t using the same variational updates but with-
out the top-level gain parameters . This simpler models
generative process is

Wmk  Gamma(a, ac); Hkn  Gamma(b, b);

Xmn  Exponential((cid:80)

k WmkHkn),

(19)
where k  {1, . . . , K} and the model order K is chosen
a priori. The hyperparameters a, b, and c are set to the
same values as in the GaP-NMF model in all experiments.
We will refer to this model as GIG-NMF, for Generalized
Inverse-Gaussian Nonnegative Matrix Factorization.
Finite non-Bayesian model. This model ts W and H
to maximize the likelihood in equation 1. Fevotte et al.
(2009) derive iterative multiplicative updates to maximize
this likelihood, calling the resulting algorithm Itakura-Saito
Nonnegative Matrix Factorization (IS-NMF).
We also compared GaP-NMF to the two nonnegative ma-
trix factorization (NMF) algorithms described by Lee &
Seung (2001). Both of these algorithms also attempt to ap-

Bayesian Nonparametric Matrix Factorization for Recorded Music

proximately decompose the spectrogram X into an M by
K matrix W and a K by N matrix H so that X  W H.
The rst algorithm, which we refer to as EU-NMF, mini-
mizes the sum of the squared Euclidean distances between
the elements of X and W H. The second algorithm, which
we refer to as KL-NMF, minimizes the generalized KL-
divergence between X and W H. KL-NMF (and its exten-
sions) in particular is widely used to analyze audio spectro-
grams (e.g. Smaragdis & Brown, 2003; Bansal et al., 2005).
We focus on approaches that explain power spectrograms
in terms of components that can be interpreted as audio
power spectra. Other approaches may be useful for some
tasks, but they do not decompose mixed audio signals into
their component sources. This requirement excludes, for
example, standard linear Gaussian factor models, whose
latent factors cannot be interpreted as audio spectra unless
audio signals are allowed to have negative power.
We normalized all spectrograms to have a maximum value
of 1.0. (The high probability densities in our experiments
result from low-power bins in the spectrograms.) To avoid
numerical issues, we forced the values of the spectrograms
to be at least 108, 80 dB below the peak value of 1.0.
In all experiments, we initialized the variational parameters
 for each W , H, and  with random draws from a gamma
distribution with shape parameter 100 and inverse-scale pa-
rameter 1000, the variational parameters  to 0.1, and each
(W )
mk = a, (H)
k = /K. This yields a dif-
fuse and smooth initial variational posterior, which helped
avoid local optima. We ran variational inference until the
variational bound increased by less than 0.001%. The GIG-
NMF and IS-NMF algorithms were optimized to the same
criterion. KL-NMF and EU-NMF were iterated until their
cost functions decreased by less than 0.01 and 0.001, re-
spectively. (We found no gains in performance from letting
EU-NMF or KL-NMF run longer.) All algorithms were
implemented in MATLAB2.
We found GaP-NMF to be insensitive to the choice of ,
and so we set  = 1 in all reported experiments.

kn = b, and ()

4.1. Synthetic Data

We evaluated the GaP-NMF models ability to correctly
discover the latent bases that generated a matrix X, and
how many such bases exist. To test this, we t GaP-NMF
to random matrices X drawn according to the process:

Wmk  Gamma(0.1, 0.1);
Hkn  Gamma(0.1, 0.1);

Xmn  Exponential((cid:80)

k WmkHkn),

(20)

2MATLAB code

http://www.cs.princeton.edu/mdhoffma

for GaP-NMF

is

available

at

Figure 1. True synthetic bases (left) and expected values under the
variational posterior of the nine bases found by the model (right).
Brighter denotes more active. The 36-dimensional basis vectors
are presented in 6  6 blocks for visual clarity.
where m  {1, . . . , M = 36}, n  {1, . . . , N = 300},
k  {1, . . . , K} for K = 9.
We ran variational inference with the truncation level L
set to 50, and hyperparameters  = 1, a = b = 0.1,
c = 1/ X (where X is the mean of X). After convergence,
only nine of these components were associated with the ob-
served data. (The smallest element of  associated with one
of these nine components was 0.06, while the next largest
element was 2.4  108). Figure 1 shows that the latent
components discovered by the model correspond closely to
those used to generate the data.

4.2. Marginal Likelihood

We want to evaluate the ability of GaP-NMF to choose a
good number of components to model recorded music. To
determine a good number of components, we use varia-
tional inference to t GIG-NMF with various orders K and
examine the resulting variational bounds on the marginal
log-likelihood log p(X|a, b, c).
As above, we set the prior parameters for the GaP-NMF
model to  = 1, a = b = 0.1, and c = 1/ X. We set the
prior parameters for the simplied model to a = b = 0.1
and c = 1/ X. The value of 0.1 for a and b was chosen
because it gave slightly better bounds than higher or lower
values. The results were not very sensitive to .
We computed power spectrograms from three songs: Pink
Moon by Nick Drake, Funky Kingston by Toots and the
Maytals, and a clip from the Kreutzer Sonata by Ludwig
van Beethoven. These analyses used 2048-sample (46 ms)
Hann windows with no overlap, yielding spectrograms of
1025 frequency bins by 2731, 6322, and 2584 time win-
dows, respectively. We t variational posteriors for GaP-
NMF and GIG-NMF, conditioning on these spectrograms.
We used a truncation level L of 100 for the nonparametric
model, and values of K ranging from 1 to 100 for the nite
GIG-NMF model.
The computational cost of tting the GaP-NMF model was
lower than the cost of tting GIG-NMF with K = 100
(thanks to the accelerated inference trick in section 3.3),

Bayesian Nonparametric Matrix Factorization for Recorded Music

Figure 2. Left: Bounds on log p(X|prior) for the nonparametric GaP-NMF model and its parametric counterpart GIG-NMF with differ-
ent numbers of latent components K. Ticks on the horizontal lines showing the bound for the GaP-NMF model indicate the number of
components K used to explain the data. For all three songs the values of K chosen by GaP-NMF are close to the optimal value of K for
the parametric model. Right: Geometric mean of the likelihood assigned to each censored observation by the nonparametric, nite, and
unregularized models. Ticks again indicate the number of components K used to explain the data. The unregularized models overt.
EU-NMF performs badly, with likelihoods orders of magnitude lower than the other models.

and much lower than the cost of repeatedly tting GIG-
NMF with different values of K. For example, on a single
core of a 2.3 GHz AMD Opteron 2356 Quad-Core Proces-
sor, tting the 100-component GIG-NMF model to Pink
Moon took 857 seconds, while tting the GaP-NMF model
to the same song took 645 seconds.
The results are summarized in gure 2 (left). The GaP-
NMF model used 50, 53, and 38 components to explain the
spectrograms of Funky Kingston, the Kreutzer Sonata, and
Pink Moon respectively. In each case the value of K chosen
by GaP-NMF was close to the best value of K tested for the
GIG-NMF model. This suggests that GaP-NMF performs
automatic order selection as well as the more expensive ap-
proach of tting multiple nite-order models.

4.3. Bandwidth Expansion

One application of statistical spectral analysis is band-
width expansion, the problem of inferring what the high-
frequency content of a signal is likely to be given only the
low-frequency content of the signal (Bansal et al., 2005).
This task has applications to restoration of low-bandwidth
audio and lossy audio compression. This is a missing data
problem. We compared the ability of different models and
inference algorithms to predict the held-out data.
We computed a power spectrogram from 4000 1024-
sample (23 ms) Hann windows taken from the middles
of the same three songs used to evaluate marginal like-
lihoods: Funky Kingston, the Kreutzer Sonata, and Pink
Moon. For each song, this yielded a 513  4000 spec-
trogram X describing 93 seconds of the song. We ran
ve-fold cross-validation to compare GaP-NMFs predic-
tions of the missing high-frequency content to those of
GIG-NMF, EU-NMF, and IS-NMF. (It is more difcult to

evaluate KL-NMFs ability to predict missing data, since it
does not correspond to a probabilistic model of continuous
data.) We divided each spectrogram into ve contiguous
800-frame sections. For each fold, we censored the top two
octaves (i.e., the top 384 out of 513 frequency bins) of one
of those sections. We then predicted the values in the cen-
sored bins based on the data in the uncensored bins.
The prior hyperparameters for the Bayesian models were
set to a = b = 1, c = 1/ X, and  = 1 (for GaP-NMF).
We chose a higher value for a and b for this experiment
since stronger smoothing can improve the models ability
to generalize to held-out data.
For each t model, we computed an estimate Xpred
mn of each
missing value Xmiss
mn . For the models t using variational
inference, we used the expected value of the missing data
under the variational posterior q, Eq[Ep[Xmiss
mn ]]. For the
GaP-NMF model, this expectation is

mn = Eq[Ep[Xmiss
Xpred

Eq[kWmkHkn],

and for the GIG-NMF model it is

mn ]] =(cid:80)
mn ]] =(cid:80)

k

mn = Eq[Ep[Xmiss
Xpred

Eq[WmkHkn].
IS-NMF and EU-NMF we predicted Xpred

k

mn =

(cid:80)

For

k WmkHkn.

To evaluate the quality of t for IS-NMF, GaP-NMF, and
GIG-NMF, we compute the likelihood of each unobserved
element Xmiss
mn under an exponential distribution with mean
Xpred
mn . To evaluate EU-NMF, we rst compute the mean
squared error of the estimate of the observed data 2 =
Mean[(Xobs  [W H]obs)2]. We then compute the like-
lihood of each unobserved element Xmiss
mn under a normal
distribution with mean Xpred

mn and variance 2.

Klog p(X)87000000875000008800000088500000890000008950000090000000396000003980000040000000402000004040000040600000408000004350000044000000445000004500000020406080100Funky KingstonKreutzer SonataPink MoonKGeometric Mean Probabilityof Hidden Data05000001000000150000020000000e+002e+064e+066e+068e+061e+070e+001e+062e+063e+064e+0620406080100Funky KingstonKreutzer SonataPink MoonmodelGaP-NMFEU-NMFGIG-NMFIS-NMFBayesian Nonparametric Matrix Factorization for Recorded Music

Figure 2 (right) plots the geometric mean of the likelihood
of each unobserved element of X for the nonparametric
model and for models t with different numbers of com-
ponents K. The Bayesian models do very well compared
with the unregularized models, which overt badly for any
number of components K greater than 1. GaP-NMF used
fewer components to explain the songs than in the previous
experiment, which we attribute to the stronger smoothing,
smaller number of observations, and smaller window size.

4.4. Blind Monophonic Source Separation

GaP-NMF can also be applied to blind source separation,
where the goal is to recover a set of audio signals that
combined to produce a mixed audio signal. For exam-
ple, we may want to separate a polyphonic piece of music
into notes to facilitate transcription (Smaragdis & Brown,
2003), denoising, or upmixing (Fevotte et al., 2009).
The GaP-NMF model assumes that the audio signal is a
linear combination of L sources (some of which have ex-
tremely low gain). Given the complex magnitude spectro-
gram X c of the original audio and an estimate of the model
parameters W , H, and , we can compute maximum-
likelihood estimates of the spectrograms of the L unmixed
sources using Wiener ltering (Fevotte et al., 2009):

Xlmn = X c

mn

(cid:80)

lWmlHln

i{1,...,L} iWmiHin

,

(21)

where Xlmn is the estimate of the complex magnitude
spectrum of the lth source at time n and frequency n. We
can invert these spectrograms to obtain estimates of the au-
dio signals that are combined in the mixed audio signal.
We evaluated GaP-NMFs ability to separate signals from
two synthesized pieces of music. We used synthetic music
rather than live performances so that we could easily isolate
each note. The pieces we used were a randomly generated
four-voice clarinet piece using 21 unique notes, and a two-
part Bach invention synthesized using a physical model of
a vibraphone using 36 unique notes.
We compared the Signal-to-Noise Ratios (SNRs) of the
separated tracks for the GaP-NMF model with those ob-
tained using GIG-NMF, IS-NMF, EU-NMF, and KL-NMF.
For the nite models we also used Wiener ltering to sepa-
rate the tracks, dropping  from equation 21.
The models do not provide any explicit information about
the correspondence between sources and notes. To decide
which separated signal to associate with which note, we
adopt the heuristic of assigning each note to the component
k whose gain signal H k has the highest correlation with the
power envelope of the true note signal. We only consider
the V components that make the largest contribution to the
mixed signal, where V is the true number of notes.

Figure 3. Average Signal-to-Noise Ratios (SNRs) across notes in
the source separation task. Approaches based on the exponential
likelihood model do well, EU-NMF and KL-NMF do less well.
Ticks on the horizontal lines showing GaP-NMFs performance
denote the nal number of components K used to explain the data.

Figure 3 shows the average SNRs of the tracks correspond-
ing to individual notes for each piece. The approaches
based on the exponential likelihood model do comparably
well. The KL-NMF and EU-NMF models perform con-
siderably worse, and are sensitive to the model order K.
GaP-NMF decomposed the clarinet piece into 34 compo-
nents, and the Bach invention into 42 components. In both
cases, some of these components were used to model the
temporal evolution of the instrument sounds.

5. Related Work
GaP-NMF is closely related to recent work in Bayesian
nonparametrics and probabilistic interpretations of NMF.

Bayesian Nonparametrics Most of the literature on
Bayesian nonparametric latent factor models focuses on
conjugate linear Gaussian models in conjunction with the
Indian Buffet Process (IBP) (Grifths & Ghahramani,
2005) or the Beta Process (BP) (Thibaux & Jordan, 2007),
using either MCMC or variational methods for posterior
inference (e.g. Doshi-Velez et al., 2009; Paisley & Carin,
2009)). (An exception that uses MCMC in non-conjugate
innite latent factor models is (Teh et al., 2007).)
A standard linear Gaussian likelihood model is not appro-
priate for audio spectrogram data, whereas the exponential
likelihood model has theoretical justication and gives in-
terpretable components. The nonlinearity and lack of con-
jugacy of the exponential likelihood model make inference
using an IBP or BP difcult. Our use of a gamma process
prior allows us to derive an innite latent factor model that
is appropriate for audio spectrograms and permits a simple
and efcient variational inference algorithm.

Probabilistic NMF Fevotte & Cemgil

(2009) sug-

KSignal-to-Noise Ratio-20-15-10-50-10-50520406080100120140Bach InventionClarinet PiecemodelGaP-NMFEU-NMFGIG-NMFIS-NMFKL-NMFBayesian Nonparametric Matrix Factorization for Recorded Music

gest the outline of a variational inference algorithm for
Itakura-Saito NMF based on the space-alternating gener-
alized expectation-maximization algorithm in Fevotte et al.
(2009). This approach introduces KMN complex hid-
den variables whose posteriors must be estimated. In our
informal experiments, this gave a much looser variational
bound, much longer convergence times, and a less exible
approximate posterior than the variational inference algo-
rithm presented in this paper.
Our approach of weighting the contribution of each com-
ponent k by a parameter k resembles the strategy of Au-
tomatic Relevance Determination (ARD), which has been
used in a Maximum A Posteriori (MAP) estimation algo-
rithm for a different NMF cost function (Tan & Fevotte,
2009). Though similar in spirit, this ARD approach is less
amenable to fully Bayesian inference.

6. Discussion
We developed the GaP-NMF model, a Bayesian nonpara-
metric model capable of determining the number of la-
tent sources needed to explain an audio spectrogram. We
demonstrated the effectiveness of the GaP-NMF model on
several problems in analyzing and processing recorded mu-
sic. Although this paper has focused on analyzing music,
GaP-NMF is equally applicable to other types of audio,
such as speech or environmental sounds.

Acknowledgments
We thank the reviewers for their helpful observations and
suggestions. David M. Blei is supported by ONR 175-
6343, NSF CAREER 0745520.

