Abstract

We consider here the problem of building a never-ending lan-
guage learner; that is, an intelligent computer agent that runs
forever and that each day must (1) extract, or read, informa-
tion from the web to populate a growing structured knowl-
edge base, and (2) learn to perform this task better than on
the previous day. In particular, we propose an approach and
a set of design principles for such an agent, describe a par-
tial implementation of such a system that has already learned
to extract a knowledge base containing over 242,000 beliefs
with an estimated precision of 74% after running for 67 days,
and discuss lessons learned from this preliminary attempt to
build a never-ending learning agent.

Introduction

We describe here progress toward our longer-term goal of
producing a never-ending language learner. By a never-
ending language learner we mean a computer system that
runs 24 hours per day, 7 days per week, forever, performing
two tasks each day:
1. Reading task: extract information from web text to further
populate a growing knowledge base of structured facts
and knowledge.

2. Learning task: learn to read better each day than the day
before, as evidenced by its ability to go back to yester-
days text sources and extract more information more ac-
curately.
The thesis underlying this research is that the vast redun-
dancy of information on the web (e.g., many facts are stated
multiple times in different ways) will enable a system with
the right learning mechanisms to succeed. One view of this
research is that it is a case study in lifelong, or never-ending
learning. A second view is that it is an attempt to advance the
state of the art of natural language processing. A third view
is that it is an attempt to develop the worlds largest struc-
tured knowledge base  one that reects the factual content
of the world wide web, and that would be useful to many AI
efforts.

In this paper, we rst describe a general approach to
building a never-ending language learner that uses semi-
Copyright c(cid:13) 2010, Association for the Advancement of Articial
Intelligence (www.aaai.org). All rights reserved.

supervised learning methods, an ensemble of varied knowl-
edge extraction methods, and a exible knowledge base rep-
resentation that allows the integration of the outputs of those
methods. We also discuss design principles for implement-
ing this approach.

We then describe a prototype implementation of our ap-
proach, called Never-Ending Language Learner (NELL). At
present, NELL acquires two types of knowledge: (1) knowl-
edge about which noun phrases refer to which specied
semantic categories, such as cities, companies, and sports
teams, and (2) knowledge about which pairs of noun phrases
satisfy which specied semantic relations, such as hasOf-
cesIn(organization,
location). NELL learns to acquire
these two types of knowledge in a variety of ways. It learns
free-form text patterns for extracting this knowledge from
sentences on the web, it learns to extract this knowledge
from semi-structured web data such as tables and lists, it
learns morphological regularities of instances of categories,
and it learns probabilistic horn clause rules that enable it to
infer new instances of relations from other relation instances
that it has already learned.

Finally, we present experiments showing that our imple-
mentation of NELL, given an initial seed ontology dening
123 categories and 55 relations and left to run for 67 days,
populates this ontology with 242,453 new facts with esti-
mated precision of 74%.

The main contributions of this work are: (1) progress to-
ward an architecture for building a never-ending learning
agent, and a set of design principles that help successfully
implement that architecture, (2) a web-scale experimental
evaluation of an implementation of that architecture, and (3)
one of the largest and most successful implementations of
bootstrap learning to date.

Approach

Our approach is organized around a shared knowledge base
(KB) that is continuously grown and used by a collection
of learning/reading subsystem components that implement
complementary knowledge extraction methods. The starting
KB denes an ontology (a collection of predicates dening
categories and relations), and a handful of seed examples
for each predicate in this ontology (e.g., a dozen example
cities). The goal of our approach is to continuously grow
this KB by reading, and to learn to read better.

icate instances from text resources, and another which
learns to infer relation instances from other beliefs in the
KB. This provides multiple, independent sources of the
same types of beliefs.

 Use coupled semi-supervised learning methods to lever-
age constraints between predicates being learned (Carl-
son et al. 2010). To provide opportunities for coupling,
arrange categories and relations into a taxonomy that de-
nes which categories are subsets of which others, and
which pairs of categories are mutually exclusive. Addi-
tionally, specify the expected category of each relation ar-
gument to enable type-checking. Subsystem components
and the KI can benet from methods that leverage cou-
pling.

 Distinguish high-condence beliefs in the KB from
lower-condence candidates, and retain source justica-
tions for each belief.

 Use a uniform KB representation to capture candidate
facts and promoted beliefs of all types, and use associ-
ated inference and learning mechanisms that can operate
on this shared representation.

Related Work

AI has a long history of research on autonomous agents,
problem solving, and learning, e.g., SOAR (Laird, Newell,
and Rosenbloom 1987), PRODIGY (Carbonell et al. 1991),
EURISKO (Lenat 1983), ACT-R (Anderson et al. 2004),
and ICARUS (Langley et al. 1991). In comparison, our fo-
cus to date has been on semi-supervised learning to read,
with less focus on problem-solving search. Nevertheless,
earlier work provides a variety of design principles upon
which we have drawn. For example, the role of the KB in
our approach is similar to the role of the blackboard in
early systems for speech recognition (Erman et al. 1980),
and the frame-based representation of our KB is a reimple-
mentation of the THEO system (Mitchell et al. 1991) which
was originally designed to support integrated representation,
inference and learning.

There is also previous research on life-long learning, such
as Thrun and Mitchell (1995), which focuses on using previ-
ously learned functions (e.g., a robots next-state function) to
bias learning of new functions (e.g., the robots control func-
tion). Banko and Etzioni (2007) consider a lifelong learning
setting where an agent builds a theory of a domain, and ex-
plore different strategies for deciding which of many possi-
ble learning tasks to tackle next. Although our current sys-
tem uses a simpler strategy of training all functions on each
iteration, choosing what to learn next is an important capa-
bility for lifelong learning.

Our approach employs semi-supervised bootstrap learn-
ing methods, which begin with a small set of labeled data,
train a model,
then use that model to label more data.
Yarowsky (1995) uses bootstrap learning to train classiers
for word sense disambiguation. Bootstrap learning has also
been employed successfully in many applications, includ-
ing web page classication (Blum and Mitchell 1998), and
named entity classication (Collins and Singer 1999).

Figure 1: Our Never-Ending Language Learner (NELL) architec-
ture. See Approach for an overview of the approach implemented
in NELL, and Implementation for subsystem details.

Category and relation instances added to the KB are parti-
tioned into candidate facts and beliefs. The subsystem com-
ponents can read from the KB and consult other external re-
sources (e.g., text corpora or the Internet), and then propose
new candidate facts. Components supply a probability for
each proposed candidate and a summary of the source evi-
dence supporting it. The Knowledge Integrator (KI) exam-
ines these proposed candidate facts and promotes the most
strongly supported of these to belief status. This ow of pro-
cessing is depicted in Figure 1.

In our initial implementation, this loop operates itera-
tively. On each iteration, each subsystem component is run
to completion given the current KB, and then the KI makes
decisions on which newly proposed candidate facts to pro-
mote. The KB grows iteration by iteration, providing more
and more beliefs that are then used by each subsystem com-
ponent to retrain itself to learn to read better on the next iter-
ation. In this way, our approach can be seen as implementing
a coupled, semi-supervised learning method in which mul-
tiple components learn and share complementary types of
knowledge, overseen by the KI. One can view this approach
as an approximation to an Expectation Maximization (EM)
algorithm in which the E step involves iteratively estimating
the truth values for a very large set of virtual candidate be-
liefs in the shared KB, and the M step involves retraining the
various subsystem component extraction methods.

This kind of iterative learning approach can suffer if label-
ing errors accumulate. To help mitigate this issue, we will
allow the system to interact with a human for 1015 min-
utes each day, to help it stay on track. However, in the
work reported here, we make limited use of human input.

The following design principles are important in imple-

menting our approach:
 Use subsystem components that make uncorrelated errors.
When multiple components make uncorrelated errors, the
probability that they all make the same error is the product
of their individual error probabilities, resulting in much
lower error rates.

 Learn multiple types of inter-related knowledge. For ex-
ample, we use one component that learns to extract pred-

candidate factsbeliefsData Resources(e.g., corpora)KnowledgeIntegratorSubsystem ComponentsCPLCSEALRLCMCKnowledge BaseBootstrap learning approaches can often suffer from se-
mantic drift, where labeling errors in the learning process
can accumulate (Riloff and Jones 1999; Curran, Murphy,
and Scholz 2007). There is evidence that constraining the
learning process helps to mitigate this issue. For example,
if classes are mutually exclusive, they can provide negative
examples for each other (Yangarber 2003). Relation argu-
ments can also be type-checked to ensure that they match
expected types (Pasca et al. 2006). Carlson et al. (2010)
employ such strategies and use multiple extraction methods,
which are required to agree. Carlson et al. refer to the idea
of adding many constraints between functions being learned
as coupled semi-supervised learning. Chang, Ratinov, and
Roth (2007) also showed that enforcing constraints given as
domain knowledge can improve semi-supervised learning.

Pennacchiotti and Pantel (2009) present a framework
for combining the outputs of an ensemble of extraction
methods, which they call Ensemble Semantics. Multi-
ple extraction systems provide candidate category instances,
which are then ranked using a learned function that uses
features from many different sources (e.g., query logs,
Wikipedia). Their approach uses a more sophisticated rank-
ing method than ours, but is not iterative. Thus, their ideas
are complementary to our work, as we could use their rank-
ing method as part of our general approach.

Other previous work has demonstrated that pattern-based
and list-based extraction methods can be combined in a syn-
ergistic fashion to achieve signicant improvements in re-
call (Etzioni et al. 2004). Downey, Etzioni, and Soder-
land (2005) presented a probabilistic model for using and
training multiple extractors where the extractors (in their
work, different extraction patterns) make uncorrelated er-
rors. It would be interesting to apply a similar probabilistic
model to cover the setting in this paper, where there are mul-
tiple extraction methods which themselves employ multiple
extractors (e.g., textual patterns, wrappers, rules).

Nahm and Mooney (2000) rst demonstrated that infer-
ence rules could be mined from beliefs extracted from text.
Our work can also be seen as an example of multi-task
learning in which several different functions are trained to-
gether, as in (Caruana 1997; Yang, Kim, and Xing 2009), in
order to improve learning accuracy. Our approach involves
a kind of multi-task learning of multiple types of functions
 531 functions in total in the experiments reported here
 in which different methods learn different functions with
overlapping inputs and outputs, and where constraints pro-
vided by the ontology (e.g., athlete is a subset of person,
and mutually exclusive with city) support accurate semi-
supervised learning of the entire ensemble of functions.

Implementation

We have implemented a preliminary version of our ap-
proach. We call this implementation Never-Ending Lan-
guage Learner (NELL). NELL uses four subsystem com-
ponents (Figure 1):
 Coupled Pattern Learner (CPL): A free-text extractor
which learns and uses contextual patterns like mayor of
X and X plays for Y  to extract instances of categories

and relations. CPL uses co-occurrence statistics between
noun phrases and contextual patterns (both dened using
part-of-speech tag sequences) to learn extraction patterns
for each predicate of interest and then uses those patterns
to nd additional instances of each predicate. Relation-
ships between predicates are used to lter out patterns
that are too general. CPL is described in detail by Carl-
son et al. (2010). Probabilities of candidate instances ex-
tracted by CPL are heuristically assigned using the for-
mula 10.5c, where c is the number of promoted patterns
that extract a candidate.
In our experiments, CPL was
given as input a corpus of 2 billion sentences, which was
generated by using the OpenNLP package1 to extract, to-
kenize, and POS-tag sentences from the 500 million web
page English portion of the ClueWeb09 data set (Callan
and Hoy 2009).

 Coupled SEAL (CSEAL): A semi-structured extractor
which queries the Internet with sets of beliefs from each
category or relation, and then mines lists and tables to
extract novel instances of the corresponding predicate.
CSEAL uses mutual exclusion relationships to provide
negative examples, which are used to lter out overly gen-
eral lists and tables. CSEAL is also described by Carlson
et al. (2010), and is based on code provided by Wang and
Cohen (2009). Given a set of seed instances, CSEAL per-
forms queries by sub-sampling beliefs from the KB and
using these sampled seeds in a query. CSEAL was con-
gured to issue 5 queries for each category and 10 queries
for each relation, and to fetch 50 web pages per query.
Candidate facts extracted by CSEAL are assigned proba-
bilities using the same method as for CPL, except that c is
the number of unltered wrappers that extract an instance.
 Coupled Morphological Classier (CMC): A set of bi-
nary L2-regularized logistic regression modelsone per
categorywhich classify noun phrases based on vari-
ous morphological features (words, capitalization, afxes,
parts-of-speech, etc.). Beliefs from the KB are used as
training instances, but at each iteration CMC is restricted
to predicates which have at least 100 promoted instances.
As with CSEAL, mutual exclusion relationships are used
to identify negative instances. CMC examines candidate
facts proposed by other components, and classies up to
30 new beliefs per predicate per iteration, with a mini-
mum posterior probability of 0.75. These heuristic mea-
sures help to ensure high precision.

 Rule Learner (RL): A rst-order relational learning al-
gorithm similar to FOIL (Quinlan and Cameron-Jones
1993), which learns probabilistic Horn clauses. These
learned rules are used to infer new relation instances from
other relation instances that are already in the KB.
Our implementation of the Knowledge Integrator (KI)
promotes candidate facts to the status of beliefs using a hard-
coded, intuitive strategy. Candidate facts that have high con-
dence from a single source (those with posterior > 0.9) are
promoted, and lower-condence candidates are promoted if

1http://opennlp.sourceforge.net

they have been proposed by multiple sources. KI exploits re-
lationships between predicates by respecting mutual exclu-
sion and type checking information. In particular, candidate
category instances are not promoted if they already belong
to a mutually exclusive category, and relation instances are
not promoted unless their arguments are at least candidates
for the appropriate category types (and are not already be-
lieved to be instances of a category that is mutually exclusive
with the appropriate type). In our current implementation,
once a candidate fact is promoted as a belief, it is never de-
moted. The KI is congured to promote up to 250 instances
per predicate per iteration, but this threshold was rarely hit
in our experiments.

The KB in NELL is a reimplementation of the THEO
frame-based representation (Mitchell et al. 1991) based on
Tokyo Cabinet2, a fast, lightweight key/value store. The KB
can handle many millions of values on a single machine.

Experimental Evaluation

We conducted an experimental evaluation to explore the fol-
lowing questions:
 Can NELL learn to populate many different categories
(100+) and relations (50+) for dozens of iterations of
learning and maintain high precision?
 How much do the different components contribute to the

promoted beliefs held by NELL?

Methodology
The input ontology used in our experiments included 123
categories each with 1015 seed instances and 5 seed pat-
terns for CPL (derived from Hearst patterns (Hearst 1992)).
Categories included locations (e.g., mountains, lakes, cities,
museums), people (e.g., scientists, writers, politicians, mu-
sicians), animals (e.g., reptiles, birds, mammals), organiza-
tions (e.g., companies, universities, web sites, sports teams),
and others. 55 relations were included, also with 1015
seed instances and 5 negative instances each (which were
typically generated by permuting the arguments of seed in-
stances). Relations captured relationships between the dif-
ferent categories (e.g., teamPlaysSport, bookWriter, compa-
nyProducesProduct).

In our experiments, CPL, CSEAL, and CMC ran once per
iteration. RL was run after each batch of 10 iterations, and
the proposed output rules were ltered by a human. Manual
approval of these rules took only a few minutes.

To estimate the precision of the beliefs in the KB pro-
duced by NELL, beliefs from the nal KB were randomly
sampled and evaluated by several human judges. Cases of
disagreement were discussed in detail before a decision was
made. Facts which were once true but are not currently (e.g.,
a former coach of a sports team) were considered to be cor-
rect for this evaluation, as NELL does not currently deal
with temporal scope in its beliefs. Spurious adjectives (e.g.,
todays Chicago Tribune) were allowed, but rare.

2http://1978th.net/tokyocabinet

Predicate
ethnicGroup
arthropod
female
sport
profession
magazine
bird
river
mediaType

Instance
Cubans
spruce beetles
Kate Mara
BMX bicycling
legal assistants
Thrasher
Buff-throated Warbler
Fording River
chemistry books

Source(s)
CSEAL
CPL, CSEAL
CPL, CMC
CSEAL, CMC
CPL
CPL
CSEAL
CPL, CMC
CPL, CMC

(troy, Michigan)

cityInState
CSEAL
musicArtistGenre (Nirvana, Grunge)
CPL
tvStationInCity
CPL, CSEAL
(WLS-TV, Chicago)
sportUsesEquip
CPL
(soccer, balls)
athleteInLeague
(Dan Fouts, NFL)
RL
(Will Smith, Seven Pounds) CPL
starredIn
productType
(Acrobat Reader, FILE)
CPL
RL
athletePlaysSport (scott shields, baseball)
cityInCountry
CPL

(Dublin Airport, Ireland)

Table 1: Example beliefs promoted by NELL.

Results
After running for 67 days, NELL completed 66 iterations of
execution. 242,453 beliefs were promoted across all pred-
icates, 95% of which were instances of categories and 5%
of relations. Example beliefs from a variety of predicates,
along with the source components that extracted them, are
shown in Table 1.

Following an initial burst of almost 10,000 beliefs pro-
moted during the rst iteration, NELL continued to promote
a few thousand more on every successive iteration, indicat-
ing strong potential to learn more if it were left to run for
a longer time. Figure 2 shows different views of the pro-
motion activity of NELL over time. The left-hand gure
shows overall numbers of promotions for categories and re-
lations in each iteration. Category instances are promoted
fairly steadily, while relation instance promotions are spiky.
This is mainly because the RL component only runs every 10
iterations, and is responsible for many of the relation promo-
tions. The right-hand gures are stacked bar plots showing
the proportion of predicates with various levels of promo-
tion activity during different spans of iterations. These plots
show that instances are promoted for many different cate-
gories and relations during the whole run of NELL.

To estimate the precision of beliefs promoted during var-
ious stages of execution, we considered three time periods:
iterations 122, iterations 2344, and iterations 4566. For
each of these time periods, we uniformly sampled 100 be-
liefs promoted during those periods and judged their cor-
rectness. The results are shown in Table 2. During the three
periods, the promotion rates are very similar, with between
76,000 and 89,000 instances promoted. There is a down-
ward trend in estimated precision, going from 90% to 71%
to 57%. Taking a weighted average of these three estimates
of precision based on numbers of promotions, the overall
estimated precision across all 66 iterations is 74%.

Figure 2: Promotion activity for beliefs over time. Left: The number of beliefs promoted for all category and relation predicates in each
iteration. Periodic spikes among relation predicates occur every 10 iterations after the RL component runs. Center and Right: Stacked bar
plots detailing the proportion of predicates (and counts of predicates, shown inside the bars) at various levels of promotion activity over time
for categories and relations. Note that, while some predicates become dormant early on, the majority continue to show healthy levels of
promotion activity even in later iterations of learning.

Iterations

Estimated Precision (%)

# Promotions

122
2344
4566

90
71
57

88,502
77,835
76,116

Table 2: Estimates of precision (from 100 sampled beliefs) and
numbers of promoted beliefs across all predicates during iterations
122, 2344, and 4566. Note that the estimates of precision only
consider beliefs promoted during a time period and ignore beliefs
promoted earlier.

Only a few items were debated by the judges: examples
are right posterior, which was judged to not refer to a
body part, and green leafy salad, which was judged accept-
able as a type of vegetable. Proceedings was promoted as
a publication, which we considered incorrect (it was most
likely due to noun-phrase segmentation errors within CPL).
Two errors were due to languages (Klingon Language and
Mandarin Chinese language) being promoted as ethnic
groups. (Southwest, San Diego) was labeled as an in-
correct instance of the hasOfcesIn relation, since South-
west Airlines does not have an ofcial corporate ofce there.
Many system errors were subtle; one might expect a non-
native reader of English to make similar mistakes.

To estimate precision at the predicate level, we randomly
chose 7 categories and 7 relations which had at least 10 pro-
moted instances. For each chosen predicate, we sampled 25
beliefs from iterations 122, 2344, and 4566, and judged
their correctness. Table 3 shows these predicates and, for
each time period, the estimates of precision and the num-
ber of beliefs promoted. Most predicates are very accurate,
with precision exceeding 90%. Two predicates in partic-
ular, cardGame and productType, fare much worse. The
cardGame category seems to suffer from the abundance of
web spam related to casino and card games, which results in
parsing errors and other problems. As a result of this noise,
NELL ends up extracting strings of adjectives and nouns
like deposit casino bonuses free online list as incorrect in-

Figure 3: Source counts for beliefs promoted by NELL after 66 it-
erations. Numbers inside nodes indicate the number of beliefs pro-
moted based solely on that component. Numbers on edges indicate
beliefs promoted based on evidence from multiple components.

stances of cardGame. Most errors for the productType rela-
tion came from associating product names with more gen-
eral nouns that are somehow related to the product but do
not correctly indicate what kind of thing the product is, e.g.,
(Microsoft Ofce, PC). Some of these productType be-
liefs were debated by the judges, but were ultimately labeled
incorrect, e.g., (Photoshop, graphics). In our ontology,
the category for the second argument of productType is a
general item super-category in the hierarchy; we posit that
a more specic product type category might lead to more
restrictive type checking.

As described in the Implementation section, NELL uses
a Knowledge Integrator which promotes high-condence
single-source candidate facts, as well as candidate facts with
multiple lower-condence sources. Figure 3 illustrates the
impact of each component within this integration strategy.
Each component is shown containing a count which is the
number of beliefs that were promoted based on that source
alone having high condence in that belief. Lines connect-
ing components are labeled with counts that are the number
of beliefs promoted based on those components each hav-
ing some degree of condence in that candidate. CPL and
CSEAL each were responsible for many promoted beliefs
on their own. However, more than half of the beliefs pro-
moted by KI were based on multiple sources of evidence.

0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 0 20 40 60 categories relations 16 22 22 23 24 8 6 17 6 6 48 60 59 57 60 65 36 21 20 23 22 16 0% 20% 40% 60% 80% 100% 1-11 12-22 23-33 34-44 45-55 56-66 501+ promotions 101-500 11-100 1-10 none 8 10 9 9 11 9 3 3 6 8 8 18 22 18 13 14 15 10 4 6 9 6 3 0% 20% 40% 60% 80% 100% 1-11 12-22 23-33 34-44 45-55 56-66 501+ promotions 101-500 11-100 1-10 none # of promoted beleifsiterationiterationsiterations% of predicatescategory activityrelation activityoverall activitycategoriesrelationspredicates with:CPL48,786CSEAL51,987CMC423RL1,91458,88045,4032534,44750978Predicate
cardGame
city
magazine
recordLabel
restaurant
scientist
vertebrate

122
40
92
96
100
96
96
100

2344
20
80
68
100
88
100
100

4566
0
96
80
100
92
100
96

athletePlaysForTeam
ceoOfCompany
coachesTeam
productType
teamPlaysAgainstTeam
teamPlaysSport
teamWonTrophy

100
100
100
28
96
100
88

100
100
100
44
100
100
72

100
100
100
20
100
86
44

Estimated Precision

# Promotions

122
584
4,311
1,235
1,384
242
768
1,196

113
82
196
35
283
79
119

2344
552
3,362
788
890
568
1
1,362

4566
2,472
1,002
664
748
523
404
714

304
8
121
156
553
158
104

39
9
12
195
232
14
174

Table 3: For selected categories (top) and relations (bottom), estimates of precision (from 25 sampled beliefs) and counts for beliefs promoted
during iterations 122, 2344, and 4566.

Predicate
emotion
beverage
newspaper
teamPlaysInLeague X ranks second in Y
bookAuthor

Pattern
hearts full of X
cup of aromatic X
op-ed page of X

Y classic X

Table 4: Example free-text patterns learned by CPL. X and Y
represent placeholders for noun phrases to be extracted.

While RL was not responsible for many promoted beliefs,
those that it did propose with high condence appear to be
largely independent from those of the other components.

RL learned an average of 66.5 novel rules per iteration,
of which 92% were approved. 12% of the approved rules
implied at least one candidate instance that had not yet been
implied by another rule, and those rules implied an average
of 69.5 such instances.

To give a sense of what is being learned by the differ-
ent components used in NELL, we provide examples for
each component. Table 4 shows contextual patterns learned
by CPL. Table 5 shows web page wrappers learned by
CSEAL. Example weights from the logistic regression clas-
siers learned by CMC are shown in Table 6. Finally, exam-
ple rules induced by RL are shown in Table 7.

Supplementary Online Materials Several types of sup-
plementary materials from our evaluation are posted online3,
including: (1) all promoted instances, (2) all categories, re-
lations, and seed instances, (3) all labeled instances sampled
for estimating precision, (4) all patterns promoted by CPL,
and (5) all rules learned by RL.

3http://rtw.ml.cmu.edu/aaai10_online

Predicate
mountain
mountain
mountain
musicArtist
musicArtist
musicArtist
newspaper
newspaper
newspaper
university
university
university
university
visualArtMovement
visualArtMovement
visualArtMovement

Feature
LAST=peak
LAST=mountain
FIRST=mountain
LAST=band
POS=DT NNS
POS=DT JJ NN
LAST=sun
LAST=press
LAST=university
LAST=college
PREFIX=uc
LAST=university
FIRST=college
SUFFIX=ism
PREFIX=journ
PREFIX=budd

Weight
1.791
1.093
-0.875
1.853
1.412
-0.807
1.330
1.276
-0.318
2.076
1.999
1.745
-1.381
1.282
-0.234
-0.253

Table 6: Example feature weights induced by the morphology
classier. Positive and negative weights indicate positive and neg-
ative impacts on predicted probabilities, respectively. Note that
mountain and college have different weights when they begin
or end an instance. The learned model uses part-of-speech features
to identify typical music group names (e.g., The Beatles, The Ra-
mones), as well as prexes to disambiguate art movements from,
say, academic elds and religions.

Discussion
These results are promising. NELL maintained high preci-
sion for many iterations of learning with a consistent rate of
knowledge accumulation, all with a very limited amount of
human guidance. We consider this to be signicant progress
toward our goal of building a never-ending language learner.
In total, NELL learned 531 coupled functions, since 3 differ-
ent subsystems (CMC, CPL, and CSEAL) learn about 123
categories, and 3 different subsystems (CPL, CSEAL, and
RL) learn about 55 relations.

Predicate
academicField
athlete
bird
bookAuthor

Web URL
http://scholendow.ais.msu.edu/student/ScholSearch.Asp
http://www.quotes-search.com/d occupation.aspx?o=+athlete
http://www.michaelforsberg.com/stock.html
http://lifebehindthecurve.com/

Extraction Template
&nbsp;[X] -
<a href=d author.aspx?a=[X]>-
<option>[X]</option>
</li> <li>[X] by [Y ] &#8211;

Table 5: Examples of web page extraction templates learned by the CSEAL subsystem. [X] and [Y ] represent placeholders for instances to
be extracted (categories have only one placeholder; relations have two).

Probability Consequent

Antecedents

0.95
0.91
0.90
0.88
 0.62

athletePlaysSport(X, basketball)  athleteInLeague(X, NBA)
teamPlaysInLeague(X, NHL)
athleteInLeague(X, Y )
cityInState(X, Y )
newspaperInCity(X, New York)  companyEconomicSector(X, media), generalizations(X, blog)

 teamWonTrophy(X, Stanley Cup)
 athletePlaysForTeam(X, Z), teamPlaysInLeague(Z, Y )
 cityCapitalOfState(X, Y ), cityInCountry(X, USA)

Table 7: Example horn clauses induced by the rule learner. Probabilities indicate the conditional probability that the literal to the left of 
is true given that the literals to the right are satised. Each rule captures an empirical regularity among the relations mentioned by the rule.
The rule marked with  was rejected during human inspection.

The stated goal for the system is to each day read more
of the web to further populate its KB, and to each day learn
to read more facts more accurately. As the KB growth over
the past 67 days illustrate, the system does read more be-
liefs each day. Each day it also learns new extraction rules
to further populate its KB, new extractors based on morpho-
logical features, new Horn clause rules that infer unread be-
liefs from other beliefs in the KB, and new URL-specic
extractors that leverage HTML structure. Although NELLs
ongoing learning allows it to extract more facts each day, the
precision of the extracted facts declines slowly over time. In
part this is due to the fact that the easiest extractions oc-
cur during early iterations, and later iterations demand more
accurate extractors to achieve the same level of precision.
However, it is also the case that NELL makes mistakes that
lead to learning to make additional mistakes. Although we
consider the current system promising, much research re-
mains to be done.

The importance of our design principle of using compo-
nents which make mostly independent errors is generally
supported by the results. More than half of the beliefs were
promoted based on evidence from multiple sources. How-
ever, in looking at errors made by the system, it is clear that
CPL and CMC are not perfectly uncorrelated in their errors.
As an example, for the category bakedGood, CPL learns the
pattern X are enabled in because of the believed instance
cookies. This leads CPL to extract persistent cookies as
a candidate bakedGood. CMC outputs high probability for
phrases that end in cookies, and so persistent cookies is
promoted as a believed instance of bakedGood.

This behavior, as well as the slow but steady decline in
precision of beliefs promoted by NELL, suggests an oppor-
tunity for leveraging more human interaction in the learning
process. Currently, such interaction is limited to approving
or rejecting inference rules proposed by RL. However, we
plan to explore other forms of human supervision, limited
to approximately 1015 minutes per day. In particular, ac-

tive learning (Settles 2009) holds much promise by allowing
NELL to ask queries about its beliefs, theories, or even
features about which it is uncertain. For example, a pattern
like X are enabled in is only likely to occur with a few
instances of the bakedGood category. This could be a poor
pattern that leads to semantic drift, or it could be an oppor-
tunity to discover some uncovered subset of the bakedGood
category. If NELL can adequately identify such opportuni-
ties for knowledge, a human can easily provide a label for
this single pattern and convey a substantial amount of infor-
mation in just seconds. Previous work has shown that label-
ing features (e.g., context patterns) rather than instances can
lead to signicant improvements in terms of reducing human
annotation time (Druck, Settles, and McCallum 2009).

Conclusion

We have proposed an architecture for a never-ending lan-
guage learning agent, and described a partial implementa-
tion of that architecture which uses four subsystem com-
ponents that learn to extract knowledge in complimentary
ways. After running for 67 days, this implementation pop-
ulated a knowledge base with over 242,000 facts with an
estimated precision of 74%.

These results illustrate the benets of using a diverse set
of knowledge extraction methods which are amenable to
learning, and a knowledge base which allows the storage of
candidate facts as well as condent beliefs. There are many
opportunities for improvement, though, including: (1) self-
reection to decide what to do next, (2) more effective use
of 1015 minutes of daily human interaction, (3) discov-
ery of new predicates to learn, (4) learning additional types
of knowledge about language, (5) entity-level (rather than
string-level) modeling, and (6) more sophisticated proba-
bilistic modeling throughout the implementation.

Langley, P.; McKusick, K. B.; Allen, J. A.; Iba, W. F.; and
Thompson, K. 1991. A design for the ICARUS architecture.
SIGART Bull. 2(4):104109.
Lenat, D. B. 1983. Eurisko: A program that learns new
heuristics and domain concepts. Artif. Intel. 21(1-2):6198.
Mitchell, T. M.; Allen, J.; Chalasani, P.; Cheng, J.; Etzioni,
O.; Ringuette, M. N.; and Schlimmer, J. C. 1991. Theo:
A framework for self-improving systems. Arch. for Intelli-
gence 323356.
Nahm, U. Y., and Mooney, R. J. 2000. A mutually benecial
integration of data mining and information extraction.
In
Proc. of AAAI.
Pasca, M.; Lin, D.; Bigham, J.; Lifchits, A.; and Jain, A.
2006. Names and similarities on the web: fact extraction in
the fast lane. In Proc. of ACL.
Pennacchiotti, M., and Pantel, P. 2009. Entity extraction via
ensemble semantics. In Proc. of EMNLP.
Quinlan, J. R., and Cameron-Jones, R. M. 1993. Foil: A
midterm report. In Proc. of ECML.
Riloff, E., and Jones, R. 1999. Learning dictionaries for in-
formation extraction by multi-level bootstrapping. In Proc.
of AAAI.
Settles, B. 2009. Active learning literature survey. Computer
Sciences Technical Report 1648, University of Wisconsin
Madison.
Thrun, S., and Mitchell, T. 1995. Lifelong robot learning.
In Robotics and Autonomous Systems, volume 15, 2546.
Wang, R. C., and Cohen, W. W. 2009. Character-level analy-
sis of semi-structured documents for set expansion. In Proc.
of EMNLP.
Yang, X.; Kim, S.; and Xing, E. 2009. Heterogeneous mul-
titask learning with joint sparsity constraints. In NIPS 2009.
Yangarber, R. 2003. Counter-training in discovery of se-
mantic patterns. In Proc. of ACL.
Yarowsky, D. 1995. Unsupervised word sense disambigua-
tion rivaling supervised methods. In Proc. of ACL.

Acknowledgments

This work is supported in part by DARPA (under con-
tract numbers FA8750-08-1-0009 and AF8750-09-C-0179),
Google, a Yahoo! Fellowship to Andrew Carlson, and the
Brazilian research agency CNPq. We also gratefully ac-
knowledge Jamie Callan for the ClueWeb09 web crawl and
Yahoo! for use of their M45 computing cluster. We thank
the anonymous reviewers for their helpful comments.

