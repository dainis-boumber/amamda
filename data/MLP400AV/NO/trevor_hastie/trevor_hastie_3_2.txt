Abstract

In this paper we study boosting methods from a new perspective. We build on recent work by Efron
et al. to show that boosting approximately (and in some cases exactly) minimizes its loss criterion
with an l1 constraint on the coefcient vector. This helps understand the success of boosting with
early stopping as regularized tting of the loss criterion. For the two most commonly used crite-
ria (exponential and binomial log-likelihood), we further show that as the constraint is relaxedor
equivalently as the boosting iterations proceedthe solution converges (in the separable case) to an
l1-optimal separating hyper-plane. We prove that this l1-optimal separating hyper-plane has the
property of maximizing the minimal l1-margin of the training data, as dened in the boosting liter-
ature. An interesting fundamental similarity between boosting and kernel support vector machines
emerges, as both can be described as methods for regularized optimization in high-dimensional
predictor space, using a computational trick to make the calculation practical, and converging to
margin-maximizing solutions. While this statement describes SVMs exactly, it applies to boosting
only approximately.
Keywords: boosting, regularized optimization, support vector machines, margin maximization

1. Introduction and Outline

Boosting is a method for iteratively building an additive model

FT (x) =

T(cid:229)

t=1

th jt (x);

(1)

where h jt 2 H a large (but we will assume nite) dictionary of candidate predictors or weak
learners; and h jt is the basis function selected as the best candidate to modify the function at
stage t. The model FT can equivalently be represented by assigning a coefcient to each dictionary

c(cid:13)2004 Saharon Rosset, Ji Zhu and Trevor Hastie.

a
ROSSET, ZHU AND HASTIE

function h 2 H rather than to the selected h jt s only:

FT (x) =

J(cid:229)

j=1

h j(x) (cid:1) b (T )

j

;

(2)

where J = jH j and b (T )
vector b (T ) as a vector in R J or, equivalently, as the hyper-plane which has b
interpretation will play a key role in our exposition.

t. The b  representation allows us to interpret the coefcient
(T ) as its normal. This

j = (cid:229)

jt = j a

Some examples of common dictionaries are:
(cid:15) The training variables themselves, in which case h j(x) = x j. This leads to our additive
model FT being just a linear model in the original data. The number of dictionary functions
will be J = d, the dimension of x.

(cid:15) Polynomial dictionary of degree p, in which case the number of dictionary functions will be

J =(cid:18) p + d
d (cid:19).

(cid:15) Decision trees with up to k terminal nodes, if we limit the split points to data points (or mid-
way between data points as CART does). The number of possible trees is bounded from
above (trivially) by J (cid:20) (np)k (cid:1) 2k2. Note that regression trees do not t into our framework,
since they will give J = 

.

The boosting idea was rst introduced by Freund and Schapire (1995), with their AdaBoost
algorithm. AdaBoost and other boosting algorithms have attracted a lot of attention due to their great
success in data modeling tasks, and the mechanism which makes them work has been presented
and analyzed from several perspectives. Friedman et al. (2000) develop a statistical perspective,
which ultimately leads to viewing AdaBoost as a gradient-based incremental search for a good
additive model (more specically, it is a coordinate descent algorithm), using the exponential loss
function C(y; F) = exp((cid:0)yF), where y 2 f(cid:0)1;1g. The gradient boosting (Friedman, 2001) and
anyboost (Mason et al., 1999) generic algorithms have used this approach to generalize the boosting
idea to wider families of problems and loss functions. In particular, Friedman et al. (2000) have
pointed out that the binomial log-likelihood loss C(y; F) = log(1 + exp((cid:0)yF)) is a more natural
loss for classication, and is more robust to outliers and misspecied data.

A different analysis of boosting, originating in the machine learning community, concentrates on
the effect of boosting on the margins yiF(xi). For example, Schapire et al. (1998) use margin-based
arguments to prove convergence of boosting to perfect classication performance on the training
data under general conditions, and to derive bounds on the generalization error (on future, unseen
data).

In this paper we combine the two approaches, to conclude that gradient-based boosting can be
described, in the separable case, as an approximate margin maximizing process. The view we de-
velop of boosting as an approximate path of optimal solutions to regularized problems also justies
early stopping in boosting as specifying a value for regularization parameter.

We consider the problem of minimizing non-negative convex loss functions (in particular the
exponential and binomial log-likelihood loss functions) over the training data, with an l1 bound on
the model coefcients:

b (c) = arg min
kb k1(cid:20)c

C(yi; h(xi)0b ):

i

942

(3)

(cid:229)
BOOSTING AS A REGULARIZED PATH

Where h(xi) = [h1(xi); h2(xi); : : : ; hJ(xi)]0 and J = jH j.1

; 8t in (1), with e

Hastie et al. (2001, Chapter 10) have observed that slow gradient-based boosting (i.e., we set
t = e
small) tends to follow the penalized path b (c) as a function of c, under
some mild conditions on this path. In other words, using the notation of (2), (3), this implies that
kb (c=e ) (cid:0) b (c)k vanishes with e , for all (or a wide range of) values of c. Figure 1 illustrates this
equivalence between e -boosting and the optimal solution of (3) on a real-life data set, using squared
error loss as the loss function. In this paper we demonstrate this equivalence further and formally

Lasso

Forward Stagewise

lcavol

lcavol

s
t

i

n
e
c
i
f
f

e
o
C

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

PSfrag replacements

0.0

0.5

1.0
j (c)j

j jb

1.5

2.0

s
t

i

n
e
c
i
f
f

e
o
C

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

svi

lweight
pgg45

lbph

gleason

age

lcp

2.5

svi

lweight
pgg45

lbph

gleason

age

lcp

0

50

100

150
Iteration

200

250

Figure 1: Exact coefcient paths(left) for l1-constrained squared error regression and boosting

coefcient paths (right) on the data from a prostate cancer study

state it as a conjecture. Some progress towards proving this conjecture has been made by Efron et al.
(2004), who prove a weaker local result for the case where C is squared error loss, under some
mild conditions on the optimal path. We generalize their result to general convex loss functions.

Combining the empirical and theoretical evidence, we conclude that boosting can be viewed as

an approximate incremental method for following the l1-regularized path.

We then prove that in the separable case, for both the exponential and logistic log-likelihood

loss functions, b (c)=c converges as c ! 

to an optimal separating hyper-plane b described by

b = arg max
kb k1=1

min

i

yib

0h(xi):

(4)

In other words, b maximizes the minimal margin among all vectors with l1-norm equal to 1.2 This
result generalizes easily to other lp-norm constraints. For example, if p = 2, then b describes the
optimal separating hyper-plane in the Euclidean sense, i.e., the same one that a non-regularized
support vector machine would nd.

Combining our two main results, we get the following characterization of boosting:

1. Our notation assumes that the minimum in (3) is unique, which requires some mild assumptions. To avoid notational
complications we use this slightly abusive notation throughout this paper. In Appendix B we give explicit conditions
for uniqueness of this minimum.

2. The margin maximizing hyper-plane in (4) may not be unique, and we show that in that case the limit b

is still dened

and it also maximizes the second minimal margin. See Appendix B.2 for details.

943

a
(cid:229)
ROSSET, ZHU AND HASTIE

e -Boosting can be described as a gradient-descent search, approximately following the
path of l1-constrained optimal solutions to its loss criterion, and converging, in the
separable case, to a margin maximizer in the l1 sense.

Note that boosting with a large dictionary H (in particular if n < J = jH j) guarantees that the data
will be separable (except for pathologies), hence separability is a very mild assumption here.

As in the case of support vector machines in high dimensional feature spaces, the non-regularized
optimal separating hyper-plane is usually of theoretical interest only, since it typically represents
an over-tted model. Thus, we would want to choose a good regularized model. Our results indicate
that Boosting gives a natural method for doing that, by stopping early in the boosting process. Fur-
thermore, they point out the fundamental similarity between Boosting and SVMs: both approaches
allow us to t regularized models in high-dimensional predictor space, using a computational trick.
They differ in the regularization approach they takeexact l2 regularization for SVMs, approximate
l1 regularization for Boosting-and in the computational trick that facilitates ttingthe kernel
trick for SVMs, coordinate descent for Boosting.

1.1 Related Work

Schapire et al. (1998) have identied the normalized margins as distance from an l1-normed sep-
arating hyper-plane. Their results relate the boosting iterations success to the minimal margin of
the combined model. Ratsch et al. (2001b) take this further using an asymptotic analysis of Ad-
aBoost. They prove that the normalized minimal margin, mini yi (cid:229)
tj, is asymptoti-
cally equal for both classes. In other words, they prove that the asymptotic separating hyper-plane is
equally far away from the closest points on either side. This is a property of the margin maximizing
separating hyper-plane as we dene it. Both papers also illustrate the margin maximizing effects of
AdaBoost through experimentation. However, they both stop short of proving the convergence to
optimal (margin maximizing) solutions.

tht(xi)=(cid:229)

t ja

t a

Motivated by our result, Ratsch and Warmuth (2002) have recently asserted the margin-maximizing

properties of e -AdaBoost, using a different approach than the one used in this paper. Their results
relate only to the asymptotic convergence of innitesimal AdaBoost, compared to our analysis of
the regularized path traced along the way and of a variety of boosting loss functions, which also
leads to a convergence result on binomial log-likelihood loss.

The convergence of boosting to an optimal solution from a loss function perspective has been
analyzed in several papers. Ratsch et al. (2001a) and Collins et al. (2000) give results and bounds on
the convergence of training-set loss, (cid:229)
tht(xi)), to its minimum. However, in the separable
case convergence of the loss to 0 is inherently different from convergence of the linear separator to
the optimal separator. Any solution which separates the two classes perfectly can drive the expo-
nential (or log-likelihood) loss to 0, simply by scaling coefcients up linearly.

iC(yi;(cid:229)

t a

Two recent papers have made the connection between boosting and l1 regularization in a slightly
different context than this paper. Zhang (2003) suggests a shrinkage version of boosting which
converges to l1 regularized solutions, while Zhang and Yu (2003) illustrate the quantitative relation-
ship between early stopping in boosting and l1 constraints.

944

BOOSTING AS A REGULARIZED PATH

2. Boosting as Gradient Descent

Generic gradient-based boosting algorithms (Friedman, 2001; Mason et al., 1999) attempt to nd a
good linear combination of the members of some dictionary of basis functions to optimize a given
loss function over a sample. This is done by searching, at each iteration, for the basis function which
gives the steepest descent in the loss, and changing its coefcient accordingly. In other words,
this is a coordinate descent algorithm in RJ, where we assign one dimension (or coordinate) for
the coefcient of each dictionary function.

Assume we have data fxi; yign

i=1 with xi 2 Rd, a loss (or cost) function C(y; F), and a set of
dictionary functions fh j(x)g : Rd ! R. Then all of these algorithms follow the same essential
steps:

Algorithm 1 Generic gradient-based boosting algorithm

1. Set b (0) = 0.

2. For t = 1 : T ,

 C(yi;Fi)

 Fi

(a) Let Fi = b (t(cid:0)1)0h(xi); i = 1; : : : ; n (the current t).
(b) Set wi =
(c) Identify jt = argmax j j(cid:229)
(d) Set b (t)

i wih jt (xi)) and b (t)

; i = 1; : : : ; n.

jt = b (t(cid:0)1)

jt (cid:0) a

i wih j(xi)j.

tsign((cid:229)

k = b (t(cid:0)1)

k

; k 6= jt.

Here b (t) is the current coefcient vector and a

t > 0 is the current step size. Notice that (cid:229)

i wih jt (xi) =

i C(yi;Fi)

.

jt
As we mentioned, Algorithm 1 can be interpreted simply as a coordinate descent algorithm in
weak learner space. Implementation details include the dictionary H of weak learners, the loss
function C(y; F), the method of searching for the optimal jt and the way in which a
t is determined.3
For example, the original AdaBoost algorithm uses this scheme with the exponential loss C(y; F) =
exp((cid:0)yF), and an implicit line search to nd the best a
t once a direction jt has been chosen (see
Hastie et al., 2001; Mason et al., 1999). The dictionary used by AdaBoost in this formulation would
be a set of candidate classiers, i.e., h j(xi) 2 f(cid:0)1; +1gusually decision trees are used in practice.

2.1 Practical Implementation of Boosting

The dictionaries used for boosting are typically very largepractically inniteand therefore the
generic boosting algorithm we have presented cannot be implemented verbatim. In particular, it is
not practical to exhaustively search for the maximizer in step 2(c). Instead, an approximate, usually
greedy search is conducted to nd a good candidate weak learner h jt which makes the rst order
decline in the loss large (even if not maximal among all possible models).

In the common case that the dictionary of weak learners is comprised of decision trees with
up to k nodes, the way AdaBoost and other boosting algorithms solve stage 2(c) is by building a

3. The sign of a

t will always be (cid:0)sign((cid:229)

i wih jt (xi)), since we want the loss to be reduced. In most cases, the dictionary

H is negation closed, and so it can be assumed WLOG that the coefcients are always positive and increasing

945


(cid:229)

b
ROSSET, ZHU AND HASTIE

decision tree to a re-weighted version of the data, with the weights jwij. Thus they rst replace step
2(c) with minimization of

jwij1fyi 6= h jt (xi)g;

i

which is easily shown to be equivalent to the original step 2(c). They then use a greedy decision-
tree building algorithm such as CART or C5 to build a k-node decision tree which minimizes this
quantity, i.e., achieves low weighted misclassication error on the weighted data. Since the tree is
built greedilyone split at a timeit will not be the global minimizer of weighted misclassication
error among all k-node decision trees. However, it will be a good t for the re-weighted data, and
can be considered an approximation to the optimal tree.

This use of approximate optimization techniques is critical, since much of the strength of the
boosting approach comes from its ability to build additive models in very high-dimensional predic-
tor spaces. In such spaces, standard exact optimization techniques are impractical: any approach
which requires calculation and inversion of Hessian matrices is completely out of the question,
and even approaches which require only rst derivatives, such as coordinate descent, can only be
implemented approximately.

2.2 Gradient-Based Boosting as a Generic Modeling Tool

As Friedman (2001); Mason et al. (1999) mention, this view of boosting as gradient descent allows
us to devise boosting algorithms for any function estimation problemall we need is an appro-
priate loss and an appropriate dictionary of weak learners. For example, Friedman et al. (2000)
suggested using the binomial log-likelihood loss instead of the exponential loss of AdaBoost for
binary classication, resulting in the LogitBoost algorithm. However, there is no need to limit
boosting algorithms to classicationFriedman (2001) applied this methodology to regression es-
timation, using squared error loss and regression trees, and Rosset and Segal (2003) applied it to
density estimation, using the log-likelihood criterion and Bayesian networks as weak learners. Their
experiments and those of others illustrate that the practical usefulness of this approachcoordinate
descent in high dimensional predictor spacecarries beyond classication, and even beyond super-
vised learning.

The view we present in this paper, of coordinate-descent boosting as approximate l1-regularized
tting, offers some insight into why this approach would be good in general: it allows us to t regu-
larized models directly in high dimensional predictor space. In this it bears a conceptual similarity
to support vector machines, which exactly t an l2 regularized model in high dimensional (RKH)
predictor space.

2.3 Loss Functions

The two most commonly used loss functions for boosting classication models are the exponential
and the (minus) binomial log-likelihood:

Exponential :
Loglikelihood : Cl(y; F) = log(1 + exp((cid:0)yF)):

Ce(y; F) = exp((cid:0)yF);

These two loss functions bear some important similarities to each other. As Friedman et al. (2000)
show, the population minimizer of expected loss at point x is similar for both loss functions and is

946

(cid:229)
BOOSTING AS A REGULARIZED PATH

Exponential
Logistic

1.5

1

0.5

0

0.5

1

1.5

2

2.5

3

2.5

2

1.5

1

0.5

0
2

Figure 2: The two classication loss functions

given by

F(x) = c (cid:1) log(cid:20) P(y = 1jx)
P(y = (cid:0)1jx)(cid:21) ;

where ce = 1=2 for exponential loss and cl = 1 for binomial loss.

More importantly for our purpose, we have the following simple proposition, which illustrates
the strong similarity between the two loss functions for positive margins (i.e., correct classica-
tions):

Proposition 1

yF (cid:21) 0 ) 0:5Ce(y; F) (cid:20) Cl(y; F) (cid:20) Ce(y; F):

(5)

In other words, the two losses become similar if the margins are positive, and both behave like
exponentials.
Proof Consider the functions f1(z) = z and f2(z) = log(1+z) for z 2 [0;1]. Then f1(0) = f2(0) = 0,
and

f1(z)
 z
f2(z)
 z

=

(cid:17) 1

1

1 + z

(cid:20) 1:

1
2

(cid:20)

Thus we can conclude 0:5 f1(z) (cid:20) f2(z) (cid:20) f1(z). Now set z = exp((cid:0)y f ) and we get the desired
result.

For negative margins the behaviors of Ce and Cl are very different, as Friedman et al. (2000)

have noted. In particular, Cl is more robust against outliers and misspecied data.

2.4 Line-Search Boosting vs. e -Boosting
As mentioned above, AdaBoost determines a
this would be

t using a line search. In our notation for Algorithm 1

t = argmina

i

C(yi; Fi + a h jt (xi)):

947



a
(cid:229)
ROSSET, ZHU AND HASTIE

The alternative approach, suggested by Friedman (2001); Hastie et al. (2001), is to shrink all a
to a single small value e . This may slow down learning considerably (depending on how small e
is), but is attractive theoretically: the rst-order theory underlying gradient boosting implies that
the weak learner chosen is the best increment only locally. It can also be argued that this ap-
proach is stronger than line search, as we can keep selecting the same h jt repeatedly if it remains
optimal and so e -boosting dominates line-search boosting in terms of training error. In practice,
this approach of slowing the learning rate usually performs better than line-search in terms of
prediction error as well (see Friedman, 2001). For our purposes, we will mostly assume e
is in-
nitesimally small, so the theoretical boosting algorithm which results is the limit of a series of
boosting algorithms with shrinking e .

t

In regression terminology, the line-search version is equivalent to forward stage-wise modeling,
infamous in the statistics literature for being too greedy and highly unstable (see Friedman, 2001).
This is intuitively obvious, since by increasing the coefcient until it saturates we are destroying
signal which may help us select other good predictors.

3. lp Margins, Support Vector Machines and Boosting

We now introduce the concept of margins as a geometric interpretation of a binary classication
model. In the context of boosting, this view offers a different understanding of AdaBoost from the
gradient descent view presented above. In the following sections we connect the two views.

3.1 The Euclidean Margin and the Support Vector Machine
Consider a classication model in high dimensional predictor space: F(x) = (cid:229)
j. We say
that the model separates the training data fxi; yign
i=1 if sign(F(xi)) = yi; 8i. From a geometrical
perspective this means that the hyper-plane dened by F(x) = 0 is a separating hyper-plane for this
data, and we dene its (Euclidean) margin as

j h j(x)b

m2(b ) = min

i

yiF(xi)
kb k2

:

(6)

The margin-maximizing separating hyper-plane for this data would be dened by b which max-
imizes m2(b ). Figure 3 shows a simple example of separable data in two dimensions, with its
margin-maximizing separating hyper-plane. The Euclidean margin-maximizing separating hyper-
plane is the (non regularized) support vector machine solution. Its margin maximizing properties
play a central role in deriving generalization error bounds for these models, and form the basis for
a rich literature.

3.2 The l1 Margin and Its Relation to Boosting
Instead of considering the Euclidean margin as in (6) we can dene an l p margin concept as

mp(b ) = min

i

yiF(xi)
kb kp

:

(7)

Of particular interest to us is the case p = 1. Figure 4 shows the l1 margin maximizing separating
hyper-plane for the same simple example as Figure 3. Note the fundamental difference between

948

BOOSTING AS A REGULARIZED PATH

3

2

1

0

1

2

3

3

2

1

0

1

2

3

Figure 3: A simple data example, with two observations from class O and two observations from

class X. The full line is the Euclidean margin-maximizing separating hyper-plane.

3

2

1

0

1

2

3

3

2

1

0

1

2

3

Figure 4: l1 margin maximizing separating hyper-plane for the same data set as Figure 3. The
difference between the diagonal Euclidean optimal separator and the vertical l1 optimal
separator illustrates the sparsity effect of optimal l1 separation

949

ROSSET, ZHU AND HASTIE

the two solutions: the l2-optimal separator is diagonal, while the l1-optimal one is vertical. To
understand why this is so we can relate the two margin denitions to each other as

yF(x)
kb k1

=

yF(x)
kb k2

(cid:1)

kb k2
kb k1

:

(8)

From this representation we can observe that the l1 margin will tend to be big if the ratio kb k2
is
kb k1
big. This ratio will generally be big if b
is sparse. To see this, consider xing the l1 norm of the
vector and then comparing the l2 norm of two candidates: one with many small components and the
othera sparse onewith a few large components and many zero components. It is easy to see that
the second vector will have bigger l2 norm, and hence (if the l2 margin for both vectors is equal) a
bigger l1 margin.

A different perspective on the difference between the optimal solutions is given by a theorem
due to Mangasarian (1999), which states that the l p margin maximizing separating hyper plane
maximizes the lq distance from the closest points to the separating hyper-plane, with 1
q = 1.
Thus the Euclidean optimal separator (p = 2) also maximizes Euclidean distance between the points
and the hyper-plane, while the l1 optimal separator maximizes l distance. This interesting result
gives another intuition why l1 optimal separating hyper-planes tend to be coordinate-oriented (i.e.,
have sparse representations): since l projection considers only the largest coordinate distance,
some coordinate distances may be 0 at no cost of decreased l distance.

p + 1

Schapire et al. (1998) have pointed out the relation between AdaBoost and the l1 margin. They
prove that, in the case of separable data, the boosting iterations increase the boosting margin of
the model, dened as

min

i

yiF(xi)
ka k1

:

(9)

In other words, this is the l1 margin of the model, except that it uses the a
incremental representation
rather than the b geometric representation for the model. The two representations give the same
l1 norm if there is sign consistency, or monotonicity in the coefcient paths traced by the model,
i.e., if at every iteration t of the boosting algorithm

6= 0 ) sign(a

t) = sign(b

jt

jt ):

(10)

As we will see later, this monotonicity condition will play an important role in the equivalence
between boosting and l1 regularization.

The l1-margin maximization view of AdaBoost presented by Schapire et al. (1998)and a
whole plethora of papers that followedis important for the analysis of boosting algorithms for
two distinct reasons:

(cid:15) It gives an intuitive, geometric interpretation of the model that AdaBoost is looking fora
model which separates the data well in this l1-margin sense. Note that the view of boosting as
gradient descent in a loss criterion doesnt really give the same kind of intuition: if the data
is separable, then any model which separates the training data will drive the exponential or
binomial loss to 0 when scaled up:

m1(b ) > 0 =) (cid:229)

C(yi; db

0xi) ! 0 as d ! 

:

i

950

b
BOOSTING AS A REGULARIZED PATH

(cid:15) The l1-margin behavior of a classication model on its training data facilitates generation
of generalization (or prediction) error bounds, similar to those that exist for support vector
machines (Schapire et al., 1998). The important quantity in this context is not the margin but
the normalized margin, which considers the conjugate norm of the predictor vectors:

yib

0h(xi)
kb k1kh(xi)k

:

When the dictionary we are using is comprised of classiers then kh(xi)k (cid:17) 1 always and
thus the l1 margin is exactly the relevant quantity. The error bounds described by Schapire
et al. (1998) allow using the whole l1 margin distribution, not just the minimal margin. How-
ever, boostings tendency to separate well in the l1 sense is a central motivation behind their
results.

From a statistical perspective, however, we should be suspicious of margin-maximization as a
method for building good prediction models in high dimensional predictor space. Margin maxi-
mization in high dimensional space is likely to lead to over-tting and bad prediction performance.
This has been observed in practice by many authors, in particular Breiman (1999). Our results in
the next two sections suggest an explanation based on model complexity: margin maximization is
the limit of parametric regularized optimization models, as the regularization vanishes, and the reg-
ularized models along the path may well be superior to the margin maximizing limiting model, in
terms of prediction performance. In Section 7 we return to discuss these issues in more detail.

4. Boosting as Approximate Incremental l1 Constrained Fitting
In this section we introduce an interpretation of the generic coordinate-descent boosting algorithm
as tracking a path of approximate solutions to l1-constrained (or equivalently, regularized) versions
of its loss criterion. This view serves our understanding of what boosting does, in particular the
connection between early stopping in boosting and regularization. We will also use this view to
get a result about the asymptotic margin-maximization of regularized classication models, and
by analogy of classication boosting. We build on ideas rst presented by Hastie et al. (2001,
Chapter 10) and Efron et al. (2004).

Given a convex non-negative loss criterion C((cid:1); (cid:1)), consider the 1-dimensional path of optimal

solutions to l1 constrained optimization problems over the training data:

b (c) = arg min
kb k1(cid:20)c

i

C(yi; h(xi)0b ):

(11)

As c varies, we get that b (c) traces a 1-dimensional optimal curve through RJ. If an optimal
solution for the non-constrained problem exists and has nite l1 norm c0, then obviously b (c) =
b (c0) = b
; 8c > c0. in the case of separable 2-class data, using either Ce or Cl, there is no nite-
norm optimal solution. Rather, the constrained solution will always have k b (c)k1 = c.

iterations. This will give an a

A different way of building a solution which has l1 norm c, is to run our e -boosting algorithm
(c=e ) vector which has l1 norm exactly c. For the norm of the
for c=e
geometric representation b (c=e ) to also be equal to c, we need the monotonicity condition (10) to
hold as well. This condition will play a key role in our exposition.

We are going to argue that the two solution paths b (c) and b (c=e ) are very similar for e small.
Let us start by observing this similarity in practice. Figure 1 in the introduction shows an example of

951

(cid:229)
ROSSET, ZHU AND HASTIE

Lasso

Stagwise

0
0
5

0



3
9


4


7


2



10
5


8



6
1



0
0
5
-

9

3
6

4

8
7
10
1

2

5

0
0
5

0



3
9


4


7


2



10
5


8



1
6

0
0
5
-

9

3
6

4

8
7
10
1

2

5

0

2000
1000






3000

0

2000
1000






3000

Figure 5: Another example of the equivalence between the Lasso optimal solution path (left) and
e -boosting with squared error loss. Note that the equivalence breaks down when the path
of variable 7 becomes non-monotone

this similarity for squared error loss tting with l1 (lasso) penalty. Figure 5 shows another example
in the same mold, taken from Efron et al. (2004). The data is a diabetes study and the dictionary
used is just the original 10 variables. The panel on the left shows the path of optimal l1-constrained
solutions b (c) and the panel on the right shows the e -boosting path with the 10-dimensional dictio-
nary (the total number of boosting iterations is about 6000). The 1-dimensional path through R10
is described by 10 coordinate curves, corresponding to each one of the variables. The interesting
phenomenon we observe is that the two coefcient traces are not completely identical. Rather, they
agree up to the point where variable 7 coefcient path becomes non monotone, i.e., it violates (10)
(this point is where variable 8 comes into the model, see the arrow on the right panel). This example
illustrates that the monotonicity conditionand its implication that ka k1 = kb k1is critical for the
equivalence between e -boosting and l1-constrained optimization.

The two examples we have seen so far have used squared error loss, and we should ask ourselves
whether this equivalence stretches beyond this loss. Figure 6 shows a similar result, but this time for
the binomial log-likelihood loss, Cl. We used the spam data set, taken from the UCI repository
(Blake and Merz, 1998). We chose only 5 predictors of the 57 to make the plots more interpretable
and the computations more accommodating. We see that there is a perfect equivalence between the
exact constrained solution (i.e., regularized logistic regression) and e -boosting in this case, since the
paths are fully monotone.

To justify why this observed equivalence is not surprising, let us consider the following l1-
increment to a given

locally optimal monotone direction problem of nding the best monotone e
model b 0:

min C(b )
s:t:

kb k1 (cid:0) kb 0k1 (cid:20) e ;
jb j (cid:23) jb 0j (component-wise):

952

(12)


BOOSTING AS A REGULARIZED PATH

Exact constrained solution

e Stagewise

s
e
u
a
v


l

6

5

4

3

2

1

0

1

2

0

2

4

||b

6
||1

8

10

12

s
e
u
a
v


l

6

5

4

3

2

1

0

1

2

0

2

4

||b

6
||1

8

10

12

Figure 6: Exact coefcient paths (left) for l1-constrained logistic regression and boosting coefcient
paths (right) with binomial log-likelihood loss on ve variables from the spam data set.
The boosting path was generated using e = 0:003 and 7000 iterations.

Here we use C(b ) as shorthand for (cid:229)

iC(yi; h(xi)0b ). A rst order Taylor expansion gives us

C(b ) = C(b 0) + (cid:209) C(b 0)0(b (cid:0) b 0) + O(e 2):

And given the l1 constraint on the increase in kb k1, it is easy to see that a rst-order optimal solution
(and therefore an optimal solution as e ! 0) will make a coordinate descent step, i.e.

j 6= b 0; j ) j(cid:209) C(b 0) jj = max

k

j(cid:209) C(b 0)kj;

assuming the signs match, i.e., sign(b 0 j) = (cid:0)sign((cid:209) C(b 0) j).

So we get that if the optimal solution to (12) without the monotonicity constraint happens to be
monotone, then it is equivalent to a coordinate descent step. And so it is reasonable to expect that if
the optimal l1 regularized path is monotone (as it indeed is in Figures 1,6), then an innitesimal
e -boosting algorithm would follow the same path of solutions. Furthermore, even if the optimal
path is not monotone, we can still use the formulation (12) to argue that e -boosting would tend to
follow an approximate l1-regularized path. The main difference between the e -boosting path and
the true optimal path is that it will tend to delay becoming non-monotone, as we observe for
variable 7 in Figure 5. To understand this specic phenomenon would require analysis of the true
optimal path, which falls outside the scope of our discussionEfron et al. (2004) cover the subject
for squared error loss, and their discussion applies to any continuously differentiable convex loss,
using second-order approximations.

We can employ this understanding of the relationship between boosting and l1 regularization
to construct lp boosting algorithms by changing the coordinate-selection criterion in the coordinate
descent algorithm. We will get back to this point in Section 7, where we design an l2 boosting
algorithm.

The experimental evidence and heuristic discussion we have presented lead us to the following

conjecture which connects slow boosting and l1-regularized optimization:

953

b
b
b
ROSSET, ZHU AND HASTIE

Conjecture 2 Consider applying the e -boosting algorithm to any convex loss function, generating
a path of solutions b (e )(t). Then if the optimal coefcient paths are monotone 8c < c0, i.e., if
8 j; jb (c) jj is non-decreasing in the range c < c0, then

b (e )(c0=e ) = b (c0):

lime !0

Efron et al. (2004, Theorem 2) prove a weaker local result for the case of squared error loss
only. We generalize their result to any convex loss. However this result still does not prove the
global convergence which the conjecture claims, and the empirical evidence implies. For the sake
of brevity and readability, we defer this proof, together with concise mathematical denition of the
different types of convergence, to appendix A.

In the context of real-life boosting, where the number of basis functions is usually very large,
and making e small enough for the theory to apply would require running the algorithm forever,
these results should not be considered directly applicable. Instead, they should be taken as an intu-
itive indication that boostingespecially the e versionis, indeed, approximating optimal solutions
to the constrained problems it encounters along the way.

5. lp-Constrained Classication Loss Functions
Having established the relation between boosting and l1 regularization, we are going to turn our
attention to the regularized optimization problem. By analogy, our results will apply to boosting
as well. We concentrate on Ce and Cl, the two classication losses dened above, and the solution
paths of their lp constrained versions:

b (p)(c) = arg min
kb kp(cid:20)c

i

C(yi;b

0h(xi)):

(13)

where C is either Ce or Cl. As we discussed below Equation (11), if the training data is separable in
span(H ), then we have k b (p)(c)kp = c for all values of c. Consequently

b (p)(c)

k

c

kp = 1:

We may ask what are the convergence points of this sequence as c ! 
shows that these convergence points describe l p-margin maximizing separating hyper-planes.
Theorem 3 Assume the data is separable, i.e., 9b s:t:8i; yib
Then for both Ce and Cl, every convergence point of
separating hyper-plane.
If the lp-margin-maximizing separating hyper-plane is unique, then it is the unique convergence
points, i.e.

b (c)
c corresponds to an lp-margin-maximizing

. The following theorem

0h(xi) > 0.

= arg max
kb kp=1

min

i

yib

0h(xi):

(14)

b (p) = lim
c!

b (p)(c)

c

Proof This proof applies to both Ce and Cl, given the property in (5). Consider two separating
candidates b 1 and b 2 such that kb 1kp = kb 2kp = 1. Assume that b 1 separates better, i.e.

m1 := min

i

yib

0
1h(xi) > m2 := min

i

yib

0
2h(xi) > 0:

Then we have the following simple lemma:

954

(cid:229)
BOOSTING AS A REGULARIZED PATH

Lemma 4 There exists some D = D(m1; m2) such that 8d > D, db 1 incurs smaller loss than db 2,
in other words:

C(yi; db

1h(xi)) < (cid:229)

0

C(yi; db

0
2h(xi)):

i

i

Given this lemma, we can now prove that any convergence point of
c must be an lp-margin
maximizing separator. Assume b (cid:3) is a convergence point of
. Denote its minimal margin on
the data by m(cid:3). If the data is separable, clearly m(cid:3) > 0 (since otherwise the loss of db (cid:3) does not
even converge to 0 as d ! 

Now, assume some b with kb kp = 1 has bigger minimal margin m > m(cid:3). By continuity of the

b (p)(c)

).

c

b (p)(c)

minimal margin in b

, there exists some open neighborhood of b (cid:3)
: kb (cid:0) b (cid:3)k2 < d g

Nb (cid:3) = fb

and an e > 0, such that

yib

0h(xi) < m (cid:0) e ; 8b 2 Nb (cid:3):

min

i

Now by the lemma we get that there exists some D = D( m; m (cid:0) e ) such that d b
for any d > D; b 2 Nb (cid:3). Therefore b (cid:3) cannot be a convergence point of

loss than db

incurs smaller
b (p)(c)

.

c

We conclude that any convergence point of the sequence

c must be an lp-margin maximiz-
ing separator. If the margin maximizing separator is unique then it is the only possible convergence
point, and therefore

b (p)(c)

b (p) = lim
c!

b (p)(c)

c

= arg max
kb kp=1

min

i

yib

0h(xi):

Proof of Lemma Using (5) and the denition of Ce, we get for both loss functions:

C(yi; db

i

0
1h(xi)) (cid:20) nexp((cid:0)d (cid:1) m1):

Now, since b 1 separates better, we can nd our desired

D = D(m1; m2) =

logn + log2

m1 (cid:0) m2

such that

8d > D; nexp((cid:0)d (cid:1) m1) < 0:5exp((cid:0)d (cid:1) m2):

And using (5) and the denition of Ce again we can write
C(yi; db

0:5exp((cid:0)d (cid:1) m2) (cid:20) (cid:229)

i

0
2h(xi)):

Combining these three inequalities we get our desired result:

8d > D;

i

C(yi; db

1h(xi)) (cid:20) (cid:229)

0

C(yi; db

0
2h(xi)):

i

955

(cid:229)
(cid:229)
(cid:229)
ROSSET, ZHU AND HASTIE

We thus conclude that if the lp-margin maximizing separating hyper-plane is unique, the nor-
malized constrained solution converges to it. In the case that the margin maximizing separating
hyper-plane is not unique, we can in fact prove a stronger result, which indicates that the limit of
the regularized solutions would then be determined by the second smallest margin, then by the third
and so on. This result is mainly of technical interest and we prove it in Appendix B, Section 2.

5.1 Implications of Theorem 3

We now briey discuss the implications of this theorem for boosting and logistic regression.

5.1.1 BOOSTING IMPLICATIONS

Combined with our results from Section 4, Theorem 3 indicates that the normalized boosting path
with either Ce or Cl used as lossapproximately converges to a separating hyper-plane

b (t)

(cid:229) u(cid:20)t a u
b
, which attains

max
kb k1=1

min

i

yib

0h(xi) = max
kb k1=1

kb k2 min

i

yidi;

(15)

where di is the (signed) Euclidean distance from the training point i to the separating hyper-plane. In
other words, it maximizes Euclidean distance scaled by an l2 norm. As we have mentioned already,
this implies that the asymptotic boosting solution will tend to be sparse in representation, due to the
fact that for xed l1 norm, the l2 norm of vectors that have many 0 entries will generally be larger.
In fact, under rather mild conditions, the asymptotic solution b = limc!
b (1)(c)=c, will have at
most n (the number of observations) non-zero coefcients, if we use either Cl or Ce as the loss. See
Appendix B, Section 1 for proof.

5.1.2 LOGISTIC REGRESSION IMPLICATIONS

Recall, that the logistic regression (maximum likelihood) solution is undened if the data is sepa-
rable in the Euclidean space spanned by the predictors. Theorem 3 allows us to dene a logistic
regression solution for separable data, as follows:

1. Set a high constraint value cmax
2. Find b (p)(cmax), the solution to the logistic regression problem subject to the constraint kb k p (cid:20)
cmax. The problem is convex for any p (cid:21) 1 and differentiable for any p > 1, so interior point
methods can be used to solve this problem.

3. Now you have (approximately) the lp-margin maximizing solution for this data, described by

b (p)(cmax)

cmax

:

This is a solution to the original problem in the sense that it is, approximately, the convergence
point of the normalized lp-constrained solutions, as the constraint is relaxed.

956

BOOSTING AS A REGULARIZED PATH

Of course, with our result from Theorem 3 it would probably make more sense to simply nd the
optimal separating hyper-plane directlythis is a linear programming problem for l1 separation and
a quadratic programming problem for l2 separation. We can then consider this optimal separator as
a logistic regression solution for the separable data.

6. Examples

We now apply boosting to several data sets and interpret the results in light of our regularization and
margin-maximization view.

6.1 Spam Data Set

We now know if the data are separable and we let boosting run forever, we will approach the same
optimal separator for both Ce and Cl. However if we stop earlyor if the data is not separable
the behavior of the two loss functions may differ signicantly, since Ce weighs negative margins
exponentially, while Cl is approximately linear in the margin for large negative margins (see Fried-
man et al., 2000). Consequently, we can expect Ce to concentrate more on the hard training data,
in particular in the non-separable case. Figure 7 illustrates the behavior of e -boosting with both

Minimal margins

Test error

exponential
logistic
AdaBoost

0.2

0

0.2

0.4

0.6

0.8

0.095

0.09

0.085

0.08

0.075

0.07

0.065

0.06

0.055

0.05

r
o
r
r
e

t
s
e
t

exponential
logistic
AdaBoost

i

n
g
r
a
m


l
a
m
n
m

i

i

1

100

101

||b

||1

102

103

0.045

100

101

||b

||1

102

103

Figure 7: Behavior of boosting with the two loss functions on spam data set

loss functions, as well as that of AdaBoost, on the spam data set (57 predictors, binary response).
We used 10 node trees and e = 0:1. The left plot shows the minimal margin as a function of the
l1 norm of the coefcient vector kb k1. Binomial loss creates a bigger minimal margin initially,
but the minimal margins for both loss functions are converging asymptotically. AdaBoost initially
lags behind but catches up nicely and reaches the same minimal margin asymptotically. The right
plot shows the test error as the iterations proceed, illustrating that both e -methods indeed seem to
over-t eventually, even as their separation (minimal margin) is still improving. AdaBoost did not
signicantly over-t in the 1000 iterations it was allowed to run, but it obviously would have if it
were allowed to run on.

We should emphasize that the comparison between AdaBoost and e -boosting presented consid-
ers as a basis for comparison the l1 norm, not the number of iterations. In terms of computational
complexity, as represented by the number of iterations, AdaBoost reaches both a large minimal mar-

957

ROSSET, ZHU AND HASTIE

gin and good prediction performance much more quickly than the slow boosting approaches, as
AdaBoost tends to take larger steps.

6.2 Simulated Data

To make a more educated comparison and more compelling visualization, we have constructed an
example of separation of 2-dimensional data using a 8-th degree polynomial dictionary (45 func-
tions). The data consists of 50 observations of each class, drawn from a mixture of Gaussians, and
presented in Figure 8. Also presented, in the solid line, is the optimal l1 separator for this data in
this dictionary (easily calculated as a linear programming problem - note the difference from the l2
optimal decision boundary, presented in Section 7.1, Figure 11 ). The optimal l1 separator has only
12 non-zero coefcients out of 45.

2

1.5

1

0.5

0

0.5

1

1.5

2
2

optimal
boost 105 iter
boost 3*106 iter

1.5

1

0.5

0

0.5

1

1.5

2

Figure 8: Articial data set with l1-margin maximizing separator (solid), and boosting models af-
ter 105 iterations (dashed) and 106 iterations (dotted) using e = 0:001. We observe the
convergence of the boosting separator to the optimal separator

We ran an e -boosting algorithm on this data set, using the logistic log-likelihood loss Cl, with
e = 0:001, and Figure 8 shows two of the models generated after 105 and 3 (cid:1) 106 iterations. We see
that the models seem to converge to the optimal separator. A different view of this convergence is
given in Figure 9, where we see two measures of convergence: the minimal margin (left, maximum
value obtainable is the horizontal line) and the l1-norm distance between the normalized models
(right), given by

b

j (cid:0)

j (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

;

b (t)
j

kb (t)k1(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where b

is the optimal separator with l1 norm 1 and b (t) is the boosting model after t iterations.

We can conclude that on this simple articial example we get nice convergence of the logistic-

boosting model path to the l1-margin maximizing separating hyper-plane.

We can also use this example to illustrate the similarity between the boosted path and the path

of l1 optimal solutions, as we have discussed in Section 4.

958

(cid:229)
BOOSTING AS A REGULARIZED PATH

i

n
g
r
a
m


l

i

a
m
n
M

i

0.01

0

0.01

0.02

0.03

0.04

0.05

101

102

||b

||1

103

104

2

1.8

1.6

1.4

1.2

1

e
c
n
e
r
e

f
f
i

d



1

l

0.8

0.6

0.4

0.2

0
101

102

||b

||1

103

104

Figure 9: Two measures of convergence of boosting model path to optimal l1 separator: minimal
margin (left) and l1 distance between the normalized boosting coefcient vector and the
optimal model (right)

l
norm:    20
1

l
norm:   350
1

l
norm:  2701
1

l
norm:  5401
1

Figure 10: Comparison of decision boundary of boosting models (broken) and of optimal con-

strained solutions with same norm (full)

Figure 10 shows the class decision boundaries for 4 models generated along the boosting path,
compared to the optimal solutions to the constrained logistic regression problem with the same
bound on the l1 norm of the coefcient vector. We observe the clear similarities in the way the
solutions evolve and converge to the optimal l1 separator. The fact that they differ (in some cases
signicantly) is not surprising if we recall the monotonicity condition presented in Section 4 for
exact correspondence between the two model paths. In this case if we look at the coefcient paths

959

ROSSET, ZHU AND HASTIE

(not shown), we observe that the monotonicity condition is consistently violated in the low norm
ranges, and hence we can expect the paths to be similar in spirit but not identical.

7. Discussion

We can now summarize what we have learned about boosting from the previous sections:

(cid:15) Boosting approximately follows the path of l1-regularized models for its loss criterion

(cid:15) If the loss criterion is the exponential loss of AdaBoost or the binomial log-likelihood loss
of logistic regression, then the l1 regularized model converges to an l1-margin maximizing
separating hyper-plane, if the data are separable in the span of the weak learners

We may ask, which of these two points is the key to the success of boosting approaches. One
empirical clue to answering this question, can be found in Breiman (1999), who programmed an
algorithm to directly maximize the margins. His results were that his algorithm consistently got
signicantly higher minimal margins than AdaBoost on many data sets (and, in fact, a higher
margin distribution beyond the minimal margin), but had slightly worse prediction performance. His
conclusion was that margin maximization is not the key to AdaBoosts success. From a statistical
perspective we can embrace this conclusion, as reecting the importance of regularization in high-
dimensional predictor space. By our results from the previous sections, margin maximization
can be viewed as the limit of parametric regularized models, as the regularization vanishes.4 Thus
we would generally expect the margin maximizing solutions to perform worse than regularized
models. In the case of boosting, regularization would correspond to early stopping of the boosting
algorithm.

7.1 Boosting and SVMs as Regularized Optimization in High-dimensional Predictor Spaces

Our exposition has led us to view boosting as an approximate way to solve the regularized optimiza-
tion problem

(16)

C(yi;b

0h(xi)) + l kb k1

min

i

which converges as l ! 0 to b (1), if our loss is Ce or Cl. In general, the loss C can be any convex
differentiable loss and should be dened to match the problem domain.

Support vector machines can be described as solving the regularized optimization problem (see

Friedman et al., 2000, Chapter 12)

(1 (cid:0) yib

0h(xi))+ + l kb k2

2

min

i

(17)

which converges as l ! 0 to the non-regularized support vector machine solution, i.e., the optimal
Euclidean separator, which we denoted by b (2).

An interesting connection exists between these two approaches, in that they allow us to solve

the regularized optimization problem in high dimensional predictor space:

4. It can be argued that margin-maximizing models are still regularized in some sense, as they minimize a norm
criterion among all separating models. This is arguably the property which still allows them to generalize reasonably
well in many cases.

960

b
(cid:229)
b
(cid:229)
BOOSTING AS A REGULARIZED PATH

(cid:15) We are able to solve the l1- regularized problem approximately in very high dimension via
boosting by applying the approximate coordinate descent trick of building a decision tree
(or otherwise greedily selecting a weak learner) based on re-weighted versions of the data.

(cid:15) Support vector machines facilitate a different trick for solving the regularized optimization
problem in high dimensional predictor space: the kernel trick. If our dictionary H spans
a Reproducing Kernel Hilbert Space, then RKHS theory tells us we can nd the regularized
solutions by solving an n-dimensional problem, in the space spanned by the kernel represen-
ters fK(xi;x)g. This fact is by no means limited to the hinge loss of (17), and applies to any
convex loss. We concentrate our discussion on SVM (and hence hinge loss) only since it is
by far the most common and well-known application of this result.

So we can view both boosting and SVM as methods that allow us to t regularized models in
high dimensional predictor space using a computational shortcut. The complexity of the model
built is controlled by regularization. These methods are distinctly different than traditional statistical
approaches for building models in high dimension, which start by reducing the dimensionality of
the problem so that standard tools (e.g., Newtons method) can be applied to it, and also to make
over-tting less of a concern. While the merits of regularization without dimensionality reduction
like Ridge regression or the Lassoare well documented in statistics, computational issues make it
impractical for the size of problems typically solved via boosting or SVM, without computational
tricks.

We believe that this difference may be a signicant reason for the enduring success of boosting

and SVM in data modeling, i.e.:

Working in high dimension and regularizing is statistically preferable to a two-step
procedure of rst reducing the dimension, then tting a model in the reduced space.

It is also interesting to consider the differences between the two approaches, in the loss (exible
vs. hinge loss), the penalty (l1 vs. l2), and the type of dictionary used (usually trees vs. RKHS).
These differences indicate that the two approaches will be useful for different situations. For ex-
ample, if the true model has a sparse representation in the chosen dictionary, then l1 regularization
may be warranted; if the form of the true model facilitates description of the class probabilities via
a logistic-linear model, then the logistic loss Cl is the best loss to use, and so on.

The computational tricks for both SVM and boosting limit the kind of regularization that can
be used for tting in high dimensional space. However, the problems can still be formulated and
solved for different regularization approaches, as long as the dimensionality is low enough:

(cid:15) Support vector machines can be tted with an l1 penalty, by solving the 1-norm version of the
SVM problem, equivalent to replacing the l2 penalty in (17) with an l1 penalty. In fact, the 1-
norm SVM is used quite widely, because it is more easily solved in the linear, non-RKHS,
situation (as a linear program, compared to the standard SVM which is a quadratic program)
and tends to give sparser solutions in the primal domain.

(cid:15) Similarly, we describe below an approach for developing a boosting algorithm for tting

approximate l2 regularized models.

Both of these methods are interesting and potentially useful. However they lack what is arguably
the most attractive property of the standard boosting and SVM algorithms: a computational trick
to allow tting in high dimensions.

961

ROSSET, ZHU AND HASTIE

7.1.1 AN l2 BOOSTING ALGORITHM
We can use our understanding of the relation of boosting to regularization and Theorem 3 to for-
mulate lp-boosting algorithms, which will approximately follow the path of l p-regularized solutions
and converge to the corresponding lp-margin maximizing separating hyper-planes. Of particular
interest is the l2 case, since Theorem 3 implies that l2-constrained tting using Cl or Ce will build a
regularized path to the optimal separating hyper-plane in the Euclidean (or SVM) sense.

To construct an l2 boosting algorithm, consider the equivalent optimization problem (12), and

change the step-size constraint to an l2 constraint:

kb k2 (cid:0) kb 0k2 (cid:20) e :

It is easy to see that the rst order solution to this problem entails selecting for modication the
coordinate which maximizes

(cid:209) C(b 0)k

b 0;k

and that subject to monotonicity, this will lead to a correspondence to the locally l2-optimal direc-
tion.

Following this intuition, we can construct an l2 boosting algorithm by changing only step 2(c)

of our generic boosting algorithm of Section 2 to

2(c)* Identify jt which maximizes j(cid:229)

i wih jt (xi)j
jb

jt j

:

Note that the need to consider the current coefcient (in the denominator) makes the l2 algorithm
appropriate for toy examples only. In situations where the dictionary of weak learner is prohibitively
large, we will need to gure out a trick like the one we presented in Section 2.1, to allow us to make
an approximate search for the optimizer of step 2(c)*.

Another problem in applying this algorithm to large problems is that we never choose the same
dictionary function twice, until all have non-0 coefcients. This is due to the use of the l2 penalty,
where the current coefcient value affects the rate at which the penalty term is increasing. In par-
ticular, if b
j = 0 then increasing it causes the penalty term kb k2 to increase at rate 0, to rst order
(which is all the algorithm is considering).

The convergence of our l2 boosting algorithm on the articial data set of Section 6.2 is illustrated
in Figure 11. We observe that the l2 boosting models do indeed approach the optimal l2 separator.
It is interesting to note the signicant difference between the optimal l2 separator as presented in
Figure 11 and the optimal l1 separator presented in Section 6.2 (Figure 8).

8. Summary and Future Work

In this paper we have introduced a new view of boosting in general, and two-class boosting in
particular, comprised of two main points:

(cid:15) We have generalized results from Efron et al. (2004) and Hastie et al. (2001), to describe

boosting as approximate l1-regularized optimization.

(cid:15) We have shown that the exact l1-regularized solutions converge to an l1-margin maximizing

separating hyper-plane.

962

BOOSTING AS A REGULARIZED PATH

2

1.5

1

0.5

0

0.5

1

1.5

2
2

optimal
boost 5*106 iter
boost 108 iter

1.5

1

0.5

0

0.5

1

1.5

2

Figure 11: Articial data set with l2-margin maximizing separator (solid), and l2-boosting models
after 5 (cid:3)106 iterations (dashed) and 108 iterations (dotted) using e = 0:0001. We observe
the convergence of the boosting separator to the optimal separator

We hope our results will help in better understanding how and why boosting works. It is an interest-
ing and challenging task to separate the effects of the different components of a boosting algorithm:

(cid:15) Loss criterion

(cid:15) Dictionary and greedy learning method

(cid:15) Line search / slow learning

and relate them to its success in different scenarios. The implicit l1 regularization in boosting
may also contribute to its success, as it has been shown that in some situations l1 regularization is
inherently superior to others (see Donoho et al., 1995).

An important issue when analyzing boosting is over-tting in the noisy data case. To deal with
over-tting, Ratsch et al. (2001b) propose several regularization methods and generalizations of the
original AdaBoost algorithm to achieve a soft margin by introducing slack variables. Our results
indicate that the models along the boosting path can be regarded as l1 regularized versions of the
optimal separator, hence regularization can be done more directly and naturally by stopping the
boosting iterations early. It is essentially a choice of the l1 constraint parameter c.

Many other questions arise from our view of boosting. Among the issues to be considered:

(cid:15) Is there a similar separator view of multi-class boosting? We have some tentative results to

indicate that this might be the case if the boosting problem is formulated properly.

(cid:15) Can the constrained optimization view of boosting help in producing generalization error

bounds for boosting that would be more tight than the current existing ones?

963

ROSSET, ZHU AND HASTIE

Acknowledgments

We thank Stephen Boyd, Brad Efron, Jerry Friedman, Robert Schapire and Rob Tibshirani for
helpful discussions. We thank the referees for their thoughtful and useful comments. This work
was partially supported by Stanford graduate fellowship, grant DMS-0204612 from the National
Science Foundation, and grant ROI-CA-72028-01 from the National Institutes of Health.

Appendix A. Local Equivalence of Innitesimal e -Boosting and l1-Constrained

Optimization

As before, we assume we have a set of training data (x1; y1); (x2; y2); : : : (xn; yn), a smooth cost
function C(y; F), and a set of basis functions (h1(x); h2(x); : : :hJ(x)).

We denote by b (s) be the optimal solution of the l1-constrained optimization problem:

min

C(yi; h(xi)0b )

n(cid:229)
i=1
kb k1 (cid:20) s:

(18)

subject to

(19)
Suppose we initialize the e -boosting version of Algorithm 1, as described in Section 2, at b (s) and
run the algorithm for T steps. Let b (T ) denote the coefcients after T steps.
The global convergence Conjecture 2 in Section 4 implies that 8D s > 0:

b (D s=e ) ! b (s + D s) as e ! 0

under some mild assumptions. Instead of proving this global result, we show here a local result
by looking at the derivative of b (s). Our proof builds on the proof by Efron et al. (2004, Theorem 2)
of a similar result for the case that the cost is squared error loss C(y; F) = (y(cid:0)F)2. Theorem 1 below
shows that if we start the e -boosting algorithm at a solution b (s) of the l1-constrained optimization
problem (18)(19), the direction of change of the e -boosting solution will agree with that of the
l1-constrained optimization problem.
Theorem 1 Assume the optimal coefcient paths b
paths b

j(T ) 8 j are also monotone as e -boosting proceeds, then

j(s) 8 j are monotone in s and the coefcient

b (T ) (cid:0) b (s)

T (cid:1) e

! (cid:209)

b (s) as e ! 0; T ! 

; T (cid:1) e ! 0:

Proof First we introduce some notations. Let

h j = (h j(x1); : : :h j(xn))0

be the jth basis function evaluated at the n training data.

Let

F = (F(x1); : : :F(xn))0

be the vector of current t.

Let

r =(cid:18)(cid:0)

 C(y1; F1)

 F1

; : : : (cid:0)

 C(yn; Fn)

 Fn

(cid:19)0

964

b
BOOSTING AS A REGULARIZED PATH

be the current generalized residual vector as dened in Friedman (2001).

Let

c j = h0
be the current correlation between h j and r.

jr;

j = 1; : : :J

Let

A = f j : jc jj = max

j

jc jjg

be the set of indices for the maximum absolute correlation.

For clarity, we re-write this e -boosting algorithm, starting from b (s), as a special case of Algo-

rithm 1, as follows:

(1) Initialize b (0) = b (s);F0 = F;r0 = r.

(2) For t = 1 : T

(a) Find jt = argmax j jh0
(b) Update

jrt(cid:0)1j.

(c) Update Ft and rt.

t; jt   b

t(cid:0)1; jt + e

(cid:1) sign(c jt )

Notice in the above algorithm, we start from b (s), rather than 0. As proposed in Efron et al. (2004),
we consider an idealized e -boosting case:
and T (cid:1) e ! 0, under the
monotone paths condition, Section 3.2 and Section 6 of Efron et al. (2004) showed

e ! 0. As e ! 0, T ! 

FT (cid:0) F0

T (cid:1) e
rT (cid:0) r0
T (cid:1) e

! u;

! v;

(20)

(21)

where u and v satisfy two constraints:

(Constraint 1) u is in the convex cone generated by fsign(c j)h j : j 2 Ag, i.e.:

u = (cid:229)

j2A

Pjsign(c j)h j; Pj (cid:21) 0:

(Constraint 2) v has equal correlation with sign(c j)h j; j 2 A:

sign(c j)h0

jv = l A for j 2 A:

The rst constraint is true because the basis functions in AC will not be able to catch up in terms
of jc jj for sufciently small T (cid:1) e ; the Pjs are non-negative because the coefcient paths b
j(T ) are
monotone. The second constraint can be seen by taking a Taylor expansion of C(y; F) around F0
to the quadratic term, letting T (cid:1) e go to zero and applying the result for the squared error loss from
Efron et al. (2004). Once the two constraints are established, we notice that

vi = (cid:0)

 2C(yi; F)

 F 2

965

ui:

(cid:12)(cid:12)(cid:12)(cid:12)F0(xi)

b
ROSSET, ZHU AND HASTIE

Hence we can plug the constraint 1 into the constraint 2 and get the following set of equations:

where

AW HAP = l A1;
HT

HA = ((cid:1) (cid:1) (cid:1)sign(c j)h j (cid:1) (cid:1) (cid:1)) ; j 2 A;

W = diag  (cid:0)

 2C(yi; F)

 F 2

P = ((cid:1) (cid:1) (cid:1)Pj (cid:1) (cid:1) (cid:1))0 ; j 2 A:

(cid:12)(cid:12)(cid:12)(cid:12)F0(xi)! ;

If H is of rank jAj (we will get back to this issue in details in Appendix B), then P, or equivalently
u and v, are uniquely determined up to a scale number.

Now we consider the l1-constrained optimization problem (18)(19). Let F(s) be the tted

vector and r(s) be the corresponding residual vector. Since F(s) and r(s) are smooth, dene

u(cid:3) (cid:17) limD s!0
v(cid:3) (cid:17) limD s!0

F(s + D s) (cid:0) F(s)

r(s + D s) (cid:0) r(s)

D s

D s

;

(22)

(23)

:

Lemma 2 Under the monotone coefcient paths assumption, u(cid:3) and v(cid:3) also satisfy constraints 12.
Proof Write the coefcient b

j as b +

j (cid:0) b (cid:0)
j , where
j = b
j;b (cid:0)
j = 0
j = (cid:0)b
j = 0;b (cid:0)
b +

(cid:26) b +

if
if

j

j > 0;
j < 0:

The l1-constrained optimization problem (18)(19) is then equivalent to

min
b +;b (cid:0)
subject to

n(cid:229)
i=1
kb +k1 + kb (cid:0)k1 (cid:20) s;b + (cid:21) 0;b (cid:0) (cid:21) 0:

C(cid:0)yi; h(xi)0(b + (cid:0) b (cid:0))(cid:1) ;

The corresponding Lagrangian dual is

L =

n(cid:229)

i=1
(cid:0)l

C(cid:0)yi; h(xi)0(b + (cid:0) b (cid:0))(cid:1) + l

(cid:1) s (cid:0)

J(cid:229)

j=1

l +
j

b +

j (cid:0)

l (cid:0)
j

b (cid:0)
j ;

J(cid:229)

j=1

J(cid:229)

j=1

(b +

j + b (cid:0)
j )

(24)

(25)

(26)

(27)

where l (cid:21) 0;l +

j (cid:21) 0;l (cid:0)

j (cid:21) 0 are Lagrange multipliers.

By differentiating the Lagrangian dual, we get the solution of (24)(25) needed to satisfy the

following Karush-Kuhn-Tucker conditions:

 L
b +
j

= (cid:0)h0

j r + l (cid:0) l +

j = 0;

966

(28)

b
b

BOOSTING AS A REGULARIZED PATH

j r + l (cid:0) l (cid:0)

j = 0;

= h0

 L
b (cid:0)
j
b +
l +
j = 0;
j
b (cid:0)
l (cid:0)
j = 0:
j

(29)

(30)

(31)

Let c j = h0
Tucker conditions:

j r and A = f j : jc jj = max j jc jjg. We can see the following facts from the Karush-Kuhn-

(Fact 1) Use (28), (29) and l (cid:21) 0;l +

j ;l (cid:0)

j (cid:21) 0, we have jc jj (cid:20) l

.

j 6= 0, then jc jj = l and j 2 A. For example, suppose b +

6= 0, then l +

j = 0 and

j

(Fact 2) If b
(28) implies c j = l
(Fact 3) If b

.

j 6= 0, sign( b

j) = sign(c j).

We also note that:
j and b (cid:0)
the same time.

(cid:15) b +

j can not both be non-zero, otherwise l +

j = l (cid:0)

j = 0, (28) and (29) can not hold at

(cid:15) It is possible that b

j = 0 and j 2 A. This only happens for a nite number of s values, where

basis h j is about to enter the model.

For sufciently small D s, since the second derivative of the cost function C(y; F) is nite, A will

stay the same. Since j 2 A if b

j 6= 0, the change in the tted vector is

F(s + D s) (cid:0) F(s) = (cid:229)

j2A

Q jh j:

Since sign( b
sign(c j). Hence we have

j) = sign(c j) and the coefcients b

j change monotonically, sign(Q j) will agree with

F(s + D s) (cid:0) F(s)

D s

= (cid:229)

j2A

Pjsign(c j)h j:

(32)

This implies u(cid:3) satises constraint 1. The claim v(cid:3) satises constraint 2 follows directly from fact
2, since both r(s + D s) and r(s) satisfy constraint 2.
Completion of proof of Theorem (1): We further notice that in both the e -boosting case and the
constrained optimization case, we have (cid:229)
j2A Pj = 1 by denition and the monotone coefcient
paths condition, hence u and v are uniquely determined, i.e.:

u = u(cid:3) and v = v(cid:3):

To translate the result into b (s) and b (T ), we notice F(x) = h(x)0b
that for (cid:209)
conditions for when this is true in Appendix B.

. Efron et al. (2004) showed
b (s) to be well dened, A can have at most n elements, i.e., jAj (cid:20) n. We give sufcient

Now Let

HA = ((cid:1) (cid:1) (cid:1)h j(xi) (cid:1) (cid:1) (cid:1)) ; i = 1; : : :n; j 2 A

967


ROSSET, ZHU AND HASTIE

be a n (cid:2) jAj matrix, which we assume is of rank jAj. Then (cid:209)

b (s) is given by

and

Hence the theorem is proved.

AW u(cid:3);

b (s) =(cid:0)H0
AW HA(cid:1)(cid:0)1 H0
AW HA(cid:1)(cid:0)1 H0
!(cid:0)H0

T (cid:1) e

b (T ) (cid:0) b (s)

AW u:

Appendix B. Uniqueness and Existence Results

In this appendix, we give some details on the properties of regularized solution paths. In section B.1
we formulate and prove sparseness and uniqueness results on l1-regularized solutions for any convex
loss. In section B.2 we extend Theorem 3 of Section 5which proved the margin maximizing
property of the limit of lp-regularized solutions, as regularization variesto the case that the margin
maximizing solution is not unique.

B.1 Sparseness and Uniqueness of l1-Regularized Solutions and Their Limits
Consider the l1-constrained optimization problem:

min
kb k1(cid:20)c

n(cid:229)

i=1

C(yi;b

0h(xi)):

(33)

In this section we give sufcient conditions for the following properties of the solutions of (33):

1. Existence of a sparse solution (with at most n non-zero coefcients),

2. Non-existence of non-sparse solutions with more than n non-zero coefcients,

3. Uniqueness of the solution,

4. Convergence of the solutions to sparse solution, as c increases.

Theorem 3 Assume that the unconstrained solution for problem (33) has l1 norm bigger than c.
Then there exists a solution of (33) which has at most n non-zero coefcients.

Proof As Lemma 2 in the Appendix A, we will prove the theorem using the Karush-Kuhn-Tucker
(KKT) formulation of the optimization problem.
The chain rule for differentiation gives us that

iC(yi;b

0h(xi))

j

= (cid:0)h0

jr(b );

(34)

where h j and r(b ) are dened in the Appendix A; r(b ) is the generalized residual vector. Using
this simple relationship and fact 2 of Lemma 2 we can write a system of equations for all non-
zero coefcients at the optimal constrained solution as follows (denote by A the set of indices for
non-zero coefcients):

(35)

Ar(b ) = l
H0

(cid:1) signb A :

968

(cid:209)

(cid:229)

b
BOOSTING AS A REGULARIZED PATH

In other words, we get jAj equations in jAj variables, corresponding to the non-zero b

js.

However, each column of the matrix HA is of length n, and so HA can have at most n linearly
independent columns, rank(HA ) (cid:20) n. Assume now that we have an optimal solution for (33) with
jAj > n. Then there exists l 2 A such that

hl = (cid:229)

j2A; j6=l

jh j:

Substituting (36) into the lth row in (35) we get

jh j)0r(b ) = l

(cid:1) signb

l:

(

j2A; j6=l

But from (35) we know that h0

jr(b ) = l

(cid:1) signb

j ; 8 j 2 A, meaning we can re-phrase (37) as

j (cid:1) signb

j (cid:1) signb

l = 1:

j2A; j6=l

(36)

(37)

(38)

In other words, we get that hl is a linear combination of the columns of HA(cid:0)flg which must obey
the specic numeric relation in (38).

Now we can construct an alternative optimal solution for (33) with one less non-zero coefcient,

as follows:

1. Start from b
2. Dene the direction g
j = a

l = (cid:0)signb

l ; g

in coefcient space implied by (36), that is:
j (cid:1) signb

l ; 8 j 2 A (cid:0) flg

3. Move in direction g until some coefcient in A hits zero, i.e., dene:

d (cid:3) = min(cid:8)d > 0 : 9 j 2 A s.t. b

lj)

j + g

jd = 0(cid:9)

(we know that d (cid:3) (cid:20) jb

4. Set b = b + d (cid:3)g

Then from (36) we get that b

0h(xi) = b
kb k1 = kb k1 (cid:0) (cid:229)

0h(xi) ; 8i and from (38) we get that

[jb

jd (cid:3)j (cid:0) jb

jj] =

(39)

j2A

j + g
= kb k1 (cid:0) d (cid:3) (cid:1) 1 (cid:0) (cid:229)

j (cid:1) signb

jsignb

l! = kb k1:

j2A(cid:0)l

So b generates the same t as b and has the same l1 norm, therefore it is also an optimal solution,
with at least one less non-zero coefcient (from the denition of d (cid:3)).

We can obviously apply this process repeatedly until we get a solution with at most n non-zero

coefcients.

This theorem has the following immediate implication:

969

a
(cid:229)
a
(cid:229)
a
g
a
ROSSET, ZHU AND HASTIE

Corollary 4 If there is no set of more than n dictionary functions which obeys the equalities (36,38)
on the training data, then any solution of (33) has at most n non-zero coefcients.

This corollary implies, for example, that if the basis functions come from a continuous non-
redundant distribution (which means that any equality would hold with probability 0) then with
probability 1 any solution of (33) has at most n non-zero coefcients.

Theorem 5 Assume that there is no set of more than n dictionary functions which obeys the equal-
ities (36,38) on the training data. In addition assume:

1. The loss function C is strictly convex (squared error loss, Cl and Ce obviously qualify),

2. No set of dictionary functions of size (cid:20) n is linearly dependent on the training data.

Then the problem (33) has a unique solution.

Proof The previous corollary tells us that any solution has at most n non-zero coefcients. Now
assume b 1; b 2 are both solutions of (33). From strict convexity of the loss we get that

h(X)0b 1 = h(X)0b 2 = h(X)0(a

1 + (1 (cid:0) a )b 2) ; 80 (cid:20) a (cid:20) 1;

and from convexity of the l1 norm we get

ka

1 + (1 (cid:0) a )b 2k1 (cid:20) kb 1k1 = kb 2k1 = c:

(40)

(41)

So (a
1 + (1 (cid:0) a )b 2) must also be a solution. Thus, the total number of variables with non-zero
coefcients in either b 1 or b 2 cannot be bigger than n, since then (a
1 + (1 (cid:0) a )b 2), would have
> n non-zero coefcients for almost all values of a
, contradicting Corollary 4. Thus, by ignoring
all coefcients which are 0 in both b 1 and b 2 we get that both b 1 and b 2 can be represented in the
same n (cid:0) dimensional (maximum) sub-space of RJ. Which leads to a contradiction between (40)
and assumption 2.

: 0 (cid:20) c (cid:20)  g of normalized solutions to the problem (33).
Corollary 6 Consider a sequence f
Assume that all these solutions have at most n non-zero coefcients. Then any limit point of the
sequence has at most n non-zero coefcients.

b (c)
c

Proof This is a trivial consequence of convergence. Assume by contradiction b (cid:3) is a convergence
jj : b (cid:3)
point with more than n non-zero coefcients. Let k = argmin jfjb (cid:3)
j 6= 0g. Then for any vector
b with at most n non-zero coefcients we know that k b (cid:0) b (cid:3)k (cid:21) jb (cid:3)
jj > 0 so we get a contradiction
to convergence.

970

b
b
b
b
BOOSTING AS A REGULARIZED PATH

B.2 Uniqueness of Limiting Solution in Theorem 3 when Margin Maximizing Separator is

not Unique

b (p)(c)

Recall, that we are interested in convergence points of the normalized regularized solutions
.
Theorem 3 proves that any such convergence point corresponds to an l p-margin maximizing sep-
arating hyper-plane. We now extend it to the case that this rst-order separator is not unique, by
extending the result to consider the second smallest margin as a tie breaker. We show that any
convergence point maximizes the second smallest margin among all models with maximal minimal
margin. If there are also ties in the second smallest margin, then any limit point maximizes the third
smallest margin among all models which still remain, and so on. It should be noted that the minimal
margin is typically not attained by one observation only in margin maximizing models. In case of
ties in the smallest margins our reference to smallest, second smallest etc.
implies arbitrary
tie-breaking (i.e., our decision on which one of the tied margins is considered smallest, and which
one second smallest is of no consequence).

c

Theorem 7 Assume that the data is separable and that the margin-maximizing separating hyper-
c will correspond to a
plane, as dened in (4) is not unique. Then any convergence point of
margin-maximizing separating hyper-plane which also maximizes the second smallest margin.

b (p)(c)

Proof The proof is essentially the same as that of Theorem 3. We outline it below.

From Theorem 3 we know that we only need to consider margin-maximizing models as limit
points. Thus let b 1, b 2 be two margin maximizing models with lp norm 1, but let b 1 have a bigger
second smallest margin. Assume that b 1 attains its smallest margin on observation i1 and b 2 attains
the same smallest margin on observation i2. Now dene

m1 = min
i6=i1

yih(xi)0b 1 > min
i6=i2

yih(xi)0b 2 = m2:

Then we have that Lemma 4 of Theorem 3 holds for b 1 and b 2 (the proof is exactly the same, except
that we ignore the smallest margin observation for each model, since these always contribute the
same amount to the combined loss).

Let b (cid:3) be a convergence point. We know b (cid:3) maximizes the margin from Theorem 3. Now
assume b also maximizes the margin but has bigger second-smallest margin than b (cid:3). Then we can
proceed exactly as the proof of Theorem 3, considering only n (cid:0) 1 observations for each model and
using our modied Lemma 4, to conclude that b (cid:3) cannot be a convergence point (again note that the
smallest margin observation always contributes the same to the loss of both models).

In the case that the two smallest margins still do not dene a unique solution, we can continue
up the list of margins, applying this result recursively. The conclusion is that the limit of the normal-
ized, lp -regularized models maximizes the margins, and not just the minimal margin. The only
case when this convergence point is not unique is, therefore, the case that the whole order statistic of
the optimal separator is not unique. It is an interesting research question to investigate under which
conditions this scenario is possible.

971

ROSSET, ZHU AND HASTIE

