Summary. We propose a method (the `gap statistic') for estimating the number of clusters (groups)
in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or
hierarchical), comparing the change in within-cluster dispersion with that expected under an
appropriate reference null distribution. Some theory is developed for the proposal and a simulation
study shows that the gap statistic usually outperforms other methods that have been proposed in the
literature.

Keywords: Clustering; Groups; Hierarchy; K-means; Uniform distribution

1.

Introduction

Cluster analysis is an important tool for `unsupervised' learning  the problem of nding
groups in data without the help of a response variable. A major challenge in cluster analysis is
the estimation of the optimal number of `clusters'. Fig. 1(b) shows a typical plot of an error
measure Wk (the within-cluster dispersion dened below) for a clustering procedure versus the
number of clusters k employed: the error measure Wk decreases monotonically as the number
of clusters k increases, but from some k onwards the decrease attens markedly. Statistical
folklore has it that the location of such an `elbow' indicates the appropriate number of
clusters. The goal of this paper is to provide a statistical procedure to formalize that heuristic.
For recent studies of the elbow phenomenon, see Sugar (1998) and Sugar et al. (1999). A
comprehensive survey of methods for estimating the number of clusters is given in Milligan
and Cooper (1985), whereas Gordon (1999) discusses the best performers. Some of these
methods are described in Sections 5 and 6, where they are compared with our method.

In this paper we propose the `gap' method for estimating the number of clusters. It is
designed to be applicable to virtually any clustering method. For simplicity, the theoretical
part of our analysis will focus on the widely used K-means clustering procedure.

2. The gap statistic
Our data fxijg, i  1, 2, . . ., n, j  1, 2, . . ., p, consist of p features measured on n inde-
pendent observations. Let dii 0 denote the distance between observations i and i 0. The most
common choice for dii0 is the squared Euclidean distance j xij  xi 0j2.
the indices of observations in cluster r, and nr  jCrj. Let

Suppose that we have clustered the data into k clusters C1, C2, . . ., Ck, with Cr denoting

Address for correspondence: Robert Tibshirani, Department of Health Research and Policy and Department of

Statistics, Stanford University, Stanford, CA 94305, USA.
E-mail: tibs@stat.stanford.edu

& 2001 Royal Statistical Society

13697412/01/63411

412

R. Tibshirani, G. Walther and T. Hastie

(a)

Fig. 1. Results for the two-cluster example: (a) data; (b) within sum of squares function Wk ; (c) functions
log (Wk ) (O) and ^E *nflog (Wk )g (E); (d) gap curve

Dr  P

dii0

i,i02Cr

1

2

be the sum of the pairwise distances for all points in cluster r, and set

Wk  Pk

r1

1
2nr

Dr.

So, if the distance d is the squared Euclidean distance, then Wk is the pooled within-cluster
sum of squares around the cluster means (the factor 2 makes this work exactly). The sample
size n is suppressed in this notation.
The idea of our approach is to standardize the graph of logWk by comparing it with its
expectation under an appropriate null reference distribution of the data. (The importance of
the choice of an appropriate null model is demonstrated in Gordon (1996).) Our estimate of
the optimal number of clusters is then the value of k for which logWk falls the farthest
below this reference curve. Hence we dene

Gapnk  E*nflogWkg  logWk,

3

where E*n denotes expectation under a sample of size n from the reference distribution. Our
estimate ^k will be the value maximizing Gapnk after we take the sampling distribution into

account. Note that this estimate is very general, applicable to any clustering method and
distance measure dii0.

Gap Statistic

413

As a motivation for the gap statistic, consider clustering n uniform data points in p
dimensions, with k centres. Then, assuming that the centres align themselves in an equally
spaced fashion, the expectation of logWk is approximately

log pn=12  2=p logk  constant.

4
If the data actually have K well-separated clusters, we expect logWk to decrease faster
than its expected rate 2=p logk for k 4 K. When k > K, we are essentially adding an
(unnecessary) cluster centre in the middle of an approximately uniform cloud and simple
algebra shows that logWk should decrease more slowly than its expected rate. Hence the gap
statistic should be largest when k  K.
As a further motivation, note that, in the case of a special Gaussian mixture model,
logWk has an interpretation as a log-likelihood; see Scott and Symons (1971). To develop
the gap statistic into an operational procedure, we need to nd an appropriate reference
distribution and to assess the sampling distribution of the gap statistic.

3. The reference distribution

In our framework we assume a null model of a single component, and we reject it in favour of
a k-component model (k > 1), if the strongest evidence for any such k warrants it, i.e. we wish
to screen the evidence over all k > 1 simultaneously. This approach of guarding against
erroneous rejection of the one-component model is similar to that of Roeder (1994). A
component (cluster) of the distribution can be appropriately modelled by a log-concave
distribution, i.e. by a density of the form expf xg, where   is a concave function (unless the
distribution is degenerate). Standard examples are of course the normal distribution (with
x   1
kxk2 and the uniform distribution with convex support. In Walther (2001) it is
shown there that it is impossible to set condence intervals (even one sided) for the number of
modes in a multivariate distribution, a crucial aspect for the goal of this paper. Thus we
model the components as log-concave densities instead of the often-used unimodal densi-
ties. We denote by S p the set of such single-component distributions (or random variables)
on Rp.

2

To see how to nd an appropriate reference distribution, consider for a moment the

population version corresponding to the gap statistic in the case of K-means clustering:

where MSEXk  Emin2Ak
k X   k2, with the k-point set Ak  Rp chosen to minim-
ize this quantity, is the population version corresponding to Wk. We subtracted off the
logarithms of the variances to make g1  0. So we are looking for a least favourable single-
component reference distribution on X * such that gk 4 0 for all X 2 S p and all k 5 1. The
rst theorem shows that in the univariate case such a reference distribution is given by the
uniform distribution U  U0, 1.

Theorem 1. Let p  1. Then for all k 5 1

gk  log



MSEXk
MSEX1





 log

MSEXk
MSEX1



,

414

R. Tibshirani, G. Walther and T. Hastie



 MSEUk
MSEU1 .

5

MSEXk
MSEX1



inf
X2S p

In other words, among all unimodal distributions, the uniform distribution is the most

likely to produce spurious clusters by the gap test.

Note that the above problem is invariant under changes in location and scale, thus
allowing us to restrict attention to the uniform distribution supported on the unit interval.
Calculations show that MSEUk=MSEU1  1=k2. So there is a formal similarity to a
proposal by Krzanowski and Lai (1985), following Marriott (1971), who suggested to estimate
k by comparing successive differences of Wkk2=p. Note, however, that their procedure is not
dened for the important single-component case k  1. Even more importantly, such an
approach will generally fail in a multivariate situation.

Theorem 2. If p > 1 then no distribution U 2 S p can satisfy equation (5) unless its support
is degenerate to a subset of a line.
Note that the assertion of the last theorem is not contingent on our denition S p of a
single-component model. The same conclusion would apply if we based it on, say, unimodal
densities instead. Simple calculations show that employing a reference distribution with
degenerate support will result in an ineffectual procedure. Thus the upshot of the theorem is
that in a multivariate situation we will not be able to choose a generally applicable and useful
reference distribution: the geometry of the particular null distribution matters.
An obvious solution would be to generate reference data from the maximum likelihood
estimate (MLE) in S p. This is the nonparametric MLE of the density under the restriction of
being log-concave. This MLE can be shown to exist, as opposed to the MLE of a unimodal
distribution. In one dimension, this MLE can be computed with the help of the iterative
convex minorant algorithm (see Walther (2000)). However, we do not know how to compute
the MLE in higher dimensions, but the next section shows how the insights gained from
theorems 1 and 2 can be used to construct a simple and effective reference distribution.

4. The computational implementation of the gap statistic

The lesson of theorem 2 was that the multivariate variance structure matters. Our idea is to
exploit the shape information in the principal components instead of the more complicated
structure provided by the MLE.

We consider two choices for the reference distribution:

(a) generate each reference feature uniformly over the range of the observed values for that

feature;
(b) generate the reference features from a uniform distribution over a box aligned with the
principal components of the data. In detail, if X is our n  p data matrix, assume that
the columns have mean 0 and compute the singular value decomposition X  UDV T.
We transform via X0  XV and then draw uniform features Z0 over the ranges of the
columns of X0, as in method (a) above. Finally we back-transform via Z  Z0V T to
give reference data Z.

Method (a) has the advantage of simplicity. Method (b) takes into account the shape of the
data distribution and makes the procedure rotationally invariant, as long as the clustering
method itself is invariant.

In each case, we estimate E*nflogWkg by an average of B copies logW *k, each of which is
computed from a Monte Carlo sample X *1, . . ., X *n drawn from our reference distribution.
Finally, we need to assess the sampling distribution of the gap statistic. Let sd(k) denote the
standard deviation of the B Monte Carlo replicates logW *k. Accounting additionally for the
simulation error in E*nflogWkg results in the quantity

Gap Statistic

415

sk  p

1  1=B sdk.

Using this we choose the cluster size ^k to be the smallest k such that Gapk 5 Gapk  1
 sk1. This `1-standard-error' style of rule is used elsewhere (e.g. Breiman et al. (1984)). In
the simulation studies later in this paper and in other real data examples, we have found
empirically that it works well. A more rened approach would employ a multiplier to the sk
for better control of the rejection of the null model.

Computation of the gap statistic proceeds as follows.
Step 1: cluster the observed data, varying the total number of clusters from k  1, 2, . . ., K,
giving within-dispersion measures Wk, k  1, 2, . . ., K.
Step 2: generate B reference data sets, using the uniform prescription (a) or (b) above, and
cluster each one giving within-dispersion measures W *kb, b  1, 2, . . ., B, k  1, 2, . . ., K.
Compute the (estimated) gap statistic

Step 3: let l  1=B b logW *kb, compute the standard deviation

Gapk  1=BP
sdk  1=BP

b

logW *kb  logWk.

flogW *kb  lg21=2

and dene sk  sdk

b

1  1=B. Finally choose the number of clusters via

p
^k  smallest k such that Gapk 5 Gapk  1  sk1.

Fig. 1 shows an example using K-means clustering. The data (Fig. 1(a)) fall in two distinct
clusters. The within sum of squares function Wk is displayed in Fig. 1(b). The functions
logWk and ^E*nflogWkg are shown in Fig. 1(c), with the gap curve displayed in Fig. 1(d),
with 1 standard error bars. The gap curve has a clear maximum at ^k  2.

Fig. 2 examines the behaviour of the gap estimate with unclustered data. The raw data are
100 observations uniformly distributed over the unit square. The observed and expected
curves are very close, and the gap estimate is ^k  1.

4.1. Example: application to hierarchical clustering and DNA microarray data
In this example our data are a 6834  64 matrix of gene expression measurements. Each row
represents a gene, and each column a human tumour. The data are taken from Ross et al.
(2000) and are available at http://www-genome.stanford.edu/nci60. The columns
have a label (cancer type), but this label was not used in the clustering. We applied
hierarchical (agglomerative) clustering to the columns, using squared error and average
linkage, and obtained the dendrogram in Fig. 3. Not surprisingly, many cancers of the same
type are clustered together. For more on the utility of hierarchical clustering for microarray
data, see Ross et al. (2000).

The results for the gap statistic are shown in Fig. 4. The estimated number of clusters is 2.
The corresponding cut of the dendrogram is indicated by the dotted line in Fig. 3. However,

416

R. Tibshirani, G. Walther and T. Hastie

Fig. 2. Results for the uniform data example: (a) data; (b) within sum of squares function Wk ; (c) functions
log (Wk ) (O) and ^E *nflog (Wk )g (E); (d) gap curve

the gap function starts to rise again after six clusters, suggesting that there are two well-
separated clusters and more less separated ones. The derivation for the gap test assumes
that there are well-separated uniform clusters. In cases where there are smaller subclusters
within larger well-separated clusters, it can exhibit non-monotone behaviour. Hence it is
important to examine the entire gap curve rather than simply to nd the position of its
maximum.

5. Other approaches

Many methods have been proposed for estimating the number of clusters: a good summary is
given by Gordon (1999). He divides the approaches into global and local methods. The
former evaluate some measure over the entire data set and optimize it as a function of the
number of clusters. The latter consider individual pairs of clusters and test whether they
should be amalgamated. Hence the gap method is a global procedure.

According to Gordon, most global methods have the disadvantage that they are undened
for one cluster and hence offer no indication whether the data should be clustered at all.
A very recent proposal is given by Cuevas et al. (2000); however, this relies on a high
dimensional density estimate, which may suffer from the curse of dimensionality.

Gap Statistic

417

Fig. 3. Dendrogram from the deoxyribonucleic acid (DNA) microarray data: the dotted line cuts the tree, leaving
two clusters as suggested by the gap statistic

Fig. 4.
the DNA microarray data

(a) Logarithmic observed (O) and expected (E) within sum of squares curves and (b) the gap statistic for

Milligan and Cooper (1985) carried out a comprehensive simulation comparison of 30
different procedures. Among the global methods performing the best was the index due to
Calinski and Harabasz (1974):

CHk  Bk=k  1
Wk=n  k

6

where Bk and Wk are the between- and within-cluster sums of squares, with k clusters.
The idea is to maximize CHk over the number of clusters k. CH1 is not dened; even if it

R. Tibshirani, G. Walther and T. Hastie

418
were modied by replacing k  1 with k, its value at 1 would be 0. Since CHk > 0 for k > 1,
the maximum would never occur at k  1.

As mentioned earlier, Krzanowski and Lai (1985) proposed the quantity Wkk2=p as a
criterion for choosing the number of clusters. This followed a proposal by Marriott (1971),
who used the determinant, rather than the trace, of the within sum of squares matrix. The
actual proposal of Krzanowski and Lai (1985) dened

DIFFk  k  12=pWk1  k2=pWk

and chose k to maximize the quantity

KLk 

DIFFk

DIFFk  1

.

7

8

This is similar to maximizing Wk k2=p, but Krzanowski and Lai (1985) argued that it may
have better properties. Note that KLk is not dened for k  1 and hence cannot be used for
testing one cluster versus more than one.
Hartigan (1975) proposed the statistic





Hk 

Wk
Wk  1  1

n  k  1.

9
The idea is to start with k  1 and to add a cluster as long as Hk is sufciently large. One
can use an approximate F-distribution cut-off; instead Hartigan suggested that a cluster be
added if Hk > 10. Hence the estimated number of clusters is the smallest k 5 1 such that
Hk 4 10. This estimate is dened for k  1 and can potentially discriminate between one
versus more than one cluster.
Kaufman and Rousseeuw (1990) proposed the silhouette statistic, for assessing clusters and
estimating the optimal number. For observation i, let ai be the average distance to other
points in its cluster, and bi the average distance to points in the nearest cluster besides its
own nearest is dened by the cluster minimizing this average distance. Then the silhouette
statistic is dened by

si  bi  ai

10
A point is well clustered if si is large. Kaufman and Rousseeuw (1990) proposed to choose
the optimal number of clusters ^k as the value maximizing the average si over the data set.
Note that si is not dened for the k  1 cluster.

maxfai, big .

6. Simulations

We generated data sets in ve different scenarios:

(a) null (single-cluster) data in 10 dimensions  200 data points uniformly distributed over

(b) three clusters in two dimensions  the clusters are standard normal variables with (25,

the unit square in 10 dimensions;
25, 50) observations, centred at (0, 0), (0, 5) and (5, 3);
(c) four clusters in three dimensions  each cluster was randomly chosen to have 25 or 50
standard normal observations, with centres randomly chosen as N0, 5I (any simu-
lation with clusters having a minimum distance less than 1.0 units between them was
discarded;

Gap Statistic

419

(d) four clusters in 10 dimensions  each cluster was randomly chosen to have 25 or 50
standard normal observations, with centres randomly chosen as N0, 1:9I (any
simulation with clusters having a minimum distance less than 1.0 units between them
was discarded; in this and the previous scenario, the settings are such that about half of
the random realizations were discarded);
(e) two elongated clusters in three dimensions  each cluster is generated as follows. Set
x1  x2  x3  t with t taking 100 equally spaced values from 0:5 to 0.5 and then
Gaussian noise with standard deviation 0.1 is added to each feature. Cluster 2 is
generated in the same way, except that the value 10 is added to each feature at the end.
The result is two elongated clusters, stretching out along the main diagonal of a three-
dimensional cube.

50 realizations were generated from each setting. In the non-null settings, the clusters have no
overlap, so there is no confusion over the denition of the `true' number of clusters. We
applied six different methods for estimating the number of clusters: CH, KL, Hartigan and
Silhouette are given by equations (6), (8), (9) and (10) respectively. Gap/unif is the gap method
with a uniform reference distribution over the range of each observed feature; Gap/pc uses the
uniform reference in the principal component orientation. The results are given in Table 1.
The gap estimate using the uniform reference does well except in the last problem, where
the oblong shape of the data adversely affects it. The Gap/pc method, using a uniform
reference in the principal components orientation, is the clear winner overall.

The other methods do quite well, except in the null setting where the gap estimate is the
only one to show a reasonable performance. Of course it might be possible to modify any of
the methods to handle the null (single-cluster) case: one possibility would be to simulate their
null distribution under uniform data, in a manner similar to the gap estimate.

7. Overlapping classes

The simulation studies suggest that the gap estimate is good at identifying well-separated
clusters. When data are not well separated, the notion of a cluster is not any more well
dened in the literature.

In this section, we did a small experiment to assess how the gap method responds to non-
separated data. Each simulated data set consists of 50 observations from each of two
bivariate normal populations, with means 0, 0 and , 0, and identity covariance. For each
sample we computed the gap estimate of the number of clusters and also recorded the
proportion of data points from the rst population that were closer to the second population
mean, or vice versa. We call this the amount of `overlap'. This was done for 10 values of 
running from 0 to 5, with 10 simulations done for each value of . The results are shown in
Fig. 5. Roughly speaking, if the overlap proportion is p, then the probability of selecting one
cluster is also about p.

8. Discussion

The problem of estimating the number of clusters in a data set is difcult, underlined by the
fact that there is no clear denition of a `cluster'. Hence, in data that are not clearly separated
into groups, different people might have different opinions about the number of distinct
clusters. In this paper, we have focused on well-separated clusters and have proposed the
gap statistic for estimating the number of groups. When used with a uniform reference

420

R. Tibshirani, G. Walther and T. Hastie

Table 1. Results of the simulation study{

Method

Estimates of the following numbers of clusters ^k:

4

5

6

1

2

Null model in 10 dimensions
CH
KL
Hartigan
Silhouette
Gap/unif
Gap/pc

0z
0z
0z
0z
49z
50z

50
29
0
49
1
0

3-cluster model
CH
KL
Hartigan
Silhouette
Gap/unif
Gap/pc

0
0
0
0
1
2

0
0
0
0
0
0

3

0
5
1
1
0
0

50z
39z
1z
50z
49z
48z

0
3
20
0
0
0

0
0
8
0
0
0

Random 4-cluster model in 3 dimensions
42z
CH
35z
KL
3z
Hartigan
15z
Silhouette
47z
Gap/unif
42z
Gap/pc

0
0
1
20
1
2

0
0
7
15
2
4

0
0
0
0
0
2

Random 4-cluster model in 10 dimensions
44z
CH
45z
KL
48z
Hartigan
16z
Silhouette
50z
Gap/unif
46z
Gap/pc

1
0
0
13
0
0

4
0
2
20
0
4

0
0
0
0
0
0

2 elongated clusters
CH
KL
Hartigan
Gap/unif
Gap/pc

0
0
0
0
0

0z
50z
0z
0z
50z

0
0
0
17
0

0
0
1
16
0

7

0
2
0
0
0
0

0
1
3
0
0
0

0
3
8
0
0
0

0
1
0
0
0
0

0
0
1
1
0

8

0
0
0
0
0
0

0
2
3
0
0
0

0
3
2
0
0
0

0
0
0
0
0
0

7
0
5
0
0

9

10

0
0
0
0
0
0

0
0
2
0
0
0

0
0
3
0
0
0

0
0
0
0
0
0

0
0
0
0
0
0

0
0
1
0
0
0

0
0
5
0
0
0

0
0
0
0
0
0

16
0
6
0
0

27
0
35
0
0

0
3
21
0
0
0

0
5
19
0
0
0

8
5
9
0
0
0

1
3
0
5
1
0

0
0
0
2
0

0
2
6
0
0
0

0
1
13
0
0
0

0
3
12
0
0
0

0
1
0
0
0
0

0
0
2
14
0

{Numbers are counts out of 50 trials. Some rows do not add up to 50 because the
number of clusters chosen was greater than 10.
zColumn corresponding to the correct number of clusters.

distribution in the principal component orientation, it outperforms other proposed methods
from the literature in our simulations. The simpler uniform reference (over the range of the
data) works well except when the data lie near a subspace.

The DNA microarray example shows the importance of graphing the gap statistic, rather
than simply extracting the estimated maximum. With real data the gap curve can have many
local maxima, and these themselves can be informative.

There are many avenues for further research. One is a consideration of other possibilities
for the reference distribution: for example, we could proceed sequentially. Having found k
clusters, we could generate reference data from k separate uniform distributions, over the
support of each of the k estimated data clusters. As before, a principal component orientation
would probably produce better results. The gap method can also be used with adaptive

Gap Statistic

421

Fig. 5. Gap method for overlapping data: the proportion of times that the method chose one cluster, as a
function of the proportion of points in the overlap region between the two subpopulations

versions of K-means clustering (see for example Diday and Govaert (1977)), which may be
better at nding elongated clusters than the standard version. Similarly, it may be applicable
to model-based clustering (Fraley and Raftery, 1998).

A referee raised the interesting question of how to carry out the gap test when the
dimension p of the data is unknown and only pairwise dissimilarities are available. One
possibility would be to use multidimensional scaling to map the data into a low dimensional
space while preserving the dissimilarities, and then to proceed in this space as described in the
paper. However, a more direct method would be preferable and we leave this as an open
problem.

It would be especially useful to develop methods for an efcient simulation of reference
data from the log-concave MLE. The use of this distribution in the gap method could then be
compared with the uniform reference distribution.

Acknowledgements

Tibshirani was partially supported by National Institutes of Health grant 2 R01 CA72028
and National Science Foundation grant DMS-9971405. Hastie was partially supported by
grant DMS-9803645 from the National Science Foundation and grant ROI-CA-72028-01
from the National Institutes of Health.

Appendix A: Proofs

=k for 1 4 j 4 k shows that MSEUk 4 Efmin j

A.1. Proof of theorem 1
Setting j :  j  1
MSEUk=MSEU1 4 1=k2. Thus it is enough to prove
X 5

Pk

PX 2 Ii varIi

2

i1

U  j2g  1=12k2, whence

varX

1
k2

11

for every partition I1, . . ., Ik of the support of X. Here we write
x dPX=PX 2 I
PX 2 I

varIX 

x 







I

I

2

dPX

R. Tibshirani, G. Walther and T. Hastie

422
for the conditional variance of X given X 2 I.

By standard arguments (e.g. convolution with a Gaussian kernel and using Ibragimov's convolution
result; see theorem 1.10 in Dharmadhikari and Joag-dev (1988)), it is enough to consider a non-
degenerate cumulative density function F of X that has a density f which is logarithmically concave and
differentiable in the interior of its support and so does not vanish there. Hence

ffF1tg  f 0fF1tg

ffF1tg  d

dx

d
dt

logf fxgjxF1t.

But dlogf fxg=dx is non-increasing as f is logarithmically concave. Together with the fact that F1t is
non-decreasing, it follows that ffF1.g has a non-increasing derivative and hence is concave on 0, 1.

Next, write

varX  1
2
 1
1
2



1

1
1
1

0

 y  x2 fx f y dx dy

1
1
1
fF1v  F1ug2 du dv
v

0

du dv

1

2
ffF1tg dt
uz

u

2
ffF1tg dt

1

0

u

1

1z

z0

u0

u

by symmetry and the fundamental theorem of calculus. The change of variable z  v  u gives

varX 

du dz.

12

Proceeding likewise with varIi

X we obtain
1z

1

z2

z0

u0

X  Pk

i1

Pk

i1

FIi varIi


F 3Ii

1
FIiz



si1FIiuz

si1FIiu

2
ffF 1tg dt

1

du dz,

where we set si : j4i FIj, i  0, . . ., k.

Using the concavity of ffF1.g and Holder's inequality it can be shown that the above expression is

not smaller than

1

Pk

i1

z2
k2

z0

1z

u0



1
z



si11zFIiuz

si11zFIiu

2

1

ffF1tg dt

FIi du dz
Pk

1

 1
k2



si11zFIi1z

vz

tv

2

dv dz

1

ffF1tg dt

z0

i1

vsi11z

proving inequality (11).

 1
k2

varX

by equation (12),

A.2. Proof of theorem 2
If X is uniformly distributed on U0, k  0, p1, then MSEX1  fk2   p  12g=12, and taking j 
 j  1=2, =2, . . ., =2, 1 4 j 4 k, shows that MSEXk 4 Eminj
kX  jk2  f1   p  12g=12. So
fMSEXk=MSEX1g 4 1=k2,

inf
X2S p

even if we were to consider only X 2 S p with non-degenerate support.
1 4 i 4 p, must be in S 1 by theorem 2.16 in Dharmadhikari and Joag-dev (1988). Hence

However, suppose that U 2 S p satises MSEUk=MSEU1  1=k2. Each of the marginals Ui of U,

k

for all i by theorem 1,

Gap Statistic

423
13

14

1 4 k2 MSEUi
MSEUi
Pp

MSEUi

i1

and clearly

So

k 4 MSEUk

for all k > 1.

MSEU1  Pp

i1

1 4 k2 Pp

i1

MSEUi

MSEUi

k 4 MSEUk,

and hence MSEUk=MSEU1  1=k2 can only hold if we have equality in expressions (13) and (14).
To avoid technicalities we shall only give the main arguments for the remainder of the proof.
Proceeding similarly as in the proof of theorem 1 we conclude from equality in expression (13) that the
Ui must have a uniform distribution, with the optimal centres
i j, 1 4 j 4 k, equally spaced. Let li be
the length of the support of Ui. We then check that expression (14) can hold with equality only if with
probability 1 the centre
i j closest to Ui has the same index j for all marginals i. But the set of u 2 Rp
i1 li=k ! 0 as k ! 1. Hence, by
for which the latter statement holds has Lebesgue measure k p
Prekopa's theorem (theorem 2.8 in Dharmadhikari and Joag-Dev (1988)), the support of U must be
degenerate and contained in a linear subspace of Rp. Repeating this argument at most p  1 times
proves the theorem.

