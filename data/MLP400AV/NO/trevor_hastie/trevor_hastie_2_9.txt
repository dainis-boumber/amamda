Abstract

We introduce a technique for extending the classical method of Linear Discriminant Analysis to data
sets where the predictor variables are curves or functions. This procedure, which we call functional linear
discriminant analysis (FLDA), is particularly useful when only fragments of the curves are observed. All
the techniques associated with LDA can be extended for use with FLDA. In particular FLDA can be used
to produce classications on new (test) curves, give an estimate of the discriminant function between
classes, and provide a one or two dimensional pictorial representation of a set of curves. We also extend
this procedure to provide generalizations of quadratic and regularized discriminant analysis.

Some key words: Classication; Filtering; Functional data; Linear discriminant analysis; Low dimensional representa-
tion; Reduced rank; Regularized discriminant analysis; Sparse curves.

1 Introduction

Linear discriminant analysis (LDA) is a popular procedure which dates back as far as Fisher (1936). Let X be
a q-dimensional vector representing an observation from one of several possible classes. Linear discriminant
analysis can be used to classify X if the class is unknown. Alternatively, it can be used to characterize the
way that classes differ via a discriminant function. There are several different ways of describing LDA. One
is using probability models. Suppose that the ith class has density fi x
i. Then Bayes
formula tells us that

and prior probability p

P Class

i

x



i

fi x
j f j x

j

(1)

It is relatively simple to show that the rule that classies to the largest conditional probability will make the
smallest expected number of misclassications. This is known as the Bayes rule or classier. If we further
assume that the ith class has a Gaussian distribution with mean m
i and covariance S
then it can be shown that
classifying to the maximum conditional probability is equivalent to classifying to

arg max

i

Li

1

(2)





p
(cid:229)

p


Female
Male

4

.

1

2

.

1

0

.

1

8

.

0

6

.

0

y
t
i
s
n
e
D


l

i

a
r
e
n
M
e
n
o
B




l

i

a
n
p
S

10

15

20

25

Age (years)

Figure 1: Data measurements of spinal bone mineral density (g
females and the grey lines males.

cm2) for 280 individuals. The black lines represent

where Li is the discriminant function

Li

xTS

1m

i

m T
i S

1m

2

i

logp

i

Note that Li is a linear function of x. When the maximum likelihood estimates for m
at the linear discriminant analysis procedure.

i and S are used we arrive

1.1 LDA on functional data

LDA can be implemented on data of any nite dimension but cannot be directly applied to innite-dimensional
data such as functions or curves. Provided the entire curve has been observed, this can be overcome by dis-
cretizing the time interval. However, this generally results in highly correlated high-dimensional data which
makes the within-class covariance matrix difcult to estimate. There are two common solutions to this prob-
lem. The rst is to use some form of regularization, such as adding a diagonal matrix to the covariance matrix.
See for example DiPillo (1976), DiPillo (1979), Campbell (1980), Friedman (1989) and Hastie et al. (1995).
We call this the regularization method. The second is to choose a nite-dimensional basis, f
, and nd
the best projection of each curve onto this basis. The resulting basis coefcients can then be used as a nite-
dimensional representation making it possible to use LDA, or any other procedure, on the basis coefcients.
We call this the ltering method.

 x

Unfortunately, it is often the case that only a fragment of each curve has been observed. Consider, for ex-
ample, the data illustrated in Figure 1. These data consist of measurements of spinal bone mineral density for
280 individuals taken at various ages, a subset of the data presented in Bachrach et al. (1999). Even though,
in aggregate, there are 860 observations measured over a period of almost two decades, we only have 2-4
measurements for each individual, typically measured over no more than a couple of years. In this situation

2










both of the common approaches to discriminant analysis can break down. The regularization method is not
feasible because discretizing would result in a large number of missing observations in each dimension. The
ltering method also has several potential problems. The rst is that an assumption is made of a common
covariance matrix for each curves basis coefcients. However, if the curves are measured at different time
points, as is the case in the growth curve data of Figure 1, the coefcients will all have different covariances.
One would ideally like to put more weight on accurate basis coefcients but the ltering method does not
allow such an approach. A second problem is that with extremely sparse data sets some of the basis coef-
cients may have innite variance, making it impossible to estimate the entire curve. For example, with the
spinal bone density data, each individual curve has so few observations that it is not possible to t any rea-
sonable basis. In this case the method fails and there is no way to proceed. For the sparse data considered in
this paper these are serious problems.

1.2 General functional model

The regularization and ltering approaches can both be viewed as methods for tting the following general
functional model. Let g t
be the curve of an individual randomly drawn from the ith class. Assume that, if
g t

is in Class i, it is distributed as a Gaussian process with

g t
One typically never observes an individual over the entire curve; rather one samples the curve with error at
distinct time points t1
tn. We assume that the measurement errors are uncorrelated with mean zero and
constant variance s 2. Let Y be the vector of observations of g t
s 2I

Cov g t

E g t

at times t1

tn. Then

i t





t

 t

Y N

i

where




i t1
i t2
...
i tn

i




(3)

t1
t1

t1

 t1
 t2
...
 tn

...

t2
t2

tn
tn

 t1
 t2
...
 tn
tn
i as given in (3) and S

 t1
 t2
...
 tn

t2

s 2I. Many func-
The Bayes rule for classifying this curve is given by (2) with m
tional classication procedures are simply methods for tting this model, i.e. estimating m
,
i t
and then using the classication rule given by (2). For example, the regularization approach attempts to es-
timate m
by producing sample estimates along a ne lattice of time points. Alternatively, the
ltering method forms estimates by modeling m
using basis functions. However, we saw in
Section 1.1 that, when confronted with sparse data sets, both methods can produce poor ts to the model.

and w

and w

and w

i t

i t

t

t

 t

 t

 t

t

1.3 The FLDA approach

In this paper we present an alternative method for tting the functional model of 1.2, which copes well with
sparse data. We call this method functional linear discriminant analysis (FLDA). The procedure uses a
spline curve plus random error to model observations from each individual. The spline is parameterized using
a basis function multiplied by a q-dimensional coefcient vector. This effectively transforms all the data into
a single q-dimensional space. Finally, the coefcient vector is modeled using a Gaussian distribution with
common covariance matrix for all classes, in analogy with LDA. The observed curves can then be pooled to

3




m





w













m

W


m






m

m

m





W






w


w





w


w


w





w


w


w





w






W










Class 1
Class 2

y

5

.

1

0
1

.

5

.

0

0

.

0

5

.

0
-

.

0
1
-

Class 1
Class 2

y

4

.

0

2

.

0

0
0

.

.

2
0
-

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Time

(a)

Time

(b)

Figure 2: (a) A simulated data set of 20 curves from 2 different classes. (b) The transformed curves after removing
the random components.

estimate the covariance and mean for each class. This makes it possible to form accurate estimates for each
individual curve based on only a few observations.

This has several advantages over the regularization and ltering methods. First, as it does not rely on
forming individual estimates for each curve, it can be used on sparse data sets such as the growth curve data.
Second, by producing an estimate for the covariance kernel it is possible to estimate the variance of the basis
coefcient for each curve and automatically put more weight on the more accurate coefcients.

As a simple illustration of the effectiveness of FLDA in separating curves from different classes consider
Figure 2(a). This is a plot of curves from a simulated data set. Class 1 curves are plotted as black lines and
Class 2 as grey. Each class has a different mean function and the curves are generated by combining the
class mean with a random curve plus random normal noise. From visual inspection alone there is no obvious
separation between classes. However, Figure 2(b) provides a plot of the transformed curves after using the
FLDA procedure to remove the random component from each curve. Now the separation is clear so when
the same procedure is applied to a new curve it will clearly be identiable which group it falls into and it can
be classied with high accuracy. The level of accuracy depends on the signal to noise ratio, which is high in
this case. However, the key point is that the strong signal is not apparent in Figure 2(a) and only emerges as
a result of using the FLDA procedure. In the classical two-class LDA setting this transformation amounts to
projecting observations onto the line segment spanned by the means.

The FLDA model and classication procedure are presented in Sections 2 and 3. One of the reasons for
the popularity of LDA is that it can be used for a variety of tasks. It can be used to project high-dimensional
data into a low dimension and hence produce a graphical representation. Furthermore it can also be used for
classication and to produce a discriminant function to identify areas of discrimination between classes. In
Section 4 we show how FLDA can be used to generalize each of these tools to functional data. Section 5 ex-
plains how the standard FLDA framework can be extended to include rank-reduced and non-identical within-
class covariance matrices. The latter of these extensions provides a functional generalization of quadratic
discriminant analysis.

4

2 The FLDA model

In this section we develop the FLDA model by generalizing the LDA model given in 1 to handle functional
data.

2.1 A generalization to functional data
Let gi j  t
corresponding vectors of observations and measurement errors at times ti j1

be the true value at time t for the jth individual or curve from the ith class. Let Yi j and e

ti jni j. Then we begin with

i j be the

Yi j

gi j

i j

i

1

K

j

1

mi

where K is the number of classes and mi is the number of individuals in the ith class. The measurement errors
are assumed to have mean zero, constant variance s 2 and be uncorrelated with each other and gi j. These
assumptions implicitly mean that we are assuming that the time points that we have failed to observe are
missing at random. As we only have a nite amount of data we need to place some restrictions on gi j in order
to t this model. A common approach to modeling functional data is to represent the functions using a exible
basis (Ramsay and Silverman 1997, Chapter 3). We choose to use natural cubic spline functions because
of their desirable mathematical properties and easy implementation (de Boor, 1978; Green and Silverman,
1994). Let

gi j t

is a spline basis with dimension q and h

where s t
leads to a more restricted model,

Th

i j

s t
i j is a q-dimensional vector of spline coefcients. This

Yi j

Si jh

i j

i j

i

1

K

j

1

mi

where

Si j

Notice that the problem of modeling Yi j has reduced to one of modeling h
i j is a q-dimensional
variable, so a natural approach is to model it using the Gaussian distribution assumed for the standard LDA
model,

i j. However, h

 s ti j1

T

s ti jni j

i j

i j

i 

i j N 0

If we assume that the error terms are also normally distributed this gives

Yi j

Si j

2.2 Rank reduced LDA

i j

s 2I

i

i j

i j N 0

i

1

K

j

1

mi

(4)

i j N 0

A reduced-rank version of LDA is often performed by transforming or projecting the variables into a lower-
dimensional subspace and classifying in this subspace. The subspace is chosen to maximize the between-
class covariance relative to the within-class covariance. These transformed variables are called linear dis-
criminants or canonical variables. Anderson (1951) and Hastie and Tibshirani (1996) outline an alternative
procedure using the constraint

l 0

L TS

1L

i

I

0

i

i

i 

5

(5)









e




















e























h

m
g

g

G



m

g


e














e



g

G


m

L
a




(cid:229)
a


i are respectively q- and h-dimensional vectors, and L

where l 0 and a
. Both
sets of authors show that using maximum likelihood to t the Gaussian LDA model of 1 with the added
constraint (5) and classifying to the maximum posterior probability is identical to the classication from the
reduced rank LDA procedure.

h matrix, h

min q

is a q

K

The same rank constraint can be placed on the means in (4). This gives the nal form of the FLDA model,

Yi j

l 0

Si j

i

i j
i j N 0

s 2I

i j N 0

i j

i

1

K

j

1

mi

(6)

in which l 0, L
tions on L

and a
and the a

i are confounded if no constraint is imposed. Therefore we place the following restric-
is,

L T STS

1SL

I

0

i

i

(7)

s 2I

SG ST and S is the basis matrix evaluated over a ne lattice of points. The constraint provides
where S
a form of normalization for the linear discriminants. More details will be given in the following section. In
practice the lattice should include, at least, all time points in the data set. For example the spinal bone density
2 years so the lattice covered the same
data was measured in 1
period. This model is identical to the general functional model of 1.2 with

10th of a year increments from age 8

8 to 26

and

3 Classifying curves

i t

l 0

s t

T 

i

 t

t

s t

TG s t

In this section we rst detail a maximum likelihood procedure for tting (6) and then a method for forming
classications by combining (1) and (6) to form an estimate of the Bayes classier.

3.1 Fitting the model
Fitting the FLDA model involves estimating l 0

where

G and s 2. Notice that (6) implies

i

l 0

i

i j

Yi j N Si j

i j

s 2I

Si jG ST

i j

Since observations from different individuals are assumed to be independent, the joint distribution of the
observed curves is

mi

K(cid:213)

i

1

j

1

 2p

1
2

ni j

exp

1

2

i j

1
2  Yi j

Si j

l 0

TS

1

i j

i

A natural approach to tting the model is to maximize (8) over l 0
maximizing this likelihood is a difcult non-convex optimization problem. If the g

i

l 0

i

Si j

(8)

 Yi j
G and s 2. Unfortunately, directly
i j had been observed, how-




6







L
a

g

e














e



g

G





(cid:229)
a







m




L
a

w







L

a


L
a


S


S




(cid:213)




S







L
a






L
a



L

a

ever, the joint likelihood of Yi j and g

i j would simplify to

mi

K(cid:213)

i

1

j

1

exp

ni j

 2p

1
2s 2  Yi j

1
2s ni j

q

1

2

Si j

l 0

i

i j

T  Yi j

Si j

l 0

i

i j

g T
i j

1
2

1g

i j

Maximizing this likelihood is much less complex, and suggests treating the g
i j as missing data and imple-
menting the EM algorithm (Dempster et al., 1977; Laird and Ware, 1982). The EM algorithm involves al-
ternately calculating the expected value of the missing data g
i j and maximizing the joint likelihood. The E
step is performed using the equation

Yi

0

i

s 2

s 2G

1

ST
i jSi j

1ST
i j Yi j

Si jl 0

Si jL

i

E

i j

while the M step involves maximizing

Si j

l 0

i

i j

T  Yi j

Si j

l 0

s 2

i

i j

Q

1
2

mi

K(cid:229)

i

1

j

ni j log

E

1
s 2

 Yi j
g T
i j

log

1g

i j 



holding g i j xed. Further details can be obtained from the web site www-rcf.usc.edu/ gareth. As with all

EM algorithms the likelihood will increase at each iteration but it is possible to reach a local rather than global
maximum. This can be a problem for very sparse data sets such as the bone mineral density data. However,
the problem is generally eliminated by enforcing a rank constraint on G as discussed in 5.1.

Other model selection questions arise in practice, such as the choice of q, the dimension of the spline
basis. There are several possible procedures that have been applied to models of this type. One is to calculate
the cross-validated likelihood for various dimensions and choose the model corresponding to the maximum
(James et al., 2000). AIC and BIC are two other, less computationally expensive, procedures that have also
proved successful on this sort of data (Rice and Wu, 2000). In practice the nal classication appears to be
relatively robust to any reasonable choice of dimension but this is an area of ongoing research.

3.2 Classication

Notice that under the standard reduced-rank LDA model,
l 0

Class

X

i

i N

Hence, using Bayes formula, the probability of Class i given X is proportional to

l 0

 X

TS

i

l 0

1 X

2 logp

i

i

where
that classifying an observation X using reduced-rank LDA is identical to classifying to

i
. Note that the second line follows from the fact that L TS

1 X

L TS

l 0

X

X 

l 0

X
C X

2

1
2

X 

i

2 logp

2

1

i

2 logp

i

X

1L

I. This means

arg min

i

X

2

i

2 logp

i

7


(cid:213)





G







L
a

g





L
a

g



G


g

g

L

a

G










a





(cid:229)




L
a

g





L
a

g






G


G



L
a

S




L
a




L
a







L

a

S




L

a

L
a


S








a

a




a









a

a





It can be shown that
additive constant.

X and a

i are equal to the linear discriminants of X and m

i that LDA produces, up to an

The same approach is used for FLDA. By combining (1) and (6) we see that the posterior probability that

a curve Y was generated from Class i is proportional to
1 Y

 Y

l 0

SY

SY

TS

i

where SY is the spline basis matrix for Y and

Y

l 0

SY

SY

i

2 logp

i

So Y will be classied to

s 2I

G SY

T

SY

Y 

arg min

i

Y

l 0

SY

SY

2

i

2 logp

i

1

Y 

Notice, however, that if one lets

then

Y 

Y

l 0

SY

SY

Hence (9) is equivalent to

L T SY

TS

1
Y SY

1 L T SY

TS

1

Y  Y

l 0

SY

YY

l 0

SY

SY

2

i

1

Y 

SY

SY

Y

2

1

Y 

Y 

2

i

1

Y 

(9)

(10)

arg min

i

Y

i

2
L TSY

TS

1

ST
Y

Y

2 logp

i

arg min

i

Y

i

2
Cov

2 logp

i

1

(11)

since

Just as with standard LDA,
i.
Therefore (11) corresponds to classifying to the class whose mean is closest to our test point in the reduced
Y. Notice also that if Y has been measured
space where distance is measured using the inverse covariance of

Cov
i are, up to an additive constant, the linear discriminants of Y and m

Y and a

Y

L T SY

TS

1
Y SY

1

Y

over the entire time period, so that SY 

S, then

and (11) reduces to

4 Applications of FLDA

Cov

Y

I

arg min

i

Y

2

i

2 logp

i

In this section we show how three of the most important tools that LDA provides, namely, low dimensional
representation, discrimination functions and classication, can be replicated using FLDA.

4.1 Low dimensional representation of curves

One of the reasons for the popularity of LDA is that it provides the ability to view high-dimensional data by
projecting it onto a low-dimensional space. This allows one to visually determine the discrimination between

8


a


L
a



L
a



S







L
a


S




a


L










L
a


S





L

a

S



L

a

L
a


S





a

a




L








a

a




a




a



L




a

a

a





a

a





Female
Male

Female
Male



e
g
A
e
g
a
r
e
v
A

5
2

0
2

5
1

0
1



e
g
A
e
g
a
r
e
v
A

5
2

0
2

5
1

0
1

-150

-100

-50

0

50

100

150

0

20

40

60

80

Alpha

(a)

Standard Error

(b)

Figure 3: (a) Linear discriminants for each curve of the spinal bone density data, plotted against the average age
people were measured at. (b) Estimates of the standard error of the linear discriminants for each curve plotted against
the average age. There is a clear trend of increasing variability with age.

classes. As mentioned in 3.2, in a standard nite-dimensional setting the linear discriminant of X equals

L TS

l 0

1 X

up to an additive constant. As Cov
distance between different observations is Euclidean and can be easily calculated by visual inspection.

X 
I, the transformed variables all have identity covariance, so the

This provides a natural approach to projecting functional data into a low-dimensional space. In the FLDA
Y is the linear discriminant for Y and that if Y
I. However, if only fragments of the curve have been

Y, given in (10). Recall that

model the analogue of
has been observed over the entire interval Cov
observed,

X is

X

Y

L T SY

TS

1
Y SY

1

Cov

Y

This makes direct comparison of points more difcult because the covariance structure may no longer be
diagonal and, in general, curves measured at different time points will have different covariances. However,
if h
1, so the linear discriminant is a scalar, the only effect this has is that each point has a different standard
error. Figures 3-6 provide examples of this.

Figure 3(a) shows linear discriminants for each curve from the growth curve data of Figure 1, plotted
versus the average age for observations from each individual. For a two-class situation, such as this, the plot
also provides a simple classication rule; curves with positive linear discriminant are classied as male and
curves with negative linear discriminant as female. The plot reveals some interesting properties of the data.
Curves measured at ages below eighteen years are relatively well-separated while individuals measured at
older ages have little discernible separation. This trend is also apparent upon close examination of the original
curves. The two solid vertical lines either side of zero give the tted values for the a
is. They represent the
class centroids in the transformed space. Their close proximity to each other relative to the variability of
the linear discriminants indicate little overall separation between classes. However, recall that, as a result

9


a





a


a

a

a

a


a



L




Female
Male

Class 1
Class 2

r
o
r
r



E
d
r
a
d
n
a
S

t

0
8

0
6

0
4

0
2

0

r
o
r
r



E
d
r
a
d
n
a
S

t

0
1

8

6

4

2

0

-150

-100

-50

0

50

100

150

-20

-10

Alpha

(a)

10

20

0

Alpha

(b)

Figure 4: (a) Plot of the linear discriminants for each curve on the spinal bone density data versus the estimated
standard errors. (b) A linear discriminant plot for a simulated data set. It shows a fairly clear difference between the
classes but also a signicant overlap.

of (7), the linear discriminant of Y will have a standard error of one if the curve is measured over the entire
interval. In fact the separation between the centroids is about 23
5 standard deviations. This implies that
the confusion between genders is a result of the small number of observations per individual, which cause
the standard error to increase dramatically. A clear separation could be achieved with more observations.
Figure 3(b) gives the standard error for each linear discriminant versus average age of observation. A curve
with measurements over the entire time period would have standard error 1, so this plot gives an indication of
the amount of information lost by only observing the curve at a limited number of time points. The standard
errors range from over ten to eighty, indicating that a great deal of accuracy has been sacriced. The increased
variability also explains the poor separation at older ages where the standard errors are large relative to the
distance between the class centroids. Once the model has been t, the standard error can be calculated for a
curve observed at an arbitrary set of time points. This provides a method for deciding on an optimal design
in terms of locating a nite number of observations for an individual to minimize the standard error.

Figure 4 gives two plots which combine the linear discriminants and their standard errors together. This
gives an easy method for deciding on the reliability of a given observation. For example points with high
standard error should be treated with caution. We call these linear discriminant plots. The left and right
vertical dotted lines indicate the class centroids while the center lines are the class discriminators. Figure 4(a)
shows that the points with relatively low standard error have far better separation than those with a large
standard error. Figure 4(b) provides a similar plot for a simulated data set consisting of 80 curves measured
at the same set of time points as that of the data set illustrated in Figure 2. Notice that even though each curve
has been measured at fairly evenly spaced points throughout the time interval the standard errors still range
up to ten. The two classes are relatively well separated but there is still some clear overlap. The distance
between the class centroids is 5 standard deviations, indicating that one could achieve near perfect separation
by sampling the curves at a wider range of time points.

Figures 5 and 6 are further linear discriminant plots. They were produced by using ethnicity as the class

10


Black
Hispanic
White
Asian



e
g
A
e
g
a
r
e
v
A

5
2

0
2

5
1

0
1

Black
Hispanic
White
Asian



e
g
A
e
g
a
r
e
v
A

5
2

0
2

5
1

0
1

-100

-50

0

Alpha

(a)

50

-100

-50

50

0

Alpha

(b)

Figure 5: Linear discriminants for the spinal bone density data using ethnicity as the class variable. (a) The dotted
lines represent, from left to right, class centroids for Blacks, Hispanics, Whites, and Asians. Notice that Blacks and
Asians are fairly well separated while Hispanics and Whites are not. (b) The solid lines represent decision boundaries
for classifying a given curve.

variable on a subset of the growth curve data, all females. A plot of the growth curves for each ethnicity
(not shown) indicates that there may be no clear separation between classes. This is borne out by Figure 5(a)
which gives a plot of linear discriminant versus average age. There is signicant overlap between the classes.
However, it is still possible to gain some information. The four vertical dotted lines represent the class cen-
troids for, from left to right, Blacks, Hispanics, Whites and Asians. It is clear that there is very little separation
between Hispanics and Whites while Blacks and Asians are relatively well separated. This is highlighted by
Figure 6 which shows linear discriminants and a plot of the raw data for Blacks and Asians alone. The differ-
ences are now clear. Figure 5(b) is identical to 5(a) except that the three discrimination boundaries are plotted
in place of the class centroids. The discrimination boundaries divide the space into four regions. Points in
the leftmost region are classied as Black, the next as Hispanic, followed by White and nally Asian.

4.2 Classication

The ability of LDA to perform classication is of equal importance to its ability to explain discrimination
Y using
between classes. In 3.2 we showed that to classify a curve using FLDA one need only produce
(10) and classify using (11). When all classes have equal weight and
Y is one dimensional this classication
rule simplies to

arg min

i

Y

2

i

(12)

While classication is not the primary goal on the spinal bone density data, we apply (12) to it to illustrate
3%.
the procedure. When using gender as the class variable the overall training error rate comes out at 29
However, the rate increases substantially to 44
0% for ages under
18. This conforms to our expectations from Figure 3(a) which shows much better discrimination for lower

1% for ages over 18 and decreases to 22

11


a

a


a

a




Black
Asian



e
g
A
e
g
a
r
e
v
A

5
2

0
2

5
1

0
1

Black
Asian

y
t
i
s
n
e
D
e
n
o
B




l

i

a
n
p
S

4
1

.

2
1

.

0
1

.

8
0

.

6
0

.

-100

-50

0

Alpha

(a)

50

10

15

20

25

Age

(b)

Figure 6: (a) Linear discriminants for Blacks and Asians. While there is still some overlap the separation is far clearer.
The vertical line gives the classication boundary. (b) A plot of the raw data for Blacks and Asians. Notice that Blacks
tend to have a higher bone density. The two solid lines represent the means for Blacks and Asians while the dotted lines
are for Hispanics and Whites.

ages.

Table 1 gives the confusion matrix when ethnicity is used as the class variable. It shows the true ethnicity
and the corresponding classication for each of the 153 individuals. For example, 22 of the 35 Asians were
classied as Asian while 10 of the 27 Hispanics were classied as Black. While the overall training error
rate is 56
9%, a large fraction of the errors are among Hispanics and Whites, while Asians and Blacks are
relatively well classied. When Asians and Blacks are considered alone the error rate drops to 25%.

In Table 2 we present the results from a simulation study where the FLDA procedure is compared with
two other classiers. The rst is the ltering method of 1.1. Recall that the ltering method consists of
tting a exible basis, in this case cubic splines, to each curve and then classifying by using LDA on the basis
coefcients. The ltering method provides a simple comparison to FLDA. The second is the Bayes classier
which is optimal if the true distribution of the classes is known. It provides the best case error rate. The

Prediction

True Ethnicity

Asian
Black

Hispanic
White
Total

Asian
9
22 62
0
7 20
1 2
9
5 14
3
35 100

Black
9
9 20
8
30 69
2 4
7
7
2 4
43 100

Hispanic White
6
19 39
8 30
0
1
13 27
10 37
0
7 14
5 18
6
5
9 18
4 14
8
8
27 100
48 100

Total
58
60
15
20
153

Table 1: Confusion matrix of classications for the four ethnicities. The numbers in parentheses give the percentages
of each ethnicity receiving the corresponding classication. Asians and Blacks have relatively little confusion while
Hispanics and Whites have a great deal.

12






































% Missing

50
80
84
90
94

Filtering
0 0
5 0
8 0
8 1
4 1

4
5
9
1

9
18
40
60

FLDA
0 0
6 0
6 0
2 0
1 0

2
3
3
5

8
12
17
28

Bayes

0
3
8
7
12
16
7
7
26

Table 2: Test error rates from the simulation study for various different fractions of missing data. The numbers in
parentheses indicate estimated standard errors for the error rates.

study consisted of a three-class problem. For each class, 50 curves were generated according to the FLDA
model (6) and sampled on a ne grid of 100 equally-spaced points. Then, for each curve, a random subset,
50-94%, of the observations, were removed to replicate curve fragments. Multiple data sets were created
and the FLDA and ltering procedures were applied to each. Error rates were then calculated on a separate,
test set, of 300 curves. Furthermore the Bayes error rate, which is the lowest possible, was also calculated
on this test set. Over the 100 time points the average deviation of mean curves between Classes 1 and 2 and
066 while it was twice this number between Classes 1 and 3. The standard
between Classes 2 and 3 was 0
deviation of the error terms was s
1. Finally the average standard deviation over the 100 time points of
the random curves Sg was approximately 0
368. The rst gure gives a guide as to the signal while the last
two indicate the noise.

0

Table 2 provides a summary of the test error rates. As one would expect, all three sets of error rates
increase with the fraction of missing data. Of far more interest is the similarity between the FLDA and Bayes
error rates. Even with 90% of the data removed the difference is only 0
5%. With less than 80% of the data
removed the ltering and FLDA methods give comparable results. However, the ltering method deteriorates
rapidly until at 94% its error rate is close to that of the naive classier which randomly assigns a class label
based on the prior probability for each class. Note that at 94% the ltering method could not even be applied
to several of the simulated data sets because individual curves could not be tted.

4.3 Class discrimination functions

In a standard two-class LDA setting the discriminant function is dened as

TS

1

1

2

l 0
S
TL T STS

i 
a 2

i

1

where S
determining the classication of a point. In the FLDA setting m

is the within group covariance matrix. This function gives the weight put on each dimension in
so the functional analogue is

 SL

SL

TS

2

1

a 1

1

s 2I

(13)
where S
SG ST and S is the spline basis matrix evaluated on a ne grid of points over the entire time
period. Equation (13) can be used to produce a discriminant function for any set of data. Figure 7 provides
examples from the growth curve data. Figure 7(a) gives the discriminant function using gender as the class
variable. There is a strong negative peak before age 15 and a large positive peak afterwards; this indicates a
phase shift between genders and explains why there is far better separation for the earlier ages. Figure 7(b)
gives a similar plot using ethnicity as the class variable. Again most of the discrimination appears to be in
the early years.

A comparison of discriminant functions produced from the simulation study of 4.2, using both the FLDA
and ltering approaches, is given in Figure 8. The population discriminant function is shown in black while

13





































m

m


L
a

a

a









n
o

i
t
c
n
u
F
n
o



i
t

a
n
m

i

i
r
c
s
D

i

4

2

0

2
-

4
-

6
-

8
-

n
o

i
t
c
n
u
F
n
o



i
t

a
n
m

i

i
r
c
s
D

i

5
0

.

0

0

.

0

5
0

.

0
-

0
1

.

0
-

5
1

.

0
-

10

15

20

25

10

15

20

25

Age

(a)

Age

(b)

Figure 7: Discriminant functions for the growth curve data of Figure 1. (a) Using gender as the class variable. There
is a strong indication of phase shift. (b) Using ethnicity as the class variable.

its estimates are in grey. Figures 8(a) and (b) present results from the study with 84% of the data removed
while Figures 8(c) and (d) present results with 90% removed. For each, the top plot shows the discriminant
functions from 40 different simulations using FLDA, while the bottom plot gives the corresponding graph
for the ltering approach. As adding or multiplying the discriminant function by a constant leaves the clas-
sication unchanged, the estimates have been transformed to produce the least squares t to the population
discriminant function. It is clear that in both cases the FLDA approach is producing more accurate discrim-
inant functions, reected in the decreased error rates of Table 2.

5 Extensions

Under the standard FLDA model

Cov Yi j

s 2I

Si jG ST

i j

(14)

In this section we consider a number of possible extensions to this model by exploring different assumptions
for G and hence the covariance matrix of Yi j.

5.1 Reduced Rank Covariance Matrices
Under (14), no restrictions are placed on the structure of G
. In practice, the likelihood function for data sets
such as the spinal bone density data has a large number of local maxima which make this model difcult to t.
As a result, for even moderate q, one can produce a highly variable t to the data. James et al. (2000) show
that such problems can be reduced by enforcing a rank constraint on G
. A rank p constraint is equivalent to
setting G

Q DQ T and

Cov Yi j

Si jQ DQ TST

i j

s 2I

14









(a)

FLDA Discriminant Functions

(c)

FLDA Discriminant Functions

8
0

.

0

4
0

.

0

0

.

0

0

20

40

60

80

100

0

20

40

60

80

100

(b)

Filtering Discriminant Functions

(d)

Filtering Discriminant Functions

8
0

.

0

4
0
0

.

0

.

0

8
0

.

0

4
0

.

0

0

.

0

8
0

.

0

4
0
0

.

0

.

0

0

20

40

60

80

100

0

20

40

60

80

100

Figure 8: Results from two different sets of simulations. Plots (a) and (b) show the true discriminant function (black
line) and estimates (grey lines) from 40 different simulations using FLDA (a) and ltering (b) methods with 84% of
each curve unobserved. Plots (c) and (d) show the equivalent results with 90% of each curve unobserved.

where Q

is a q

p (p

q) matrix and D is a diagonal matrix.

James et al. (2000) suggest several methods for choosing the rank and conclude that the optimal rank for
2

2. The results from Section 4 were produced using such a reduced rank model with p

these data is p
because this gave a far better t to the data.

5.2 Functional Quadratic Discriminant Analysis
The FLDA procedure, in analogy with LDA, makes an assumption of a common covariance matrix, G
, for
the g
i j vectors from each class. This can result in a considerable reduction in variance for classes with a small
sample size. However, the assumption can cause a considerable increase in bias if the covariances are not
common. In a standard setting quadratic discriminant analysis (QDA) provides a less restrictive procedure
by allowing different covariance matrices. In a similar manner the FLDA model of 2.2 can be generalized
by removing the assumption of a common covariance term, G
Si jG

. This gives the covariance structure

s 2I

iST
i j

Cov Yi j

where G

where

i is the covariance matrix for Class i. The posterior probability is now proportional to

di Y



Y

l 0

SY

SY

2

i

ln

1

iY 

1

iY 

2 logp

i

s 2I
By tting this model and classifying to arg mini di Y
quadratic discriminant analysis (FQDA) is produced.

iY 

T

iSY

SY
a generalization of QDA, which we call functional

15













L
a


S


S



S

G

5.3 Functional Regularized Discriminant Analysis

It is well known that LDA can perform badly if the assumption of a common within-class covariance matrix is
violated, while QDA generally requires a larger sample size (Wald and Kronmal, 1977). A small sample size
causes a covariance matrix to be produced which is close to singular and hence excessive weight is placed
on the directions corresponding to small eigenvalues. Regularization has been highly successful in this sort
of poorly-posed inverse problem (Titterington (1985), OSullivan (1985)). Friedman (1989) suggests the
following regularization approach. Let Si
ni be the within class sample covariance matrix for class i and let
ni is used for QDA.
S
A compromise between the two approaches can be achieved by setting the within-class covariance matrix
equal to

n is the pooled covariance matrix which is used for LDA while Si

1 Si. Then S

(cid:229) K
i

where

G

Si w1

i w1
ni w1
w1 S and ni w1

Si w1

 1

Si

w1

ni

w1n

 1

w1

A second level of regularization, namely shrinkage towards the identity matrix, is provided through

G

i w1

w2
where q is the dimension of the space. G
is used as the within-class covariance matrix for the ith class.
i w1
Friedman calls this approach regularized discriminant analysis (RDA). RDA has been shown to outperform
both LDA and QDA in a large variety of situations.

 1
w2

i w1

i w1

w2

(15)

I

A generalization to functional regularized discriminant analysis (FRDA) can be achieved using the fol-

G

G

tr

w2
q

lowing covariance structure

Cov Yi j

s 2I

Si j G

ST
i j

i w1

w2

w2

i w1

where G
is dened as in (15). The choice of w1 and w2 is made using cross-validation. Applying
cross-validation to the FRDA model is potentially computationally expensive. However, in the RDA setting,
an algebraic update allows for a signicantly faster implementation (Friedman, 1989). This update can be
used in the FLDA setting by tting the FQDA model, treating the resulting g
i js as q dimensional data and
tting RDA to estimate w1 and w2, and nally tting the FRDA model with w1 and w2 xed.

FRDA contains both FLDA and FQDA as sub models. By setting both w1 and w2 equal to zero the FQDA
0 produces the FLDA model. Furthermore, by setting
1 a functional generalization of the nearest-means classier is produced where an observation is

1 and w2 

model is produced. While setting w1 
w1 

w2 

assigned to the closest class mean, in Euclidean distance.

6 Conclusion

We have presented a method, which we call functional linear discriminant analysis (FLDA), for generaliz-
ing linear discriminant analysis to functional data. FLDA possesses all the usual LDA tools, including a
low-dimensional graphical summary of the data, and classication of new curves. When the functional data
have been measured over a large number of time points the procedure provides similar results to the ltering
method introduced in Section 1.1. However, when only fragments of the function are available the FLDA
approach can still produce favorable outcomes while the ltering method fails completely. FLDA can also be
generalized in a number of ways. A reduced rank version can be implemented when the data are very sparse
and a quadratic version can be used when an assumption of a common covariance matrix is inappropriate. A
regularized version, which is a compromise between FLDA and FQDA, is also available.

16




























Another possible generalization, which we have not considered, is to model heterogeneous variance and
autocorrelation in the error terms. This may well improve the classication accuracy of the method provided
enough time points have been observed per curve to provide accurate estimates. Unfortunately, allowing s 2
to vary would make it impossible to enforce the constraint,

L T STS

1SL

I

(16)

I. In turn (16) allows gures such as
which ensures that, if a curve is measured at all time points, Cov
3(b) to provide a measure of the amount of information lost through missing observations. If s 2 was allowed
to vary this interpretation would no longer be feasible.



a Y

Acknowledgments

The authors would like to thank the Editor and referees for many constructive suggestions. Trevor Hastie was
partially supported by grants from the National Science Foundation and the National Institutes of Health.

