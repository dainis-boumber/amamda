Abstract

We propose a novel method for using the World Wide Web to acquire trigram estimates for statistical lan-
guage modeling. We submit an N-gram as a phrase query to web search engines. The search engines return
the number of web pages containing the phrase, from which the N-gram count is estimated. The N-gram
counts are then used to form web-based trigram probability estimates. We discuss the properties of such
estimates, and methods to interpolate them with traditional corpus based trigram estimates. We show that
the interpolated models improve speech recognition word error rate significantly over a small test set.

DISTRIBUTION STATEMENT A

Approved for Public Release A-AAJAA^    A 0 7
P Distribution Unlimited 20001207    023

Keywords: Language models, Speech recognition and synthesis, Web-based services

1 Introduction

A language model is a critical component for many applications, including speech recognition. Enormous
effort has been spent on building and improving language models. Broadly speaking, this effort develops
along two orthogonal directions: The first direction is to apply increasingly sophisticated estimation methods
to a fixed training data set (corpus) to achieve better estimation. Examples include various interpolation and
backoff schemes for smoothing, variable length N-grams, vocabulary clustering, decision trees, probabilistic
context free grammar, maximum entropy models, etc [1]. We can view these methods as trying to "squeeze
out" more benefit from a fixed corpus. The second direction is to acquire more training data. However,
automatically collecting and incoiporating new training data is non-trivial, and there has been relatively
little research in this direction. Adaptive models are examples of the second direction. For instance, a cache
language model uses recent utterances as additional training data to create better N-gram estimates. The
recent rapid development of the World Wide Web (WWW) makes it an extremely large and valuable data
source. Just-in-time language modeling [2] submits previous user utterances as queries to WWW search
engines, and uses the retrieved web pages as unigram adaptation data. In this paper, we propose a novel
method for using the WWW and its search engines to derive additional training data for N-gram language
modeling, and show significant improvements in terms of speech recognition word error rate.

The rest of the paper is organized as follows. Section 2 gives the outline of our method, and discusses
the relevant properties of the WWW and search engines. Section 3 investigates the problem of combining
a traditional corpus with data from the web. Section 4 presents our experimental results. Finally Section 5
discusses both the potential and the limitations of our proposed method, and lists some possible extensions.

2 The WWW as trigram training data

The basic problem in trigram language modeling is to estimate 2>{w3\wiiw2)> i-e- the probability of a word
given the two words preceding it. This is typically done by smoothing the maximum likelihood estimate

PML{W3\WI,W2)  -

c(w1w2)

with various methods, where c(wiio2ws) and c(wiW2) are the counts of ,7iviw2w^ and "wiu^" in some
training data respectively. The main idea behind our method is to obtain the counts of nwiW2W3n and
"wiw2" as they appear on the WWW, to estimate

-      /     , x       cweb(wiw2w3)
|W>(W3|'1,U'2) =

Cweb{WiW2)

and combine pweb with the estimates from a traditional corpus (here and elsewhere, when cweb (wiw2) = 0,
we regard pweb(w2,\wi, w2) as unavailable). Essentially, we are using the searchable web as additional
training data for trigram language modeling.

There are several questions to be addressed. First, how to obtain the counts from the web? What is the
quality of these web estimates? How could they be used to improve language modeling? We will examine
these questions in the following sections, in the context of N-best list rescoring for speech recognition.

2.1    Obtaining N-gram counts from the WWW

v from the web, we use the 'exact phrase search' function
To obtain the count of an N-gram " w\... wn
v as a single quoted phrase query to a search engine.
of web search engines. That is, we send " w\... wn
Ideally, we would like the search engine to report the phrase count, i.e. the total number of occurrences of

1

the phrase in all its indexed web pages. However in practice, most search engines only report the web page
count, i.e. the number of web pages containing the phrase. Since one web page may contain one or more
occurrence of the phrase, we need to estimate the phrase count from the web page count.

Many web search engines claim they can perform exact phrase search. However, most of them seem to
use an internal stop word list to remove common words from a query phrase. An interesting test phrase is
"to be or not to be": Some search engines return totally irrelevant web pages for this query, since most, if
not all, words are ignored. In addition, a few search engines perform stemming so the query "she say" will
return some web pages only containing "she says" or "she said". Furthermore, some search engines report
neither phrase counts nor web page counts. We experimented with a dozen popular search engines, and
found three that meet our criteria: AltaVista [3] advanced search mode, Lycos [4], and FAST [5] ]. They all
report web page counts.

One brate force method to get the phrase counts is to actually download all the web pages the search
engine finds. However, queries of common words typically result in tens of thousands of web pages, and
this method is clearly infeasible. Fortunately at the time of our experiment AltaVista had a simple search
mode, which reported both the phrase count and the web page count for a query. Figure 1 shows the phrase
count vs. web page count for 1200 queries. Trigram queries (phrases consisting of three consecutive words),
bigram queries and unigram queries are plotted separately. There are horizontal branches in the bigram and
trigram plots that don't make sense (more web pages than total phrase counts). We regard these as outliers
due to idiosyncrasies of the search engine, and exclude them from further consideration. The three plots are
largely log-linear. This prompted us to perform the following log-linear regression separately for trigrams,
bigrams, and unigrams:

c = Q0 * pg'M

where c is the phrase count, and pg the web page count. Table 1 lists the coefficients. The three
regression functions are also plotted in Figure 1. We assume these functions apply to other search engines as
well. In the rest of the paper, all web N-gram counts are estimated by applying the corresponding regression
function to the web page counts reported by search engines.

10

10

J
    Unigram          /

c
3 o
     s
0>  10
CO

10

10

/

10

10 10" 10"

webpage number returned by search engines

Figure 1: Web phrase count vs. web page count

'Our selection is admittedly incomplete. In addition, since search engines develop and change rapidly, all our comments are

only valid during the period of this experiment.

'O

Unigram  2.427
Bigram
1.209
Trigram
1.174

Ql
1.019
1.014
1.025

Table 1: Coefficients of log-linear regression for estimating Web N-gram counts from Web page counts
reported by search engines.

2.2   The quality of web estimates

To investigate the quality of web estimates, we needed a baseline corpus for comparison. The baseline we
used is a 103 million word Broadcast News corpus.

2.2.1   Web N-gram coverage

The first experiment we ran was N-gram coverage test on unseen text. That is, we wanted to see how many
N-grams in the test text are not on the web, and/or not in the baseline corpus. We were hoping to show
that the web covers many more N-grams than the baseline corpus. Note that by 'the web' we mean the
searchable portion of the web as indexed by the search engines we chose.

The unseen news test text consisted of 24 randomly chosen sentences from 4 web news sources (CNN,
ABC, Fox, BBC) and 6 categories (world, domestic, technology, health, entertainment, politics). All the
sentences were selected from the day's news stories, on the day the experiment was carried out. This was
to make sure that the search engines hadn't had the time to index the web pages containing these sentences.
After the experiment was completed, we checked each sentence, and indeed none of them were found by the
search engines yet. Therefore the test text is truly unseen to both the web search engines and the baseline
corpus. (The test text is of written news style, which might be slightly different from the broadcast news
style in the baseline corpus.)

There are 327 unigram types (i.e. unique words), 462 bigram types and 453 trigram types in the test
text. Table 2 lists the number of N-gram types not covered by the different search engines and the baseline
corpus, respectively.

Unique Types

Not Covered By

Unigram
Bigram
Trigram

327
462
453

AltaVista  Lycos  FAST  Corpus

0
4
46

0
5
46

0
5
46

8
68
189

Table 2: Novel N-gram types in 24 news sentences

Clearly, the web's coverage, under any of the search engines, is much better than that of the baseline
corpus. It is also worth noting that for this test text, any N-gram not covered by the web was also not covered
by the baseline corpus.

In the next experiment, we focused on the trigrams in the test text to answer the question "if one ran-
domly picks a trigram from the test text, what's the chance the trigram has appeared c times in the training
data?" Figure 2 shows the comparison, with the training data being the baseline corpus and the web through

I baseline corpus

AltaVista

'} Lycos
I FAST

JJJJlJlJ..J.aiJ._.iJj.-_l. ,^_8
0 2 4 6 8 10 12 14 16 18 20

frequency c

Figure 2: Empirical frequency-frequency plot

the different search engines, respectively. This figure is also known as a "frequency-of-frequency" plot. Ac-
cording to this figure, a trigram from the test text has more than 40% chance of being absent in the baseline
corpus, and the chance goes down to about 10% on the web, regardless of the search engine. This is consis-
tent with Table 2. Moreover, the trigram has a much larger chance in having a small count in the baseline
corpus than on the web. Since small counts usually mean unreliable estimates, resorting to the web could be
beneficial.

2.2.2    The effective size of the web

Recently, Fienberg et al. [6] estimated the size of the indexable web as of 1997 to be close to 1 billion pages.
The web grows exponentially, and as of this writing some search engines claim they have indexed more than
1 billion pages. We would like to estimate the effective size of the web as a language model training corpus.
Let's assume that the web and the baseline corpus are homogeneous (which is patently false, since
the web has much more than news, but we will ignore this for the time being). Then the probability of a
particular N-gram appearing in the baseline corpus is the same as the probability that it appears on the web:

Since the probabilities can be approximated by their respective frequencies, we have

?Wpus(N-gram) = pW(b{ N-gram)

Ccorp us(N-gram) _ cwcb (N-gram

| corpus |

| web |

, from which we can estimate |web|, the size of the web in words. Note that it doesn't matter if the N-gram is
a unigram, bigram or trigram, though N-grams with small counts are unreliable and should be excluded. In
our experiment, we considered all unigrams, bigrams and trigrams in the test text with ccorpus > 10. Each
such N-gram will gave us an estimate, and we took the median of all these estimates for robustness. Table 3
gives our estimates of the size with different search engines.

Some points to notice:

1. The 'effective web size' estimates we obtained are very rough at best. Moreover, they are defined
relative to the specific baseline corpus and specific test set we happened to choose. Therefore, Table 3
should not be used to rank the performance of individual search engines.

Effective size of the web

AltaVista
Lycos
FAST

108 billion words
79 billion words
83 billion words

Table 3: The effective size of the web for language model training

2. This method tends to underestimate the web size. We assumed homogeneity, which in actuality does
not hold. The test text comes from a news domain, and so does the baseline corpus. We used N-
grams from the test text to estimate the web size, which gives rise to a selectional bias. Intuitively,
only "news terms" are in the test text. And since the corpus is in news domain, as a whole we have
Pcorpus(news terms) > p,e;,(news terms). This is what leads to underestimation.

2.2.3 Normalization of the web counts

An interesting sanity check is to see whether

Cweb{lV1W2) =    Yl   cu>eb{w1U>2W3)

U-SGV

holds for any bigram "wiw2"-  If this is true, the relative frequency estimation plueb{'m\ti, w2) would
already be normalized, i.e.

X]   Pweb(w3\w1,W2) = l,Vwi,W2

Of course there are too many "wiw2'3" combinations to verify this directly. Instead, we randomly
chose six "io1w2" pairs from the baseline corpus. For each pair, we chose 2000 w3's according to the fol-
lowing heuristic: First, we selected words from a list of all w3's such that the trigram " wi w2 w3" appeared
in the baseline corpus, sorted by decreasing frequency; If fewer than 2000 words were chosen that way, we
added words from a list of all w3's such that the bigram " w2 w3" appeared in the baseline corpus, in decreas-
ing frequency order; If this was still not enough, we added w3's according to their unigram frequencies. We
expected this heuristic to give us a list of w3's that covers the majority of the conditional probability mass
given history " wi w2".

Table 4 shows web bigram count estimates obtained with FAST search, together with their respective
cumulative web trigram count estimates as described above. Ideally, the ratio should be close to, but less
than, 100%. It is evident from the table that the web counts are not perfectly normalized. The reasons are
not entirely clear to us, but the fact that the N-gram counts are estimated from page counts is an obvious
candidate. The web N-gram count estimates should therefore be used with caution.

2.2.4 The variance and bias of web trigram estimates

As stated earlier, we are interested in estimating conditional trigram probabilities based on their relative
frequency on the web:

Pweb(W3\WUW2)

CWeb{WiW2W3)

(W1W2

It would be informative to compare pweb(w3\wi, w2) to a traditional (corpus derived) trigram probability
estimate.

"u'j w2'

about seventy
and there's
group being
lewinsky after
two hundred

willy b.

<Wfc('Y'2)  E2000Vsc"&('M'iw'2w'3)
14807.7
724870.0
16246.5
1631.7
457656.0
607.2

16498.3
662697.0
20248.4
1431.9
389949.0
1334.6

ratio
90%
109%
80%
114%
117%
45%

Table 4: Sanity check: are web counts normalized?

+    AltaVistn
O   Lycos
     FAST



 -

6<   >
10     -

10* i '

102                          a

10"*"

!!                                                      *

10J -

10 100 1000 1OOO0 1OO0O0

count of www  in baseline corpus

Figure 3: Ratio of web trigram estimates to corpus trigram estimates

To this end, we created a baseline trigram language model LJ\I0 from the 103 million word baseline cor-
pus. We used modified Kneser-Ney smoothing [7] [8] which, according to [8], is one of the best smoothing
methods available. In building LM0, we discarded all singleton trigrams in the baseline corpus, a common
practice to reduce language model size. We denote LMQ'S probability estimates by JJQ.

With LM0, we were able to compare p,lf6('3|'i^ '2) with 7^0('31''ii "'2)- We computed the ratio r:

/'   U'j,-2>'3    =

pu,ct(w3|M'i,tf'2)

Po('3|''l.'"'2)

between these two estimates. We expected r to be more spread out (having larger variance) when ccorpus (wi w2 '"3)
is small, since in this case poiu^wi, '2) tends to be unreliable.

We computed r(wi, w2, u'3) for every trigram in the test text, excluding those with cu,ci,(wiW2) = 0.

We plot r(wi, w-2, '3) vs. ccorpUs(wiiC2W3) in Figure 3. We found that:

1. For trigrams with large cCorpu.s('iW'2W'3). ?' averages to about 1. Thus the web estimates are consistent

with LMo in this case.

2. As we expected, the variance of r is largest when corpus(!<'i'2'3)=0, and decreases when it gets

large. Hence the 'funnel' shape.

3. When ccor7,us(u>iu>2*"3) is small, especially 0 and 1, r is biased upward. This is of course good news,

as it suggests that this is where the web estimates tend to improve on the corpus estimates.

4. All the search engines give similar results.

3   Combining web estimates with existing language model

In the previous section, we saw the potential of the web: it is huge, it has better trigram coverage, and its
trigram estimates are largely consistent with the corpus-based estimates. Nevertheless, to query each and
every N-gram on the web is infeasible. This prevents us from building a full fledged language model from
the web via search engines. More over, Table 4 indicates that web estimates are not well normalized. In
addition, the content of the web is heterogeneous and usually doesn't coincide with our domain of interest.
Based on these considerations, we decided not to try to build an entire language model from the web. Rather,
we will start from a traditional language model LM0, and interpolate its least reliable trigram estimates with
the appropriate estimates from the web.

Unreliable trigram estimates, especially those involving backing off to lower order N-grams, have been
shown to be correlated with increased speech recognition errors [9] [10]. By going to the much larger web
for reliable estimates, Our hope was to alleviate this problem. We used the trigram counts in the baseline
corpus as a heuristic to decide the reliability of trigram estimates in LM0 . A trigram estimate po{w31 ui, w2)
is deemed unreliable, if

CWpus(u'l'2'3)  < T

where r, the 'reliability threshold', is a predetermined small positive integer, e.g. 1. Admittedly this defini-
tion of unreliable estimates is biased.

Even with this definition, there are still too many unreliable trigram estimates to query the web for. Since
we were interested in N-best list rescoring, we further restricted the queries to those unreliable trigrams that
appeared in the particular N-best list being processed. This greatly reduces the number of web queries at the
price of some further bias. Let UWl W2 be the set of words that have unreliable trigram estimates with history
"j1w2"in the current N-best list, i.e.

UWlW2 = {w3\n wiw2w3

n G N-best A ccorpus(w1w2i3) < r}

V"wiw2"  N-best A cweb(w1w2) > 0, (1)

We obtain cweb(wi, w2, u),u G UWlW2 and cweb(wiw2) via search engines, and compute pweb(u\wi, w2),
the web relative frequency estimates, from these web counts.

Letj)*(u\wi, w2) denote the final interpolated estimates, which combine p0(u\wi, w2) sxidpweb{u\wu w2)
We would like to have a tunable parameter so that on one extreme p*(u\wi, w2)  po(u\wi, w2), while on
the other extreme p*(u\wi, w2) > pweb{u\wi, w2). We now present three different methods for doing this.

3.1    Exponential Models with Gaussian Priors

We define a set of binary functions, or 'features', as follows:

, ,     , _ J   1    if u = ID3
UuW2,u^3)- |   0     otherwise

for all u?i, w2, u G UWlW2 in the N-best list. Next, for any given wi, w2, we define a conditional exponential
model px

E with these features:

p*E{w3\Wl,w2)= (2)

z^Po{w3\wllW2) exp(e[/u,iu,2 Au/u,li(2iU(u>3))

where p0 is the estimate provided by LM0, A's are parameters to be optimized, and ZWl ,, is a normalization
factor. This model has exactly the same form as a conventional Maximum Entropy / Minimum Discrim-
inative Information (ME/MDI) model [11] [12]. Let A denote the set of parameters. If we maximize the
likelihood of the web counts:

i(A) = n ^(u'3i,

1,'2)r,"r',^''iw''2u'^

II'I .U'2,U'P,

with the standard Generalized Iterative Scaling algorithm (GIS) [13], we get the ME/MDI solution that
satisfies the following constraints:

'e> '

7^(|'i,'2)    =    P,rrb(ii\iri,ir2) (3)

fi(.('l, M'2, W)

Cweb{ll'l,W-2)

-ywi.w2.it e f'*.]l(,2

This corresponds to one extreme of the interpolation. But since we want to control the degree of interpola-
tion, we introduce a Gaussian prior with mean 0 and variance a2 over A:

And instead of seeking the maximum likelihood solution, we seek the maximum a posteriori (MAP) solution
that maximizes

L{A)*p(A)

This can be done by slightly modifying the GIS algorithm, as described in [14].

With this Gaussian prior, we can control the degree of interpolation by choosing the value of a2 
(0,+oo). a2 acts as a tuning parameter: If a2 -> +oo, the Gaussian prior is flat and has virtually no
restriction on the values of the A's. Thus the A's can reach their ME/MDI solutions, and hence pE reaches
one extreme as in (3). On the other hand if a2 -> 0, the Gaussian prior forces A's to be close to the
mean, which is 0. From (2) we know in this case p*E > p0. This corresponds to the other extreme of the
interpolation. A a2 between 0 and +oo results in an intermediate p*E distribution.

For the purpose of comparison, we experimented with two other interpolation methods, which are easy
to implement but may be theoretically less well motivated: linear interpolation and geometric interpolation.

3.2   Linear Interpolation

In linear interpolation, we have

PL('3|W'I,'2) = (4)

(1 - a)po{u^\wuw2) + aPwfb(w3\wi,w2)
, if '3 e umu,2

-r-* ^ r-; rVn   !3 U'I , w2
1-Dueir.lu.2

Po(ul,"1"U'i>

, otherwise

In this case, a  [0,1] is the tuning parameter. If a = 0, pi = p0. If a = 1, p*L satisfies (3). An o in
between results in an intermediate p*L.

3.3   Geometrie Interpolation

In geometric interpolation, we have

PG(W3\W1IWV

7>0(U'3|'l, W'2

|(l-/3)

Cweb{WiW2) + \V\c_

(5)

l-T
1-Eerr Po(|'l,M'2)

p*G(u\wi,iv2)

,ifw3 e uunW2

Po(w3\wi,w2)

, otherwise

Note that here we have to smooth the web estimates to avoid zeros (which is not a problem in the previous
two methods). To do this, we simply add a small positive value e to the web counts. This is known as
additive smoothing [8]. The value of e is determined to minimize the perplexity with  = 1. Once e is
chosen it is fixed, and we tune .  e [0,1] is the interpolation parameter. If  = 0, pG = po- If  = 1, PG
satisfies the smoothed web estimates. A  in between results in an intermediate pG.

4   Experimental Result

We randomly selected 200 utterance segments from the TREC-7 Spoken Document Retrieval track data [15]
as our test set for this experiment. For each utterance we have its correct transcript and an N-best list with
N = 1000, i.e. 1000 decoding hypotheses. We performed N-best list rescoring to measure the word error
rate (WER) improvement, and computed the perplexity of the transcript. Note that the test set is relatively
small and N = 1000 is not very deep, since we wanted to limit the number of web queries to within a
practical range.

4.1   Word Error Rate

If we rescore the N-best lists with LM0 and pick the top hypotheses, the WER is 33.45%. This is our
baseline WER. The oracle WER, i.e. if we were able to pick the least errorful hypothesis among the 1000
for each N-best list, is 25.26%. Of course we cannot achieve the oracle WER, but it indicates there is room
for improvement over LMQ.

Since each utterance has 1000 hypotheses in the N-best list, the total number of trigrams is very large.
Table 5 lists the number of trigram tokens (occurrences) and types (unique ones) in all the N-best lists com-
bined, together with the percentage of unreliable trigram types and tokens as determined by the reliability
threshold r. Note that trigrams containing start-of-sentence or end-of-sentence (commonly designated by
< s > and < /s >) are excluded from the table, since they can't be queried from the web. For each N-best
list, we queried the unreliable trigrams (and associated bigrams) in the list, from which we computed p*
with the three different interpolation methods. We then used p* to rescore the N-best list, and calculated the
WER of the top hypothesis after rescoring.

First, we set the reliability threshold r = 0, i.e. we regard only those trigrams that never occur in the
baseline corpus as unreliable. Figure 4(a) shows the WER with exponential models and Gaussian priors.
The three curves stand for different search engines, which turn out to be very similar. The horizontal dashed
line is the baseline WER. As predicted, when the variance of the Gaussian prior a2 > 0 (the left side
of the figure), p*E converges to p0 and the WER converges to the baseline WER. On the other hand when
a2 > +oo, the estimates of the unreliable trigrams come solely from the web. Such estimates seem inferior

trigram

total

tokens

types

0
5,311,303  2,002,530
37.7%
36,190
63.4%

57,107

1
2,310,416
43.5%
39,059
68.4%

reliability threshold r
3
2,650,340
49.9%
42,158
73.8%

2
2,496,312
47.0%
40,893
71.6%

4
2,772,500
52.2%
43,110
75.5%

5
2,889,348
54.4%
43,863
76.8%

Table 5: Number of unreliable trigrams in the N-best lists

(a) Exponential Models

(b) Linear Interpolation

/<
0.334  !*^/ 'J

*"**^\                                            /   /

xr* -                                     jf"'  /
X  ^*                                                     /    '

X x                                         /'   '

\  \                                         /   .      '

Li-    0.33
LU

0.328

0.32S

0.324

0.3341

0.332

LU033

0.326

0.324

0.334

0.332

0.328

0.326

0.324

0

.>

Y \ 
\v
\ \\                              . 

.*    /

.7             /

J         /

/    J

\ *;                      ^'       ^^

\vn              DO             ^                       y*^

^Sc    , + -  '           ~f<



\    *   0   p        f        / /
\ \V*? / ,>'
\   '    if . /
V       ' /  "

>     fr
\ fr,'               AltaVista    

 Lycos
-   FAST

a2

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

a

(c) Geometric Interpolation

(d) Reliability Threshold



.

0.334

0.332

o        *

,4



R
E
W

i

0.328



0.326

0.324

* + "





-

.0....,

*- -/cL

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9



0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5

X

Figure 4: Word Error Rates of web-improved language models as function of the smoothing parameter for
several different interpolation schemes, based on N-best rescoring

10

and the model has higher WER than the baseline. Between these two extremes, WER reaches minimum
(32.53% with AltaVista) around a2 = 1.

Figure 4(b) is the WER with linear interpolation. Again, the minimum WER 32.56% is reached between

the two extremes at a = 0.4 by AltaVista.

To use geometric interpolation, we needed to choose a value for e first. We chose e = 0.01 because this
minimized the perplexity when  = 1. Next we vary  while keeping e fixed, and plotted the WER of the
interpolated model in Figure 4(c). As with the previous interpolation methods, the WER reaches minimum
when the interpolation factor is near the middle. The minimum is 32.69% when  = 0.3 with FAST.

Next, we adjusted the reliability threshold r and observe its effect on WER. The interpolation method
used here is the exponential model with Gaussian prior and a2 = 1. We varied r from 0 to 5. With larger
threshold, more trigrams are regarded as unreliable, and hence more web queries had to be issued. As
shown in Figure 4(d), there is a slight but definite improvement in WER when we increase r from 0 to 1.
For example, The WER with r = 1 and AltaVista is 32.45%. Further increment results in about the same
WER, averaged over search engines. Note that LM0, the language model we are incorporating web estimate
into, was built after excluding all singleton trigrams in the corpus. This may explain why r = 1 is better
since trigrams with counts 0 or 1 in the corpus are indeed unreliable: in LM0 they must backoff to bigram
or unigram.

To analyze the source of improvement, we broke down the WER according to the trigram backoff modes
in LMQ. First, we marked each word W{ in the transcript with one of several labels, using the following rules:
Let Wj-_2 and WJ_I be the two words preceding W{. If the trigram " w^w'i-iti';" exists in LM0, label u?,-
as '3'. Otherwise if the trigram doesn't exist in LM0, but the bigram "u>j_i>;" does, label w; as '3-2',
meaning LM0 has to backoff to the bigram for ;,;. If the bigram doesn't exist in LM0 either, label W{ as
'3-2-1' since LM0 has to backoff to the unigram. In the second step, we compared the transcript with the
top hypotheses after rescoring the N-best lists with p0. Each word in the transcript obtains a second label of
either "correct" or "wrong" depending on whether the word is correct in the corresponding top hypothesis.
We then collect the percentage of correct words within categories '3', '3-2' and '3-2-1' respectively. In the
third step we repeated the second step, except that the top hypotheses are now obtained by rescoring the
N-best lists with pE, where a2 = 1, r = 1, and the search engine is AltaVista. We compare the percentage
of errors in step 2 and step 3 in Table 6. Note that insertion errors are not counted in our error break down.
Not surprisingly, the '3-2-1' category has the highest error rate for both p0 and pE, since the words in this
category are the hardest from the language model's point of view. The '3-2' category has lower error rate,
and '3' has the lowest. The interpolated language model pE improves error rate for all three categories,
compared to p0. The largest improvement is in the '3-2-1' category, which suggests the web helps LM0
most with the hardest cases. It is not clear though why the '3-2' category is not improved as much.

category  words
3480
2236
479

3
3-2
3-2-1

error rate
Po
P%

23.3%  22.8%
30.7%  30.1%
50.1%  46.1%

Table 6: Error break down by LMQ backoff mode

11

4.2   Approximate Perplexity

There are 6195 words in the transcript. The baseline perplexity of the transcript with LM0 is 196.7. We
wanted to compute the perplexity of the transcript with different interpolated language models. We define
UWlwo in (1) based on the transcript. However this introduces a subtle bias: the interpolated models now
depend on the transcript. In other words, we are dynamically choosing models according to the words we
will be predicting. The resulting scores are therefore not strictly interpretable as probabilities. For this
reason we consider the perplexities we get on the transcript to be approximate only. We still report these
values in this section because we believe that the distortion is not too severe, and the approximation still
provides useful insight into the true perplexity of web-improved language models. Note that, although the
same kind of bias exists in WER computation, it doesn't diminish the validity of the WER improvement we
get there, since in classification it is not the particular probability value but the ranking that matters.

Figure 5(a-c) compares different interpolation methods when the reliability threshold r = 0. There are
2274 unique unreliable trigrams in the transcript. We submitted them (and the corresponding bigrams) as
queries to the search engines, and computed p* with the three different interpolation methods described in
the last sections respectively. From p* we computed the approximate perplexities.

Figure 5(a) shows the approximate perplexity with the exponential model and a Gaussian prior. Like the
WER in Figure 4(a), the approximate perplexity converges to the baseline when the Gaussian prior a2 -> 0.
The approximate perplexity worsens when a2 > +oc. The best value 156.9 is achieved by FAST also
between these two extremes at a2 = 1. Again, different search engines are similar.

Figure 5(b) is the approximate perplexity with linear interpolation. It is also similar to the WER in

Figure 4(b). The minimum 156.2 is reached by FAST at a = 0.45.

Figure 5(c) shows the approximate perplexity with geometric interpolation and f = 0.01. As with the
previous interpolation methods, the approximate perplexity converges to the baseline when  > 0 and is
worse when  y 1. But unlike the other methods, approximate perplexity seems to be always worse than
the baseline, and increases monotonically with .

Figure 5(d) compares the effect of the reliability threshold r on the approximate perplexity. As in
Figure 4(d), the interpolation method used is exponential model with Gaussian prior and a2 = 1. Again we
see improvement when we increase r from 0 to 1. For example, FAST's approximate perplexity goes down
to 147.5. We believed this can be explained similarly to Figure 4(d).

5   Discussions

In this paper, we demonstrated that trigram estimates obtained from the web can significantly improve WER
relative to pure corpus-based estimates, even though the web estimates are noisy, and the web and the test
set are not in the same domain. We believe the improvement largely comes from better trigram coverage due
to the sheer size of the web, which acts as a 'general English' knowledge source. Interestingly, which search
engine is used doesn't make much difference. Furthermore, which interpolation method is used doesn't
make much difference either (at least for WER), as long as an appropriate interpolation parameter is chosen.
Our method has certain advantages. Besides having better N-gram coverage, the content of the web is
constantly changing. Our method would enable automatic up-to-date language modeling. However, there
are also several disadvantages. The most severe one is the large number of web queries. In our experiment,
we needed to submit an average of 340 queries to the web for each utterance. This results in heavy web
traffic and workload on the search engines, and very slow rescoring process. Another concern is privacy:
one may be sending fragments of potentially sensitive utterances to the web. Both problems, however, can
be partly solved by using a web-in-a-box setting, i.e. if we have a snapshot of the text content of the whole
WWW on local storage. Yet another problem is the lack of focus on domain specific language. This might

12

(a) Exponential Models

           

-- AltaVista
 O'   Lycos
. -t- FAST

//

-/

/./

/-/

J:i

'

^!S-v.                                                                                                / -'S

/./

N&\                 S**'

^^frptf*

280

260

240
fr
<= 220

200(

180

160

(b) Linear Interpolation

-- AltaVista
O'  Lycos
-1- FAST

A

'
.

-

.

.

r*--i-i-i-~i.

0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1

 

(c) Geometrie Interpolation

(d) Reliability Threshold

-*- AltaVista
O'   Lycos
-+- FAST

-*- AltaVista
O'  Lycos
-+- FAST

0.1 0.2 0.3 0.4 0.5
P

0.7 0.8 0.9 1

Figure 5: Approximate perplexity of web-improved language models as function of the smoothing parameter
for several different interpolation schemes.

13

be solved by querying specific domain hosts instead of the whole web, although by doing so the N-gram
coverage may deteriorate.

The method proposed in this paper is only one crude way of exploiting the web as a knowledge source
for language modeling. Instead of focusing on trigrams, one could look for more complex phenomena, e.g.
semantic coherence [16] among the content words in a hypothesis. Intuitively, if a hypothesis has content
words that 'go with each other', it is more likely than one whose content words seldom appear together in
a large training text set. The web + search engine approach seems well suited for this purpose. We are
currently pursing this direction.

Acknowledgement

The authors are grateful to Stanley Chen, Matthew Siegler, Chris Paciorek and Kevin Lenzo for their help.
The first author has been supported in part by NSF LIS under grant REC-9720374.

