ABSTRACT
We address the task of learning rankings of documents from
search engine logs of user behavior. Previous work on this
problem has relied on passively collected clickthrough data.
In contrast, we show that an active exploration strategy can
provide data that leads to much faster learning. Specically,
we develop a Bayesian approach for selecting rankings to
present users so that interactions result in more informative
training data. Our results using the TREC-10 Web corpus,
as well as synthetic data, demonstrate that a directed ex-
ploration strategy quickly leads to users being presented im-
proved rankings in an online learning setting. We nd that
active exploration substantially outperforms passive obser-
vation and random exploration.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Measurement, Performance

Keywords
Clickthrough data, Web search, Active exploration, Learn-
ing to rank

1.

INTRODUCTION

There has recently been an interest in training search en-
gines automatically using machine learning (e.g. [20, 4, 24]).
The ideal training data would be rankings of documents or-
dered by relevance for some set of queries. In some cases
it is practical to hire experts to manually provide relevance
information for particular queries (as in [4]), but usually
data provided by experts is too expensive and is not guar-
anteed to agree with the judgments of regular users. This
is a particular problem when typical user queries are short
and ambiguous, as is often the case in web search.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
KDD07, August 1215, 2007, San Jose, California, USA.
Copyright 2007 ACM 978-1-59593-609-7/07/0008 ...$5.00.

As an alternative source of training data, previous work
has used clickthrough logs that record user interactions with
search engines. However, as far as we are aware, all previous
work has only used logs collected passively, simply using the
recorded interactions that take place anyway. We instead
propose techniques to guide users so as to provide more use-
ful training data for a learning search engine.

To see the limitations of passively collected data, consider
the typical interactions of search engine users. Usually, a
user executes a query then perhaps considers the rst two
or three results presented [14, 15]. The feedback (clicks) on
these results is recorded and used to infer relevance judg-
ments. These judgments are then used to train a learning
algorithm such as a ranking support vector machine [18].
In particular, users very rarely evaluate results beyond the
rst page, so the data obtained is strongly biased toward
documents already ranked highly. Highly relevant results
that are not initially ranked highly may never be observed
and evaluated, usually leading to the learned ranking never
converging to an optimal ranking.

To avoid this presentation eect, we propose that the
ranking presented to users be optimized to obtain useful
data, rather than strictly in terms of estimated document
relevance. For example, one possibility would be to inten-
tionally present unevaluated results in the top few positions,
aiming to collect more feedback on them. However, such an
ad-hoc approach is unlikely to be useful in the long run and
would hurt user satisfaction. We instead introduce princi-
pled modications that can be made to the rankings pre-
sented. These changes, which do not substantially reduce
the quality of the ranking shown to users, produce much
more informative training data and quickly lead to higher
quality rankings being shown to users.

The primary contribution of this paper is to present a
principled approach to eciently obtain training data that
leads to rankings of higher quality. We start by presenting a
summary of observations about user behavior in Section 2,
as how real users behave guides our approach. Next, in Sec-
tion 3 we formalize the learning problem as an optimization
task, present a suitable Bayesian probabilistic model and
discuss inference and learning. Following this we present
strategies to modify the rankings shown to users so that
performance of learned rankings improves rapidly over time
in Section 4. We describe our evaluation method in Section
5 and present results on synthetic data and TREC-10 Web
data in Section 6. In particular, we see the improvements
using our exploration strategies are much faster than with
passive or random data collection.

2. USER BEHAVIOR

Learning rankings relies on training data collected from
users. We will now examine the specic properties of user
behavior as they will guide our approach for the remainder
of this work. In particular we will see what sort of data can
reasonably be collected from clickthrough logs and how we
should include user behavior in our learning task.

A number of studies have shown that users tend to click
on results ranked highly by search engines much more of-
ten than those ranked lower. For example, in recent work
Agichtein et al.[1] present a summary distribution of the rel-
ative click frequency on web search results for a large search
engine as a function of rank for 120,000 searches for 3,500
queries. They show that the relative number of clicks rapidly
drops with the rank  compared with the top ranked result,
they observe approximately 60% as many clicks on the sec-
ond result, 50% as many clicks on the third, and 30% as
many clicks on the fourth. While we may hypothesize that
this is simply because better results tend to be presented
higher, Joachims et al.[21] showed that there is an inherent
bias to rank in user behavior. They showed that users still
click more often on higher ranked results even if presented
with rankings reversing the top ten results.
In fact, eye
tracking studies performed by Granka et al.[15] show that
the probability that users even look at a search result decays
very quickly with rank.

On the one hand, this explains why most common per-
formance measures in information retrieval place greater
emphasis on highly ranked results (such as Mean Average
Precision, Normalized Discriminative Cumulative Gain and
Mean Reciprocal Rank). On the other hand, this observa-
tion means that rank strongly inuences how many times a
document is evaluated by users, and hence how much train-
ing data can be collected from clickthrough logs.

Given the recorded clicks, the question also remains how
best to interpret the log entries to infer relevance infor-
mation about a document collection. Two alternative ap-
proaches to interpreting clickthrough logs are (1) consider a
click on a document in a result set as an absolute sign of rel-
evance, or (2) consider a click on a document as a pairwise
judgment comparing the relevance of that document to some
other document considered earlier. Specically, a common
approach is to assume that a clicked on result is more rel-
evant than a non-clicked higher ranked result. Joachims et
al.[21] showed that interpreting clicks as relative relevance
judgments is generally reliable, while absolute judgments are
not. Such pairwise relevance judgments have been success-
fully used to learn improved rankings [20, 24].

A nal observation of user behavior is that clickthrough
logs are inherently very noisy. Users often click on search
results without carefully considering them [15]. In partic-
ular this means that any single judgment that states that
one document is more relevant than another has a signi-
cant probability of being incorrect. However, previous work
has shown that if the dierence in relevance between docu-
ments is larger, relative relevance judgments are less likely
to be noisy [25]. This work also showed that if adjacent pairs
of documents in the ranking shown to users are randomly
swapped to compensate for presentation bias, provably re-
liable pairwise relevance judgments about adjacent pairs of
documents can be collected.

We summarize that (1) Clickthrough data is best inter-
preted as relative relevance judgments; (2) The relevance

judgments are noisy; (3) The top ranked documents are the
most important to estimate correctly. Guided by these prop-
erties, we now turn to formalizing the learning problem.

3. LEARNING PROBLEM

Assume we have a document corpus C = {d1, . . . , d|C|}
and some xed user query q. For this query, we want to
i  < of each document
estimate the average relevance 
di (for some user population). From the previous section,
we know that users can provide us with noisy judgments of
the form 
j . We assume that some ranking function
can provide initial estimates of 
i . The goal is accurately
estimate 

i with as little training data as possible.

i > 

The estimation task involves a three step iterative process:
First, given relevance estimates, we must select a ranking
to display to users. Second, given the ranking displayed,
users provide relevance feedback. Third, using the relevance
feedback, we update the relevance estimates and repeat the
process for the next user. In this paper, we focus most on the
rst step, namely selecting rankings of documents to show
users so that the collected judgments allow the relevance
estimates to be improved quickly, while at the same time
maximizing the quality of the rankings.
3.1 Probabilistic Model
|C|)  M be the true relevance values
of the documents in C. Modeling the problem of nding M
given training data D in a Bayesian framework, we want to
maintain our knowledge about M in the distribution

Let M = (

1, . . . , 

P (M|D) =

P (D|M )P (M )

P (D)

We assume that P (M|D) is a multivariate normal with zero
covariance:

P (M|D) = N (1, . . . , |C|; 2

(1)
Graphically, we can draw P (M|D) as a set of Gaussians
centered at i with variance 2

i . For example:

1, . . . , 2|C|)

-

3

-
2

2

1

4

5

3
Relevance estimate

This model is motivated by ability estimates maintained
for chess players [11]. In the most closely related previous
work, Chu and Ghahramani address a similar problem using
Gaussian Processes [8]. However, instead maintaining the
distribution P (M|D), they directly estimate M given D.
This is also true of other related prior work [9, 10, 22]. The
key dierence in our approach is that we are not simply nd-
ing the optimizing ranking. Rather, maintaining P (M|D) is
key as it allows us to optimize for collected training data.
3.2
We measure the dierence between relevance assignments
using a loss function L : M  M  <. To nd good rele-
vance estimates, we want to nd an M = (1, . . . , |C|)  M
such that L(M, M) is small. Noting that M is unknown,
we want to nd the ranking that minimizes the expected
loss given what we know about M, namely P (M|D):

Inference

EMP (M|D) [L(M, M



)]

(2)

argmin

M

where M is drawn from the probability distribution P (M|D).
Suppose the loss function L can be decomposed over pairs
of documents in C. We can then decompose the expected
loss into a form easier to work with:

EP (M|D) [L(M, M

)]



24 |C|X

Lpair(M, M

|C|X
hLpair(M, M



, i, j)

35
i

= EP (M|D)

|C|X

|C|X

=

EP (M|D)

i=1

j=i+1



, i, j)

(3)

i=1

j=i+1

where P (M|D) is shorthand for M  P (M|D).

We will now show that the mode of P (M|D), namely
M = (1, . . . , |C|), is often the solution to Equation 2. Con-
sider solving Equation 2 for a loss function that counts the
number of misordered pairs of documents. The assignment
with minimum expected loss is the mode of P (M|D).

Lemma 1. Let P (M|D) = N (1, . . . , |C|; 1, . . . , |C|) be
a distribution over models. Assume L(M, M) counts the
number of dierently ordered pairs of documents, when they
are sorted by i and 

i respectively. A solution of
EMP (M|D) [L(M, M

)]



argmin

M

is M = (1, . . . , |C|).

Proof. Assume M opt= (opt

1 , . . . , opt|C| ) is the minimizing
relevance assignment, and has lower loss than M. There
must exist two documents di and dj that are ranked adja-
cently when documents are ordered by M opt yet are ordered
dierently by M, i.e. opt
and i < j. Let M f lip
be the ranking obtained by reversing di and dj. Let these
rankings have expected loss Ef lip and Eopt.

i > opt

j

As the documents are adjacent, the loss of M opt and M f lip
only diers in the contribution of the pair (di, dj). Plugging
in the loss function we get
Ef lip  Eopt = P (

j )  P (



j > 


i )


i > 

0@ i  j
q

2
i + 2
j

1A  

0@ j  i
q

1A < 0

2
i + 2
j

= 

where  is the cumulative distribution function of the stan-
dard normal distribution, since i < j. Hence we have a
contradiction as M opt is not the minimizing ranking.

We see a similar result for the loss function that penal-
izes any error in the dierence of document relevances, i.e.
Lpair(M, M, i, j) = ((i  j)  (

i  

j ))2.

Lemma 2. Let P (M|D) = N (1, . . . , |C|; 1, . . . , |C|) be
a distribution over models. Assume Lpair(M, M, i, j) =
((i  j)  (

i  
argmin

M

j ))2. A solution of
EMP (M|D) [L(M, M



)]

!

Z 


 (

for M and M respectively. Let ij = (2
j )1/2. The
contribution to the expected loss for the pair of documents
(di, dj) is
EP (M|D)[Lpair(M, M

i + 2

, i, j)]



ij  ij)2
22
ij


1

ij  opt

(

ij )2d


ij

=

2



exp
ij
ij + (ij  opt
ij )2
= 2
ij = ij. Hence M opt = M mini-
which is minimized if opt
mizes all terms in the sum in Equation 3 simultaneously and
thus minimizes the expected loss.
We see that the mode of the distribution P (M|D) minimizes
the expected loss for two reasonable loss functions. As it
can also be obtained very eciently given P (M|D), for the
remainder of this work we will assume that the mode is,
or is close to, the minimizer of the expected loss. We will
refer to the ranking obtained by sorting documents by their
relevance according to the mode of P (M|D) as the mode
ranking.
3.3 Loss Function

Given our analysis of real user behavior in Section 2, we
see that the loss functions discussed above are too simple.
Specically, two properties to expect of an appropriate loss
function are (1) The loss for ranking a less relevant docu-
ment above a more relevant document should be larger if the
documents are presented higher in the ranking (i.e. where
users are more likely to observe them); (2) The loss should
be larger if the dierence in relevance is larger. To the best
of our knowledge there is no common pairwise decomposable
loss function with these properties so we propose a quadratic
hinge-loss function with cost of misordering decaying expo-
nentially with rank:
Lpair(M, M
With rij we denote the minimum rank of di or dj when all
documents are ordered by M (i.e. the relevance assignments
used to present results to users) divided by 10, and 1 is
the indicator function. A pair of documents is considered
misordered if the relative ranking according to M does not
agree with that according to M. Making use of the pairwise
form of the loss function and plugging in the mode ranking
M , the inner term of Equation 3 can now be written as
EP (M|D)[Lpair( M , M

rij`(ij)  (

j )2 1misordered


, i, j) = e

i 




Z



P (

Z

 (
j , ij = i  j and 2

, i, j)]
j|j, j) Lpair( M , M

ij  ij)2
22
ij

Lpair(

!

=

P (

i |i, i)


Z 

=

exp

1

2ij
i  
ij = 

where 
j , noting
that the dierence of two normally distributed variables is
also normally distributed. Plugging in the loss, and choos-
ing to sum over the pairs such that ij is always negative,
Equation 4 becomes:

ij = 2

i + 2



Z 


0


 (

2
!!

exp

 ijij
2

ij  ij)2
22
ij

2

exp

ij
22
ij

!
!#


ij

d

ij  ij



ij

2ij

erij
"

2ij

rij

2
ij
2

= e

1+erf

(5)





, i, j) d
i d
j


ij, ij, rij) d


ij (4)

is M = (1, . . . , |C|).

=

Proof. Let M opt be the minimizing model. Let opt
i  opt
opt
dj according to M opt, and ij and 

ij =
be the dierence in relevance estimates of di and
ij be dened equivalently

j

where erf() is the error function. Substituting this into
Equation 3 gives an easy to compute closed form expres-
sion for the expected loss.
3.4 Estimating P(M|D)

As discussed in Section 2, clickthrough data is best inter-
preted as relative relevance judgments. We can write them
in the form di (cid:31) dj, indicating that di was judged more
relevant that dj. A standard approach to modeling noise in
pairwise comparisons is to assume that the probability of an
outcome is determined by the Bradley-Terry model [2]:

i  i +

 1

2
i

g(2

j )(si  E(s|i, j, 2
j ))

q
+ 1
2

1

1
2

1
2
i

+

(7)

(8)

i 
2

where

q =

g(2) =

log 10
400

p1 + 3q22/2

1

P (di (cid:31) dj) =

rel(di)

rel(di) + rel(dj)

,

(6)

E(s|i, j, 2

j ) =

1
j )(ij )/400
g(2

1 + 10

where rel(di) is the relevance of di. The Bradley-Terry
model can be reparameterized setting rel(di) = 10i/ where
 is a known, global and xed parameter. Assuming the
pairwise judgments are independent (as can be reasonably
expected with clickthrough data from multiple users),
P (D|M = (1, . . . , |C|)) =

P (di  dj|i, j)

Y
Y

di(cid:31)djD

=

di(cid:31)djD

1

1 + 10(ij )/

Given this likelihood model and a Gaussian prior, we can ap-
ply an o-the-shelf algorithm to maintain P (M|D), namely
the glicko rating system commonly used for rating chess
players [11]. Given an estimate of player ability (document
relevance) i and error in the estimate i, this algorithm
provides a set of approximate online update equations for
maintaining the estimated relevance and error as data is col-
lected. The update to the estimates for di following a single
comparison to dj (where si is 1 if di wins and 0 otherwise)
is presented in Table 1.
While it would also be interesting to compare alternative
ways of maintaining P (M|D) (e.g. [17]), or using a batch
algorithm (see [19] for a discussion of alternatives), the sim-
plicity and online aspects of the glicko system are appealing.
In particular, in real world settings where large amounts of
data are collected for large document collections with a large
number of queries, a global optimization is likely to be slow
and thus infeasible.
4. EXPLORATION STRATEGIES

As we have seen that users are much more likely to pro-
vide feedback on highly ranked documents, we turn to the
question of optimizing the data collection process to most
quickly minimize the loss. In particular, by selecting which
documents to present at high rank, we inuence the pairs of
documents for which we obtain relevance judgments. In this
paper, we will consider only modications that change two
documents in a ranking, limiting ourselves to the top two
most of the time. We will see that despite the simplicity
of this approach, substantial improvements in performance
can be obtained at small cost in presented ranking quality.

We consider the following algorithms for determining which

ranking to present users.
Passive Collection (Top2). Present the mode ranking, i.e.
sorting documents by M = (1, . . . , |C|).

The algorithm Top2 assumes no changes are made to the
mode ranking, ignoring bias in data collection. This is the

2 =

1

q2g(2

j )2 
E(s|i, j, 2

1

j )(1  E(s|i, j, 2
j ))

Table 1: glicko update equations

approach used in all previous work in learning to rank, and
would be eective if users provided feedback about results
throughout the ranking. In some settings this may be the
case, for example in search engines for academic articles
where many users thoroughly consider all retrieved results.
However in general web search settings, as discussed above,
users focus their attention on the highest ranked results.
Random Exploration (Random). Select a random pair of
documents and present them rst and second. Then rank
the remaining documents according to M .

This algorithm is a naive modication to the mode rank-
ing. Two random documents are picked uniformly and in-
serted at the top of the ranking presented to users. Given
the uniform distribution, this perturbation is likely to often
pick documents that have a low prior expectation of be-
ing relevant, thus likely presents users with poorer results.
However, it benets from the potential for feedback on all
documents regardless of rank, even in the presence of signif-
icant user bias. A similar method was proposed by Pandey
et al. [23] in the context of identifying new web pages that
would soon become popular, suggesting to randomly insert
new documents into web search results.
Largest Expected Loss Pair (LELpair). Select the pair of
documents di and dj that have the largest pairwise expected
loss contribution, and present these rst and second. Rank
the remaining documents according to M . Formally, this
means we select the pair di and dj that satises:

EP (M|D)[Lpair( M , M



, i, j)]

argmax

i6=j

LELpair selects the pair of documents with largest pairwise
contribution to the expected loss out of all pairs of docu-
ments. By presenting these documents at a high rank, the
feedback given on them will reduce the uncertainty in the
relative relevance of the documents. This will, in the long
run, drive the expected loss contribution of the pair of doc-
uments down. Given the glicko update rules, the pairwise
contribution of all other pairs of documents will not increase.
Hence this method will eventually drive the expected loss

down. Additionally, due to the exponential decay in the loss
of misordered pairs as the rank increases, LELpair tends to
select pairs of documents where at least one has a high esti-
mated relevance. Lower ranked documents are also eventu-
ally selected, but only after high rank documents have been
evaluated and their expected loss contribution is reduced.
If we ignore the eect of rank in the loss function, this ap-
proach is similar to previous work in active learning where
users are asked to label items where the predicted label is
most uncertain (e.g. [3, 27]). In our setting, document pairs
with high pairwise contribution tend to be those with large
estimated error in relevance.
One Step Lookahead (OSL). For each pair of documents,
compute the expected pairwise loss and the expected pair-
wise loss after a comparison based on the Bradley-Terry
model (using M to estimate the probability of possible out-
comes) and glicko updates. Select the pair of documents
with the largest expected reduction in the pairwise loss and
present these rst and second. Rank the remaining doc-
uments according to M . Formally, if M0 is the mode of
P (M|D) after updating it given the outcome of a compari-
son of di and dj, we select the pair di and dj that satises:

h
 E M0

argmax

i6=j

h

EP (M|D)[Lpair( M , M
EP (M|D)[Lpair( M



, i, j)]

0

, M



, i, j)]

ii

Intuitively, this algorithm performs approximate gradient
descent on the loss function. OSL nds the pair whose con-
tribution to the expected loss is likely to decrease most fol-
lowing a pairwise comparison. The expected contribution of
the pair after a comparison is a weighted sum of the expected
loss contribution for the two possible outcomes (either di
wins the comparison or dj wins). In this computation, the
eect of possible rank changes is ignored for eciency rea-
sons. This method is also related to an approach proposed
by Chajewska et al. in the context of utility estimation where
they found that the true utility of many dierent outcomes
can be quickly discovered by maximizing the reduction in
expected loss given new data [6].
Largest Expected Loss Documents (LELdoc). For each
document di, compute the total contribution of all pairs
including di to the expected loss of the ranking. Present
the two documents with highest total contributions rst and
second, and rank the remainder according to M . Formally,
this method selects the pair di and dj that satises:

hX
X

a6=i

a6=j

argmax

i6=j

EP (M|D)[Lpair( M , M



, i, a)]

+

EP (M|D)[Lpair( M , M



, j, a)]

i

This method addresses a potential limitation of LELpair and
OSL: They only consider individual pairwise document con-
tributions to the expected loss despite the contributions of
pairs not being independent. LELdoc addresses this by com-
puting the total contribution of each document by summing
over all pairs including that document. For example, if some
document d is ranked third, its total contribution is that
from d and the top ranked document, plus the contribution
from d and the second document, plus that from d and the
fourth document and so forth. LELdoc selects the two doc-

uments with highest total contributions and presents them
rst and second. By comparing these two documents and
reducing the uncertainty in their relevances, we are likely to
reduce the contributions to the risk of all pairs including the
documents.

An alternative selection algorithm proposed in previous
work (e.g.
[13, 7]) is to compare pairs of items such that
the probability distribution over models changes most in
terms of KL-divergence or entropy. We do not pursue this
alternative as it does not take into account the loss function
being optimized.

Finally, we note that explorations strategies for rankings
are related to the opponent assignment problem in sports
tournaments. However, there are two key dierences. First,
a tournament has a dierent concept of loss. A criterion
often optimized is the probability of the true best player
winning the nal game (e.g.[12, 26]). Second, pairwise com-
parisons in a tournament have no cost. In most sports, a
common constraint is that all teams or players must com-
pete for at least n rounds. This means that each item
must be compared with some other item every round and
the optimization problem is to select which pairs are com-
pared such that the loss is eventually minimized rather than
aiming to minimize the loss as quickly as possible.

5. EVALUATION METHODOLOGY

We now have a number of strategies for eliciting useful
training data from users of a search system, and have a
method to estimate the relevance of the documents using
our probabilistic model. In this section, we will describe how
these strategies were evaluated. In particular, we will com-
pare how eective each strategy is at improving the quality
of the rankings shown to users.

We evaluate as follows: Given an initial ranking of one
thousand documents as returned by a search engine in re-
sponse to a query, we derive a prior P (M ). This prior initial-
izes P (M|D). For a particular exploration strategy, we next
select a ranking to present to users. We evaluate the loss
of the presented ranking and of the mode ranking derived
from M . Next, we simulate user behavior on the presented
ranking, using a simple behavioral model, and collect train-
ing data that is used to update the model parameters. We
repeat this process 3,000 times for each initial ranking. This
experimental setup is formalized in Algorithm 1.

The behavioral model we use to simulate clickthrough
data is detailed in Algorithm 2. By using a simulation, it
is possible to evaluate the exploration strategies in detail
without needing large numbers of test subjects, and avoid
eects that may be unique to specic users (e.g. to academic
users). Our model simplies real behavior by assuming that
users only click on top two results, and do so with probabil-
ity specied by the Bradley-Terry model. This is motivated
by the fast decay observed in the number of clicks as rank in-
creases in real search systems. Clearly, in a real setting some
additional data would be collected from lower ranks, making
the results we report conservative in this respect. However,
the amount of data collected about results at lower ranks
would be signicantly smaller.

We repeated each experiment with either 30 or 100 ini-
tial rankings, each giving a dierent initial set of relevance
estimates. We report the mean loss across all runs (nor-
malized such that the initial loss is 1), or the mean average
precision (MAP). In some results we present a single nal

Algorithm 1 Evaluation Setup
1: Input: Estimated relevances {i} for di  C
2: i  0 for di  C
3: for iteration 1 through 3,000 do
4:
5:
6:
7:
8: Update i, j, i, j per Equations 7 and 8
9: end for

Pick two documents di, dj to rank 1st and 2nd
Randomly swap di and dj (see Section 2)
Show the selected ranking to user
Record training data given user feedback

Algorithm 2 User Behavioral Model
1: Input: Ranking of documents (d1, . . . , d|C|)
2: Input: True relevances of documents (
3: if U nif ormRandom(0, 1) <
1
4: Winner is d1: s1  1; s2  0
5: else
6: Winner is d2: s1  0; s2  1
7: end if

1
(

1, . . . , 
|C|)
2 )/400 then

1+10

performance, i.e. the loss or MAP after 3,000 pairwise com-
parisons and model updates. Note that as our rankings are
of 1,000 documents, 3,000 comparisons is on average just six
noisy pairwise comparisons involving each document.

6. RESULTS

We start by evaluating the exploration strategies on syn-
thetic data, where we evaluate their eectiveness if the as-
sumed prior distribution matches the true generating model.
This will be followed by an evaluation using TREC-10 data.
6.1 Synthetic Data

We randomly generated a corpus of 1000 documents with
i drawn from N (1500, 1472). We chose
expected relevances 
this scale as it is comparable to typical chess scores. We
then drew ten independent initial models, drawing i from
i , 1472) and initializing i = 147  0. We repeated
N (
this process three times, giving 30 initial rankings over three
dierent random corpora.

For each initial ranking, we ran each strategy for 3,000
iterations. Figure 1 shows the loss of the mode ranking
at each iteration. Along the horizontal axis is the number
of pairwise comparisons. After each pairwise comparison,
P (M|D) is updated and a new pair to compare is selected.
The vertical axis is the average loss of the mode ranking
relative to the initial average loss. The error bars indicate
one standard error in the mean scaled loss.

Which exploration strategy learns fastest?
The rst question to answer is which strategy learns fastest.
The passive Top2 approach does not lead to a meaningful
overall reduction in the loss. This is because our user model
assumes that users only provide feedback on the top two
documents. After a few comparisons, the top few documents
have their relative position correctly established and no new
documents are ever compared again. Our other baseline
algorithm, Random, sees the loss decrease slowly.

We see that the other exploration strategies all perform
substantially better than the baselines. Both LELpair and
OSL quickly reduce the total loss by selecting pairs of doc-

Figure 1: Change in loss as a function of the number
of pairwise comparisons for each exploration strat-
egy on synthetic data

Figure 2: Eect of weight of prior on the nal loss
evaluated on synthetic data (true noise is 0 = 147)

uments with high contributions to the expected loss, and
high expected reductions in it. We see this improvement
continues for a large number of comparisons. In contrast,
LELdoc appears to asymptote more quickly. This is be-
cause the documents selected continue to be those at high
ranks even after many comparisons.
In eect, LELdoc is
too biased toward highly ranked documents. Comparing
with LELpair and OSL, we see that while lower ranked doc-
uments may have lower total contribution to expected loss,
they often have higher individual pairwise contributions.
How robust is the approach to prior assumptions?
The second natural question to ask is how robust the results
are to the weight given to the initial ranking, as in the case
of real data the correct weight is likely to be unknown. This
weight is encoded by the initial values of i, i.e. 0. We now
explore the eect of changing that value. This experiment
is possible because we know the level of noise used when
generating synthetic data.

Figure 2 shows the eect of selecting an assumed noise
level that diers from the true noise level. With our default
setting, and that used to generate our synthetic datasets,
the probability of a document in the top 10 according to the
prior not being in the top 100 according to true relevance is

0 0.2 0.4 0.6 0.8 1 0 500 1000 1500 2000 2500 3000Loss as a fraction of original lossNumber of pairwise comparisonsTop2RandomLELpairOSLLELdocs 0 0.2 0.4 0.6 0.8 1 1.2 1.4 50 100 150 200 250 300Loss after 3000 pairwise comparisons, as a fraction of initial lossAssumed initial standard deviation of prior relevance estimatesTop2RandomLELpairOSLLELdocFigure 3: Change in loss as a function of the number
of pairwise comparisons for each selection algorithm
on TREC-10 data

Figure 4: Change in MAP score as a function of the
number of pairwise comparisons for each selection
algorithm on TREC-10 data

about 8%. We can see that selecting a suboptimal 0 does
not drastically reduce performance after a xed number of
pairwise comparisons. Apart from LELdoc, the best perfor-
mance is achieved when the actual noise is correctly known.
Interestingly, LELdoc performs best when the error in the
prior is underestimated as it then selects documents further
down the ranking for comparison.
6.2 TREC Data

In addition to the synthetic data, we also evaluated the
exploration strategies using the TREC-10 web track queries
(topics 501 through 550) in the WT10g document corpus.
This subset of the corpus includes 50 topics and topic de-
scriptions, run as queries on documents that are part of
the corpus. As part of the 10th Text REtrieval Confer-
ence (TREC-10) [16], 18 teams submitted a ranking of doc-
uments for each topic. Then, for each topic, documents
ranked highly by the teams were manually judged to be ei-
ther highly relevant (relevance score of 2), relevant (score
of 1) or non-relevant (score of 0). All other documents in
the corpus were assumed non-relevant (score of 0). The
discretized nature of these relevance judgments is unrealis-
tic, as few documents are likely to have precisely the same
relevance in the real world. To compensate and make the
learning problem more realistic, we added uniform random
noise in the range [0.5, 0.5] to the true relevance judgments,
preserving the relative order of highly relevant, relevant and
non-relevant documents.

For each of the 50 TREC-10 topics, we randomly selected
two submissions and used the submitted scores to initialize
our model. We then repeated the evaluation described for
synthetic data. Each submission includes a ranking of typi-
cally 1000 documents, with a score given to each document.
The scores are arbitrary and unnormalized. Clearly they
can be interpreted as a prior in any number of ways. As
each ranking typically contains both highly relevant docu-
ments and non-relevant documents, we chose to normalize
the scores to a linear interval [1500 + 0, 1500  0] with
0 = 147. The resulting scores were used to set the initial
i. The initial estimated error i were set to 0. This means
that a document with a score near the maximum score is es-
timated to have approximately a 30% percent chance of in
fact being in the lower half of the ranking.

Which exploration strategy learns fastest?
Figure 3 shows the performance of the exploration strate-
gies. We see that the loss improves rapidly with LEL-
pair, OSL and LELdoc. We also see that Top2 performs
as it did on the synthetic data. One the other hand, Ran-
dom performs dierently: The loss of the mode ranking ini-
tially increases, then improves slightly but remains high.
We believe that this is due to the mismatch between the
prior and model. When two documents are compared, if the
outcome is not the expected one then the update to the rel-
evance estimates can be large. Sometimes, the lower ranked
document moves to a much higher rank, and the loss does
not quickly recover due to few comparisons per document.
Interestingly, performance of Random depends on the loss of
the initial ranking. When the initial loss is high, after 3,000
pairwise comparisons the loss tends to be reduced. The op-
posite is true when we start with a very good ranking.

How do the strategies perform with respect to MAP?
The loss function presented earlier is not one commonly used
to evaluate rankings. A measure much more widely used
by the research community is the mean average precision
(MAP). To compute the MAP, we consider each document
with true relevance 
i above some threshold as relevant and
others as irrelevant. The average precision of a ranking is
the average of the precision measured at each relevant docu-
ment1. The MAP score is the mean of the average precisions
across all 100 experiments. We used a threshold of 0.5 scaled
in the same way the scores were scaled.

Figure 4 shows how the MAP of the ranking changes as
more pairwise comparisons are performed. MAP visibly be-
haves very similarly to our loss function. We see that LEL-
pair and OSL result in the largest MAP improvement, and
appear likely to continue to improve further with more com-
parisons. As before, LELdoc performance plateaus quickly
and Top2 sees almost no improvement. Random performs
poorly, with an initial drop in MAP although after 2,000
pairwise comparisons the MAP is above baseline.

1

For simplicity, we only consider the 1000 ranked documents when
computing the MAP score. While the corpus may contain relevant
documents that never made it into the top 1000, these do not con-
tribute to the MAP score.

0.2 0.4 0.6 0.8 1 1.2 0 500 1000 1500 2000 2500 3000Loss as a fraction of original lossNumber of pairwise comparisonsTop2RandomLELpairOSLLELdoc 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 500 1000 1500 2000 2500 3000Mean Average PrecisionNumber of pairwise comparisonsTop2RandomLELpairOSLLELdocInitial MAPFigure 5: Eect of dierent noise levels in pair-
