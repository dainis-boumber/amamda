Abstract

Indexing  systems  for  the  World Wide Web, such
as  Lycos and Alta  Vista,  play  an essential  role  in
making the  Web useful  and usable.  These systems
are  based  on  Information  Retrieval  methods for
indexing  plain  text  documents,  but  also  include
heuristics
for  adjusting  their  document rankings
based  on the  special  HTML structure  of  Web doc-
uments. In  this  paper,  we describe  a wide range  of
such  heuristics--including  a  novel  one inspired  by
reinforcement  learning  techniques  for  propagating
rewards  through  a  graph--which  can  be  used  to
affect  a  search  engines  rankings.  We then  demon-
strate
to  combine these
heuristics  automatically,  based  on feedback  col-
lected  unintrusively  from users,  resulting  in  much
improved rankings.

a  system  which  learns

1  Introduction

Lycos (Mauldin  & Leavitt  1994),  Alta  Vista,  and sim-
ilar  Web search  engines  have become essential  as  tools
for  locating
information  on  the  ever-growing  World
Wide Web. Underlying  these  systems  are  statistical
text  documents.  How-
methods  for
of  HyperText
ever,
Markup Language  (HTML) documents,  which  exhibit
two kinds of  structure  not  present  in  general  text  doc-
uments:

the  bulk  of  the  Web consists

indexing  plain

1. They  have  an  internal

structure

of
consisting
typed  text  segments  marked by  meta-linguistic
tags
(markup).  HTML defines  a set  of  roles  to  which text
in  a  document can  be assigned.  Some of  these  roles
relate  to  formatting,  such  as  those  defining  bold and
italic
text.  Others have richer  semantic import,  such
as  headlines  and anchors,  the  text  segments  which
serve  as  hyperlinks  to  other  documents.

2. They also  have an  external  structure.  As a  node in
to  potentially

a  hypertext,  a  HTML page  is  related

huge numbers of  other  pages,  through  both  the  hy-
perlinks  it  contains  and the  hyperlinks  that  point  to
it  from other  pages.

Because  HTML pages  are  more structured
text,  Web search  engines  enhance traditional
methods with  heuristics
structure.
such heuristics  most effectively,  however.

than  general
indexing
that  take  advantage  of  this
It  is  by  no  means clear  how to  integrate

Paper  Overview
In  the  following  section  we describe  our  prototype
Web-indexing  system,  called  LASER, and  outline
its
heuristics  for  exploiting  the  internal  and external  struc-
ture  of  the  Web. In  the  section  entitled  Automatic
for
Optimization,  we describe  how the  parameters
combining  these  heuristics
tuned
based  on  system  usage.  Finally,  we present  and  dis-
cuss  our  first  empirical  results  with the  system.

are  automatically

2  LASER
LASER, a  Learning  Architecture

is  a  system designed  to  investigate

for  Search  Engine
Retrieval,
the  ap-
plicability  of  Machine Learning  methods to  the  index-
ing  of  Web pages.  From a  users  perspective,  much of
LASERs functionality  is  identical  to  that  of  other  pop-
ular  Web search  engines  (see  Figure  1).  The user  enters
unstructured  keyword  queries,  which  LASER matches
against  its  index  of  pages,  returning  abstracts  of  and
links  to  the  60 pages matching the  query most closely.
From this  page of  search  results,
the  user  can proceed
to  any of  the  abstracted  pages or  enter  a  new query.

LASERs retrieval

function

is  based  on  the  TFIDF

vector  space  retrieval  model (Salton  1991).  In  this
model documents and  queries  are  represented  as  vec-
tors  of  real  numbers, one for  each  word; documents and
queries  with similar  contents  are  transformed into  sim-
ilar  vectors.  LASER uses  an  inner  product  similarity
metric  to  compare documents with  a  query.  Typically,
frequency  in
the  value  of  a  word depends  both  on its

1)  THE REINFORCEMENT

LEARNING GROUP AT CARNEGIE MELLON "~~~A  fre~foccement learr~mol

hffp~/ww~c$~c~q/~M$/~n~q/p~/~t[r~nt#t)~jcthorne/x~ht~i~
Reinforcement Learning and Friends at  CMU  Talks of  Interest  (  ~=~.,~=~,a~:s.c~.~u)(tha e*,  1~ ~-~ster
RL t~k$ ~ IX* hek:l on "#e~s, 12:00, Wean 7220 unless otherwise noted. Pizza protided; BYOBI)=Nov 15: Anthony Robins (U. Ot~o, New Ze~and,
md CMU Psych), Rehear~ and pi~Ktorehe~k~ = solutions to  the c&ta~trophlc forgettlrKI  woblem "3:30, R,I Nov 17, Well 4623: ~e Keams (AT&T B~I
L~), Declalon tree lelltlll~  ~$tlthma are boosting ~,~~ocitP, ma =Nov 22: 3edaatian lhrun, Task cluaterkR ~ selective transfer of knowledge zeroes multiple
lemdl~l ta.,,k$: "lho*Rhte ~ Results "Nov 29: no talk--  NIPS "Dec 6: Geoff Gordon, Rar=k-Based Tests ( siides.ps.Z)---End Of ~eme~ter ---.  /  ...

(4  kt~s,  43  linkS, size=SK)

3)  ML-95 WORKSHOP REINFORCEMENT

LEARNING ~~  ~e~forcernent

learnlno|
(1  b~,ge,  21 Inke,  eize=8K)

hff/a.~c~.ol~u.~b/af$tc~.cmu.~tlminfolcem~t~-vfa(.Ihtmt
Workshop Call  for  Papers  Twelfth  International  Conference on Machine Learning; Value Function
Approximation
In Reinforcement Learning ~ s, 1~ ~ ~=t~,~ Re=~t, tahoe city,  c~ifornia, U.B.A. I  ~ worksiu~o
eXl01me the I~aues that ariae In nlk~elceme~ ~ when the v~ue function u/~ot be lea-ned exactly, but must be ~olor O~T~.t ed. It  ~ Ioncj been
recognlzed
the state space is too large to permit t~ble -Io~up ~opro~hes.
need to ger~rl~ze from past experlmces
 e r~evmt here, bet in ~actlce ...

that appr o~rnatlon Is eesantid o1 brge, ~e= -wodd problems because

to futwe ones, which inevit~01y involves m~kin9 ~opro~matJor~s,

h principle, ~t methods

In addition, we
for lel/Idl~J from exl~ole$

Figure  1:  The LASER Interface.  Our prototype  system  indexes  approximately  30,000  hypertext  documents  avail-
able  from  the  CMU Computer  Science  Department  Web server.

the  document under consideration  and its  frequency  in
the  entire  collection  of  documents.  If  a  word occurs
more frequently  in  a  particular  document than  in  the
collection  as  a whole, then  it  is  considered salient  for
that  document and is  given  a high score.  In  its  simplest
form,  TFIDF assigns  to  each  word a  score  proportional
to  its  frequency  in  the  document (term  frequency  or
TF) and a  decreasing  function  of  the  number of  docu-
ments it  occurs  in  overall  (inverse  document frequency
or IDF).

LASERs retrieval

function,  based on this  model, of-
fers  a  number of  parameters  which influence  the  rank-
ings  it  produces.  The parameters  affect  how the  re-
trieval
fields  (like  headlines),  how hyperlinks are  incorporated,
how to  adjust  for  partial-word  matches or  query-term
there  are  18  real-
adjacency,  and  more:  altogether,

function  responds  to  words in  certain  HTML

valued  parameters. 1  Using a  particular  parameter  set-
ting  makes it  possible  to  pick  a certain  retrieval  func-
tion  from the  family  of  functions  LASER offers.
In  this
function  can be adjusted  to  the  dif-
way, the  retrieval
ferent  characteristics
of  various  document collections
and user  groups.

in  Retrieval

Using  HTML Formatting

2.1
in  HTML. HTML is  a
Most  Web pages  are  written
markup language  which allows  the  designer  of  a  page
to  assign  certain  semantics  to  parts  of  a  document and
to  control  the  layout.  The designer  can specify,  for  ex-
ample, the  title  of  a  document, hierarchies  of  headlines
and hyperlinks,  and character  formats  such  as  boldfac-
ing.

~A listing  of  the  parameters LASER uses,  in  the  form
of  a function for  calculating  document scores,  can be found
in  Appendix A.

LASER makes  use  of  the  structure  HTML imposes
on  documents.  For  example,  one parameter  governs  to
what extent  words in  the  title  of  a  document should re-
ceive  stronger  indexing  weight  than  words near  the  end
of  a  document.  LASER has  parameters  for  weighting
words in  the  following  HTML fields:

 TITLE
 H1, H2, H3 (headlines)
 B (bold),  I  (italics),  BLINK
 A (underlined  anchor  text)

The parameters  for  these  HTML tags  are  simply  mul-
tiplicative
factors  for  the  "term  frequency"  of  words
within  their  scope.

Hypertext

Links

Incorporating

2.2
Unlike  most other  document collections,  Web pages  are
part  of  a  hypertext  graph.  For  retrieval
it  might  be
useful  not  only  to  look at  a document in  isolation,  but
also  to  take  its  neighboring  documents into  account.

The approach  we took  is  motivated  by  an  analogy
to  reinforcement learning  as  studied  in  artificial
intel-
ligence  (Barto,  Bradtke,  & Singh  1995).  Imagine that
an  agent  searching  for
information  on  the  Web can
move from page to  page only  by following  hyperlinks.
Whenever the  agent  finds  information  relevant
to  its
search  goal,  it  gets  a  certain  amount of  reward.  Re-
inforcement  learning  could  be used  to  have  the  agent
learn  how to  maximize the  reward it  receives,  i.e.
learn
how to  navigate  to  relevant  information.

The idea,  then,  is  to  have LASER rank  highly  pages
that  would serve  as  good starting  points  for  a  search
by such  an  agent.  Good starting  points  are  pages from
which it  is  easy to  reach  other  pages with relevant  in-
formation.  We conjecture  that  these  pages are  relevant
to  a  query  even if  they  do  not  contain  much relevant
information  themselves,  but  just
link  to  a  number of
relevant  documents.

Hyperlinks are  incorporated  as  follows.  First,  given
a query q the  retrieval  status  values rsv0(q,  d)  are  cal-
culated  for  each page d in  the  collection  independently,
based  on  the  HTML-specific  TFIDF parameters  de-
scribed  above.  In  reinforcement-learning
terms,  these
values  represent
the  "immediate  reward"  associated
with  each  page.  Then,  LASER propagates  the  rewards
back through  the  hypertext  graph,  discounting  them at
each step,  by value  iteration  (Bellman 1957):
d)  = rsv0(q,  d)  + 7  Z ]links(d)

rsvt+l(q,

rsvt (q, d)

~ ( 1)

dlElinks(d)

3  is  a  discount  factor  that  controls  the  influence  of
is  the  set  of  pages
neighboring  pages,  and  links(d)

referenced  by  hyperlinks
in  page  d.  This  dynamic-
programming update  formula  is  applied  repeatedly  for
each  document in  a  subset  of  the  collection.  This  sub-
set  consists  of  the  documents with a  sigificant  rsvo,  and
it  also  includes  the  documents that  link  to  at  least  one
of  those.  After  convergence (in  practice,  5  iterations),
pages  which  are  n  hyperlinks  away  from  document d
make a  contribution  proportional  to  7n  times  their  re-
trieval  status  value to  the  retrieval  status  value of  d.
the  behavior
of  this  mechanism: one is  7,  and the  other,  v E [0,  1],
controls  the  normalization  of  the  denominator in  For-
mula  1  in  a  range  from  Ilinks(d)l
down to  1.  Alto-
gether,  our  retrieval
function  has  18  parame-
the  score  assigned  to  document d  in  the  context
ters;
of  query q is  computed by rsv5(q,  d)  as  detailed  in  Ap-
pendix  A on  page  8.

to  LASER influence

Two parameters

status

3

Automatic

Optimization

The  18  numerical  parameters  of  LASERs retrieval
function  allow  for  a  wide variety  of  search  engine  be-
havior,
from  plain  TFIDF to  very  complex  ranking
schemes.  Qualitatively,  different
functions
produce markedly different  rankings  (see  Table 1).  Our
goal  is  to  analyze  system usage  patterns
to  (1)  quan-
tify  these  differences,  and (2)  automatically  optimize
the  parameter  settings.

retrieval

3.1 Measuring

Search

Engine

Performance

In  order  to  keep the  system interface  easy  to  use,  we
made a  design  decision  not  to  require  users  to  give
explicit
feedback  on  which search  hits  were good and
which  were  bad.  Instead,  we simply  record  which
hits  people  follow,  e.g.  "User  searched  for  vegetar-
ian  restaurant  and clicked  on  Restaurant  Reviews and
Eating  Indian  in  Pittsburgh."  Because the  user  gets
to  see  a detailed  abstract  of  each hit  (see  Figure 1),
believe  that  the  hits  actually  clicked  by  the  user  are
highly likely  to  be relevant.

A good retrieval

function  will  obey the  probability
ranking  principle  (van Rijsbergen  1979).  This  means
places  documents which are  most likely
to  be relevant
to  the  users  query near  the  top of  the  hit  list.  To eval-
uate  a  retrieval
function  fs  performance on  a  single
query q,  we simply  take  the  mean ranking  according  to
f  of  all  documents the  user  followed.  (Example scores
are  shown in  Table 1.)  We then  define  the  overall  per-
formance of  retrieval
function  f  to  be  the  average  of
in  the  database.  In
its  performance over  all  queries

in  Pittsburgh

standard  TFIDF
1.  Vegetarian  Chili  Recipes
2.  Vegetarian  Recipes
3.  Eating  "Indian"
4.  Restaurant  Reviews
5.  Greek Dishes
6.  Focus on  Vegetarian
7.  For the  Professional  Cook
SCORE: 3.5
simple  count  of  query  terms
1.  Collection:  Thai  Recipes
2.  Food Stores  Online
3.  A List  of  Food and Cooking Sites
4.  Cookbook of  the  Year
5.  Collection:  Tofu
6.  Eating  "Indian"  in  Pittsburgh
...
SCORE: 11

16.  Restaurant  Reviews

hand-tuned  parameters

automatically-learned

params

using  HTML structure;
1.  Restaurant  Reviews
2.  Eating  "Indian"  in  Pittsburgh
3.  A List  of  Food and Cooking Sites
4.  Duanes  Home Page &~ Gay Lists
5.  Eating  & Shopping  Green in  Pittsburgh
6.  Living  Indian  in  Pittsburgh
7.  For  the  Professional  Cook
SCORE: 1.5
using  HTML structure;
1.  Eating
2.  Restaurant  Reviews
3.  A List  of  Food and Cooking Sites
4.  Vegetarian  Chili  Recipes
5.  For  the  Professional  Cook
6.  Eating  ~ Shopping  Green in  Pittsburgh
7.  Vegetarian  Recipes
SCORE: 1.5

in  Pittsburgh

"Indian"

Table 1:  Rankings produced by four  different  retrieval
functions  in  response  to  the  query "vegetarian  restaurant."
Supposing  that  the  user  had clicked  on  the  Eating  "Indian"  in  Pittsburgh  and Restaurant  Reviews  pages,  these
retrieval

functions  would be  scored  as  shown.

symbols:

1  ~  1
Perf(f)  =  ~-~~i=1~

IDd
=

rank(f,  Qi,  Dij)  (2)

where Q1...QIQI  are  the  queries
in  our  database  and
Di is  the  set  of  documents the  user  followed after  pos-
ing  query  Qi.  The  input  used  by  this  performance
method is  clearly  noisier  and  more biased  than  that
used  in  methods based on precision-recall
(van  Rijsber-
gen 1979),  which employ exhaustive  relevance  feedback
information  assigned  manually by experts.

In  practice,

the  users  choice  of  hits  to  follow  is
strongly  biased  toward  documents appearing  early  in
the  hit  list--regardless  of  the  quality  of  retrieval  func-
tion  used.  Users  rarely  have  the  patience
to  scroll
through  pages  and  pages  of  hits.  Thus,  when eval-
uating  performances  of  new retrieval
functions  using
our  collected  database,  we attempt  to  equalize  these
"presentation  biases."  We do  this  by  evaluating  Perf
on  a  subsample  Q of  our  query  database,  where  Q
is  constructed
to  contain  an equal  number of  queries
from each different  presentation  bias;  or  alternatively,
we weight each query Qi so as  to  give  equal total  weight
to  each presentation  bias.

tions  performance,  we can  now pose  the  problem  of
finding  the  best  retrieval  function  as  a problem of  func-
tion  optimization:
find  the  parameter  vector  f  mini-
mizing Perf(ff).

The calculation  of  Perf  is  based on averages  of  dis-
crete  rankings,  so  we expect  it  to  be quite  discontinu-
ous  and probably  riddled  with  local  minima.  Thus,  we
chose to  apply  a  global  optimization  algorithm,  simu-
lated  annealing.  In  particular,  we applied  the  "modi-
fied  downhill  simplex" variant  of  simulated  annealing,
as  described  in  (Press  et  al.  1992).

Because  we calculate  Perf  from  only  a  fixed  sub-
sample of  queries,  aggressive  minimization  introduces
that  is,  our  converged pa-
the  danger  of  overfitting;
rameter  vector/7  may exploit  particular
idiosyncracies
of  the  subsample at  the  expense of  generalization  over
the  whole space.  To guard  against  overfitting,  we use
early  stopping with a  holdout set,  as  is  frequently  done
in  neural  network  optimization
1990), as  follows:

(Morgan  & Bourlard

1. We consider  the  sequence of  parameter  vectors  which
are  the  "best  so  far"  during  the  simulated  anneal-
ing  run.  These  produce  a  monotonically  decreasing
learning  curve (see,  for  example, Figure 2).

3.2  Optimization
Given our  parametrization  of  the  space  of  retrieval
functions  and our metric for  evaluating a  retrieval  rune-

Method

2. We then  evaluate

the  performance  of  each  of  these

vectors  on  a  separate  holdout  set  of  queries.  We
smooth the  holdout-set  performance  curve  and pick

$ f  used for  presentation
count
TFIDF
hand-tuned
Overall

Performance

count

6.26  1.14
54.0210.63
48.524-  6.32
36.27  4.14

TFIDF

46.949.80
6.181.33
24.614.65
25.913.64

hand-tuned
28.877.39
13.763.33
6.040.92
16.222.72

Table 2:  Performance comparison for  three  retrieval
functions  as  of  March 12,  1996. Lower numbers indicate  better
performance.  Rows correspond  to  the  indexing  method  used  by  LASER at  query  time;  columns  hold  values  from
subsequent  evaluation  with  other  methods.  Figures  reported  are  means   two  standard  errors
(95% confidence
intervals).

its  minimum; the  parameter  setting
the  final  answer from.our  optimization  run.

thus  chosen  is

Each evaluation  of  Perf(ff)  on a  new set  of  parame-
ters  is  quite  expensive, since  it  involves one call  to  the
search  engine  for  each  query  in  Q.  These evaluations
could  be  sped  up  if  Q were  subsampled  randomly  on
each call  to  Perf;  however, this  adds noise  to  the  eval-
uation.  We are  investigating  the  use of  stochastic  opti-
mization  techniques,  which are  designed  for  optimiza-
tion  of  just  this  type  of  noisy  and expensive objective
function  (Moore & Schneider  1996).

4  Empirical

Results

indexes

consisting

The  system  currently

since  February  14,
LASER has  been  in  operation
a  docu-
1996.
of  about  30,000  pages
ment  database
by  the  CMU Computer  Science  Depart-
served
The  sys-
ment  web server,
CMU com-
tem  is  available
home page,
munity  from  the  departments
http://www, cs. cmu. edu/Web/SCS-HOME, html. (We
are  considering  plans  for  larger  indexes  and wider re-
lease.)

,,,.
cs.cmu.edu.
for  use  by  the
local

Validity

of  Performance  Measure

ran  an  experiment  to  determine  whether  our

4.1
We first
performance function  could  really  measure significant
differences  between search engines,  based only on unin-
trusive  user  feedback.  We manually constructed
three
retrieval  functions:

simple-count

scores  a  document  by  counting

the

number of  query  terms  which appear  in  it;

standard-TFIDF

captures  word  relevance  much
better  but  does  not  take  advantage  of  HTML struc-
ture;  and

hand-tuned  includes  manually-chosen  values  for  all
func-

18  parameters  of  our  HTML-specific retrieval
tion.

From February  14  through  March  12,  we operated
LASER in  a  mode where  it  would randomly  select  one
of  these  three  retrieval  functions  to  use  for  each query.
During  this  time  LASER answered a  total  of  1400 user
queries  (not  including  queries  made by its  designers).
For about  half  these  queries,  the  user  followed one or
more of  the  suggested  documents.

We evaluated  Perf(f)

for  each  engine  according  to
shown  in  the  bottom  row
Equation  2.  The  results,
that  our  performance metric  does
of  Table  2,  indicate
the  three  ranking  functions:  hand-
indeed  distinguish
tuned  is  significantly  better  than  TFIDF, which in  turn
is  significantly  better  than  simple-count.

The first

three  rows of  Table 2  break  down the  per-

to  which  retrieval
formance  measurement  according
function  generated  the  original
ranking  seen  by  the
user.  The presentation  bias  is  clear:  the  diagonal  en-
tries  are  by  far  the  smallest.  Note that  the  diagonal
entries  are  not  significantly  different  from one another
with the  quantity  of  data  we had collected  at  this  point.
However, we do see  significant  differences  when we

average  down the  columns to  produce  our  full  perfor-
mance measurements  in  row four.  Moreover,  ranking
the  three  methods according  to  these  scores  produces
the  expected  order.  We take  this  as  evidence
that
our  performance  measure  captures
to  some extent  the
"goodness"  of  a  retrieval
function  and can serve  as  a
reasonable  objective  function  for  optimization.

4.2

Optimization

Results

To date,  we have  had  time  to  run  only  one optimiza-
tion  experiment,  so  the  results  of  this  section  should
be  considered  preliminary.  Our goal  was to  minimize
ranking
Perf(ff),
function.

thereby  producing  a  new and better

For efficiency  in  evaluating  Perf,  we let  Q be a  fixed
subsample of  150 queries  from our  full  query database,
50  from each  presentation  bias.  To make the  search
space  more tractable,  we optimized  over  only  a  10-
dimensional  projection  of  the  full  18-dimensional  pa-
allowed  for
rameter  space.  These 10  parameters  still

20

18

16

14

12

e-
,m

(31.

i

simulated annealing --
best-so-far" points -e--.

holdout set +

+ +

+

+

0

1 O0

++

+

I

200

+
+

+

+  +
+

+

I

Evaluations

300

+  +

+

I

400

500

Figure  2:  Optimization  of  search  engine  performance  by simulated  annealing.  Evaluations  on  a  separate  holdout
set  are  used to  prevent overfitting.

tuning  of  such heuristics  as  title  and heading bonuses,
query-word  adjacency  bonus,  partial-match
document length  penalty,  near-top-of-page  bonus,  and
7,  our  hypertext  discount  factor.

penalty,

As described  above,  we ran  simulated  annealing  to
find  a  new, optimal  set  of  retrieval
function  parame-
ters.  Simulated annealings  own parameters  were set  as
i/2,
follows:  temperature at  evaluation  #i  = 10.0    0.95
stepsize  = 10% of  the  legal  range  for  each
and initial
dimension.  This  run  converged  after  about  500  eval-
uations  of  Perf(ff)
(see  Figure  2).  Using  the  early-
stopping  technique,  we chose the  parameter  setting  at
evaluation  #312 as  our  final  answer.

Compared to  our  hand-tuned  parameter  setting,

the

learned  parameter  setting  gave  more weight  to  title
and  words  near  the
words,  underlined  anchor  text,
beginning  of  a document. Surprisingly,
it  set  3  (our
graph-propagation  discount  factor)  almost  to  0.  In-
stalling
function  into  our  search  en-
gine  interface,  we found it  produced qualitatively  good
rankings (e.g.,  refer  back to  Table 1).

the  new retrieval

From  March  26  through  May 6,  LASER generated

function  half  the
its  rankings  with  the  new retrieval
function  half
time  and with  our  hand-tuned  retrieval
the  time.  The cumulative  results
are  shown in  Ta-
ble  3.  According  to  our  overall  performance  metric,
functions  both
the  hand-tuned  and  learned  retrieval

significantly
differ  significantly  from one another.

outperform  count  and  TFIDF, but  do not

function

However, the  diagonal  entries,  which reflect
tual  use  of  the  system,  provide  some indication
is  an  improvement:  with  88%
the  learned
functions  value  of
confidence,  the  learned  retrieval
4.87  -4-  0.56  is  better
than  our  hand-tuned  functions
value of  5.33 -4-  0.57.  If  this  trend continues, we will  be
satisfied
learned  a  new and
better  ranking  scheme.

that  we have successfully

ac-
that

Work

5  Related
Many retrieval
engines  have  been  developed  to  index
World  Wide  Web pages.  Descriptions
of  some  can
be  found  in  (Mauldin  &5 Leavitt  1994)  and  (Pinker-
ton  1994).  These  retrieval
engines  make use  of  the
internal
structure  of  documents,  but  they  do  not  in-
corporate  hyperlinks.  Other  researchers  have  focused
on  retrieval  using  hypertext  structure  without  making
use  of  the  internal  structure  of  documents (Savoy 1991;
Croft  & Turtle  1993).

Automatic  parameter  optimization  was  previously
proposed  by  (Fuhret  al.  1994)  as  well  as  (Bartell,
Cottrell,  & Belew 1994).  Both approaches  differ
from
LASERs in  that  they  use  real  relevance  feedback data.
LASER does  not  require
feedback  assign-
it  uses  more noisy  data  which can
ment by  the  user;

relevance

$ f  used for  presentation
count
TFIDF
hand-tuned
learned
Overall  Performance

count

6.33+  1.13
55.43+10.32
50.55+  4.68
47.36+  4.99
39.92+  3.11

TFIDF

48.19+9.90
6.05+1.25
21.34+2.98
13.14+2.21
22.18+2.66

hand-tuned
3O.62+ 7.75
13.31+3.14
5.33+  0.57
7.22+  0.95
14.12-4-2.11

learned

32.60+7.97
8.222.12
8.95+1.58
4.87+0.56
13.66+2.10

Table  3:  Cumulative performance comparison for  four  retrieval
in  the  same format  as  in  Table 2.

functions  as  of  May 6,  1996.  The data  is  reported

be collected  unintrusively  by observing users  actions.

6  Conclusions

and  Future  Work

results

from  LASER are  promising.  We have

an  index  which  takes  advantage  of  HTML

Initial
shown that  unintrusive  feedback can  provide sufficient
information  to  evaluate  and optimize  the  performance
of  a retrieval
function.  According to  our  performance
metric,
structure  outperforms a  more traditional
results  demon-
Furthermore,  we have  begun to  collect
strating
that  it  is  possible  to  automatically  improve a
retrieval  function  by learning  from user  actions,  with-
out recourse  to  the  intrusive  methods of  relevance  feed-
back.

"flat"

index.

There  are  many directions

for  further

research,

which we see  as  falling  into  three  general  areas:

retrieval

function  parametrization  LASER cur-

into  a  retrieval

rently  offers  18  tunable  parameters  for  combining
heuristics
function,  but  certainly
many other  heuristics
are  possible.  For  example,
we would like  to  further  refine  our  method for  in-
corporating  hyperlinks.  We are  also  planning  to  in-
clude  per-document popularity  statistics,
gathered
from regular  LASER usage,  into  the  relevance  func-
tion.
users,  the  system should  learn  to  punish  that  docu-
ment in  the  rankings.

If  a  document  is  always  skipped  by  LASER

from the  probabilistic

evaluation  metrics  While  our  performance  function
and agrees  with  our
has  an  appealing  simplicity,
qualitative
judgments on the  three  search  engines  of
Table 2,  we cannot  defend it  on theoretical  grounds.
A metric  derived  directly
ranking  principle
(van  Rijsbergen  1979),  for  exam-
ple,  would allow  us  to  make stronger  claims  about
our  optimization  procedure.  Another alternative
is
to  implement a  cost  function  over  rankings,  where
the  cost  increases  with the  number of  irrelevant  links
(i.e.,
those  which the  user  explicitly  skipped over)
high  in  the  ranking.  It  is  not  clear  whether this  is
a  useful  metric,  or  even  how to  decide  among these
alternatives.

issue,  we have  documented  a  pro-
On a  related
nounced tendency  for  users  to  select
links  that  are
high  in  the  rankings,  no matter  how poor  the  index,
resulting
in  "presentation  bias."  This  complicates
the  problem of  evaluating  new retrieval
functions  of-
fline  during  optimization,  since  our  query database
will  strongly  bias  the  retrieval  parameters  toward
those  used  for  the  original  presentation.  We have an
ad  hoc method for  compensating for  this  effect,  but
would be interested
in  more principled  approaches.

its

optimization  As mentioned in  Section  3.2,  we plan  to
investigate
the  use  of  stochastic  optimization  tech-
niques,  in  place  of  simulated  annealing,  for  optimiz-
ing the  parameter settings.  There is  also  an interest-
ing possibility
for  "lifetime  learning."  We would like
to  see  how the  system improves over time,  iteratively
replacing
index  with  a  new and  improved  one
learned  from user  data.  We can  only  speculate  about
the  trajectory
the  system might  take.  There  is  the
possibility  of  an interesting  kind of  feedback between
the  system and its  users;  as  the  system changes  its
indexing  behavior,  perhaps  the  users  of  the  system
will  change  their  model  of  it  and  use  it  somehow
Is  there  a  globally  optimal
differently  from at  first.
parameter  setting
for  LASER? It  may be  that,  given
the  presentation  bias  and the  possibility  of  drifting
patterns  of  use,  its  parameters  would never  settle
into  a stable  state.

Acknowledgments

to  thank  Tom Mitchell

We would  like
and  Andrew
Moore for  the  computational  and  cognitive  resources
they  shared  with  us  for  these  experiments.  Thanks,
too,  to  Darrell  Kindred for  counseling  us  on indexing
the  local  Web, and to  Michael  Mauldin  for  authoring
the  Scout retrieval  engine which we used  as  a  basis  for
our  own. Finally,  we acknowledge the  support  of  NSF
Grant  IRI-9214873.

Appendix  A  Parametric  Form of  Retrieval

Function

rsvt+ 1 (q, d)

=  rsvo(q,d)

+  $gamma

dt  Elinks (d)

rsvt (q, p)
Ilinks(d) *nu

rsv0(d,q)= multihit(q,d).  ~ ~[q, = dr]"

lql  Idl

i=1

j=l

qweight(i,
Iql

qi,

d j)

dweight(j, qi,  d j)
  idl,doc,~_~xp   adjacency(qi_x,dr_,)

qweight(i,  qi,  dj) = -

1  $query_pos_exp

  idf(q~).

(1  + isfullmatch(qi,

dj).  $fullmatch_factor

dweight(j, dj) = idf(dj) .  (1 +in-hl_headline(dj).$hl_factor

+ispartmatch( qi  ,  r)   Spartmatch_f actor )

+in_h2_headline(d3 )  .  $h2_f  actor
+in_h3.headline( dj  )  .  $h3_f actor
+in_title(dr)    Stifle_factor
+in_bold( dj  )    $bold_f actor
+in_italics(d r)    $italics_f  actor
-t-in_blink(d r)    $blink_f  actor
+in_anehor( dj  )  .  $anchor_f actor

Stoppage_factor

+ log(j  + Stoppage_add) )

adjacency(qi-1,  dj-l

multihit(q,  d)

= [q~-i  ~  d3_,]  + [qi-,  = dj_,].  $adjacency_factor
= (number_of_words_in_q_that_occur_in_d)

Sm~t"hit-~*p

