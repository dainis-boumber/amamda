Abstract

The Google search engine has enjoyed huge success with its web page
ranking algorithm, which exploits global, rather than local, hyperlink
structure of the web using random walks. Here we propose a simple
universal ranking algorithm for data lying in the Euclidean space, such
as text or image data. The core idea of our method is to rank the data
with respect to the intrinsic manifold structure collectively revealed by a
great amount of data. Encouraging experimental results from synthetic,
image, and text data illustrate the validity of our method.

1

Introduction

The Google search engine [2] accomplishes web page ranking using PageRank algorithm,
which exploits the global, rather than local, hyperlink structure of the web [1]. Intuitively,
it can be thought of as modelling the behavior of a random surfer on the graph of the web,
who simply keeps clicking on successive links at random and also periodically jumps to
a random page. The web pages are ranked according to the stationary distribution of the
random walk. Empirical results show PageRank is superior to the naive ranking method, in
which the web pages are simply ranked according to the sum of inbound hyperlinks, and
accordingly only the local structure of the web is exploited.
Our interest here is in the situation where the objects to be ranked are represented as vectors
in Euclidean space, such as text or image data. Our goal is to rank the data with respect
to the intrinsic global manifold structure [6, 7] collectively revealed by a huge amount of
data. We believe for many real world data types this should be superior to a local method,
which rank data simply by pairwise Euclidean distances or inner products.
Let us consider a toy problem to explain our motivation. We are given a set of points
constructed in two moons pattern (Figure 1(a)). A query is given in the upper moon, and the
task is to rank the remaining points according to their relevances to the query. Intuitively,
the relevant degrees of points in the upper moon to the query should decrease along the
moon shape. This should also happen for the points in the lower moon. Furthermore, all
of the points in the upper moon should be more relevant to the query than the points in the
lower moon. If we rank the points with respect to the query simply by Euclidean distance,
then the left-most points in the lower moon will be more relevant to the query than the
right-most points in the upper moon (Figure 1(b)). Apparently this result is not consistent
with our intuition (Figure 1(c)).
We propose a simple universal ranking algorithm, which can exploit the intrinsic manifold

(a) Two moons ranking problem

(b) Ranking by Euclidean distance

(c) Ideal ranking

query

Figure 1: Ranking on the two moons pattern. The marker sizes are proportional to the
ranking in the last two (cid:2)gures. (a) toy data set with a single query; (b) ranking by the
Euclidean distances; (c) ideal ranking result we hope to obtain.

structure of data. This method is derived from our recent research on semi-supervised learn-
ing [8]. In fact the ranking problem can be viewed as an extreme case of semi-supervised
learning, in which only positive labeled points are available. An intuitive description of our
method is as follows. We (cid:2)rst form a weighted network on the data, and assign a positive
ranking score to each query and zero to the remaining points which are ranked with respect
to the queries. All points then spread their ranking score to their nearby neighbors via the
weighted network. The spread process is repeated until a global stable state is achieved,
and all points except queries are ranked according to their (cid:2)nal ranking scores.
The rest of the paper is organized as follows. Section 2 describes the ranking algorithm in
detail. Section 3 discusses the connections with PageRank. Section 4 further introduces a
variant of PageRank, which can rank the data with respect to the speci(cid:2)c queries. Finally,
Section 5 presents experimental results on toy data, on digit image, and on text documents,
and Section 6 concludes this paper.

2 Algorithm

Given a set of point X = fx1; :::; xq; xq+1; :::; xng (cid:26) Rm; the (cid:2)rst q points are the queries
and the rest are the points that we want to rank according to their relevances to the queries.
Let d : X (cid:2) X (cid:0)! R denote a metric on X , such as Euclidean distance, which assigns
each pair of points xi and xi a distance d(xi; xj): Let f : X (cid:0)! R denote a ranking
function which assigns to each point xi a ranking value fi: We can view f as a vector
f = [f1; ::; fn]T : We also de(cid:2)ne a vector y = [y1; ::; yn]T , in which yi = 1 if xi is a
query, and yi = 0 otherwise. If we have prior knowledge about the con(cid:2)dences of queries,
then we can assign different ranking scores to the queries proportional to their respective
con(cid:2)dences.
The algorithm is as follows:

1. Sort the pairwise distances among points in ascending order. Repeat connecting
the two points with an edge according the order until a connected graph is ob-
tained.

2. Form the af(cid:2)nity matrix W de(cid:2)ned by Wij = exp[(cid:0)d2(xi; xj)=2(cid:27)2] if there is
an edge linking xi and xj: Note that Wii = 0 because there are no loops in the
graph.

3. Symmetrically normalize W by S = D(cid:0)1=2W D(cid:0)1=2 in which D is the diagonal

matrix with (i; i)-element equal to the sum of the i-th row of W:

4. Iterate f (t + 1) = (cid:11)Sf (t) + (1 (cid:0) (cid:11))y until convergence, where (cid:11) is a parameter

in [0; 1):

5. Let f (cid:3)

i denote the limit of the sequence ffi(t)g: Rank each point xi according its

ranking scores f (cid:3)

i (largest ranked (cid:2)rst).

This iteration algorithm can be understood intuitively. First a connected network is formed
in the (cid:2)rst step. The network is simply weighted in the second step and the weight is
symmetrically normalized in the third step. The normalization in the third step is necessary
to prove the algorithms convergence. In the forth step, all points spread their ranking score
to their neighbors via the weighted network. The spread process is repeated until a global
stable state is achieved, and in the (cid:2)fth step the points are ranked according to their (cid:2)nal
ranking scores. The parameter (cid:11) speci(cid:2)es the relative contributions to the ranking scores
from neighbors and the initial ranking scores. It is worth mentioning that self-reinforcement
is avoided since the diagonal elements of the af(cid:2)nity matrix are set to zero in the second
step. In addition, the information is spread symmetrically since S is a symmetric matrix.
About the convergence of this algorithm, we have the following theorem:
Theorem 1 The sequence ff (t)g converges to f (cid:3) = (cid:12)(I (cid:0) (cid:11)S)(cid:0)1y; where (cid:12) = 1 (cid:0) (cid:11):
See also [8] for the rigorous proof. Here we only demonstrate how to obtain such a closed
form expression. Suppose f (t) converges to f (cid:3): Substituting f (cid:3) for f (t + 1) and f (t) in
the iteration equation f (t + 1) = (cid:11)Sf (f ) + (1 (cid:0) (cid:11))y; we have

f (cid:3) = (cid:11)f (cid:3) + (1 (cid:0) (cid:11))y;

(1)

which can be transformed into

Since (I (cid:0) (cid:11)S) is invertible, we have

(I (cid:0) (cid:11)S)f (cid:3) = (1 (cid:0) (cid:11))y:

f (cid:3) = (1 (cid:0) (cid:11))(I (cid:0) (cid:11)S)(cid:0)1y:

Clearly, the scaling factor (cid:12) does not make contributions for our ranking task. Hence the
closed form is equivalent to

f (cid:3) = (I (cid:0) (cid:11)S)(cid:0)1y:

(2)

We can use this closed form to compute the ranking scores of points directly. In large-scale
real-world problems, however, we prefer using iteration algorithm. Our experiments show
that a few iterations are enough to yield high quality ranking results.

3 Connections with Google

Let G = (V; E) denote a directed graph with vertices. Let W denote the n (cid:2) n adjacency
matrix W; in which Wij = 1 if there is a link in E from vertex xi to vertex xj; and Wij = 0
otherwise. Note that W is possibly asymmetric. De(cid:2)ne a random walk on G determined
by the following transition probability matrix

P = (1 (cid:0) (cid:15))U + (cid:15)D(cid:0)1W;

(3)
where U is the matrix with all entries equal to 1=n. This can be interpreted as a probability
(cid:15) of transition to an adjacent vertex, and a probability 1 (cid:0) (cid:15) of jumping to any point on the
graph uniform randomly. Then the ranking scores over V computed by PageRank is given
by the stationary distribution (cid:25) of the random walk.
In our case, we only consider graphs which are undirected and connected. Clearly, W is
symmetric in this situation. If we also rank all points without queries using our method, as
is done by Google, then we have the following theorem:

Theorem 2 For the task of ranking data represented by a connected and undirected graph
without queries, f (cid:3) and PageRank yield the same ranking list.
Proof. We (cid:2)st show that the stationary distribution (cid:25) of the random walk used in Google is
proportional to the vertex degree if the graph G is undirected and connected. Let 1 denote
the 1 (cid:2) n vector with all entries equal to 1. We have

1DP = 1D[(1 (cid:0) (cid:15))U + (cid:15)D(cid:0)1W ] = (1 (cid:0) (cid:15))1DU + (cid:15)1DD(cid:0)1W

= (1 (cid:0) (cid:15))1D + (cid:15)1W = (1 (cid:0) (cid:15))1D + (cid:15)1D = 1D:

Let vol G denote the volume of G; which is given by the sum of vertex degrees. The
stationary distribution is then

(cid:25) = 1D=vol G:

(4)
Note that (cid:25) does not depend on (cid:15): Hence (cid:25) is also the the stationary distribution of the
random walk determined by the transition probability matrix D(cid:0)1W:
Now we consider the ranking result given by our method in the situation without queries.
The iteration equation in the fourth step of our method becomes

(5)
A standard result [4] of linear algebra states that if f (0) is a vector not orthogonal to the
principal eigenvector, then the sequence ff (t)g converges to the principal eigenvector of
S. Let 1 denotes the n (cid:2) 1 vector with all entries equal to 1: Then

f (t + 1) = Sf (t):

SD1=21 = D(cid:0)1=2W D(cid:0)1=2D1=21 = D(cid:0)1=2W 1 = D(cid:0)1=2D1 = D1=21:

Further, noticing that the maximal eigenvalue of S is 1 [8], we know the principal eigen-
vector of S is D1=21: Hence

f (cid:3) = D1=21:

(6)

Comparing (4) with (6), it is clear that f (cid:3) and (cid:25) give the same ranking list. This completes
our proof.

4 Personalized Google

Although PageRank is designed to rank all points without respect to any query, it is easy to
modify for query-based ranking problems. Let P = D(cid:0)1W: The ranking scores given by
PageRank are the elements of the convergence solution (cid:25) (cid:3) of the iteration equation

(7)
By analogy with the algorithm in Section 2, we can add a query term on the right-hand side
of (7) for the query-based ranking,

(cid:25)(t + 1) = (cid:11)P T (cid:25)(t):

(cid:25)(t + 1) = (cid:11)P T (cid:25)(t) + (1 (cid:0) (cid:11))y:

(8)
This can be viewed as the personalized version of PageRank. We can show that the se-
quence f(cid:25)(t)g converges to (cid:25) (cid:3) = (1 (cid:0) (cid:11))(I (cid:0) (cid:11)P T )(cid:0)1y as before, which is equivalent
to

(cid:25)(cid:3) = (I (cid:0) (cid:11)P T )(cid:0)1y:

(9)

Now let us analyze the connection between (2) and (9). Note that (9) can be transformed
into

(cid:25)(cid:3) = [(D (cid:0) (cid:11)W )D(cid:0)1](cid:0)1y = D(D (cid:0) (cid:11)W )(cid:0)1y:

In addition, f (cid:3) can be represented as

f (cid:3) = [D(cid:0)1=2(D (cid:0) (cid:11)W )D(cid:0)1=2](cid:0)1y = D1=2(D (cid:0) (cid:11)W )(cid:0)1D1=2y:

(10)

Hence the main difference between (cid:25) (cid:3) and f (cid:3) is that in the latter the initial ranking score
yi of each query xi is weighted with respect to its degree.
The above observation motivates us to propose a more general personalized PageRank
algorithm,

(11)
in which we assign different importance to queries with respect to their degree. The closed
form of (11) is given by

(cid:25)(t + 1) = (cid:11)P T (cid:25)(t) + (1 (cid:0) (cid:11))Dky;

(cid:25)(cid:3) = (I (cid:0) (cid:11)P T )(cid:0)1Dky:

(12)

If k = 0; (12) is just (9); and if k = 1; we have

(cid:25)(cid:3) = (I (cid:0) (cid:11)P T )(cid:0)1Dy = D(D (cid:0) (cid:11)W )(cid:0)1Dy;

which is almost as same as (10).
We can also use (12) for classi(cid:2)cation problems without any modi(cid:2)cation, besides setting
the elements of y to 1 or -1 corresponding to the positive or negative classes of the labeled
points, and 0 for the unlabeled data. This shows the ranking and classi(cid:2)cation problems
are closely related.
We can do a similar analysis of the relations to Kleinbergs HITS [5], which is another
popular web page ranking algorithm. The basic idea of this method is also to iteratively
spread the ranking scores via the existing web graph. We omit further discussion of this
method due to lack of space.

5 Experiments

We validate our method using a toy problem and two real-world domains: image and text.
In our following experiments we use the closed form expression in which (cid:11) is (cid:2)xed at 0:99:
As a true labeling is known in these problems, i.e.
the image and document categories
(which is not true in real-world ranking problems), we can compute the ranking error using
the Receiver Operator Characteristic (ROC) score [3] to evaluate ranking algorithms. The
returned score is between 0 and 1, a score of 1 indicating a perfect ranking.

5.1 Toy Problem

In this experiment we considered the toy ranking problem mentioned in the introduction
section. The connected graph described in the (cid:2)rst step of our algorithm is shown in Figure
2(a). The ranking scores with different time steps: t = 5; 10; 50; 100 are shown in Figures
2(b)-(e). Note that the scores on each moon decrease along the moon shape away from the
query, and the scores on the moon containing the query point are larger than on the other
moon. Ranking by Euclidean distance is shown in Figure 2(f), which fails to capture the
two moons structure.
It is worth mentioning that simply ranking the data according to the shortest paths [7] on
the graph does not work well. In particular, we draw the readers attention to the long edge
in Figure 2(a) which links the two moons. It appears that shortest paths are sensitive to
the small changes in the graph. The robust solution is to assemble all paths between two
points, and weight them by a decreasing factor. This is exactly what we have done. Note
that the closed form can be expanded as f (cid:3) = Pi (cid:11)iSiy:

5.2

Image Ranking

In this experiment we address a task of ranking on the USPS handwritten 16x16 digits
dataset. We rank digits from 1 to 6 in our experiments. There are 1269, 929, 824, 852, 716
and 834 examples for each class, for a total of 5424 examples.

(a) Connected graph

Figure 2: Ranking on the pattern of two moons. (a) connected graph; (b)-(e) ranking with
the different time steps: t = 5; 10; 50; 100; (f) ranking by Euclidean distance.

1

0.998

C
O
R

0.996

0.994

0.992

0.99

C
O
R

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

(a) Query digit 1

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

(d) Query digit 4

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

C
O
R

C
O
R

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

(b) Query digit 2

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

(e) Query digit 5

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

C
O
R

C
O
R

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

(c) Query digit 3

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

(e) Query digit 6

2

4

Manifold ranking
Euclidean distance
6
# queries

8

10

Figure 3: ROC on USPS for queries from digits 1 to 6. Note that this experimental results
also provide indirect proof of the intrinsic manifold structure in USPS.

Figure 4: Ranking digits on USPS. The top-left digit in each panel is the query. The left
panel shows the top 99 by the manifold ranking; and the right panel shows the top 99 by
the Euclidean distance based ranking. Note that there are many more 2s with knots in the
right panel.

We randomly select examples from one class of digits to be the query set over 30 trials,
and then rank the remaining digits with respect to these sets. We use a RBF kernel with
the width (cid:27) = 1:25 to construct the af(cid:2)nity matrix W; but the diagonal elements are set to
zero. The Euclidean distance based ranking method is used as the baseline: given a query
set fxsg(s 2 S), the points x are ranked according to that the highest ranking is given to
the point x with the lowest score of mins2Skx (cid:0) xsk:
The results, measured as ROC scores, are summarized in Figure 3; each plot corresponds
to a different query class, from digit one to six respectively. Our algorithm is comparable
to the baseline when a digit 1 is the query. For the other digits, however, our algorithm
signi(cid:2)cantly outperforms the baseline. This experimental result also provides indirect proof
of the underlying manifold structure in the USPS digit dataset [6, 7].
The top ranked 99 images obtained by our algorithm and Euclidean distance, with a random
digit 2 as the query, are shown in Figure 4: The top-left digit in each panel is the query.
Note that there are some 3s in the right panel. Furthermore, there are many curly 2s in
the right panel, which do not match well with the query: the 2s in the left panel are more
similar to the query than the 2s in the right panel. This subtle superiority makes a great
deal of sense in the real-word ranking task, in which users are only interested in very few
leading ranking results. The ROC measure is too simple to re(cid:3)ect this subtle superiority
however.

5.3 Text Ranking

In this experiment, we investigate the task of text ranking using the 20-newsgroups dataset.
We choose the topic rec which contains autos, motorcycles, baseball and hockey from the
version 20-news-18828.
The articles are processed by the Rainbow software package with the following options:
(1) passing all words through the Porter stemmer before counting them; (2) tossing out
any token which is on the stoplist of the SMART system; (3) skipping any headers; (4)
ignoring words that occur in 5 or fewer documents. No further preprocessing was done.
Removing the empty documents, we obtain 3970 document vectors in a 8014-dimensional
space. Finally the documents are normalized into TFIDF representation.
We use the ranking method based on normalized inner product as the baseline. The af(cid:2)nity
matrix W is also constructed by inner product, i.e. linear kernel. The ROC scores for 100
randomly selected queries for each class are given in Figure 5.

0.9

0.8

0.7

0.6

0.5

0.4

t
c
u
d
o
r
p

r
e
n
n

i

0.3

0.3

0.4

0.9

0.8

0.7

0.6

0.5

0.4

t
c
u
d
o
r
p

r
e
n
n

i

0.3

0.3

0.4

(a) autos

(b) motorcycles

0.9

0.8

0.7

0.6

0.5

0.4

t
c
u
d
o
r
p

r
e
n
n

i

0.5

0.6

0.7

manifold ranking

0.8

0.9

0.3

0.3

0.4

0.5

0.6

0.7

manifold ranking

(c) baseball

(d) hockey

0.9

0.8

0.7

0.6

0.5

0.4

t
c
u
d
o
r
p

r
e
n
n

i

0.5

0.6

0.7

manifold ranking

0.8

0.9

0.3

0.3

0.4

0.5

0.6

0.7

manifold ranking

0.8

0.9

0.8

0.9

Figure 5: ROC score scatter plots of 100 random queries from the category autos, motor-
cycles, baseball and hockey contained in the 20-newsgroups dataset.

6 Conclusion

Future research should address model selection. Potentially, if one was given a small la-
beled set or a query set greater than size 1, one could use standard cross validation tech-
niques. In addition, it may be possible to look to the theory of stability of algorithms to
choose appropriate hyperparameters. There are also a number of possible extensions to
the approach. For example one could implement an iterative feedback framework: as the
user speci(cid:2)es positive feedback this can be used to extend the query set and improve the
ranking output. Finally, and most importantly, we are interested in applying this algorithm
to wide-ranging real-word problems.

