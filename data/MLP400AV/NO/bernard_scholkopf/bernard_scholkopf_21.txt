Abstract

In this paper a correspondence is derived between regularization operators used in regularization networks and support vector kernels. We
prove that the Greens Functions associated with regularization operators are suitable support vector kernels with equivalent regularization
properties. Moreover, the paper provides an analysis of currently used support vector kernels in the view of regularization theory and
corresponding operators associated with the classes of both polynomial kernels and translation invariant kernels. The latter are also analyzed
on periodical domains. As a by-product we show that a large number of radial basis functions, namely conditionally positive denite
functions, may be used as support vector kernels. q 1998 Elsevier Science Ltd. All rights reserved.

Keywords: Support vector machines; Mercer kernel; Regularization networks; Ridge regression; Greens functions; Conditionally positive
denite functions; Polynomial kernels; Radial basis functions

Nomenclature

i  Lagrange multipliers and expansion

p
p
i , b i, b

R  set of real numbers
C  set of complex numbers
N  set of integers
x  (lowercase latin) scalars
x  (boldface) elements of Rn
a i, a
coefcients
h..i  dot product in Hilbert space
k.k  norm, induced by a dot product
f  functions
f  complex conjugate (of a function or a scalar)
f  Fourier transform of f
F  feature space
F,F(x)  elements and mappings into F
P  operators
D, Dij  matrices, matrix elements
dx0(x), d ij  delta distribution, Kronecker delta
(li, f i), (Li, W i)  (eigenvector, eigenvalue) pairs

Y
X

n
i  1  product
#n
i  1  convolution
n
i  1  summation

* Corresponding author. Tel: (++49)-30-682-1833; Fax: (++49)-30-682-
1805; E-mail: smola@rst.gmd.de. URL: http://svm.rst.gmd.de.

0893-6080/98/$19.00 q 1998 Elsevier Science Ltd. All rights reserved.
PII S0893-6080(98)00032-X

1 I  indicator function on a set I
1  identity map
~1  vector with all entries equal to 1

1. Introduction

Support vector (SV) machines for pattern recognition,
regression estimation, and operator inversion exploit the
idea of mapping data into a high dimensional feature
space where they perform a linear algorithm. Instead of
evaluating this mapping explicitly, one uses integral opera-
tor kernels k(x, y) which correspond to dot products of the
mapped data in high dimensional space, Aizerman et al.
(1964); Boser et al. (1992), i.e.
k(x, y) hF(x)F(y)i
(1)
with F: R n ! F denoting the map into feature space F.
Mostly, this map and many of its properties are unknown.
Even worse, so far no general rule was available which
kernel should be used, or why mapping into a very high
dimensional space often provides good results, seemingly
defying the curse of dimensionality. In order to clarify this
dilemma we show how these kernels k(x, y) correspond to
regularization operators P, the link being that k is the
p denoting the adjoint
Greens function of
operator of P). In other words  given a support vector

p P (with P

P

638

A.J. Smola et al. / Neural Networks 11 (1998) 637649

kernel we show how to nd the corresponding regulariza-
tion operator and vice versa. For the sake of simplicity, we
shall limit ourselves to the case of regression  our con-
siderations, however, also hold true for the other cases
mentioned earlier.

This paper1 starts by briey reviewing the concepts of SV
Machines (Section 2) and regularization networks (Section
3). Section 4 contains the main result, the derivation of a
correspondence between regularization operators used in
regularization networks and SV kernels. In Section 5 appli-
cations of this nding to translation invariant kernels for
both unbounded and bounded support are presented. Section
6 presents the operators corresponding to polynomial
kernels, another frequently used class of SV kernels. Sub-
sequently Section 7 introduces a new class of possible SV
kernels which do not necessarily satisfy Mercers condition,
namely kernels derived from conditionally positive denite
functions. Section 8 concludes the paper with a discussion.
Finally Appendix A contains a worked through example and
Appendix B applies the methods presented in this paper to
nd a connection between ridge regression and SV
machines. Due to its specic setting, however, only a less
formal exposition is possible.

2. Support vector machines

The SV algorithm for regression estimation, as described
in Vapnik (1995); Vapnik et al. (1997), exploits the idea of
computing a linear function in high dimensional feature
space F (furnished with a dot product). Thereby this algo-
rithm can compute a nonlinear function in the space of the
input data R n. The functions take the form
f (x) hwF(x)i  b
(2)
with F:R n ! F being the map into feature space and w [ F.
from a given training set
In order
{(xi,yi)li  1,,l, xi [ Rn,y i [ R}, one tries to minimize
the regularized risk functional

to estimate f

Rreg[f ]  Remp[f ]  l
2

kwk2 

1
l

c(f (xi), yi)  l
2

kwk2

(3)
i.e. the empirical risk functional R emp[f] together with a
complexity term kwk2, thereby enforcing atness in feature
space. Here c(f(x i),y i) is the cost function determining how
the distance between f(x i) and the target values y i should be
penalized, and l [ R is a regularization constant. The idea
of atness is derived from pattern recognition where this
corresponds to nding a hyperplane that has maximum dis-
tance in F from the classes to be separated Boser et al.
(1992); Cortes and Vapnik (1995). As shown in Vapnik

1 Portions of this work have been published in Smola and Scholkopf

(1998).

Xl

i  1

(

(1995) for the case of e-insensitive cost functions,

c(f (x), y) 

lf (x)  yl  e

for lf (x)  yl $ e

0

otherwise

(4)

Eq. (3) can be minimized by solving a quadratic program-
ming problem formulated in terms of dot products in F. It
turns out that the solution w can be expressed in terms of
support vectors. Note that the representation can be sparse.
Therefore, the points corresponding to nonzero a i, which
sufce for describing f, are called support vectors.

Xl

i  1

Xl

i  1

w 

aiF(xi):

Therefore, via Eq. (1)
f (x) hwF(x)i  b 

Xl

i  1

aihF(xi)F(x)i  b



aik(xi, x)  b

(5)

6

Xl

1
2

i, j  1

Xl

i  1

((b

Xl

where k(x i, x) is a kernel function computing a dot product
in feature space (a concept introduced by Aizerman et al.,
1964). The coefcients a i can be found by solving a
quadratic programming problem (with Kij:  k(x i, x j),
p
ai  bi  b
i being the solution of the optimiza-
tion problem below):

p
i and b i, b

minimize

(b

p

i  bi)(b

p

j  bj)Kij



p

i  bi)yi  (b

p

i  bi)e)

subject to

i  1

(bi  b

p

i )  0, bi; b

(cid:20)
i [ 0, 1
ll

p

(cid:21)

7

Note that Eq. (4) is not the only possible choice of cost
functions resulting in a quadratic programming problem
(many convex cost function, in particular quadratic parts
and innities are admissible, too). For a detailed discussion
see Smola and Scholkopf (1997); Smola et al. (1998). Also
note that any continuous symmetric function k(x, y) [ L2 #
L 2 may be used as an admissible kernel if it satises a weak
form of Mercers condition (Riesz and Nagy, 1955)

ZZ

k(x, y)g(x)g(y)dxdy $ 0 for all g [ L2(Rn)

(8)

3. Regularization networks

Here again we start with minimizing the empirical risk
functional R emp[f] plus a regularization term k Pf k2 dened
by a regularization operator P in the sense of Tikhonov and
Arsenin (1977), i.e. P is a positive semidenite operator

A.J. Smola et al. / Neural Networks 11 (1998) 637649

639

mapping from the Hilbert Space H of functions f under
consideration to a dot product space D such that the expres-
sion h Pf Pgi is well dened. For instance by choosing a
suitable operator that penalizes large variations of f one
can reduce the well-known overtting effect. Another
possible setting also might be an operator P mapping from
L2(R n)
space
(Kimeldorf and Wahba, 1971; Girosi, 1997). In Appendix
A, we provide a worked through example (mainly taken
from Girosi et al., 1993) for a simple regularization operator
to illustrate our reasoning.

into some reproducing kernel Hilbert

Similar to Eq. (3), we minimize

Rreg[f ]  Remp  l
2

k Pf k2 

1
l

c(f (xi), yi)  l
2

k Pf k2

(9)

Xl

i  1

Using an expansion of f in terms of some symmetric func-
tion k(x i, x j) (note here, that k need not fulll Mercers
condition),
f (x) 

aik(xi, x)  b,

X

(10)

i

and the cost function dened in Eq. (4), this leads to a
quadratic programming problem similar to the one for
SVs. By computing Wolfes dual (for details of the calcu-
lations see Smola and Scholkopf, 1997), and using
Dij : h( Pk)(xi, :)( Pk)(xj, :)i
(11)
(hfgi denotes the dot product of the functions f and
get
in Hilbert
g
~a  D  1K(~b  ~b

e.g.
Space,
p), with bi, b
p
i being the solution of

f (x)g(x)dx), we

Z

minimize

(b

p

i  bi)(b

p

j  bj)KD  1K)ij

Xl

1
2

i, j  1

Xl

Xl

i  1



((b

p

i  bi)yi  (b

p

i  bi)e)

subject to

i  1

(bi  b

p

i )  0, bi; b

(cid:20)
i [ 0, 1
ll

p

(cid:21)

:

12

Unfortunately, this setting of the problem does not preserve
sparsity in terms of the coefcients, as a potentially sparse
p
is spoiled by D -1K,
decomposition in terms of bi and b
i
which in general is not diagonal (Eq. (6), on the other
hand, does typically have many vanishing coefcients, see
e.g. Scholkopf et al., 1995; Vapnik, 1995).

4. The relation between both methods

Comparing Eq. (7) with Eq. (12) leads to the question if
and under which condition the two methods might be
equivalent and,
therefore, also under which conditions,
given a suitable cost function, regularization networks

might lead to sparse decompositions (i.e. only a few of the
expansion coefcients a i in f would differ from zero). A
sufcient2 condition is D  K (thus KD1K  K), i.e.
k(xi, xj) h( Pk)(xi, :)( Pk)(xj, :)i (self consistency):
This is the main equation of this paper. Our goal now is to
solve the following two problems:

(13)

1. Given a regularization operator P, nd a kernel k such
that a SV machine using k will not only enforce atness
in feature space, but also correspond to minimizing a
regularized risk functional with P as regularization
operator;

2. Given a SV kernel k, nd a regularization operator P
such that a SV machine using this kernel can be viewed
as a regularization network using P.

These two problems can be solved by employing the con-
cept of Greens functions as described in Girosi et al. (1993).
These functions had been introduced in the context of solving
differential equations. For our purpose, it is sufcient to know
that the Greens functions Gxi
( P
)(x)  dxi
p PGxi
(14)
(x) is the d-distribution (not to be confused with the
Here, dxi
i 
Kronecker symbol d ij) which has the property that hfdxi
f(x i). The relationship between kernels and regularization
operators is formalized in the following proposition:

p P satisfy

(x) of P

(x)

Proposition 1 (Greens functions and Mercer kernels).
Let P be a regularization operator, and G be the Greens
p P. Then G is a Mercer kernel such that D  K.
function of P
SV machines using G minimize risk functional Eq. (9) with P
as regularization operator.3

(xi) hGxj

(:)dxi

(:)i

Proof. Substituting Eq. (14) into Gxj
yields

(xi) h( PGxi

)(:)( PGxj

)(:)i

(15)
Gxj
hence G(x i,x j):  Gxi(x j) is symmetric and satises Eq. (13).
Thus the SV optimization problem Eq. (7) is equivalent to
the regularization network counterpart Eq. (12). Further-
more, G is an admissible non-negative kernel, as it can be
written as a dot product in Hilbert space, namely

G(xi, xj) hF(xi)F(xj)i with F : xi (cid:176) ( PGxi(:):

(16)

B

A similar result can be obtained by exploiting Mercers
theorem in a more straightforward manner, by using the fact
that a Mercer kernel k can be expanded into a convergent

2 In the case of K not having of full rank D is only required to be the
inverse on the image of K. The pseudoinverse for instance is such a matrix.
3 This condition is sufcient but not necessary for satisfying Eq. (13). Any
projection of G onto an invariant subspace of P
P would also satisfy this
equation. Note that as G(.,.) being a function on R n # R n the projection
operator has to be applied to it as a function of both the rst and the second
argument.

p

X

series of its eigensystem (f l(x),l l) with ll $ 0,
k(xi, xj) 

(xi)fl

(xj)

ll fl

(17)

l

This is particularly useful for the approximation of period-
ical functions and will come handy in example 6 as we will
have to deal with a discrete eigensystem in this case.

Proposition 2 (a discrete counterpart). Given a regular-
p P into a discrete
ization operator P with an expansion of P
eigensystem (L l, W l) and a kernel k with
k(xi, xj) 

X

(xi)Wl

(xj)

(18)

Wl

dl
Ll

l

where dl [ {0, 1} for all l, and o l (d l /Ll) convergent. Then k
satises Eq. (13).

!



+

X

Proof. Evaluating Eq. (13) and using orthonormality of the
system (d l/L l, W l), yields:
*
X
hk(xi, :)( P
p Pk)(xj, :)i
dl
X
Ll
dl 9
dl
X
Ll
Ll 9
dl
Ll

dl 9
Wl 9(xj)Wl 9(:)
Ll 9
p PWl 9(:)i

(xi)Wl
(xi)Wl 9(xj)hWl

(xj)  k(xi, xj)

(xi)Wl

(:) P

(:) P

19

p P

l , l 9

Wl

Wl

Wl







l 9

l

l

B

Rearranging of the summation coefcients is allowed as
the eigenfunctions are orthonormal and the series o l (d l /Ll)
converges. Consequently a large class of kernels can be
associated with a given regularization operator (and vice
versa) thereby restricting ourselves to some subspace of
the eigensystem of P

Excluding eigenfunctions of P

p P from the kernel expan-
sion effectively decreases the expressive power of the set of
approximating functions, i.e. we limit the capacity of the
system of functions. Removing low capacity (i.e. very at)
eigenfunctions from the expansion will have an adverse
effect, though, as the data will have to be approximated
by the higher capacity functions.

p P.4

In the following we will exploit this relationship in both
ways: to compute Greens functions for a given regulariza-
tion operator P and to infer the regularization operator from
a given kernel k.

Note that a similar reasoning can be applied to connect
ridge regression schemes with support vector kernels as
shown in Appendix B.

4 The intuition of this reasoning is that there exists a one to one corre-
spondence between kernels and regularization operators only on the image
of H under the integral operator ( Of)(x): 
k(x, y)f(y) dy, namely that O
O, however, the
and P
p P may take on an arbitrary form. In this case k
regularization operator P
still will fulll the self consistency condition.

p P are inverse to another. On the null space of

(cid:132)

640

A.J. Smola et al. / Neural Networks 11 (1998) 637649

5. Translation invariant kernels

Let us now more specically consider regularization
operators P that may be written as multiplications in Fourier
space
h Pf Pgi 

Z

(20)

1

f (w) g(w)
P(w) dw

(2p)n=2

Q

with f (w) denoting the Fourier transform of f(x), and
Pw  P  w real valued, nonnegative and converging
to 0 for lwl !  and Q  supp[P(w)]. Small values of
P(w) correspond to a strong attenuation of the correspond-
ing frequencies. Hence small values of P(w) for large w are
desirable since high frequency components of f correspond
to rapid changes in f. P(w) describes the lter properties of
p P  note that no attenuation takes place for P(w)  0 as
P
these frequencies have been excluded from the integration
domain.

For regularization operators dened in Fourier space by
Eq. (20) it can be shown by exploiting P(w)  P(  w) 
P(w) that

G(xi, x) 

1

(2p)n=2

eiw(xi  x)

P(w)dw

(21)

Z

Rn

is a corresponding Greens function satisfying translational
invariance, i.e.
G(xi, xj)  G(xi  xj) and G(w)  P(w)
For the proof, one only has to show that G satises Eq. (13).
This provides us with an efcient tool for analyzing SV
kernels and the types of capacity control they exhibit. In
fact the above is a special case of Bochners theorem (Boch-
ner, 1959) stating that the Fourier transform of a positive
measure constitutes a positive Hilbert Schmidt kernel.

Example 3 (Bq-splines). In Vapnik et al. (1997) the use of
B q-splines was proposed (see Fig. 1) as building blocks for
kernels, i.e.

k(x) 

Bq(xi)

(22)

Yn

i  1

with x [ R n. For the sake of simplicity, we consider the case
n  1. Recalling the denition (up to scaling factors) by
Unser et al. (1991)
Bq  #q  1 1[  0:5, 0:5]
we can utilize the above result and the FourierPlancherel
identity to construct the Fourier representation of the corre-
sponding regularization operator. Up to a multiplicative
constant, it equals
P(w)  k(w)  sinc

(cid:16) (cid:17)

(24)

(23)

(q  1) wi
2

This answers the question why only B-splines of odd order
are admissible although both even and odd order B-splines

A.J. Smola et al. / Neural Networks 11 (1998) 637649

641

Fig. 1. Left: B 3-spline kernel. Right: Fourier transform of the kernel.

converge to a Gaussian for q !  due to the law of large
numbers: The even ones have negative parts in the Fourier
spectrum (which would result in an amplication of the
corresponding frequency components). The zeros in k
that B q has only compact support
stem from the fact
[(q  1)/2, (q  1)/2]. By using this kernel we trade
reduced computational complexity in calculating f (we
only have to take points into account with kx i  x jk # c
from some limited neighborhood determined by c) for a
possibly worse
regularization
operator as it completely removes frequencies w p with
k(w p)  0.

performance

of

the

Example 4 (Gaussian kernels). Following the exposition
of Yuille and Grzywacz (1988) as described in Girosi et al.
(1993), one can see that for

Z

X

dx

m

k Pf k2 

j2m
m!2m

( Omf (x))2

(25)

with O2m  Dm and O2m  1  D=m, D being the Laplacian
and = the gradient operator, we get Gaussians kernels (see

Fig. 2)

k(x)  exp 



!

kxk2
2j2

(26)

P in terms of its Fourier properties,

Moreover, we can provide an equivalent representation
i.e. P(w) 
of
exp[(j 2kwk2)/2) up to a multiplicative constant. Training
a SV machine with Gaussian RBF kernels (Scholkopf et al.,
1997) corresponds to minimizing the specic cost function
with a regularization operator of type Eq. (25).

Recall that Eq. (25) means that all derivatives of f are
penalized (we have a pseudodifferential operator) to obtain
a very smooth estimate. This also explains the good per-
formance of SV machines in this case, as it is by no means
obvious that choosing a at function in some high dimen-
sional space will correspond to a simple function in low
dimensional space, as shown in example 5.

Gaussian kernels tend to yield good performance under
general smoothness assumptions and should be considered
especially if no additional knowledge of
the data is
available.

Fig. 2. Left: Gaussian kernel with standard deviation 0.5. Right: Fourier transform of the kernel.

642

A.J. Smola et al. / Neural Networks 11 (1998) 637649

Fig. 3. Left: Dirichlet kernel of order 10. Note that this kernel is periodical. Right: Fourier transform of the kernel.

Fig. 4. Left: Regression with a Dirichlet Kernel of order N  10. One can clearly observe the overtting. Right: regression of the same data with a Gaussian
Kernel of width j 2  1.

Example 5 (Dirichlet kernels). In Vapnik et al. (1997), a
class of kernels generating Fourier expansions was intro-
duced for interpolating data on R n,
k(x) 

sin(2N  1)x=2

(27)

factor spaces. Without loss of generality assume the period
to be 2p  consequently one gets translation invariance on
R/2p.

In the following we will show the consequences of this

setting for the operator dened in example 4.

sinx=2

X

this

kernel

N

construction,

(As in example 3 consider x [ R1 to avoid tedious notation.)
By
to
i   Ndi(w). A regularization operator with
P(w)  1=2
these properties, however, may not be desirable as it only
damps a nite number of frequencies (cf. Fig. 3) and leaves
all other frequencies unchanged which can lead to overt-
ting (Fig. 4).

corresponds

In some cases it might be useful

to approximate
periodical functions, e.g. functions dened on a circle.
This leads to the second possible type of translation
invariant5 kernel functions, namely functions dened on

5 Obviously dening translation invariant kernels on a bounded interval is
not a reasonable concept as the data would hit the bounds of the interval
when translated by a large amount. Therefore, only unbounded intervals
and factor spaces are possible domains.

Example 6 (periodical Gaussian kernels). Analogously
to Eq. (25), dene a regularization operator on functions
on [0, 2p] n by

k Pf k2  p

 n

j2m
m!2m

( Omf (x))2

(28)

Z

X

dx

[0, 2p]n

m

with O as in example 4. For the sake of simplicity assume
n  1. A generalization to multidimensional kernels is
straightforward.

It is easy to check that the Fourier basis {1/2, sin(lx),
cos(lx), l [ N} is an eigensystem of the operator dened
above, with eigenvalues exp((l2j 2)/2). Now apply proposi-
tion 2, taking into account all eigenfunctions except l  0.

A.J. Smola et al. / Neural Networks 11 (1998) 637649

643

Fig. 5. Left: periodical Gaussian kernel for several values of j (normalized to 1 as its maximum and 0 as its minimum value). Peaked functions correspond to
small j. Right: Fourier coefcients of the kernel for j 2  0.1

This yields the following kernel:

X
X

l  1

l  1

k(x, x9) 



l 2j2
2 (sin(l x)sin(l x9)  cos(l x)cos(l x9))



e

l 2j2
2 cos(l (x  x9))



e

29

P
P

For practical purposes one may truncate the expansion
after a nite number of terms. Moreover we rescale k to
have a range of exactly [0, 1] by using the positive
l  1 (  1)l  1e  ((l 2j2)=2) and the scaling factor
offset
1=2

l  1 e  (((2l  1)2j2)=2)

(cf. Fig. 5).

In the context of periodical functions, the difference
between this kernel and the Dirichlet kernel of example 5
is that the latter does not distinguish between the different
frequency components in w [ {Np,,Np}. However, it
effectively limits the maximum capacity of the system to
approximating the data with a Fourier expansion up to the
order N.

The question that arises now is which kernel to choose.



Let us think about two extreme situations.
 Suppose we already knew the shape of the power spec-
trum Pow(w) of the function we would like to estimate.
In this case we choose k such that k matches the power
spectrum.
If we happen to know very little about the given data a
general smoothness assumption is a reasonable choice.
Hence we might want to choose one of the Gaussian
kernels in example 4 or 6. If computing time is important
one might moreover consider kernels with compact sup-
port, e.g. using the B-spline kernels of example 3. This
choice will cause many matrix elements k ij  k(x i  x j)
to vanish.

The usual scenario will be in between the two extreme
cases and we will have some limited prior knowledge

available. For more information on using prior knowledge
for choosing kernels see Scholkopf et al. (1998).

Prior knowledge can also be used to determine the free
parameters of the kernel, e.g. its width (j) in the examples 4
and 6. Besides that model selection principles like structural
risk minimization (Vapnik, 1982), cross validation (Bishop,
1995; Amari et al., 1997; Kearns, 1997), MDL (Rissanen,
1985), Bayesian methods (MacKay, 1991; Bishop, 1995),
etc. can be employed. Choosing a small width of the kernels
leads to high generalization error as it effectively decouples
the separate basis functions of the kernel expansion into
very localized functions which is equivalent to memorizing
the data, whereas a wide kernel tends to oversmooth.

Note that the choice of the width may be more important
than the actual functional form of the kernel. There may be
little difference in the relevant part of the lter properties
between e.g. a B-spline and a Gaussian kernel (cf. Fig. 6).
The invariance of the kernels presented so far has been
exploited only in the context of invariance with respect to
the translation symmetry group in R n. Yet they could also be
applied to other symmetry transformations corresponding to
other canonical coordinate systems such as the rotation and
scaling group as proposed by Segman et al. (1992); Ferraro
and Caelli (1994), i.e. to a logpolar parametrization of R n or
the parametrization of manifolds.

6. Kernels of dot-product type

There exists a large class of support vector kernels which

are not translation invariant, namely kernels of the type
k(x, x9)  t(hxx9i)
(30)
For instance, polynomial kernels (hxx9i  c) p of homo-
geneous (c  0) or inhomogeneous type (c . 0) belong to
this class. It follows directly from Poggio (1975) that poly-
nomial kernels satisfy Mercers condition. Now the question

644

A.J. Smola et al. / Neural Networks 11 (1998) 637649

operator of this type can be obtained for degree 2 homo-
geneous polynomials on R2 i.e. for the kernel
k(x, y) hxyi2:
Denoting (1=2)]2
x1
corresponding monomials we have

the projectors onto the

, (1=2)]2
x2

, ]x1

(35)

]x2

p

(cid:129)(cid:129)(cid:129)

P  e1

1
2

]2
x1  e2

2

]x1

]x2  e3

1
2

]2
x1

corresponding to
h(x1, x2)(y1, y2)i2 h(x2
1,

x1x2, x2

2)(y2
1,

2

p

(cid:129)(cid:129)(cid:129)

p

(cid:129)(cid:129)(cid:129)

2

2)i
y1y2; y2

(36)

(37)

Fig. 6. Comparison of regularization properties in the low frequency
domain of B 3-spline kernel and Gaussian kernel (j 2  20). Up to an
attenuation factor of 510 3 both types of kernels exhibit qualitatively
similar lter characteristics.

arises which regularization operator P these kernels might
correspond to, and which functions t might be admissible
ones. Obviously P can not be translation invariant, as this is
not the case for k. Note that although lacking translation
invariance, these kernels still exhibit (by construction) the
property of rotation invariance  orthogonal transforma-
tions R are isometries of the Euclidean dot product: hxyi 
hRxRyi.
Skipping tedious calculations, we give an example of an
operator satisfying
k(x1, x2) h Pk(x1, :) Pk(x2, :)i
for homogeneous polynomials. We then use this result to
give an analogous expansion for the inhomogeneous case,
and present a sufcient condition for t(hxx9)) to be an
admissible Mercer kernel.
Let m  (m 1,,mn) [ Nn

0 be a multi index and denote

(31)

!

lml : 

mi and

i  1

p

m

: 

p!
(p  lml)!

(32)

n
i  1mi!

Y

Xn

the multinomial coefcient. Moreover, let

Dm

]m1
x1

1
m1!

, , 1
mn!

xn f (x)lx  0
]mn

0 f : 

(33)
he me m9)  d mm9.
and e m be an orthonormal basis,
Observe how for each m9 Dm
0 extracts exactly one coef-
cient from the monomials of degree m. Now we can dene
an operator Pp which will act as a regularization operator
and satisfy Eq. (31), namely

i.e.

X

! 1

2

p

m

Pp 

em

lml  p

Dm
0

(34)

An intuitive description of P would be that the data is
mapped from R 2 into 3-dimensional feature space (F  R3)
by computing monomials of degree 2. Subsequently one
seeks to compute the attest function in this new space.

Note that Pp is only well-dened on functions that are p
times differentiable. Accordingly, we will have to restrict
the space of functions under consideration to C p. This is not
a major restriction as polynomial kernels are in C  by
construction.

It is interesting that the homogeneous polynomial kernel
also satises the self consistency condition Eq. (13) for the
following operator

X

P 

Pi

i  0

(38)

!

In order to construct an operator for inhomogeneous poly-
nomials, we make use of the expansion
(hxyi  c)p 

cp  lmlhxyii

Xp

(39)

p

i  1

i

(for convenience set c  1). Hence one may decompose the
inhomogeneous polynomial kernel into a series of homo-
geneous kernels and construct the corresponding operator
by

! 1

p

2

Xp

i  0

i

X

! 1

2

p

m

Pinh 

Pi 

em

lml#p

Dm
0

(40)

Exploiting this idea even further allows us to state a suf-
cient condition for t(hxyi) to be a Mercer kernel. As homo-
geneous polynomial kernels satisfy Mercers condition so
does any positive linear combination of them.

Corollary 8 (functions with non-negative power-series).
For every function t(x) that can be expanded into a uni-
formly convergent power series on R with nonnegative
expansion coefcients, i.e.

Example 7 (Vapnik, 1995). A simple example of an

t(x) 

aixi with ai $ 0

(41)

X

i  0

X

i  0

the kernel k(x, y):  t(hxyi) is a Mercer kernel and a corre-
sponding regularization operator is

Pt 

a1=2
i

Pp

(42)

Consequently, functions like e x, cosh(x), sinh(x), etc. could
be used as possible Mercer kernels. Moreover, note that the
same argument applies for t(k(x, y)): if k is any Mercer
kernel, and t satises the conditions of Corollary 8 then
t(k(x, y))  t(hF(x)F(y)i)
is a Mercer kernel. So Eq. (43) provides further means to
construct more general kernels, e.g. sinh(e

(43)

hxyi

).

7. A new class of support vector kernels

We will follow the lines of Madych and Nelson (1990) as
pointed out by Girosi et al. (1993). The main statement is
that conditionally positive denite (cpd) functions generate
admissible SV kernels. This is very useful as the property of
being cpd often is easier to verify than Mercers condition,
especially when combined with the results of Schoenberg
and Micchelli on the connection between cpd and comple-
tely monotonic functions Schoenberg (1938a); Schoenberg
(1938b); Micchelli (1986). Moreover, cpd functions lead to
a class of SV kernels that do not necessarily satisfy Mercers
condition.

Denition 9 (conditionally positive denite functions). A
continuous function h, dened on [0, ), is said to be con-
ditionally positive denite (cpd) of order m on R n if for any
distinct points x 1,xl [ Rn the quadratic form

cicjh(kxi  xjk2)

i, j  1
is non-negative provided that the scalars c 1,,c l satisfy
i  1cip(xi)  0 for all polynomials p on R n of degree

l

lower than m.

(44)

Xl
X

Denition 10 (completely monotonic functions). A func-
tion h(x) is called completely monotonic of order m if
(  1)n dn

dxnh(x) $ 0 for x [ R 

0 and n $ m

(45)

It can be shown (Schoenberg, 1938a; Schoenberg, 1938b;
Micchelli, 1986) that a function h(x 2) is conditionally posi-
tive denite if and only if h(x) is completely monotonic of
the same order. This gives a (sometimes simpler) criterion
for checking whether a function is cpd or not.

Proposition 11 (cpd functions and admissible kernels).
Dene Pn
m to be the space of polynomials of degree
lower than m on R n. Every cpd function h of order m
generates an admissible Kernel for SV expansions on the

A.J. Smola et al. / Neural Networks 11 (1998) 637649

645

space of functions f orthogonal to Pn
h(kx i  x jk2).

m by setting k(x i, x j): 

Proof. In Dyn (1991); Madych and Nelson (1990) it was
shown that cpd functions h of order m generate semi-norms
k.k h by
kf k2

dxidxjh(kxi  xjk2)f (xi)f (xj)

h :

Z

(46)

provided that the projection of f onto Pn
m is zero. For these
functions, this, however, also denes a dot product in some
feature space. Hence they can be used as SV kernels. B

Consequently, one may use kernels like those proposed in
the context of regularization networks by Girosi et al. (1993)
as SV kernels:
k(x, y)  e  bkx  yk2

Gaussian, (m  0)

(47)

q
(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)
q
(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)(cid:129)

kx  yk2  c2

1

kx  yk2  c2

k(x, y)  

k(x, y) 

multiquadric, (m  1)

(48)

inverse multiquadric, (m  0)

k(x, y)  kx  yk2ln kx  yk thin plate splines, (m  2)

(49)

(50)
Here the corresponding regularization operator P is given
implicitly by the seminorm (Eq. (46)) as
k Pf k2 :  kf k2
(51)
However, one has to ensure the orthogonality of our esti-
i  1cip(xi)  0
mate with respect to Pn
for all polynomials p on R n of degree lower than m with c i
being the expansion coefcients of the estimate, i.e. a i.

m, i.e. ensure that

X

h

We proceed with algorithmic details how to actually
compute the expansion. In order not to loose expressive
power
is necessary to take the
polynomials separately into account, i.e. modify Eq. (10)
to get

in the estimate f

it

l

f (x) 

cik(xi, x)  p(x) with p(x) [ Pn

m

(52)

Xl

i  1

m for which kf k2

Both of these issues can be addressed by splitting f into a
term f orthogonal to Pn
h is well dened
and a polynomial term which will not be regularized at all.
(Of course one could dene an additional regularization
operator for the polynomial part but this would without
need render the notation more tedious.) Hence, the regular-
ized risk functional (Eq. (9)) takes on the following form
Rreg[f ]  Remp[f ]  l
(53)
2
with f:  (1  Proj[Pn
m])f and Proj[.] denoting the
projection operator. Repeating the calculations that led to
Eq. (7), yields a similar optimization problem with the

kf k2

h

A.J. Smola et al. / Neural Networks 11 (1998) 637649

difference being that the equality constraint

646

Xl
Xl

(bi  b

p

i )  0

i  1
has been replaced with

(54)

(55)

(bi  b

p

i )p(xi)  0 for all p [ P

n

m

i  1
Note that for the m  1 condition, Eq. (55) reduces to
Eq. (54) as Pn
1 contains only the constant function. The
resulting optimization problem is positive semidenite,
however, only in the feasible region given by the equality
constraints. Some of the eigenvalues of the matrix K may be
negative in the space of coefcients not satisfying Eq. (55).
It can be seen very easily for the multiquadric case (Eq. (48))
 all entries in K ij are negative. This can lead to numerical
instabilities for quadratic programming codes as they
usually assume the quadratic matrix to be positive semi-
denite not only in the feasible region of the parameters
but on the whole space (cf. More and Toraldo, 1991;
Vanderbei, 1994). A practical solution to this problem is
to remove the space S spanned by all polynomials Pn
m on
the data {x 1,,xl} from the image of K ij while keeping it
symmetric by substituting Kij with ((1  P Proj[S]) t K(1 
P Proj[S])) ij. Here PProj is the projection matrix on R l corre-
sponding to Proj[S].

1). The space Pn

Example 12 (projecting out Pn
1 consists of
all polynomials on R n of degree lower than 1, i.e. only of the
constant function. Hence S, the span of Pn
1 on any nonempty
set {x 1,,xl} , R n is span{~1}. Consequently, (1=l )~1~1
is a
projector onto that space and we get6

t

(cid:19)

(cid:18)

(cid:19)

(cid:19)

(cid:18)
(cid:18)

Kij (cid:176) 1 

K 1 

t

~1~1

1
l

t

~1~1

1
l

ij

(56)

Note that in the standard SV problem this modication of k ij
leads to the same solution due to the constraint o i(a i  a
i 
0.

p

2). Pn

Example 13 (projecting out Pn
2 consists of all
constant and linear functions on {x 1,,xl}. Here S 
span({v 0,,v n}) with
v0 :  (1, , 1)
vi :  (xi1, , xil
In the case of l # n  1 already a linear model will sufce
for reducing R reg[f] to 0. In this case the solution of the
quadratic optimization problem is just 0 as K ij will have
rank 0 after the projection.
For l . n  1 we will have to transform v 0,,v n into an
orthonormal basis e 0,,e n of S, e.g. by applying the Gram

) for i [ 1, , n

6 Curiously enough the matrix we obtain by this method is identical to the
one that is being diagonalized in Kernel PCA (Scholkopf et al., 1996). This
is clear as projecting out the span of constant polynomials is equivalent to
centering in feature space.

Schmidt procedure. This in turn allows us to construct an
orthogonal projector onto S and the corresponding modied
matrix from Kij.

As one can observe, only cpd functions of order up to 2
are of practical interest for SV methods as the number of
additional constraints and projection operations increases in
a combinatoric way thereby rendering the calculations com-
putationally infeasible for m . 2.

8. Discussion

A connection between SV kernels and regularization
operators has been shown, which may provide one key to
understanding why SV machines have been found to exhibit
high generalization ability. In particular for the common
choices of kernels, the mapping into feature space is not
arbitrary but corresponds to good regularization operators
(see examples 3, 4 and 6). For kernels, however, where this
is not the case, SV machines may show poor performance
(example 5). Consequently the regularization framework
enables us to analyze the regularization properties of kernels
used in practice.

Capacity control is one of the strengths of SV machines;
however, this does not mean that the structure of the learn-
ing machine, i.e. the choice of a suitable kernel for a given
task, should be disregarded. On the contrary, the rather gen-
eral class of admissible SV kernels should be seen as another
strength, provided that we have a means of choosing the right
kernel. The newly established link to regularization theory can
thus be seen as a tool for constructing the structure consisting
of sets of functions in which the SV machine (approximately)
performs structural risk minimization (e.g. Vapnik, 1995). In
other words it allows to choose an appropriate kernel given
the data and the problem specic knowledge.

For completeness an explicit construction of the regular-
ization operators for polynomial kernels has been given in
order to provide corresponding operators not only for trans-
lation invariant kernels. To make things more transparent
Appendix A contains a worked through example for com-
puting a SV kernel for a specic choice of regularization
operators.

Note that the regularized risk approach can also be dealt
with in a reproducing kernel Hilbert space (RKHS) approach
which may lead to sometimes more elegant exposition of the
subject, see Kimeldorf and Wahba (1971); Micchelli
(1986); Wahba (1990); Girosi (1997); Scholkopf (1997).

Finally the regularization framework made it possible to
extend the class of admissible kernels to those dened by
conditionally positive denite functions  a class of
kernels that do not necessarily have to satisfy Mercers
condition.

A simple consequence of the proposed link is a Bayesian
interpretation of support vector machines. In this case the
choice of a special kernel can be regarded as a prior on the
hypothesis space with P[f] ~ exp(  l
2

kP fk2).

A.J. Smola et al. / Neural Networks 11 (1998) 637649

647

Future work will be necessary for understanding
Vapniks capacity bounds (Vapnik, 1995) from a regulari-
zation network point of view.

Acknowledgements

The authors thank Chris Burges, Federico Girosi, Leo van
Hemmen, Takashi Onoda, John Shawe-Taylor, Vladimir
Vapnik, Grace Wahba, and Alan Yuille for helpful discus-
sions and comments. A.J. Smola is supported by a grant
from the DFG (JA 379/71), and B. Scholkopf is supported
by a grant from the Studienstiftung des deutschen Volkes.

Appendix A A worked through example

In this section we will construct a support vector kernel

for the regularization operator
k Pf k2 h Pf Pfi hf P
p Pfi  kf k2
2 

Xn

i  1

k]xif k2

2

(A1)

Z

This example is taken from Girosi et al. (1993) and used to
illustrate our reasoning in detail. For ease of notation
assume f:R ! R.
A corresponding representation of P

p P in Fourier space (f

denoting the Fourier transform of f) yields

k Pf k2 

dwlf (w)l2(1  w2)

(A2)

R

or equivalently (cf. Section 5, Eq. (20)) P(w)  1/(1  w 2).
In order to satisfy the self consistency condition (Eq. (13)),
we have to compute the inverse Fourier transform of P(w) to
p P (cf. Eq. (21)). This leads
obtain the Greens functions of P
to a kernel of the form
k(x, x9)  e  lx  x9l

(A3)

A function expansion in terms of this Laplacian kernel (it
has the same shape as a Laplacian distribution but should
not be confused with the latter at all) however, may not
always be desirable as it is by far not as smooth as regres-
sions using a Gaussian kernel (see Fig. 7).

Appendix B Ridge regression

Another frequently used method for selecting the regular-
ization operator is to select D (see Eq. (11)) to be the unit-
matrix (D ij  d ij). This approach often is called ridge regres-
sion and is a very popular, method in the context of shrink-
age estimators. Now one may pose a similar question as in
Section 4, namely regarding the equivalence of ridge regres-
sion and support vectors. No answer is available for a direct
equivalence, however, we will show that one may obtain
models generated by the same type of regularization opera-
tors. The requirement for an equivalence of the latter type
would be
Dij  D(xi, xj) h( Pk)(xi, :)( Pk)(xj, :)i  dij
for all possible choices of x i [ R n. Unfortunately this
requirement cannot be met for the case of the Kronecker
d, as Eq. (B1) implies the function D(x 0,.) to be nonzero
only on a set with (Lebesgue) measure 0. The solution is to
change the nite Kronecker d into the more appropriate d-
distribution, i.e. d(x i  x j).

(B1)

By a similar reasoning as in Proposition 1, one can see
that this is true for k(x, y) being the Greens function of P.
p P)1=2 is equivalent
Note that as a regularization operator, ( P
to P, as we can always replace the latter by the former
without any difference in the regularization properties.
Therefore, without loss of generality, we will assume that
P is a positive semidenite endomorphism. Formally we
hence require

h( Pk)(xi, :)( Pk)(xj, :)i hdxi

(:)dxj

(:)i  dxi, xj

(B2)

Fig. 7. Left: Laplacian kernel. Right: regression with a Gaussian (j  1) and a Laplacian kernel (kernel width 2) of the data shown in Fig. 4.

648

A.J. Smola et al. / Neural Networks 11 (1998) 637649

Again, this allows us to connect regularization operators and
kernels (we have to nd the Greens function of P to satisfy
the equation above). For the special case of translation
invariant operators denoted in Fourier space we can associ-
ate P with P ridge(w), leading to

Z(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

k Pf k2 

f (w)

Pridge(w)

dw

(B3)

Comparing Eq. (B3) with Eq. (20) leads to the conclusion
that
the following relation between kernels for support
vector machines and ridge regression has to hold:
PSV(w)  lPridge(w)l2
This also explains the good performance of ridge regression
models in a smoothing regularizer context (the squared
norm of the Fourier transform of kernel functions describes
the regularization properties of the corresponding kernel)
and allows us to transform support vector machines to
ridge regression models and vice versa. Note, however,
that we are loosing the sparsity properties of support
vectors.

(B4)

