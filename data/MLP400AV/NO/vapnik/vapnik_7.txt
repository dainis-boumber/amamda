Abstract

Support Vector Learning Machines (cid:16)SVM(cid:17) are (cid:18)nding application
in pattern recognition(cid:0) regression estimation(cid:0) and operator inver(cid:9)
sion for ill(cid:9)posed problems(cid:11) Against this very general backdrop(cid:0)
any methods for improving the generalization performance(cid:0) or for
improving the speed in test phase(cid:0) of SVMs are of increasing in(cid:9)
terest(cid:11) In this paper we combine two such techniques on a pattern
recognition problem(cid:11) The method for improving generalization per(cid:9)
formance (cid:16)the (cid:19)virtual support vector(cid:20) method(cid:17) does so by incor(cid:9)
porating known invariances of the problem(cid:11) This method achieves
a drop in the error rate on (cid:0) NIST test digit images of (cid:11)(cid:21)
to (cid:11)(cid:21)(cid:11) The method for improving the speed (cid:16)the (cid:19)reduced set(cid:20)
method(cid:17) does so by approximating the support vector decision sur(cid:9)
face(cid:11) We apply this method to achieve a factor of (cid:18)fty speedup in
test phase over the virtual support vector machine(cid:11) The combined
approach yields a machine which is both  times faster than the
original machine(cid:0) and which has better generalization performance(cid:0)
achieving (cid:11)(cid:21) error(cid:11) The virtual support vector method is appli(cid:9)
cable to any SVM problem with known invariances(cid:11) The reduced
set method is applicable to any support vector machine(cid:11)

 INTRODUCTION

Support Vector Machines are known to give good results on pattern recognition
problems despite the fact that they do not incorporate problem domain knowledge(cid:11)

(cid:0)Part of this work was done while B(cid:0)S(cid:0) was with AT(cid:1)T Research(cid:2) Holmdel(cid:2) NJ(cid:0)

However(cid:0) they exhibit classi(cid:18)cation speeds which are substantially slower than those
of neural networks (cid:16)LeCun et al(cid:11)(cid:0) 		(cid:17)(cid:11)

The present study is motivated by the above two observations(cid:11) First(cid:0) we shall
improve accuracy by incorporating knowledge about invariances of the problem at
hand(cid:11) Second(cid:0) we shall increase classi(cid:18)cation speed by reducing the complexity of
the decision function representation(cid:11) This paper thus brings together two threads
explored by us during the last year (cid:16)Sch(cid:13)olkopf(cid:0) Burges (cid:23) Vapnik(cid:0) 		(cid:24) Burges(cid:0)
		(cid:17)(cid:11)

The method for incorporating invariances is applicable to any problem for which
the data is expected to have known symmetries(cid:11) The method for improving the
speed is applicable to any support vector machine(cid:11) Thus we expect these methods
to be widely applicable to problems beyond pattern recognition (cid:16)for example(cid:0) to
the regression estimation problem (cid:16)Vapnik(cid:0) Golowich (cid:23) Smola(cid:0) 		(cid:17)(cid:17)(cid:11)

After a brief overview of Support Vector Machines in Section (cid:0) we describe how
problem domain knowledge was used to improve generalization performance in Sec(cid:9)
tion (cid:11) Section  contains an overview of a general method for improving the
classi(cid:18)cation speed of Support Vector Machines(cid:11) Results are collected in Section (cid:11)
We conclude with a discussion(cid:11)

 SUPPORT VECTOR LEARNING MACHINES

This Section summarizes those properties of Support Vector Machines (cid:16)SVM(cid:17) which
are relevant to the discussion below(cid:11) For details on the basic SVM approach(cid:0) the
reader is referred to (cid:16)Boser(cid:0) Guyon (cid:23) Vapnik(cid:0) 		(cid:24) Cortes (cid:23) Vapnik(cid:0) 		(cid:24) Vapnik(cid:0)
		(cid:17)(cid:11) We end by noting a physical analogy(cid:11)

Let the training data be elements xi  L(cid:0) L (cid:25) Rd(cid:0) i (cid:25) (cid:0) (cid:1) (cid:1) (cid:1)(cid:0) (cid:2)(cid:0) with corresponding
class labels yi  f(cid:1)g(cid:11) An SVM performs a mapping (cid:26) (cid:27) L (cid:2) H(cid:0) x (cid:2) (cid:28)x into a
high (cid:16)possibly in(cid:18)nite(cid:17) dimensional Hilbert space H(cid:11) In the following(cid:0) vectors in
H will be denoted with a bar(cid:11) In H(cid:0) the SVM decision rule is simply a separating
hyperplane(cid:27) the algorithm constructs a decision surface with normal (cid:28)(cid:29)  H which
separates the xi into two classes(cid:27)

(cid:28)(cid:29) (cid:4) (cid:28)xi (cid:30) b (cid:5) k (cid:6) (cid:3)i(cid:0) yi (cid:25) (cid:30)
(cid:28)(cid:29) (cid:4) (cid:28)xi (cid:30) b (cid:7) k (cid:30) (cid:3)i(cid:0) yi (cid:25) (cid:6)

(cid:16)(cid:17)
(cid:16)(cid:17)
where the (cid:3)i are positive slack variables(cid:0) introduced to handle the non(cid:9)separable
case (cid:16)Cortes (cid:23) Vapnik(cid:0) 		(cid:17)(cid:0) and where k and k are typically de(cid:18)ned to be (cid:30)
and (cid:6)(cid:0) respectively(cid:11) (cid:28)(cid:29) is computed by minimizing the objective function

(cid:28)(cid:29) (cid:4) (cid:28)(cid:29)



(cid:0)

(cid:30) C(cid:16)

X

(cid:3)i(cid:17)p

i(cid:2)

(cid:16)(cid:17)

subject to (cid:16)(cid:17)(cid:0) (cid:16)(cid:17)(cid:0) where C is a constant(cid:0) and we choose p (cid:25) (cid:11) In the separable case(cid:0)
the SVM algorithm constructs that separating hyperplane for which the margin
between the positive and negative examples in H is maximized(cid:11) A test vector x  L
is then assigned a class label f(cid:30)(cid:0) (cid:6)g depending on whether (cid:28)(cid:29) (cid:4) (cid:26)(cid:16)x(cid:17) (cid:30) b is greater
or less than (cid:16)k (cid:30) k(cid:17)(cid:4)(cid:11) Support vectors sj  L are de(cid:18)ned as training samples
for which one of Equations (cid:16)(cid:17) or (cid:16)(cid:17) is an equality(cid:11) (cid:16)We name the support vectors
s to distinguish them from the rest of the training data(cid:17)(cid:11) The solution (cid:28)(cid:29) may be
expressed

(cid:28)(cid:29) (cid:25)

NS

X

j(cid:2)

(cid:5)jyj(cid:26)(cid:16)sj(cid:17)

(cid:16)(cid:17)

where (cid:5)j (cid:5)  are the positive weights(cid:0) determined during training(cid:0) yj  f(cid:1)g the
class labels of the sj(cid:0) and NS the number of support vectors(cid:11) Thus in order to
classify a test point x one must compute

(cid:28)(cid:29) (cid:4) (cid:28)x (cid:25)

NS

X

j(cid:2)

(cid:5)jyj(cid:28)sj (cid:4) (cid:28)x (cid:25)

NS

X

j(cid:2)

(cid:5)jyj(cid:26)(cid:16)sj(cid:17) (cid:4) (cid:26)(cid:16)x(cid:17) (cid:25)

NS

X

j(cid:2)

(cid:5)jyjK(cid:16)sj(cid:0) x(cid:17)(cid:1)

(cid:16)(cid:17)

One of the key properties of support vector machines is the use of the kernel K to
compute dot products in H without having to explicitly compute the mapping (cid:26)(cid:11)

It is interesting to note that the solution has a simple physical interpretation in
the high dimensional space H(cid:11) If we assume that each support vector (cid:28)sj exerts a
perpendicular force of size (cid:5)j and sign yj on a solid plane sheet lying along the
hyperplane (cid:28)(cid:29) (cid:4) (cid:28)x (cid:30) b (cid:25) (cid:16)k (cid:30) k(cid:17)(cid:4)(cid:0) then the solution satis(cid:18)es the requirements of
mechanical stability(cid:11) At the solution(cid:0) the (cid:5)j can be shown to satisfy PNS
j(cid:2) (cid:5)jyj (cid:25) (cid:0)
which translates into the forces on the sheet summing to zero(cid:24) and Equation (cid:16)(cid:17)
implies that the torques also sum to zero(cid:11)

 IMPROVING ACCURACY

This section follows the reasoning of (cid:16)Sch(cid:13)olkopf(cid:0) Burges(cid:0) (cid:23) Vapnik(cid:0) 		(cid:17)(cid:11) Problem
domain knowledge can be incorporated in two di(cid:31)erent ways(cid:27) the knowledge can
be directly built into the algorithm(cid:0) or it can be used to generate arti(cid:18)cial training
examples (cid:16)(cid:19)virtual examples(cid:20)(cid:17)(cid:11) The latter signi(cid:18)cantly slows down training times(cid:0)
due to both correlations in the arti(cid:18)cial data and to the increased training set size
(cid:16)Simard et al(cid:11)(cid:0) 		(cid:17)(cid:24) however it has the advantage of being readily implemented for
any learning machine and for any invariances(cid:11) For instance(cid:0) if instead of Lie groups
of symmetry transformations one is dealing with discrete symmetries(cid:0) such as the
bilateral symmetries of Vetter(cid:0) Poggio(cid:0) (cid:23) B(cid:13)ultho(cid:31) (cid:16)		(cid:17)(cid:0) then derivative(cid:12)based
methods (cid:16)e(cid:11)g(cid:11) Simard et al(cid:11)(cid:0) 		(cid:17) are not applicable(cid:11)

For support vector machines(cid:0) an intermediate method which combines the advan(cid:9)
tages of both approaches is possible(cid:11) The support vectors characterize the solution
to the problem in the following sense(cid:27) If all the other training data were removed(cid:0)
and the system retrained(cid:0) then the solution would be unchanged(cid:11) Furthermore(cid:0)
those support vectors (cid:28)si which are not errors are close to the decision boundary
in H(cid:0) in the sense that they either lie exactly on the margin (cid:16)(cid:3)i (cid:25) (cid:17) or close to
it (cid:16)(cid:3)i (cid:6) (cid:17)(cid:11) Finally(cid:0) di(cid:31)erent types of SVM(cid:0) built using di(cid:31)erent kernels(cid:0) tend to
produce the same set of support vectors (cid:16)Sch(cid:13)olkopf(cid:0) Burges(cid:0) (cid:23) Vapnik(cid:0) 		(cid:17)(cid:11) This
suggests the following algorithm(cid:27) (cid:18)rst(cid:0) train an SVM to generate a set of support
vectors fs(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) sNs g(cid:24) then(cid:0) generate the arti(cid:18)cial examples (cid:16)virtual support vec(cid:0)
tors(cid:17) by applying the desired invariance transformations to fs(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) sNsg(cid:24) (cid:18)nally(cid:0)
train another SVM on the new set(cid:11) To build a ten(cid:12)class classi(cid:18)er(cid:0) this procedure is
carried out separately for ten binary classi(cid:18)ers(cid:11)

Apart from the increase in overall training time (cid:16)by a factor of two(cid:0) in our ex(cid:9)
periments(cid:17)(cid:0) this technique has the disadvantage that many of the virtual support
vectors become support vectors for the second machine(cid:0) increasing the number of
summands in Equation (cid:16)(cid:17) and hence decreasing classi(cid:18)cation speed(cid:11) However(cid:0) the
latter problem can be solved with the reduced set method(cid:0) which we describe next(cid:11)

 IMPROVING CLASSIFICATION SPEED

The discussion in this Section follows that of (cid:16)Burges(cid:0) 		(cid:17)(cid:11) Consider a set of
vectors zk  L(cid:0) k (cid:25) (cid:0) (cid:1) (cid:1) (cid:1)(cid:0) NZ and corresponding weights (cid:7)k  R for which

(cid:28)(cid:29) (cid:8)

NZ

X

k(cid:2)

(cid:7)k(cid:26)(cid:16)zk(cid:17)

minimizes (cid:16)for (cid:18)xed NZ(cid:17) the Euclidean distance to the original solution(cid:27)

(cid:8) (cid:25) k (cid:28)(cid:29) (cid:6) (cid:28)(cid:29)k(cid:1)

(cid:16)(cid:17)

(cid:16)(cid:17)

Note that (cid:8)(cid:0) expressed here in terms of vectors in H(cid:0) can be expressed entirely
in terms of functions (cid:16)using the kernel K(cid:17) of vectors in the input space L(cid:11) The
f(cid:16)(cid:7)k(cid:0) zk(cid:17) j k (cid:25) (cid:0) (cid:1) (cid:1) (cid:1)(cid:0) NZg is called the reduced set(cid:11) To classify a test point x(cid:0) the
expansion in Equation (cid:16)(cid:17) is replaced by the approximation

(cid:28)(cid:29) (cid:4) (cid:28)x (cid:25)

NZ

X

k(cid:2)

(cid:7)k(cid:28)zk (cid:4) (cid:28)x (cid:25)

NZ

X

k(cid:2)

(cid:7)kK(cid:16)zk(cid:0) x(cid:17)(cid:1)

(cid:16)(cid:17)

The goal is then to choose the smallest NZ (cid:9) NS(cid:0) and corresponding reduced
set(cid:0) such that any resulting loss in generalization performance remains acceptable(cid:11)
Clearly(cid:0) by allowing NZ (cid:25) NS(cid:0) (cid:8) can be made zero(cid:11) Interestingly(cid:0) there are non(cid:9)
trivial cases where NZ (cid:6) NS and (cid:8) (cid:25) (cid:0) in which case the reduced set leads to
an increase in classi(cid:18)cation speed with no loss in generalization performance(cid:11) Note
that reduced set vectors are not support vectors(cid:0) in that they do not necessarily lie
on the separating margin and(cid:0) unlike support vectors(cid:0) are not training samples(cid:11)

While the reduced set can be found exactly in some cases(cid:0) in general an uncon(cid:9)
strained conjugate gradient method is used to (cid:18)nd the zk (cid:16)while the corresponding
optimal (cid:7)k can be found exactly(cid:0) for all k(cid:17)(cid:11) The method for (cid:18)nding the reduced set
is computationally very expensive (cid:16)the (cid:18)nal phase constitutes a conjugate gradient
descent in a space of (cid:16)d (cid:30) (cid:17) (cid:4) NZ variables(cid:0) which in our case is typically of order
(cid:0)(cid:17)(cid:11)

 EXPERIMENTAL RESULTS

In this Section(cid:0) by (cid:19)accuracy(cid:20) we mean generalization performance(cid:0) and by (cid:19)speed(cid:20)
we mean classi(cid:18)cation speed(cid:11) In our experiments(cid:0) we used the MNIST database of
(cid:30) handwritten digits(cid:0) which was used in the comparison investigation
of LeCun et al (cid:16)		(cid:17)(cid:11) In that study(cid:0) the error rate record of (cid:11)(cid:21) is held by a
boosted convolutional neural network (cid:16)(cid:19)LeNet(cid:20)(cid:17)(cid:11)

We start by summarizing the results of the virtual support vector method(cid:11) We
trained ten binary classi(cid:18)ers using C (cid:25)  in Equation (cid:16)(cid:17)(cid:11) We used a polynomial
kernel K(cid:16)x(cid:0) y(cid:17) (cid:25) (cid:16)x (cid:4) y(cid:17)(cid:11) Combining classi(cid:18)ers then gave (cid:11)(cid:21) error on the (cid:0)
test set(cid:24) this system is referred to as ORIG below(cid:11) We then generated new train(cid:9)
ing data by translating the resulting support vectors by one pixel in each of four
directions(cid:0) and trained a new machine (cid:16)using the same parameters(cid:17)(cid:11) This machine(cid:0)
which is referred to as VSV below(cid:0) achieved (cid:11)(cid:21) error on the test set(cid:11) The results
for each digit are given in Table (cid:11)

Note that the improvement in accuracy comes at a cost in speed of approximately
a factor of (cid:11) Furthermore(cid:0) the speed of ORIG was comparatively slow to start
with (cid:16)LeCun et al(cid:11)(cid:0) 		(cid:17)(cid:0) requiring approximately  million multiply adds for one

Table (cid:27) Generalization Performance Improvement by Incorporating Invariances(cid:11)
NE and NSV are the number of errors and number of support vectors respec(cid:9)
tively(cid:24) (cid:19)ORIG(cid:20) refers to the original support vector machine(cid:0) (cid:19)VSV(cid:20) to the machine
trained on virtual support vectors(cid:11)

Digit NE ORIG NE VSV NSV ORIG NSV VSV













































	



	


	
	


Table (cid:27) Dependence of Performance of Reduced Set System on Threshold(cid:11) The
numbers in parentheses give the corresponding number of errors on the test set(cid:11)
Note that Thrsh Test gives a lower bound for these numbers(cid:11)

Digit












Thrsh VSV
(cid:11)	 (cid:16)	(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11) (cid:16)	(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11)		 (cid:16)	(cid:17)
(cid:9)(cid:11)	 (cid:16)	(cid:17)
(cid:11) (cid:16)(cid:17)

Thrsh Bayes
(cid:11) (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:9)(cid:11)	 (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)

Thrsh Test
(cid:11)	 (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:11) (cid:16)	(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11)		 (cid:16)(cid:17)
(cid:11) (cid:16)(cid:17)
(cid:11)		 (cid:16)(cid:17)
(cid:11)	 (cid:16)(cid:17)
(cid:9)(cid:11) (cid:16)(cid:17)
(cid:11) (cid:16)	(cid:17)

classi(cid:18)cation (cid:16)this can be reduced by caching results of repeated support vectors
(cid:16)Burges(cid:0) 		(cid:17)(cid:17)(cid:11)
In order to become competitive with systems with comparable
accuracy(cid:0) we will need approximately a factor of (cid:18)fty improvement in speed(cid:11) We
therefore approximated VSV with a reduced set system RS with a factor of (cid:18)fty
fewer vectors than the number of support vectors in VSV(cid:11)

Since the reduced set method computes an approximation to the decision surface in
the high dimensional space(cid:0) it is likely that the accuracy of RS could be improved
by choosing a di(cid:31)erent threshold b in Equations (cid:16)(cid:17) and (cid:16)(cid:17)(cid:11) We computed that
threshold which gave the empirical Bayes error for the RS system(cid:0) measured on
the training set(cid:11) This can be done easily by (cid:18)nding the maximum of the di(cid:31)erence
between the two un(cid:9)normalized cumulative distributions of the values of the dot
products (cid:28)(cid:29) (cid:4) (cid:28)xi(cid:0) where the xi are the original training data(cid:11) Note that the e(cid:31)ects of
bias are reduced by the fact that VSV (cid:16)and hence RS(cid:17) was trained only on shifted
data(cid:0) and not on any of the original data(cid:11) Thus(cid:0) in the absence of a validation
set(cid:0) the original training data provides a reasonable means of estimating the Bayes
threshold(cid:11) This is a serendipitous bonus of the VSV approach(cid:11) Table  compares
results obtained using the threshold generated by the training procedure for the
VSV system(cid:24) the estimated Bayes threshold for the RS system(cid:24) and(cid:0) for comparison

Table (cid:27) Speed Improvement Using the Reduced Set method(cid:11) The second through
fourth columns give numbers of errors on the test set for the original system(cid:0) the
virtual support vector system(cid:0) and the reduced set system(cid:11) The last three columns
give(cid:0) for each system(cid:0) the number of vectors whose dot product must be computed
in test phase(cid:11)

Digit ORIG Err VSV Err RS Err ORIG   SV VSV   SV   RSV
























































	



	


	
	













purposes only (cid:16)to see the maximum possible e(cid:31)ect of varying the threshold(cid:17)(cid:0) the
Bayes error computed on the test set(cid:11)

Table  compares results on the test set for the three systems(cid:0) where the Bayes
threshold (cid:16)computed with the training set(cid:17) was used for RS(cid:11) The results for all ten
digits combined are (cid:11)(cid:21) error for ORIG(cid:0) (cid:11)(cid:21) for VSV (cid:16)with roughly twice as
many multiply adds(cid:17) and (cid:11)(cid:21) for RS (cid:16)with a factor of  fewer multiply adds than
ORIG(cid:17)(cid:11)

The reduced set conjugate gradient algorithm does not reduce the objective function
(cid:8) (cid:16)Equation (cid:16)(cid:17)(cid:17) to zero(cid:11) For example(cid:0) for the (cid:18)rst  digits(cid:0) (cid:8) is only reduced
on average by a factor of (cid:11) (cid:16)the algorithm is stopped when progress becomes too
slow(cid:17)(cid:11) It is striking that nevertheless(cid:0) good results are achieved(cid:11)

 DISCUSSION

The only systems in LeCun et al (cid:16)		(cid:17) with better than (cid:11)(cid:21) error are LeNet
(cid:16)(cid:11)	(cid:21) error(cid:0) with approximately K multiply(cid:9)adds(cid:17) and boosted LeNet (cid:16)(cid:11)(cid:21)
error(cid:0) approximately K multiply(cid:9)adds(cid:17)(cid:11) Clearly SVMs are not in this league yet
(cid:16)the RS system described here requires approximately K multiply(cid:9)adds(cid:17)(cid:11)

However(cid:0) SVMs present clear opportunities for further improvement(cid:11) (cid:16)In fact(cid:0) we
have since trained a VSV system with (cid:11)(cid:21) error(cid:0) by choosing a di(cid:31)erent kernel(cid:17)(cid:11)
More invariances (cid:16)for example(cid:0) for the pattern recognition case(cid:0) small rotations(cid:0)
or varying ink thickness(cid:17) could be added to the virtual support vector approach(cid:11)
Further(cid:0) one might use only those virtual support vectors which provide new infor(cid:9)
mation about the decision boundary(cid:0) or use a measure of such information to keep
only the most important vectors(cid:11) Known invariances could also be built directly
into the SVM objective function(cid:11)

Viewed as an approach to function approximation(cid:0) the reduced set method is cur(cid:9)
rently restricted in that it assumes a decision function with the same functional
form as the original SVM(cid:11) In the case of quadratic kernels(cid:0) the reduced set can be
computed both analytically and e!ciently (cid:16)Burges(cid:0) 		(cid:17)(cid:11) However(cid:0) the conjugate
gradient descent computation for the general kernel is very ine!cient(cid:11) Perhaps re(cid:9)

laxing the above restriction could lead to analytical methods which would apply to
more complex kernels also(cid:11)

Acknowledgements

We wish to thank V(cid:11) Vapnik(cid:0) A(cid:11) Smola and H(cid:11) Drucker for discussions(cid:11) C(cid:11) Burges
was supported by ARPA contract N(cid:9)	(cid:9)C(cid:9)(cid:11) B(cid:11) Sch(cid:13)olkopf was supported
by the Studienstiftung des deutschen Volkes(cid:11)

