Abstract

Learning is posed as a problem of function estimation, for  which two princi(cid:173)
ples of solution are considered:  empirical risk minimization and structural
risk minimization.  These two  principles are applied  to two different state(cid:173)
ments  of the  function  estimation  problem:  global  and  local.  Systematic
improvements in prediction power are illustrated in application to zip-code
recognition.

1

INTRODUCTION

The structure of the  theory of learning differs  from  that of most other theories for
applied  problems.  The search for  a  solution to an applied problem usually requires
the  three  following  steps:

1.  State the problem in mathematical terms.
2.  Formulate a  general principle to look  for  a  solution  to the problem.
3.  Develop  an algorithm based on such general principle.

The  first  two  steps  of  this  procedure  offer  in  general  no  major  difficulties;  the
third  step  requires  most  efforts,  in  developing  computational  algorithms  to  solve
the  problem at hand.

In  the case of learning theory, however,  many algorithms have  been  developed,  but
we still lack a clear understanding of the mathematical statement needed to describe
the learning procedure, and of the general principle on which the search for solutions
831

832

Vapnik

should  be  based.  This  paper  is  devoted  to  these  first  two  steps,  the statement  of
the problem and  the general principle of solution.

The  paper  is  organized  as  follows.  First,  the  problem  of function  estimation  is
stated,  and  two  principles of solution are discussed:  the principle of empirical risk
minimization  and  the  principle  of structural  risk  minimization.  A  new  statement
is then given:  that of local estimation of function,  to which  the same principles are
applied.  An application  to zip-code  recognition is  used  to illustrate these ideas.

2  FUNCTION ESTIMATION MODEL

The learning process is  described  through  three components:

1.  A generator of random vectors x, drawn independently from a fixed  but unknown
distribution  P(x).
2.  A supervisor which returns an output vector y  to every input vector x, according
to a  conditional distribution function  P(ylx), also fixed  but unknown.
3.  A  learning machine  capable of implementing a  set of functions  !(x, w),  wE W.
The problem of learning is  that of choosing from  the given set of functions  the one
which  approximates  best  the  supervisor's  response.  The  selection  is  based  on  a
training set of e independent observations:

(1)

The formulation  given  above  implies  that  learning  corresponds  to  the  problem  of
function  approximation.

3  PROBLEM OF RISK  MINIMIZATION

In  order  to  choose  the  best  available  approximation  to  the  supervisor's  response,
we  measure  the  loss  or  discrepancy  L(y, !(x, w  between  the  response  y  of  the
supervisor  to  a  given  input  x  and  the  response  !(x, w)  provided  by  the  learning
machine.  Consider  the expected value of the loss,  given  by  the risk  functional

R(w) = J L(y, !(x, wdP(x,y).

(2)

The  goal  is  to  minimize  the  risk  functional  R( w)  over  the  class  of  functions
!(x, w),  w  E  W.  But  the  joint  probability  distribution  P(x, y)  =  P(ylx )P(x)
is  unknown  and  the only  available information is  contained  in  the training set (1).

4  EMPIRICAL RISK  MINIMIZATION

In  order  to  solve  this  problem,  the following  induction  principle  is  proposed:  the
risk  functional  R( w)  is  replaced  by  the empirical  risk functional

1

l

E(w) = i  LL(Yi,!(Xi'W

i=l

(3)

Principles of Risk Minimization for  Learning Theory

833

constructed on the basis of the training set (1).  The induction principle of empirical
risk minimization (ERM) assumes that the function I(x, wi) ,which minimizes E(w)
over  the set w E W,  results in  a  risk  R( wi) which  is  close  to its minimum.
This induction principle is quite general; many classical methods such as least square
or  maximum likelihood  are  realizations  of the ERM principle.

The evaluation of the soundness of the ERM  principle requires  answers  to the fol(cid:173)
lowing  two  questions:
1.  Is  the  principle  consistent?  (Does  R( wi) converge  to its  minimum value  on  the
set  wE W  when f - oo?)
2.  How  fast  is  the convergence as  f  increases?

The  answers  to  these  two  questions  have  been  shown  (Vapnik  et  al.,  1989)  to  be
equivalent  to the answers to the following  two  questions:
1.  Does  the  empirical  risk  E( w)  converge  uniformly  to  the  actual  risk  R( w)  over
the full  set I(x, w),  wE W?  Uniform convergence  is  defined  as
f  -

Prob{ sup  IR(w) - E(w)1  > }  -

(4)

wEW

0

as

00.

2.  What is  the rate of convergence?

It is  important to stress that uniform convergence (4)  for  the full  set of functions is
a  necessary and  sufficient condition for  the consistency  of the  ERM  principle.

5  VC-DIMENSION  OF THE SET  OF FUNCTIONS

The  theory  of  uniform  convergence  of empirical  risk  to  actual  risk  developed  in
the  70's  and  SO's,  includes  a  description  of necessary  and  sufficient  conditions  as
well  as  bounds  for  the  rate  of convergence  (Vapnik,  19S2).  These  bounds,  which
are  independent  of the  distribution  function  P(x,y),  are  based  on  a  quantitative
measure of the capacity of the set offunctions implemented by the learning machine:
the  VC-dimension  of the set.

For  simplicity,  these  bounds will  be  discussed  here only  for  the case  of binary  pat(cid:173)
tern  recognition, for  which  y  E {O, 1}  and  I(x, w),  wE W  is  the class  of indicator
functions.  The loss function  takes only  two  values  L(y, I(x, w)) = 0 if y = I(x, w)
and  L(y, I(x, w))  =  1 otherwise.  In  this  case,  the  risk  functional  (2)  is  the prob(cid:173)
ability  of error,  denoted  by  pew).  The  empirical  risk  functional  (3),  denoted  by
v(w),  is  the frequency  of error in  the training set.
The  VC-dimension  of a  set  of indicator  functions  is  the  maximum  number  h  of
vectors  which  can  be  shattered  in  all  possible  2h  ways  using  functions  in  the  set.
For  instance,  h =  n + 1 for  linear  decision  rules  in  n-dimensional space, since  they
can shatter at most  n + 1 points.

6  RATES  OF UNIFORM CONVERGENCE

The notion of VC-dimension provides a  bound to the rate of uniform convergence.
For a set of indicator functions with VC-dimension h,  the following inequality holds:

834

Vapnik

Prob{ SUp  IP(w) - v(w)1  > c} < (-h)  exp{-e fl

2

2fe  h

wEW

It then follows  that with probability  1 - T},  simultaneously for  all  w  E W,

pew)  < v(w) + Co(f/h, T}),

with confidence  interval

C  (f/h

o

,T}-V

) _  . Ih(1n 21/h + 1) - In T}

f

.

(5)

(6)

(7)

This  important  result  provides  a  bound  to  the  actual  risk  P( w)  for  all  w  E  W,
including  the w  which  minimizes  the empirical  risk  v(w).
The  deviation  IP(w)  - v(w)1  in  (5)  is  expected  to  be  maximum  for  pew)  close
to  1/2, since  it  is  this  value  of pew)  which  maximizes  the error  variance  u(w)  =
J P( w)( 1 - P( w)).  The  worst  case  bound  for  the  confidence  interval  (7)  is  thus
likely  be  controlled  by  the worst  decision  rule.  The  bound  (6)  is  achieved  for  the
worst  case  pew)  =  1/2,  but  not  for  small  pew),  which  is  the  case  of interest.  A
uniformly  good  approximation  to P( w)  follows  from  considering

Prob{ sup
wEW

pew) - v(w)

(j(w)

> e}.

(8)

The variance of the relative deviation (P( w) - v( w))/ (j( w)  is now independent of w.
A  bound for  the  probability  (8),  if available,  would  yield  a  uniformly  good  bound
for  actual risks for  all  P( w).
Such a  bound has not yet been established.  But for  pew)   1,  the approximation
(j(w) ~ JP(w) is  true,  and  the following  inequality  holds:

Prob{ sup
wEW

pew)  - v(w)

JP(w)

> e}  < (-) exp{--}.

2le  h
h

e2f
4

It then follows  that with probability  1 - T},  simultaneously for  all  w  E W,

pew) < v(w) + CI(f/h, v(w), T}),

(9)

(10)

with confidence interval

CI(l/h,v(w),T}) =2 (h(ln2f/h;l)-lnT}) (1+  1

.
(11)
Note  that  the  confidence  interval  now  depends  on  v( w),  and  that  for  v( w)  =  0  it
reduces to

+ h(1n 2f/h + 1) -In T}

v(w)f

)

CI(f/ h, 0, T})  = 2C'5(f/ h, T}),

which  provides a  more  precise bound  for  real  case  learning.

7  STRUCTURAL RISK  MINIMIZATION

The  method  of ERM  can  be  theoretically justified  by  considering  the  inequalities
(6) or (10).  When l/h is  large,  the confidence intervals Co  or C1  become small,  and

Principles of Risk Minimization for  Learning Theory

835

can  be  neglected.  The actual risk  is  then bound by only the empirical risk,  and the
probability of error on the test set can  be expected to be small when  the frequency
of error in  the  training set  is  small.
However,  if ljh  is  small,  the  confidence  interval  cannot  be  neglected,  and  even
v( w)  = 0 does not guarantee a small probability of error.  In this case the minimiza(cid:173)
tion  of P( w)  requires  a  new  principle,  based  on  the simultaneous  minimization  of
v( w)  and the confidence interval.  It is  then necessary  to control the  VC-dimension
of the learning machine.
To do this, we  introduce a nested structure of subsets Sp  = {lex, w), wE Wp}, such
that

The corresponding VC-dimensions of the subsets satisfy

SlCS2C ...  CSn .

hl < h2  < ... < hn .

The principle of structure risk minimization (SRM) requires a  two-step process:  the
empirical  risk  has to  be  minimized for  each  element  of the structure.  The optimal
element  S*  is  then  selected  to  minimize  the  guaranteed  risk,  defined  as  the  sum
of the empirical  risk  and  the  confidence  interval.  This process  involves  a  trade-off:
as  h  increases  the  minimum  empirical  risk  decreases,  but  the  confidence  interval
mcreases.

8  EXAMPLES  OF  STRUCTURES FOR NEURAL NETS

The  general  principle  of SRM  can  be  implemented  in  many  different  ways .  Here
we  consider  three  different  examples  of structures  built  for  the  set  of  functions
implemented  by  a  neural network .

1.  Structure given by the architecture of the neural network.  Consider an
ensemble of fully  connected neural  networks in which  the number of units in one of
the  hidden  layers  is  monotonically  increased.  The set  of implement able  functions
makes a  structure as  the number of hidden  units is  increased.

2.  Structure given by  the learning procedure.  Consider  the set of functions
S  =  {lex, w),  w  E  W}  implementable  by  a  neural  net  of fixed  architecture.  The
parameters  {w}  are  the weights  of the  neural  network.  A  structure is  introduced
through  Sp  =  {lex, w),  Ilwll  <  Cp }  and  Cl  <  C2  <  ... <  Cn.  For  a  convex
loss  function,  the  minimization  of the  empirical  risk  within  the element  Sp  of the
structure is  achieved  through  the  minimizat.ion  of

1

l

E(w"P) = l  LL(Yi,!(Xi'W +'PllwI1 2

i=l

with appropriately chosen  Lagrange multipliers II > 12  >  ...  > In' The well-known
"weight  decay"  procedure refers  to the minimization of this functional.

3.  Structure  given  by  preprocessing.  Consider  a  neural  net  with  fixed  ar(cid:173)
chitecture.  The input  representation  is  modified  by  a  transformation  z = K(x, 13),
where  the  parameter  f3  controls  the  degree  of the  degeneracy  introduced  by  this
transformation  (for  instance f3  could  be  the  width of a smoothing kernel).

836

Vapnik
A  structure  is  introduced  in  the  set  of functions  S  = {!(Ix, 13), w),  w  E  W}
through 13  > CP1  and  Cl  > C2  > ... > Cn

9  PROBLEM  OF LOCAL FUNCTION ESTIMATION

The problem of learning  has  been  formulated  as  the problem of selecting from  the
class  of functions  !(x, w),  w E  W  that which  provides  the  best available  approxi(cid:173)
mation to the response of the supervisor.  Such a statement of the learning problem
implies that a unique function !( x, w) will be used for prediction over the full input
space X.  This is  not necessarily  a  good  strategy:  the set !(x, w),  w  E  W  might
not contain  a  good  predictor for  the full  input space,  but  might  contain  functions
capable of good  prediction on specified  regions of input space.

In order to formulate  the  learning  problem as a  problem of local function  approxi(cid:173)
mation,  consider  a  kernel  Ix - Xo, b)  ~ 0 which  selects  a  region  of input space of
width  b,  centered at xo.  For  example,  consider  the rectangular kernel,

I<  (x _  x  b)  =  {  1

,.

0,

if Ix - ~o I < b

0  otherwIse

and  a  more general general continuous kernel,  such as  the gaussian

r

/ig(x-xo,b)=exp-{

(x-xO)2

b2

}.

The goal  is  to minimize the local  risk  functional

R(w, b, xo)  =

L(y, !(x, w  K(xo, b)  dP(x, V)

K(x - Xo, b)

J

The normalization is  defined  by

K(xo, b)  = J K(x - Xo, b) dP(x).

(12)

(13)

The  local  risk  functional  (12)  is  to  be  minimized  over  the  class  of  functions
!(x, w),  w  E  Wand  over  all  possible  neighborhoods  b  E  (0,00)  centered  at  xo.
As before,  the joint probability distribution P( x, y)  is  unknown,  and the only avail(cid:173)
able  information is  contained  in  the training set (1).

10  EMPIRICAL RISK  MINIMIZATION FOR LOCAL

ESTIMATION

In  order  to  solve  this  problem,  the  following  induction  principle  is  proposed:  for
fixed  b,  the local risk functional  (12)  is  replaced  by  the empirical risk functional

E(w,b,xo) = l  L..tL(Yj,!(Xj,w

1 ~

i=l

K(Xi - Xo, b)

1

b)

,

(14)

Xo,

Principles of Risk Minimization for Learning Theory

837

constructed  on  the  basis  of the  training set.  The empirical  risk  functional  (14)  is
to be minimized  over  w  E W.  In the simplest case,  the class of functions  is  that of
constant functions,  I(x, w)  = C( w).  Consider  the following examples:
1.  K-Nearest  Neighbors  Method:  For  the  case  of  binary  pattern  recogni(cid:173)
tion,  the  class  of constant  indicator  functions  contains  only  two  functions:  either

I(x, w)  =  for  all  x,  or  I(x, w)  = 1 for  all  x.  The  minimization  of the  empirical

risk functional  (14)  with the rectangular kernel Kr(x-xo,b) leads to the K-nearest
neighbors algorithm.
2.  Watson-Nadaraya Method:  For  the  case  y  E  R,  the  class of constant func(cid:173)
tions  contains  an  infinite  number  of elements,  I(x,w)  =  C(w),  C(w)  E  R.  The
minimization of the empirical risk functional (14) for  general kernel and a quadratic
loss function  L(y, I(x, w)) =  (y - I(x, w))2  leads  to the estimator

l

I(

Xo

) - "".  K(Xi - Xo, b)
- ~YI  l .

i=1  L;=I/\ (x;  - xo, b)

,

which  defines  the  Watson-Nadaraya algorithm.

These  classical  methods  minimize  (14)  with  a  fixed  b  over  the  class  of constant
functions.  The supervisor's response in the vicinity of Xo  is  thus approximated by a
constant, and the characteristic size b of the neighborhood is kept fixed,  independent
of Xo.
A  truly  local  algorithm would  adjust  the  parameter b to  the  characteristics of the
region  in  input space centered at  Xo  .  Further improvement is  possible by  allowing
for  a  richer  class  of predictor  functions  I(x, w)  within  the selected  neighborhood.
The SRM  principle for  local estimation provides a  tool for  incorporating these  two
features.

11  STRUCTURAL RISK  MINIMIZATION  FOR LOCAL

ESTIMATION

The  arguments  that  lead  to  the  inequality  (6)  for  the  risk  functional  (2)  can  be
extended  to  the  local  risk  functional  (12),  to  obtain  the  following  result:  with
probability 1 - T},  and simultaneously for  all  w  E Wand all  b E (0,00)

R(w,b,xo) < E(w,b,xo) + C2(flh, b, T}).

(15)

The confidence  interval C2(flh, b, T})  reduces  to Co(llh, T})  in  the b -+ 00  limit.
As before, a nested structure is introduced in the class of functions,  and the empirical
risk  (14)  is minimized with respect to both w  E  Wand bE (0,00) for  each element
of the structure.  The optimal element  is  then selected  to minimize  the guaranteed
risk,  defined  as  the sum of the empirical risk and  the confidence interval.  For fixed
b this  process  involves an  already  discussed  trade-off:  as  h increases,  the empirical
risk  decreases  but  the  confidence  interval  increases.  A  new  trade-off  appears  by
varying b at fixed  h:  as  b increases  the empirical  risk  increases,  but the confidence
interval  decreases.  The  use  of b as  an  additional free  parameter  allows  us  to find
deeper  minima of the guaranteed  risk.

838

Vapnik

12  APPLICATION  TO  ZIP-CODE  RECOGNITION

We  now  discuss results for  the recognition of the hand written and  printed  digits in
the US  Postal database, containing 9709  training examples and  2007  testing exam(cid:173)
ples.  Human  recognition  of this  task  results  in  an  approximately  2.5%  prediction
error  (Sackinger et al.,  1991).
The  learning  machine  considered  here  is  a  five-layer  neural  network  with  shared
weights  and  limited  receptive fields.  When  trained  with  a  back-propagation  algo(cid:173)
rithm for  the minimization of the empirical risk,  the network  achieves  5.1%  predic(cid:173)
tion  error  (Le  Cun et al.,  1990).

Further performance improvement with the same network architecture has required
the  introduction a  new  induction  principle.  Methods based on  SRM  have  achieved
prediction  errors  of 4.1%  (training  based  on  a  double-back-propagation  algorithm
which incorporates a special form of weight decay (Drucker, 1991  and 3.95% (using
a  smoothing  transformation in  input space  (Simard,  1991.

The best result  achieved so  far,  of 3.3%  prediction error, is  based on  the use  of the
SRM for  local estimation of the predictor function  (Bottou,  1991).

It is  obvious  from  these  results  that  dramatic  gains  cannot  be  achieved  through
minor algorithmic modifications,  but require  the introduction of new  principles.

Acknowledgements

I thank the members of the Neural  Networks research group at Bell  Labs,  Holmdel,
for  supportive  and  useful  discussions.  Sara Solla,  Leon  Bottou,  and  Larry  Jackel
provided invaluable help  to render my  presentation more clear and accessible  to the
neural networks community.

