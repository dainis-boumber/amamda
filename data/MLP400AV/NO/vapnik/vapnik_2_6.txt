Abstract

that the probability
is at most

ofmisclassifying

the (l+1)th

point

of

given classifications

from a continuous proba
Our method is a modifica
machine;

of an object

set, assuming
that

in the training

distribution.

a method for predicting
a clas

We describe
sification
the objects
the pairs object/classification
are generated
by an i.i.d. process
bility
tion of Vapnik's
main novelty is
diction
itself
the evidence
tion. We also describe
ing degrees
by the support
mental results
tensions

that it gives not only the pre
but also a practicable
measure of
found in support

vector
machine.
are presented,

Some experi
and possible

support-vector
its

of the algorithms

to predictions

are discussed.

of confidence

a procedure

of that predic

made

for assign

ex

E(number

of support

among xi , ... ,xl+I)

vectors
l + 1

(1)

where the points XI, ... ,xl+l are generated
dently from the underlying
are defined in Section
vectors
theorem
we need to know  the
P, while the only information

5 below. To apply this
probability
we do know  is

distribution

indepen
P; support

distribution

the expecta

to estimate

this is not sufficient

inference; in our present

Clearly
tion in (1).
Remark 1 Dawid [2] distinguishes
between
context
and stochastic
nomi
nal inference
in
ference is some assertion
about the accuracy
prediction.
provides
course,
oped, the situation

To use this terminology

devel
to change in the future.)

of this
, the SV method

only nominal
since the SV method is being  actively

but no stochastic

is the prediction

and stochastic

is likely

nominal

itself

inference. (Of

1  THE PROBLEM

labeled

are specified

distribution.

by n real-valued

points (xi, Yi)  (i =  1, 2, . .. ), where
and Yi E { -1, 1}, are  generated
indepen
from an unknown (but the same for all points)

Suppose
Xi E lRn (our objects
attributes)
dently
probability
i =  1, . .. , l, together
{ -1, 1}, and an (l + 1)th unclassified
should it be classified?
tion, in the sense that we are interested
cation
of a particular
rule for classifying
6.)
sion of transduction,

of transduc
in the classifi
than in a general
for further discus

with their classifications

We are given l points Xi,

future examples;

(This is a problem

point Xt+l How

see Section

example

rather

Yi E

is Vapnik's [7]

vector (SV) machines.
The SV

approach

and well-known

A natural
method of support
method works very well in practice,
no practicable
of the accuracy
tions are known if our only information
points
in this context,  theorem

and one unclassified
point.

estimates

from [7] (Theorem

of its predic
is l classified
The most relevant,

but unfortunately

5.2) says

2  PREDICTING WITH

CONFID ENCE

following

in the
set and one point to be classified),
and the only

[4], our transduc
off its substantiation
until Sec
in the space lRn:
two pictures
(the l points
(l + 1) points

Now we briefly describe,
tive algorithm,
putting
tion 5. We consider
both pictures
contain
training
in the training
difference  between
the (l+l)th
sified as -1 and in the 1-picture
can be proven that the (l+ l)th point will be a support
vector
Let SV(l) (resp.
SV ( -1)) be the set of indices
1-picture

in at least one of the pictures.
of support
vectors
in the
we let #A stand for the

set are classified
the pictures
in the -!-picture

that point is clas
it is classified

-1-picture);

the points

(resp.

point;

as 1. It

as before,
is the classification

of

Learning by Transduction 149

cardinality
predictions

of set A. Our algorithm
and "incertitudes":

gives the following

 1 if

((l + 1) E SV( -1)) & ((l + 1)  SV(1))

or

((l + 1) E SV(-1) n SV(1))
& (#SV( -1) < #SV(1));
of this prediction

is

the incertitude

we found that its performance

by the number of mistakes

made) is not as

of the standard

SV algorithm

in some applications confidence

this finding does not
is definitely
might be even

better,

7 below).

however,

algorithm

Of course,

experiments,
(measured
good as the performance
(see Section
mean that the standard
because
more important
desirable
the standard
provides
gorithm
it gives the same confidences).

SV algorithm.

to introduce

some measure

than performance;

but it does make it
for

of confidence

a measure
(and in the case of our transductive

The following
al

procedure
for any prediction
algorithm

of confidence

#SV(-1)

l+1

 -1 if

((l + 1) E SV(1))

& ((l + 1)  SV( -1))

or

((l + 1) E SV( -1) n SV(1))
< #SV( -1)),
& (#SV(1)
with incertitude

#SV(1).
l + 1 '

 any prediction

if

((l + 1) E SV( -1) n SV(1))
& (#SV( -1) = #SV(1))

with incertitude

we have two pictures,

As before,
the 1-picture. Let fi be the prediction
by the given prediction
this prediction

the -1-picture and
for Yl+l made
of

algorithm.

is defined to be

The incertitude

0
1

#SV( -fi) if (l + 1) E SV(-A)
2. oo, otherwise.

l + 1 '

y

'

of a prediction

of incertitude

is the same as be
of incertitude

(The interpretation
fore: failure
as a win of   on a  1 ticket
SV algorithm,
ways true that (l + 1) E SV( -fi). So in this most

.) For the
1 can realize:
it is al

only possibility

in a fair lottery

J-L is as likely

inter

for us case our procedure

esting
is extremely

simple:

the confidence

of assigning

confidence

is just 1-#S'(/:";vl .

In our experiments
role is played by what we call the "possibility"
data set; this is discussed

we found that a  very

in Sections

importan

4 and 7 below.

of the

t

#SV(-1)

#SV(1)

l + 1

l + 1 

3  MEASURES OF IMPOSSIBILITY

of incertitude

is as follows:
as a win of

J-L is as likely

of incertitude

The interpretation
of a prediction
  on a 1 ticket
will be given below.

in a fair lottery.

Exact definitions

failure

we introduce

In this section
us to define the notion of incertitude;
will partly

[9] and [10].

notions

follow

which will enable

our exposition

or, in other words, confident

predictions
of small in
predictions)

vectors

is small in both

the SV method,
will be satisfied;
in Vapnik [7] (footnote

one hopes
in the experi
4 on p. 131)
3% to 5% of the data

constituted

When applying

Our method works well (gives
certitude
when the number of support
pictures.
that this assumption
ments presented
the support
vectors
set. Our experiments
given incertitudes
0.05-0.10.
in terms of confidences
fine confidence
certitudes

5-10% correspond

rather

(see Section

7) have typically

to be 1-I, I being incertitude.

It is often easier
than incertitudes;

to think

we de
(So in
to confidences

90-95%.)

The transductive
signed to optimize

algorithm

described

above was de

the confidence.

In our computer

space Xi E 1R n with their classifications

in

sample space is
( x1, ... , xl+ 1) of l + 1 points
with the usual
If P is a probability distribution

Let n be some sample space (a typical
the set of all sequences
the Euclidean
Yi E { -1, 1}, i = 1, ... , l + 1, equipped
O"-algebra).
measure
p : n -7 1R such that
measurable function
k p(w)P(dw)
 1.

of impossibility

(2)

inn, a P

is defined to be a non-negative

of the notion of lottery;

P as the randomizing

This is our explication
sualize
lots and p(w) as the value of the prize won by a par
w. Notice that we do
ticular
(2) with an
not exclude  "fair"

when P produces
lotteries

which satisfy

device

ticket

we vi

used for drawing

150 Gammerman, Vovk, and Vapnik

sign (which

equality
ing the tickets
though in real lotteries
ally much less than 1.

means that all proceeds
are redistributed
side of (2) is usu

the left-hand

from sell

in the form of prizes),

By Chebyshev
ability:

's inequalit
y, pis large with small prob
C > 0,
for any constant

1
P{w E S1: p(w)  C}   c
our  intuition

that if p is chosen in ad

space).

measurable

Put Z =  X x { -1, 1}
We are given a sam-
Zi =  (xi,Yi) E Z,
example Xf+1 E X;
independently
P in Z.
distribution
Yl+ 1 E { -1, 1}
the classification

arbitrary
(Y = { -1, 1} is our label  space).
ple z1, ... ,zz of classified
examples,
i =  1, .. .  , l, and one unclassified
(Xi, Yi) are assumed to be generated
from some unknown probability
Our goal is to predict
of xz+l
for doing so is as follows.
Our algorithm
choose a permutation  measure

of impossibility

First we
p :
z1, ... 'Zz, XZ+1 we cal

z1+1 --t JR. After observing

This confirms
vance and we believe
tribution
possible

generating
that p(w) will turn

up large.

that P is the true probability dis
the data w E 0, then it is hardly

culate

two values:

P,1 = 1jp(z1, ... , zz,(Xf+1,1)).

Then we predict
P.-1 < P,1, with -1 if fi.-1 > P,1, and predict arbitrarily
if P,-1 =  P,1); the incertitude

of our prediction

with argmaxp. (i.e.,

predict

with 1 if

is

p. = min(P,-1,f1.1)

(and our confidence
interpretation
our prediction
a lottery;
prediction

if p. is small,
is correct.

in our prediction

of this measure of our incertitude
is right unless

a  1 ticket

is that

is 1 -p.). The
wins   in
sure that our

we can be pretty

pm in zm' p running over
in Z. Our interpretation

for any constant

if pis a cm(z)-measure

The quality

Notice that Chebyshev

P {p.  E & prediction

implies

's inequality
is wrong}  E,
P.

E > 0 and any distribution
of data is given by the possibility

and

is essen

region"

a special

statistical

used in
hypotheses

Remark 2 The notion of a "critical
the theory of testing
tially
possibility:
a small probability
P-measure

case of our notion of a measure
of im
a subset A   n of the sample space of
o =  P (A) is identified
with the
{ 1/8, ifw E A,

of impossibility

p(w) =  0, otherwise.

from a continuous distribution,

m  is a positive

we define

distributions,
to be a function
which is
for all P E P. Most of

space,

for the set

(the sample size),

and em ( Z) stands

of probability

distributions

distributions

in the cm(z)-measures
of impos

ty, where Z is a measurable

If P is a family
of impossibility
a P-measure
a P-measure of impossibility
all we are interested
sibili
integer
of all product
the continuous
of this definition
is as follows:
and z1, . . .  , Zm are generated
of impossibility
dently
it is hardly pos
sible that p(z1,     , Zm) is large (provided
before

the data z1, ... , Zm are generated).
subclass
of the
of impossibility.
mea
p : zm --t 1R is a permutation
if, for any sequence

Now we shall introduce
cm(z)-measures
surable
Z1, . . .  , Zm in zm,
of impossibility
 p(z1, ... ,zm) =  oo  if Zi = Zj for some i =/:. j;
 ,;,, L1r p(z1f(1),     , Z1r(m)) =  1 (the sum is over all
1r of the set { 1, ... , m}), if all ele
permutations
ments of the set { z1, ... , Zm} are different.

an important

function

pis chosen

A non-negative

measure

indepen

It is obvious
measure of impossibil

ity.

that every such p is indeed a cm(z)

4  GEN ERAL SCHEME

max(f.t-1,f.t!).
p(z1, ... , zz+1) is guaranteed

to be
such
have

If this value is small,
big no matter which Yl+1 will turn up; therefore,
data are hardly possible,
and our experiments
shown that the quality
typically
ity does not depend (unlike
tion algorithm
will usually
1 in the case where it exceeds

of prediction
very poor. Notice that the notion

the value of possibility

used and is a property

confidence)

on the predic

for such data is

truncate

of the data. We

1.

of possibil

reporting

described
of the predictions

The  prediction  algorithm
the confidence
we have already
we can associate
gorithm's predictions
prediction

decided
to be used,
with the al
a measure of incertitude
of a

above optimizes
made. If, however,

the incertitude

on the algorithm

as follows:

y for Y1+1 is

P.-iJ

First we describe
l (a positive

our task. W fix a training
set size
space X (an

and an attribute

integer)

The interpretation
analogous
is correct

of this measure
to what we had before:
unless

of incertitude
the prediction
wins   in a lottery.

a 1 ticket

is
y

Learning by Transduction 151
(4) with i > 0 and
b1 =f. b2 is clearly impossible. D

at least one of the inequalities
bE {b1, b2} would be strict
would not be attained);

and so the minimum in (3)

however,

in the separable

case

for which

inequality

to be any (xi, Yi)

We define a support
vector
in (4) holds as equality.
a sample (x1, yl), ... , (xi, YL) and one unclas
the corresponding
Consider
the 1-picture,
sified example
we consider
where Yl+l = 1, and the -1-picture,
The most important,
for our purposes,
port vectors

where Yl+l =  -1.
property

is the following.

x1. As before,

of sup

Lemma 2 If the sample (x1, Yl), ... , (xi, YL) contains
positive
examples,
in at least one of the two pictures.

and negative

Zl+I is a support

vector

follows  from  Lemma

of impossibility

D

Yi =  -1.

that an inequality

We define a permutation
measure

Proof This immediately
the simple observation

1 and
of type (4)
cannot be strict  (with i =  0) for both Yi =  1 and
p(zl, ... ,Zl+l) =  #SV(zl, ... ,,+l) { l+l  , if Zl+l E SV,
set { z1, ... ,zl+r} andzi =  (xi,Yi)
,  i =  1, ... ,l+ l.

(6)
vectors
in the

. . .  , Zl+l) are the support

where  SV(z1,

0, otherw1se,

p by

that Zi are all different;
if not,

(We were assuming
p(z1, ... , Zl+l) =  oo  by definition.)
Now we can apply
Lemma 2, we have three possibilities:

4. By
scheme of Section

the general

1. ZL+l is a support

vector

only in -1-picture;

2. Zl+l is a support

vector

only in 1-picture;

3. Zl+l is a support

vector

in both pictures.

Let cL1 be the fraction
cL1 and J1 are small (as already

of the support
and J1 in the 1-picture;

-1-picture

typically
of our prediction  is

vectors

in the

we will assume that
mentioned,

this is

the case). In cases 1 and 2, the incertitude

(7)

5  SV IMPLEMENT AT ION

section

we described

a general  pre

In the previous
diction
ser's [3) procedure
this section
tation

of this general

scheme (in particular,

of nonparametric

prediction);

this scheme covers
Fra
in

we shall consider

a powerful

implemen

scheme.

vectors

(see Cortes

of support

one of the possi

3 and A.2, or Vapnik [7)). This defini

To begin with, we briefly describe
ble definitions
nik [1), Sections
tion is usually
their images
tion. In this paper, we shall always assume that this
transformation
the general

is identical; extension

under some, often

not to the original

case is trivial.

data but to
non-linear,
transforma

of our results

applied

and Vap

to

YL+d), where Xi E

Examples
positive

Let our data be ((x1, yl), ... , (xl+l,
IRn and Yi E  { -1, 1}, i E { 1, ... , l +1} (our notation
l+
1 for the sample size is chosen for agreement
with the
with Yi =  1 (resp.
rest of the paper).
negative).
-1) will be called
the quadratic

(l+l )
<I> ( w, ) = 2 ( w  w) + C t; ; -+ min (3)
(wE lRn,  = (6, ... ,l+I) E JR1+1),

(resp.
optimization
problem

Yi =

1

Consider

where Cis an a priori
to the constraints

fixed positive

constant,

subject

i2:0, i=1, ... ,l+l.

(5)
Lemma 1 Quadratic
(3) with
(4) and (5) has a unique solution
the sample (x1, y1), . . .  , (xl+l, YL+d contains
provided
constraints
both pos

optimization  problem

itive and negative

examples.

Let

Proof First,

it is clear that a solution

exists.

( w(l), b(l), (1)) , ( w(2), b(2), (2))
(where (j) =  ( dj), ... , 1S}1), j =  1, 2) be any two
(w(l) + w(2) b(l) + b(2) (1) + (2))

solutions.

Their mixture

2  '  2  '  2

constraints

will satisfy
strict
a smaller

( 4) and (5) and, because
of the
w(l) =  w(2)

unless

convexity

of the functional

<I>(w, ), will provide

value for this functional

and (l) = (2). Therefore,
w and  are determined
uniquely.
and b = b2 and b1 =f. b2, then all i are zero (otherwise,

at both b =  b1

If the minimum is attained

m  x -b  1

in other  words,
case 3, whatever
sure of impossibility

we make a confident
Yl+l turns up, our permutation
p will take a large value (at least
) , and so this case is hardly possible.

In
mea

prediction.

a d 6 )

Remark 3 Even in case 3 our algorithm
a confident

that cL1 and J1 are

prediction

(assuming

still gives

152 Gammerman, Vovk, and Vapnik

which looks counterintuitive.
quoting

A. P. Dawid sug
both min(L1,81) and max(L1,8I) as
inference.

small),
gested
the stochastic
Notice that (7) is the incertitude
arg maxy 8y in case 3 as well. This justifies
the al
gorithm
in the Introduction.
We can see that Vapnik's SV method provides
sures of impossibility
the scheme of Section
that:

mea
that are especially
to
4. The reason why this is so is

described

in the inductive

of transductive
inference,

that inductive

infer

that, given the training

set

of our procedure

we are interested

In this  section
implications
ence (with k = 1); recall
for our problem, requires
z1, ... , zz, we should work out a general
sifying
a future object
our procedure
and what we are interested
of this rule.

describes

x as -1 or 1. It is clear that
such a general

rule implicitly

,

in are the explicit

aspects

rule for clas

Let us solve the quadratic

optimization

problem

of the prediction

well-suited

 there are usually
 Zl+l is a support

tures.

few support

vectors;

vector

in at least one of the pic

Yi ((w Xi)+ b) ;:::  1-i, i ;::: 0, i = 1, ... , l,

which is an analogue

of (3)-(5)

for the training
set.

by "essential

notion being defined as fol

vector if the value of the optimization

the latter
(xi, Yi ), j E {1, . .. , l+ 1 }, is an essential

will
Remark 4 It is clear that the above argument
hold if we replace  "support
vectors"
sup
port vectors",
lows. A vector
support
lem (3)-(5)
from the sum in (3) and deleting
and (5) corresponding
ple shows that these two notions
essential
sider the set ( ( x1, yt), ... , ( xwo, Ywo)) of 100 classified
examples

prob
the term J
in ( 4)
to i = j. The following
exam
and
Con

vectors
are indeed different.

does not change after deleting

the constraints

in the plane defined as

vectors)

(support

support

Xi= (i,-1),

Yi = -1, i = 1, ... , 50,

Xi= (i-50, 1), Yi = 1, i =51, ... ' 100.

Here we have 100 support
port vectors.

vectors

and no essential

sup

Let the unique (see Lemma 1) solution
be ( w*, b*, C), and let the number of support
be N. We shall say that x is a y-point,

to this problem
vectors
y E { -1, 1}, if

y ((w*  x) + b*) > 1.

predict

with incertitude

It is easy to see that our method will always
y for a y-point
#SV( -y) is the number of support
picture);
vectors
be reliable.
derland"

#S'(J-; y) (recall
#S':'.t(-;y) of support

if the fraction
is small,

x belongs
The situation where

in the -y-picture

our prediction

therefore,

vectors

to the "bor

in the -y

that

will

i(w*  x) + b*l  1

our algorithm

is more complicated:
depend on the exact positions
ative examples;
this border region

to explicate

of the positive
and neg
our prediction

rule inside

is an interesting

open problem.

's prediction

will

We omit the derivation
confidences
(see the end of Section
described

at the end of Section

to the predictions

4.

of the procedure

of assigning

made by the SV machine

2) from the general

procedure

6 TRAN SDUCTION AND

INDUCTION

is naturally

related

to a set

as instance-based,

or case-based
algorithm

the most well-known

in
algorithm.

The trans

Perhaps,

neighbour

Remark 5 Transduction
of algorithms known
learning.
this class is k-nearest
ductive
is not based on the similarities
between
most of the instance-based
selection
tors allows
bility

vectors, and

us to introduce

described

measures.

algorithm

of support

in this paper, however,

(as
examples
techniques),
but relies
on

using the support vec

the confidence

and possi

for the problem of pattern

is inference from particular

to par
recognition,
it
Yi, i  = 1, . .. , l,
set, we are
to guess the classifications

"Transduction"
ticular;
means that, given the classifications
x1, . .. , X! in the training
of the l points
only trying
x1+1, ... , X!+k in the test set. In the main part of this
paper we only consider
to the case k > 1 (see
methods
Subsection 8.3

the case k = 1, though our

can be easily
below).

of the k points

extended

7  COMPUTER  EXPERIMENTS

the transductive

vari

for testing

Some experiments
ant of the SV method (described
have been conducted.
tern recognition
problem
using a database
digits
its, where each digit was a 16 x 16 vector

We have chosen a simple pat
of identifying

of US postal

in Section

data of 9300 dig

(cf. LeCun

5 above)

handwritten

Learning by Transduction 153

for a sub

for the training
set and

were conducted

for the test

et al. [6]). The experiments
set of these data (800 examples
set), and included
100 examples
tion of two-class
to separate
a digit "7". A set of preliminary
that the minimum number of errors
polynomials

classifier

of degree 2.

experiments
showed
is achieved
with

a construc
a digit "2" from

1.0 .-------------,---------

described

ly recognised

out of 100 examples

2,
digit 2 was
as 7 and three  times
digit 7 was
example.
For

The transductive  algorithm,
in Section
made 5 errors
mistaken
recognised
comparison
vector
machine
nised as digit 7).

as 2) with one undecided
the results
of prediction
show just 1 error (digit

2 was recog

(twice

using the support


 0.5
 a.

93-O's

1-X
6-O's
<G

Our explanation
fewer mistakes
cases where the new example
in one of the pictures
made no mistakes.
ample is a support
case the transductive
confidence,
predicts
larger number
be the wrong picture
the power
actly by the fact that
be separated
We can see that optimizing
performance
tary tasks,
between
search.

and we believe

of support

of why support

vector
than our algorithm

makes
is as follows.

machine

The

o.oo.Lo---

 ---

is not a support

vector

are easy and both algorithms

So let us suppose

that the new ex

vector in both pictures.

In this

o-'-.5---

 -- ---:-"1.0

Confidence

algorithm,

trying to optimize

Figure 1: Measures

of Confidence

and Possibility

according

with a
to the picture
But typically
it will

vectors.

that have more support

vectors:

of support vector machines

is explained

ex

real-world

data sets can usually

with a small number of support vectors.

confidence

and optimizing

are, at least to some degree,
that studying

complemen
the trade-off
of future

re

them is an interesting

direction

how to identify

problem:

to medical

diagnostic

(or diseases)

bility
the disease
for a new patient
symptoms given a set of past patients
The records were collected
at a hospital
and our main purpose
of the transductive
classifiers
Thatcher

(such as those presented
[5]).

with various

is to compare

algorithm

with certain
record data.
in Scotland,

in Gammerman and

the performance

alternative

using the support  vector  machine  with

to combine

the strength
of pre
mea
obtained

and possibility

through

approach
2). The results

(as described
are presented

in the end
in Figure 1.

the data have been split into two clusters:

decided

We therefore
diction
sures of confidence
our transductive
of Section
Clearly,
with possibility
with possibility
correct
ter and 5 correct
sifications
general

measure
less than 1 (cluster

equal to 1 (cluster
1), and
2). There are 93

classifications
(denoted

and 1 incorrect
in the second cluster.
characteristics

for both clusters.

by O's) in the first clus
by X's) clas
Table 1 gives some

(denoted

on a number of parameters

modifications

The SV method depends
C in (3); it is clear that there
(such as the constant
are many possible
of the SV method:
say, we could replace a by ef+c5, where J > 0, in (3)).
in the transductive
especially
It is important,
of the SV method (see Section
5), that the number
of the support
conduct
parameters

should be small. We plan to
of

experiments

are  best

vectors

variant

for determining  which  values
in practice.

We expect that good

Table 1: Some characteristics
(which can be identified
in Figure 1

by their average

of the two clusters

possibility)

One of the results
that we can assess
possibility
measure:
with high accuracy
to 1; and the
us to classify
characterised

that follow
the quality

from these experiments
is
of the data by using the

can be
the new example
when possibility
measure

is close

classified

data which do not enable

poor quality
the new example confidently
with low measure

are usually
of possibility.

We are currently
together

the described
with the measures of confidence

algorithms
and possi-

applying

CLUSTER

1

2

Minimal
confidence
0.883
Maximal confidence
0.915
confidence
Average
0.902
possibility
Average
1

0.901
0.914
0.910
0.0906

154 Gammerman, Vovk, and Vapnik

will be obtained

results
1
2(w. w) + cL.::t+<>-+ min,

for the objective

function

i

the degree of Zi 's "supportiveness";

for
we could use the value of the Lagrange

into account
example,
tiplier
to (6) that will allow us to cope with the distortion
phenomenon

O:i corresponding

to zi. A possible

alternative

mul

is

with C large and  8 >  0 small (or even 8 = 0). The
8 > 0 ensures
requirement
that the objective
is strictly
convex;
sible and the arguments

therefore,

of Section

5 apply.

function

it is computationally

fea

p(zt, ... ,zz+t)=f()

f(o:z+t)(l + 1)
O:t + ... + 0:!+1

f(  )'  (8)

8  DISCUSSION

In this section
rections
developed

further.

we will very briefly

discuss

di
of this paper could be

possible

in which the results

non-decreasing
function
are defined

vectors

support

where f is some monotonic
with f(O) = 0. Sometimes
as the vectors
(6) becomes
function
when o: > 0).

Zi for which O:i > 0; under this definition
case of (8) corresponding
a special

to the

f(o:) =signa: (that is, f(O) = 0 and f(o:) = 1

8.1 REGRESSION

8.3 MORE  THAN  ONE UNCLASSIFIED

EXAMPLE

problem

is to extend our ap

direction

estimation

the classifications

of research
feasible
way to the prob
(see Vapnik [7]). In the
Yi are no longer

An important
proach in a computationally
lem of regression
latter
re
quired to be binary and can take any real values.
In
the regression
case the key observation
(which is an
of Lemma 2 above) is the following:
analogue
if the
classifications
are
2E apart,
in at least one of these two pic
more than
tures the last object will be a support
vector.
our tolerance
E is the constant
inaccurate
of E or less from the
true classification
that
if the fraction
we will be able to give a prediction
most E and high confidence.

of the last object in two pictures

are not punished.)

predictions:

that specifies

deviations

of support

vectors

with accuracy

(Here

at

towards

This implies

is small in all pictures,

to predict

If our task is
of k new examples
cations
training

Yl+l, ... , Yl+k
the classifications
Xt+l, ... , Xt+k given the classifi
Xt, . . .  , xz in the
to

Yt, ... , Yt of the examples
set, (8) can be generalized

PZl,  ,Zl+l -
(

) - f(o:l+l) + ... + f(o:t+k) l + k.
O:t + ... + O:!+k
f( )  f(  ) -k-,

it is easy to check that this formula defines a valid
permutation

of impossibility.

measure

W ith each possible

prediction

Yl+l = a1,  .. , Yl+k = ak

we can  associate

its incertitude

1

8.2 DISTORTION PHENOMENON

min(Yl+l>,Yl+k)#(al

, ... ,ak) p( (xt' Yt),   ' (xl+k' Yt+k))

is typically

and make a prediction

with the smallest

incertitude.

vectors

algorithm

all data points

is determined

by the number of support

vectors.
In our
of a correct
pre

our data are far from being ran
random data we can expect that

number of support
usually

will be support
the incertitude

The relative
small because
dom; for completely
nearly
transduction
diction
in the "wrong picture",
that if that picture
ber of support
confidence
observed
there is little
for very large data sets; in this subsection
a possible

and so the
will drop. We have not
yet, but

of our prediction

in our experiments

this phenomenon

will grow sharply

is "too wrong",

and a natural

the relative

vectors

remedy.

num

we discuss

vectors

apprehension

is

doubt that it will be a serious obstacle

8.4 NON-CONTINUOUS  CASE

to generalize

It is easy (but tedious)
to the case of a probability  distribution
necessarily
generalize
impossibility

continuous;
the definition

we shall only
measure

of a permutation
of

in this subsection

all our results

that is not

.
in Z is a subset of Z to each element
of
num
integer
is the sum of the
of a finite se

A hyperset
which is assigned
ber); the cardinality of a hyperset
arities
quence

some arity (a positive

of its elements.

The signature

The value of our permutation
(see (6)) depends
on every example
being a support
vector.
A natural
Zi is a support
just whether

Zi only through
Zi
idea is to use not
vector,

but to take

or not

measure of impossibility

is the hyperset
the arity of each element
it occurs

in (9).

consisting

of all elements

of (9), with

equal to the number of times

(9)

Learning by Transduction 155

distri

is analogous

distributions

of pm(z)-measures
of impos

pm in zm (with p running
continuous,
of a subclass

We let pm ( Z) stand for the set of all product
butions
over all, not nec
essarily
definition
sibility
measures
measurable
of impossibility
measure
min Z,
dinality
1
N  L  p(z1, ... ,zm) = 1,

in Z). The following
to the definition
of permutation
3. A non-negative
p : zm ---+ 1R is an exchangeable

of impossibility

if, for any hyperset

in Section

function

b of car

(z1 , ... ,z,.) of signature

b

where N is the number of all possible
(z1, ... , zm) of signature
arities
N =  (h+,+b/)1).
to its elements,

b (if b assigns
bl ... .  b,.

sequences
b1, ... , bj

8.5 THE EXCHANGEABILITY  MODEL

son, Editors).
pp. 89-105.

W iley, New York, 1983, vol. 4,

[3) D. A. S. Fraser. Sequentially
Ann.  Math.

determined
Statist.

statisti
22:372,

cally equivalent blocks.
1951.

[4] A. Gammerman.

Machine

learning:

progress

prospects.
partment
University

Technical
of Computer
of London,

Report CSD-TR-96-21,
Royal Holloway,
1996.

Science,

December

and
De

[5] A. Gammerman

and A. R. Thatcher.

Bayesian

di

agnostic
dence of symptoms.
ics, 1992, pp. 323-330.

probabilities
without
Yearbook

assuming

indepen
Informat

of Medical

[6) Y.  LeCun,

B. Boser, J. S. Denker,

D. Hender

son, R. E. Howard,
Handwritten
tion network.
Processing
pp. 396-404.

Systems

W .  Hubbard,

and L. J. Jackel.

digit recognition

with backpropa
ga
2. Morgan Kaufmann,

1990,

in Neural Information

Advances

[7] V. N. Vapnik.

The Nature of Statistical

Learning

Theory. Springer,

New York, 1995.

[8] V. G. Vovk. On the concept

of the Bernoulli

prop

erty. Russ. Math. Surv. 41:247-248,

1986.

[9] V .  G. Vovk. A logic of probability
of statistics
Soc. B 55:317-351,

to the foundations
R. Statist.

, with application
(with discussion).
1993.

J.

(10] V. G. Vovk and V .  V .  V'yugin.

validity
B 55:253-266, 1993.

of the Bayesian

method.

On the empirical
J. R. Statist.
Soc.

model

were gen

source.

is strictly

that our examples

which only assumes

What we actually

under the exchangeability

used,
was not this i.i.d. model but a weaker model

So far we have assumed
erated by an i.i.d.
however,
that the ex
of exchangeability,
amples z1, ... , Zt+k are equiprobable.
The example
model shows that the model of ex
of the Bernoulli
changeability
weaker than the i.i.d.
(see, e.g, [8]). It is clear that the scheme
4
is "universal"
remains
of the extra strength
other hand, if the idea of replacing
by the exchangeability
it would be natural
surability
of impossibility
sibility
practical
easily
sures of impossibility).

model, but it
one can make use
On the
of the i.i.d. assumption.
model

to drop the requirement
of a permutation
of impos

the i.i.d.
,
of mea
measure

and exchangeable
this would make little

model is to be taken seriously

applications:

in the definitions

an open question

and so a fortiori

computable,

measurable,

in practice

(however,

difference

of Section

measure

whether

mea

in

we are interested in

Acknowledgments

support

financial

Learning

for providing

We thank  EPSRC
through grant GR/L35812
Bayesian
ments by the members of Program Committee
gratefully
to Craig
Saunders
computer
experiments.

We are also grateful
for help  with

appreciated.
and Mark Stitson

("Support Vector and
com

Algorithms").

Enlightening

are

