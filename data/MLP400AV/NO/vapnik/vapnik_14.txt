Abstract. The problem of automatically tuning multiple parameters for pattern recognition Support Vector
Machines (SVMs) is considered. This is done by minimizing some estimates of the generalization error of SVMs
using a gradient descent algorithm over the set of parameters. Usual methods for choosing parameters, based
on exhaustive search become intractable as soon as the number of parameters exceeds two. Some experimental
results assess the feasibility of our approach for a large number of parameters (more than 100) and demonstrate
an improvement of generalization performance.

Keywords:
selection

support vector machines, kernel selection, leave-one-out procedure, gradient descent, feature

Introduction

1.
In the problem of supervised learning, one takes a set of input-output pairs Z = {(x1, y1),
. . . , (x(cid:6), y(cid:6))} and attempts to construct a classier function f that maps input vectors x  X
onto labels y Y. We are interested here in pattern recognition or classication, that is the
case where the set of labels is simply Y = {1, 1}. The goal is to nd a f F which
minimizes the error ( f (x) (cid:8)= y) on future examples. Learning algorithms usually depend
on parameters which control the size of the class F or the way the search is conducted in F.
Several techniques exist for performing the selection of these parameters. The idea is to nd
the parameters that minimize the generalization error of the algorithm at hand. This error can
be estimated either via testing on some data which has not been used for learning (hold-out
testing or cross-validation techniques) or via a bound given by theoretical analysis.

Tuning multiple parameters. Usually there are multiple parameters to tune at the same time
and moreover, the estimates of the error are not explicit functions of these parameters, so

132

O. CHAPELLE ET AL.

that the naive strategy which is exhaustive search in the parameter space becomes intractable
since it would correspond to running the algorithm on every possible value of the parameter
vector (up to some discretization). We propose here a methodology for automatically tuning
multiple parameters for the Support Vector Machines (SVMs) which takes advantage of the
specic properties of this algorithm.

The SVM algorithm. Support vector machines (SVMs) realize the following idea: map a
n-dimensional input vector x  Rn 1 into a high dimensional (possibly innite dimensional)
feature spaceH by  and construct an optimal separating hyperplane in this space. Different
mappings construct different SVMs.
When the training data is separable, the optimal hyperplane is the one with the maximal
distance (in H space) between the hyperplane and the closest image (xi ) of the vector xi
from the training data. For non-separable training data a generalization of this concept is
used.

Suppose that the maximal distance is equal to  and that the images (x1), . . . , (x(cid:6)) of
the training vectors x1, . . . , x(cid:6) are within a sphere of radius R. Then the following theorem
holds true (Vapnik & Chapelle, 2000).
Theorem 1. Given a training set Z = {(x1, y1), . . . , (x(cid:6), y(cid:6))} of size (cid:6), a feature space
H and a hyperplane (w,b), the margin  (w, b, Z ) and the radius R(Z ) are dened by

yi (w  (xi ) + b)

 (w, b, Z ) = min
(xi ,yi )Z
R(Z ) = min

(cid:11)w(cid:11)
(cid:11)(xi ) + a(cid:11)

a,xi

The maximum margin algorithm L (cid:6) : (X  Y)(cid:6) H R takes as input a training set of size
(cid:6) and returns a hyperplane in feature space such that the margin  (w, b, Z ) is maximized.
Note that assuming the training set separable means that  > 0. Under this assumption, for
all probability measures P underlying the data Z, the expectation of the misclassication
probability

perr (w, b) = P(sign(w  (X) + b) (cid:8)= Y )
(cid:3)

(cid:2)

has the bound

E{ perr (L (cid:6)1(Z ))}  1

E

R2(Z )

.

(cid:6)

 2(L(Z ), Z )

The expectation is taken over the random draw of a training set Z of size (cid:6)  1 for the left
hand side and size (cid:6) for the right hand side.

This theorem justies the idea of constructing a hyperplane that separates the data with a
large margin: the larger the margin the better the performance of the constructed hyperplane.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

133

Note however that according to the theorem the average performance depends on the ratio
E{R2/ 2} and not simply on the large margin  .

Why multiple parameters? The SVM algorithm usually depends on several parameters.
One of them, denoted C, controls the tradeoff between margin maximization and error
minimization. Other parameters appear in the non-linear mapping into feature space. They
are called kernel parameters. For simplicity, we will use a classical trick that allows us
to consider C as a kernel parameter, so that all parameters can be treated in a unied
framework.

It is widely acknowledged that a key factor in an SVMs performance is the choice of the
kernel. However, in practice, very few different types of kernels have been used due to the
difculty of appropriately tuning the parameters. We present here a technique that allows
to deal with a large number of parameters and thus allows to use more complex kernels.

Another potential advantage of being able to tune a large number of parameters is the
possibility of rescaling the attributes. Indeed, when no a priori knowledge is available about
the meaning of each of the attributes, the only choice is to use spherical kernels (i.e. give the
same weight to each attribute). But one may expect that there is a better choice for the shape
of the kernel since many real-world database contain attributes of very different natures.
There may thus exist more appropriate scaling factors that give the right weight to the right
feature. For example, we will see how to use radial basis function kernels (RBF) with as
many different scaling factors as input dimensions:

(cid:6)

.

(xi  zi )2

2 2
i

i

(cid:4)

(cid:5)

K (x, z) = exp



The usual approach is to consider  = 1 =  = n and to try to pick the best value for
 . However, using the proposed method, we can choose automatically good values for the
scaling factors i . Indeed, these factors are precisely parameters of the kernel.

Moreover, we will demonstrate that the problem of feature selection can be addressed
with the same framework since it corresponds to nding those attributes which can be
rescaled with a zero factor without harming the generalization.

We thus see that tuning kernel parameters is something extremely useful and a procedure
that allows to do this would be a versatile tool for various tasks such as nding the right
shape of the kernel, feature selection, nding the right tradeoff between error and margin,
etc. All this gives a rationale for developing such techniques.

In summary, our goal is not only to nd the hyperplane which maximizes
Our approach.
the margin but also the values of the mapping parameters that yield best generalization
error. To do so, we propose a minimax approach: maximize the margin over the hyperplane
coefcients and minimize an estimate of the generalization error over the set of kernel
parameters. This last step is performed using a standard gradient descent approach.

What kind of error estimates. We will consider several ways of assessing the generalization
error.

134

O. CHAPELLE ET AL.

 Validation error: this procedure requires a reduction of the amount of data used for
learning in order to save some of it for validation. Moreover, the estimates have to be
smoothed for proper gradient descent.
 Leave-one-out error estimates: this procedure gives an estimate of the expected general-

ization as an analytic function of the parameters.

We will examine how the accuracy of the estimates inuences the whole procedure of nding
optimal parameters. In particular we will show that what really matters is how variations of
the estimates relate to variations of the test error rather than how their values are related.

Outline. The paper is organized as follows. The next section introduces the basics of
SVMs. The different possible estimates of their generalization error are described in
Sections 3 and 4 explains how to smooth theses estimates. Then we introduce in
Section 5 a framework for minimizing those estimates by gradient descent. Section 6 deals
with the computation of gradients of error estimates with respect to kernel parameters. Fi-
nally, in Sections 7 and 8, we present experimental results of the method applied to a variety
of databases in different contexts. Section 7 deals with nding the right penalization along
with the right radius for a kernel and with nding the right shape of a kernel. In Section 8
we present results of applying our method to feature selection.

2. Support vector learning

We introduce some standard notations for SVMs; for a complete description, see (Vapnik,
1998). Let {(xi , yi )}1i(cid:6) be a set of training examples, xi  Rn which belong to a class
labeled by yi  {1, 1}. In the SVM methodology, we map these vectors into a feature
space using a kernel function K (xi , x j ) that denes an inner product in this feature space.
Here, we consider a kernel K depending on a set of parameters . The decision function
given by an SVM is:
f (x) = sign

(cid:4)
(cid:6)(cid:5)

(cid:6)

(1)

0

i are obtained by maximizing the following functional:

i  j yi y j K (xi , x j )

i, j=1

(2)

i=1

,

i yi K (xi , x) + b
(cid:6)(cid:5)

and

i  0,

i = 1, . . . , (cid:6).

where the coefcients 0
i  1
2

W () = (cid:6)(cid:5)
(cid:6)(cid:5)

under the constraints
i yi = 0

i=1

i=1

The coefcients 0
i dene a maximal margin hyperplane in a high-dimensional feature
space where the data are mapped through a non-linear function  such that (xi ) (x j ) =
K (xi , x j ).

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

135

This formulation of the SVM optimization problem is called the hard margin formulation
since no training errors are allowed. Every training point satises the inequality yi f (xi )  1
and for points xi with corresponding i > 0 an equality is satised. These points are called
support vectors.
Notice that one may require the separating hyperplane to pass through the origin by
choosing a xed b = 0. This variant is called the hard margin SVM without threshold. In
(cid:7)
that case, the optimization problem remains the same as above except that the constraint

i yi = 0 disappears.

Dealing with non-separability. For the non-separable case, one needs to allow training
errors which results in the so called soft margin SVM algorithm (Cortes & Vapnik, 1995). It
can be shown that soft margin SVMs with quadratic penalization of errors can be considered
as a special case of the hard margin version with the modied kernel (Cortes & Vapnik,
1995; Cristianini & Shawe-Taylor, 2000).

K  K + 1
C

I,

(3)

where I is the identity matrix and C a constant penalizing the training errors. In the rest of
the paper, we will focus on the hard margin SVM and use (3) whenever we have to deal
with non-separable data. Thus C will be considered just as another parameter of the kernel
function.

3. Estimating the performance of an SVM

Ideally we would like to choose the value of the kernel parameters that minimize the true
risk of the SVM classier. Unfortunately, since this quantity is not accessible, one has to
build estimates or bounds for it. In this section, we present several measures of the expected
error rate of an SVM.

3.1.

Single validation estimate

If one has enough data available, it is possible to estimate the true error on a validation
set. This estimate is unbiased and its variance gets smaller as the size of the validation set
increases. If the validation set is {(x(cid:18)

)}1i p, the estimate is

(cid:18)
, y
i

i

p(cid:5)

i=1

T = 1
p

(y

(cid:18)
(cid:18)
i f (x
i

)),

(4)

where  is the step function: (x) = 1 when x > 0 and (x) = 0 otherwise.

136

O. CHAPELLE ET AL.

3.2. Leave-one-out bounds

The leave-one-out procedure consists of removing from the training data one element,
constructing the decision rule on the basis of the remaining training data and then testing
on the removed element. In this fashion one tests all (cid:6) elements of the training data (using (cid:6)
different decision rules). Let us denote the number of errors in the leave-one-out procedure
by L(x1, y1, . . . , x(cid:6), y(cid:6)). It is known (Luntz & Brailovsky, 1969) that the the leave-one-out
procedure gives an almost unbiased estimate of the expected generalization error:

Lemma 1.
E p(cid:6)1

err

= 1
(cid:6)

E (L(x1, y1, . . . , x(cid:6), y(cid:6))),

where p(cid:6)1
and the expectations are taken over the random choice of the sample.

is the probability of test error for the machine trained on a sample of size (cid:6) 1

err

Although this lemma makes the leave-one-out estimator a good choice when estimating
the generalization error, it is nevertheless very costly to actually compute since it requires
running the training algorithm (cid:6) times. The strategy is thus to upper bound or approximate
this estimator by an easy to compute quantity T having, if possible, an analytical expression.
If we denote by f 0 the classier obtained when all training examples are present and f i

the one obtained when example i has been removed, we can write:

L(x1, y1, . . . , x(cid:6), y(cid:6)) = (cid:6)(cid:5)
L(x1, y1, . . . , x(cid:6), y(cid:6)) = (cid:6)(cid:5)

p=1

p=1

which can also be written as

(yp f p(x p)),

(5)

(yp f 0(x p) + yp( f 0(x p)  f p(x p))).

Thus, if U p is an upper bound for yp( f 0(x p)  f p(x p)), we will get the following upper
bound on the leave-one-out error:

L(x1, y1, . . . , x(cid:6), y(cid:6))  (cid:6)(cid:5)

(U p  1),

p=1

since for hard margin SVMs, yp f 0(x p)  1 and  is monotonically increasing.

3.2.1. Support vector count. Since removing a non-support vector from the training set
does not change the solution computed by the machine (i.e. U p = f 0(x p)  f p(x p) = 0
for x p non-support vector), we can restrict the preceding sum to support vectors and upper

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

137

bound each term in the sum by 1 which gives the following bound on the number of errors
made by the leave-one-out procedure (Vapnik, 1995):

T = NSV

(cid:6)

,

where NSV denotes the number of support vectors.

3.2.2. Jaakkola-Haussler bound. For SVMs without threshold, analyzing the optimiza-
tion performed by the SVM algorithm when computing the leave-one-out error, Jaakkola
and Haussler (1999) proved the inequality:

yp( f 0(x p)  f p(x p))  0

p K (x p, x p) = U p

which leads to the following upper bound:

(cid:8)

(cid:6)(cid:5)



p=1

T = 1

(cid:6)

(cid:9)

.

p K (x p, x p) 1

0

Note that Wahba, Lin, and Zhang (2000) proposed an estimate of the number of errors
made by the leave-one-out procedure, which in the hard margin SVM case turns out to be

(cid:5)

T =

0
p K (x p, x p),

which can be seen as an upper bound of the Jaakkola-Haussler one since (x  1)  x for
x  0.

3.2.3. Opper-Winther bound. For hard margin SVMs without threshold, Opper and
Winther (2000) used a method inspired from linear response theory to prove the following:
under the assumption that the set of support vectors does not change when removing the
example p, we have

yp( f 0(x p)  f p(x p)) =

(cid:8)

(cid:9)

0
p
1
SV

K

,

pp

where KSV is the matrix of dot products between support vectors; leading to the following
estimate:

(cid:4)

(cid:8)

(cid:6)(cid:5)



p=1

(cid:9)

0
p
1
SV

K

1

pp

(cid:6)

.

T = 1

(cid:6)

138

O. CHAPELLE ET AL.

3.2.4. Radius-margin bound. For SVMs without threshold and with no training errors,
Vapnik (1998) proposed the following upper bound on the number of errors of the leave-
one-out procedure:

T = 1

(cid:6)

R2
 2

.

where R and  are the radius and the margin as dened in Theorem 1.

3.2.5. Span bound. Vapnik and Chapelle (2000) and Chapelle and Vapnik (1999) derived
an estimate using the concept of span of support vectors.

Under the assumption that the set of support vectors remains the same during the leave-

one-out procedure, the following equality is true:

where Sp is the distance between the point (x p) and the set  p where

yp( f 0(x p)  f p(x p)) = 0
(cid:5)

(cid:10) (cid:5)

p S2
p

,

i (xi ),

i(cid:8)= p

i(cid:8)= p, 0

i

>0

 p =

(cid:11)

i = 1

(cid:8)

(cid:6)(cid:5)



p=1

T = 1

(cid:6)

 1

0
p S2
p

(cid:9)

.

(6)

This gives the exact number of errors made by the leave-one-out procedure under the
previous assumption:

.

(7)

The span estimate can be related to other approximations:

If we consider SVMs without threshold, the con-
Link with Jaakkola-Haussler bound.
i = 1 can be removed in the denition of the span. Then we can easily upper
straint
 K (x p, x p), and thus recover the Jaakkola-Haussler bound.
bound the value of the span: S2
p
Link with R2/ 2. For each support vector, we have yp f 0(x p)= 1. Since for x  0,
(x  1) x, the number of errors made by the leave-one-out procedure is bounded by:

It has been shown (Vapnik & Chapelle, 2000) that the span Sp is bounded by the diameter
= 1/ 2, we nally get
of the smallest sphere enclosing the training points and since

(cid:7)

0
p

(cid:7)

(cid:5)

p

0
p S2
p

.

T  4

R2
 2

.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

139

A similar derivation as the one used in the span bound has been proposed in Joachims (2000),
where the leave-one-out error is bounded by|{ p, 20
p R2 > yp f 0(x p)}|, with 0 K (xi , xi )
R2,i.

Link with Opper-Winther. When the support vectors do not change, the hard margin case
without threshold gives the same value as the Opper-Winther bound, namely:

(cid:8)

=

S2
p

(cid:9)

1
1
SV

K

.

pp

4. Smoothing the test error estimates

The estimate of the performance of an SVM through a validation error (4) or the leave-
one-out error (5) requires the use of the step function . However, we would like to use a
gradient descent approach to minimize those estimates of the test error. Unfortunately the
step function is not differentiable. As already mentioned in Section 3.2.5, it is possible to
bound (x  1) by x for x  0. This is how the bound R2/ 2 is derived from the leave-one-
out error. Nevertheless by doing so, large errors count more than one, therefore it might be
advantageous instead to use a contracting function of the form (x) = (1 + exp(Ax +
B))1 (see gure 1).

However, the choice of the constants A and B is difcult. If A is too small, the estimate

is not accurate and A is too large, the resulting estimate is not smooth.

Instead of trying to pick good constants A and B, one can try to get directly a smooth
approximation of the test error by estimating posterior probabilities. Recently, Platt proposed
the following estimate of the posterior distribution P(Y = 1| X = x) of an SVM output f (x)
(Platt, 2000):

PA,B (x)= P(Y = 1| X = x)=

1

1+ exp(A f (x)+ B)

,

where f (x) is the output of the SVM. The constants A and B are found by minimizing the
Kullback-Leibler divergence between P and an empirical approximation of P built from a
validation set (x(cid:18)

)1inv :

(cid:18)
, y
i

i

, B

(A

) = arg max

A,B

(cid:18)
i

1 + y
2

log( PA,B (x

(cid:18)
i

)) + 1  y

(cid:18)
i

2

log(1  PA,B (x

(cid:18)
i

(cid:12)

nv(cid:5)

i=1

(cid:13)

))

.

This optimization is carried out using a second order gradient descent algorithm (Platt, 2000).
is such that
 (cid:8)= 0, we obtained a correction compared to

f (x) = sign( PA,B (x)  0.5). Note that if B

According to this estimate the best threshold for our SVM classier f

the usual SVM threshold.

140

O. CHAPELLE ET AL.

Figure 1. Validation error for different values of the width (in log scale) of an RBF kernel. Top left: with a
step function, (x)= 1x > 0. Top right: sigmoid function, (x) = (1 + exp(5x))1. Bottom: linear function,
(x) = 1 + x for x > 1, 0 otherwise. Note that on the bottom picture, the minimum is not at the right place.

By denition the generalization error of our classier is
P(Y = 1| x)d(x) +

P(Y (cid:8)= f (X )) =

x, f (x)=1

(cid:14)

(Y = 1| x)d(x).

x, f (x)=1

(cid:14)

(cid:5)
= nv(cid:5)

i, P(x(cid:18)

i

i=1

This error can be empirically estimated as2:

P(Y (cid:8)= f (X )) 

(cid:5)

P(x
(cid:18)
i

) +

)<0.5

min( P(x

(cid:18)
i

i

i, P(x(cid:18)
), 1  P(x

)>0.5
(cid:18)
i

)).

1  P(x

(cid:18)
i

)

(8)

(9)

Note that the labels of the validation set are not used directly in this last step but indirectly

through the estimation of the constants A and B appearing in the parametric form of PA,B.

To have a better understanding of this estimate, let us consider the extreme case where there
is no error on the validation set. Then the maximum likelihood algorithm is going to yield

A = and PA,B (x) will only take binary values. As a consequence, the estimate of the

error probability will be zero.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

141

5. Optimizing the kernel parameters

Lets go back to the SVM algorithm. We assume that the kernel k depends on one or several
parameters, encoded into a vector  = (1, . . . , n). We thus consider a class of decision
functions parametrized by , b and :

(cid:4)
f,b, (x) = sign

(cid:6)(cid:5)

i=1

(cid:6)

i yi K (x, xi ) + b

.

We want to choose the values of the parameters  and  such that W (see Eq. (2)) is
maximized (maximum margin algorithm) and T , the model selection criterion, is minimized
(best kernel parameters). More precisely, for  xed, we want to have 0 = arg max W ()
and choose 0 such that

0 = arg min



T (0, ).

When  is a one dimensional parameter, one typically tries a nite number of values and
picks the one which gives the lowest value of the criterion T . When both T and the SVM
solution are continuous with respect to , a better approach has been proposed by Cristianini,
Campbell, and Shawe-Taylor (1999): using an incremental optimization algorithm, one can
train an SVM with little effort when  is changed by a small amount. However, as soon as
 has more than one component computing T (, ) for every possible value of  becomes
intractable, and one rather looks for a way to optimize T along a trajectory in the kernel
parameter space.

Using the gradient of a model selection criterion to optimize the model parameters has
been proposed in Bengio (2000) and demonstrated in the case of linear regression and
time-series prediction. It has also been proposed by Larsen et al. (1998) to optimize the
regularization parameters of a neural network.

Here we propose an algorithm that alternates the SVM optimization with a gradient step
is the direction of the gradient of T in the parameter space. This can be achieved by the
following iterative procedure:

1. Initialize  to some value.
2. Using a standard SVM algorithm, find the maximum of the

quadratic form W :
0() = arg max

W (, ).



3. Update the parameters  such that T is minimized.

This is typically achieved by a gradient step (see below).

4. Go to step 2 or stop when the minimum of T is reached.

Solving step 3 requires estimating how T varies with . We will thus restrict ourselves to
the case where K can be differentiated with respect to . Moreover, we will only consider
cases where the gradient of T with respect to  can be computed (or approximated).

142

O. CHAPELLE ET AL.

Note that 0 depends implicitly on  since 0 is dened as the maximum of W . Then, if
we have n kernel parameters (1, . . . , n), the total derivative of T 0()  T (0(),) with
respect to  p is:

(cid:15)(cid:15)(cid:15)(cid:15)

T 0
 p

= T 0
 p

+ T 0
0

0
 p

.

0 xed

Having computed the gradient  T (0, ), a way of performing step 3 is to make a

gradient step:
k = 

T (0, )

k

,

for some small and eventually decreasing . The convergence can be improved with the use
of second order derivatives (Newtons method):

k = ( T )1

T (0, )

k

where the Laplacian operator  is dened by

( T )i, j =  2T (0, )

i  j

.

In this formulation, additional constraints can be imposed through projection of the gradient.

6. Computing the gradient

In this section, we describe the computation of the gradient (with respect to the kernel
parameters) of the different estimates of the generalization error. First, for the bound R2/ 2
(see Theorem 1), we obtain a formulation of the derivative of the margin (Section 6.1) and of
the radius (Section 6.2). For the validation error (see Eq. (4)), we show how to calculate the
derivative of the hyperplane parameters 0 and b (see Section 6.3). Finally, the computation
of the derivative of the span bound (7) is presented in Section 6.4.

We rst begin with a useful lemma.

Suppose we are given a (n  1) vector v and an (n  n) matrix P smoothly

Lemma 2.
depending on a parameter . Consider the function:

L( ) = max
xF

xT v  1
2

xT P x

where

F = {x : bT x = c, x  0}.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

143

Let x be the the vector x where the maximum in L( ) is attained. If this minimum is unique
then

 L( )



= xT

v


xT

 1
2

x.

P


In other words, it is possible to differentiate L with respect to  as if x did not depend on
. Note that this is also true if one (or both) of the constraints in the denition of F are
removed.

Proof: We rst need to express the equality constraint with a Lagrange multiplier  and
the inequality constraints with Lagrange multipliers i :

L( ) = max

x,,

xT v  1
2

xT P x  (bT x  c) +  T x.

(10)

At the maximum, the following conditions are veried:

v  P x = b   ,
i.

bT x = c,
i xi = 0,

We will not consider here differentiability problems. The interested reader can nd de-
tails in Bonnans and Shapiro (2000). The main result is that whenever x is unique, L is
differentiable.

We have

 L( )



= xT

v


xT

 1
2

P


x + xT



(v  P x),

where the last term can be written as follows,

xT



(v  P x) = 

xT



b  xT



 .

Using the derivatives of the optimality conditions, namely

xT
b = 0,
 xi
= 0,



xi + i



 i



and the fact that either i = 0 or xi = 0 we get:

 i



xi = i

 xi



= 0,

144

hence

xT



(v  P x) = 0

O. CHAPELLE ET AL.

and the result follows.



6.1. Computing the derivative of the margin
Note that in feature space, the separating hyperplane {x : w(x)+b = 0} has the following
expansion

w = (cid:6)(cid:5)

i=1

0
i yi (xi )

and is normalized such that

yi (w  (xi ) + b) = 1.

min
1i(cid:6)

It follows from the denition of the margin in Theorem 1 that this latter is  = 1/(cid:11)w(cid:11). Thus
we write the bound R2/ 2 as R2(cid:11)w(cid:11)2.
The previous lemma enables us to compute the derivative of (cid:11)w(cid:11)2. Indeed, it can be

shown (Vapnik, 1998) that
(cid:11)w(cid:11)2 = W (0),

1
2

and the lemma can be applied to the standard SVM optimization problem (2), giving

=  (cid:6)(cid:5)

i, j=1

(cid:11)w(cid:11)2
 p

0
i

0

j yi y j

 K (xi , x j )

 p

6.2. Computing the derivative of the radius

Computing the radius of the smallest sphere enclosing the training points can be achieved
by solving the following quadratic problem (Vapnik, 1998):

(cid:6)(cid:5)

i=1

i K (xi , xi )  (cid:6)(cid:5)

i, j=1

R2 = max



i  j K (xi , x j )

(cid:6)(cid:5)
under constraints
i = 1
i=1
i i  0

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

145

We can again use the previous lemma to compute the derivative of the radius:

= (cid:6)(cid:5)

i=1

 R2
 p

 (cid:6)(cid:5)

i, j=1

0
i

 K (xi , xi )

 p

i  j

 K (xi , x j )

 p

,

where 0 maximizes the previous quadratic form.

6.3. Computing the derivative of the hyperplane parameters

Let us rst compute the derivative of 0 with respect to a parameter  of the kernel. For
this purpose, we need an analytical formulation for 0. First, we suppose that the points
which are not support vectors are removed from the training set. This assumption can be
done without any loss of generality since removing a point which is not support vector does
not affect the solution. Then, the fact that all the points lie on the margin can be written

(cid:6)

(cid:4)

(cid:6)

(cid:4)
(cid:16)

(cid:4)

(cid:6)
(cid:19)

KY Y
YT
0

1
0

=

0
b

,

(cid:17)(cid:18)
= yi y j K (xi , x j ). If there are n support vectors, H is a (n + 1)(n + 1) matrix.

H

where KY
ij
The parameters of the SVMs can be written as:

(0, b)T = H

1(1 1 0)T .

We are now able to compute the derivatives of those parameters with respect to a kernel
parameter  p. Indeed, since the derivative of the inverse of a matrix M depending on a
parameter  p can be written3

(11)

M1
 p

= M

1

1,

M

M
 p

it follows that

(0, b)

 p

= H

1

H
 p

1(1 1 0)T ,
H

and nally

(0, b)

 p

= H

1

H
 p

(0, b)T.

146

O. CHAPELLE ET AL.

We can easily use the result of this calculation to recover the computation (cid:11)w(cid:11)2
we denote = (0, b), we have (cid:11)w(cid:11)2 = (0)T KY 0 = T H  and it turns out that:

 p

. Indeed, if

(cid:11)w(cid:11)2
 p

= T
H
 p
= T
H
 p
=  T

 
 + 2 H
 p
  2 HH
1

H
 p
= (0)T

0.

KY
 p



H
 p

6.4. Computing the derivative of the span-rule

Now, let us consider the span value. Recall that the span of the support vector x p is dened
as the the distance between the point (x p) and the set  p dened by (6). Then the value
of the span can be written as:

(cid:4)

(cid:5)

= min



S2
p

max



(x p) 

i (xi )

i(cid:8)= p

(cid:6)2 + 2

(cid:4)(cid:5)

i(cid:8)= p

(cid:6)

i  1

.

(cid:7)

i = 1.

Note that we introduced a Lagrange multiplier  to enforce the constraint
Introducing the extended vector  = (T )T and the extended matrix of the dot products
between support vectors

(cid:4)

(cid:6)

KSV =

K 1
1T
0

,

the value of the span can be written as:

= min



S2
p

max



(K (x p, x p)  2vT  + 

T H),

where H is the submatrix of KSV with row and column p removed, and v is the p-th column
of KSV .
From the fact that the optimal value of  is H1v, it follows:

S2
p

= K (x p, x p)  vT H
= 1/

(cid:8) K

(cid:9)

1
SV

.

pp

1v

(12)

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

147

The last equality comes from the following block matrix identity, known as the Woodbury
formula (Lutkepohl, 1996)

where B1 = (A1  AA
The closed form we obtain is particularly attractive since we can compute the value of

the span for each support vector just by inverting the matrix KSV .
Combining Eqs. (12) and (11), we get the derivative of the span

(cid:4)

A1 AT
A A2

(cid:4)

(cid:6)1 =

B1 BT
B B2
1
2 AT )1.

(cid:4)

 S2
p
 p

= S4

p

K

1
SV

 KSV
 p

K
1
SV

(cid:6)

,

(cid:6)

pp

Thus, the complexity of computing the derivative of the span-rule with respect to a
and the inversion of

parameter  p of the kernel requires only the computation of  K (xi ,x j )
the matrix KSV . The complexity of these operations is not larger than that of the quadratic

 p

optimization problem itself.

There is however a problem in this approach: the value given by the span-rule is not
continuous. By changing smoothly the value of the parameters , the coefcients  p change
continuously, but the span S2
p does not. There is actually a discontinuity for most support
vectors when the set of support vectors changes. This can be easily understood from Eq. (6):
suppose that upon changing the value of the parameter from  to  + , a point xm is not a
support vector anymore, then for all other support vectors (x p) p(cid:8)=m, the set  p is going to
be smaller and a discontinuity is likely to appear for the value of Sp = d((x p),  p).
The situation is explained in gure 2: we plotted the value of the span of a support vector
x p versus the width of an RBF kernel  . Almost everywhere the span is decreasing, hence a
negative derivative, but some jumps appear, corresponding to a change in the set of support
vectors. Moreover the span is globally increasing: the value of the derivate does not give us
a good indication of the global evolution of the span.

One way to solve is this problem is to try to smooth the behavior of the span. This can
be done by imposing the following additional constraint in the denition of  p in Eq. (6):
|i|  c 0
i , where c is a constant. Given this constraint, if a point xm is about to leave or
has just entered the set of support vectors, it will not have a large inuence on the span of
the other support vectors, since 0
m will be small. The effect of this constraint is to make
the set  p become continuous when the set of support vectors changes.

However this new constraint prevents us from computing the span as efciently as in
Eq. (12). A possible solution is to replace the constraint by a regularization term in the
computation of the span:

(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(x p)  n(cid:5)

i(cid:8)= p

(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)2 + 

n(cid:5)

i(cid:8)= p

1
0
i

2
i

i (xi )

(cid:7)
= min
i=1

,

S2
p

148

O. CHAPELLE ET AL.

(cid:7)

Figure 2. Value of
kernel varying in the small vicinity.

S2
p, the sum of the span of the training points for different values of the width of an RBF

With this new denition of the span, Eq. (12) becomes:

= 1/( KSV + D)1

pp

 D pp,

S2
p

where D is a diagonal matrix with elements Dii = /0
i and Dn+1,n+1 = 0. As shown on
gure 3, the span is now much smoother and its minimum is still at the right place. In our
experiments, we took  = 0.1.

Note that computing the derivative of this new expression is no more difcult than the

previous span expression.

Figure 3. Left: the minima of the span prediction with regularization (dashed line) and without regular-
ization (solid line) are close. Right: detailed behavior of the span for different values of the regularizer,
 = 0, 0.001, 0.01, 0.1.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

149

It is interesting to look at the leave-one-out error for SVMs without threshold. In this

case, the value of the span with regularization writes:

As already pointed out in Section 3.2.5, if  = 0, the value of span is:

(cid:5)

i (xi )

i(cid:8)= p

(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)2 + 

n(cid:5)

i(cid:8)= p

1
0
i

2
i

(cid:20)(cid:20)(cid:20)(cid:20)(cid:20)(x p) 
(cid:9)

.

pp

= min



S2
p

=

S2
p

(cid:8)

1
1
SV

K

and we recover the Opper-Winther bound.

On the other hand, if  = +, then  = 0 and S2

bound is identical to the Jaakkola-Haussler one.

= K (x p, x p). In this case, the span

p

In a way, the span bound with regularization is in between the bounds of Opper-Winther

and Jaakkola-Haussler.

7. Experiments

Experiments have been carried out to assess the performance and feasibility of our method.
The rst set of experiments consists in nding automatically the optimal value of two
parameters: the width of an RBF kernel and the constant C in Eq. (3). The second set of
experiments corresponds to the optimization of a large number of scaling factors in the
case of handwritten digit recognition. We then show that optimizing scaling factors leads
naturally to feature selection and demonstrate the application of the method to the selection
of relevant features in several databases.

7.1. Optimization details

The core of the technique we present here is a gradient descent algorithm. We used the
optimization toolbox of Matlab to perform it. It includes second order updates to improve
the convergence speed. Since we are not interested in the exact value of the parameters
minimizing the functional, we used a loose stopping criterion.

7.2. Benchmark databases

In a rst set of experiments, we tried to select automatically the width  of a RBF kernel,

(cid:4)

 n(cid:5)

i=1

(cid:6)

(xi  zi )2
2n 2

K (x, z) = exp

along the constant C penalizing the training error appearing in Eq. (3).

150

O. CHAPELLE ET AL.

In order to avoid adding positivity constraints in the optimization problem (for the constant
C and the width  of the RBF kernel), we use the parameterization  = (log C, log  ).
Moreover, this turns out to give a more stable optimization. The initial values are C = 1 and
log  = 2. Each component being normalized by its standard deviation, this corresponds
to a rather small value for  .

We used benchmark databases described in Ratsch, Onoda, and Muller (2001). Those

databases, as long as the 100 differents training and test splits are available at
http://ida.first.gmd.de/raetsch/data/benchmarks.htm.

We followed the same experimental setup as in Ratsch, Onoda, and Muller (2001). On
each of the rst 5 training sets, the kernel parameters are estimated using either 5-fold
cross-validation, minimization of R2/ 2, or the span-bound. Finally, the kernel parameters
are computed as the median of the 5 estimations.

The results are shown in Table 1.
It turns out that minimizing R2/ 2 or the span estimates yields approximately the same
performances as picking-up the parameters which minimize the cross-validation error. This
is not very surprising since cross-validation is known to be an accurate method for choosing
the hyper-parameters of any learning algorithm.

A more interesting comparison is the computational cost of these methods. Table 2
shows how many SVM trainings in average are needed to select the kernel parameters on
each split. The results for cross-validation are the ones reported in Ratsch, Onoda, and

Table 1. Test error found by different algorithms for selecting the SVM parameters C and  .

Breast cancer
Diabetis
Heart
Thyroid
Titanic

Cross-validation
26.04  4.74
23.53  1.73
15.95  3.26
4.80  2.19
22.42  1.02

Span-bound
25.59  4.18
23.19  1.67
16.13  3.11
4.56  1.97
22.5  0.88
The rst column reports the results from Ratsch, Onoda, and Muller (2001). In the second
and last column, the parameters are found by minimizing R2/ 2 and the span-bound using a
gradient descent algorithm.

R2/ 2

26.84  4.71
23.25  1.7
15.92  3.18
4.62  2.03
22.88  1.23

Table 2. Average number of SVM trainings on one training set needed to select the parameters C and  using
standard cross-validation or by minimizing R2/ 2 or the span-bound.

Cross-validation

R2/ 2

Span-bound

Breast cancer
Diabetis
Heart
Thyroid
Titanic

500
500
500
500
500

14.2
12.2

9
3
6.8

7
9.8
6.2
11.6
3.4

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

151

Muller (2001). They tried 10 different values for C and  and performed 5-fold cross-
validation. The number of SVM trainings on each of the 5 training set needed by this
method is 10  10  5 = 500.

The gain in complexity is impressive: on average 100 times fewer SVM training iterations
are required to nd the kernel parameters. The main reason for this gain is that there were
two parameters to optimize. Because of computational reasons, exhaustive search by cross-
validation can not handle the selection of more than 2 parameters, whereas our method can,
as highlighted in the next section.

Discussion. As explained in Section 3.2, R2/ 2 can seem to be a rough upper bound of the
span-bound, which is in an accurate estimate of the test error (Chapelle & Vapnik, 1999).
However in the process of choosing the kernel parameters, what matters is to have a bound
whose minimum is close to the optimal kernel parameters. Even if R2/ 2 cannot be used
to estimate the test error, the previous experiments show that its minimization yields quite
good results. The generalization error obtained by minimizing the span-bound (cf Table 1)
are just slightly better. Since the minimization of the latter is more difcult to implement
and to control (more local minima), we recommend in practice to minimize R2/ 2. In the
experiments of the following section, we will only relate experiments with this bound, but
similar results have been obtained with the span-bound.

7.3. Automatic selection of scaling factors

In this experiment, we try to choose the scaling factors for an RBF and polynomial kernel
of degree 2. More precisely, we consider kernels of the following form:

K (x, z) = exp
(cid:4)

K (x, z) =

1 +

and

(cid:6)

2 2
i

(xi  zi )2
(cid:6)2

(cid:4)



(cid:5)

i

(cid:5)

i

xi zi
 2
i

Most of the experiments have been carried out on the USPS handwritten digit recognition
database. This database consists of 7291 training examples and 2007 test examples of digit
images of size 16 16 pixels. We try to classify digits 0 to 4 against 5 to 9. The training
set has been split into 23 subsets of 317 examples and each of this subset has been used
successively during the training.

To assess the feasibility of our gradient descent approach for nding kernel parameters,
we rst used only 16 parameters, each one corresponding to a scaling factor for a squared
tile of 16 pixels as shown on gure 4.

The scaling parameters were initialized to 1. The evolution of the test error and of the
bound R2/ 2 is plotted versus the number of iterations in the gradient descent procedure
in gures 5 (polynomial kernel) and 6 (RBF kernel).

152

O. CHAPELLE ET AL.

Figure 4. On each of the 16 tiles, the scaling factors of the 16 pixels are identical.

Figure 5. Evolution of the test error (left) and of the bound R2/ 2 (right) during the gradient descent optimization
with a polynomial kernel.

Figure 6. Evolution of the test error (left) and of the bound R2/ 2 (right) during the gradient descent optimization
with an RBF kernel.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

153

Figure 7. Scaling factors found by the optimization procedure: darker means smaller scaling factor.

Note that for the polynomial kernel, the test error went down to 9% whereas the best test
error with only one scaling parameter is 9.9%. Thus, by taking several scaling parameters,
we managed to make the test error decrease.

It might be interesting to have a look at the value of the scaling coefcients we have have
found. For this purpose, we took 256 scaling parameters (one per pixel) and mini- mized
R2/ 2 with a polynomial kernel. The map of the scaling coefcient is shown in gure 7.
The result is quite consistent with what one could expect in such a situation: the coef-
cients near the border of the picture are smaller than those in the middle of the picture,
so that these coefcients can be directly interpreted as measures of the relevance of the
corresponding features.

Discussion. This experiment can be considered as a sanity check experiment. Indeed, it
proves it is feasible to choose multiple kernel parameters of an SVM and that it does not
lead to overtting. However, the gain in test error was not our main motivation since we
did not expect any signicant improvement on such a problem where most features play a
similar role (taking all scaling factors equal on this database seems a reasonable choice).
However as highlighted by gure 7, this method can be a powerful tool to perform feature
selection.

8. Feature selection

The motivation for feature selection is three-fold:

1. Improve generalization error
2. Determine the relevant features (for explanatory purposes)
3. Reduce the dimensionality of the input space (for real-time applications)

Finding optimal scaling parameters can lead to feature selection algorithms. Indeed, if
one of the input components is useless for the classication problem, its scaling factor
is likely to become small. But if a scaling factor becomes small enough, it means that it
is possible to remove it without affecting the classication algorithm. This leads to the
following idea for feature selection: keep the features whose scaling factors are the largest.

154

O. CHAPELLE ET AL.

This can also be performed in a principal components space where we scale each principal
component by a scaling factor.

We consider two different parametrization of the kernel. The rst one correspond to

rescaling the data in the input space:

K (x, z) = K (T x, T z)

where   Rn.

The second one corresponds to rescaling in the principal components space:

K (x, z) = K (T x, T z)

where  is the matrix of principal components.

We compute  and  using the following iterative procedure:

1. Initialize  = (1, . . . , 1)
2. In the case of principal component scaling, perform

principal component analysis to compute the matrix .

3. Solve the SVM optimization problem
4. Minimize the estimate of the error T with respect to 

with a gradient step.

5. If a local minimum of T is not reached go to step 3.
6. Discard dimensions corresponding to small elements in 

and return to step 2.

We demonstrate this idea on two toy problems where we show that feature selection
reduces generalization error. We then apply our feature selection algorithm to DNA Micro-
array data where it is important to nd which genes are relevant in performing the classica-
tion. It also seems in these types of algorithms that feature selection improves performances.
Lastly, we apply the algorithm to face detection and show that we can greatly reduce the
input dimension without sacricing performance.

8.1. Toy data

We compared several algorithms
 The standard SVM algorithm with no feature selection
 Our feature selection algorithm with the estimate R2/ 2 and with the span estimate
 The standard SVM applied after feature selection via a lter method

The three lter methods we used choose the m largest features according to: Pearson
correlation coefcients, the Fisher criterion score,4 and the Kolmogorov-Smirnov test.5 Note
that the Pearson coefcients and Fisher criterion cannot model nonlinear dependencies.

In the two following articial datasets our objective was to assess the ability of the
algorithm to select a small number of target features in the presence of irrelevant and
redundant features (Weston et al., 2000).

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

155

Figure 8. A comparison of feature selection methods on (a) a linear problem and (b) a nonlinear problem both
with many irrelevant features. The x-axis is the number of training points, and the y-axis the test error as a fraction
of test points.

For the rst example, six dimensions of 202 were relevant. The probability of y = 1 or1
was equal. The rst three features {x1, x2, x3} were drawn as xi = y N (i, 1) and the second
three features {x4, x5, x6} were drawn as xi = N (0, 1) with a probability of 0.7, otherwise
the rst three were drawn as xi = N (0, 1) and the second three as xi = y N (i  3, 1). The
remaining features are noise xi = N (0, 20), i = 7, . . . , 202.
For the second example, two dimensions of 52 were relevant. The probability of y = 1
or 1 was equal. The data are drawn from the following: if y = 1 then {x1, x2} are drawn
from N (1, ) or N (2, ) with equal probability, 1 = { 3
, 3} and
 = I , if y = 1 then {x1, x2} are drawn again from two normal distributions with equal
probability, with 1 = {3,3} and 2 = {3, 3} and the same  as before. The rest of the
features are noise xi = N (0, 20), i = 3, . . . , 52.
In the linear problem the rst six features have redundancy and the rest of the features

,3} and 2 = { 3

4

4

are irrelevant. In the nonlinear problem all but the rst two features are irrelevant.

We used a linear kernel for the linear problem and a second order polynomial kernel for

the nonlinear problem.

We imposed the feature selection algorithms to keep only the best two features. The
results are shown in gure 8 for various training set sizes, taking the average test error on
500 samples over 30 runs of each training set size. The Fisher score (not shown in graphs
due to space constraints) performed almost identically to correlation coefcients.

In both problem, we clearly see that our method outperforms the other classical meth-
ods for feature selection. In the nonlinear problem, among the lter methods only the
Kolmogorov-Smirnov test improved performance over standard SVMs.

8.2. DNA microarray data

Next, we tested this idea on two leukemia discrimination problems (Golub et al., 1999) and
a problem of predicting treatment outcome for Medulloblastoma.6 The rst problem was to

156

O. CHAPELLE ET AL.

classify myeloid versus lymphoblastic leukemias based on the expression of 7129 genes.
The training set consists of 38 examples and the test set 34 examples. Standard linear SVMs
achieve 1 error on the test set. Using gradient descent on R2/ 2 we achieved 0 error using
30 genes and 1 error using 1 gene. Using the Fisher score to select features resulted in 1
error for both 1 and 30 genes.

The second leukemia classication problem was discriminating B versus T cells for
lymphoblastic cells (Golub et al., 1999). Standard linear SVMs make 1 error for this problem.
Using either the span bound or gradient descent on R2/ 2 results in 0 error using 5 genes,
whereas the Fisher score get 2 errors using the same number of genes.

The nal problem is one of predicting treatment outcome of patients that have
Medulloblastoma. Here there are 60 examples each with 7129 expression values in the
dataset and we use leave-one-out to measure the error rate. A standard SVM with a Gaussian
kernel makes 24 errors, while selecting 60 genes using the gradient descent on R2/ 2 we
achieved an error of 15.

8.3. Face detection

The trainable system for detecting frontal and near-frontal views of faces in gray images
presented in Heisele, Poggio, and Pontil (2000) gave good results in terms of detection rates.
The system used gray values of 19  19 images as inputs to a second-degree polynomial
kernel SVM. This choice of kernel lead to more than 40,000 features in the feature space.
Searching an image for faces at different scales took several minutes on a PC. To make

Figure 9. ROC curves for different number of PCA gray features.

CHOOSING MULTIPLE PARAMETERS FOR SUPPORT VECTOR MACHINES

157

the system real-time reducing the dimensionality of the input space and the feature space
was required. The feature selection in principal components space was used to reduce the
dimensionality of the input space (Serre et al., 2000).

The method was evaluated on the large CMU test set 1 consisting of 479 faces and about
57,000,000 non-face patterns. In gure 9, we compare the ROC curves obtained for different
numbers of selected components.

The results showed that using more than 60 components does not improve the perfor-

mances of the system (Serre et al., 2000).

9. Conclusion

We proposed an approach for automatically tuning the kernel parameters of an SVM. This
is based on the possibility of computing the gradient of various bounds on the generalization
error with respect to these parameters. Different techniques have been proposed to smooth
these bounds while preserving their accuracy in predicting the location of the minimum
of test error. Using these smoothed gradients we were able to perform gradient descent
to search the kernel parameter space, leading to both an improvement of the performance
and a reduction of the complexity of the solution (feature selection). Using this method,
we chose in the separable case appropriate scaling factors. In the non separable case,
this method allows us to choose simultaneously scaling factors and parameter C (see
Eq. (3)).

The benets of this technique are many. First it allows to actually optimize a large number
of parameters while previous approaches only could deal with 2 parameters at most. Even
in the case of a small number of parameters, it improves the run time by a large amount.
Moreover experimental results have demonstrated that an accurate estimate of the error is
not required and that a simple estimate like R2/ 2 has a very good behaviour in terms of
nding the right parameters. In a way this renders the technique even more applicable since
this estimate is very simple to compute and derive. Finally, this approach avoids holding
out some data for validation and thus makes full use of the training set for the optimization
of parameters, contrary to cross-validation methods.

This approach and the fact that it has be proven successful in various situation opens
new directions of research in the theory and practice of Support Vector Machines. On
the practical side, this approach makes possible the use of highly complex and tunable
kernels, the tuning of scaling factors for adapting the shape of the kernel to the problem
and the selection of relevant features. On the theoretical side, it demonstrates that even
when a large number of parameter are simultaneously tuned the overtting effect remains
low.

Of course a lot of work remains to be done in order to properly understand the reasons.
Another interesting phenomenon is the fact that the quantitative accuracy of the estimate
used for the gradient descent is only marginally relevant. This raises the question of how to
design good estimates for parameter tuning rather than accurate estimates.

Future investigation will focus on trying to understand these phenomena and obtain
bounds on the generalization error of the overall algorithm, along with looking for new
problems where this approach could be applied as well as new applications.

158

Acknowledgments

O. CHAPELLE ET AL.

The authors would like to thank Jason Weston and Elodie Nedelec for helpful comments
and discussions.

Notes

1. In the rest of this article, we will reference vectors and matrices using bold notation.

(cid:15)(cid:15)(cid:15), where 

2. We note P(x) as an abbreviation for PA,B (x).
(cid:15)(cid:15)(cid:15) +
3. This inequality can be easily proved by differentiating MM
r 
4. F (r ) =
+
r +
5. KStst(r ) = 
is the standard deviation.
example, and P is the corresponding empirical distribution.

r

1 = I.

r
r

is the mean value for the r-th feature in the positive and negative classes and 
(cid:6) sup( P{X  fr}  P{X  fr , yr = 1}) where fr denotes the r-th feature from each training

r

6. The database will be available at : http://waldo.wi.mit.edu/MPR/datasets.html.

