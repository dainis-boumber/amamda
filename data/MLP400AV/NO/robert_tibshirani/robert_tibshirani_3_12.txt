ABSTRACT

-We  consider  the  problem  of  summarizing  a scatterplot  with  a  smooth,  monotone
that  combines  local  averaging  and  isotonic  regression  is proposed.  We
curve.  A  solution
two
give  some  theoreticai
examples.
to
some data  from  Box  and  Cox  (1984)  and  it  is shown  how  this  new  procedure  generalizes
Box  and Coxs well  known  family  of transformations.
In  the  same example,  the  bootstrap
is  applied  to  get  a

In  the  second  example,  the  procedure  is  applied,

for  the  procedure-and  demonstrate

of  the  variability  of  the  procedure.

justification

its  use with

in  a  regression  sethng,

Keywords:  scatterplot  smoothing,

iaotoflic  regression

(Submitted

to  Technometrics)

*  Work  supported  by  the  Department  of  Energy  under  contracts  DEACQb76SF00515
and  DCAT03-81-ER10843,
the  Off-ice of  Naval  Research  under  contract  ONR-NO0014
8f--K-0340,  and  the  Army  Research  Office  under  contract  DAAG29-82-K-0056.

1.  INTRODUCTION

We consider  the  following  problem.  Given  a set of  n data  points--{(zl,  yl),  . ..(zn.  y,)},
how  can  we summarize  the  association  of  the  response g on the  predictor  z  by  a smooth,
moaotone  function  s(z)?  Put  another  way,  how  can  we  pass a smooth,  monotone  curve
through  a  scatterplot  of  f/  vs  z  to  capture
the  trend  of  y  as  a  function  of  Z?  This
problem  is related  to  both  isotonic  regression  (see e.g.  Barlow  et  al  1972) and  scatterplot
smoothing  (see e.g.  Cleveland  1979).
-
.In  this  paper  we propose  a solution  to  the  problem  that  uses ideas from  both  isotonic
regression  and  scatterplot  smoothing  (Section  3).  This  procedure  proves to  be useful  not
only  as a descriptive
transformations
of  the  response  in  linear  regression  (Section  4,  example  2),  a method  closely  related  to
those  of  Box  and  Cox( 1964) and  Kruskal(l965).  We begin  with  a brief  review  of  isotonic
regression  and  scatterplot  smoothing

tool  but  also as a method  for  determining  optimal

in  the  next  section.

<

-

-

2.  A  REVIEW  OF  ISOTONIC  REGRESSION  AND  SCATTERPLOT

SMOOTHING

2.1  Isotonic  Regression

.

to  the  restriction

r%l  _< ti2  5

The  problem  of  isotonic

. ..+a.,.  A  unique  solution
from  the  pool adjacent  violators  algorithm
is  too  complex
to  fully  describe  here,  but

regression  on  an  ordered  set  is  as  follows.  Given  real
the  problem  is to  find  {&~;11,&2, . . 61~) to  minimize  CT  (y;  -  hi,;)2
numbers  (~1, ~2, . ..y.},
to  this  problem  exists
subject
(see Barlow  et  al,  pg.
and  can  be obtained
the  basic  idea  is  the
13).  This  algorithm
following.
Imagine  a scatterplot  of  y;  vs  i.  Starting  with  yl,  we  move  to  the  right  and
stop  at  the  first  place  that  yi  >  yi+l.  Since yi+l  violates  the  monotone  assumption,  we
pool  vi  and  vi+1  replacing
them  both  by  their  average.  Call  this  average vr  =  vi++1 =
if  not,  we
(vi  +  yi+I)/2.  We  then  move  to  the  left
their  average.  We  continue  to  the
is  satisfied,  then  proceed  again  to  the  right.  This
is continued  until  we reach  the
, are then  given  by  the  last  average assigned

pool
left  until
process of  pooling  the  first  violator  and  back-averaging
rig6t hand  edge.  The  solutions  at  each i,  hi

to  make  sure  that  vi-1  5  VT-

the  monotone  requirement

replacing  all  three  with

vi-1  with  gz  and  $+l,

2

to  the  point  at  i.

To  find  the  solution

for  the  dual  problem  (Gal  2  +i~2 . . .  >

the  pool  adjacent
is  applied,  starting  at  yn  and  moving  to  the  left.  And  to  find  Ais
violators  algorithm
to  minimize  Cy  (vi  -  hi)2  subject
to  his  non-decreasing  OR  non-increasing,  we  can
choose the  best  set  from  the  two  solutions.  We  will  refer  to  this  two  step  algorithm  as
the  pool  adjacent  violators  algorithm.

rit,)

Its  not  obvious

the  pool  adjacent  violators  algorithm-solves

the  isotonic  re-
that
a  proof  appears  in  Barlow  et  al  (pg.  12).  There  are,  however,  two

gression  problem-
facts  we  can  notice  about  the  solution:

l

l

,  are  monotone,

reproduces  the  data.

if  the  data  { yl,  f12, . ..v.,}
algorithm
each  tii  will  be  an  average  of  gjs  near  i.  The  average will  span  over  the  local
non-monotonicity

then  tii  =  vi  for  all  i;  that

of  the  yis.

is,  the

The  solution

to  the  isotonic  regression  problem  is not  the  solution

to  the  problem  of
monotone  smoothing  because the  solution  sequence tiI
is not  necessarily  smooth.
For  example,  as we  noted,  if  the  data  are  monotone,  the  pool  adjacent  violators  simply
reproduces  the  data;  any  jaggedness  in  the  data  will  be passed on  to  the  solution.

, . . . ti,

In  the  next  subsection,  we  briefly  review  scatterplot  smoothing.

3

2.2  Scatterplot

Smoothing

Given  n  pairs  of  data  points  ((~1,  yI),  . ..(z.,,  vn)},  zl  <  22  <

..Iz~,  assumed to  inde-
th e goal  of  a scatterplot  smoother
pendent  realizations  of  random  variables  (X,Y),
is  find  a smooth  function  a(~)  that  summarizes  the  dependence  of  Y  on  X.  We  assume
that  2  is  some smooth  function  of  X  plus  a random  component:

[*I

Y=.f(X)+c

r

-

-,

(1)

where  E(i)  =  0  and  Var(c)  =  o2  <  00.  One  way  to  formulate
cally  is  to  require  that  S(Z) minimize

the  predictive  squared  error

the  problem  mathemati-

PSE  =  E(Y  -  8(X))2

(2)

is  over  the  joint  distribution

where  the  expectation
were  known,
distribution
averaging.  Many  techniques  have been suggested for  this-
will  make  use of,  is  the  running  mean:

the  solution  would  be  i(x)  =  E(YIX
is  rarely  known,  so the  conditional  expectation

of  (X,Y).

If  this  joint  distribution
=  z)  for  all  z.  Of  course  this
local
the  simplest  and  the  one we

is  estimated

through

i9k (Xi)  =  AWe(Si-k,  sa.Zi, . ..Zi+k)

(3)

The  windows  are shrunken  near  the  left  and  right  endpoints-
in  a window
i  -  k),  . ..i.  . ..min(n.  i  +  k)}.

is  actually

{maz(l,

that

is, the  set  of  indices

The  width  of  the  window  over which  the  average is taken,  2k+  1, is called  the  span.
In  order  to  choose the  span,
the

Typically,
a criterion  based on  the  notion  of  cross-validation  can  be  used.  Denote  by  iki(zi)
running  average  at  Zi  leaving  out

the  span  is  10 to  50 percent  of  the  observations.

i.e.

Zi,

.

IFi  (2;)  =  Aue( 2i-k  f s**Zi-*,  Zi+*,  as* %+k)

(4

(k  2
l),  with
xi,  i.e.  2;  =

the  same endpoint  convention  as before.  Let  2;  be  a  new  observation  at
f(zi)  +  ct  where  pi *  is  independent  of  the  cis.  Then  it  can be shown  that

(5)

[*IIf  the  z  values  are not  random  but  fixed  by  design,  we would  assume that
xdependent.
of  X  replaced  by  an appropriate  sum.

The  derivations  are  still  valid,  with  expectations  over  the  distribution

the  Yis are

.

4

.

by  using  the  fact  that  gki(zi)
etytimate  of  PSE,  a sensible  procedure  is  to  choose  k  to  minimize-ACT
We  will  denote  this  value  of  k  by  k.

is independent  of  vi.  Since the  right  hand  side of  (5)  is  an
(vi  -  ~L(z;))~.

Note  also  that  A E  Cy  (2;  -

(f(Zi)
also  minimizes  an estimate  of  the  expected  squared  error

i,  (Zi))2  =  AEC;

-  &k (Si))2  +  nu2,  SO  that  L

ESE  =

iEe(/(zi)-ik(zi)p

1

-

-

(6)

For  a discussion  of  running  mean  smoothers  and  more  sophisticated  smoothers,  see

Friedman  and  Stuetzle  (1982).

The  running  mean  smoother  produces  a smooth  function

that

is  proposed  in  the  next  section.

pendence of  Y  on  X,  but  this  function
isotonic  regression  produces  a  monotone  function
Y  on  X,  but  this  function
function,  why  not  smooth  the  data  first,
This  is  exactly

the  solution

is  not  necessarily  smooth.

that  summarizes  the  de-
is not  necessarily  monotone.  On  the  other  hand,
that  summarizes  the  dependence  of
If  we  want  a  smooth,  monotone
then  apply  isotonic  regression  to  the  smooth?

3.  MONOTONE  SMOOTHING

3.1  The  Problem

and  a  Proposed  Solution

Suppose we  have  a set  of  n  data  points  ((21,  yl),  . ..(zn.  vn)},  where  ~1  <  x2...  <  zn
If  we

the  dependence  of  y  on  z.

and  our  goal  is  to  model,  with  a  monotone  function,
break  this  problem  down  into  2-steps
-

l

Find  a  smooth  function  g(m) that  summarizes  the  dependence  of  Y  on  X
Find  the  monotone  function

t?~ (a) closest  to  i(a)

l

T

-

-

then  using  the  tools  of  isotonic  regression  and scatterplot  smoothing  discussed in  Section
2,  the  solution

is obvious:

l

l

smooth  the  (X,Y)
apply  the  pool  adjacent  violators  algorithm

pairs

to  the  smooth

In  the  next  subsection,  this  heuristic  procedure  is given some theoretical

justification.

3.2  Theoretical

Justification

for

the  Procedure

Assume  the  setup  described  in  Section  2.2.  A  reasonable  property

to  require  of  the

function  h(e)

is  that

it  should  satisfy

h(X)

=  min-l  ExEzw(Zx

-  GJ(X))~  =  min-1  PSEM

(7)

non-decreasing

in  X,  where  2X  has  the  distribution

to  h(X)
is  the  integrated  prediction  squared  error  in  predicting

of  2  given  X.
subject
the  response  for  a  new
PSEM
tia (e). If  we knew  the  true  joint  distribution  of
observation,  using  the  monotone  function
X  and  Y,  or  we  had  a infinite
test  sample  of  Zis, we  could  minimize  PSEM  over  h(e).
Of  course,  we dont  know  the  joint  distribution  and  we  have only  a training  sample,  so
we  will
from  the  training
sample  alone.

instead  derive  a  approximate  criterion

that  we  can  calculate

As  in  Section  2.2,  we can  equivalently  minimize

the  expected  squared  error

ESEM  =kEt(f(~i)-ti(~i))~

1

(8)

-.

_

.

since  PSEM  =  ESEM  +  nu 2.  It  turns  out  to  be more  convenient  to  work  with  ESEM.
We  can  first  replace  the  marginal  distribution  of  X  by  the  marginal  empirical  dis-

tribution

to  obtain

ESE;

=iEe(f(~i)-h(~i))~

1

(9)

If  weknew

the  problem
-

<

to  that  of  finding  an

=  ESE M,  so  we  can  simplify

f(e),  we could  simply  minimize  an estimate  of  (9),  AC;

Clearly,  Ex(ESEk)
estimate  of  ESEL.
-  +ia (zi))2,
-
to  f(e).  Since  we  dont
over  ti, (a) by  applying
know  f(s),  the  next  best  thing  is to  replace  f(e)  with  our  best  estimate  (in  terms  of  mean
squared  error)  of  f(a).  In  the  class of  running  mean  estimates,  the  best  estimate  is  ii(.)
(from  Section  2.2).  Hence  our  approximate  criterion

the  pool  adjacent  violators  algorithm

-

(f(zi)

is

EkEh

=

t$

(ii  (Xi)  -

fil  (Xi))2

OQ)

Ta  minimize  EiE*  M  over  t?a(*), we  simply  apply  the  pool  adjacent  violators  algorithm
to  ii(-).

How  far  off  (on  the  average)  will

that  minimizes  ESEM  ?  Unfortunately,

the  ti(*)
We  can  expand  the  expected  value  of  E.$Eh  as follows:

l

the  ti(.)  obtained  by  minimizing  E,!?Eb  be from
to  get  a  handle  on  this.

it  is  difficult

-.  _

.

kE-&i$(zi)

-~(~i))~=~E~(O~(~i)-/(Zi)+f(~i)-l(zi))~

1

1

=~E~(.,~(zi)-l(zi))~+~E~(nZi)-~(zi))l

1

1

+

:E

$

(ii

(xi)

-

f(zi)Xf(zi)

-  h

(zi))

(11)

Note  that  only  the  last  two  terms  in  (11)  involve  +?a (s).  If  gg (w) is  exactly  equal  to  f(e),
then  the  expected  value  of  EkEa
is equal  to  ESE;M.  Otherwise,  we can just  hope that
since  Iii  (m) -  f(  .)I should  b e small,  the  cross product
term  will  be small  compared  to  the
2nd  term.

c

7

I

.

3.3  A  Summary  of  the  Algorithm

We  can  summarize  the  monotone  smoothing  algorithm  as foltows:

0  SmootA Y on x:

i  (Xi)  +  Aue(zi-$,  ss.Zi, . ..~i+k  ) where E is chosen to  minimize

l

( Xi))2

Cy  (vi  -  iii
the  closest  monotone  function
Find
violators  algorithm  applied to  &i(s)
_

3.4  Remarks

to  in:(*):

h(s)  +

<

result  of  pool  adjacent
-

-

o  As  a slight  refinement  of  the  algorithm,

linear  smoother  in  the  numerical  examples  that

the  running  mean smoother  was replaced
by  a running
follow.  Running
(least  squares) lines  givve results  very  close to  running  means in  the  middle  of  the
data,  and eliminate  some of  the  bias  near the  endpoints.
Notice  that
if  the  smooth  g(s) is  monotone,  then  h(s)  =
sense-
functions

i(e).  This  makes  good
in  the  class  of  monotone

is the  best  estimate  over  all  functions,

it  just  says  that

the  best estimate  of  E(Y]X)

if  the  latter

is  monotone.

l
-

In  the  next  section,  we give  two  examples  of  the  use of  this  procedure.

4.  EXAMPLES

-.

.

1.

I&ample
_  200 points  were generated from  11 =  e2  +  error,  where X  was uniformly  distributed
on  [0,2]  and  the  errors  had  a  normal  distribution  with  mean  0  and  variance  1.  The
result  of  applying
the  monotone  smoother  is  shown  in  figure  1.  A  span  of  87  points
was  chosen by  the  procedure.  For  comparison,  the  isotonic  regression  sequence is  also
plotted.
from  the  smooth  (not
shown),  since  the  smooth  was  almost  monotone.

In  this  case, the  monotone  smooth  differed  only  slightly

L

Example  2.

In  this  example,  we  use  the  monotone  smoothing  procedure  to

find  an  optimal

.

8

transformation
by  Kruska1(1965),  is a non-parametric  version  of  the  Box-Cox  procedure(1964).
a special  case of  the  Alternating  Conditional  Expectation
and  Friedman(1982).  Given  a  set  of  responses  and  covariates  {(yI,xl),
goal  is  to  find  a smooth,  monotone  function  a(.)  and  estimate  b  to  minimize

for  the  response in  a regression.  This  procedure,  similar  to  that  proposed
It  is also
(ACE)  algorithm  of  Breiman
the

. ..(yn.xn)},

subject  to  Var(3 (y))  =  1 where  Var  denotes  the  sample  variance.  The  procedure  is  an
alternating  one,  finding  b  for  fixed  i(s)  and  vice-versa:

(11)

Initialize:
Repeat:

i  (-) 4-  *I

b  4-  least  squares estimate  of  i(e)  on  x
-i(e)  +  monotone  smooth  of  x  b  on  y
- iq.)+&
Until

residual  sum  of  squares(l1)

A

fails  to  decrease

i(s)  belongs  to  the  parametric
We  applied

this  procedure

Both

the  Kruskal  and  Box-Cox  procedures  are  essentially  variants  of  the  above
algorithm.  Kruskal  uses isotonic  regression  to  estimate  0 (e), while  Box  and  Cox  assume
that

family  (vx  -  1)/X.

A

to  data  on  strength  of  yarns  taken  from  Box  and  Cox
the  response Y  being  number  of  cycles
(1964).  The  data  consists  of  a 3x3x3  experiment,
to  failure,  and  the  factors  length  of  test  specimen  (Xl)
(250,300  or  350 mm);  amplitude
of  loading  cycle  (X2)  (8,  9,  or  10 mm),  and  load  (X3)  (40,  45 or  50 gm).  As  in  Box  and
the  factors  as quantitive  and  allowed  only  a linear  term  for  each.  Box
Cox,  we  treated
and  Cox  found  that  a logarithmic
transformation  was  appropriate,  with  their  procedure
producing  a value  of  -.06  for  i  with  an  estimated  95 percent  confidence  interval  of
(-.18,.06).

Figure  2 shows the  transformation

chose a span  of  9 observations.  For  comparison,
on  the  same  figure.  The  similarity
K&&als  procedure  plotted  along  with

is  truly

the  log function

selected  by  the  above algorithm.  The  procedure
is  plotted  (normalized)
remarkable!  Figure  3  shows  the  result  of
the  log  function.  The  monotone  smooth  gives

9

.

very  persuasive  evidence  for  a  log  transformation,  while  Kruskals
hampered  by  its  lack  of  smoothness.

transformation

is

The  advantage,  of  course,  of  the  monotone  smoothing  algorithm

assume  a parametric
tion  from  a much  larger  class than  the  Box  and Cox family.

for  the  transformations,

family

it  doesnt
and  hence it  selects  a transforma-

is  that

In  order  to  assess the  variability  of  the  monotone  smooth,  we  applied  the  bootstrap
is  fixed  by  design,  we  resampled
the  (X,  Y)  p airs.  The  bootstrap  procedure  was  the

the  residuals  instead  of  from

in  this  problem

of  Efron(  1979).  S ince  the  X  matrix
from
following:

Calculate  residuals  ri  =  3 (vi)  -  xi  b,  i  =  1,2,  . ..n
DO  j=l,

NBOOT

Choose  a sample  ri,
Calculate  yf  =
i-

. ..ri  with  replacement

from  rl,  . ..r.

(Xi6

+  rt),

i  =  1,2,  . ..n

-  Compute  aj  (a) =  monotone  smooth  of  vi,  . ..yi  on  xl,

. ..x.,

END

NBOOT,

the  number  of  bootstrap  replications,  was 20.  It  is important

to  note  that,
via  the  ris,  this  procedure  assumes that
in  estimating  a common  residual  distribution
the  model  8 (v)  =  x  b  +  r  is  correct  (see Efron  (1979)).  The  20  monotone  smooths,
il  (e), . . . &JO (e), along  with
i(a),  are  shown  in  Figure  4.
The  tight  clustering  of  the  smooths  indicate  that  the  original  smooth  has low  variability.
This  agrees with  the  short  confidence  interval  for  X given  by  the  Box  and  Cox  procedure.

the  original  monotone  smooth,

-.

.

. . .  _

.

5.  FURTHER  REMARKS

The  monotone  smoothing  procedure  that

tool  as well  as a primitive

is discussed here should  prove  to  be useful
for  any  procedure  requiring  estimation
It  already  being  used in  the  ACE  program  of  Breiman

both  as a descriptive
of  a smooth,  monotone  function.
and  Eriedman(  1982).

Some further  points:

l

The  use of  running  mean  or  running

linear  fits  in  the  algorithm

is  not  essential.

.

10

l

l

l

fit  like  that  proposed

if  the  procedure

first,

that  smooths  the  data  using  both
the  global  information  provided  by  the-monotonicity
lower  error  of  estimation

Any  reasonable  smooth  (e.g.  kernel  smoother  or  cubic  splines)  should  perform
equally  well.
If  robustness  to  outlying  y  values  is  a concern,  a resistant
in  Friedman  and  Stuetzle  (1982)  might  be used.
in  any  broad  sense.  It  may  be  pos-
The  procedure  described  here  is  not  optimal
local
sible  to  develop  a  one  step  procedure
as-
information  and  AND
then
sumption.  Such  a  procedure  might  have  slightly
is  to  be  used  as
the  monotone  smoother  described  here.  But
either  a data  summary  or  as a method  to  suggest  a response transformation,  we
dont  think
Another  way  to  estimate  a monotone  smooth  would  be to  apply  the  pool  adjacent
violators  algorithm
then  smooth  the  monotone  sequence. This  has a serious
drawback:  while  it  is true  that  a running  mean smooth  of  a monotone  sequence is
monotone,  the  running
linear  smooth  of  a monotone  sequence is  NOT  necessarily
Therefore,  one  would
monotone.
the  final  smooth
have  to  apply  the  pool  adjacent  violators  again  to  ensure  that
was  monotone.  This  non-monotonicity  preserving  property
true  of
other  popular  smoothers.  We  didnt  try
this  procedure,  partly  because  of  this
fact  but  mostly  because we didnt  see a sensible justification

the  gain  would  be worthwhile.

(It

is  easy  to  construct  a  counter-example).

is probably

for  it.

Acknowlegmentr

We  would  like  to  thank  ltevor  Ha&

for  his  valuable  comments.  This  research
was supported  in  part  by  the  Natural  Sciences and  Engineeering  Research Council  of
Canada.

--

11

