1. Introduction. Suppose that we wish to estimate the probability den-
sity g y that produced an observed random sample of vectors y , y , . . . , y ,
n

(cid:14)

.

1

2

(cid:14)

1.1

.

i.i.d.

y ; g y
i

(cid:14)

.

for i s 1, 2, . . . , n.

i

.

The vectors y take values in a sample space YY. The numerical examples in
this paper have YY being portions of the real line or of the plane, but the
methodology applies just as well to higher dimensionalities and to more
complicated spaces.

Estimates of g y are traditionally constructed in two quite different ways:
by maximum likelihood tting within some parametric family such as the
normal or by nonparametric methods such as kernel density estimation.
These two methods can be combined by putting an exponential family
through a nonparametric estimator. The resulting hybrid estimators are
the specially designed exponential families of the title.

(cid:14)

Figure 1 shows a simple example of this methodology. The y are pain
scores for n s 67 women, each obtained by averaging the results from a
questionnaire administered after an operation. The scale runs from y s 0 s
x
no pain to y s 4 s worst pain, so the sample space YY is the interval 0, 4 .
The 67 scores y , indicated by the histogram, run from 0.02 to 3.08. The
dashed curve g y is a normal kernel density estimator with window width
ls 1, described more carefully in Section 2. Also shown are two special
exponential family estimates, g y and g y , described below.

0

i
(cid:14)

.

.

(cid:14)

.

w

i

(cid:14)

1


2

Received January 1995; revised October 1995.
1Supported by NSF Grant DMS-95-04379 and Public Health Service Grant 5 ROI CA59039-20.
AMS 1991 subject classications. 62F05, 62G05.
Key words and phrases. Poisson regression, degrees of freedom, expected deviance, local and

global smoothing, moment-matching.

2431

2432

B. EFRON AND R. TIBSHIRANI

FIG. 1. Pain-score data. Left: Histogram of pain scores for 67 women following an operation:
0 s no pain; 4 s worst pain. Right: g y is normal kernel density estimator, window width 1;
(cid:14)

g y is the special exponential family through g y with sufcient statistic t y s y; g y uses
1
sufcient statistics t y s y, y . Density g y matches the empirical mean and variance of the
67 data points.


0
.

0

2


2

2.

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

.

(cid:14)

(cid:14)

.

(cid:14)

(cid:20)

(cid:14)
(cid:14)

.4
.
(cid:14)

1

0

0

0

b

b

.

.

(cid:14)

(cid:14)

(cid:14)

(cid:14)

y

g

1.2

.
y s g

An exponential family of densities on YY, g
.

is given by
.
.
y exp b q t y b ,

(cid:14)
which is called the exponential family through g y with sufcient statistics
(cid:14)
t y . Here g y is a carrier density, t y is a 1 = p vector of sufcient
statistics, b is a p = 1 parameter vector and b is a normalizing parameter
.
y integrate to 1 over YY. For example, the one-dimensional
that makes g
normal family, with all possible choices of expectation and variance, can be
obtained using the standard normal carrier g y s w y s exp y0.5y r

2p, with the sufcient statistics t y s y, y . The densities in 1.2 are
dened with respect to some background measure, which we will take to be
.
Lebesgue measure. See Section 1.4 of Lehmann 1983 .

(cid:14)
0
.
2

24

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:20)

b

1

0

0

(cid:14)

(cid:14)

b

g

0

1.3

.

y s g
0

.
.
y exp b q t y b ,

The estimates g y and g y in Figure 1 are of the form
.

(cid:14)
where g y is the kernel density estimate indicated by the dashed curve.
Estimate g y uses the single sufcient statistic t y s y, so p s 1, while
 
(cid:14)
2
g y uses t y s y, y , p s 2. The parameter values bs b , b were
0
1
.
(cid:14)
n
chosen by maximum likelihood, that is, by maximizing (cid:3) g
y , ignoring

is1 b
i
the fact that the carrier g y is itself data-dependent. This choice of b
matches the t y moments of g

.
y to their empirical averages:

.
(cid:14)
1
(cid:14)

.
.


0


1

0



(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

(cid:14)

.

.

2


2

(cid:14)

(cid:14)

1

.

.
(cid:14)

.
b

(cid:14)

1.4

.

H

YY

(cid:14)
t y g

.


b

(cid:14)

.
y dy s

n1
(cid:7)
n
is1

(cid:14)
.
t y .

i

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2433

2

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

Thus g matches the rst two empirical moments of the 67 pain scores or,
.
equivalently, it matches the empirical mean and variance. Property 1.4
implies that the special exponential family estimate of g y is unbiased for
the moments of t y . Linear transformations to post-repair the mean and
variance of a kernel estimate are familiar in the density-estimation litera-
ture; see, for example, Jones 1993 . The two-dimensional example of Section
4 shows a more ambitious example of moment-matching.

(cid:14) .

We can think of a special exponential family estimator in two complemen-
tary ways: 1 as being a standard exponential family estimator, except one
that is preceded by an adaptive choice of the carrier, or 2 as being a
standard nonparametric smoothing estimator, except one that is followed by
a correction to match certain sample moments.

(cid:14) .

Either way, we will argue that special exponential families can be a
favorable compromise between parametric and nonparametric density esti-
mation. From the rst point of view, we will be able to use exponential family
theory more exibly than when restricted to the usual small catalogue of
normals, gammas, betas and so forth. An approach more in the spirit of a
regression analysis than a density estimate is possible, including exploratory
.
choices of the sufcient statistics t y .

From the second point of view, the moment-matching correction will
.
usually reduce the bias of a nonparametric smoother, since the moments t y
are estimated unbiasedly. This has an important practical consequence shown
in the numerical examples: it allows the nonparametric smoother to use a
substantially greater window width without badly degrading the overall t to
.
the data. The result is a substantially smoother estimate of the density g y .
.
(cid:14)
This phenomenon is illustrated in the bivariate example of Figures 3 and 4.
.
To put things another way, the special exponential family estimate 1.3
works at two different scales. The nonparametric smoother allows local
adaptation to the data, while the exponential term matches some of the
datas global properties.

(cid:14)

(cid:14)

(cid:14)

(cid:14)

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.
.

Sections 2]5 describe how to compute and interpret special exponential
family SEF estimates such as those in Figure 1. Most of our computations
.
are done using a Poisson regression model for density estimation Section 2 ,
originally introduced in Lindsey 1974a, b . Section 3 gives a delta-method
formula for the covariance of b in 1.3 , which takes into account the
data-based choice of the carrier g y . We can use this formula in the usual
way to select among possible choices of the sufcient statistic. Section 6
concerns formulas for choosing the window width of the smoother that
produces g y . This choice involves degrees of freedom calculations like
those introduced by Hastie and Tibshirani 1990 . Multisample SEF esti-
mates are discussed in Section 7. Remarks appear in Section 9.

(cid:14)

1
0

Special exponential families are an example of what Green and Silverman
(cid:14)
1994 call semiparametric methods. Many other semiparametric methods
have been proposed for density estimation. Hjort and Glad 1995 propose
reversing the SEF order: rst t a parametric family to the data and then t
a nonparametric smoother to the residuals from the parametric estimator.

0

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

2434

B. EFRON AND R. TIBSHIRANI

Hastie and Tibshiranis backtting approach can also be applied to semipara-
metric density estimation. It amounts to an iteration to convergence between
the smoother and the exponential family. The principal advantage of the SEF
estimator is its theoretical tractability. The relationship between these ideas
is discussed in Section 8.

Stone 1994 and Kooperberg and Stone 1991 use adaptive exponential
families based on spline functions for density estimation. The carrier g y is
effectively the uniform density in this work, but adaptation is still possible
because of the local character of the spline basis. Olkin and Spiegelman
(cid:14)
1987 combine parametric and nonparametric density estimates by mixtures,
rather than by exponential family methods as in this paper.

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

0

2. Poisson regression for density estimation. Density estimation
problems can be rephrased in terms of Poisson regression models. This
technique was introduced by Lindsey 1974a, b as a way of using generalized
linear model software to t difcult exponential family models. See also
Aitken 1993 , Lindsey and Mersch 1992 and Efron 1988 . Our results here
will be phrased in Poisson regression terms, mainly for reasons of conceptual
clarity, though there are also some computational advantages.

(cid:14)

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

Lindseys construction begins with a discretization of the problem: the

sample space YY is partitioned into K disjoint cells YY ,k

(cid:14)

2.1

.

and the data y s y , y , . . . , y
(cid:14)
2.2

s s a y g YY
k
k

.

(cid:20)

1

2

i

4

(cid:14)

YY s

K

D k
YY ,
ks1

.

n

are reduced to the cell counts

for k s 1, 2, . . . , K .
.

The vector of counts s s s , s , . . . , s
has sum s s n. Table 1 shows the
counts for the pain-score data of Figure 1, where YY s 0, 4 has been parti-
tioned into K s 40 cells YY each of length 0.1.

Suppose that g y , ug Q is a family of probability densities on YY. The

q

(cid:14)

.

w

x

(cid:20)

4

K

u

k

1

2

(cid:14)

probability of observing y in the kth cell is

(cid:14)

2.3

.

p u s g

k

u

.

(cid:14)

H

YYk

(cid:14)

.
y dy,

Discretized version of the pain-score data of Figure 1; YY s 0, 4 partitioned into K s 40 cells of

length 0.1; s s 3 of the 67 scores occurred in YY s 0, 0.1 , 7 in YY s 0.1, 0.2 and so forth

.

.

w

w

1

1

2

w

x

TABLE 1

3
4
2
1

7
4
2
0

6
1
0
0

1
3
0
0

2
3
0
0

3
5
1
0

3
0
0
0

1
1
0
0

7
0
1
0

5
0
1
0

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2435

(cid:14)

.

q

with sum p u s 1. Then s has a multinomial distribution on K categories,
..
with n draws and probability vector p u s p u , p u , . . . , p u ,
(cid:14)
2.4
We could nd the maximum likelihood estimate MLE of u, based on s, by
maximizing the multinomial probability of s.

(cid:14)
(cid:14)
1
(cid:14)
.
n, p u .

s ; Mult

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

K

K

2

Instead we consider the s to be independent Poisson observations

k

ind

<

k

k

k

.

.

(cid:14)

.

.

.

.

.

(cid:14)

(cid:14)

(cid:14)

(cid:14)

(cid:14)

(cid:14)
.

k s 1, 2, . . . , K ,

.
s ; Po m g, u ,
k

.
m g, u s gp u .

s ; Po g and s s ; Mult
q

(cid:14)
2.5
with expectations
(cid:14)
2.6
Here g is a free parameter, restricted only to be positive. Standard Poisson
properties allow 2.5 and 2.6 to be expressed as
(cid:14)
(cid:14)
2.7
K q
This means that the maximum likelihood estimates from 2.5 and 2.6 are
(cid:14)
2.8
and u is equal to the MLE for u in 2.4 . Lindseys method is to approxi-
mately maximize the original likelihood (cid:3) g y by nding the Poisson
MLE in 2.5 and 2.6 . Note: The parameters g and u are orthogonal in Cox
.
and Reids 1987 sense, so that the information for estimating u is the same
.
in 2.4 and 2.5 .

(cid:14)
This method of nding the MLE is particularly convenient when the
original densities are of the exponential family form 1.2 . We will consider
the density estimation problem 1.1 and 1.2 in the Poisson regression form
.
(cid:14)
2.5 and 2.6 :

.
s , p u .
(cid:14)


gs s s n,

n
is1 u

q
(cid:14)



q

(cid:14)

.

.

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

.

i

ind

.

(cid:14)

k

k

(cid:14)

.

.

.

.

(cid:14)

o
k

s ; Po m b
k

for k s 1, 2, . . . , K ,

.
m b s mo exp b q t b .

(cid:14)
2.9a
with
(cid:14)
2.9b
k
Here m is proportional to p s H
carrier, and
(cid:14)
2.10
the sufcient vector t y evaluated at a convenient point y
parameter b corresponds to gs e
.
x s 1, t
k

t s t y
k

.
in 2.6 .

(cid:14)
(cid:14)

0
.

(cid:14) k.

o
k

b0

(cid:14)

.

.

.

(cid:14)

.

(cid:14)

(cid:14)

YY

,

k

0

1

k

.

Dene X to be the K = p q 1 matrix whose kth row equals
(cid:14)
2.11
The maximum likelihood equations for bs b , b in the generalized linear
model 2.9 are
(cid:14)
2.12

X 9 s y m b s 0,

.
(cid:14)



(cid:14)

.

(cid:14)

.

.

k

0

1

(cid:14)

.

g y dy, a discretized version of the
0

in YY . The free

k

(cid:14) k.

2436

B. EFRON AND R. TIBSHIRANI

(cid:14)



.



.
.

(cid:14)
(cid:14)

k

where m b indicates the vector with kth component m exp x b . Standard
generalized linear model software easily solves for b in 2.12 , even for
difcult nonstandard forms of the exponential family 1.2 . This was Lindseys
principal point.

o
k

.

Here is how Figure 1 was constructed. The problem was discretized into
(cid:14)
.
o
K s 40 cells as in Table 1. The carrier vector m s m , m , . . . , m in 2.9b
1
(cid:14) .
was estimated using a K = K smoothing matrix M l ,
(cid:14)
Matrix M l was taken to be a normal kernel smoother, having kjth element


mo s M l s.

2.13

(cid:14) .

o .
K

o
2

(cid:14)

.

.

(cid:14)

(cid:14)

o

(cid:14)

2.14

.

M l s w

k j

.

(cid:14)

c
k
l

(cid:15)

y y y
(cid:14) k.

(cid:14) j.

l

/

,

.

(cid:14)

(cid:14) k.

with y s k y 0.5 r10, the midpoint of cell YY . The constants c were
chosen to make M s 1. The starred curve labelled g in Figure 1 is actually
k
o
m plotted as a function of y
, with the window width l in 2.14 set equal
to 1.


0

kq

(cid:14) k.

(cid:14)

.

k

k

The curves labelled g y and g y are really the discrete analogs of the

.

.

(cid:14)

1


2

(cid:14)


.
special exponential family 1.3 , say m s m , m , . . . , m ,
(cid:14)

/

m s m exp b q t b ,

 
1
2

1

2.15


0





o
k

.

(cid:14)

.

(cid:14)

K

k

k

(cid:15)

, while g
2


k

k

(cid:14)

.

(cid:14) k.


is based on the quadratic
1
.
. The MLE estimates b were obtained by iterative

plotted versus y : g uses t s y
(cid:14) k.
(cid:14)
2
vector t s y
, y
(cid:14) k.
(cid:14) k.
.
solution of 2.12 , so
(cid:14)
2.16
where X is a 40 = 2 matrix for g and a 40 = 3 matrix for g . The estimates
were actually computed using a centered version of y, y s y y 2 r4. To
t this model in the GLIM language or the glm function in SPlus, one simply
includes log mo as an offset in a Poisson generalized linear model.

X 9 s y m s 0,

1


2
(cid:14)

(cid:14) k.



(cid:14) k.

w

x

.

w

x

Equation 2.16 shows that

.


(cid:14)

.

(cid:14)
2.17a
and

(cid:14)

2.17b

.

q
m s s s n

q

K

(cid:7)
ks1

k
m
n

K

(cid:7)k
t s
ks1

s
k
n

t ,
k

.

(cid:14)

the discrete analog of the moment-matching property 1.4 . Notice that
because of 2.17a and the fact that the cells are of length 0.1, the curves in
Figure 1 integrate over YY to 6.7 rather than to 1.

Changing K to 20 or to 80 made very little difference in Figure 1. The
numerical calculations in this paper were insensitive to the form of dis-
cretization. In fact, discretization is not really necessary for any of our
results, as discussed in Remark E of Section 9.

(cid:14)

.

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2437

Nevertheless, it is conceptually easier to discuss special exponential family
density estimation in terms of the discrete Poisson model 2.9 . The problem
becomes one of tting a smooth regression curve to the independent observa-
tions s , and this lets us make use of the arsenal of regression tools. It also
emphasizes the important point that density estimation is equivalent to
Poisson regression, and not to ordinary least squares regression. The Poisson
nature of the problem will be evident in the formulas developed below.

(cid:14)

.

k


 
1
0

.

(cid:14)

.

.





(cid:14)
(cid:14)

3. Estimating the covariance of b. This section derives an approxi-
mate covariance matrix for bs b , b , the estimated parameters in a
special exponential family such as 1.3 or 2.15 . The formula for the covari-
ance takes into account the data-based choice of the carrier. We will use the
estimated standard errors of the components of b for model building, check-
ing the signicance of the corresponding components of the sufcient statistic
(cid:14)
t y in the usual way.

.
We consider the Poisson form 2.9 of the SEF model,
.

(cid:14)
.
s ; Po m b

with m b s moe Xb,

.
K
x k b
, x s 1, t

(cid:14)
3.1
where Po m indicates a vector of K independent Poisson variates having
o Xb
expectations m , m , . . . , m s m. The notation m e
indicates the vector
.
.
with kth component m e
, as in 2.11 . Generalizing 2.13 , we
rst estimate m by some function of s, say m s , and then solve for b in the
.
MLE equations 2.16 :
(cid:14)

3.2a
The special exponential family estimate of m is
(cid:14)
3.2b

and b: X 9 s y m e s 0.

(cid:14) .

o
m s m s


o Xb
.

(cid:14)
(cid:14) .


m s m e


o Xb

o
(cid:14)

.
(cid:14)







o
k

(cid:14)

.

(cid:14)

(cid:14)

.

.

.

.

(cid:14)

(cid:14)

(cid:14)

1

2

.

K

K

k

k



LEMMA 1. Let D be the K = K diagonal matrix with kth diagonal element


o
log m s
m s m e
k
k
(cid:14)
o.

log m , log m , . . . , log m with respect to s, with  j indexing columns,
1

and let H be the K = K derivative matrix of
o .

2


(cid:14)


x b





(cid:14)

(cid:14)

(cid:14)

.

o

k

(cid:14)

3.3

.


H s

o ..
K

d log mo

ds

(cid:15)

s

.

k
 log mo

(cid:14)
sj

/

.

Then the p q 1 = K derivative matrix of b with respect to s is

(cid:14)

.



db
ds


s X 9DX

w

y1

x

Z9,

.
Z9 s X 9 I y DH .

 

(cid:14)

3.4a

(cid:14)
.
where
(cid:14)
.
3.4b
In case 2.13 , m s Ms, this becomes
(cid:14)
3.4c
(cid:14)
where D e
.
proof appears below.


Xb



.

(cid:14)

.

.

o

.
Z9 s X 9 I y D e M ,

Xb

(cid:14)

(cid:14)

.

is the diagonal matrix with kth diagonal element e


x bk
. The

(cid:14)

2438

B. EFRON AND R. TIBSHIRANI

By the usual delta-method argument, an approximate covariance matrix

estimate for b is given by



(cid:14)

3.5

.

(cid:15)


db
ds

/


db$
(cid:14) .
Cov s
ds

(cid:15)

/

9

.

COROLLARY 1. The SEF vector b obtained from 3.1 and 3.2 has approx-

(cid:14)

.

(cid:14)

.



imate covariance matrix

(cid:14)

3.6a

.


Cov b s X 9DX



(cid:14)

.

w

y1

x

w


Z9DZ X 9DX

x

w

y1

,

x

where D is the diagonal matrix with kth diagonal element s . An alternative
estimate is

k

(cid:14)

3.6b

.

$

Cov b s X 9DX



(cid:14)

.

w

y1

x

w


Z9DZ X 9DX



x w

y1

.

x

(cid:14) .

k

k

(cid:14)

.

.

.

.

.

(cid:14)

(cid:14)

(cid:14)

.

(cid:14)

(cid:14)

(cid:14)



For any K-vector v, we let D v be the K = K diagonal matrix with kth
diagonal element v . The true covariance of s ; Po m is D m . Approxima-

tion 3.6a estimates Cov s by D s s D in 3.5 , while 3.6b uses D m s D.
The former may be preferred if model 3.1 is suspect. In our numerical
examples the two formulas gave nearly the same results.

(cid:14) .

(cid:14) .

.
(cid:14)

. (cid:14)


1
w

Table 2 applies the corollary to the pain-score data. The quadratic model,
described in 2.13 ] 2.16 has b s y2.74, y3.80 , with standard errors
(cid:14)
0.93, 2.45 according to 3.6a . b serves only to normalize m to m s n,
(cid:14)
2.17a , so its value is of no statistical interest. The coefcient of y s y y
.2 r4, the centered version of y, is nearly 3 standard errors below zero. The
coefcient of y 2 is y1.55 standard errors below zero, so it is not so clear that
3.
the quadratic term is signicant. The cubic model, in which t y s y, y , y ,
has the cubic coefcient only 0.07 standard errors above zero. Either the
linear or the quadratic model seem reasonable here; the cubic model is
denitely excessive. Section 6 discusses model selection in more detail.

  

q
(cid:14)

.
(cid:14)









(cid:14)

.

.

.

.

(cid:14)

.

(cid:14)

x

0

2



Parameter estimates b and estimated standard errors for the pain-score data of Table 1. The
quadratic model is described in 2.13 ] 2.16 ; y s y y 2 r4, centered version of y; se square root
of diagonal elements 3.6a ; se from 3.6b ;  jack is jackknife standard error; naive is the usual
exponential family sterr estimated ignoring data-based choice of carrier. The cubic model uses the

. (cid:14)

$



(cid:14)

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

TABLE 2

same M, but t y s y, y , y ; the cubic term is not at all signicant

  

2

3.

(cid:14)

.

(cid:14)


b

1

y2.74
y3.80

se

0.93
2.45


y
2

y
3y


Quadratic model
$
se

(
)
ratio

(cid:14)
.
y2.95
(cid:14)
.
y1.55

0.96
2.50

jack

1.07
2.66

naive

1.18
2.85

Cubic model

b

se

1

y2.78
y3.59
0.59

1.24
2.70
8.30

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2439

(cid:14)

.x

The jackknife standard errors for the components of b formula 11.5 of
Efron and Tibshirani 1993 were computed as a check on the corollary. They
.
came out a little larger than the delta-method estimates from 3.6a or 3.6b ,
.
as is often the case; see Section 2 of Efron 1992 .

Suppose that we ignore the fact that the carrier m in 3.2 is a function of
the data s. This amounts to taking H s 0 in 3.3 , so Z9 s X 9 in 3.4b . Then
(cid:14)
3.6b reduces to the usual covariance estimate for exponential families,





(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

1

o

(cid:14)

.

 w

$

Cov b s X 9DX



(cid:14)

(cid:14)

.

y1

.

,

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

.(cid:14)

y1

3.7

(cid:14)

.
y1
while 3.6a becomes the sandwich estimate X 9DX
.
The standard error estimates from 3.7 , labelled naive in Table 2, are
considerably larger than the standard errors that take into account the
adaptive choice of the carrier.


X 9DX X 9DX

The naive standard errors will usually exceed those from 3.6 . The reason
is that the adaptive choice of m absorbs some of the variability in b. Here is
a simple normal-theory version of the same phenomenon: suppose we observe
z ; N m q b, 1 and we wish to estimate b, the distance of the expectation
ms m q b from some origin of measurement m . Then bs z y m has
standard error 1. However, if the origin is chosen adaptively, say by mo s
0.75 ? z, then bs z y m has standard error 0.25. Observing z s 4, for exam-
ple, gives bs 1, which in the adaptive case is 4 standard errors above 0. See
Remark B in Section 9.













(cid:14)
o

.

(cid:14)

.

o

o

o

o

o

PROOF OF LEMMA 1. Using the D notation for diagonal matrices, 3.2 for

(cid:14)

.

.

3.8

b can be expressed as
(cid:14)
where e
produces change db in the MLE vector and change

is the vector with components e

.
o


Xb


xb



(cid:14)

X 9 s y D m e s 0,

x bk
. A small change ds in s

(cid:14)
in m . Then 3.8 gives

3.9


.
o

(cid:14)

.

 
o
dm 5 D m H ds

o 
.



(cid:14)


.

(cid:15)

(cid:14)
(cid:14)


0 s X 9 s q ds y D m q dm exp X bq db





(cid:14)

.

o

o

.



s X 9 s q D m exp Xb q ds y D dm exp Xb



(cid:14)

.

.

o

o

(cid:14)

(cid:14)



3.10


(cid:14)
(cid:14)
Using 3.8 , 3.9 and the fact that D dm e s D e
(cid:14)


0 s X 9 ds y X 9D m e



Xb


o Xb

(cid:14)
.



(cid:14)

(cid:14)

(cid:14)

.

(cid:14)

.

o


Xb

yD m exp Xb X db .
.

o

H ds y X 9D m e



o Xb

.

o


s X 9 I y DH ds y X 9DX db.

 

(cid:14)

3.11

.



.

.


/
(cid:14)
.
.

dm , 3.10 becomes
.

(cid:14)

X db

.

This veries 3.4 . I

(cid:14)

.

2440

B. EFRON AND R. TIBSHIRANI

.

(cid:14)

.

.

i

(cid:14)

.

4. A bivariate example. Density estimation becomes more interesting,
and more difcult, when the sample space YY is of higher dimension. This
section introduces an example when YY is a portion of the plane. We will use
this example to illustrate some of the advantages of special exponential
family density estimation.
(cid:14)

Figure 2 shows l s log redshift and m equal to the apparent magnitude

for 486 galaxies taken from Loh and Spillars 1988 redshift survey,
(cid:14)
Hubbles law, that larger redshift implies greater distance from Earth, is
apparent in the gure. The galaxies with larger values of l tend to appear
dimmer, that is, to have larger apparent magnitudes, leaving the lower right
corner nearly empty.

for i s 1, 2, 3, . . . , n s 486.

y s l , m
i

4.1

The data in Figure 2 are a truncated subsample of the 879 galaxies in the
Loh]Spillar catalog. It is all of the catalog entries falling into the rectangle
(cid:14)
We will take this rectangle to be the sample space YY. Some of the scientic
.
reasons for truncation are discussed in Efron and Petrosian 1992 .

and 17.2 F m F 21.5.

log 0.2 F l F log 1.2

4.2

.

(cid:14)

.

.

Figure 2 shows YY partitioned into K s 285 rectangular cells YY , by
dividing the l axis into 15 equal strips and dividing the m axis into 19 equal
strips. The corresponding counts s , 3.2 , are shown on the right side of the
gure. It is convenient to index the cells by
(cid:14)
(cid:14)
The midpoint of rectangle YY is y  l

i s 1, 2, . . . , 15, j s 1, 2, . . . , 19.

.
k s i, j ,

.
, m .

4.3

(cid:14)

.

(cid:14)

.

k

k

(cid:14)

(cid:14)

i

k

(cid:14) k.

(cid:14)i.

(cid:14) j.

FIG. 2. The galaxy data: 486 galaxies from Loh and Spillars 1988 redshift survey log
redshift l and apparent magnitude m discretized into 285 s 15 = 19 equal cells. The counts s
for the 285 cells are shown at right.

.

(cid:14)

.

(cid:14)

(cid:14)

4.4

.

M l s exp y

k k 9

(cid:14)

.

(cid:15)

c

k
2
l

.

(cid:14)

1
2
2l

(cid:14)

i y i9 q j y j9

2

.

(cid:14)

2

.

/

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2441

(cid:14) .

The right side of Figure 3 shows the results of applying a linear smoother

m s M l s to the 285-dimensional count vector s given in Figure 2. The
K = K matrix M l is a two-dimensional version of 2.14 , with kk9th
element

(cid:14) .

(cid:14)

.

(cid:14) .

(cid:14)

.

o

k

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

k 9

(cid:14) k.



k k 9

for k s i, j and k9 s i9, j9 . The c are chosen to make (cid:7) M l s 1. The
choice ls 1.5, suggested by the expected deviance calculations of Section 6,
gave the smoothed estimate m s M 1.5 ? s plotted versus y
on the right
side of Figure 3.

.

mo s M 2 s and X s 1, i, j, i2 , j2 , ij .

The left side of Figure 3 shows an SEF estimate m of form 3.2 , with
.

(cid:14)
4.5
Here i is the K vector with kth element i y 8, and j is the K vector with kth
element j y 9. This choice amounts to using the usual sufcient statistics for
(cid:14)
t y s l, m, l , m , lm . The tted density
a bivariate normal
matches the empirical means, variances and correlation of the galaxy data.

.
in 1.2 ,

The variance calculations of Section 5 and the expected deviance calcula-
tions of Section 6 suggest that these two estimates are roughly equal in their
overall ability to predict the true density. However, the SEF estimate is much
smoother than the smoothing-only choice. This is obvious in Figure 4, which
shows contour plots of the two density estimates.



(cid:14)

o
k

(cid:14) .

Suppose that the carrier m in 4.5 was taken to be the constant vector

m  1 instead of M 2 s. Then the SEF m would be the discretized trun-
cated bivariate normal MLE for the galaxy data, the truncation being to the
rectangle 4.2 . In fact, the SEF in Figure 3 looks like the lower corner of a
bivariate normal, though there are some discrepancies due to the adaptation
of mo to the galaxy data.



(cid:14)

.

(cid:14)



.

(cid:14)

(cid:14)

.

.

.

2

2

o



(cid:14) .
FIG. 3. Left panel: SEF estimate 3.2 for the galaxy data as discretized in Figure 2; m s M 2 ?
s, quadratic matrix X, 4.5 . Right panel: The smoothing-only estimate m s M 1.5 ? s. The
calculations of Sections 5 and 6 suggest that the two estimates are roughly equal in accuracy.
However, the SEF estimate is much smoother.





(cid:14)

.

(cid:14)

.

(cid:14)

.

o

o

2442

B. EFRON AND R. TIBSHIRANI

FIG. 4. Contour plots of the density estimates in Figure 3. The SEF contours left panel are
much smoother than those from the smoothing-only estimate right panel . The smoothing-only
estimate has a weak second mode at the starred point.

(cid:14)

.

(cid:14)

.

Parametric models such as the bivariate normal are erce data smoothers.
This can be a big advantage if the statistician is interested in global proper-
ties of the density, like the general shape of its contours, and especially if the
data-sampling process is suspect. In Figure 2 we can see that the galaxy data
are quite patchy and clumpy, which is not surprising since the Loh]Spillar
catalog was a census of the brighter galaxies in a few degrees of sky, and not
a random sample of the full sky.

If those few degrees of sky are of great individual interest, then a narrow-
band smoother like that in the right side of Figure 3 may be appropriate.
w
Notice that it estimates a weak second mode near l, m s y1, 20 . How-
ever if we really want to know the density for the whole sky, then it pays to
oversmooth the estimate from a awed sample like that in Figure 2. The SEF
methodology allows us to oversmooth without losing much estimating ef-
ciency compared to smoothing-only estimates and without making the drastic
assumptions of the usual parametric models.

. x

(cid:14)

.

(cid:14)

Various SEF models besides the quadratic choice of X in 4.5 were tried.

(cid:14)
2 2
is the component of i j


mo s M 2 s and X s 1, i, j, i2 , j2 , ij, i2 j2
(cid:14) 2 2.

.
One of these added a further cross-term to 4.5 ,
(cid:14)
4.6
.
where i j
orthogonal to 1, i, j, i , j , ij .
Orthogonalization makes the rst six components of b have roughly the
.
same values and standard errors as in 4.5 . The term i j
in 4.6
allows the regression surface to turn more quickly near the corners of the
rectangle YY.

orthog
(cid:14)

(cid:14) 2 2.

orthog

orthog

The MLE vector b for model 4.6 appears in Table 3, along with the
standard error estimates se from 3.6a and the t-values br se. All of
the coefcients except the one for m are signicantly different than zero the



(cid:14)

.


1



(cid:14)

.

,

2

2

(cid:14)

2

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

w

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2443

Parameter estimates and standard errors for the components of b in the SEF model 4.6 . All of
the components of b are signicantly nonzero, except for the coefcient of m . l indicates the
.
coefcient corresponding to i, lm to ij and so forth. Standard errors are from 3.6a . Model 4.5

.
gave similar estimates, standard errors and t-values for l, m, l 2, m2, lm


1

(cid:14)

(cid:14)

2

(cid:14)

.


1

TABLE 3

l

y0.105
0.019

y5.6

m

0.084
0.015
5.8

b
1
se
t-Value

2

l

2

m

lm

(

2
l m orthog

)

2

y0.0115
0.0021

y5.5

y0.0018
0.0014

y1.4

0.0158
0.0027
5.8

0.000267
0.000071
3.75

(cid:14)

.x

2.

same is true for model 4.5 . This includes the coefcient for the term
(cid:14) 2
.
, which takes us beyond the normal-theory SEF analogue 4.5 .
l m
Compared to the left side of Figure 3, the SEF density estimate from 4.6 has
its highest point at the upper right corner of the rectangle YY.

orthog

The next two sections discuss several criteria for choosing among possible
SEF models: observed deviance, expected deviance, degrees of freedom and so
forth. However, none of these criteria is sharp enough to entirely free the
statistician from model-choice quandries. A considerable amount of subjectiv-
ity must still go into the model-building process, just as in ordinary regres-
sion situations.

(cid:14)
.

(cid:14)

5. Total relative variance. A variety of diagnostic tools is available to
assist model selection in standard regression situations. Similar tools are
available for model selection in special exponential families. These ideas are
developed in the next two sections, beginning here with the total relative
variance, a simple measure of overall variability for an SEF density estimate.
For example, looking ahead to Table 4 the reader can compare the total
relative variances for the two galaxy-data SEF estimates in Figure 3: 6.8 for
the quadratic model in the left panel versus 8.8 for the smoothing-only model
in the right panel.

Let
.

(cid:14)

.






m s SEF s; m, X

(cid:14)
5.1

o Xb
indicate m s m e
, the special exponential family estimate 3.2 . First we
will compute the K = K derivative matrix of m with respect to s, which leads
immediately to a delta-method estimate of Cov m . This derivative matrix
involves the projection matrices
.
(cid:14)
5.2
where D s D m as before, P is the symmetric projection matrix into the
linear space spanned by the columns of X, in the inner product D, the
projection of vector v being PDv, and Q is the projection orthogonal to X s
column space. Because they represent orthogonal projections, we have


X 9 and Q s D y P ,



P s X X 9DX



y1



y1











(cid:14)

.

(cid:14)

.



(cid:14)

.



(cid:14)

.

(cid:14)

5.3

.

 

PDP s P ,

 
QDQ s Q and PDQ s 0.

 



2444

B. EFRON AND R. TIBSHIRANI

LEMMA 2. The derivative matrix of m s SEF s; m, X with respect to s is



(cid:14)

.

(cid:14)

5.4a

.

.
where, in terms of H s d log m rds, 3.3 ,



o



 
s DO

dm
ds
(cid:14)


 
O s P q QDH s P q H y PDH.

 





.

(cid:14)
5.4b

o
If m s Ms, then H s D 1rm M and

(cid:14)




(cid:14)
O s P q QD e

5.4c





.

(cid:14)

.

o

X 9b

.

M.

(cid:14)
.
The proof appears below.
The canonical parameter vector for the Poisson family 3.1 is
.

.
.
h s log m s log m , log m , . . . , log m .

(cid:14)
5.5
Likewise, dene h s log m and h s log m . Then we can write Lemma 2
as

(cid:14)
(cid:14)


(cid:14)


2
o.

.
(cid:14)





(cid:14)

.

(cid:14)

.

(cid:14)

.

.

(cid:14)

k

1

o

(cid:14)

5.6

.


dh
ds


s O s P q QD






dho
ds

.



Xb

This decomposes dhrds into a part P coming from the exponential family
and an orthogonal part QD dh rds coming from the adaptive
factor e
choice of the carrier.

Lemma 2 leads directly to delta-method estimates of the covariance matrix


.
of m, as in 3.6 ,



(cid:14)

.

o


(cid:14)

$

.

(cid:14)

.



   

   
Cov m s DODO9D or Cov m s DODO9D

(cid:14)
5.7
with D s D s . The diagonal elements give variance estimates var m or
$(cid:14)
var m for the individual components. For Poisson variables it is natural to
measure variance relative to the etsimate m . We dene the total relative
variance TRV estimate for m to be

(cid:14) .

k

k

k



(cid:14)

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

5.8

.

TRV s

K

(cid:7)
ks1


(cid:14)

m
k


var m
k

.

$

or TRV s

$

var m
k

(cid:14)

m
k

.

.

K

(cid:7)
ks1

COROLLARY 2. For m s SEF s; m, X the total relative variance estimates



(cid:14)

.

are

5.9(cid:14)

.

(cid:14)



 

/
 
TRV s tr DP q HDH9 DQD
or
$
.(cid:15)
/
 
TRV s p q 1 q tr HDH9 DQD ,


.(cid:15)
 

(cid:14) .

(cid:14)

.

(cid:14)

where p q 1 is the number of columns of X, D s D s , D s D m and P, Q are
.
as in 5.2 .

(cid:14)

(cid:14)



.

 

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2445

The proof of Corollary 2 follows directly from 5.7 by writing TRV in the
trace form tr D Cov m and using the orthogonality relationship 5.3 , and
similarly for TRV. Computationally more efcient expressions for TRV and
$
TRV appear in Remark I, Section 9.

y1
(cid:14)
$

.

(cid:14)

.

(cid:14)

.

(cid:14)

o

.

(cid:14)

(cid:14)

.

.

(cid:14)

(cid:14)

o

.

(cid:14)

.

q





Table 4 shows TRV for various SEF estimates for the galaxy data, as
discretized in Figure 2. The three estimates correspond to three choices of X
in 5.1 : sef 2 is for X as in 4.5 , sef 3 for X as in 4.6 , and sef 0 for X s 1.
This last choice is the smoothing-only estimate m rescaled to nrm m , so
that it sums to n s s . The carrier mo for the SEF is
(cid:14)
5.10
.
where M l is the matrix 4.4 .


mo s M l s,

o .
 q

(cid:14) .

All three TRV estimates decrease as the smoothing parameter l increases
because greater window width l decreases the variability of mo. If the carrier

mo were prechosen instead of adaptive, then sef 2 would exceed sef 0 by about
5, this being the increased number of free parameters, and likewise sef 3
would exceed sef 2 by about 1. This is seen in 5.9 for the case H s 0. In fact,
this is nearly the case at the right side of Table 4, where l is so large that mo
has nearly constant entries. The difference between the three estimates
decreases at smaller values of l because the adaptability of the common
carrier m s M l ? s absorbs some of the difference in the exponential family
ts e


Xb
.

(cid:14) .

Small variability is a good property of course, but we also want m to have
small bias for estimating the true density vector m. The next section puts
TRV into the context of bias]variance tradeoffs for Poisson regression esti-
mates.







(cid:14)

.

o

PROOF OF LEMMA 2.

In the notation following 5.5 , h s h q Xb. Differ-

o





(cid:14)

.



entiating this with respect to s and using 3.4 gives

(cid:14)

.

(cid:14)

5.11

.


s H q X X 9DX



(cid:14)

y1

.

 
X 9 I y DH s P q H y PDH,

 





dh
ds
.

which is 5.6 , the equivalent of Lemma 2. I

(cid:14)

Total relative variance TRV for three SEF estimates, galaxy data, at increasing values of

smoothing parameter l

TABLE 4

l s 1.5

a

8.8
9.6
10.2

l s 2

b

5.2
6.8
7.6

l s 4

1.2
5.2
6.2

l s 6

0.4
5.2
6.2

(cid:14)
(cid:14)
(cid:14)

.
sef 0 smoothing only
.
sef 2 4.5
.
sef 3 4.6
aRight panel of Figure 3.
bLeft panel of Figure 3.

2446

B. EFRON AND R. TIBSHIRANI







6. Degrees of freedom and estimated deviance. Selecting a good
SEF estimate m for a particular application involves making the usual
tradeoffs between variance and bias. This section concerns estimating the
total expected deviance of m from the expectation vector m. This is a measure
of accuracy for m that involves both variance and bias. An important role is
played by the degrees of freedom of the estimator m, an idea related to the
total relative variance of Section 5. The ideas here are an extension of those
in Section 6.8 of Hastie and Tibshirani 1990 . $
Figure 5 relates to the expected deviance measure EDEV developed below,
a diagnostic measure for comparing the goodness-of-t of different SEF
$
models. It shows EDEV for both the pain-score and galaxy examples. EDEV
is plotted versus the smoothing parameter l for the normal kernel estimates
(cid:14)
2.13 and 4.4 used to obtain m s M l ? s. The different curves correspond
to different choices of X in the SEF formula 3.2 . Smoothing-only refers to
X s 1, which gives the estimate mo renormalized to sum to n. For the
.
pain-score data, linear and quadratic are the cases referred to in 2.15 .
For the galaxy data, 4.5 and 4.6 are as in Table 4.

$

(cid:14) .

o



(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.



(cid:14)

.

$(cid:14) .
(cid:14)

Notice that EDEV l has a sharp minimum as a function of l for both
smoothing-only cases. At ls 0.63 for the pain-score data and at ls 1.5 for
the galaxy data. This is not true for the genuine SEF estimates. They allow
the smoothing parameter l to be chosen much larger without incurring too
much EDEV penalty. In other words, the SEF methodology allows us to
oversmooth the density estimate, with the advantages seen in Figure 4.

.

.



(cid:14)

. (cid:14)

FIG. 5. Expected deviance estimates EDev, 6.22 . Left panel: SEF estimates for pain-score data
(cid:14)
2.13 ] 2.15 . Right panel: SEF estimates for galaxy data as in Figure 5. In both cases the SEF
estimates permit the use of larger smoothing parameters l, compared to the smoothing-only
estimates.

.

$ (cid:14)

.

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2447

$

5

.

5

(cid:20)

.

.

(cid:14)

K





In order to motivate EDEV, it helps to begin the discussion with the
normal case. Suppose that the statistician observes a K-dimensional normal
vector
(cid:14)
.
6.1
and wishes to estimate m using some linear estimator
(cid:14)
6.2
S being a K = K matrix. Model selection amounts to making a good choice
of S.

z ; N m, I


m s Sz,

4
n s E m ,

5 2
err s z y m .

DF s tr S and TV s tr SS9,

Let err be the observed total residual squared error of m:
.

(cid:14)
6.3
Also dene
(cid:14)
6.4
where DF stands for degrees of freedom and TV stands for total variance.
Total variance TV equals (cid:7) var m , and if S is a projection matrix, then
tr S is the usual degrees of freedom. It is easy to prove the following two
relationships:
(cid:14)
6.5a
and
(cid:14)
6.5b

(cid:20)
E err y K y 2DF s E m y m

(cid:20)
5 2
E err y K y 2DF q TV s n y m .
(cid:14)

Both 6.5a and 6.5b play an important role in normal-theory model
selection. The rst of these is essentially the C or AIC criterion. Its exten-
sion to nonlinear estimators is Steins unbiased risk estimate. Extensions of
formula 6.5b are used in hypothesis testing. The quantity K y 2DF q TV
equals the residual degrees of freedom:
(cid:14)
6.6
If S is a projection matrix, then 6.5b can be improved to
(cid:14)
6.7
where the notation indicates a noncentral chi-square variate with noncentral-
ity parameter n y m . We test the adequacy of the estimate m s Sz by
comparing err to a central chi-square distribution x2

RDF s tr I y S I y S 9.

err ; x

n y m

2
RDF

.
(cid:14)

. (cid:14)

5 2

5 2

5 2



5



.

4

.

4

.

(cid:20)

(cid:14)



k

.

.

.

.

.

4

(cid:14)

(cid:14)

.

.

.

,

k

(cid:14)

5

p

(cid:14)

(cid:14)

(cid:14)

.

5

.

RDF

We now return to the Poisson situation where we observe
.

(cid:14)
6.8
and estimate m by some estimator m s m s , not necessarily of the SEF form
(cid:14)
.3.2 , having expectation
(cid:14)
6.9
The observed error err s err s in the Poisson context is
(cid:14)
6.10

(cid:14) .
.
err s s Dev s, m ,

(cid:14)
.
s ; Po m
K
(cid:14) .


(cid:14) .
n  E m s

(cid:14) .



.

.

(cid:14)

(cid:20)

4

.

2448

B. EFRON AND R. TIBSHIRANI

4
Dev m, n s 2 (cid:7) log m rn y m y n .

where Dev indicates the total Poisson deviance
.
(cid:14)

k
The Poisson equivalents of K, DF and TV in 6.5 are
(cid:7) k

4
K m s E Dev s, m ,

DF m s E

(cid:14)
.

(cid:14)

.

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:20)

(cid:20)

k

k

k

k

(cid:21)

k

s y m log m ,

.

(cid:14)

.

k

k

5

(cid:14)

6.11

.

4
TRV m s 2 E m log n rm ,

.

(cid:14)

(cid:20)

(cid:14)

.

(cid:7)

k

k

k

k

.
all expectations being with respect to s ; Po m .

(cid:14)

K

LEMMA 3.

.
In the Poisson situation 6.8 ] 6.11 ,
(cid:20)
(cid:20)
.
E err s y K m y 2DF m s E Dev m, m

. (cid:14)
4
.

(cid:14) .

(cid:14)

(cid:14)

.

(cid:14)

(cid:14)

4

(cid:20)
.
E err s y K m y 2DF m q TRV m s Dev m, n .

(cid:14) .

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

4

(cid:14)
6.12a
and
(cid:14)

6.12b

.

.

(cid:14)
.
6.5a, b .

(cid:14)
The proof is given below. Formulas 6.12a, b are the Poisson versions of

.

(cid:14)

.

.

(cid:14)



In order to use Lemma 3, we can approximate K m , DF m and TRV m by
their plug-in estimates K m , DF m and TRV m . Using bootstrap notation,
let s* given s have Poisson distribution
(cid:14)
6.13
and let E# indicate expectations with respect to 6.13 , with s and m s xed.
Then

(cid:14) .
s* s ; Po m s

(cid:14)
K

(cid:14) .







.

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

<

(cid:14)

.

(cid:14)

.

(cid:14)

.


K m s E# Dev sU , m
k



k

.

(cid:14)

(cid:14)

.

4

(cid:14)

6.14

.


s E# 2 s log s rm y s y m
k

U
k

U
k

U
k

.

k

(cid:14)

(cid:14)

.

4

.

It is easy to evaluate 6.14 numerically since it is the sum of univariate
Poisson deviances, the m being just xed constants in the E# expectations.
Using this same notation we can write the degrees of freedom estimate
(cid:14)


DF m as

.

(cid:14)

6.15

.

DF m s E#



.

(cid:14)

(cid:7) k

(cid:14)

sU y m log mU s E# s* y m 9h*,

 





k

k

(cid:14)

.

.

5

.

(cid:14)

(cid:14)

.



k
where h* s log m* s log m s* , the vector with kth component log m .
Since by 5.6 , dhrds s O, 6.15 has the Taylor series approximation
(cid:14)
6.16
.
See Remark K. An alternative estimate is DFs tr DO as in 5.8 .

$
DF m 5 E# s* y m 9O s* y m s tr DO  DF.
(cid:14)

 


(cid:14)


(cid:14)

 

..
.









(cid:14)

.

(cid:14)

.

.

.

U

(cid:7)

k

(cid:7)

k
(cid:14)
k

(cid:21)

(cid:20)

(cid:20)

.

k
(cid:14)

(cid:14)

(cid:14)

6.18

.

.
var m(cid:14)
.k
.  (cid:7) mk

.

(cid:14)

TRV m s
k$ (cid:14)
.
k

k

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2449

.
The quadratic expansion log mrn 5 mrny 1 y mrny 1



(cid:14)

.

2

gives

.
 
(cid:14)

m var m
k
k
2
2
n
k
permitting us to approximate 6.11 by

/m

E m log

n
k
k

6.17


s

(cid:21)

5

(cid:15)

(cid:14)

.

(cid:14)

(cid:14)

k

.


s

(cid:14)
2
1
(cid:14)

var m
k
2m
k

.

,

.

The quantities TRV and TRV in 5.8 are the obvious plug-in estimates for
(cid:14)
6.18 . The substitution of m for n in the denominator of 6.17 looks
worrisome, but Remark K of Section 9 shows that the resulting error is
asymptotically negligible.

(cid:14)

.

The TRV estimates 5.8 can be written as
$

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

.

(cid:14)

(cid:14)

.

.

.

.

.

(cid:14)

.





(cid:14)
(cid:14)

. (cid:14)

.

$

  

$

 
DF s tr DO or DF s tr DO.

   
TRV s tr DO9DO or TRV s tr DO9DO,

(cid:14)
6.19
using 5.7 , compared to the DF estimates
(cid:14)
6.20
Notice the similarity to the normal-theory denitions in 6.4 . Comparing
(cid:14)
6.5b with 6.12b , we can also dene residual degrees of freedom RDF for
the Poisson situation, estimated by
(cid:14)
6.21
See Remark L. The quantity graphed in Figure 5 was the expected deviance
.
estimate obtained from 6.12a , 6.14 and 6.20 :
(cid:14)
6.22

$ $
RDF s K m y 2DF q TRV or RDF s K m y 2DF q TRV.

.
(cid:14) .

(cid:14)
$
$
EDEV s err s y K m q 2DF.
$

The vertical scale in Figure 5 is misleading. For the galaxy data, reducing
l from 4 to 1.5 reduces EDEV for the quadratic SEF 4.5 from 22.2 to 11.5, a
.
considerable amount. Is this signicant? The observed deviance error 6.10 ,$ (cid:14)
.
err, decreases by 45.3, while the residual degrees of freedom RDF, 6.21 ,
decreases by 29.3. This gives the naive chi-square signicance value
prob x ) 45.3 s 0.03. A more trustworthy signicance level could be

obtained by Monte Carlo methods, bootstrapping with s* ; Prob m , with m
the 1.5, 2 SEF vector.

(cid:14)
The quantitative aspects of Figure 5 cannot be taken too literally. For
instance, using EDEV instead of EDEV moved the smoothing-only curve
.
below the others for l- 2 in the galaxy data. In general the careted hat
formulas gave less erratic answers than the bar formulas, but there are really
no strong reasons for preferring EDEV. The fact is that it is usually difcult
to estimate the performance of competing decision rules, and the SEF density
estimates are no exception. Formulas like 5.9 and 6.12 help with model
selection, but considerable subjectivity remains. The main point of Figure 5 is
the qualitative one that the SEF estimates permit extensive oversmoothing.

$

$

2
29.3



.

(cid:14)

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:20)

4

PROOF OF LEMMA 3. Dene EE m s E Dev s , m , where s ; Po m inde-
.
pendent of s, so EE m is the expected prediction error of m s . Efron 1986


(cid:14) .

(cid:14)

.

K



(cid:14) 

.
(cid:14)

.4

(cid:14)

(cid:14)

.

(cid:20)



2450

B. EFRON AND R. TIBSHIRANI

(cid:20)

(cid:14)

.

.

.

(cid:14)

.

4
EE m s E err s q 2DF m .

(cid:14) .

shows that
(cid:14)
(cid:14)
6.23a
We also have the identities
(cid:14)
6.23b
and
(cid:14)
6.23c
All three of these relationships are easy to prove directly from the denition
.
.
of the total Poisson deviance. Substituting 6.23b into 6.23a gives 6.12a .
Substituting 6.23c on the right side of 6.12a gives 6.12b . I

.
E Dev m, m s Dev m, n q TRV m .

.
EE m s K m q E Dev m, m



(cid:14)

.

.

(cid:14)

(cid:14)

.

.

(cid:14)

.

(cid:14)

(cid:14)

(cid:14)

.

4

(cid:14)

(cid:14)

.

(cid:14)

(cid:20)

4

.

(cid:14)

.

(cid:14)

0

7. Multisample problems. So far we have only considered one-sample
problems. SEF estimates are particularly useful for investigating density
differences in multisample situations. We use the exponential family model
(cid:14)
.1.2 for the different densities, with a shared carrier g estimated nonpara-
metrically, but with possibly different values of the exponential b parame-
ters. An example will precede the theory.

Figure 6 concerns a two-sample application of SEF modelling. The data are
the compliances of men in the Stanford arm of a randomized trial of the
.
cholesterol-lowering drug Cholostyramine; see Efron and Feldman 1991 .
There were n s 172 men in the control group and n s 165 men in the
treatment group. Compliance ran from 0 to 100%, so YY s 0, 100 . A dis-
cretization YY s D YY partitioned YY into K s 46 intervals of equal length.
The left panel of Figure 6 shows the counts in the two groups. Compliance is
signicantly worse in the treatment group, as shown by standard two-sample
tests.

(cid:14)

w

x

k

k

1

2

(cid:14)

FIG. 6. An application of SEF modelling to the Cholostyramine trial compliance data of Efron
and Feldman 1991 . Left panel: The count vectors for the control group and the treatment group;
K s 46 equal divisions of YY s 0, 100 . Right panel: SEF density estimates 7.4 for the two
groups; m and X as in 7.11 . The poorer compliance in the treatment group is graphically
evident.

(cid:14)

.

(cid:14)

.

.

w

x

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2451

The right panel of Figure 6 is the output of a two-sample SEF analysis. It
neatly displays the compliance differences between the two groups in terms of
.

their estimated densities. The ratio g
y decreases almost lin-
treat
early as y goes from 0 to 100%, but both densities are greatest at y s 100%.
These densities were derived from a simple extension of the previous
theory. In the multisample situation we observe independent random sam-
ples from L possibly different densities g , g , . . . , g
on the same sample
space YY,

.

y rg
cont

(cid:14)

(cid:14)

L

1

2

i.i.d.

(cid:14)

7.1

(cid:14) .

y l ; g y
i

.
(cid:14) .
We discretize the problem as in Section 2, obtaining count vector s l

for i s 1, 2, . . . , n and l s 1, 2, . . . , L.

(cid:14)

.

l

l

for

the lth sample,
(cid:14)
The many-sample version of the Poisson regression model 3.1 is

for k s 1, 2, . . . , K and l s 1, 2, . . . , L.

(cid:14) .
l s a y l g YY
k

(cid:14) .

7.2

s

.

(cid:20)

4

(cid:14)

.

k

i

ind.

.

(cid:14)

.

K

.

(cid:14)

.

7.3

(cid:14) .

o Xb(cid:14)l.

(cid:14) .
(cid:14) .
s l ; Po m l

for l s 1, 2, . . . , L, with m l s m e

(cid:14)
The SEF estimates corresponding to 3.2 are

(cid:14)
.
7.4a
where
(cid:14)
.
s 0.
In this model, a common carrier mo, estimated from all of the data
(cid:14) (cid:14) .
s 1 , s 2 , . . . , s L is modied by an exponential factor e
which can vary
with l.


o
m s m s 1 , s 2 , . . . , s L

and b l : X 9 s l y m e

(cid:14) .
m l s m e

7.4b

o Xb(cid:14)l.

o Xb(cid:14)l.

Xb(cid:14) L.

(cid:14) .

(cid:14) .

(cid:14) .

..









(cid:14)

.

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)



,

The many-sample version of Lemma 1 in Section 3 is the following lemma:

LEMMA 4. Let

.

(cid:14)

7.5

(cid:14)
.
 log mo
s l(cid:14) .
(cid:14)
o .
k
be the K = K matrix with kjth entry  log m rds l . Then the p q 1 = K
(cid:14) .
derivative matrix of b l with respect to s j
is


H l s

(cid:14) .

(cid:14) .

(cid:14) .

(cid:14)

.

j

(cid:14)

7.6a

.

b l(cid:14) .
s j(cid:14)
.

(cid:14) .
s G l



y1

X
Z ,
l j

where
.
(cid:14)
7.6b
(cid:14) .
D l  D m l

(cid:14)



(cid:14)

7.7

.




(cid:14) .
(cid:14)
G l s X 9D l X and Z s X d I y D l H j
(cid:14) ..

, d equalling 1 or 0 as l does or does not equal j. If

(cid:14) .

(cid:14) .

.

,





X
l j

X
l j

l j

l j


mo s Ms
q

(cid:15)

s  s ,
q

(cid:7)

l

l

/

2452

then
(cid:14)
.
7.8

B. EFRON AND R. TIBSHIRANI

(cid:14)
X
Z s X d I y D e
l j

X
l j

l j



Xb(cid:14)l.

.

M .

(cid:14)
.
The proof is nearly the same as for Lemma 1 and will not be given here.
Lemma 4 leads to delta-method estimates of the covariance matrix for

(cid:14) .
.
b l , just as in 3.6 ,

(cid:14)

7.9(cid:14)

.

(cid:14) .
Cov b l s G l

(cid:14) .





(cid:14)

$
(cid:14) .
Cov b l s G l

(cid:14) .





(cid:14)

L

. (cid:7)
js1
L
. (cid:7)
js1

y1

y1

(cid:14) .
Z D j Z G l

(cid:14)

.

X
l j

l j



(cid:14) .
Z D j Z G l

(cid:14)

.

X
l j

jl





y1

or

y1

,

(cid:14)

(cid:14) .
D j  D s . We can also obtain covariance estimates for functions of the

(cid:14) .
b l . For example, if gs b 2 y b 1 , then


(cid:14) .



.

j


(cid:14) .
g
s 1(cid:14)
g
s 2(cid:14)

.

.

(cid:14)
s G 2



(cid:14)
s G 2



.

.

y1

(cid:14)
X
Z y G 1
21



.

y1

X
Z ,
11

y1

(cid:14)
X
Z y G 1
22



.

y1

Z

X
12

(cid:14)

7.10a

.

and

(cid:14)

7.10b

.

Cov g s



(cid:14)

.

$
Cov g s



(cid:14)

.

js1

2

(cid:7) (cid:15)
(cid:7) (cid:15)

2

js1


g
(cid:14)
s j


g
(cid:14)
s j

.

.

/
/

(cid:14)
D j

.


(cid:14)
D j

.

(cid:15)
(cid:15)


g
(cid:14)
s j


g
(cid:14)
s j

.

.

9

9

,

.

/
/

(cid:14)

.

SEF model 7.4 was used to estimate the two compliance densities in

,

.

.

.

(cid:14) k.

(cid:14) k.


, y 2
(cid:14) k.


x s 1, y
(cid:14) k.
k

(cid:14)
being the midpoint of YY ; mo s Ms as in

Figure 6. The kth row of X was quadratic in compliance,
(cid:14)
7.11a

where y s y y 50r100, y
(cid:14) k.
(cid:14)
.7.7 , with
(cid:14)
7.11b
(cid:14)
as in 2.14 . The estimated difference gs b treatment y b control was
(cid:14)
.
7.12
with the standard errors taken from Cov g , 7.10b . We see that the linear
coefcient is signicantly negative, t-value s y3.6, but the quadratic coef-
cient is not.


(cid:14)
.
0.26, y1.36, y0.44 " 0.22, 0.38, 1.51 ,

.
M s M 7 ,

(cid:14)

(cid:14)
.
$(cid:14)


. (cid:14)



(cid:14)

(cid:14)

.

.

.

.

k

1

8. Other types of estimators.

In the SEF methodology the application
of an initial nonparametric smoother is followed by the tting of an exponen-
tial family parametric model. Hjort and Glads 1995 semiparametric density

(cid:14)

.

EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2453

estimator reverses this order, with the parametric model coming before the
.
smoother. Backtting, as applied to generalized linear models like 3.1 ,
repeatedly iterates between the parametric and nonparametric tting meth-
ods. This section discusses the SEF methodology in the context of these other
possibilities.

(cid:14)

As in Section 6, we begin with the normal case where the statistician

K

.

(cid:14)

.

(cid:14)
.

.
z ; N m, I .

.

m s PP z; j  j q P z y j .

wishes to estimate m having observed
(cid:14)
8.1
The linear model m s j q Xb, with j a known origin vector and X a known
K = p q 1 structure matrix, gives the estimate
(cid:14)
8.2
Here P s X X 9X
X 9 is the K = K projection matrix into LL X , the col-
umn space of X. In the normal case, PP z; j plays the role of the parametric
exponential family model estimator. The role of the nonparametric smoother
is played by
(cid:14)
8.3
where M is some xed K = K smoothing matrix such as M l in 2.13 . Once
again j represents a xed and known origin. Often we take j s 0, but it will
be important here to consider more general choices of the origin.

.
MM z; j s j q M z y j ,

.y1

(cid:14) .

(cid:14)

(cid:14)

(cid:14)

.

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

The normal-theory analog of the SEF estimate 3.2 is
.
sef
m s PP z; MM z; 0 s P q M y PM z

(cid:14)

.

(cid:14)

(cid:14)

.

8.4(cid:14)

.

(cid:14)

.
s P q QM z.

(cid:14)

.

H(cid:14)

.

o

(cid:14)

.

(cid:14)



o.

(cid:14)
(cid:14)



o

Here Q s I y P is the projection matrix into LL X , the orthocomplement to
LL X . In other words, we begin with 0 as the origin, apply MM to get an
updated origin m s MM z, 0 s Mz and nally take the estimate of m to be

m s PP z; m s m q P z y m . Notice that
(cid:14)

X 9z s X 9m
8.5

according to 8.4 , which says that z and m have the same projection into
LL X , namely, Pz. Equality 8.5 is the normal-theory analog of the moment-
.
matching property 2.17 .

.


o.

.

(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

.

Reversing the order of PP and MM, as Hjort and Glad do in the density

(cid:14)

.

.

(cid:14)

.

(cid:14)

 HG
m s MM z; PP z; 0 s P q MQ z.

estimation problem, gives the estimator
(cid:14)
.
8.6
This no longer enjoys the moment-matching property 8.5 , but we can restore
it with one further application of PP. This denes the symmetrized estimator
(cid:14)
8.7
If M is a symmetric matrix with eigenvalues between 0 and 1, then so is
.
P q QMQ. This makes m a formal Bayes estimator for m in situation 8.1 ,
versus a normal prior distribution on m possibly having innite variance in
some directions.

s P q QMQ z.



m s PP z; m

sym

sym

HG

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

2454

B. EFRON AND R. TIBSHIRANI

The backtting estimator m

 back

is dened as follows in Chapter 5 of Hastie

o

1

.

.

.

.

(cid:14)

.

(cid:14)










(cid:14)



mo s MM z; m1 y m1 and m1 s PP z; mo y mo.

and Tibshirani 1990 : suppose we can nd vectors m and m such that
(cid:14)
8.8a
Then
(cid:14)
8.8b
w

so m s MM z; m s P z; m . Letting v s z y m , it is easy to show that
(cid:14)
.
8.9
(cid:14)
which implies the moment-matching property 8.5 . Hastie and Tibshirani
show that m s S
that is symmetric and with
x
eigenvalues in 0, 1 if M has these same properties.



m s mo q m1
o. x

.
v y Mv s z y m y m g LL X ,

z for a matrix S

 back
w

Further insight into the backtting estimator can be gained by expressing


H

.

back

back

back

back

1.









(cid:14)

(cid:14)

(cid:14)

(cid:14)

.

1

1

o

it in an explicit form. The backtting estimator satises

.

(cid:14)

y1

bs X 9X
(cid:14)
.
X 9 z y m ,
.


1
m s M z y Xb .

(cid:14)

0

Solving these equations yields the explicit expression

bs X 9 I y M X

(cid:14)

.

y1

X 9 I y M z.

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

H

sef







.


This looks like a weighted least squares estimate with weight matrix I y M
or, equivalently, variance matrix I y M . It has the same form as Hjort
and Glads estimator except that they use an unweighted least squares
estimator. The weights I y M can be justied from a mixed effects model in
which m1 is a random effect.

.y1

sym
.

 back

H(cid:14)

We have discussed four linear estimators m s Sz, three of which have the
moment-matching property 8.5 . These three can be described as follows: the
LL X component of m matches the LL X component of z; the LL X compo-
.
nent of m equals the LL X component of a point v in the at space z [ LL X ;
v s z for m , v s Qz for m and v equals the point or points satisfying the
orthogonality condition 8.9 for m
. Note: All four estimators are the same
if M and P commute, MP s PM.

We calculated the equivalent kernels for each of the four estimators, as
in Figure 2.5 of Hastie and Tibshirani 1990 . The calculation was done for
the situation that produced the quadratic estimator in Figure 1: 40 equally
spaced x values, P based on a matrix X with rows 1, x, x
and M of the
form 2.14 . The smoothing parameter l was chosen to give tr S s 5 in each
case. The SEF and backtting kernels were remarkably similar, with both
the HG and symmetrical kernels being slightly different.

and m

for the Poisson
situation s ; Po m . Given a positive origin vector j and structure matrix
X, we let the analog of 8.2 be

(cid:14)
Xb(cid:14) j.


We can dene analogs to m , m , m

.
, where X 9 s y D j e

.
PP s; j  D j e

s 0;

8.10

Xb(cid:14) j.

back

2.

sym

HG





sef

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

(cid:14)

(cid:14)

.

(cid:14)

(cid:14)

K

.

.

(cid:14)

.

(cid:14)



EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2455

(cid:14)

.

(cid:14)

.

.

(cid:14)

.

.

(cid:14)

.

.

(cid:14)

.

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

sef

HG

.x

sym

(cid:14)
(cid:14)

(cid:14)
(cid:14) . Xb

. The analog of 8.3 is


m s PP s; j is the MLE estimate of m in the offset generalized linear model
m s D j e
(cid:14)
8.11

m s MM s; j is a discretized version of what Hjort and Glad 1994 call a
nonparametric density estimate with a parametric start, the start being the
choice of j.

.
MM s; j s D j MD 1rj s s D j M srj ;


and m
(cid:14)


sef
.

m s PP s; MM s; 1 ,

. (cid:14)

The Poisson analogs of m , m
HG
.
.

m s MM s; PP s; 1 ,


.
(cid:14)
8.12
.
HG

o
The backtting estimate 8.8 is now dened by m s m q m , where m

and m1 satisfy the xed-point relationships
(cid:14)
.

8.13
.
Chapter 6 of Hastie and Tibshirani 1990 discusses an iterative algorithm
for computing m

.
and m1 s D 1rmo PP s; mo .
(cid:14)

w(cid:14)
8.4 ] 8.7 are
(cid:14)



m s PP s; m
sym

o



mo s D 1rm1 MM s; m1


mo s Ms,


The moment-matching property 2.17 is satised by m , m

sym
.
.
and mo s M srm1 .

Each of these estimators can be written in the form m e
(cid:14)
8.14
We have used m in all of our examples because it makes the computation of

H s d log m rds, a crucial part of the formulas in Sections 2]7, so simple.
w
An even simpler choice, log m s Hs for some xed matrix H, does not
x
satisfy 5.14 and seems to have undesirable small-sample properties.
(cid:14) .

In theory at least we can compute H for any choice of m s m s . Here,

sef

o Xb (cid:14)
, 3.26 , with



mo s m

sef
 sef


and m

 back

back

back

back

sym

o.

HG

























(cid:14)

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

o

.

(cid:14)

(cid:14)

.

.

(cid:14)

(cid:14)

.

1

o

.

 sym
o
without proof, is H for m :



..
LEMMA 5. For m s m s MM s; PP s, 1 ,

 sym
o



(cid:14)

(cid:14)

o

the derivative matrix H s



(cid:14)

o.


.



d log m rds is
(cid:14)

o o
8.15a H s D m rm MD 1rm q I y D m rm MD srm
where
(cid:14)
8.15b


and P s X X 9D m


o o
m s PP s; 1

X 9.

y1

X













o o

o o

o o

o o

o o

(cid:14)

.

(cid:14)

.

.

(cid:14)

(cid:14)

(cid:14)

.

.

(cid:14)

.

o

o

.


o o
P ,

sef


in Figure 3, m s PP s; MM s; m


The symmetrized version of the m

..
,
gave similar but somewhat rougher contours than those in the left panel of
Figure 4. There is no compelling theoretical reason for preferring m
or

to m , though they seem closer in structure to Bayes and maximum
m
likelihood estimators. In practice, the specic choices of M and X seem more

crucial to successful estimation than does the choice between m , m
or
 back
m

 sym

back

sym

sym







sef

sef

sef

(cid:14)

(cid:14)

.

9. Remarks. The following remarks apply to the indicated sections.

(cid:14)

REMARK A Section 2 . There is an interesting connection between moment-
matching and function-preserving properties of smoothers. For a smoother

ms Ms, the condition (cid:7)m s (cid:7)s requires 19M s 19 where 1 is a column of

i

(cid:14)

i

.

2456

B. EFRON AND R. TIBSHIRANI

.

ones , or equivalently M91 s 1. Now most smoothers preserve constants so
that M1 s 1. For symmetric M we see that matching the zeroth moment is
the same as preserving the constant vector. However, most smoothers such as
kernels are not symmetric, and hence will not match the zeroth moment
exactly. Similarly, a smoother may preserve a vector t so Mt s t without
satisfying the moment-matching property M9t s t. In general, moment-
matching is equivalent to function-preserving for the transpose of the
smoother matrix.

(cid:14)

.

(cid:14)

.
k

o

w

.

(cid:14)

.

.

(cid:14)

o Xb

REMARK B Section 3 . What is the true parameter b being estimated by

b? Suppose that s ; Po m , but that m is not necessarily of the form m b in
(cid:14)
3.1 . From m we determine m s m m and then b the solution vector to the
equations X 9 m y m e s 0. Under reasonable conditions b will be an
asymptotically normal estimate for b, in the usual manner of an MLE. For
instance if m s np, with p xed and n  , and if the mo estimate is
homogeneous, m cs s cm s , then it is easy to show that
(cid:14)
9.1
where, with Z9 s X 9 I y D e


n by b  N
w
o

(cid:14)
x
dm rdm ,

0, A BA

(cid:14) .


pq1

y1

y1



(cid:14)

.

.

.

(cid:14)

(cid:14)

.

x

,

(cid:14)

9.2

.

A s lim X 9D


n

The bias of b as an estimate of b is of order 1rn.

X and B s lim Z9D

n

(cid:15)

m
n

/

Z.

.
(cid:14) Xb.
moe Xb

n

/

(cid:15)

o

.

(cid:14)

(cid:14)



(cid:14)

.

(cid:14)
.

.
REMARK C Section 3 .

In case 2.13 , m s Ms, the matrix Z in 3.4a is a

o
function of b but not of m , say Z s Z b . If ds is a K vector orthogonal to
the columns of Z b , then

(cid:14)
9.3
.
This implies that the level surfaces of constant b value are K y p q 1 -
dimensional at subspaces in the K-dimensional s space. Flat level surfaces
tend to make delta-method covariance estimates such as 3.6 more accurate.


dbs X 9DX

Z9 b ds s 0.

y1





.

(cid:14)

w

x

.

(cid:14)

.

(cid:14)

(cid:14)

.

REMARK D Section 4 . Here are the total Poisson deviances:

(cid:14)

.

Dev s, m s 2 s log



.

(cid:14)


y s y m
k

k

(cid:14)

.

5

(cid:15)

sk

/mk

(cid:21)

(cid:7) k
k
(cid:14) .
(cid:14)

(cid:14)

9.4

.

9.5(cid:14)

.

.

1
for four choices of m: m s M 2 s; m the SEF based on this m and X s 1, i, j ;
.

2
m the SEF 4.5 ; m the SEF 9.3 :

m1

 

3


m0


m2


m3



(cid:14)

.

(cid:14)

o

o

247.1 240.3 226.1 220.3.
(cid:14)

2.

3.

(cid:14)



2
The deviance decrease D s, m y D sm s 5.77 is much smaller than 3.75 ,
the square of the corresponding t-value in Table 3. The naive t-value,
calculated using the standard error estimate from 3.7 , is the right one for
approximating the deviance decrease. In this case the naive t-value is
0.000267r0.000111 s 2.74, predicting 2.742 s 5.76 for the deviance decrease.

(cid:14)

.



EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2457

(cid:14)

REMARK E Section 4 . Results like 3.6 can be directly derived for
continuous special exponential families 1.3 without going through the Pois-
son discretization argument. Suppose we estimate g y in 1.2 by a continu-
.
ous version of 2.13 ,

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

0

(cid:14)

.

.

(cid:14)

9.6

.


g
0

(cid:14)

.
y s

n1
(cid:7)
n is1

.
M y y ,

(cid:14)

i

<

(cid:14)

<

<

i

where, for any y , M y y is a distribution over YY not necessarily satisfying
(cid:14)
H M y y dy s 1 . We dene the SEF density estimate to be g
y s
YY
0
g y exp b q t y b , where bs b , b satises the maximum likeli-
hood equations

 
1
0

.

0


1

(cid:14)
.



i
(cid:14)

(cid:14)

(cid:14)

.

.

(cid:14)

.

.

x

b

i

w

.

(cid:14)

9.7

.

H

YY

(cid:14)
t y g

.


b

(cid:14)

.
y dy s t s

n1
(cid:7)
n
is1

(cid:14)
.
t y ,

i

as well as the constraint H g

YY b

Dene

(cid:14)

.
y dy s 1.

i

(cid:14)

.

(cid:14)

.

9.8

(cid:14)
z s t y y t y
i
$(cid:14) .
and let Cov t
corresponding to g

.

(cid:14)

H

YY

(cid:14)

(cid:14)
t y y t exp b q t y y t b M y y dy

.

(cid:14)

.

i

<

(cid:14)

.

(cid:14)

.

.


1

.

(cid:14)


0

indicate the covariance matrix of t y for the distribution on YY

(cid:14)

.

.
y . Then the continuous analog of 3.6a is

(cid:14)

.

b
(cid:14)

(cid:14)

9.9

.

(cid:14) .
Cov b s Cov t


1

.

$

1
n

y1

n

(cid:7)
is1

i

X
z z
i
n

$
(cid:14) .
Cov t

y1

.

.

(cid:14)

.

(cid:14)
9.9 is the limit of 3.6a as the discretization 2.1 becomes innitely ne,
after the superuous parameter b is removed.

$

In order to use 9.9 , we need to evaluate the integrals over YY involved in
(cid:14)
.
9.7 , 9.8 and Cov t . The discretization argument effectively does such
integrals by summation over k s 1, 2, . . . , K. If YY is high dimensional, we
might prefer to work in the continuous mode, doing the integrals by some
more efcient algorithm such as componentwise Simpson rules.

.
(cid:14) .


0

(cid:14)

(cid:14)

.

(cid:14)

.

.

(cid:14)

REMARK F Section 5 . We will often be interested in the probability




p s mrm s mrn,
(cid:14) .
(cid:14)

 
q

(cid:14) .
m cs s cm s

vector
(cid:14)
.
9.10
rather than in m itself. Suppose that m s m s is scale homogeneous,
.
(cid:14)
9.11

The m s SEF s; m, X is also scale homogeneous and p cs s p s . Familiar
homogeneity properties give

dm
ds

s s m and

I y p19

.
c ) 0 .


dp
ds


dm
ds


dm
ds

s 19,

9.12

(cid:14) .

s

19









.

(cid:14)

.

(cid:14)

.

(cid:14)

(cid:14)

.

(cid:14)

.

.

o

1
q
m

2458

B. EFRON AND R. TIBSHIRANI

.

(cid:14)

Using 9.12 it is not difcult to show that
1
2
n

Cov p 


dp
ds


dp
ds

s

D



(cid:14)

.

.

9

(cid:14)

9.13

(cid:15)

/

(cid:15)

/

Cov m y



(cid:14)

.

 
mm9
n

and that the diagonal elements of Cov p satisfy

(cid:14)

9.14

.

trv  n

 TRV y 1.

.

(cid:14)

var p(cid:14)
.i
(cid:7) p k

k

Thus the comparisons in Table 4 remain valid for trv. The results for the
equivalent of TRV are a little less neat.

$

REMARK G Section 5 . Let p s srn, the vector of empirical probabilities,

(cid:14)

.

and dene

(cid:14)

9.15

.

.

(cid:14)

d  D p s Drn,
y1



p  X X 9dX

(cid:14)

.


X 9 s nP ,


d  D p s Drn,

.



(cid:14)


y1



q  d y p s nQ,
.
.





(cid:14)

o



h  nH.

(cid:14)

.

(cid:14)
.

w
x
Under the homogeneity condition 5.14 , h s d log p rdp, for p  m rn.
Then 5.9 can be written as
/
 
 
(cid:14)

or TRV s p q 1 q tr hdh dqd
not depending on n. This shows that TRV and TRV are O 1 as n  . See
Remark K.

 

TRV s tr dp q hdh 9 dqd

$ (cid:14) .

$

 

9.16









(cid:14)

.

(cid:14)

.

(cid:15)

(cid:15)

(cid:14)

.

p

o

o

.

(cid:14)

REMARK H Section 5 . Suppose that the discretization of YY becomes
innitely ne, with K going to innity and the kth cell YY having volume D
k
going to zero. Under sufcient regularity conditions, p y rD will approach
.
(cid:14)
 (cid:14) k.
, an estimate of the original continuous density with approximate
g y
variance

k
(cid:14)
 (cid:14) k.

.

k

(cid:14)

9.17

.

var g y

(cid:14)
 (cid:14) k.

(cid:20)

4

.

s

(cid:20)

var p y(cid:14)

2D k

4

.

(cid:14) k.

.

(cid:14)

.

.

(cid:14)

9.18

Then trv, 9.14 , will approach
var g y
.
.
with CV y  Var g y
rg y , a coefcient of variation measure for g y .
From Remark G we see that CV y is typically O 1r n as n  . The
estimated average value for the coefcient of variation of SEF 4.5 was 0.15.

(cid:20)
(cid:14)

g y(cid:14)

..x
1r2

dy s n CV y

H
n
YY
(cid:14)
(cid:14)


(cid:14)

g y dy,

H

(cid:14)


(cid:14)




YY

.

(cid:14)

.

.

4

(cid:14)

.

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

w

p

2

REMARK I Section 5 . A computationally more efcient expression than

(cid:14)

.

(cid:14)
.5.9 is

(cid:14)

9.19

.

TRV s tr D P q H9DH y H9DPDH .

   

  



EXPONENTIAL FAMILIES FOR DENSITY ESTIMATION

2459

.

(cid:14)

(cid:14)

2.

2

.

(cid:14)

(cid:14)





3.

Each of the three terms in 9.19 can be evaluated using O K multiplica-
tions rather than O K . This makes values of K as large as 1000 practical.
Letting G s X 9DX, the O K computing formula is

(cid:14)
X 9DX q D HD
5
 

(cid:14)
1r2
y1
q tr G
D H9DX
$
.

2
a  (cid:7) (cid:7) a for matrix a , and similarly for TRV, substituting D for D.
5
When m s Ms the middle term in 9.20 equals (cid:7) m A , where


y1
TRV s tr G

.
 

1r2
.

X 9DHD

9.20

k

1r2

1r2

i


(cid:14)5



2
i j

(cid:14)

.

(cid:14)

.

.

5

k

k

2

o

j

(cid:14)

9.21a

.


A s M s r
j

2
k j

k

(cid:7)



j

(cid:7)

j


M  m rm M ,





k j

k j

o
k

(cid:14)

.

k

(cid:15)

(cid:14)
2

/

j


M s
k j
$




j

j
$

and s equals s for TRV or m for TRV. For the TRV case A F 1, but A can
blow up for TRV. The calculations of Section 6 replace A with
(cid:14)
9.21b
in the TRV case.

min A , 1

.
$

(cid:14)

.

k

k

k

k

j

REMARK J Section 6 . The degrees of freedom formula 6.20 can be

(cid:14)

.

(cid:14)

.

rewritten as

$

(cid:14)





.


 
Xb

.
(cid:14)
9.22 DF s tr D P q H y PDH or DF s p q 1 q tr D H y PDH .
The P term corresponds to the e
part of the SEF denition 3.2 , while the

H term corresponds to the choice of the carrier m . The PDH subtraction
term corrects for collinearity between the carrier and the exponential family
terms. The degrees of freedom denition for p rather than m, as in Remark
F, subtracts 1 from formula 6.20 or 9.22 . It takes O K multiplications to
compute DF or DF from either formula.

(cid:14)
 
.
(cid:14)

 

 

$

2.







.

(cid:14)

.

(cid:14)

.

(cid:14)

o

.

(cid:14)

(cid:14)

.

(cid:14)

.
$  

REMARK K Section 6 . Write s s np as in Remark G and consider expres-
.
sion 6.15 for DF m as n   with p xed. Assuming m s s cm s , 9.11 ,
the approximation DFs DO does not depend on n and so is O 1 just as in
(cid:14)
9.16 . Using higher-order expansions it is easy to show that
(cid:14)
9.23
so at least in this sense DF is a good approximation to DF m . The corre-
$
sponding results hold for DF, TRV, and TRV.

(cid:14)
.
DF m y DF s O 1rn ,
$

(cid:14) . (cid:14)

.
.

$

(cid:14) .

(cid:14) .





.

(cid:14)

(cid:14)

.

p

p

.
(cid:14)

REMARK L Section 6 . The Poisson residual degrees of freedom formula

(cid:14)

.

.

.

(cid:14)

RDF m s K m y 2DF m q TRV m

(cid:14)
9.24
is always nonnegative. To prove this we use 6.12b to write
(cid:14)
9.25
and note that E s, m s m, n . The proof is completed with the fact that the
.
Poisson deviance Dev m, n is a jointly convex function of m, n .

.
RDF m s E Dev s, m y Dev m, n ,


.4
(cid:14)

.
(cid:14)

(cid:20)(cid:14)



(cid:14)

.

(cid:14)

.

(cid:14)

.

(cid:14)

.

.

(cid:20)

4

(cid:14)

.

.

(cid:14)

(cid:14)

2460

B. EFRON AND R. TIBSHIRANI

The RDF estimates 6.21 are not necessarily nonnegative. They become so

if we replace K m in 6.21 with the Taylor series estimates

(cid:14)

.

(cid:14)
(cid:14)

.
.

(cid:14)

9.26

.


y1
K s tr DD

.
or K s tr DD s p q 1 ,

y1

(cid:14)

 



but these were poor approximations in our examples.

.

(cid:14)

REMARK M Section 8 . We tried a loess smoother Cleveland and Gross
(cid:14)
.x
for the galaxy data, in the form of a generalized additive Poisson
1992
w
model Hastie and Tibshirani 1990 . We used a degree 2 loess model, that is,
one that ts second degree polynomials locally. This gave an expected de-
viance picture similar to Figure 5. This is not surprising given the similarity
between the moment-matching and function-preserving properties mentioned
in Remark A.

.x

(cid:14)

w

