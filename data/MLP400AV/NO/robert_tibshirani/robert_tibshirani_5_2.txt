Abstract

We propose a method for the classication of more than two classes, from high-dimensional fea-
tures. Our approach is to build a binary decision tree in a top-down manner, using the optimal
margin classier at each split. We implement an exact greedy algorithm for this task, and compare
its performance to less greedy procedures based on clustering of the matrix of pairwise margins.
We compare the performance of the margin tree to the closely related all-pairs (one versus one)
support vector machine, and nearest centroids on a number of cancer microarray data sets. We also
develop a simple method for feature selection. We nd that the margin tree has accuracy that is
competitive with other methods and offers additional interpretability in its putative grouping of the
classes.

Keywords: maximum margin classier, support vector machine, decision tree, CART

1. Introduction

We consider the problem of classifying objects into two or more classes, from a set of features. Our
main application area is the classication of cancer patient samples from gene expression measure-
ments.

When the number of classes K is greater than two, maximum margin classiers do not gen-
eralize easily. Various approaches have been suggested, some based on the two-class classier
(one-versus-all and one-versus one or all pairs), and others modifying the support vector loss
function to deal directly with more than two classes (Weston and Watkins, 1999; Lee et al., 2004;
Rosset et al., 2005). These latter proposals have a nice generalization of the maximum margin
property of the two class support vector classier. Statnikov et al. (2004) contains a comparison of
different support vector approaches to classication from microarray gene expression cancer data
sets. While these methods can produce accurate predictions, they lack interpretability. In particular,
with a large number of classes, the investigator may want not only a classier but also a meaningful
organization of the classes.

In this paper we propose a tree-based maximum margin classier. Figure 1 illustrates our idea.
There are three classes and two features, as shown in the top left panel. We seek the line that
partitions the classes into two groups, that has the maximum margin. (The margin is the minimum
distance to the decision line among all of the data points.)

c(cid:13)2007 Robert Tibshirani and Trevor Hastie.

TIBSHIRANI AND HASTIE

The best line is shown in the top right panel, splitting class 1 from classes 2 and 3. We then
focus just on classes 2 and 3, and their maximum margin classier is shown in the bottom left. The
overall top-down classier is summarized by the binary tree shown in the bottom right panel.

We employ strategies like this for larger numbers of classes, producing a binary decision tree

with a maximum margin classier at each junction in the tree.

In Section 2 we give details of the margin tree classier. Section 3 shows the application of
the margin tree to a number of cancer microarray data sets. For construction of the tree, all of the
classiers in the margin tree use all of the features (genes). In Section 4 we discuss approaches to
feature selection. Finally in Section 5 we have some further comments and a discussion of related
work in the literature.

2. The Margin Tree Classier

Denote the gene expression proles by x j = (x1 j; x2 j; : : :xp j) for j = 1;2; : : :N samples falling into
one of K classes. The features (genes) are indexed by i = 1;2; : : : p.

Consider rst the case of K = 2 classes, C1 and C2. The class outcome is denoted by y j =
(cid:6)1. The maximum margin classier is dened by the constant b 0 and the weight vector b with
components (cid:229)

i = 1 that maximizes the gap between the classes, or the margin. Formally,

i b 2

(b 0;b ) = argmax(C)

where y j(b 0 +(cid:229)

ixi j) (cid:21) C 8 j:

i

The achieved margin M = 2 (cid:1)C. In the examples of this paper, p > N so that all classes are separable
and M > 0. We have some discussion of the non-separable case in Section 5.

Now suppose we have K > 2 classes. We consider three different strategies for constructing the
tree. These use different criteria for deciding on the best partition of the classes into two groups at
each juncture. Having settled on the partition, we use the maximum margin classier between the
two groups of classes, for future predictions.

Let M( j; k) be the maximum margin between classes j and k. Also, let G1; G2 be groups of
classes, and let M(G1; G2) denote the maximum margin between the groups. That is, M(G1; G2) is
the maximum margin between two hyper-classes: all classes in G1 and all classes in G2. Finally,
denote a partition by P = fG1; G2g.

Then we consider three approaches for splitting a node in the decision tree:

(a) Greedy: maximize M(G1; G2) over all partitions P.

(b) Single linkage: Find the partition P yielding the largest margin M0 so that min M( j1; j2) (cid:20) M0

for j1; j2 2 Gk; k = 1;2 and min M( j1; j2) (cid:21) M for j1 2 G1; j2 2 G2.

(c) Complete linkage: Find the partition P yielding the largest margin M0 so that max M( j1; j2) (cid:20)

M0 for j1; j2 2 Gk; k = 1;2 and max M( j1; j2) (cid:21) M0 for j1 2 G1; j2 2 G2.

The greedy method nds the partition that maximizes the resulting margin over all possible
partitions. Although this may seem prohibitive to compute for a large number of classes, we derive
an exact, reasonably fast algorithm for this approach (details below).

The second and third methods require some explanation. They are derived from the bottom up
2(cid:1) margin classiers for

(as opposed to top-down) clustering methods. Each one requires just the (cid:0)K

638

b
MARGIN TREES

2
2
222
2
2
2

2

2

3
3
3
3

3

3

3
33

3

1
1
1
1
1
11

1
1
1

2
x

8

6

4

2

0

2


4


6


2
2
222
2
2
2

2

2

3
3
3
3

3

3

3
33

3

1
1
1
1
1
11

1
1
1

6

4

2

0

2

4

6

8

6

4

2

0

2

4

6

8

x1

2
2
222
2
2
2

2

2

3
3
3
3

3

3

3
33

3

1
1
1
1
1
11

1
1
1

x1

Margin tree

0
4

.

5
3

.

0
3

.

5
2

.

0
2

.

1

t

i

h
g
e
H

6

4

2

0

2

4

6

8

x1

2

3

2
x

2
x

8

6

4

2

0

2


4


6


8

6

4

2

0

2


4


6


Figure 1: Simple illustration of a margin tree. There are three classes shown in the top left panel.
The largest margin is between class 1 and (2,3), with the optimal classier shown on the
top right. Then we separate class 2 from 3, in the bottom left. These top-down splits are
summarized in the margin tree in the bottom right.

639

TIBSHIRANI AND HASTIE

each pair of classes. Single linkage clustering successively merges groups based on the minimum
distance between any pair of items in each of the group. Complete linkage clustering does the same,
but using the maximum distance. Now having built a clustering tree bottom-up, we can interpret
each split in the tree in a top-down manner, and that is how criteria (b) and (c) above were derived.
In particular it is easy to see that the single and complete linkage problems are solved by single and
complete linkage agglomerative clustering, respectively, applied to the margin matrix M( j1; j2).
Note that we are applying single or complete linkage clustering to the classes of objects C j, while
one usually applies clustering to individual objects.

The greedy method focuses on the form of the nal classier, and tries to optimize that classi-
cation at each stage. Note that the greedy method cares only about the distance between classes in
the different partitions, and not about the distance between classes within the same partition. Both
the single linkage and complete linkage methods take into account both the between and within
partition distances. We will also see in the next section that the complete linkage method can be
viewed as an approximation to the greedy search.

Figure 2 shows a toy example that illustrates the difference between the greedy and complete
linkage algorithms. There are six classes with circular distributions. The greedy algorithm splits
off group 1,2, and 3 in succession, and then splits off 4,5,6 as a group. This is summarized in the
bottom left panel. The complete linkage algorithm in the bottom right panel instead groups 1,2 and
3 together and 4,5, and 6 together. The complete linkage tree is more balanced and hence may be
more useful biologically.

In the experiments in this paper we nd that:

(cid:15) All three methods produce about the same test set accuracy, and about the same as the all-pairs

maximum margin classier.

(cid:15) The complete linkage approach gives more balanced trees, that may be more interpretable that
those from the other two methods; the single linkage and greedy methods tend to produce long
stringy trees that usually split off one class at a time at each branch. The complete linkage
method is also considerably faster to compute than the greedy method.

Thus the complete linkage margin tree emerges as our method of choice. It requires computation of
(cid:0)K
2(cid:1) support vector classiers for each pair of classes for the complete linkage clustering and then
for the nal tree, one computation of a support vector classier for each node in the tree (at most K
and typically (cid:25) log2(K) classiers.)

2.1 An Exact Algorithm for the Greedy Criterion

A key fact is

M(G1; G2) (cid:20) minfM( j1; j2); j1 2 G1; j2 2 G2g:

(1)

That is, the margin between two groups of classes is less than or equal to the smallest margin
between any pair of classes, one chosen from each group.

Now suppose we seek a partition P with margin M. Rather than enumerate all possible partitions
(and their associated maximum margin classiers), we can speed up the computation by constructing
the complete linkage clustering tree, and collapsing all nodes at height M. We know that all classes
in any collapsed node must be on the same side of the decision plane, since each class has margin

640

MARGIN TREES

1

2

3

4

6

5

0.0

0.2

0.4

0.6

0.8

Greedy

Complete

1

2

3

t

i

h
g
e
H

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

5

1

2 3

5 4 6

4 6

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

0
3
0

.

0

5
2
0

.

0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

t

i

h
g
e
H

Figure 2: A toy example illustrating the difference between the greedy and complete linkage algo-
rithms. There are six classes with circular distributions (top panel). The greedy algorithm
splits off groups 1,2, and 3 in succession, and then splits off 4,5,6 as a group. This is sum-
marized in the bottom left panel. The complete linkage algorithm (bottom right panel)
instead groups 1,2 and 3 together, and 4,5, and 6 together. For example, the margin be-
tween classes 1 and 2 is 0.0296, while that between 3 and 4 is less: 0.0256. The height in
each plot is the margin corresponding to each join.

at least M with every other class in that node. Hence we need only consider partitions that keep the
collapsed nodes intact.

We summarize the algorithm below:

Exact computation of the best greedy split

1. Construct the complete linkage clustering tree based on the margin matrix M( j1; j2).

2. Starting with all classes at the top of the tree, nd the partition of each individual class versus
the rest, and also the partition that produces two classes in the complete linkage tree (that
is, make a horizontal cut in the tree to produce two classes). Let M0 be the largest margin
achieved amongst all of these competitors.

641

TIBSHIRANI AND HASTIE

3. Cut the complete linkage tree at height M0, and collapse all nodes at that height.

4. Consider all partitions of all classes that keep the collapsed nodes intact, and choose the one

that gives maximal margin M.

This procedure nds the partition of the classes that yields the maximum margin. We then apply
this procedure in a top-down recursive manner, until the entire margin tree is grown.

This algorithm is exact in that it nds the best split at each node in the top-down tree building
process. This is because the best greedy split must be among the candidates considered in step 4,
since as mentioned above, all classes in a collapsed node must be on the same side of the decision
plane. But it is not exact in a global sense, that is, it does not nd the best tree among all possible
trees.

Note that if approximation (1) is an equality, then the complete linkage tree is itself the greedy

margin classier solution. This follows because M = M0 in the above algorithm.

As an example, consider the problem in Figure 2. We cut the complete linkage tree to produce
two nodes (5,4,6) and (1,2,3). We compute the achieved margin for this split and also the margin for
partitions (1) vs. (2,3,4,5,6), (2) vs. (1,3,4,5,6) etc. We nd that the largest margin corresponds to
(1) vs. (2,3,4,5,6), and so this becomes the rst split in the greedy tree. We then repeat this process
on the daughter subtrees: in this case, just (2,3,4,5,6). Thus we consider (2) vs. (3,4,5,6) , (3) vs
(2,4,5,6) etc, as well as the complete linkage split (2,3) vs (4,5,6). The largest margin is achieved
by the latter, so me make that split and continue the process.

2.2 Example: 14 Cancer Microarray Data

As an example, we consider the microarray cancer data of Ramaswamy et al. (2001): there are
16,063 genes and 198 samples in 14 classes. The authors provide training and test sets of size 144
and 54 respectively.

The margin trees are shown in Figure 3. The length of each (non-terminal) arm corresponds
to the margin that is achieved by the classier at that split. The nal classiers yielded 18, 18
and 19 errors, respectively on the test set. By comparison, the all-pairs support-vector classier
yielded 20 errors and the nearest centroid classier had 35 errors. Nearest centroid classication
(e.g., Tibshirani et al., 2001) computes the standardized mean feature vector in each class, and then
assigns a test sample to the class with the closest centroid. Later we do a more comprehensive
comparison of all of these methods. We note that the greedy and single linkage margin tree are
stringy, with each partition separating off just one class in most cases. The complete linkage tree
is more balanced, producing some potentially useful subgroupings of the cancer classes.

In this example, full enumeration of the partitions at each node would have required computation
of 16,382 two class maximum margin classiers. The exact greedy algorithm required only 485 such
classiers. In general the cost savings can vary, depending on the height M0 of the initial cut in the
complete linkage tree.

Figure 4 displays the margins that were achieved by each method at their collection of splits.
We see that the complete method gives larger margins than the other methods. The largest margin
achieved is about 49,000, corresponding to the split between class CNS and Collerectal and so on..
This is larger than the margin between Leukemia and the rest at the top of the greedy tree. This
shows that the greediness of the exact algorithm can hurt its overall performance in nding large
margin splits.

642

MARGIN TREES

0

50000

100000

150000

200000

leuk

cns

lymp

uterus

vmeso

renal

pros

melanoma

collerectal

ovary

G
r
e
e
d
y

breast

lung

bladder

pancreas

0

50000

100000

150000

200000

Height

leuk

cns

lymp

uterus

vmeso

renal

pros

melanoma

collerectal

ovary

0

20000

40000

60000

80000

100000

120000

Height

lymp

leuk

cns

collerectal

vmeso

pros

melanoma

uterus

renal

ovary

bladder

pancreas

breast

lung

bladder

pancreas

breast

lung

Figure 3: Margin trees for the 14-tumor cancer data of Ramaswamy et al. (2001)

.

643

i

S
n
g
e

l


l
i

n
k
a
g
e

C
o
m
p
l
e
t
e

l
i

n
k
a
g
e

TIBSHIRANI AND HASTIE

0
0
0
0
5

0
0
0
0
3

0
0
0
0
1

Single

Complete Greedy

Figure 4: 14 tumor cancer data: margins achieved by each method over the collection of splits. The
number of points represented in each boxplot is the number of splits in the corresponding
tree.

SVM(All pairs)
MT(Greedy)
MT(Single)
MT(Complete)

SVM(All pairs) MT(Greedy) MT(Single) MT(Complete)
10
2
9
0

0
10
11
10

10
0
7
2

11
7
0
9

Table 1: Number of disagreements on the test set, for different margin tree-building methods.

Table 1 shows the number of times each classier disagreed on the test set. The number of
disagreements is quite large. However the methods got almost all of the same test cases correct
(over 90% overlap), and the disagreements occur almost entirely for test cases in which all methods
got the prediction wrong.

Figure 5 shows the test errors at each node of the complete linkage tree, for the 14 tumor data

set.

3. Application to Other Cancer Microarray Data Sets

We applied the methods described earlier to the seven microarray cancer data sets shown in Table
2. In each case we randomly sampled 2=3rds of the data to form a training set, and the balance
of the data became the test set. The sampling was done in a stratied way to retain balance of the
class sizes. This entire process was repeated 50 times, and the mean and standard errors of the
test set misclassication rates are shown in Table 3. The nearest centroid method is as described in

644

MARGIN TREES

1/54

0/42

1/12

p
m
y

l

k
u
e

l

s
n
c

3/38

5/31

0/7

3/22

2/16

o
s
e
m
v

s
o
r
p

l

a
t
c
e
r
e

l
l

o
c

a
m
o
n
a
e
m

l

2/14

3/8

1/6

g
n
u

l

t
s
a
e
r
b

r
e
d
d
a
b

l

s
a
e
r
c
n
a
p

1/9

0/7

s
u
r
e
u

t

l

a
n
e
r

y
r
a
v
o

4
0
+
e
8

4
0
+
e
6

4
0
+
e
4

4
0
+
e
2

0
0
+
e
0

t

i

h
g
e
H

Figure 5: Test errors for the 14 tumor data set using the complete linkage approach. Error rates at
each decision junction is shown: notice that the errors tend to increase farther down the
tree.

Tibshirani et al. (2001) and uses no shrinkage for feature selection: we discuss feature selection in
Section 4. We see that for problems involving more than 4 or 5 classes, the one-versus-one support
vector classier and the margin tree methods sometimes offer an advantage over nearest centroids.
The margin tree methods are all very similar to each other and the one-versus-one support vector
classier.

645

TIBSHIRANI AND HASTIE

Name
Brain
Lymphoma
Small round blue cell tumors
Stanford
9 tumors
11 tumors
14 tumors

# Classes
5
3
4
14
9
11
14

# Samples
22
62
63
261
60
174
198

Source
# Features
5597
Pomeroy et al. (2002)
4026 Alizadeh et al. (2000)
2308 Khan et al. (2001)
4718 Munagala et al. (2004)
Staunton et al. (2001)
5726
12,533
Su et al. (2001)
16063 Ramaswamy et al. (2001)

Table 2: Summary of data sets for comparative study

.

Data set
Brain
Lymphoma
SRBCT
Stanford
9 tumors
11 tumors
14 tumors

Nearest centroids
0.236(0.026)
0.010(0.006)
0.065(0.020)
0.075(0.006)
0.478(0.009)
0.139(0.006)
0.493(0.007)

SVM (OVO) MT(Single) MT(Complete) MT(Greedy)
0.221(0.017)
0.207(0.022)
0.000(0.000)
0.000(0.000)
0.011(0.011)
0.014(0.014)
0.072(0.005)
0.063(0.006)
0.545(0.014)
0.507(0.014)
0.110(0.005)
0.110(0.005)
0.345(0.006)
0.315(0.007)

0.229(0.018)
0.000(0.000)
0.014(0.014)
0.070(0.005)
0.526(0.013)
0.106(0.005)
0.318(0.007)

0.229(0.021)
0.000(0.000)
0.014(0.014)
0.079(0.007)
0.522(0.014)
0.106(0.005)
0.322(0.007)

Table 3: Mean test error rates (standard errors) over 50 simulations, from various cancer microarray
data sets. SVM (OVO) is the support vector machine, using the one-versus-one approach;
each pairwise classier uses a large value for the cost parameter, to yield the maximal
margin classier; MT are the margin tree methods, with different tree-building strategies.

4. Feature Selection

The classiers at each junction of the margin tree each use all of the features (genes). For in-
terpretability it would be clearly benecial to reduce the set of genes to a smaller set, if one can
improve, or at least not signicantly worsen, its accuracy. How one does this depends on the goal.
The investigator probably wants to know which genes have the largest contribution in each
classier. For this purpose, we rank each gene by the absolute value of its coefcient b
j. Then to
form a reduced classier, we simply set to zero the rst nk coefcients at split k in the margin tree.
We call this hard-thresholding.

How do we choose nk? It is not all clear that nk should be the same for each tree split. For
example we might be able to use fewer genes near the top of the tree, where the margins between
the classes is largest.

Our strategy is as follows. We compute reduced classiers at each tree split, for a range of
values of nk, and for each, the proportion of the full margin achieved by the classier. Then we use
a common value a
for the margin proportion throughout the tree. This strategy allows the classiers
at different parts of the tree to use different number of genes. In real applications, we use tenfold
cross-validation to estimate the best value for a

.

Figure 6 shows the result of applying hard thresholding to the 14-class cancer data. The plot
is varied. The average number of genes at each of

shows the test error as the margin proportion a

646

MARGIN TREES

r
o
r
r
e

5
3

0
3

5
2

0
2

10

50

500

5000

mean number of genes

Figure 6: 14 tumor data set: Test errors for reduced numbers of genes.

the 13 tree junctions is shown along the horizontal axis. We see that average number of genes can
be reduced from about 16;000 to about 2;000 without too much loss of accuracy. But beyond that,
the test error increases.

Figure 7 shows a more successful application of the feature selection procedure. The gure
shows the result for one training/test split of the 11 class data (12,533 genes) described earlier. With
no feature selection the margin tree (left panel) achieves 4=61 errors, the same as the one-versus one
support vector machine. Hard-thresholding (middle panel) also yields 4 errors, with an average of
just 167 genes per split. The margin proportion is shown at the top of the plot. The right panel shows
the number of genes used as a function of the height of the split in the tree, for margin proportion
0.6.

The feature selection procedure described above is simple and computationally fast. Note that
having identied a set of features to be removed, we simply set their coefcients b
i to zero. For
reasons of speed and interpretability, we do not recompute the maximum margin classier in the
subspace of the remaining features (we do however recompute the classier cutpoint, equal to mid-
point between the classes). How much do we lose in this approximation? For the top and bottom
splits in the tree of Figure 7, Figure 8 shows the margins achieved by the maximum margin classier
(black points) and the approximation (blue points) as the numbers of genes is reduced. The approx-
imation gives margins remarkably close to the optimal margin until the number of genes drop below
100. Also shown in the gure are the margins achieved by recursive feature elimination (RFE)
(Guyon et al., 2002). This is a full backward stepwise procedure, in which successive coefcients
are dropped and the optimal margin classier for the remaining features is recomputed. We see that
RFE offers only a small advantage, when the number of genes becomes quite small.

647

TIBSHIRANI AND HASTIE

0.2

0.5

0.8

1

0

.

7

5

.

6

0

.

6

5

.

5

0

.

5

5

.

4

0

.

4

y
e
n
d
K

i

s
a
e
r
c
n
a
P

r
o
r
r
e

l

a

t
c
e
r
o
o
C

l

s
u
g
a
h
p
o
s
e
o
r
t
s
a
G

s
e
n
e
g


f

o


r
e
b
m
u
N

0
0
5

0
0
4

0
0
3

0
0
2

0
0
1

0

e

t

a

t
s
o
r
P

r
e
v
L

i

0
0
0
0
6

0
0
0
0
4

0
0
0
0
2

t

i

h
g
e
H

s
u
o
m
a
u
q
S
g
n
u
L

/

o
n
e
d
A
g
n
u
L

/

y
r
a
v
O

t
s
a
e
r
B

t

r
e
e
r
u

/
r
e
d
d
a
B

l

50

200

1000

5000

20000

40000

60000

mean number of genes

Height in tree

Figure 7: Results for 11 tumor data set. The left panel shows the margin tree using complete link-
age; the test errors from hard-thresholding are shown in the middle, with the margin
proportion a
indicated along the top of the plot; for the tree using a = 0:6, the right panel
shows the resulting number of genes at each split in the tree, as a function of the height
of that split.

4.1 Results on Real Data Sets

Table 4 shows the results of applying the margin tree classier (complete linkage) with feature
selection, on the data sets described earlier. Tenfold cross-validation was used to choose the margin
fraction parameter a
, and both CV error and test set error are reported in the table. Also shown are
results for nearest shrunken centroids (Tibshirani et al., 2001), using cross-validation to choose the
shrinkage parameter. This method starts with centroids for each class, and then shrinks them towards
the overall centroid by soft-thresholding. We see that (a) hard thresholding generally improves upon
the error rate of the full margin tree; (b) margin trees outperform nearest shrunken centroids on the
whole, but not in every case. In some cases, the number of genes used has dropped substantially;
to get smaller number of genes one could look more closely at the cross-validation curve, to check
how quickly it was rising.

If two genes are correlated and both contribute too the classier, they might both remain in the
model, under the above scheme. One the other hand, if there is a set of many highly correlated
genes that contribute, their coefcients will be diluted and they might all be removed.

Hence it might be desirable to to select among the genes in a more aggressive fashion. There are
a number of approaches one might try here, for example the recursive feature elimination of Guyon
et al. (2002), mentioned above. One could also try the L1-norm support-vector machine (see, for
example, Zhu et al., 2003), but this is also quite slow to compute. Another approach would be to
apply the lasso (Tibshirani, 1997). All of these methods would be worth trying; however they also

648

MARGIN TREES

Top split in tree

Simple
Recomputed
RFE

0

2000

4000

6000

8000

10000

12000

Number of genes

Bottom split in tree

Simple
Recomputed
RFE

i

s
n
g
r
a
M

i

s
n
g
r
a
M

0
0
0
0
3

0
0
0
0
1

0

0
0
0
5
1

0
0
0
5

0

0

2000

4000

6000

8000

10000

12000

Number of genes

Figure 8: Results for the 11 tumor data set: margins achieved by the maximum margin classier
using simple hard thresholding without recomputing the weights (black points), with re-
computation (blue points) and recursive feature elimination (red points). Top panel refers
to the top split in the margin tree; bottom panel refers to the bottom split.

suffer from interpretability issues. In particular, the best classier with say 50 genes might have
only a few genes in common with the best classier with 100 genes. The hard thresholding method
described above does not suffer from this drawback. It gives a single ranked list of weights for all
genes, for the classier at each node of the tree.

649

TIBSHIRANI AND HASTIE

Nearest shrunken centroids

Margin tree with selection

CV errors
0.192(0.024)
0.022(0.002)
0.00(0.00)
0.064(0.005)
0.381(0.008)
0.125(0.004)
0.397(0.007)

Test errors
0.276(0.04)
0.005(0.005)
0.014(0.009)
0.080(0.012)
0.400(0.016)
0.156(0.009)
0.400(0.018)

# genes used
260(119.3)
3463.4(126.4)
44.7(6.9)
4420.1(254.6)
1163.4(527.2)
4857.8(1685.9)
3928.5(1392.8)

CV errors
0.264(0.011)
0.000(0.000)
0.004(0.002)
0.064(0.006)
0.503(0.019)
0.111(0.008)
0.327(0.01)

Test errors
0.176(0.02)
0.01(0.006)
0.010(0.007)
0.076(0.007)
0.476(0.022)
0.110(0.012)
0.311(0.015)

# genes/split
483.3(194.3)
408.5(145.0)
5.08(8.6)
1518(532.4)
2678.1(847.1)
2720.5(1523.9)
7563.8(1990.1)

Brain
Lymphoma
SRBCT
Stanford
9 tumors
11 tumors
14 tumors

Table 4: CV and test error rates for nearest shrunken centroids and margin trees with feature se-
lection by simple hard thresholding. The rightmost column reports the average number of
genes used at each split in the tree.

5. Discussion

The margin-tree method proposed here seems well suited to high-dimensional problems with more
than two classes. It has prediction accuracy competitive with multiclass support vector machines
and nearest centroid methods, and provides a hierarchical grouping of the classes.

All of the classiers considered here use a linear kernel, that is, they use the original input
features. The construction of margin tree could also be done using other kernels, using the support
vector machine framework. The greedy algorithm and linkage algorithms will work without change.
However in the p > N case considered in this paper, a linear SVM can separate the data, so the utility
of a non-linear kernel is not clear. And importantly, the ability to select features would be lost with
a non-linear SVM.

We have restricted attention to the case p > N in which the classes are separable by a hyperplane.
When p < N and the classes may not be separable, our approach can be modied to work in principle
but may not perform well in practice. The nodes of the clustering tree will be impure, that is contain
observations from more than one class. Hence a larger treeone with more leaves than there are
classesmight be needed to effectively classify the observations.

In addition to the papers on the multiclass support vector classier mentioned earlier, there is
other work related to our paper. The decision tree methods of Breiman et al. (1984) (CART) and
Quinlan (1993) use top-down splitting to form a binary tree, but use other criteria (different from the
margin) for splitting. With p (cid:29) N, splits on individual predictors can get unwieldy and exhibit high
variance. The use of linear combination splits is closer to our approach, but again it is not designed
for large numbers of predictors. It does not produce a partition of the classes but rather operates on
individual observations.

Closely related to CARTs linear combination splits is the FACT approach of Loh and Vanichse-
takul (1988) and the followup work of Kim and Loh (2001). These use Fishers linear discriminant
function to make multi-way splits of each node of the tree. While linear discriminants might per-
form similarly to the support vector classier, the latter has the maximum margin property which
we have exploited in this paper.

Probably the closest paper to our work is that of Vural and Dy (2004), who use a top-down
binary tree approach. They use K-means clustering of the class means to divide the points in two
groups at each node, before applying a support vector classier. Bennett and Blue (1997) investigate

650

MARGIN TREES

decision trees with support vector classiers at the node, but do not discuss adaptive construction of
the tree topology. Park and Hastie (2005) propose hierarchical classication methods using nearest
centroid classiers at each node. They use clustering methods to nd the topology of the tree, and
their paper has some ideas in common with this one. In fact, the mixture model used for each
merged node gives a decision boundary that is similar to the support vector classier. However the
maximum margin classier used here seems more natural and the overall performance of the margin
tree is better.

Acknowledgments

We would like the thank the referees for helpful comments that led to improvements in this
manuscript. Tibshirani was partially supported by National Science Foundation Grant DMS-9971405
and National Institutes of Health Contract N01-HV-28183. Trevor Hastie was partially supported
by grant DMS-0505676 from the National Science Foundation, and grant 2R01 CA 72028-07 from
the National Institutes of Health.

