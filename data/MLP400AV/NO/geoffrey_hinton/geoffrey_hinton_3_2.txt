Abstract

This paper describes two new methods for modelling the manifolds of digitised images

of handwritten digits. The models allow a priori information about the structure of

the manifolds to be combined with empirical data. Accurate modelling of the man-

ifolds allows digits to be discriminated using the relative probability densities under

the alternative models. One of the methods is grounded in principal components

analysis, the other in factor analysis. Both methods are based on locally linear,

low-dimensional approximations to the underlying data manifold. Links with other

methods that model the manifold are discussed.

I

Introduction

A standard discriminative approach to digit recognition is to train a classi(cid:12)er to

output one of the ten classes based on the input image. The classi(cid:12)er could, for

instance, be a multilayer feedforward neural network []. However, it is also possible

to discriminate by (cid:12)tting a separate probability density model to each class and then

picking the class of the model that assigns the highest density to a test image. This

relative density approach typically requires more computation during recognition, and

it can devote a lot of parameters to modelling aspects of the image that are irrelevant

for discrimination, but it has several advantages:

(cid:15) During training, each model need only consider training examples of its own

class, saving an order of magnitude in computation. If the models are correct,

this saving is achieved with no reduction in discriminative performance.

(cid:15) After training, it is possible to add a new class without retraining the previous

class models.

(cid:15) It is possible to (cid:12)t far more parameters before over(cid:12)tting occurs because the

input vectors contain much more information than the class label. Assuming

the same number of examples in each class, a class label only contains log 

bits so each example only provides : bits of constraint on the function that

maps inputs to class labels. However, it takes many more bits to specify the

input image, so each example provides far more constraint on the parameters

One way to see why, is to imagine a table look-up scheme in which each input vector is randomly

mapped to a di(cid:11)erent word of memory. To (cid:12)t a training set perfectly, we need as many words of

memory as training examples and each word only needs enough bits to specify the correct label.



of a density model. For the  (cid:2)  real-valued images we use, ;  training

examples are su(cid:14)cient to (cid:12)t about ;  parameters.

(cid:15) Aspects of the image that are irrelevant for discrimination between classes may

nevertheless be very relevant for detecting occasional bad images that do not fall

into any of the given classes. Relative density methods have a natural rejection

criterion when all the densities are low.

(cid:15) The density models we describe can be (cid:12)tted by methods like singular value de-

composition (SVD) and expectation-maximisation (EM) that are considerably

more e(cid:14)cient than gradient descent.

We are not claiming that the relative density approach is necessarily better than the

discriminative approach, just that it is an alternative worth considering.

Certain discriminative methods can be seen in terms of relative densities. For in-

stance, kernel density estimation [], [] is a popular non-parametric modelling tech-

nique. For this, the probability density for a particular digit is the weighted sum of

a collection of kernel functions. The functions all have the same shape, but each is

centred on one of the patterns in that class in the training set. Each kernel function

typically integrates to , and the weights in the sum are usually =M , where M is the

number of the patterns in the training set, so the overall kernel density estimate is

correctly normalised. Having built ten such models, one for each digit class, the class

to which a new image belongs is inferred by evaluating the density under each of the

models at the location of the new image, and reporting the one that is the highest. If

the kernel functions are radially symmetric, monotonically decreasing, and have un-

bounded extent (e.g a Gaussian), then relative density estimation becomes identical

to nearest neighbour classi(cid:12)cation as the width parameter of the kernel goes to zero.



This is because the model that contains the kernel function closest to the data will,

in the limit, have in(cid:12)nitely higher density than any other model, even if other models

have many kernel functions that are almost as close.

Kernel density estimators are convenient models in the case that there is ample data,

ample computational time for inference (it is e(cid:11)ectively a memory-based technique),

and in which there is little a priori information about the nature of the data. The

latter two are not true for images of handwritten digits. We sought to build better

models for such images on the basis of such information. Elastically deformable tem-

plates [], [] are one example, and have been shown to model non-normalised images

of characters well []. Unfortunately, they are also computationally too expensive

for normal use. We therefore turned to Gaussian blended linear models, which are

computationally much cheaper but are also appropriate for such images.

Simard et al [] pointed out the locally low-dimensional linear structure underlying

these images. Take the -dimensional space of all (normalised)  (cid:2)  images and

consider the subset of images of any particular digit, say the digit . Since small

changes to the image of a  preserve its identity, this subset will have some properties

of a surface { it will mostly be continuous and di(cid:11)erentiable. In particular, a(cid:14)ne

transformations (translations, rotations, scalings, and shearing) as well as manipu-

lations to the thickness of the strokes of a digit preserve its identity. Considering

the e(cid:11)ect on an image of small (ie sub-pixel) transformations like these suggests that

the surface is locally at least  dimensional and probably somewhat more. Di(cid:11)erent

styles of  will likely generate separated continuous patches. Simard et al [] used a

nearest neighbour method (which, as pointed out, is equivalent to a limiting case of

a relative density method) in the space of these  dimensional planes, where the dis-

tance between two points in the space is the closest distance between the underlying



planes in image space. Using this as the distance rather than the simple Euclidean

distance between two images substantially improved recognition performance.

Unfortunately, Simards technique is founded on a nearest neighbour method, and

therefore recognition is again computationally expensive. We can build much cheaper

models by combining information from many examples about the local structure of

the manifold in image space representing a digit. Combining information in this way

should have the added advantage of averaging out some of the noise in the estimate

of its local structure that arises from the gradient operators that Simard used to

extract the  dimensional subspace. We are also not limited to only the  a priori

dimensions listed above, but learn the local structure from the training examples,

which may contain other invariances. Although the manifold seems to be locally

linear and low dimensional, there is no reason for the manifold to be globally linear.

Di(cid:11)erent styles of digits, and even a(cid:14)ne transformations that are not con(cid:12)ned to a

sub-pixel regime, will lead to di(cid:11)erent local linear patches. We were therefore forced

to mix together numbers of linear models for images of each digit, ie to use a blended

linear approximation to the surface ((cid:12)gure ) [	], [], [], [], []. The mixture

is (cid:12)t using either an expectation-maximization (EM) based algorithm [] or the k-

means algorithm, which is actually a limiting case of EM. The expectation (E) phase

involves assigning to the linear models responsibilities for the training examples; the

maximization (M) phase involves re-estimating the parameters of the linear models

in the light of this assignment.

This distance is not a metric since it does not satisfy the triangle inequality.
Signi(cid:12)cant performance improvements can be achieved by using various speedup mechanisms,

for example []



Figure : Illustration of how a highly non-linear one-dimensional surface (left panel)

is captured by  locally linear models (right panel)

A convenient framework to describe our models comes from the version of neural

nets called autoencoders. An autoencoder is a feedforward neural network with a

single hidden layer that attempts to reconstruct its input activities at its output.

Hinton & Zemel [] and Zemel [] show how to understand the relationship between

statistical modelling and autoencoders. The code for an example is given by the

combination of the activations of the hidden units and the output error, which is the

information necessary to reconstruct the output given these hidden unit activations

and the weights between the hidden units and the output units. The cost of this code

is counted in bits, and is based on empirical prior distributions for the activations

of the hidden units and the reconstruction errors. The main link between modelling

and autoencoders is that this cost function can be considered as the negative log

probability of the data under a particular generative model { so maximum likelihood

model (cid:12)tting and minimum cost are equivalent. We give cost functions below which

include or exclude various elements of the code cost.

Two well established linear models are principal components analysis (PCA) and fac-

tor analysis (FA). Performing PCA requires nothing more than singular value decom-



position of the covariance matrix of the examples. Performing FA is computationally

more challenging. However, FA o(cid:11)ers a sounder statistical model of examples, and

one might expect it to be more pro(cid:12)cient. Section II describes principal components

analysis; section III describes factor analysis; section IV shows how to incorporate

some of the tangent information that Simard et al [] use to such good e(cid:11)ect; and

section V shows how the models perform on a large-scale digit recognition task.

II Mixtures of Principal Component Analysers

The r principal components of N examples xi = fx

i
; x

i
i
ng of n-dimensional data
; : : : x

(assumed, without loss of generality, to have  mean) are the r orthogonal directions

in n-space which capture the greatest variation in the examples. Put another way, if

the examples are projected onto r-dimensional subspaces and the resulting variance of

the projected examples is measured, then the principal components de(cid:12)ne a subspace

such that this captured variance is the highest. Alternatively, assuming that the

examples come from a multivariate Gaussian distribution, the information conveyed

by the magnitude of the projections onto these r directions is the greatest. These

properties, together with the computational simplicity of performing PCA (involving

no more than (cid:12)nding the top r eigenvectors of the covariance matrix of the examples)

make it an obvious candidate for the linear model in the mixture. The r principal

components de(cid:12)ne the local surface assumed for the manifold. In the context of an

autoencoder, the projections of the input along the r directions are the activities of

the hidden units hi = fh

i
 ; h

i
 ; ::h

i
rg for input pattern i. This can be written as:

hi = Rxi



()

As discussed above, the activities of the hidden units are part of the code for an

example. The activities of the output units are generated by a linear combination of

the hidden units:

yi = Ghi

()

The other part of the code is the di(cid:11)erence between the image itself and these output

activities. The resulting squared reconstruction error (E

i = kxi

(cid:0) yi

) is a measure

k

of how well the model (cid:12)ts the image { the smaller it is, the better the image was

captured.

As mentioned above, either an EM or a k-means procedure is used to assign the

N examples among m di(cid:11)erent PCA models (we call these sub-models) for each

digit. During the E step, responsibility for each pattern is assigned amongst the

sub-models; during the M-step PCA is performed, altering the parameters of a sub-

model appropriately to minimise the reconstruction cost of the data for which it is

responsible. In the soft , EM version, the responsibility of sub-model a for example

i is calculated as q

i
a = e

(cid:0)E i

a=(cid:27)

=(Pb e

(cid:0)E i

b=(cid:27)

) where E

i
b is the squared reconstruction

error and (cid:27)

 acts like a temperature parameter. In the hard , k-means version (which

can be viewed as the limiting case as (cid:27)



! ), example i is allocated to the sub-model

a for which E

i
a is smallest. Note that the M step is more complicated and powerful

than just taking the means of those examples for which responsibility is taken.

Formally, the k-means version of the algorithm is:

. Choose initial assignments among the sub-models for each example in the

training set (typically at random, or using samples from the initial data);

Strictly speaking there is a third component, the model-cost, which encodes the cost of specifying

the weights in each model []. We assume that this cost is the same for all models.



. Perform PCA separately for each sub-model;

. Reassign patterns to the sub-model that reconstructs them the best;

. Stop if no patterns have changed sub-model, otherwise return to step .

For the soft version, in step , the examples are weighted for the PCA by the re-

sponsibilities, and convergence is assessed by examining the change in the overall

log-likelihood (which is equivalent to negative code-cost using Shannon optimal cod-

ing) of the data at each iteration. This log-likelihood is based on a model for the

image under which the (incorrectly normalised) log probability of image i is []:

(cid:0)


(cid:27)



m

Xa=

i
aE

i
a (cid:0)

q

m

Xa=

i
a log q

i
a

q

()

Minus this quantity can be considered as a cost function for learning the assign-

ments of responsibilities and the principal components. The value of (cid:27)

 is somewhat

arbitrary.

Altogether, either procedure generates a set of local linear models for each digit.

Given a test pattern we evaluate the reconstruction errors against all the models for

all the digits. We use a hard method for classi(cid:12)cation { determining the identity of

the pattern only by the model which reconstructs it best. The absolute quality of the

best reconstruction and the relative qualities of slightly sub-optimal reconstructions

are available to reject ambiguous cases.

Somewhat similar methods have been used for modelling images of lips for lip reading

[], cartoon-like drawings [], digit and character recognition [], [] and data

compression [].



III Mixtures of Factor Analysers

Unfortunately, as noted above, PCA does not provide a correct statistical model for

the images because it is not properly normalisable. Components of the image that

lie in the directions of the principal components generate no reconstruction error,

and therefore can be added at will without changing the cost. Poggio & Sung []

suggested using as a component of the cost a quantity they called the normalised

Mahalanobis distance in the subspace of the principal components. This is:

C (cid:0)



jh

i
asj

(cid:21)as

(cid:0)




r

Xs=

ln(cid:21)as

where C is a constant, h

i
as is the activity of hidden unit s for example i of sub-

model a and (cid:21)as is the eigenvalue associated with that component. This amounts to

employing a zero mean hyper-elliptical Gaussian prior aligned in the direction of the

principal components. This prior is used to code the activation of the hidden units

in the autoencoder. Choosing the relative weighting in the overall cost function for

the squared reconstruction error E

i
a and the coding cost is somewhat arbitrary.

Factor analysis (FA) is a di(cid:11)erent way of analysing the covariance matrix of the

inputs (see [], for an excellent introduction) which starts from a proper probabilistic

model, and correctly blends the reconstruction cost and a term playing a similar role

to this normalised Mahalanobis distance. FA emerges for the same linear autoencoder

network if the cost of coding the hidden units is taken into account, using as a prior

a speci(cid:12)ed multivariate Gaussian (often just the identity matrix and traditionally

known as the prior for the factor loadings), and if the reconstruction errors are coded

according to an elliptical multivariate Gaussian whose axes are aligned with the input

dimensions. In terms of the notation introduced above, the standard factor analysis



model is written as []:

yi = Ghi + (cid:24)

i

()

hi

(cid:24) N (; (cid:8)); (cid:24)

i

(cid:24) N (; (cid:9))

The hidden-output weights, G, are the factor loadings and the activities of the hidden

units, hi, are the factors (the prior over which is Gaussian). A key assumption of the

FA model is that the observed variables are conditionally independent of each other,

given the factors. This is equivalent to noting that the individual components of the

residuals, (cid:24)

i
j, are also independent of each other, or that (cid:9) is a diagonal matrix with

variances f(cid:27)


 ; : : : ; (cid:27)



ng along the diagonal. It is common to take (cid:8) to be the identity

matrix. Model () implies that the covariance of the observed variables is given by:

C(G; (cid:9)) = GG

T + (cid:9)

()

Under the model, the sample covariance matrix (S) follows a Wishart distribution

[	] about C and Everitt [] introduces the function:

F (S; C(G; (cid:9))) = lnjC j + trace(SC

(cid:0)) (cid:0) lnjSj (cid:0) n

()

which, up to some constant factors, is the likelihood. Maximum likelihood FA (cid:12)ts

the parameters of the model G and (cid:9) by maximising equation (). Unfortunately,

there is no technique as computationally cheap as singular value decomposition for

determining the factors from a collection of images.

Consider the example in (cid:12)gure  in which inputs A and B are perfectly correlated

but have low variance and input C is independent but has much higher variance.

Consider what happens if we allow only one hidden unit. The principal component

will align perfectly with C and be orthogonal to A and B. The factor, however, will

align perfectly with A and B and will be orthogonal to C. The di(cid:11)erence between the



two methods is easy to understand in coding terms. PCA is equivalent to minimizing

the description length of the data if we make the following simplifying assumptions:

. Ignore the cost of communicating the model (i.e. the directions of the principal

components)

. Ignore the cost of communicating the projections of each data point onto the

principal components (i.e. the cost of conveying the activities of the hidden

units).

. Use a Gaussian distribution with the same variance on each dimension as an

agreed prior for communicating the residual errors when each dimension of the

data point is reconstructed from the projections onto the principal components.

Assumption  makes it much cheaper to communicate input C by (cid:12)rst copying it

to the principal component. But this is only because the cost of communicating

this component is ignored. Factor analysis has a somewhat more realistic coding

interpretation. It still ignores the cost of communicating the factor loadings, but it

takes into account the cost of communicating the projections onto each factor. Given

a data point, there is a multivariate posterior probability distribution across the factor

values and this distribution is typically not spherical. The cost of communicating the

factor values is the Kullback-Leibler divergence between the posterior distribution

and the spherical prior distribution for the factor values. Nothing is gained by having

a factor that is just a copy of C because communicating this factor value is just as

expensive as communicating the full value of C as a residual error.

Assumption  is another important di(cid:11)erence between the two methods. Factor

analysis allows di(cid:11)erent variances to be used for coding the residual errors on di(cid:11)erent



input dimensions. If the residuals are coded using a (cid:12)xed variance prior the average

information required to convey a residual is linear in its variance. If, however, the

variance of the prior is adapted to match the variance of the residual the information

is proportional to the log of the variance. This dramatically reduces the cost of

high variance residuals as compared with low variance ones and can make it better

to decrease two low variance residuals by a little rather than decreasing one high

variance residual by a lot.

Figure  illustrates how the use of di(cid:11)erent noise models for di(cid:11)erent input variables

can allow factor analysis to extract a more sensible model than PCA. The task in

(cid:12)gure  is to infer a measured length from  noisy measurements. Clearly this is a

situation where we wish to extract correlations amongst the inputs. In the example,

three dimensional data were generated according to the rule shown, and the results of

performing PCA and FA are illustrated by the projection of the generative (hidden-

output) weight vector (solid line) onto the x (cid:0) y plane.

(We have shown the -

dimensional projections, for clarity. Because of the symmetry in the way the data

was generated, the x (cid:0) z projection is similar, while the y (cid:0) z projection does not

illustrate the point being made here.) In the low noise case, PCA and FA extract

similar models, the generative weights show that equal attention is paid to both

the x and y dimensions. However, in the high noise case, PCA must have a large

weight to generate the large variance along the x dimension. On the other hand, FA

correctly has recognized that all output dimensions have identical dependencies on

the hidden variable, s, and so sets the generational weights accordingly. Of course,

the (cid:27)x component in (cid:9) (equation ()) is in(cid:13)ated to account for the large variance

in this input. This example illustrates how FA can model covariance amongst input

dimensions separately from variance whereas PCA cannot.



H

A

B

C

PCA
FA
data

(a)

C

(b)

B
=
A

Figure : (a) Shows three inputs and a single hidden unit. Inputs A and B have low

variance but are perfectly correlated. Input C has high variance but is independent of

A and B. The hidden unit behaves quite di(cid:11)erently in principal component analysis

and factor analysis. (b) Scatterplot of the data with the vector of incoming weights

of the hidden unit for PCA (solid line) and FA (dotted line).



PCA

FA

Y

Y

4

3

2

1

0

-1

-2

-3

-4

4

3

2

1

0

-1

-2

-3

-4

-4

-3

-2

-1

0
X

1

2

3

4

-4

-3

-2

-1

0
X

1

2

3

4

Y

Y

4

3

2

1

0

-1

-2

-3

-4

4

3

2

1

0

-1

-2

-3

-4

(cid:27)x = :

(cid:27)x = :

-4

-3

-2

-1

0
X

1

2

3

4

-4

-3

-2

-1

0
X

1

2

3

4

Figure : Noisy observations of a signal s, x = s + nx, y = s + ny, z = s + nz

where s, nx, ny and nz are normal random variables with zero mean. The standard

deviation of s and ny nz are held constant at : and : respectively on all three

panels, while that of nx is as shown. The projection of  training examples onto

the x (cid:0) y plane are shown by the dots, while the solid lines are the projections of

hidden-output weight vectors onto this plane. See text for further explanation.

For modelling digits, FA should be immune to the fact that di(cid:11)erent dimensions might

have di(cid:11)erent intrinsic amounts of noise and look for shared structure in digits. The

second di(cid:11)erence is that PCA is rotationally symmetric whereas FA is not. For FA,

the particular dimensions used to describe the image are special in the sense that the

noise corrupting them is taken to be mutually independent. This restriction seems



reasonable for images, since the axes de(cid:12)ned by the input pixels are indeed privileged.

Following analysis by Neal and Dayan [] on the relationship between factor anal-

ysis, PCA, autoencoders and the Helmholtz machine [], [], we actually used an

autoencoder to implement factor analysis. The linearity of the model implies that

the posterior distribution of the factors hi given the input xi is Gaussian:

hi

(cid:24) N (Rxi

; (cid:6))

()

As Rubin & Thayer [] show in their discussion of the use of EM for FA, the correct

values of R and (cid:6) are determined by G and (cid:9) as:

R = (G

T (cid:9)(cid:0)

G + I)(cid:0)

T (cid:9)(cid:0)

G

(cid:6)(cid:0) = I + G

T

(cid:0)



G

()

Following the Helmholtz machine, we call R and (cid:6) parameters of the recognition

model, since they are responsible for producing the bottom-up receptive (cid:12)elds of the

hidden units. Studying the form of these recognition parameters can give insight into

the elements of the images that the factors code.

Preliminary experiments with this model suggested that it was prone to over(cid:12)tting

in a very particular manner. Take a pixel on the outskirts of the  (cid:2)  grid. It can

easily occur that for some digit the activity of this pixel is always  in the training set

for some sub-model. The factor analysis model might correctly decide that this pixel

shares nothing in common with the other pixels and furthermore that its intrinsic

noise level is . If, in the test

set, by some quirk of the noise, there is activity in

this pixel, then the likelihood of the image under this sub-model will be . This is

unreasonable, since the pixel is only conveying noise. One way to regularise learning

is to impose a minimum allowable intrinsic variance (this is a conventional way of



regularising mixtures of Gaussian models); another is to add diagonal terms to the

covariance matrix of the examples so it is as if they all su(cid:11)er from extra independent

noise. These regularisation methods were roughly equally e(cid:14)cacious, and we adopt

the latter for the empirical studies below.

The full non-linear model uses a mixture of local factor analysers in the same way

that the non-linear PCA model used a mixture of principal component analysers.

The same EM-based method can be used to (cid:12)t the combined model, with the E-

phase assigning responsibilities for images to the factor analysers in a hard or soft

manner, and the M-phase adapting the generative model within in analyser according

to the new covariance matrix of the data for which it takes responsibility. Because

factor analysis is a genuine generative model we avoid the arbitrary choice of (cid:27)

 that

is required to apply EM to a mixture of PCA models.

IV Tangent Information

Nearest neighbour methods o(cid:11)er simple, non-parametric, ways of discriminating be-

tween the digits. However, the metric that is used to judge proximity can make a

substantial di(cid:11)erence to the quality of the resulting inference. There are discrimi-

native and maximum likelihood ways to look at this issue. For instance, Hastie and

Tibshirani [] choose a metric for the nearest neighbours at a point based on infor-

mation from local linear discriminant analysis { emphasizing directions in which the

images from the di(cid:11)erent classes di(cid:11)er and downplaying directions in which they are

similar.

On the other hand, Simard et al [], in the method described in the introduction,



used an approach that owes more to modelling the local structure of the classes.

Their approach is based on prior information, known to be valid for the particular

task of digit modelling, that certain local manipulations of an image preserve the

identity of the digit that is described. E(cid:11)ectively, each point is replaced by a local

low dimensional linear manifold, deformations in the direction of which incur no cost.

Equivalently, each image is modelled as a local linear surface. Note that these local

surfaces are chosen to model the images of each digit as best as possible, and not to

support the best possible discrimination between them. This local low-dimensional

and linear behaviour is what motivated our linear models. Schwenk and Milgram

[], [] take a slightly di(cid:11)erent approach and compile down all knowledge about a

character into a single prototype trained to directly minimize the distance between

the prototype and the tangent planes around each of the training examples.

Figure  illustrates the idea. Imagine that the four points - portray in image space

di(cid:11)erent examples of the same digit, subject to some smooth transformation. As

in tangent distance, one could represent this curve using the points and their local

tangents (thick lines). However one might do better splitting it into three local linear

models rather than four { model a (just a line in this simple case) averages the upper

part of the curve more e(cid:11)ectively than the combination of the two tangents at  and

. Given just the points, one might also construct model b for  and , which

would be unfortunate. Incorporating information about the tangents as well would

encourage the separation of these segments. Care should be taken in generalising this

picture to high dimensional spaces.

For both PCA and FA, the output is linearly related to the input, ie yi = Axi with

A = RG. In order for the models to be tolerant to distortions along tangent direction

 j indexes the direction, for example, horizontal and vertical translations, scalings and shears



a

1

2

3

4

b

Figure : Didactic example of tangent distance and local linear models. See text for

details.

ti;j requires that both xi + (cid:22)jti;j and xi

(cid:0) (cid:22)jti;j be reconstructed well, where (cid:22)j is

a parameter that indicates how far along the tangent vector ti;j good reconstruction

should be enforced. The overall reconstruction error is then proportional to

i

(cid:24) jxi

(cid:0) Axi

j

E

 +Xj



j jti;j

(cid:22)

(cid:0) Ati;j


j

(	)

and so the model has to reconstruct the tangent vectors as well as the underlying

images. The relative importance of the latter with respect to the former is controlled

by (cid:22)



j . A similar expression governs the code cost for the hidden units in FA.

An alternative way of viewing this prior is in terms of adding additional examples

to the training set that are generated by these manipulations. Each example would

be replaced by a Gaussian cloud of (cid:12)ctitious examples. Adding these examples is

straightforward using methods such as PCA and FA which are based on just the

covariance matrix of the inputs { it amounts to adding into the covariance matrix the

a priori tangent vectors, weighted by an amount, (cid:22)j, that trades o(cid:11) the importance

of the underlying pattern and the importance of the invariances. The one di(cid:11)erence

from just adding extra examples to the dataset is that we set the responsibilities of

each model for the tangent vectors according to how well the model reconstructed just

the parent image, since the intention is to force the local linear models to capture the



invariances themselves, using the tangents to shape the local structure within a sub-

model. If the database of training examples had just been expanded, this constraint

would not have been applied.

There are two di(cid:11)erences between this use of the tangent vectors and that in Simard

et al []. One is that for us, the e(cid:11)ect of these tangents has a limited spatial ex-

tent, whereas for them, the linear manifold about each image extends to in(cid:12)nity. In

practice, in high dimensional image spaces, it is unlikely that images will have very

large projections within the tangent space and small projections o(cid:11) it. The second

di(cid:11)erence is that Simard et al consider tangent manipulations to the test image as

well as the training images { this requires (cid:12)nding the closest approach between the

linear manifold about the test image and the linear manifolds about all the training

images. It would be straightforward to do this during recognition for both PCA and

FA; however doing it during learning is computationally more tricky [].

Simard et als metric would be irrelevant in the limit of very large numbers of training

images, since the database itself would contain all the transformations that actually

preserve digit identity. In the same limit, the local linear PCA and FA methods would

also not bene(cid:12)t from the tangents.

V Experimental Results

We have evaluated the e(cid:14)cacy of PCA and FA based density models at classifying

images of handwritten digits. For both PCA and FA models, with and without tan-

gents, we proceed as follows: During the training phase, only images of one class (say

images of s) are presented to the mixture model. The learning algorithm outlined



in section II is executed and builds a mixture of linear sub-models for each class. In

order to save time, we initialize the search for FA models from the PCA models. For

PCA models the coding cost is just the reconstruction cost, while for the FA models,

one form of the complete code cost for example i is obtained as the sum of coding

the reconstruction error and the code cost of the factors. Using Gaussian models for

both, this can be expressed as:

i =

E

jI j
j(cid:6)j

+ trace((cid:6)) + (Rxi)T

Rxi# +



 "log
Xj=


(cid:27)

n



j h(x

i
j (cid:0) gj Rx

i
j) + g

T

j (cid:6)gji +




log (cid:27)



j + C

()

where gj is the j

th row of G. A test image x is presented to all  density models and

estimates of the class conditional probabilities, P (xjk); k =  : : :  are obtained. If

all incorrect classi(cid:12)cations are equally costly and if the prior belief is that all classes

are equally likely then the Bayes decision rule stipulates that we should assign x to

the class k

(cid:3) where:

(cid:3) = argmaxkP (xjk)

k

We used images from the CEDAR CDROM  database of Cities, States, ZIP Codes,

Digits, and Alphabetic Characters []. The br training set of binary segmented dig-

its was subdivided into two sets of size , and , respectively. The former

subset was used to train the density models and the latter subset was used as a cross-

validation set to allow us to choose various parameters such as the number of local

linear sub-models to use in the mixtures and the number of principal components

(factors) in the PCA (FA) models. When building models that use tangent informa-

tion we are also free to specify the relative weightings ((cid:22)j in equation (	)). In reality,



we did not perform an exhaustive search for optimal values for all these parameters,

but simply chose values that did reasonably well on the cross-validation images. For

the results reported here, we allowed up to  principal components (or factors) in

each sub-model. There were also  sub-models in each mixture.

The CEDAR database includes two designated test sets. The goodbs ( images)

set is a subset of the bs

( images) set containing only well segmented digits.

After picking all the parameter values, we used all ;  images to train a (cid:12)nal

version of the models which was used to evaluate performance on the test sets.

The binary images in the data set are of varying sizes, so we (cid:12)rst scaled them to

lie on an  (cid:2)  pixel grid and then smoothed with a Gaussian (cid:12)lter with a standard

deviation of half a pixel.

Figure  illustrates reconstructions of two very di(cid:11)erent styles of s. Sub-models

which have specialized for one style reconstruct poorly images of the other style.

Examples of the PCA and FA models weights are shown in (cid:12)gures  and  respec-

tively. In the PCA models, the recognition and generative weight matrices are simply

transposes of each other while for the FA models they di(cid:11)er.

The performance of the di(cid:11)erent methods are presented in Table . There are no sig-

ni(cid:12)cant di(cid:11)erences between the performances of the di(cid:11)erent methods at the p < :

level on the bs

test set when compared pairwise using a two-tailed McNemars

test

[].

In comparing the columns of table , it is important to note that the

goodbs is a carefully chosen subset of the bs test data when poorly segmented dig-

This was usually su(cid:14)cient to explain at least 	% of the training set variance assigned to that

sub-model. Principal components that explained in excess of 	% were discarded to avoid over(cid:12)tting.
This follows because the weight vectors are mutually orthogonal and so the the generative model

inverse is simply its transpose.



its were manually removed []. The training data was also manually screened. It

is therefore reasonable to conclude by comparing the validation and goodbs perfor-

mances that the models did not over(cid:12)t.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

Figure : Operation of the mixture of PCA models on two di(cid:11)erent styles of s. The

 (cid:2)  image is shown at the bottom. The (cid:12)rst two rows show the reconstructions

from each of the  sub-models in the density estimator trained on images of s.

Attached to each sub-models reconstruction is its reconstruction cost relative to the

best sub-model (which has cost of zero)

.

As a comparison, other state of the art methods obtain about % error rates [] on

the original data which had a mean size of around  (cid:2)  pixels. Thus the images

we used had areas about  times smaller and so could well have lost information.

As a rough guide k-nearest neighbour has an error rate of :% on the bs test set.

This is signi(cid:12)cantly worse (p < :) than the performance obtained with the PCA

with tangents method or the FA methods. The k-nearest neighbour method requires

k =  chosen on the basis of the validation set



Figure : Weights in the  sub-models of the PCA density model for the digit .

On the left are the means. The upper row immediately to the right of the mean

shows the input-hidden weights (recognition weights), while the lower row gives the

hidden-output weights (generative weights).

of the order of N = , n dimensional (in this case n = ) dot products to classify

an image in one of the last  columns in table . Our local models (PCA or FA)

require of the order  (cid:2) m (cid:2) r n-dimensional dot products for each density model.

Since there are  digits and we used m = r = , the density models require of

the  (cid:2)  (cid:2)  (cid:2)  =  or about a factor of (cid:12)ve times less computation than

a straightforward implementation of the k-nearest neighbour method. The number

of dot products for the tangent distance method depends on the number of tangent

directions []. If  tangent directions are used on these data then of the order of 	(cid:2)N

dot products are needed, or about a factor of  times the local model version.



Figure : Weights in the  sub-models of the FA density model for the digit . The

layout is the same as in (cid:12)gure .

VI Discussion

We have constructed two di(cid:11)erent sorts of locally linear models of images of digits, one

based on principal components analysis and one on factor analysis, and have shown

that they can both perform well at digit discrimination. These models illustrate three

major points. First, linear models with just a few dimensions can be used to good

e(cid:11)ect for representing the local structure of the high dimensional complex manifolds

of the pixel images of digits. Second, allowing multiple components in the mixtures

can be very bene(cid:12)cial. In general, this could be both for capturing grossly di(cid:11)erent

styles of the digits (such as s with and without loops in their tails) and for the e(cid:11)ects

of a(cid:14)ne transformations of the digits by more than about a single pixel, which have



Validation

Set

.	

.

.

.

goodbs

test set

bs

test set

.

.

.

.

.	

.

.

.

PCA

FA

PCA + Tangents

FA + Tangents

Table : Percentage of images incorrectly classi(cid:12)ed by each of the methods. The

validation results were obtained when the density models were trained using the

;  training examples. The results in the last two columns were obtained with

models trained on all ;  training images.

highly non-linear e(cid:11)ects on the observed images.	 Third, a priori information about

the local structure of the manifolds that comes from knowledge about invariances

of digit identities over certain transformations [] is very easy to incorporate into

these linear models. Note that FA is just a particular way of limiting the number of

parameters that de(cid:12)ne the covariance matrix used to model data.

It is clear that if the manifold of the images is mostly di(cid:11)erentiable, then local lin-

ear models will do well at representing them. Using the EM scheme for iterative

competitive clustering (cid:12)nds the sets of images that respect the local structure and

(cid:12)nds the natural separation points between the distinct parts. Nearest neighbour

Since we did not have enough data to (cid:12)t a very large number of local models for each digit, and

the digits were reasonably well normalised, the di(cid:11)erent local models were mostly due to di(cid:11)erent

styles.



schemes are the logical limit of such schemes, and should work best if one has so

many examples that the local structure of the manifold can be inferred by (cid:12)nding the

nearest point or points and (cid:12)tting a linear surface to them. There are two potential

advantages to the EM scheme. First, it is using information from somewhat distant

images to determine the local directions in the manifold, allowing it to average away

the substantial noise that corrupts the images. Of course, if too much averaging is

done, then the directions could be systematically biased. Second, at recognition time,

rather than having to search the entire training set to (cid:12)nd the patterns closest to the

new input, knowledge about these patterns has been pre-compiled into the limited

number of centres and the limited number of principle component or factor directions

associated with them. Recognition can therefore be quite fast.

In this study, we did not (cid:12)nd that much di(cid:11)erence between our two sorts of models

for the covariance structure within a linear patch. One might have expected factor

analysis to have had better performance, since its prior model of the image genera-

tion process is more reasonable. For FA, given the factors, any discrepancies between

the model and the image are independent from one pixel to the next. PCA uses a

spherical Gaussian in the directions not covered by the retained components, and this

can amount to a complicated distribution over the pixels themselves. Also FA explic-

itly models covariance structure, whereas PCA models both variance and covariance.

This advantage may have been nulli(cid:12)ed by our normalisation and regularisation pro-

cedures. On the other hand, it is computationally much cheaper to perform PCA

during the learning phase, although they are equally expensive during recognition.

Both models perform soundly on these data. Finding a better way to regularise the

FA model against irrelevant pixels is important. Given its model, FA is completely

correct to assign a zero variance to outlying pixels that are silent throughout the



training set. Our prior knowledge that this might happen can be used to specify a

more complicated prior that makes it possible that pixels that are generally inactive

can occasionally be corrupted by noise.

We also found that the inclusion of tangent vectors did not substantially improve the

performance. Our use of tangent vectors is essentially an instantiation of tangent-prop

[], which constrains the output of the network to satisfy appropriate invariances

through its directional derivatives. Since our networks are linear, these directional

derivatives are particularly simple, allowing the tangent vectors just to be added into

the covariance matrices. If the tangents about input xk were perfectly captured by a

linear model centred at that point, then the reconstruction error E

i for input xi would

be exactly the one-sided tangent distance from xi to the tangent space of input xk.

Hastie & Simard [], developed a locally linear mixture model analogous to the one

described here, except using two-sided tangent distances during the whole of learning.

Tangents would be expected not to help if there are enough data points that they

express directly all the actual invariances.

It is challenging to model the low dimensional manifolds of high dimensional pixel

images of digits in a computationally tractable manner. Our locally linear models

are designed to capture aspects of the short-range structure of the manifold and to

respect other knowledge about the digit modelling problem, such as the fact that

there are di(cid:11)erent styles of handwriting even for the same digit. The models are

conceptually and computationally straightforward.

Acknowledgments
This research was funded by Apple and by the Ontario Information Technology Research
Centre. We thank Patrice Simard, Zoubin Ghahramani, Rob Tibshirani and Yann Le Cun
for helpful discussions. Geo(cid:11)rey Hinton is the Nesbitt-Burns fellow of the Canadian Institute
for Advanced Research.



