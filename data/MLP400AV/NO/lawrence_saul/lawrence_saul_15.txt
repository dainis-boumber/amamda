Abstract

We show how to learn a Mahanalobis distance metric for k-nearest neigh-
bor (kNN) classication by semidenite programming. The metric is
trained with the goal that the k-nearest neighbors always belong to the
same class while examples from different classes are separated by a large
margin. On seven data sets of varying size and difculty, we nd that
metrics trained in this way lead to signicant improvements in kNN
classicationfor example, achieving a test error rate of 1.3% on the
MNIST handwritten digits. As in support vector machines (SVMs), the
learning problem reduces to a convex optimization based on the hinge
loss. Unlike learning in SVMs, however, our framework requires no
modication or extension for problems in multiway (as opposed to bi-
nary) classication.

1 Introduction

The k-nearest neighbors (kNN) rule [3] is one of the oldest and simplest methods for pattern
classication. Nevertheless, it often yields competitive results, and in certain domains,
when cleverly combined with prior knowledge, it has signicantly advanced the state-of-
the-art [1, 14]. The kNN rule classies each unlabeled example by the majority label among
its k-nearest neighbors in the training set. Its performance thus depends crucially on the
distance metric used to identify nearest neighbors.
In the absence of prior knowledge, most kNN classiers use simple Euclidean distances
to measure the dissimilarities between examples represented as vector inputs. Euclidean
distance metrics, however, do not capitalize on any statistical regularities in the data that
might be estimated from a large training set of labeled examples.
Ideally, the distance metric for kNN classication should be adapted to the particular
problem being solved.
It can hardly be optimal, for example, to use the same dis-
tance metric for face recognition as for gender identication, even if in both tasks, dis-
tances are computed between the same xed-size images. In fact, as shown by many re-
searchers [2, 6, 7, 8, 12, 13], kNN classication can be signicantly improved by learning
a distance metric from labeled examples. Even a simple (global) linear transformation of
input features has been shown to yield much better kNN classiers [7, 12]. Our work builds
in a novel direction on the success of these previous approaches.

In this paper, we show how to learn a Mahanalobis distance metric for kNN classication.
The metric is optimized with the goal that k-nearest neighbors always belong to the same
class while examples from different classes are separated by a large margin. Our goal for
metric learning differs in a crucial way from those of previous approaches that minimize the
pairwise distances between all similarly labeled examples [12, 13, 17]. This latter objective
is far more difcult to achieve and does not leverage the full power of kNN classication,
whose accuracy does not require that all similarly labeled inputs be tightly clustered.
Our approach is largely inspired by recent work on neighborhood component analysis [7]
and metric learning by energy-based models [2]. Though based on the same goals, however,
our methods are quite different. In particular, we are able to cast our optimization as an
instance of semidenite programming. Thus the optimization we propose is convex, and
its global minimum can be efciently computed.
Our approach has several parallels to learning in support vector machines (SVMs)most
notably, the goal of margin maximization and a convex objective function based on the
hinge loss. In light of these parallels, we describe our approach as large margin nearest
neighbor (LMNN) classication. Our framework can be viewed as the logical counterpart
to SVMs in which kNN classication replaces linear classication.
Our framework contrasts with classication by SVMs, however, in one intriguing respect:
it requires no modication for problems in multiway (as opposed to binary) classica-
tion. Extensions of SVMs to multiclass problems typically involve combining the results
of many binary classiers, or they require additional machinery that is elegant but non-
trivial [4]. In both cases the training time scales at least linearly in the number of classes.
By contrast, our learning problem has no explicit dependence on the number of classes.

2 Model
Let {(~xi, yi)}n
i=1 denote a training set of n labeled examples with inputs ~xi Rd and dis-
crete (but not necessarily binary) class labels yi. We use the binary matrix yij  {0, 1} to
indicate whether or not the labels yi and yj match. Our goal is to learn a linear transforma-
tion L:RdRd, which we will use to compute squared distances as:

D(~xi, ~xj) = kL(~xi  ~xj)k2.

(1)
Specically, we want to learn the linear transformation that optimizes kNN classication
when distances are measured in this way. We begin by developing some useful terminology.
Target neighbors
In addition to the class label yi, for each input ~xi we also specify k target neighbors
that is, k other inputs with the same label yi that we wish to have minimal distance to ~xi,
as computed by eq. (1). In the absence of prior knowledge, the target neighbors can simply
be identied as the k nearest neighbors, determined by Euclidean distance, that share the
same label yi. (This was done for all the experiments in this paper.) We use ij  {0, 1} to
indicate whether input ~xj is a target neighbor of input ~xi. Like the binary matrix yij, the
matrix ij is xed and does not change during learning.
Cost function
Our cost function over the distance metrics parameterized by eq. (1) has two competing
terms. The rst term penalizes large distances between each input and its target neighbors,
while the second term penalizes small distances between each input and all other inputs
that do not share the same label. Specically, the cost function is given by:

ij(1yil)(cid:2)1 + kL(~xi~xj)k2kL(~xi~xl)k2(cid:3)

X

(L) =X

ijkL(~xi~xj)k2 + c

ij

ijl

(2)

+ ,

where in the second term [z]+ = max(z, 0) denotes the standard hinge loss and c > 0 is
some positive constant (typically set by cross validation). Note that the rst term only
penalizes large distances between inputs and target neighbors, not between all similarly
labeled examples.
Large margin
The second term in the cost function in-
corporates the idea of a margin. In par-
ticular, for each input ~xi, the hinge loss
is incurred by differently labeled inputs
whose distances do not exceed, by one
absolute unit of distance, the distance
from input ~xi to any of its target neigh-
bors. The cost function thereby favors
distance metrics in which differently la-
beled inputs maintain a large margin of
distance and do not threaten to invade
each others neighborhoods. The learn-
ing dynamics induced by this cost func-
tion are illustrated in Fig. 1 for an input
with k =3 target neighbors.
Parallels with SVMs
The competing terms in eq. (2) are anal-
ogous to those in the cost function for
SVMs [11]. In both cost functions, one
term penalizes the norm of the parame-
ter vector (i.e., the weight vector of the maximum margin hyperplane, or the linear trans-
formation in the distance metric), while the other incurs the hinge loss for examples that
violate the condition of unit margin. Finally, just as the hinge loss in SVMs is only trig-
gered by examples near the decision boundary, the hinge loss in eq. (2) is only triggered by
differently labeled examples that invade each others neighborhoods.
Convex optimization
We can reformulate the optimization of eq. (2) as an instance of semidenite program-
ming [16]. A semidenite program (SDP) is a linear program with the additional constraint
that a matrix whose elements are linear in the unknown variables is required to be posi-
tive semidenite. SDPs are convex; thus, with this reformulation, the global minimum of
eq. (2) can be efciently computed. To obtain the equivalent SDP, we rewrite eq. (1) as:

Figure 1: Schematic illustration of one inputs
neighborhood ~xi before training (left) versus
after training (right). The distance metric is op-
timized so that: (i) its k =3 target neighbors lie
within a smaller radius after training; (ii) differ-
ently labeled inputs lie outside this smaller ra-
dius, with a margin of at least one unit distance.
Arrows indicate the gradients on distances aris-
ing from the optimization of the cost function.

D(~xi, ~xj) = (~xi  ~xj)>M(~xi  ~xj),

(3)
where the matrix M = L>L, parameterizes the Mahalanobis distance metric induced by
the linear transformation L. Rewriting eq. (2) as an SDP in terms of M is straightforward,
since the rst term is already linear in M = L>L and the hinge loss can be mimicked by
introducing slack variables ij for all pairs of differently labeled inputs (i.e., for all hi, ji
such that yij = 0). The resulting SDP is given by:

ij ij(1  yil)ijl subject to:

MinimizeP

ij ij(~xi  ~xj)>M(~xi  ~xj) + cP

(1) (~xi  ~xl)>M(~xi  ~xl)  (~xi  ~xj)>M(~xi  ~xj)  1  ijl
(2) ijl  0
(3) M (cid:23) 0.

The last constraint M (cid:23) 0 indicates that the matrix M is required to be positive semidef-
inite. While this SDP can be solved by standard online packages, general-purpose solvers

!xi!ximarginlocal neighborhood!xi!ximarginBEFOREAFTERSimilarly labeledDifferently labeledDifferently labeledtarget neighbortend to scale poorly in the number of constraints. Thus, for our work, we implemented our
own special-purpose solver, exploiting the fact that most of the slack variables {ij} never
attain positive values1. The slack variables {ij} are sparse because most labeled inputs are
well separated; thus, their resulting pairwise distances do not incur the hinge loss, and we
obtain very few active constraints. Our solver was based on a combination of sub-gradient
descent in both the matrices L and M, the latter used mainly to verify that we had reached
the global minimum. We projected updates in M back onto the positive semidenite cone
after each step. Alternating projection algorithms provably converge [16], and in this case
our implementation worked much faster than generic solvers2.

3 Results

We evaluated the algorithm in the previous section on seven data sets of varying size and
difculty. Table 1 compares the different data sets. Principal components analysis (PCA)
was used to reduce the dimensionality of image, speech, and text data, both to speed up
training and avoid overtting. Except for Isolet and MNIST, all of the experimental results
are averaged over several runs of randomly generated 70/30 splits of the data. Isolet and
MNIST have pre-dened training/test splits. For the other data sets, we randomly gener-
ated 70/30 splits for each run. Both the number of target neighbors (k) and the weighting
parameter (c) in eq. (2) were set by cross validation. (For the purpose of cross-validation,
the training sets were further partitioned into training and validation sets.) We begin by
reporting overall trends, then discussing the individual data sets in more detail.
We rst compare kNN classication error rates using Mahalanobis versus Euclidean dis-
tances. To break ties among different classes, we repeatedly reduced the neighborhood
size, ultimately classifying (if necessary) by just the k = 1 nearest neighbor. Fig. 2 sum-
marizes the main results. Except on the smallest data set (where over-training appears to
be an issue), the Mahalanobis distance metrics learned by semidenite programming led to
signicant improvements in kNN classication, both in training and testing. The training
error rates reported in Fig. 2 are leave-one-out estimates.
We also computed test error rates using a variant of kNN classication, inspired by previous
work on energy-based models [2]. Energy-based classication of a test example ~xt was
done by nding the label that minimizes the cost function in eq. (2).
In particular, for
a hypothetical label yt, we accumulated the squared distances to the k nearest neighbors
of ~xt that share the same label in the training set (corresponding to the rst term in the
cost function); we also accumulated the hinge loss over all pairs of differently labeled
examples that result from labeling ~xt by yt (corresponding to the second term in the cost
function). Finally, the test example was classied by the hypothetical label that minimized
the combination of these two terms:

X

ij(1yil)(cid:2)1 + kL(~xi~xj)k2kL(~xi~xl)k2(cid:3)

+

X

j

yt =argminyt

tjkL(~xt~xj)k2+c

j,i=tl=t

As shown in Fig. 2, energy-based classication with this assignment rule generally led to
even further reductions in test error rates.
Finally, we compared our results to those of multiclass SVMs [4]. On each data set (except
MNIST), we trained multiclass SVMs using linear and RBF kernels; Fig. 2 reports the
results of the better classier. On MNIST, we used a non-homogeneous polynomial kernel
of degree four, which gave us our best results. (See also [9].)

conditions, then using the resulting solution as a starting point for the actual SDP of interest.

1A great speedup can be achieved by solving an SDP that only monitors a fraction of the margin
2A matlab implementation is currently available at http://www.seas.upenn.edu/kilianw/lmnn.

examples (train)
examples (test)
classes
input dimensions
features after PCA
constraints
active constraints
CPU time (per run)
runs

Iris Wine
106
126
52
44
3
3
13
4
4
13
7266
1396
8s
100

5278
113
2s
100

Faces
280
120
40
1178
30

78828
7665
7s
100

Bal
445
90
3
4
4

76440
3099
13s
100

Isolet
6238
1559
26
617
172
37 Mil
45747
11m

1

News
16000
2828
20

30000
200

164 Mil
732359

1.5h
10

MNIST
60000
10000

3.3 Bil
243596

10
784
164

4h
1

Table 1: Properties of data sets and experimental parameters for LMNN classication.

Figure 2: Training and test error rates for kNN classication using Euclidean versus Ma-
halanobis distances. The latter yields lower test error rates on all but the smallest data set
(presumably due to over-training). Energy-based classication (see text) generally leads to
further improvement. The results approach those of state-of-the-art multiclass SVMs.

Small data sets with few classes
The wine, iris, and balance data sets are small data sets, with less than 500 training exam-
ples and just three classes, taken from the UCI Machine Learning Repository3. On data
sets of this size, a distance metric can be learned in a matter of seconds. The results in
Fig. 2 were averaged over 100 experiments with different random 70/30 splits of each data
set. Our results on these data sets are roughly comparable (i.e., better in some cases, worse
in others) to those of neighborhood component analysis (NCA) and relevant component
analysis (RCA), as reported in previous work [7].
Face recognition
The AT&T face recognition data set4 contains 400 grayscale images of 40 individuals in
10 different poses. We downsampled the images from to 38  31 pixels and used PCA to
obtain 30-dimensional eigenfaces [15]. Training and test sets were created by randomly
sampling 7 images of each person for training and 3 images for testing. The task involved
40-way classicationessentially, recognizing a face from an unseen pose. Fig. 2 shows
the improvements due to LMNN classication. Fig. 3 illustrates the improvements more
graphically by showing how the k = 3 nearest neighbors change as a result of learning a
Mahalanobis metric. (Though the algorithm operated on low dimensional eigenfaces, for
clarity the gure shows the rescaled images.)

3Available at http://www.ics.uci.edu/mlearn/MLRepository.html.
4Available at http://www.uk.research.att.com/facedatabase.html

MNISTNEWSISOLETBALFACESIRISWINE2.117.613.413.012.48.64.73.73.314.49.78.47.85.92.62.719.04.34.75.84.42.62.7 1.71.31.21.91.220.011.09.44.714.110.08.20.330.01.14.33.52.230.1Energy basedclassificationkNN MahalanobisdistancekNN EuclideandistanceMulticlassSVMtesting error rate (%)training error rate (%)Figure 3: Images from the AT&T face recognition data base. Top row: an image correctly
recognized by kNN classication (k = 3) with Mahalanobis distances, but not with Eu-
clidean distances. Middle row: correct match among the k =3 nearest neighbors according
to Mahalanobis distance, but not Euclidean distance. Bottom row: incorrect match among
the k =3 nearest neighbors according to Euclidean distance, but not Mahalanobis distance.

Spoken letter recognition
The Isolet data set from UCI Machine Learning Repository has 6238 examples and 26
classes corresponding to letters of the alphabet. We reduced the input dimensionality (orig-
inally at 617) by projecting the data onto its leading 172 principal componentsenough
to account for 95% of its total variance. On this data set, Dietterich and Bakiri report test
error rates of 4.2% using nonlinear backpropagation networks with 26 output units (one per
class) and 3.3% using nonlinear backpropagation networks with a 30-bit error correcting
code [5]. LMNN with energy-based classication obtains a test error rate of 3.7%.
Text categorization
The 20-newsgroups data set consists of posted articles from 20 newsgroups, with roughly
1000 articles per newsgroup. We used the 18828-version of the data set5 which has cross-
postings removed and some headers stripped out. We tokenized the newsgroups using the
rainbow package [10]. Each article was initially represented by the weighted word-counts
of the 20,000 most common words. We then reduced the dimensionality by projecting the
data onto its leading 200 principal components. The results in Fig. 2 were obtained by av-
eraging over 10 runs with 70/30 splits for training and test data. Our best result for LMMN
on this data set at 13.0% test error rate improved signicantly on kNN classication using
Euclidean distances. LMNN also performed comparably to our best multiclass SVM [4],
which obtained a 12.4% test error rate using a linear kernel and 20000 dimensional inputs.
Handwritten digit recognition
The MNIST data set of handwritten digits6 has been extensively benchmarked [9]. We
deskewed the original 2828 grayscale images, then reduced their dimensionality by re-
taining only the rst 164 principal components (enough to capture 95% of the datas overall
variance). Energy-based LMNN classication yielded a test error rate at 1.3%, cutting the
baseline kNN error rate by over one-third. Other comparable benchmarks [9] (not exploit-
ing additional prior knowledge) include multilayer neural nets at 1.6% and SVMs at 1.2%.
Fig. 4 shows some digits whose nearest neighbor changed as a result of learning, from a
mismatch using Euclidean distance to a match using Mahanalobis distance.

4 Related Work

Many researchers have attempted to learn distance metrics from labeled examples. We
briey review some recent methods, pointing out similarities and differences with our work.

5Available at http://people.csail.mit.edu/jrennie/20Newsgroups/
6Available at http://yann.lecun.com/exdb/mnist/

Among 3 nearest neighbors before but not after training:Test Image:Among 3 nearest neighbors after but not before training:Figure 4: Top row: Examples of MNIST images whose nearest neighbor changes dur-
ing training. Middle row: nearest neighbor after training, using the Mahalanobis distance
metric. Bottom row: nearest neighbor before training, using the Euclidean distance metric.

Xing et al [17] used semidenite programming to learn a Mahalanobis distance metric
for clustering. Their algorithm aims to minimize the sum of squared distances between
similarly labeled inputs, while maintaining a lower bound on the sum of distances between
differently labeled inputs. Our work has a similar basis in semidenite programming, but
differs in its focus on local neighborhoods for kNN classication.
Shalev-Shwartz et al [12] proposed an online learning algorithm for learning a Mahalanobis
distance metric. The metric is trained with the goal that all similarly labeled inputs have
small pairwise distances (bounded from above), while all differently labeled inputs have
large pairwise distances (bounded from below). A margin is dened by the difference of
these thresholds and induced by a hinge loss function. Our work has a similar basis in its
appeal to margins and hinge loss functions, but again differs in its focus on local neigh-
borhoods for kNN classication. In particular, we do not seek to minimize the distance
between all similarly labeled inputs, only those that are specied as neighbors.
Goldberger et al [7] proposed neighborhood component analysis (NCA), a distance metric
learning algorithm especially designed to improve kNN classication. The algorithm min-
imizes the probability of error under stochastic neighborhood assignments using gradient
descent. Our work shares essentially the same goals as NCA, but differs in its construction
of a convex objective function.
Chopra et al [2] recently proposed a framework for similarity metric learning in which
the metrics are parameterized by pairs of identical convolutional neural nets. Their cost
function penalizes large distances between similarly labeled inputs and small distances
between differently labeled inputs, with penalties that incorporate the idea of a margin.
Our work is based on a similar cost function, but our metric is parameterized by a linear
transformation instead of a convolutional neural net. In this way, we obtain an instance of
semidenite programming.
Relevant component analysis (RCA) constructs a Mahalanobis distance metric from a
weighted sum of in-class covariance matrices [13]. It is similar to PCA and linear discrim-
inant analysis (but different from our approach) in its reliance on second-order statistics.
Hastie and Tibshirani [?] and Domeniconi et al [6] consider schemes for locally adaptive
distance metrics that vary throughout the input space. The latter work appeals to the goal
of margin maximization but otherwise differs substantially from our approach. In partic-
ular, Domeniconi et al [6] suggest to use the decision boundaries of SVMs to induce a
locally adaptive distance metric for kNN classication. By contrast, our approach (though
similarly named) does not involve the training of SVMs.

5 Discussion

In this paper, we have shown how to learn Mahalanobis distance metrics for kNN clas-
sication by semidenite programming. Our framework makes no assumptions about the
structure or distribution of the data and scales naturally to large number of classes. Ongoing

Test Image:Nearest neighbor before training:Nearest neighbor after training:work is focused in three directions. First, we are working to apply LMNN classication to
problems with hundreds or thousands of classes, where its advantages are most apparent.
Second, we are investigating the kernel trick to perform LMNN classication in nonlin-
ear feature spaces. As LMMN already yields highly nonlinear decision boundaries in the
original input space, however, it is not obvious that kernelizing the algorithm will lead to
signicant further improvement. Finally, we are extending our framework to learn locally
adaptive distance metrics [6, 8] that vary across the input space. Such metrics should lead
to even more exible and powerful large margin classiers.

