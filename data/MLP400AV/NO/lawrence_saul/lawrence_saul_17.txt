Abstract

We derive multiplicative updates for solving the nonnegative quadratic
programming problem in support vector machines (SVMs). The updates
have a simple closed form, and we prove that they converge monotoni-
cally to the solution of the maximum margin hyperplane. The updates
optimize the traditionally proposed objective function for SVMs. They
do not involve any heuristics such as choosing a learning rate or deciding
which variables to update at each iteration. They can be used to adjust all
the quadratic programming variables in parallel with a guarantee of im-
provement at each iteration. We analyze the asymptotic convergence of
the updates and show that the coefcients of non-support vectors decay
geometrically to zero at a rate that depends on their margins. In practice,
the updates converge very rapidly to good classiers.

1 Introduction

Support vector machines (SVMs) currently provide state-of-the-art solutions to many prob-
lems in machine learning and statistical pattern recognition[18]. Their superior perfor-
mance is owed to the particular way they manage the tradeoff between bias (undertting)
and variance (overtting). In SVMs, kernel methods are used to map inputs into a higher,
potentially innite, dimensional feature space; the decision boundary between classes is
then identied as the maximum margin hyperplane in the feature space. While SVMs pro-
vide the exibility to implement highly nonlinear classiers, the maximum margin criterion
helps to control the capacity for overtting. In practice, SVMs generalize very well  even
better than their theory suggests.

Computing the maximum margin hyperplane in SVMs gives rise to a problem in nonnega-
tive quadratic programming. The resulting optimization is convex, but due to the nonneg-
ativity constraints, it cannot be solved in closed form, and iterative solutions are required.
There is a large literature on iterative algorithms for nonnegative quadratic programming
in general and for SVMs as a special case[3, 17]. Gradient-based methods are the simplest
possible approach, but their convergence depends on careful selection of the learning rate,
as well as constant attention to the nonnegativity constraints which may not be naturally
enforced. Multiplicative updates based on exponentiated gradients (EG)[5, 10] have been

investigated as an alternative to traditional gradient-based methods. Multiplicative updates
are naturally suited to sparse nonnegative optimizations, but EG updateslike their addi-
tive counterpartssuffer the drawback of having to choose a learning rate.

Subset selection methods constitute another approach to the problem of nonnegative
quadratic programming in SVMs. Generally speaking, these methods split the variables
at each iteration into two sets: a xed set in which the variables are held constant, and a
working set in which the variables are optimized by an internal subroutine. At the end of
each iteration, a heuristic is used to transfer variables between the two sets and improve
the objective function. An extreme version of this approach is the method of Sequential
Minimal Optimization (SMO)[15], which updates only two variables per iteration. In this
case, there exists an analytical solution for the updates, so that one avoids the expense of a
potentially iterative optimization within each iteration of the main loop.

In general, despite the many proposed approaches for training SVMs, solving the quadratic
programming problem remains a bottleneck in their implementation. (Some researchers
have even advocated changing the objective function in SVMs to simplify the required
optimization[8, 13].) In this paper, we propose a new iterative algorithm, called Multi-
plicative Margin Maximization (M3), for training SVMs. The M3 updates have a simple
closed form and converge monotonically to the solution of the maximum margin hyper-
plane. They do not involve heuristics such as the setting of a learning rate or the switching
between xed and working subsets; all the variables are updated in parallel. They pro-
vide an extremely straightforward way to implement traditional SVMs. Experimental and
theoretical results conrm the promise of our approach.

2 Nonnegative quadratic programming

We begin by studying the general problem of nonnegative quadratic programming. Con-
sider the minimization of the quadratic objective function

F (v) =

vT Av + bT v;

(1)

1
2

subject to the constraints that vi (cid:21) 0 8i. We assume that the matrix A is symmetric and
semipositive denite, so that the objective function F (v) is bounded below, and its opti-
mization is convex. Due to the nonnegativity constraints, however, there does not exist an
analytical solution for the global minimum (or minima), and an iterative solution is needed.

2.1 Multiplicative updates

Our iterative solution is expressed in terms of the positive and negative components of the
matrix A in eq. (1). In particular, let A+ and A(cid:0) denote the nonnegative matrices:

A+

ij =(cid:26) Aij

0

if Aij > 0,
otherwise,

and A(cid:0)

ij =(cid:26) jAij j

0

if Aij < 0,
otherwise.

(2)

It follows trivially that A = A+(cid:0)A(cid:0). In terms of these nonnegative matrices, our proposed
updates (to be applied in parallel to all the elements of v) take the form:

vi  (cid:0) vi" (cid:0)bi +pb2

i + 4(A+v)i(A(cid:0)v)i
2(A+v)i

# :

(3)

The iterative updates in eq. (3) are remarkably simple to implement. Their somewhat mys-
terious form will be claried as we proceed. Let us begin with two simple observations.
First, eq. (3) prescribes a multiplicative update for the ith element of v in terms of the
ith elements of the vectors b, A+v, and A+v. Second, since the elements of v, A+,
and A(cid:0) are nonnegative, the overall factor multiplying vi on the right hand side of eq. (3)
is always nonnegative. Hence, these updates never violate the constraints of nonnegativity.

2.2 Fixed points

We can show further that these updates have xed points wherever the objective func-
tion, F (v) achieves its minimum value. Let v(cid:3) denote a global minimum of F (v). At
such a point, one of two conditions must hold for each element v (cid:3)
i > 0 and
i = 0 and (@F=@vi)jv(cid:3) (cid:21) 0. The rst condition applies to the
(@F=@vi)jv(cid:3) = 0, or (ii), v(cid:3)
positive elements of v(cid:3), whose corresponding terms in the gradient must vanish. These
derivatives are given by:

i: either (i) v(cid:3)

@F

@vi(cid:12)(cid:12)(cid:12)(cid:12)v(cid:3)

= (A+v(cid:3))i (cid:0) (A(cid:0)v(cid:3))i + bi:

(4)

The second condition applies to the zero elements of v(cid:3). Here, the corresponding terms of
the gradient must be nonnegative, thus pinning v (cid:3)
i to the boundary of the feasibility region.
The multiplicative updates in eq. (3) have xed points wherever the conditions for global
minima are satised. To see this, let

(cid:13)i

4
=

(cid:0)bi +pb2

i + 4(A+v(cid:3))i(A(cid:0)v(cid:3))i
2(A+v(cid:3))i

(5)

denote the factor multiplying the ith element of v in eq. (3), evaluated at v(cid:3). Fixed points
of the multiplicative updates occur when one of two conditions holds for each element vi:
either (i) v(cid:3)
i = 0. It is straightforward to show from eqs. (45)
that (@F=@vi)jv(cid:3)= 0 implies (cid:13)i = 1. Thus the conditions for global minima establish the
conditions for xed points of the multiplicative updates.

i > 0 and (cid:13)i = 1, or (ii) v(cid:3)

2.3 Monotonic convergence

The updates not only have the correct xed points; they also lead to monotonic improve-
ment in the objective function, F (v). This is established by the following theorem:

Theorem 1 The function F (v) in eq. (1) decreases monotonically to the value of its global
minimum under the multiplicative updates in eq. (3).

The proof of this theorem (sketched in Appendix A) relies on the construction of an auxil-
iary function which provides an upper bound on F (v). Similar methods have been used to
prove the convergence of many algorithms in machine learning[1, 4, 6, 7, 12, 16].

3 Support vector machines

We now consider the problem of computing the maximum margin hyperplane in SVMs[3,
17, 18]. Let f(xi; yi)gN
i=1 denote labeled examples with binary class labels yi = (cid:6)1, and
let K(xi; xj) denote the kernel dot product between inputs. In this paper, we focus on the
simple case where in the high dimensional feature space, the classes are linearly separable
and the hyperplane is required to pass through the origin1.
In this case, the maximum
margin hyperplane is obtained by minimizing the loss function:

L((cid:11)) = (cid:0)Xi

1

2Xij

(cid:11)i +

(cid:11)i(cid:11)jyiyjK(xi; xj);

(6)

subject to the nonnegativity constraints (cid:11)i (cid:21) 0. Let (cid:11)(cid:3) denote the location of the minimum
i yixi

of this loss function. The maximal margin hyperplane has normal vector w = Pi (cid:11)(cid:3)

and satises the margin constraints yiK(w; xi) (cid:21) 1 for all examples in the training set.

1The extensions to non-realizable data sets and to hyperplanes that do not pass through the origin

are straightforward. They will be treated in a longer paper.

Kernel
Data
Sonar
Breast cancer

Polynomial
k = 4
(cid:27) = 0:3
k = 6
9.6% 9.6% 7.6%
5.1% 3.6% 4.4%

Radial
(cid:27) = 1:0
6.7%
4.4%

(cid:27) = 3:0
10.6%
4.4%

Table 1: Misclassication error rates on the sonar and breast cancer data sets after 512
iterations of the multiplicative updates.

3.1 Multiplicative updates

The loss function in eq. (6) is a special case of eq. (1) with Aij = yiyjK(xi; xj) and
bi = (cid:0)1. Thus, the multiplicative updates for computing the maximal margin hyperplane
in hard margin SVMs are given by:

(cid:11)i  (cid:0) (cid:11)i" 1 +p1 + 4(A+(cid:11))i(A(cid:0)(cid:11))i

2(A+(cid:11))i

#

(7)

where A(cid:6) are dened as in eq. (2). We will refer to the learning algorithm for hard margin
SVMs based on these updates as Multiplicative Margin Maximization (M3).

It is worth comparing the properties of these updates to those of other approaches. Like
multiplicative updates based on exponentiated gradients (EG)[5, 10], the M3 updates are
well suited to sparse nonnegative optimizations2; unlike EG updates, however, they do
not involve a learning rate, and they come with a guarantee of monotonic improvement.
Like the updates for Sequential Minimal Optimization (SMO)[15], the M3 updates have
a simple closed form; unlike SMO updates, however, they can be used to adjust all the
quadratic programming variables in parallel (or any subset thereof), not just two at a time.
Finally, we emphasize that the M3 updates optimize the traditional objective function for
SVMs; they do not compromise the goal of computing the maximal margin hyperplane.

3.2 Experimental results

We tested the effectiveness of the multiplicative updates in eq. (7) on two real world prob-
lems: binary classication of aspect-angle dependent sonar signals[9] and breast cancer
data[14]. Both data sets, available from the UCI Machine Learning Repository[2], have
been widely used to benchmark many learning algorithms, including SVMs[5]. The sonar
and breast cancer data sets consist of 208 and 683 labeled examples, respectively. Train-
ing and test sets for the breast cancer experiments were created by 80%/20% splits of the
available data.

We experimented with both polynomial and radial basis function kernels. The polynomial
kernels had degrees k = 4 and k = 6, while the radial basis function kernels had variances
of (cid:27) = 0:3; 1:0 and 3:0. The coefcients (cid:11)i were uniformly initialized to a value of one in
all experiments.

Misclassication rates on the test data sets after 512 iterations of the multiplicative updates
are shown in Table 1. As expected, the results match previously published error rates on
these data sets[5], showing that the M3 updates do in practice converge to the maximum
margin hyperplane. Figure 1 shows the rapid convergence of the updates to good classiers
in just one or two iterations.

2In fact, the multiplicative updates by nature cannot directly set a variable to zero. However,
a variable can be clamped to zero whenever its value falls below some threshold (e.g., machine
precision) and when a zero value would satisfy the Karush-Kuhn-Tucker conditions.

iteration

support vectors

non-support vectors

00

01

02

04

08

16

32

64

s
t
n
e
i
c
i
f
f
e
o
c

0
0

100

200
training examples

300

400

500

(%)      e
t

e
(%)
g
2.9           3.6

2.4           2.2

1.1           4.4

0.5           4.4

0.0           4.4

0.0           4.4

0.0           4.4

0.0           4.4

Figure 1: Rapid convergence of the multiplicative updates in eq. (7). The plots show
results after different numbers of iterations on the breast cancer data set with the radial
basis function kernel ((cid:27) = 3). The horizontal axes index the coefcients (cid:11)i of the 546
training examples; the vertical axes show their values. For ease of visualization, the training
examples were ordered so that support vectors appear to the left and non-support vectors,
to the right. The coefcients (cid:11)i were uniformly initialized to a value of one. Note the rapid
attenuation of non-support vector coefcients after one or two iterations. Intermediate error
rates on the training set ((cid:15)t) and test set ((cid:15)g) are also shown.

3.3 Asymptotic convergence

The rapid decay of non-support vector coefcients in Fig. 1 motivated us to analyze their
rates of asymptotic convergence. Suppose we perturb just one of the non-support vector
coefcients in eq. (6)say (cid:11)iaway from the xed point to some small nonzero value (cid:14)(cid:11)i.
If we hold all the variables but (cid:11)i xed and apply its multiplicative update, then the new
displacement (cid:14)(cid:11)0

i after the update is given asymptotically by ((cid:14)(cid:11)0

i) (cid:25) ((cid:14)(cid:11)i)(cid:13)i, where

(cid:13)i =

;

(8)

1 +p1 + 4(A+(cid:11)(cid:3))i(A(cid:0)(cid:11)(cid:3))i

2(A+(cid:11)(cid:3))i

and Aij = yiyjK(xi; xj). (Eq. (8) is merely the specialization of eq. (5) to SVMs.) We can
thus bound the asymptotic rate of convergencein this idealized but instructive setting
by computing an upper bound on (cid:13)i, which determines how fast the perturbed coefcient
decays to zero. (Smaller (cid:13)i implies faster decay.) In general, the asymptotic rate of con-
vergence is determined by the overall positioning of the data points and classication hy-
perplane in the feature space. The following theorem, however, provides a simple bound in
terms of easily understood geometric quantities.

feature space from xi to the maximum margin hyperplane, and let d = minj dj =

Theorem 2 Let di = jK(xi; w)j=pK(w; w) denote the perpendicular distance in the
1=pK(w; w) denote the one-sided margin of the classier. Also, let i =pK(xi; xi)

denote the distance of xi to the origin in the feature space, and let  = maxj j denote the
largest such distance. Then a bound on the asymptotic rate of convergence (cid:13)i is given by:

(cid:13)i (cid:20) (cid:20)1 +

1
2

(di (cid:0) d)d

i

(cid:21)(cid:0)1

:

(9)

+

+

+

l

i

d

i

_

+

d

classification hyperplane

_

_

_

Figure 2: Quantities used to bound the asymptotic rate of convergence in eq. (9); see text.
Solid circles denote support vectors; empty circles denote non-support vectors.

The proof of this theorem is sketched in Appendix B. Figure 2 gives a schematic repre-
sentation of the quantities that appear in the bound. The bound has a simple geometric
intuition:
the more distant a non-support vector from the classication hyperplane, the
faster its coefcient decays to zero. This is a highly desirable property for large numeri-
cal calculations, suggesting that the multiplicative updates could be used to quickly prune
away outliers and reduce the size of the quadratic programming problem. Note that while
the bound is insensitive to the scale of the inputs, its tightness does depend on their relative
locations in the feature space.

4 Conclusion

SVMs represent one of the most widely used architectures in machine learning. In this
paper, we have derived simple, closed form multiplicative updates for solving the non-
negative quadratic programming problem in SVMs. The M3 updates are straightforward
to implement and have a rigorous guarantee of monotonic convergence. It is intriguing
that multiplicative updates derived from auxiliary functions appear in so many other areas
of machine learning, especially those involving sparse, nonnegative optimizations. Exam-
ples include the Baum-Welch algorithm[1] for discrete hidden markov models, general-
ized iterative scaling[6] and adaBoost[4] for logistic regression, and nonnegative matrix
factorization[11, 12] for dimensionality reduction and feature extraction. In these areas,
simple multiplicative updates with guarantees of monotonic convergence have emerged
over time as preferred methods of optimization. Thus it seems worthwhile to explore their
full potential for SVMs.

