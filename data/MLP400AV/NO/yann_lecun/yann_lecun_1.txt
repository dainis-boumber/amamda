Abstract

Many modern visual recognition algorithms in-
corporate a step of spatial pooling, where the
outputs of several nearby feature detectors are
combined into a local or global bag of features,
in a way that preserves task-related information
while removing irrelevant details. Pooling is
used to achieve invariance to image transforma-
tions, more compact representations, and better
robustness to noise and clutter. Several papers
have shown that the details of the pooling oper-
ation can greatly inuence the performance, but
studies have so far been purely empirical. In this
paper, we show that the reasons underlying the
performance of various pooling methods are ob-
scured by several confounding factors, such as
the link between the sample cardinality in a spa-
tial pool and the resolution at which low-level
features have been extracted. We provide a de-
tailed theoretical analysis of max pooling and av-
erage pooling, and give extensive empirical com-
parisons for object recognition tasks.

1. Introduction

Modern computer vision architectures often comprise a
spatial pooling step, which combines the responses of
feature detectors obtained at nearby locations into some
statistic that summarizes the joint distribution of the fea-
tures over some region of interest. The idea of feature
pooling originates in Hubel and Wiesels seminal work
on complex cells in the visual cortex (1962), and is re-
lated to Koenderinks concept of locally orderless im-
ages (1999). Pooling features over a local neighborhood
to create invariance to small transformations of the input

Appearing in Proceedings of the 27 th International Conference
on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the
author(s)/owner(s).

is used in a large number of models of visual recogni-
tion. The pooling operation is typically a sum, an av-
erage, a max, or more rarely some other commutative
(i.e.,
independent of the order of the contributing fea-
tures) combination rule. Biologically-inspired models of
image recognition that use feature pooling include the
neocognitron (Fukushima & Miyake, 1982), convolutional
networks which use average pooling (LeCun et al., 1989;
1998), or max pooling (Ranzato et al., 2007; Jarrett et al.,
2009), the HMAX class of models which uses max pool-
ing (Serre et al., 2005), and some models of the primary
visual cortex area V1 (Pinto et al., 2008) which use av-
erage pooling. Many popular methods for feature ex-
traction also use pooling, including SIFT (Lowe, 2004),
histograms of oriented gradients (HOG) (Dalal & Triggs,
2005) and their variations.
In these methods, the domi-
nant gradient orientations are measured in a number of re-
gions, and are pooled over a neighborhood, resulting in a
local histogram of orientations. Recent recognition sys-
tems often use pooling at a higher level to compute lo-
cal or global bags of features. This is done by vector-
quantizing feature descriptors and by computing the code-
word counts over local or global areas (Sivic & Zisserman,
2003; Lazebnik et al., 2006; Zhang et al., 2007; Yang et al.,
2009), which is equivalent to average-pooling vectors con-
taining a single 1 at the index of the codeword, and 0 ev-
erywhere else (1-of-k codes).

In general terms, the objective of pooling is to transform
the joint feature representation into a new, more usable one
that preserves important information while discarding ir-
relevant detail, the crux of the matter being to determine
what falls in which category. For example, the assumption
underlying the computation of a histogram is that the aver-
age feature activation matters, but exact spatial localization
does not. Achieving invariance to changes in position or
lighting conditions, robustness to clutter, and compactness
of representation, are all common goals of pooling.

The success of the spatial pyramid model (Lazebnik et al.,

A Theoretical Analysis of Feature Pooling

2006), which obtains large increases in performance by
performing pooling over the cells of a spatial pyramid
rather than over the whole image as in plain bag-of-features
models (Zhang et al., 2007), illustrates the importance of
the spatial structure of pooling neighborhoods. Perhaps
more intriguing is the dramatic inuence of the way pool-
ing is performed once a given region of interest has been
chosen. Thus, Jarrett et al. (2009) have shown that pooling
type matters more than careful unsupervised pretraining of
features for classication problems with little training data,
obtaining good results with random features when appro-
priate pooling is used. Yang et al. (2009) report much better
classication performance on several object or scene clas-
sication benchmarks when using the maximum value of
a feature rather than its average to summarize its activity
over a region of interest. But no theoretical justication of
these ndings is given. In previous work (Boureau et al.,
2010), we have shown that using max pooling on hard-
vector quantized features (which produces a binary vector
that records the presence of a feature in the pool) in a spa-
tial pyramid brings the performance of linear classication
to the level of that obtained by Lazebnik et al. (2006) with
an intersection kernel, even though the resulting feature is
binary. However, it remains unclear why max pooling per-
forms well in a large variety of settings, and indeed whether
similar or different factors come into play in each case.

This paper proposes to ll the gap and to conduct a thor-
ough theoretical investigation of pooling. We compare dif-
ferent pooling operations in a categorization context, and
examine how the behavior of the corresponding statistics
may translate into easier or harder subsequent classica-
tion. We provide experiments in the context of visual object
recognition, but the analysis applies to all tasks which in-
corporate some form of pooling (e.g., text processing from
which the bag-of-features method was originally adapted).
The main contributions of this paper are (1) an extensive
analytical study of the discriminative powers of different
pooling operations, (2) the discrimination of several fac-
tors affecting pooling performance, including smoothing
and sparsity of the features, (3) the unication of several
popular pooling types as belonging to a single continuum.

2. Pooling Binary Features

Consider a two-class categorization problem. Intuitively,
classication is easier if the distributions from which points
of the two classes are drawn have no overlap.
In fact,
if the distributions are simply shifted versions of one an-
other (e.g., two Gaussian distributions with same vari-
ance), linear separability increases monotonically with the
magnitude of the shift (e.g., with the distance between
the means of two Gaussian distributions of same vari-
ance) (Bruckstein & Cover, 1985). In this section, we ex-

amine how pooling affects the separability of the resulting
feature distributions when the features being pooled are bi-
nary vectors (e.g., 1-of-k codes obtained by vector quanti-
zation in bag-of-features models).

2.1. Model

Let us examine the contribution of a single feature in a
bag-of-features representation (i.e., if the unpooled data is
a P  k matrix of 1-of-k codes taken at P locations, we
extract a single P -dimensional column v of 0s and 1s, in-
dicating the absence or presence of the feature at each lo-
cation). For simplicity, we model the P components of v
as i.i.d. Bernoulli random variables. The independence as-
sumption is clearly false since nearby image features are
strongly correlated, but the analysis of this simple model
nonetheless yields useful predictions that can be veried
empirically. The vector v is reduced by a pooling operation
f to a single scalar f (v) (which would be one component
of the k-dimensional representation using all features, e.g.,
one bin in a histogram). We consider two pooling types:
average pooling fa(v) = 1
i=1 vi, and max pooling
fm(v) = maxi vi.

P PP

2.2. Distribution Separability

Given two classes C1 and C2, we examine the separation
of conditional distributions p(fm|C1) and p(fm|C2), and
p(fa|C1) and p(fa|C2). Viewing separability as a signal-
to-noise problem, better separability can be achieved by ei-
ther increasing the distance between the means of the two
class-conditional distributions, or reducing their standard
deviation.

We rst consider average pooling. The sum over P i.i.d.
Bernoulli variables of mean  follows a binomial distri-
bution B(P, ). Consequently, the distribution of fa is
a scaled-down version of the binomial distribution, with
mean a = , and variance 2
a = (1  )/P . The ex-
pected value of fa is independent of sample size P , and
the variance decreases like 1
P ; therefore the separation ra-
tio of means difference over standard deviation decreases
monotonically like 1P
Max pooling is slightly less straightforward, so we examine
means separation and variance separately in the next two
sections.

.

2.2.1. MEANS SEPARATION OF MAX-POOLED

FEATURES

fm is a Bernoulli variable of mean m = 1  (1  )P
and variance 2
m = (1 (1 )P )(1 )P . The mean in-
creases monotonically from 0 to 1 with sample size P . Let
 denote the separation of class-conditional expectations of

A Theoretical Analysis of Feature Pooling

max-pooled features,

(P ) , |E(fm|C1)E(fm|C2)| = |(12)P(11)P|,
where 1 , P(vi = 1|C1) and 2 , P(vi = 1|C2).
We abuse notation by using  to refer both to the function
dened on sample cardinality P and its extension to R. It
is easy to show that  is increasing between 0 and

PM , (cid:12)(cid:12)(cid:12)(cid:12)

log(cid:18) log(1  2)

1  2(cid:19)(cid:12)(cid:12)(cid:12)(cid:12)
log(1  1)(cid:19)/log(cid:18) 1  1

,

and decreasing between PM and , with lim0  =
lim  = 0.
Noting that (1) = |1  2| is the distance between the
class-conditional expectations of average-pooled features,
there exists a range of pooling cardinalities for which the
distance is greater with max pooling than average pooling
if and only if PM > 1. Assuming 1 > 2, it is easy to
show that PM  1  1 > 1  1
e > 0.63. This implies
that the feature is selected to represent more than half the
patches on average, which in practice does not happen in
usual bag-of-features contexts, where codebooks comprise
more than a hundred codewords.

2.2.2. VARIANCE OF MAX-POOLED FEATURES

The variance of the max-pooled feature is 2
m = (1  (1 
)P )(1  )P . A simple analysis of the continuous exten-
sion of this function to real numbers shows that it has limit
0 at 0 and , and is increasing then decreasing, reaching its
maximum of 0.5 at log(2)/| log(1  )|. The increase of
the variance can play against the better separation of the ex-
pectations of the max-pooled feature activation, when pa-
rameter values 1 and 2 are too close for the two classes.
Several regimes for the variation of means separation and
standard deviations are shown in Fig. 1.

2.2.3. CONCLUSIONS AND PREDICTIONS

Our simplied analysis leads to several predictions:

 Max pooling is particularly well suited to the separa-
tion of features that are very sparse (i.e., have a very
low probability of being active)

 Using all available samples to perform the pooling

may not be optimal

 The optimal pooling cardinality should increase with

dictionary size

The rst point can be formalized by observing that the char-
acteristic pooling cardinality |
 in the case
  1), scales the transition to the asymptotic regime (low

log(1)| ( 1

1

variance, high probability of activation): the maximum of
the variance is reached at P = log(2)/| log(1  )|, and:

P(fm(v) = 1) >   P >

log(1  )
log(1  )

.

Consequently, the range of cardinalities for which max
pooling achieves good separation between two classes dou-
bles if the probability of activation of the feature for both
classes is divided by two. A particularly favorable regime
is 2  1  1  that is, a feature which is rare, but rela-
tively much more frequent in one of the two classes; in that
case, both classes reach their asymptotic regime for very
different sample cardinalities ( 1
1

and 1
2

).

We have recently conducted preliminary experiments re-
lated to the second point (Boureau et al., 2010)  namely,
that better performance can be obtained by using smaller
pooling cardinalities. We have compared the performance
of whole-image pooling, regular two-level spatial pyramid
pooling, and a two-level pyramid where the smaller pools
are taken randomly instead of spatially.
In the random
pyramid setting, the performance of max pooling is inter-
mediate between that obtained with whole-image and spa-
tial pyramid pooling, while the classication using aver-
age pooling becomes worse than with whole-image pool-
ing. However, a number of concurrent factors could explain
the increased accuracy: (1) smaller pooling cardinality, (2)
smoothing over multiple estimates (one per ner cell of the
pyramid), (3) estimation of two distinct features (the max-
imum over the full and partial cardinalities, respectively).
The more comprehensive experiments presented in the next
section resolve this ambiguity by isolating each factor.

Finally, the increase of optimal pooling cardinality with
dictionary size is related to the link underlined above be-
tween the sparsity of the features (dened here as the prob-
ability of them being 0) and the discriminative power of
max-pooling, since the expected feature activations sum to
one in the general bag-of-features setting (exactly one fea-
ture is activated at each location), resulting in a mean ex-
pected activation of 1
k with a k-word codebook. Thus, k
gives an order of magnitude for the characteristic cardinal-
ity scale of the transition to the asymptotic regime, for a
large enough codebook.

2.3. Experiments

test our

conjectures by running experiments
We
(Lazebnik et al., 2006) and Caltech-
on the Scenes
101 (Fei-Fei et al., 2004) datasets, which respectively
comprise 101 object categories (plus a background cate-
gory) and fteen scene categories. In all experiments, the
features being pooled are local codes representing 16  16
SIFT descriptors that have been densely extracted using
the parameters yielding the best accuracy in our previous

A Theoretical Analysis of Feature Pooling

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.


1 +  2
max
avg

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0

5

10

15

20

25

0

200

400

600

800

1000

Pool cardinality

Pool cardinality

(a) 1 = 0.4, 2 = 0.2

(b) 1 = 1.102, 2 = 5.103

(c) 1 = 1.102, 2 = 1.104

Figure 1. (P ) = |(1  1)P  (1  2)P|, 1 and 2 denote the distance between the expectations of the max-pooled
respectively. max = /(1 + 2) and avg =
features of mean activation 1 and 2, and their standard deviations,
|1  2|.P /(p1.(1  1) + p2.(1  2)) give a measure of separability for max and average pooling.  reaches its peak

at smaller cardinalities than max. (a) When features have relatively large activations, the peak of separability is obtained for small
cardinalities (b) With sparser feature activations, the range of the peak is much larger (note the change of scale in the x axis). (c) When
one feature is much sparser than the other, max can be larger than avg for some cardinalities (shaded area). Best viewed in color.

work (2010) (every 8 pixels for the Scenes and every 4
pixels for Caltech-101). The codes jointly represent 2  2
neighborhoods of SIFT descriptors, with subsampling
of 1 and 4 for the Scenes and Caltech-101, respectively.
Features are pooled over the whole image using either
average or max pooling. Classication is performed with
a one-versus-one support vector machine (SVM) using a
linear kernel, and 100 and 30 training images per class for
the Scenes and Caltech-101 datasets, respectively, and the
rest for testing, following the usual experimental setup.
We report the average per-class recognition rate, averaged
over 10 random splits of training and testing images.

2.3.1. OPTIMAL POOLING CARDINALITY

We rst test whether recognition can indeed improve for
some codebook sizes when max pooling is performed over
samples of smaller cardinality, as predicted by our analysis.
Recognition performance is compared using either average
or max pooling, with various combinations of codebook
sizes and pooling cardinalities. We use whole-image rather
than pyramid or grid pooling, since having several cells of
same cardinality provides some smoothing that is hard to
quantify. Results are presented in Fig. 2. Recognition per-
formance of average-pooled features (Average in Fig. 2)
drops with pooling cardinality for all codebook sizes, as
expected; performance also drops with max pooling (1 es-
timate in Fig. 2) when the codebook size is large. However,
noticeable improvements appear at intermediate cardinal-
ities for the smaller codebook sizes (compare blue, solid
curves on the left and right of Fig. 2), as predicted by our
analysis.

Next, we examine whether better recognition can be
achieved when using a smoother estimate of the expected
max-pooled feature activation. We consider two ways of
rening the estimate. First, if only a fraction of all sam-
ples is used, a smoother estimate can be obtained by re-
placing the single max by an empirical average of the max

over different subsamples, the limit case as pool cardinal-
ity decreases being average pooling. The second approach
directly applies the formula for the expectation of the max-

imum (1  (1  )P , using the same notation as before)
to the empirical mean computed using all samples. This
has the benet of removing the constraint that P be smaller
than the number of available samples, in addition to be-
ing computationally very simple. Results using these two
smoothing strategies are plotted in Fig. 2 under labels Em-
pirical and Expectation, respectively. Smoothing the esti-
mate of the max-pooled features always helps, especially
at smaller pooling cardinalities. The best performance is
then obtained with pooling cardinalities smaller than the
full cardinality in all our experiments. As predicted, the
maximum of the curve shifts towards larger cardinality as
codebook size increases. The best estimate of the max-
pooled feature is the expectation computed from the em-

pirical mean, 1  (1  )P . P here simply becomes the

parameter of a nonlinear function applied to the mean. In
all cases tested, using this nonlinear function with the opti-
mal P outperforms both average and max pooling.

2.3.2. COMBINING MULTIPLE POOLING

CARDINALITIES

The maximum over a pool of smaller cardinality is not
merely an estimator of the maximum over a large pool;
therefore, using different pool cardinalities (e.g., using a
spatial pyramid instead of a grid) may provide a more
powerful feature, independently of the difference in spa-
tial structure. Using a codebook of size 256, we com-
pare recognition rates using jointly either one, two, or three
different pooling cardinalities, with average pooling, max
pooling with a single estimate per pooling cardinality, or
max pooling smoothed by using the theoretical expectation.
Results presented in Table 1 show that combining cardinal-
ities improves performance with max pooling only if the
estimate has not been smoothed. Thus, the simultaneous
presence of multiple cardinalities does not seem to provide

A Theoretical Analysis of Feature Pooling

5
7

0
7

5
6

0
6

5
5

t
c
e
r
r
o
C
%



1 estimate
Empirical
Expectation
Average

5
7

0
7

5
6

0
6

5
5

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

5
5

t
c
e
r
r
o
C
%



1

5

50

500

1

5

50

500

1

5

50

500

1

5

50

500

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

(a) 128 codewords

(b) 256 codewords

(c) 512 codewords

(d) 1024 codewords

t
c
e
r
r
o
C
%



5
4

5
3

5
2

t
c
e
r
r
o
C
%



5
4

5
3

5
2

t
c
e
r
r
o
C
%



5
4

5
3

5
2

1 estimate
Empirical
Expectation
Average

5
7

0
7

5
6

0
6

5
5

t
c
e
r
r
o
C
%



t
c
e
r
r
o
C
%



5
4

5
3

5
2

1

5

50

500

1

5

50

500

1

5

50

500

1

5

50

500

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

(e) 128 codewords

(f) 256 codewords

(g) 512 codewords

(h) 1024 codewords

Figure 2. Inuence of pooling cardinality and smoothing on performance. Top row: Scenes dataset. Bottom row: Caltech-101 dataset.
1 estimate: max computed over a single pool. Empirical: empirical average of max-pooled features over several subsamples (not plotted

for smaller sizes, when it reaches the expectation) Expectation: theoretical expectation of the maximum over P samples 1  (1  )P ,
computed from the empirical average . Average: estimate of the average computed over a single pool. Best viewed in color.

any benet beyond that of an approximate smoothing.

Table 1. Classication results with whole-image pooling over bi-
nary codes (k = 256). One indicates that features are pooled
using a single cardinality, Joint that the larger cardinalities are

also used. SM: smooth maximum (1  (1  )P ).

512

256

1024

Avg, Joint
Max, One
Max, Joint
SM, One
SM, Joint

Smallest cardinality
Caltech 101 Avg, One 32.4  1.1 31.3  1.0 28.6  1.1
31.9  1.2 32.1  1.2
31.7  1.4 32.7  1.3 30.4  2.3
34.4  0.7 35.8  0.9
37.9  0.6 40.5  0.7 42.0  1.4
39.4  1.3 40.6  0.8
15 Scenes Avg, One 69.8  0.7 68.7  0.8 66.3  0.7
69.6  0.7 69.2  1.0
63.5  0.6 64.8  0.7 64.3  0.4
65.4  0.6 67.1  0.6
67.2  0.8 70.4  0.7 72.6  0.7
69.2  0.7 70.7  0.7

Avg, Joint
Max, One
Max, Joint
SM, One
SM, Joint

2.3.3. PRACTICAL CONSEQUENCES

In papers using a spatial pyramid (Lazebnik et al., 2006;
Yang et al., 2009), there is a coupling between the pool-
ing cardinality and other parameters of the experiment: the
pooling cardinality is the density at which the underlying
low-level feature representation have been extracted (e.g.,
SIFT features computed every 8 pixels in (Lazebnik et al.,
2006)) multiplied by the spatial area of each spatial pool.
While using all available samples is optimal for average

pooling, this is usually not the case with max pooling over
binary features, particularly when the size of the codebook
is small. Instead, the pooling cardinality for max pooling
should be adapted to the dictionary size, and the remaining
samples should be used to smooth the estimate. Another,
simpler way to achieve similar or better performance is to
apply to the average-pooled feature the nonlinear transfor-
mation corresponding to the expectation of the maximum,

(i.e., 1  (1  )P , using the same notation as before);
in addition, the parameter P is then no longer limited by
the number of available samples in a pool, which may be
important for very large codebooks. Our experiments us-
ing binary features in a three-level pyramid show that this
transformation yields improvement over max pooling for
all codebook sizes (Table 2). The increase in accuracy is
small, however the difference is consistently positive when
looking at experimental runs individually instead of the dif-
ference in the averages over ten runs.

Table 2. Recognition accuracy with 3-level pyramid pooling over
binary codes. One-vs-all classication has been used in this ex-
periment. Max: max pooling using all samples. SM: smooth max-
imum (expected value of the maximum computed from the aver-

age 1  (1  )P ), using a pooling cardinality of P = 256 for
codebook sizes 256 and 512, P = 512 for codebook size 1024.

Codebook size
Caltech 101 Max

15 Scenes Max

256

512

1024

67.5  1.0
77.9  0.7

71.0  0.8
SM 68.6  0.9 70.0  1.2 71.8  0.8
80.2  0.4
SM 78.2  0.4 79.9  0.5 80.5  0.6

69.2  1.1
79.4  0.5

A Theoretical Analysis of Feature Pooling

3. Pooling Continuous Sparse Codes

Sparse codes have proven useful in many image applica-
tions such as image compression and deblurring. Com-
bined with max pooling, they have led to state-of-the-
art image recognition performance with a linear classi-
er (Yang et al., 2009; Boureau et al., 2010). However, the
analysis developed for binary features in the previous sec-
tion does not apply, and the underlying causes for this good
performance seem to be different.

3.1. Inuence of Pooling Cardinality

In the case of binary features, and when no smoothing is
performed, we have seen above that there is an optimal
pooling cardinality, which increases with the sparsity of
the features. Smoothing the features displaces that opti-
mum towards smaller cardinalities. In this section, we per-
form the same analysis for continuous features, and show
that (1) it is always better to use all samples for max pool-
ing when no smoothing is performed, (2) however the in-
crease in signal-to-noise ratio (between means separation
and standard deviation) does not match the noise reduction
obtained by averaging over all samples.

Let P denote cardinality of the pool. For a Gaussian dis-
tribution, a classical result is that the expectation of the
max over samples from a distribution of variance 2 grows

asymptotically (when P  ) like p22 log(P ). Thus,

the separation of the maxima over two Gaussian samples
increases indenitely with sample size if their standard de-
viations are different, but the rate of growth is very slow.

 and variance 1

The cumulative distribution function of the max-pooled

Exponential distribution (or Laplace distributions for fea-
ture values that may be negative) are often preferred to
Gaussian distributions to model visual feature responses
because they are highly kurtotic.
In particular, they are
a better model for sparse codes. Assume the distribution
of the value of a feature for each patch is an exponen-
tial distribution with mean 1
2 . The cor-
responding cumulative distribution function is 1  ex.
feature is (1  ex)P . The mean and variance of the
distribution can be shown to be respectively m = H(P )
m = 1
and 2
l (2H(l)  H(P )), where H(k) =
1
i denotes the harmonic series. Thus, for all P ,
1
, and the distributions will be better sep-
2
arated if the scaling factor of the mean is bigger than the
scaling factor of the standard deviations, i.e., H(P ) >
qPP
l (2H(l)  H(P )), which is true for all P . Fur-
thermore, since H(P ) = log(P ) +  + o(1) when P 
 (where  is Eulers constant), it can be shown that
PP
l (2H(l)  H(P )) = log(P ) + O(1), so that the
distance between the means grows faster (like log(P ))

2 PP

l=1

Pk

i=1
= 1
2

= 1
2



1

1

l=1

1

l=1

than the standard deviation, which grows like plog(P ).

Two conclusions can be drawn from this:
(1) when no
smoothing is performed, larger cardinalities provide a bet-
ter signal-to-noise ratio, but (2) this ratio grows slower than
when simply using the additional samples to smooth the es-

timate (1/P assuming independent samples, although in

reality smoothing is less favorable since the independence
assumption is clearly false in images).

3.2. Experiments

We perform the same experiments as in the previous section
to test the inuence of codebook size and pooling cardinali-
ties, using continuous sparse codes instead of binary codes.
Results are presented in Fig. 3. As expected from our anal-
ysis, using larger pooling cardinalities is always better with
continuous codes when no smoothing is performed (blue
solid curve): no bump is observed even with smaller dictio-
naries. Max pooling performs better than average pooling
on the Caltech dataset (bottom row in Fig. 3); this is not
predicted by the analysis using our very simple model. On
the Scenes dataset (top row in Fig. 3), max pooling and av-
erage pooling perform equally well when the largest dictio-
nary size tested (1024) is used. Slightly smoothing the esti-
mate of max pooling by using a smaller sample cardinality
results in a small improvement in performance; since the
grid (or pyramid) pooling structure performs some smooth-
ing (by providing several estimates for the sample cardinal-
ities of the ner levels), this may explain why max pooling
performs better than average pooling with grid and pyra-
mid smoothing, even though average pooling may perform
as well when a single estimate is given.

3.3. Mixture Distribution

Our simple model does not account for the better discrim-
ination sometimes achieved by max pooling for continu-
ous sparse codes with large dictionaries.
In a previous
paper (Boureau et al., 2010), we showed that max pool-
ing may perform better than average pooling with expo-
nential features sampled from mixture distributions, with
one of the components of the mixture being shared be-
tween classes and having a lower expected activation. This
may play a role in the better performance of max pooling.
In fact, the sparse code vectors extracted on images have
an overwhelming number of zero components, and may
thus be better modeled as a mixture distribution of a Dirac
delta function and an exponential distribution, than as a sin-
gle exponential distribution. Assuming the mixture coef-
cients vary between images, the mean of the distribution
computed over an image shifts between 0 and the mean of
the exponential; this may result in a larger overlap between
class-conditional distributions when using average pooling
than max pooling, as illustrated in our work (Boureau et al.,
2010).

A Theoretical Analysis of Feature Pooling

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

t
c
e
r
r
o
C
%



5
4

5
3

5
2

1 estimate
Empirical
Average

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

10 20

50

200 500

10 20

50

200 500

10 20

50

200 500

10 20

50

200 500

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

(a) 128 codewords

(b) 256 codewords

(c) 512 codewords

(d) 1024 codewords

1 estimate
Empirical
Average

t
c
e
r
r
o
C
%



5
4

5
3

5
2

t
c
e
r
r
o
C
%



5
4

5
3

5
2

t
c
e
r
r
o
C
%



5
4

5
3

5
2

10

50

200

1000

10

50

200

1000

10

50

200

1000

10

50

200

1000

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

Pool cardinality, Log scale

(e) 128 codewords

(f) 256 codewords

(g) 512 codewords

(h) 1024 codewords

Figure 3. Inuence of pooling cardinality and smoothing on performance. Top row: Scenes dataset. Bottom row: Caltech-101 dataset.
1 estimate: maximum computed over a single pool. Empirical: empirical average of max-pooled features over several subsamples of
smaller cardinality. Average: estimate of the average computed over a single pool. Best viewed in color.

4. Transition from Average to Max Pooling

The previous sections have shown that depending on the
data and features, either max or average pooling may per-
form best. The optimal pooling type for a given classi-
cation problem may be neither max nor average pooling,
but something in between; in fact, we have shown that it
is often better to take the max over a fraction of all avail-
able feature points, rather than over the whole sample. This
can be viewed as an intermediate position in a parametriza-
tion from average pooling to max pooling over a sample
of xed size, where the parameter is the number of feature
points over which the max is computed: the expected value
of the max computed over one feature is the average, while
the max computed over the whole sample is obviously the
real max.

This is only one of several possible parametrizations that
continuously transition from average to max pooling. The
P -norm of a vector (more accurately, a version of it nor-
malized by the number of samples N ) is another well-

1
P

N PN

known one: fP (v) = (cid:16) 1
, which gives the
average for P = 1 and the max for P  . This
parametrization accommodates square-root pooling (for
P = 2) and absolute value pooling (for P = 1), that have
also used in the literature (e.g. (Yang et al., 2009)).

i (cid:17)
i=1 vP

exp(xi)

A third parametrization is the sum of samples weighted by
a softmax function: Pi
xi. This gives average
pooling for  = 0 and max pooling for   . Finally,
a fourth parametrization is 1
N Pi exp(xi), which
gives the average for   0 and the max for   .

 log 1

Pj exp(xj)

As with the P -norm, the result only depends on the empir-
ical feature activation mean in the case of binary vectors;
thus, these functions can be applied to an already obtained
average pool.

Fig. 4 plots the recognition rate obtained on the Scenes
dataset using sparse codes and each of the four parametriza-
tions mentioned. Instead of using the expectation of the
maximum for exponential distributions, we have used the

expectation of the maximum of binary codes (1(1)P ),

applied to the average, as we have observed that it works
well; we refer to this function as the expectation of the
maximum (maxExp in Fig. 4), although it does not con-
verge to the maximum when P   for continuous codes.
Both this parametrization and the P -norm perform better
than the two other pooling functions tested, which present
a marked dip in performance for intermediate values.

5. Discussion

This paper has looked more closely into the factors un-
derlying the recognition performance of pooling opera-
tions. By carefully adjusting the pooling step of fea-
ture extraction, relatively simple systems of local features
and classiers can become competitive to more complex
ones: our previous work (Boureau et al., 2010) had already
demonstrated that it was possible to achieve similar lev-
els of performance with a linear kernel as with Lazebnik
et al. (2006)s intersection kernel, using vector quantized
binary codes. Here, we have investigated what properties
of max pooling may account for this good performance,
and shown that this pooling strategy was well adapted to

A Theoretical Analysis of Feature Pooling

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

Pnorm
softmax
logExp
maxExp

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

t
c
e
r
r
o
C
%



5
7

0
7

5
6

0
6

1

2

5

10

20

1

2

5

10

20

1

2

5

10

20

1

2

5

10

20

Parameter P, Log scale

Parameter P, Log scale

Parameter P, Log scale

Parameter P, Log scale

(a) 128 codewords

(b) 256 codewords

(c) 512 codewords

(d) 1024 codewords

Figure 4. Recognition rate obtained using several pooling functions that perform a continuous transition from average to max pooling
when varying parameter P (see text). Best viewed in color.

features with a low probability of activation. We have pro-
posed several methods for further improving pooling: (1)
use directly the formula for the expectation of the maxi-
mum to obtain a smoother estimate in the case of binary
codes, (2) pool over smaller samples and take the average.
When using sparse coding, some limited improvement may
be obtained by pooling over subsamples of smaller cardi-
nalities and averaging, and conducting a search for the opti-
mal pooling cardinality, but this is not always the case. Fur-
ther directions we envision include pooling across several
feature types, and adapting pooling parameters separately
for each feature. We hope that paying more attention to the
choice of the pooling function will lead to better designed
recognition architectures.

Acknowledgments. This work was funded in part by
NSF grant EFRI/COPN-0835878 to NYU, and ONR con-
tract N00014-09-1-0473 to NYU.

