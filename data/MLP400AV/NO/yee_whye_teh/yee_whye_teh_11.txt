Abstract

We propose a semiparametric model for regression and classi(cid:12)cation problems in-
volving multiple response variables. The model makes use of a set of Gaussian processes
to model the relationship to the inputs in a nonparametric fashion. Conditional depen-
dencies between the responses can be captured through a linear mixture of the driving
processes. This feature becomes important if some of the responses of predictive interest
are less densely supplied by observed data than related auxiliary ones. We propose an
e(cid:14)cient approximate inference scheme for this semiparametric model whose complexity
is linear in the number of training data points.

1 Semiparametric Latent Factor Models

We are interested in predicting multiple responses yc 2 Yc; c = 1; : : : ; C from covariates
x 2 X , and we would like to model the responses as conditionally dependent. In statistical
terminology, we would like to \share statistical strength" between the yc; in machine learning
parlance this is often referred to as \transfer of learning." As we demonstrate empirically,
such sharing can be especially powerful if the data for the responses is partially missing.

Models related to the one proposed here are used in geostatistics and spatial prediction
under the name of co-kringing [4], and an example helps to demonstrate what we want to
achieve with our technique. After an accidental uranium spill, a spatial map of uranium
concentration is sought. We can take soil samples at locations of choice and measure their
uranium content, then use kriging, Gaussian process regression or another spatial prediction
technique to infer a map. However, carbon concentration is easier to measure than uranium
so the space can be sampled more densely. Moreover, it is known that these two responses
are often signi(cid:12)cantly correlated. In co-kriging we setup a joint spatial model for several
responses with the aim of improving our prediction of one of them. The model to be de-
scribed here can be used in the same context, but goes beyond simple co-kriging in several
ways. First, by using latent random (cid:12)elds our model can represent conditional dependencies
between responses directly. This is more (cid:13)exible and expressive than schemes in which re-
gression functions for each response are mixed in a posthoc manner [3], because the latent

1

(cid:12)elds are (cid:12)tted using the data from all responses and can be used to model chacteristics
of the dependencies rather than marginal relationships only. Second, the true nature of the
dependencies does not have to be known in advance but can be learned from training data
using empirical Bayesian techniques.
Writing y = (yc)c and introducing a latent variable v 2 RC, our model comes with a
factorizing likelihood

P (yjv) =Yc

P (ycjvc):

We intend to model the prior P (vjx) using Gaussian processes. The simplest possibility
is to assume that the vc are independent given x, i.e. P (vjx) = Qc P (vcjx). In this case
we can represent P (vcjx) as a Gaussian process (GP) with mean function 0 and covariance
function ~K (c):

This model will be called the baseline model in the sequel.

E(cid:2)vc(x)vc0(x0)(cid:3) = (cid:14)c;c0 ~K (c)(x; x0):

Note that under the baseline model, the inference and learning task simply decompose into
C independent ones. The components of v are independent a posteriori, so even if there are
dependencies in the data the prediction under the baseline model cannot pro(cid:12)t from them.
While this is often appropriate if the data is complete in all components yc, it can behave
suboptimal in situations where part of the yc data is missing.
On the other end of the spectrum, we can model P (vjx) as a set of dependent Gaussian
processes with C(C + 1)=2 cross-covariance functions. Tasks such as inference, hyperparam-
eter learning and prediction can be performed in much the same way as in a single process
model. This model and related algorithms will be called the naive method. Approximate
inference in nonparametric models typically scales superlinearly in the number of variables
which can be dependent a posteriori. If n is the number of training datapoints, we have
to deal with n variables at a time in the baseline method, but with as many as C n in the
naive one. The latter scaling is usually not acceptable for large C n.
In this paper we propose a model in which vjx can be dependent in a (cid:13)exible (and adaptive)
way, yet inference and learning is more tractable than for the naive model. The key is to
restrict the dependencies in a way which can be exploited in inference. We introduce a
second latent variable u 2 RP . Here and in the following it is understood that for typical
applications of our model we will have P (cid:28) C. For a mixing matrix (cid:8) 2 RC;P we set

v = (cid:8)u + v(0)

where u and v(0) are independent. The components v(0)
c have independent GP priors with
mean 0 and covariance function ~K (c), and the components up have independent zero-mean
GP priors with kernel K (p). The baseline model is a special case (P = 0), but for P > 0 the
vc will be dependent a posteriori. Note that the dependencies themselves are represented by
nonparametric latent random (cid:12)elds u. We refer to this setup as semiparametric latent factor
model (SLFM), owing to the fact that the model combines nonparametric (the processes
u; v) and parametric elements (the mixing matrix (cid:8)j).
Note that by integrating out the u processes, we obtain induced cross-covariance functions
for x 7! v:

(cid:30)c;p(cid:30)c0;pK (p)(x; x0):

E[vc(x)vc0(x0)] = (cid:14)c;c0 ~K (c)(x; x0) +Xp

We can therefore perform inference and prediction using the naive method. One goal of this
paper is to exploit the structure in this particular setup in order to obtain a signi(cid:12)cantly
more e(cid:14)cient method.
Suppose we observe some independently and identically distributed data D = f(x i; yi)j i =
1; : : : ; ng. We are interested in approximating the posterior P (v (cid:3)jx(cid:3); D). Let v =
(vi;c)i;c; vi;c = vc(xi). From the sampling model it is clear that for a test point x(cid:3),
v(cid:3) = v(x(cid:3)) is independent of D given v. Therefore,

P (v(cid:3)jD) =Z P (v(cid:3)jv)P (vjD) dv

which can be computed straightforwardly if a Gaussian approximation to P (vjD) is known.
In Section 2 we show how such an approximation can be represented and computed in an
e(cid:14)cient way. Our framework can deal with missing values for yi;c e(cid:11)ortlessly. While in the
following we treat y = (yi;c)i;c as completely given for notational convenience, incorporating
missing values amounts to nothing more than reducing the dimensionality of the vector y.
In this case, we let n be the maximum number of yi;c given for any single class c. Variables
vi;c corresponding to a missing yi;c do not have direct evidence associated with them, but
are constrained through u. If yi = (yi;c)c is missing completely, the corresponding datapoint
can be removed.

2 Approximate Inference

In this section we show how a sparse approximation to the posterior can be represented
and updated as we condition on evidence. We make use of the informative vector machine
(IVM) [5, 8] framework which allows for inference approximations within time and memory
requirements scaling only linearly in the number n of datapoints. The latter has been used
successfully in the context of models with a single GP and its application to the baseline
model of Section 1 is straightforward, but the application to a model with coupled processes
is novel and of substantial additional complexity.

2.1 The Single Process IVM

We provide a brief introduction to the principles behind the single process IVM scheme
applied to binary classi(cid:12)cation. Details can be found in [5, 8]. The IVM scheme has so
far been applied to models featuring a single GP. As mentioned in Section 1, we replace
the posterior P (vjD) by a Gaussian approximation Q(v). In the single process case, v =
(v1; : : : ; vn). Possible likelihoods include the Gaussian P (yjv) = N (yjv; (cid:27) 2) for regression
and the probit for classi(cid:12)cation:

P (yjv) = (cid:8)(y(v + (cid:27))); (cid:8)(x) =Z x

(cid:0)1

N (tj0; 1) dt:

In the IVM scheme, we have

Q(v) / P (v) exp(cid:18)(cid:0)

1
2

vT Dv + bT v(cid:19) ; D = diag((cid:25)i)

(1)

(2)

where P (v) = N (0; ~K ) is the prior. The Gaussian term parameterized by the site pa-
rameters b; D is called likelihood approximation. In general, we would allow (cid:25)i > 0 for
all i, but this leads to costs of O(n3) for training and O(n2) for each prediction. In con-
trast, the IVM scheme is a sparse approximation in which we constrain (cid:25)i = bi = 0 for all
i 62 I (cid:26) f1; : : : ; ng; jIj = d (cid:28) n, which leads to O(n d2) training time and O(d2) cost for
each prediction (in the IVM algorithm). The choice of the active set I is done sequentially
using greedy forward selection with an information-theoretic criterion: among all remaining
patterns outside the current I, we choose the one which maximizes the (instantaneous)
information gain realized by including the pattern into I (note that this score is relative
to the current approximation Q, which is why the scheme is sequential). In order to ease
notation we write I (cid:1);IDI I;(cid:1); D 2 Rd;d instead of D and I (cid:1);Ib; b 2 Rd instead of b.
We brie(cid:13)y remind the reader of the IVM representation which is derived in [5, 8]. Our ability
to score all remaining patterns for each inclusion comes at a cost of O(n d) memory and is
the reason for the O(n d2) time scaling. If Q(u) = N (h; A), then we see from Eq. 2 that

(cid:0)1

I+D1=2 ~K I D1=2 = LLT ;

+ I (cid:1);I DI I;(cid:1)(cid:17)(cid:0)1

= ~K(cid:0)M M T ; M = ~K (cid:1);ID1=2L(cid:0)T ;

A =(cid:16) ~K
(3)
furthermore h = M (cid:12); (cid:12) = L(cid:0)1D(cid:0)1=2b. Here, L is lower triangular with positive diagonal
(i.e. a Cholesky factor), and the rows of M 2 Rn;d are called m-stubs. In order to score
all remaining points, we require the posterior marginal means h and variances diag A. A
scheme to update L; M ; (cid:12); h and diag A after inclusion of a new point into I is given in
[5, 8], the cost is O(n d). The computation of the site parameters (cid:25)i; bi for a pattern i to be
included requires a so-called ADF projection (or moment matching) [7, 6] and typically a
one-dimensional numerical quadrature if Gaussian expectations over the likelihood P (yju)
are not analytically tractable. Importantly, these computations can be done to high accuracy
with no signi(cid:12)cant additional cost, because they are one-dimensional (see Section 2.6).

We then make use of the conditional inference scheme as a subroutine in order to drive
hyperparameter learning (the parameters of K and the intercept (cid:27)). This is done using a
variational bound similar to what we employ in this paper here. A look at Eq. 3 reveals
how to compute the latent predictive distribution Q(u(cid:3)) = N (u(cid:3)jh(cid:3); a(cid:3)), namely
a(cid:3) = ~K(x(cid:3); x(cid:3)) (cid:0) km(cid:3)k2 ; m(cid:3) = L(cid:0)1D1=2( ~K(x(cid:3); xi))i2I ;

h(cid:3) = mT

(cid:3) (cid:12);

and the predictive distribution is then obtained as P (y(cid:3)jx(cid:3); D) = EQ[P (y(cid:3)ju(cid:3))] which is
computed by one-dimensional quadrature in the general case, or can be computed analyti-
cally for the probit likelihood.

2.2 Gaussian Process Belief Propagation

In this section we show how the IVM representation of Section 2.1 can be combined with
the standard belief propagation algorithm for inference in parametric graphical models in
order to obtain an e(cid:14)cient inference machinery for the SLFM.

If we simply apply the IVM technology to the naive method mentioned in Section 1, we
have C n variables of which we select an active set of size C d (say), so that the running
time complexity is O(C 3 n d2), and the memory requirements are O(C 2 n d). This should
be compared to O(C n d2) time and O(C n d) memory required for the baseline model if we

select d active points for each c. The ratio of C 2 is due to the fact that the naive method
does not exploit the structure in the e(cid:11)ective v prior at all.

In parametric graphical models, conditional independence statements between variables of
a domain are asserted. Under this model assumption, there are algorithms which can exploit
certain Markovian aspects of this structure in order to perform inference very e(cid:14)ciently. A
typical assumption for such models is that conditioned on all parameters (of the conditional
distributions in the graph), data cases are independent. Strictly speaking there are two di-
mensions of conditional dependence in such models, the one between di(cid:11)erent variables of
the domain (the model dimension), and the one between datapoints (the data dimension).
For parametric models, the latter is usually trivial.1 In contrast to that, in nonparametric
models the data dimension has a very rich structure in that typically there is no (cid:12)nite num-
ber of parameters which render the data independent. This means that usually there are no
su(cid:14)cient statistics in which the data can be described in a subtantially compressed manner,
implying the scaling with the number of training points. However, common nonparametric
process models have so far almost exclusively been proposed for very simple model dimen-
sions, by essentially focussing on a single real variable. In cases where this is not su(cid:14)cient,
strong assumptions such as complete posterior independence of all (cid:12)elds are used (leading
to methods such as our baseline), or the variables are taken to be fully coupled (leading to
the naive method).

In this paper we are interested in a model which has nontrivial structure along both dimen-
sions. We propose to combine structured graphical models tools along the model dimension
with sparse inference approximations along the data dimension in order to speed up the
total inference and learning process.
Let us specify what we require the representation to deliver. Denote v c = (vi;c)i 2 Rn.
Within the IVM framework, active sets are selected based on current marginal posterior
distributions. In order to generalize this to our model, the representation has to maintain
posterior means and variances for all vi;c. The key idea is to make use of the structure of
the graphical model along the model dimensional, i.e. for P (u; v). If we treat u as a single
variable, we have a tree-structured network (see Figure 1). This allows us to employ the
(exact) belief propagation (BP) algorithm (CITE!!) in order to maintain marginals over the
vc as more and more evidence is accommodated.

Note that running BP e(cid:14)ciently on the network of Figure 1 is not straightforward due to
the dimensionality of the variables involved (along the data dimension). We need to combine
the basic message passing with the IVM framework in order to obtain an acceptable scaling.

Just as in the case of the single process IVM, we will perform inference sequentially, including
a certain number of active patterns from the sample one at a time. The ability to maintain
marginals of the vi;c at any time will be crucial to select which point to include next. The
evidence potentials vc 7! P (ycjvc) are non-Gaussian in general. We will replace them by
low rank Gaussian factors in the same way as in the single process IVM (our notation is
described in Appendix A):

(cid:9)v(vc) = N U(cid:16)I (cid:1);Icb(c); I (cid:1);IcD(c)I Ic;(cid:1)(cid:17) ;

where Ic (cid:26) f1; : : : ; ng is the active set and b(c); D(c) the site parameters. Initially, Ic = ;

1The graphical symbol for this conditional data independence is the plate.

u

v1

1y

vC

yC

Figure 1: SLFM as a tree-structured graphical model

and (cid:9)v (cid:17) 1. The edge potentials are

(cid:9)u!v(vc; u) = P (vcju) = N(cid:16)((cid:30)T

c (cid:10) I)u; ~K

(c)(cid:17)

where (cid:30)c = (cid:8)T

c;(cid:1). Finally,

(cid:9)u(u) = P (u) = N (0; K )

where K = diag(K (p))p. Now suppose new evidence is introduced in the sense that j is
included into Ic with site parameters bj;c; (cid:25)j;c. This will change the message vc sends to u
which is

mvc!u(u) /Z (cid:9)v(vc)(cid:9)u!v(vc; u) dvc

(4)

(5)

which in turns modi(cid:12)es the messages u sends to vc0; c0 6= c:

mu!vc0 (vc0) /Z Yc006=c0

mvc00 !u(u)(cid:9)u(u)(cid:9)u!v(vc0; u) du:

The message mu!vc remains the same. Finally, all marginals have to be updated:

Q(vc0) / (cid:9)v(vc0)mu!vc0 (vc0);

Q(vc) because (cid:9)v(vc) changed, and Q(vc0) because mu!vc0 changed, c0 6= c.
The details for the propagation scheme will be worked out in the following section. We will
see that the key problem of applying BP to our nonparametric setup is that messages have
to represented by a number of parameters which grows as new evidence is incorporated.
This situation is very di(cid:11)erent from BP on a parametric graphical model where messages
have a (cid:12)xed size depending on the size of the su(cid:14)cient statistics of potentials and the graph
connectivity.

2.3 The Representation

In this section we work out the details for the propagation scheme introduced in Section 2.2
and give the representation for the posterior approximation Q.

We (cid:12)rst note that a direct implementation of BP as described in the previous section does
not signi(cid:12)cantly improve the scaling of the naive method mentioned there. The reason is
that if the active sets Ic can be chosen independently, they may end up to be disjoint and
have a combined size of about C d. Since each vi;c directly in(cid:13)uences all variables ui, we
basically need about P C d of the components of u in order to represent each of the messages
mu!vc, and this leads to prohibitive costs. On the other hand, restricting all active sets to
be the same would mean a drawback compared to the independent baseline.

We opt for an approximate inference method which limits the number of u components
that the message mvc!u depends upon. To this end let dc = jIcj denote the (cid:12)nal active set
sizes.2 Let d (cid:20) minfdcg and I (cid:26) Ic for all c with d = jIj. In our inference approximation
we will use the BP algorithm introducing evidence sequentially, but in a sense squeeze the
messages from vc to u through the bottleneck of uI = (ui;p)i2I;p 2 RP d. The common
active set I is selected in the beginning, in what we call common inclusion phase. Once
it has attained the (cid:12)nal size d, we set Ic = I for all c and continue to add elements to
the Ic independently. Each inclusion is equivalent to introducing new evidence, but the
corresponding message mvc!u is restricted not to depend on uIcnI . This idea will be made
more precise below. We note that our inference approximation may not lead to a consistent
joint posterior approximation Q(v), but it does lead to joint marginal posteriors Q(v c) over
the di(cid:11)erent classes. This is not much di(cid:11)erent from some general extensions of BP on loopy
networks where a consistent joint posterior approximation cannot be extracted.

The complete representation for Q is quite complicated and consists of a sequence of
representations as used for the single process IVM. We denote the representations by
R1(c);R2(c);R3(c);R4; c = 1; : : : ; C. We aim to use consistent notation which is the same
as used in Section 2.1 or in [5, 8]. Namely, L will be lower-triangular Cholesky factors of
matrices A, (cid:12) will be additional vectors of the form (cid:12) = L(cid:0)1c, and M will be matrices of
the form B L(cid:0)T .

For notational simplicity below we will order the components in the vector uI
in
a di(cid:11)erent way than usual (see Appendix A). Recall that for v; u, etc. we have
that v = (v1;1; : : : ; vn;1; v1;2; : : : ; vn;C)T . However for uI we use the ordering uI =
(ui1;1; : : : ui1;P ; ui2;1; : : : ; uid;P )T where I = fi1; : : : ; idg. Let (cid:5) be the permutation ma-
trix which converts from the latter to the standard ordering, so that (cid:5)uI is in standard
ordering. Note that (cid:5)T ((cid:30)c (cid:10) I) = (I (cid:10) (cid:30)c), so that ((cid:30)c (cid:10) I) in the standard ordering
becomes (I (cid:10) (cid:30)c) in the uI ordering. If K = diag(K (p))p 2 RP n;P n is the kernel matrix in
the standard ordering, we de(cid:12)ne ^K = (cid:5)T K (cid:5) which is the kernel matrix in the ordering
used for uI. Note that although ^K is just as sparse as K it does not have block-diagonal
structure. In general, the superscript \^" indicates that the uI ordering is used.

2For simplicity we denote both the current and (cid:12)nal active set size by dc. In complexity statements, dc

is the (cid:12)nal size.

In order to represent mvc!u we need a IVM representation of size dc:

R1(c) : L(1;c)L(1;c)T = A(1;c) = I + D(c)1=2 ~K
(cid:12)(1;c) = L(1;c)(cid:0)1D(c)(cid:0)1=2b(c);
E(c) = D(c)1=2L(1;c)(cid:0)T :

(c)
Ic D(c)1=2;

(6)

in the common inclusion phase,

i.e. Ic = I. If P (c) =
Suppose that we are still
((cid:30)c (cid:10) I)D(c)1=2L(1;c)(cid:0)T ,
shown in Appendix B.1 that mvc!u is given by
N U (ujI (cid:1);IP (c)(cid:12)(1;c); I (cid:1);IP (c)P (c)T I I;(cid:1)), so the message depends on uI only. The \bottle-
neck" approximation we are using ensures that mvc!u will always depend on uI only, even
if eventually Ic n I 6= ;. Namely, if

is

it

(where we assume that I is a pre(cid:12)x of Ic), the message is de(cid:12)ned to be

(c)

^P

= (I (cid:10) (cid:30)c)E(c)

1:::d;(cid:1) 2 RP d;dc

(7)

mvc!u(uI) = N U(cid:16) ^P

(c)

(cid:12)(1;c); ^P

(c) ^P

(c)T(cid:17) :

The representation R2(c) is needed to form the message mu!vc, it basically represents the
distribution

Rc(u) / P (u)Yc06=c

mvc0 !u(uI):

Because all mvc0 !u are functions of uI we have Rc(u) = Rc(uI)P (u n uIjuI). R2(c) there-
fore needs to be of size P d only, and its size grows only during the common inclusion phase.
Its exact form is determined by what is required to (cid:12)nally compute the single marginals of
Q(vc). To this end, we need IVM representations R3(c) of size dc. The di(cid:11)erence between
R1(c) and R3(c) lies in the \prior distributions" used for the IVM representation. For R1(c)
this is N (vcj0; ~K
) which does not depend on messages coming from u. For R3(c) this is
replaced by the \e(cid:11)ective prior" N (vcj(cid:22)(c); (cid:6)(c)) whose parameters depend on the message
mu!vc, so that R3(c) has to be modi(cid:12)ed whenever the message changes. Apart from that,
the both representations share the same evidence potential (cid:9)v(vc) which is modi(cid:12)ed with
each inclusion into Ic.

(c)

A glance at Eq. 5 reveals that we have

(cid:6)(c) = ~K

(c)

+ ((cid:30)T

c (cid:10) I)VarRc[u]((cid:30)c (cid:10) I):

(8)

By a standard formula,

VarRc[u] = ERc[VarP [ujuI]] + VarRc[EP [ujuI]]

= K (cid:0) K(cid:1);I(cid:0)K (cid:0)1

I (cid:0) K (cid:0)1

I (cid:5)VarRc[uI](cid:5)T K (cid:0)1

where we used that Rc(ujuI) = P (ujuI). Next, let dnc =Pc06=c dc and
(C)(cid:17) 2 RP d;dnc;
=(cid:16) ^P
=(cid:16)(cid:12)(1;1)T : : : (cid:12)(1;c(cid:0)1)T (cid:12)(1;c+1)T : : : (cid:12)(1;C)T(cid:17)T

(c(cid:0)1) ^P

: : : ^P

: : : ^P

(nc)

^(cid:12)

(nc)

^P

2 Rdnc:

(1)

(c+1)

I (cid:1) K I;(cid:1)

The order of the columns of ^P
Recall that ^K I = (cid:5)T KI(cid:5). We have

(nc)

is not important as long as ^(cid:12)

(nc)

follows the same ordering.

VarRc[uI] =(cid:16) ^K

Furthermore,

(cid:0)1
I + ^P

(nc) ^P

(nc)T(cid:17)(cid:0)1

; ERc[uI] = VarRc[uI] ^P

(nc)

(nc) ^(cid:12)

:

K (cid:0)1

I (cid:0) K (cid:0)1

I (cid:5)VarRc[uI](cid:5)T K (cid:0)1

I = K(cid:0)1

Plugging this into Eq. 8 we have

(cid:6)(c) = ~K

(c)

+ ((cid:30)T

c (cid:10) I)K ((cid:30)c (cid:10) I) (cid:0) ((cid:30)T

^P

(nc) ^P

I (cid:0) (cid:5)(cid:16) ^K I + ^K I
c (cid:10) I)M (4)M (4)T ((cid:30)c (cid:10) I) + M (2;c)M (2;c)T

(nc)T ^K I(cid:17)(cid:0)1

(cid:5)T :

where M (4) does not depend on c and actually has a simple blockdiagonal structure. The
role of R2(c) is the maintenance of M (2;c) (see Eq. 11). We also have
(nc) ^P
(cid:22)(c) = ((cid:30)T

(nc) ^(cid:12)

(nc)

^P

c (cid:10) I)ERc[EP [ujuI]] = ((cid:30)T

c (cid:10) I)K (cid:1);IK (cid:0)1

(cid:0)1
I + ^P

I (cid:5)(cid:16) ^K

(nc)T(cid:17)(cid:0)1

(9)

:
(10)

(11)

(12)

(13)

(14)

Let

R2(c) : L(2;c)L(2;c)T = A(2;c) = ^K I + ^K I
(nc) ^(cid:12)

^P

^P
(nc)

;

(cid:12)(2;c) = L(2;c)(cid:0)1 ^K I
M (2;c) = ((cid:30)T

c (cid:10) I)K (cid:1);I(cid:5)L(2;c)(cid:0)T 2 Rn;P d
This representation is of size O(n P d). From Eq. 10 it is easy to see that

(nc) ^P

(nc)T ^K I;

(cid:22)(c) = M (2;c)(cid:12)(2;c):

R4 is required to maintain M (4) which is needed in Eq. 9:

R4 : L(4)L(4)T = K I; M (4) = K(cid:1);IL(4)(cid:0)T :

Since all matrices here are blockdiagonal, the representation size is only O(n P d). K (p)
I
may be ill-conditioned as d gets large, so we use the common remedy of replacing K (p)
I by
K (p)
Finally, R3(c) is a normal IVM representation based on the \e(cid:11)ective prior" N ((cid:22)(c); (cid:6)(c)):

I + "I for some small " > 0.

R3(c) : L(3;c)L(3;c)T = A(3;c) = I + D(c)1=2(cid:6)(c)

Ic D(c)1=2;

M (3;c) = (cid:6)(c)
(cid:1);Ic

D(c)1=2L(3;c)(cid:0)T 2 Rn;dc;

Ic (cid:17) :
(cid:12)(3;c) = L(3;c)(cid:0)1(cid:16)D(c)(cid:0)1=2b(c) (cid:0) D(c)1=2(cid:22)(c)

The size is O(n Pc dc) for all R3(c). We also maintain (cid:22)(c) and diag (cid:6)(c) explicitly in R3(c).

It easy to see that the Gaussian posterior Q(vc) is given by

EQ[vc] = (cid:22)(c) + M (3;c)(cid:12)(3;c); VarQ[vc] = (cid:6)(c) (cid:0) M (3;c)M (3;c)T :

Both h(c) = EQ[vc] and a(c) = diag VarQ[vc] are maintained explicitly with R3(c) (their
maintenance is in fact the prime reason for all of the representations). The size of the
combined representation is O(n (Pc dc + d C P )). This should be compared to O(n Pc dc)
for the baseline method and to O(n C Pc dc) for the naive method.

2.4 Primitives

The update of the representation after the inclusion of a point into I is most easily described
in terms of some computation primitives which are required in the context of low-rank
updates of matrices. We assume in general that

A = LLT 2 Rp;p; M = BL(cid:0)T 2 Rq;p;

where A is symmetric positive de(cid:12)nite. The primitives update L ! L0; M ! M 0 after
certain modi(cid:12)cations A ! A0; B ! B0.

2.4.1 Primitive cholext

cholext is used if A grows by a number of rows/columns. Namely,

A0 =(cid:18) A A(cid:1);(cid:3)

(cid:1);(cid:3) A(cid:3) (cid:19) ; B0 = (B B(cid:3)) ; A(cid:1);(cid:3) 2 Rp;r:

AT

cholext(L; M ; A(cid:1);(cid:3); A(cid:3); B(cid:3)) computes

L0 =(cid:18) L

0

LT

(cid:1);(cid:3) L(cid:3) (cid:19) ; M 0 = (M M (cid:3))
(cid:3) = A(cid:3) (cid:0) LT

L(cid:1);(cid:3) = L(cid:0)1A(cid:1);(cid:3); L(cid:3)LT

(cid:1);(cid:3)L(cid:1);(cid:3); M (cid:3) = (B(cid:3) (cid:0) M L(cid:1);(cid:3)) L(cid:0)T

(cid:3)

:

as

The complexity is O(p2r + qpr) if r < p.

2.4.2 Primitive chollrup

Here, A0 = A + sV V T ; s 2 f(cid:0)1; +1g; V 2 Rp;r. Let P = L(cid:0)1V which has to be com-
puted if not given. We can call chollrup(L; M ; V ; s; f alse) or chollrup(L; M ; P ; s; true).
An algorithm for chollrup can be found in [8] or [9]. The version for s = (cid:0)1 is numerically
less stable for a given condition number of L, so if positive and negative low-rank updates
have to be done, it is recommended to do all positive ones (cid:12)rst. The complexity is O(p2r)
for the computation of P and O(p2r + pqr) for the rest (if r < p). We refer to M ! M 0 as
M being dragged along.

2.5 Update of the Representation

The representation has to be updated after new inclusions are made to the Ic. This changes
the evidence potentials and therefore some of the messages. The process is di(cid:11)erent during
the common inclusion phase and afterwards. The former is more complicated and will be
discussed here, for the latter we comment on how it di(cid:11)ers from the former.

During the common inclusion phase, an elementary step consists of the inclusion of j into I
with corresponding new site parameters (bj;c)c; ((cid:25)j;c)c. We will comment in REF!! on how j
and the new parameters are determined. We make use of the following conventions. If x is
a quantity before the update, x0 denotes its value after the update. If the update proceeds
in more than one step, x0 becomes x before the second step. For example, I 0 = I [ fjg.

2.5.1 Update of R1(c) and R4
First, R1(c) are updated just like in the single process IVM:

cholext(cid:16)L(1;c); (cid:12)(1;c)T ; (cid:25)1=2

j;c D(c)1=2 ~K

(c)
I;j; 1 + (cid:25)j;c ~K

(c)
j

; (cid:25)(cid:0)1=2

j;c

bj;c(cid:17) :

Note that d = dc, and that D(c)1=2L(1;c)(cid:0)T is upper triangular. Let e(c) 2 Rd+1 be the new
column of E(c), i.e.

where (lT l) is the new row of L(1;c). The new ^P
the bottom of ^P

(c)

e(c) =(cid:18)(cid:16)(cid:0)l(cid:0)1E(c)l(cid:17)T

(cid:25)j;c=l(cid:19)T
(c)0 is obtained by appending 0 2 RP;d to
, then the new column (I (cid:10) (cid:30)c)e(c) to the right.3 The update cost is

O(Pc dc) = O(C d).
Next we update R4. Since L(4) = diag(L(4)
p ; K (p)

p ; M (4)

cholext(cid:16)L(4)

I;j ; K (p)

j

; K (p)

(cid:1);j(cid:17) ; p = 1; : : : ; P

p )p; M (4) = diag(M (4)

p )p, the update consists of

(recall that we implicitly add a small " > 0 to the kernel diagonal). The cost is O(n P d).

2.5.2 Update of R2(c); R3(c) ((cid:12)rst part)
Next we need to update R2(c); R3(c) to account for the change in mu!vc. We do this in
two steps. First, we make a low-rank adjustment to A(2;c) (a chollrup to R2(c)), second we
extend A(2;c) by a row/column (a cholext to R2(c)). Both result in low-rank adjustments to
R3(c) which are done directly after the changes to R2(c). The (cid:12)rst step can be described
as follows. Let

(nc)

)0 is obtained from ^P

( ^P
the right. Also, let ( ^(cid:12)

(nc)

E(nc) =(cid:16)(I (cid:10) (cid:30)c0)e(c0)(cid:17)c06=c 2 RP (d+1);C(cid:0)1:
(cid:3) )T with (cid:12)(cid:3) 2 RC(cid:0)1. Now,

)0 = ( ^(cid:12)

by appending 0 2 RP;d(C(cid:0)1) to the bottom, then E(nc) to
(nc)T (cid:12)T

(nc)

(cid:16)A(2;c)0(cid:17)1:::d

= A(2;c) + V V T ; V = ^K I;I 0E(nc) 2 RP d;C(cid:0)1:

( ^P

(nc) ^(cid:12)

(nc)

Let

)0 is obtained by appending 0 2 RP to ^P
(nc) ^(cid:12)

(nc) ^(cid:12)

^P

= ^K I

^K I;I 0(cid:16) ^P

(nc)(cid:17)0

(nc) ^(cid:12)

(nc)

, then adding E(nc)(cid:12)(cid:3). Therefore

(nc)

+ ^K I;I 0E(nc)(cid:12)(cid:3):

W = L(2;c)(cid:0)1V ; w = L(2;c)(cid:0)1K I;I 0E(nc)(cid:12)(cid:3) = W (cid:12)(cid:3):

Then, R2(c) is updated as

chollrup(cid:18)L(2;c);(cid:16)M (2;c)T ; (cid:12)(2;c) + w(cid:17)T

; W ; +1; true(cid:19) :

3We do not explicitly indicate the dimensionality of I within each ((cid:30)c (cid:10) I) or ((cid:30)T

c (cid:10) I), it is always clear

from the context (here it is dc + 1).

The cost is O(n P C 2 d) for all R2(c). Note that some quantities required for the subsequent
update of R3(c) have to be computed before M (2;c) is modi(cid:12)ed (notably ~M ).
To update R3(c) accordingly, we (cid:12)rst need to work out what changes the update of R2(c)
implies for (cid:6)(c); (cid:22)(c).4 This is done in Appendix B.2. If we let

the result is

LLT = I + W T W ;

((cid:6)(c))0 = (cid:6)(c) (cid:0) ~M ~M
((cid:22)(c))0 = (cid:22)(c) + (cid:22)(c)

(cid:3) ; (cid:22)(c)

T

;

~M = M (2;c)W L(cid:0)T ;

(cid:3) = ~M L(cid:0)1(cid:16)(cid:12)(cid:3) (cid:0) W T (cid:12)(2;c)(cid:17) :

We update diag (cid:6)(c) by subtracting diag( ~M ~M
then R3(c) is updated as

T

). If Q = L(3;c)(cid:0)1D(c)1=2 ~M Ic;(cid:1) 2 Rdc;C(cid:0)1,

(cid:3) )Ic(cid:17)(cid:19)T

; Q;(cid:0)1; true! :

;(cid:16)(cid:12)(3;c) (cid:0) L(3;c)(cid:0)1D(c)1=2((cid:22)(c)

chollrup L(3;c);(cid:18)(cid:16)M (3;c) (cid:0) ~M QT(cid:17)T
It is clear that we have to compute ~M explicitly in order to obtain ~M QT e(cid:14)ciently. The
computation of ~M is O(n C P d), so the cost for each R3(c) is O(n C (dc + P d)), the total
cost adds up to O(n C(C P d +Pc dc)) which is O(n C 2 P d) during the common inclusion
phase.
2.5.3 Update of R2(c); R3(c) (second part)
Next we extend A(2;c) by P new rows/columns. Let B(nc) = ^P
by padding with zeros at the bottom and right, then adding

which is updated

(nc) ^P

(nc)T

Therefore,

(cid:16)A(2;c)0(cid:17)(cid:1);d+1

B(nc)

(cid:3) = E(nc)E(nc)T :

= ^K I 0;j + ^K I 0;I B(nc) ^K I;j + ^K I 0;I 0B(nc)

(cid:3)

^K I 0;j

(again, " > 0 needs to be added to the kernel diagonal). We can use that K J;I(cid:5)(I (cid:10) (cid:30)c) =
((cid:30)c;pK (p)

J;I)p. The update of R2(c) is

cholext(cid:18)L(2;c);(cid:16)M (2;c)T ; (cid:12)(2;c)(cid:17)T

(cid:16)(cid:0)((cid:30)T

c (cid:10) I)K (cid:1);j(cid:1)T

; ^K j;I 0 ^P

(nc) ^(cid:12)

(nc)(cid:17)T(cid:19)

; (A(2;c)0)1:::d;d+1; (A(2;c)0)d+1;

which costs O(n P 2 d) (recall that the variables L(2;c); M (2;c); P (nc), etc. denote the values
after the (cid:12)rst update step has been done, not the initial values at the beginning of the
update).

4We do not yet accommodate the update of R4, this is done in the second step.

Let M (cid:3) 2 Rn;P denote the new columns of M (2;c), (cid:12)(cid:3) 2 RP the new entries of (cid:12)(2;c), and
M (cid:3)(cid:3) 2 RnP;P the new columns of M (4) (the latter does not depend on c and is block-
diagonal). Then,

((cid:6)(c))0 = (cid:6)(c) (cid:0) ((cid:30)T

c (cid:10) I)M (cid:3)(cid:3)M T

(cid:3)(cid:3)((cid:30)c (cid:10) I) + M (cid:3)M T

(cid:3)

(15)

which indicates how diag (cid:6)(c) is updated. Furthermore,

((cid:22)(c))0 = (cid:22)(c) + M (cid:3)(cid:12)(cid:3):

In order to update R3(c) we need two rank-P chollrup calls, we do the positive one (cid:12)rst.
The derivation is given in Appendix B.3. Let Q = L(3;c)(cid:0)1D(c)1=2(M (cid:3))Ic;(cid:1) 2 Rdc;P , then

; Q; +1; true! :

chollrup L(3;c);(cid:18)(cid:16)M (3;c) + M (cid:3)QT(cid:17)T

Next, with Q = L(3;c)(cid:0)1D(c)1=2((cid:30)T

; (cid:12)(3;c) (cid:0) Q(cid:12)(cid:3)(cid:19)T
c (cid:10) I)(M (cid:3)(cid:3))Ic;(cid:1) 2 Rdc;P we have
; (cid:12)(3;c)(cid:19)T

; Q;(cid:0)1; true! :

c (cid:10) I)M (cid:3)(cid:3)QT(cid:17)T

chollrup L(3;c);(cid:18)(cid:16)M (3;c) (cid:0) ((cid:30)T
Both cost O(n P Pc dc) in total (for all c).
2.5.4 Update of R3(c) and marginals
Finally, we update R3(c) to incorporate the new evidence (everything so far has only been
done to update its \e(cid:11)ective prior"). This is a standard IVM update, we only have to
evaluate a new column of (cid:6)(c):

(cid:6)(c)

(cid:1);j = ~K

(c)
(cid:1);j + ((cid:30)T

c (cid:10) I)K (cid:1);j(cid:30)c (cid:0) ((cid:30)T

j;(cid:1)(cid:17)T
c (cid:10) I)M (4)(cid:16)M (4)

(cid:30)c + M (2;c)M (2;c)

j;(cid:1)

T

where we used that ((cid:30)c (cid:10) I)I (cid:1);j = I (cid:1);j(cid:30)c. This computation is O(n P C d) for all c. The
update is

cholext(cid:18)L(3;c);(cid:16)M (3;c)T ; (cid:12)(3;c)(cid:17)T

(cid:16)(cid:25)1=2
j;c (cid:6)(c)

(cid:1);j

T ; (cid:25)(cid:0)1=2

j;c

bj;c (cid:0) (cid:25)1=2

j;c (cid:22)(c)

j (cid:17)T(cid:19);

; (cid:25)1=2

j;c D(c)1=2(cid:6)(c)

I;j; 1 + (cid:25)j;c(cid:6)(c)

j

;

costing O(n Pc dc) in total. Finally, the posterior means and variances h(c); a(c) are recom-
puted from scratch (based on the new M (3;c) matrices) at total cost of O(n Pc dc).

2.5.5 Remarks

The cost is dominated by the (cid:12)rst step updates of R2(c), namely of the M (2;c) matrices,
and by the (cid:12)rst step updates of R3(c) (the M (3;c) matrices). For d inclusions it amounts to
O(n d2 C 2 P ). The memory cost is dominated by O(n d C P ) for the M (2;c) matrices.

After the common inclusion phase, patterns are included w.r.t. speci(cid:12)c classes, say j into I c.
That means that the update process described above uses rank 1 updates instead of rank

C (cid:0) 1. As for Section 2.5.1, de(cid:12)ne e(c) = (cid:0)l(cid:0)1E(c)

1:::d;(cid:1)l 2 Rd. Then,

(c0)

and ^P

^P

(c)0 =(cid:16) ^P

(c)

; (I (cid:10) (cid:30)c)e(c)(cid:17) ;

(nc0)

remain the same for c0 6= c. R4 is not modi(cid:12)ed (it is in fact not needed anymore).
Furthermore, K I remains the same and ^P
are modi(cid:12)ed by appending (I (cid:10) (cid:30)c)e(c)
for c0
6= c. In Section 2.5.2, R2(c); R3(c) are not updated (since they do not change).
E(nc0) = e(c), and V ; W have a single column. The R2(c0) updates cost O(n P C d), and
the R3(c0) updates are O(n(P C d + Pc dc)). This is cheaper by a factor C than in the
common inclusion phase, because the message updates are rank 1 instead of rank C (cid:0) 1.
The second update step of Section 2.5.3 does not exist here, since I does not grow anymore.
The (cid:12)nal update of R3(c) (for the single c) costs O(n(P d + dc)). Finally, h(c0) and a(c0) are
recomputed from scratch (for all c0) at cost O(nPc dc).
dc! Xc

O n  P C d +Xc

Therefore, the overall running time complexity is

dc!

and the memory requirements are

O n  P C d +Xc

dc!! :

In that case, the memory requirements of our method are the same as for the baseline up
to a constant factor. However, it seems that modelling conditional dependencies between

In large sample situations it makes sense to require P C d to be of the same order as Pc dc.
classes comes at a signi(cid:12)cant additional price of at least O(n (Pc dc)2) as compared to
O(n Pc d2

c) for the independent baseline. On the other hand, our method is faster than the

naive implementation by a factor of C.

Note that if the active sets and site parameters are (cid:12)xed, then the complete representation
can be computed in

O n  Xc

d2

c + P d  C P d +Xc

dc!!!

which is signi(cid:12)cantly faster and actually fairly close to what the independent baseline re-
quires. Therefore, in contrast to the situation for earlier IVM variants, conditional inference
with active set selection comes at a signi(cid:12)cantly higher cost than without.5 The problem is

that while R2(c) is of limited size P d, for each of the Pc dc inclusions C (cid:0) 1 of them have

to be updated by rank 1. In other words, the matrices ^P
are of size P d, but are
in fact updated dnc > P d times by rank 1. Each such update has to be worked through the

(nc) ^P

(nc)T

5Even for other IVM variants the active set selection takes signi(cid:12)cantly more time in practice, but does

not have a higher complexity.

whole representation in order to make sure that the marginals h(c); a(c) are up-to-date all
the time. Delaying these updates does not help. We would have to delay the updates for a
class c more than P d times before it would be cheaper to simply recompute R2(c); R3(c)
from scratch.

2.5.6 Prediction

In order to predict on test data, the dominant bu(cid:11)ers scaling as O(n) are not required. We
need to compute the marginals of Q on the test point which is done just as above for the
training points: compute M (2;c), (cid:6)(c)
(cid:1);Ic, M (4), and M (3;c) w.r.t. the test points. The cost
is the same as computing the representation for the training set from scratch (with (cid:12)xed
active sets and site parameters), but with n replaced by the number of test points m:

O m  Xc

d2

c + P d  C P d +Xc

dc!!! :

Again, this is fairly close to the requirements of the baseline method. If m is large, the
computation can be done in multiple chunks.6 Predictive distributions can be computed
from the marginals using Gaussian quadrature in general.

2.6 Selection of Points. Computing Site Parameters

In this section we show how new site parameters are computed and how the candidate j
for the next inclusion into I is found. The principal tool for models with non-Gaussian
likelihood factors P (ycjvc) is the ADF projection mentioned in Section 2.1.
As mentioned in Section 2.3, there are two di(cid:11)erent phases in which patterns are included
into the active sets. During the initial common inclusion phase, each pattern is included
into all Ic in parallel, so that Ic = I for all c during this phase. During the second phase,
additional patterns are included w.r.t. speci(cid:12)c classes c only. Both the forward selection of
points to be included and the computation of site parameters is relatively straightforward
in the second phase, while we need additional approximations during the common phase.
We concentrate on describing the initial common phase.

c

c

; bj;c = yj;c(cid:27)(cid:0)2

Suppose that during the common inclusion phase, we have selected j for inclusion into I
and want to compute the new site parameters (bj;c)c; ((cid:25)j;c)c. First note that for a Gaussian
likelihood, the site parameters are (cid:12)xed in advance, i.e. if P (yj;cjvj;c) = N (yj;cjvj;c; (cid:27)2
c ),
then (cid:25)j;c = (cid:27)(cid:0)2
. Therefore, we assume that P (yj;cjvj;c) is not Gaussian. The
idea behind ADF is as follows. Let Q(v) be a Gaussian and f (vj) a positive non-Gaussian
potential depending only on a small number vj of components of v. If ^P (v) / Q(v)f (vj),
the goal is to approximate ^P by a Gaussian Q0(v). In ADF, this is achieved by moment
matching, i.e. ^P and Q0 have the same mean and covariance matrix. Due to the special
form of f , it is easy to see that Q0(v) = Q0(vj)Q(v n vjjvj), so we only need to match the
moments of ^P (vj) / Q(vj)f (vj). This is feasible in general if jvjj is very small, in our case
it will be a single component. Note that we only need to know Q(v j) in order to do the
ADF projection. ^P is called tilted distribution.

6On the other hand, with specialized code for large matrix computations, the chunks should be made as

large as memory resources permit.

In the second phase, we can apply ADF directly to obtain the site parameters, but during
the common phase we would have to base it on vj = (vj;c)c. This would require to com-
pute a C-dimensional Gaussian expectation over a non-Gaussian function which is hard
in general. Even for a Gaussian likelihood, we would have to have access to Q(v j) which
is a joint marginal spanning all of the vc. The representation developed above cannot be
used to obtain these joint marginals e(cid:14)ciently, and we do not know of a way to obtain a
large number of these joint marginals which is substantially more e(cid:14)cient than the naive
method mentioned at the beginning of Section 2.2. We need a way of computing the site
parameters which uses the single marginals Q(vj;c) only. The simplest choice is to compute
the parameters for (j; c) under the assumption that j is included into Ic only, requiring a
one-dimensional ADF projection and Q(vj;c) only. Since for each inclusion we use the old
Q(vj;c) marginal which does not incorporate the new information from the other vj;c0 this
is an approximation. Its e(cid:11)ect on the overall approximation could be tested by comparing
it with other more complicated schemes in which the site parameters are computed in some
ordering over c and the computations are interleaved with marginal updates. We have not
done this so far.
If ^P (vj;c) / P (yj;cjvj;c)Q(vj;c), Q0(vj;c) is the Gaussian which minimizes D[ ^P k(cid:1)]. If Q0(vj;c) =
N (^hj;c; ^aj;c), these parameters can be computed to high accuracy using one-dimensional
Gauss-Hermite quadrature if the likelihood P (yj;cjvj;c) is reasonably smooth. In the case of
the probit likelihood (Eq. 1) or the Gaussian likelihood we are mainly interested in here,
the computation is analytic. Details are provided in Appendix C. The new site parameters
then follow directly from

Q0(vj;c) / Q(vj;c) exp((cid:0)(cid:25)j;cv2

j;c=2 + bj;cvj;c):

Finally, we need a procedure for selecting a good inclusion candidate j among the indices
not already in I. Just as in the single process IVM we make use of greedy forward selection
where the selection criterion is an information measure: select the point whose subsequent
inclusion changes the posterior most, i.e. which introduces the most new information into
Q. During the second phase we need to score pairs (j; c). A convenient criterion based on
the marginal Q(vj;c) is the information gain (cid:1)j;c = (cid:0)D[Q0(vj;c)k Q(vj;c)], where Q0 is the
marginal after the inclusion of (j; c). (cid:1)j;c can be computed in O(1) given Q(vj;c). During
the common inclusion phase, a good generalization of the information gain would depend
on joint marginals Q(vj) which we cannot obtain feasibly. However, a forward selection
criterion which can be computed very e(cid:14)ciently is essential in order to be able to score
most (or all) of the remaining patterns for each inclusion. In our case, we are looking for a
criterion which depends on the single marginals Q(vj;c) only, since we can a(cid:11)ord to update
all of them after each inclusion using our representation.

For the common inclusion phase, we propose to use the C information gain values (cid:1)j;c in
order to construct an appropriate single score (cid:1)j. Possibilities are the average

or the maximum

(cid:1)avg

j = C (cid:0)1Xc

(cid:1)j;c

(cid:1)max

j = max

c

(cid:1)j;c:

Note that Q0(vj;c) is not the marginal after the inclusion of j. The latter would require to
propagate the new evidence to u and back to the vc. Alternatively, one could replace the

Q0 by these new marginals, however they may be costly to compute. We will explore this
possibility in future work.

2.7 Joint Common Inclusion Phase

As mentioned in Section 2.6, there are two di(cid:11)erent phases for inclusions into the active
sets, and during the (cid:12)rst common inclusion phase our inability of computing joint marginals
of vj = (vj;c)c leads to further somewhat unsatisfactory approximations. In Section 2.5.5 we
determined the overall complexity and noted that a realistic scaling is obtained if P C d (cid:25)
Pc dc. Under this assumption conditional inference with active set selection requires about
O(n(P C d)2) time. In this Section we suggest a way of working with joint marginals during
the common inclusion phase while still matching this scaling.
Note that the common inclusion phase requires O(n P (C d)2) and is clearly dominated by
the second phase. If we run the common phase in the naive way mentioned at the beginning
of Section 2.2 it scales as O(n C(C d)2) which is O(n(P C d)2) if P 2 (cid:25) C. Recall that the
naive method simply uses the single process IVM representation of Section 2.1 over n C
variables v with the kernel induced by ~K (c); K (p).
The memory requirement for the naive method is O(n d C 2) which is by a factor C=P too
large. However, we can limit the memory requirements by reducing the number of candidates
which are scored for each inclusion. Suppose the (common) active set I has size d0 (cid:20) d. The
matrix M 2 Rm;Cd0
dominates the memory requirements, it has one row for every candidate
which can be scored. If m (cid:20) n P d=d0, then a bu(cid:11)er of n d C P elements is su(cid:14)cient. For the
(cid:12)rst few inclusions we can set m = C n. Eventually, rows have to removed from M after
each inclusion. When selecting these rows priority is given to the ones attaining the worst
current scores.

Based on the stub matrix M we can compute the joint distribution Q(v j) =
N (hj; Aj); vj 2 RC for every inclusion candidate j. If the likelihood P (yjv) is Gaus-
sian, then the tilted distribution ^P (vj) is Gaussian, i.e. Q0 = ^P , and the information gain
(cid:1)j = (cid:0)D[Q0(vj)k Q(vj)] can be computed analytically. In general, computing (cid:1)j requires
C-dimensional quadrature which quickly becomes very costly or inaccurate. We suggest the
following greedy proxy. Start with Q0(vj) = Q(vj); D0 = ;, and (cid:1)0 = 0. For c = 1; : : : ; C
compute

(cid:1)c;c0

= (cid:0)D[Q0

c;c0(vj)k Qc(cid:0)1(vj)]

for all c0 2 f1; : : : ; Cgn Dc(cid:0)1. Here, ^Pc;c0(vj) / P (yc0jvc0)Qc(cid:0)1(vj) and Q0
ADF projection of ^Pc;c0. Note that (cid:1)c;c0
projections are one-dimensional only. If c0 is the argument minimizing these (cid:1)c;c0
(cid:1)c(cid:0)1 + (cid:1)c;c0

c;c0 is the Gaussian
c;c0(vc0)k Qc(cid:0)1(vc0)], and that the ADF
, set (cid:1)c =

and Dc = Dc(cid:0)1 [ fc0g. The (cid:12)nal score value for j is (cid:1)C.

= (cid:0)D[Q0

2.8 Limiting Resource Requirements

It is important to note that the scaling of O(n) for the inference approximation is only due
to the fact that we wish to consider all remaining points as inclusion candidates for the
active sets Ic for all classes. In a situation of limited resources, we may reduce the size of
the sample or restrict the active set sizes dc, but a third option is to score only subsets of

points as candidates for later inclusions. It is quite likely that later inclusion decisions are
less important than earlier ones, so that this way of limiting resource requirements may be
much less intrusive than the other ones. In this section we outline a simple scheme which
can be used to this end.
Let Jc (cid:26) f1; : : : ; ng be the selection index for class c, and nc = jJcj. For the (cid:12)rst inclusions,
ideally for all of the common inclusion phase, nc = n for all c, but during later stages the
nc decrease following some schedule which may be aimed at keeping the memory cost below
a (cid:12)xed threshold at all times. If we only insist on considering the points in Jc as inclusion
Jc;(cid:1) ; M (3;c)
candidates into Ic, we only need to maintain h(c)
Jc;(cid:1)
have to be stored and updated. A point which is excluded from Jc should not be re-included
later on, because it is easy to show that doing so costs no less than just leaving it in Jc
in the (cid:12)rst place. Interestingly we can use the scores of all points in Jc in order to decide
which subset to exclude, for example by retaining the top-scorers. Some randomization is
recommended as well. In fact the problem of maintaining Jc is similar to organizing a cache,
with the important di(cid:11)erence that the scores we care about can be evaluated e(cid:14)ciently for
all elements all the time, so we may draw on existing strategies developed for the latter.

Jc which means that only M (2;c)

Jc ; a(c)

3 Parameter Learning

In this section we show how free parameters can be learned automatically from the data D
in an empirical Bayesian manner. It turns out that the conditional inference approximation
developed above is required to drive this optimization.

We can separate the unobserved variables in our model in two categories: primary and sec-
ondary parameters. The former are the latent processes v((cid:1)), or equivalently their values
v at the training points, the latter consist of (cid:8); (cid:27) and the parameters of the covari-
ance functions ~K (c); K (p). Secondary parameters are sometimes called hyperparameters in
a Bayesian setting, although in a semiparametric setup such as ours one is typically careful
to distinguish parameters such as (cid:8) from \nuisance" parameters (the kernel parameters
in our context). For the former we are explicitly interested in a point estimate, while the
latter would be best integrated out and are estimated only if such a marginalization proves
intractable (such as in our case). Denote the vector of all secondary parameters by (cid:11).

Empirical Bayesian techniques combine marginalization and maximization in the sense that
primary parameters are integrated out, but hyperparameters are maximized over. In our
case, the marginal likelihood is

P (yj(cid:11)) =Z P (yjv)P (vj(cid:11)) dv:

In the maximum likelihood II (ML-II) method we choose the hyperparameters ^(cid:11) as (local)
maximizer of P (yj(cid:11)) or P (y; (cid:11)). The latter requires the speci(cid:12)cation of a hyperprior P ((cid:11))
and implies that ML-II can also be seen as maximum a posteriori (MAP) technique in that
the posterior P ((cid:11)jy) is maximized.
The computation of log P (yj(cid:11)) is as hard as doing conditional inference (i.e. computing
marginals of the conditional posterior P (vjy; (cid:11))). In terms of statistical physics, log P (yj(cid:11))
is the log partition function of the model. Interestingly a general variational framework

allows us to use a method for approximate inference in order to lower bound the log partition
function. We can then maximize the lower bound in order to determine ^(cid:11). Note that
log P (yj(cid:11)) is a convex function of v 7! log P (y; vj(cid:11)). By Legendre-Fenchel duality [1, 2]
we have

log P (yj(cid:11)) (cid:21) EQ [log P (y; vj(cid:11))] + H[Q(v)] = EQ [log P (yjv; (cid:11))] (cid:0) D[Q(v)k P (vj(cid:11))]

for any distribution Q(v). The maximizer Q(v) for the lower bound is the posterior
P (vjy; (cid:11)) (in general duality terms, v 7! log P (y; vj(cid:11)) and P (vjy; (cid:11)) are dually cou-
pled), but any other posterior approximation gives a valid lower bound. In this paper, we
use the particular Q(v) described in Section 2 as variational distribution.
By a slight abuse of notation let vI denote the vector of all components vi;c; c = 1; : : : ; C; i 2
Ic. In order to compute the relative entropy term, we note that Q(v n v IjvI) = P (v n vIjvI),
therefore

D[Q(v)k P (v)] = D[Q(vI)k P (vI)]:

Recall that the e(cid:14)cient representation we use for Q(v) allows access to the marginals
Q(vc); c = 1; : : : ; C only, so especially Q(vI) is not available. If vI;c = (vi;c)i2Ic, we make
use of the additional bounding step

D[Q(vI)k P (vI)] (cid:20)Xc

D[Q(vI;c)k P (vI;c)]:

Thus, the learning criterion to be minimized is

G =

C

Xc=1  n
Xi=1

EQ[(cid:0) log P (yi;cjvi;c)] + D [Q(vI;c)k P (vI;c)]! :

It turns out that given the representation of the Q(v c), the criterion and its gradient w.r.t.
(cid:11) can be computed e(cid:14)ciently if the dependence of fIc; b(c); D(c)g on (cid:11) is neglected. Collect
the latter variables in the inner state s. The computation of r(cid:11)G is involved, it is sketched
in Appendix D. Importantly, the time complexity is the same as for conditional inference,
and the computation is actually signi(cid:12)cantly cheaper because software for large matrix
computations can be employed. Furthermore, if the bu(cid:11)ers required for conditional inference
are overwritten, the additional memory requirements are subdominant.

We use a simple double-loop optimization strategy. In the inner loop, we (cid:12)x the inner state
s and perform gradient-based minimization of G using a Quasi-Newton method. In the
outer loop, s is re-selected as described in Section 2. There is no obvious stopping criterion
for the outer loop, so we run for a (cid:12)xed number of outer loop iterations, or until no more
improvement is recorded in the inner loop.

We close this section with a number of comments. First, note that our practice di(cid:11)ers
from most other variational methods in which all of Q is kept (cid:12)xed for an update of (cid:11). Q
depends on both the inner state s and (cid:11), and the latter dependence is strong. If we (cid:12)xed
all of Q during the inner loop, the criterion could not be improved much. In a variant of
our double-loop method we could split s into the active set indices and the site parameters
and recompute the latter more often. However, it is important to note that in the case of
Gaussian likelihoods P (yjv) for the regression model, the site parameters are (cid:12)xed anyway,
so that s contains the active set indices only. In this important special case, both proposals
are the same.

Note that, as opposed to many standard variational methods, the optimization strategy is
not a pure descent method. Re-selection of s can actually lead to an increase in G. The
problem is that \closeness" to the true posterior P (vjy; (cid:11)) is measured in di(cid:11)erent ways in
the variational bound and in our sparse approximation. In other words, the active sets I c
are not selected with the minimization of G in mind. Futhermore, the optimization is not
convex in any sense. The overall process involves the selection of subsets which really is a
discrete optimization. Since our goal is to improve upon a \practically intractable" yet just
cubic scaling, invoking any of the heavy machinery to deal with combinatorial problems
would certainly not be sensible. But even for (cid:12)xed s, the inner loop problem is not convex
in general. Convexity is only obtained in rather unnatural parameterizations of the kernel
matrices whose relevance in practice is unclear. It would be very interesting to (cid:12)nd convex
relaxations of our problem which retain the statistical features to some quanti(cid:12)able extent.

A Notation

In this section we describe the notation used in this paper.

A.1 Matrices and Vectors

Vectors a 2 Rn and matrices A 2 Rn;m are set in boldface. We write a = (ai)i =
(a1; : : : ; an)T and A = (ai;j)i;j for the components. Ordered set subscripts are used to
extract corresponding parts of vectors or matrices: for I (cid:26) f1; : : : ; ng; J (cid:26) f1; : : : ; mg we
have AI;J = (ai;j)i2I;j2J and aI = (ai)i2I . We write short "(cid:1)" for the whole range and i
(instead of fig) for single element sets, i.e. ai = ai; A(cid:1);j = (ai;j)i; A(cid:1);(cid:1) = A, etc. We also
abbreviate BI;I to BI.
The matrices I (cid:1);I and I I;(cid:1) (with (cid:1) = f1; : : : ; ng) should be regarded as distribution and
selection operators respectively. I (cid:1);Ib for b 2 Rd is the vector in Rn with bk at position
ik and 0 elsewhere, if I = fi1; : : : ; idg. Furthermore, I I;(cid:1)a = (ai1; : : : ; aid)T . Note that for
matrices we have I K;(cid:1)AI (cid:1);I = AK;I.

In this paper we need to use matrices and vectors with double indexes (i; c) or (i; p). In order
to be able to present our derivations in a reasonable (cid:13)uid manner, we choose a \lightweight"
notation which may seem unfamiliar and ambiguous to the reader at (cid:12)rst. In general we
treat double indexes as (cid:13)at with i (the index over datapoints) meant to be the inner one,
changing faster than than the index c over classes or p over latent processes. For example,
u = (ui;p)i;p = (u1;1; : : : ; un;1; : : : ; un;P )T . We also write ui = (ui;p)p and up = (ui;p)i, in
this context we will exclusively use i; j as indexes over datapoints, c as index over classes,
and p as index over latent processes.
Subset indexing is \overloaded" to the double index case as follows. If I; J (cid:26) f1; : : : ; ng
and A 2 RnP;nP , then AI;J is short for AI(cid:2)f1;:::;P g;J(cid:2)f1;:::;P g. Thus, selection is done only
on the index over datapoints. However, if an index p or c is used, selection is meant to be
w.r.t. latent process or class. For example, if A 2 RnC;nP , then Ac;p = (a(i;c);(j;p))i;j 2 Rn;n.
The same holds for obvious variants such as p0; c0; p1; c1, etc.

A.2 Other Notation

We need to manipulate Gaussians in complicated ways and make use of the convenient
notation introduced in [8]. Let

N U (xjr; V ) = exp(cid:18)(cid:0)

1
2

xT V x + rT x(cid:19)

denote an unnnormalized Gaussian. Here, V must be symmetric but need not be positive
de(cid:12)nite. If so, we have

N U (xjr; V ) = N(cid:0)xjV (cid:0)1r; V (cid:0)1(cid:1)(cid:12)(cid:12)2(cid:25)V (cid:0)1(cid:12)(cid:12)

1=2

exp(cid:18)1

2

rT V (cid:0)1r(cid:19) :

B Derivations for Representation and Updates

In this section we collect derivations for the representation and the scheme to update it after
inclusions. The subsections are referenced from the main text and are not self-contained.

B.1 The Message mvc!u

The message mvc!u is de(cid:12)ned in Eq. 4. If (cid:22) = ((cid:30)T

c (cid:10) I)u, then
rT (cid:6)r (cid:0)

1
2

(cid:22)T ~K

2

Z (cid:9)v(vc)(cid:9)u!v(vc; u) dvc / exp(cid:18) 1

(cid:22)(cid:19) ;
where r = I (cid:1);Ib(c) + ~K
(cid:0) M (1;c)M (1;c)T (a
collection of useful formulae for manipulating Gaussians can be found in [8], Sect. A.4.3).
Plugging these in, some algebra gives

+ I (cid:1);ID(c)I I;(cid:1))(cid:0)1 = ~K

(cid:22) and (cid:6) = ( ~K

(c)(cid:0)1

(c)(cid:0)1

(c)(cid:0)1

(c)

mvc!u / exp(cid:18)(cid:12)(1;c)T (cid:13) (cid:0)

1
2

(cid:13)T (cid:13)(cid:19) ; (cid:13) = L(1;c)(cid:0)1D(c)1=2(cid:22)I = L(1;c)(cid:0)1D(c)1=2((cid:30)T

c (cid:10)I)I I;(cid:1)u:

With the de(cid:12)nition of P (c) (Eq. 7) we have (cid:13) = P (c)T uI (recall that we use a di(cid:11)erent
component ordering for uI).

B.2 First Update of (cid:22)(c)

; (cid:6)(c)

We have

(cid:16)A(2;c) + V V T(cid:17)(cid:0)1

Thus,

= A(2;c)(cid:0)1 (cid:0) M M T ; M = L(2;c)(cid:0)T W L(cid:0)T ; LLT = I + W T W :

(cid:16)M (2;c)M (2;c)T(cid:17)0

= M (2;c)M (2;c)T (cid:0) ~M ~M

T

;

~M = ((cid:30)T

c (cid:10) I)K (cid:1);I(cid:5)M = M (2;c)W L(cid:0)T 2 Rn;C(cid:0)1

which shows how to update (cid:6)(c). ~M is required explicitly in the subsequent R3(c) update,
the cost is O(n C P d). Furthermore,

((cid:22)(c))0 = ((cid:30)T

= (cid:22)(c) + (cid:22)(c)

(cid:3) ; (cid:22)(c)

c (cid:10) I)K (cid:1);I(cid:5)(cid:16)A(2;c)(cid:0)1 (cid:0) M M T(cid:17) ^K I;I 0(cid:16) ^P
(cid:3) = M (2;c)w (cid:0) ~M M T ^K I;I 0(cid:16) ^P
(nc)(cid:17)0
= L(cid:0)1W T (cid:16)(cid:12)(2;c) + w(cid:17) :

M T ^K I;I 0(cid:16) ^P

(nc) ^(cid:12)

(nc) ^(cid:12)

(nc)(cid:17)0
(nc)(cid:17)0

(nc) ^(cid:12)

and

Using the facts w = W (cid:12)(nc)

(cid:3)

and I + W T W = LLT , some algebra gives

(cid:22)(c)

(cid:3) = ~M L(cid:0)1(cid:16)(cid:12)(cid:3) (cid:0) W T (cid:12)(2;c)(cid:17) :

B.3 Second Part of R3(c) Update
Recall the (cid:6)(c) update from Eq. 15. We need to adjust the R3(c) representation to incorpo-
rate this change and the one of (cid:22)(c). This is done in two steps, (cid:12)rst a positive chollrup for
M (cid:3) and the (cid:22)(c) change, then a negative chollrup for ((cid:30)T
c (cid:10) I)M (cid:3)(cid:3). Recall our convention
that after each of these steps, the new variables x0 overwrite the old ones x.
For the positive update we need Q = L(3;c)(cid:0)1D(c)1=2(M (cid:3))Ic;(cid:1). As for M (3;c), we (cid:12)rst replace
(cid:6)(c)

(cid:1);Ic which amounts to

M (3;c) ! M (3;c) + M (cid:3)QT ;

then drag it along the chollrup in order to replace L(3;c)(cid:0)T by (L(3;c)0)(cid:0)T . As for (cid:12)(3;c), we
update (cid:22)(c)

Ic which amounts to

(cid:12)(3;c) ! (cid:12)(3;c) (cid:0) L(3;c)(cid:0)1D(c)1=2(M (cid:3))Ic;(cid:1)(cid:12)(cid:3);

then drag it along.
For the negative chollrup we need Q = L(3;c)(cid:0)1D(c)1=2((cid:30)T

c (cid:10) I)(M (cid:3)(cid:3))Ic;(cid:1) and replace

M (3;c) ! M (3;c) (cid:0) ((cid:30)T

c (cid:10) I)M (cid:3)(cid:3)QT

before dragging it along. (cid:12)(3;c) is simply dragged along. Both updates are of rank P , so
the individual cost is O(n P dc) (which is also the cost of computing M (cid:3)QT and ((cid:30)T
c (cid:10)
I)M (cid:3)(cid:3)QT ).

C Details for ADF Projection

In this section we provide details for the ADF projection discussed in Section 2.6. Recall
that we need to compute mean and variance of the tilted distribution / P (yjv)N (vjh; a).
Let

Z = E[P (yjv)]; (cid:11) =

log Z;

(cid:23) = (cid:0)

d
dh

d2
dh2 log Z;

where E[(cid:1)] is over N (vjh; a). It is easy to see that mean ^h and variance ^a of the tilted
distribution are given by

^h = h + (cid:11)a;

^a = a(1 (cid:0) a(cid:23)):

Furthermore, the information gain criterion described in Section 2.6 can be computed as

1

(cid:1)inf o
j;c = (cid:0)

2(cid:0)(cid:0) log(1 (cid:0) a(cid:23)) + 1 (cid:0) a(cid:23) (cid:0) 1 + (cid:11)2a(cid:1) :

If the ADF projection is used for an inclusion, the new site parameters can be determined
from

they are given as

N (vj^h; ^a) / N (vjh; a) exp(cid:0)(cid:0)dv2=2 + bv(cid:1) ;

d =

;

b =

(cid:23)

1 (cid:0) a(cid:23)

h(cid:23) + (cid:11)
1 (cid:0) a(cid:23)

:

It is interesting to note that if P (yjv) is (strictly) log-concave, then one can show that log Z
is (strictly) concave in h, so that (cid:23) (cid:21) 0 ((cid:23) > 0). For such a likelihood, d will always be set
to a nonnegative number. In our implementation we reject inclusions if the corresponding
ADF update leads to a very small d.7
For a general smooth likelihood P (yjv), (cid:11) and (cid:23) can be computed by Gauss-Hermite quadra-
ture. For the probit likelihood (Eq. 1), the computations are analytic. We have

Then,

and

Z = Ev(cid:24)N (h;a);n(cid:24)N (0;1)(cid:2)Ifn(cid:20)y(v+(cid:27))g(cid:3) = (cid:8)(z);

z = y(h + (cid:27))s; s =

1

p1 + a

:

(cid:11) =

N (z)
(cid:8)(z)

(cid:23) = (cid:0)ys

ys = exp (log N (z) (cid:0) log (cid:8)(z)) ys
((cid:0)zys (cid:0) (cid:11)) = (cid:11)(cid:18)(cid:11) +
1 + a(cid:19) :

h + (cid:27)

N (z)
(cid:8)(z)

For the Gaussian likelihood P (yju) = N (yju; (cid:27)2), we have Z = N (yjh; a + (cid:27)2), so that

(cid:11) =

y (cid:0) h
a + (cid:27)2 ;

(cid:23) =

1

a + (cid:27)2 :

D Learning Criterion and Gradient

In this section we provide details for the computation of the learning criterion G developed
in Section 3 and its gradient. The complete derivation is lengthy and some details are
ommitted here.
Let G = G1 + G2 with

G1 =Xc;i

EQ[(cid:0) log P (yi;cjvi;c)]; G2 =Xc

G2(c);

G2(c) = D [Q(vI;c)k P (vI;c)] :

7Our selection criterion actually favours updates which lead to larger d.

Recall that vI;c = (vi;c)i2Ic. We have G1 = (cid:0)1T z with

zi;c = EQ[log P (yi;cjvi;c)]; Q(vi;c) = N (hi;c; ai;c); h(c) = (hi;c)i; a(c) = (ai;c)i:

Recall that h(c); a(c) denote marginal means and variances of Q(vc). If

i;c EQ [(log P (yi;cjvi;c)) ~vi;c] ;

ci;c = (cid:0)a(cid:0)1=2
~vi;c = a(cid:0)1=2

i;c

(vi;c (cid:0) hi;c) ;

gi;c =

1

2ai;c (cid:0)zi;c (cid:0) EQ(cid:2)(log P (yi;cjvi;c)) ~v2

i;c(cid:3)(cid:1) ;

then

dzi;c = (cid:0)gi;cdai;c (cid:0) ci;cdhi;c; i:e:
dG1 = gT (da) + cT (dh) =Xc (cid:16)gT

c (cid:16)da(c)(cid:17) + cT

c (cid:16)dh(c)(cid:17)(cid:17) :

(16)

Due to the multi-part representation and the nontrivial message (cid:13)ow, the gradient com-
putation is very challenging. First, we need to (cid:12)nd the general form of dG in terms of
accumulator matrices. If we ignore the parameters ((cid:27)c)c of the likelihood for the moment,
we can infer from the representation that

dG =Xc
+Xc
+Xc

z(1)

c

c

(c)

T (cid:16)d diag ~K
T (cid:16)d ~K
T (cid:16)d ~K

(c)(cid:17) +Xp
(cid:1);Ic(cid:17) +Xp
Ic (cid:17) :

(c)

tr Z (2)

tr Z (5)

c

z(2)
p

tr Z(3)
p

T (cid:16)d diag K (p)(cid:17) + tr Z(1)T (d(cid:8))
T (cid:16)dK (p)
T (cid:16)dK (p)

(cid:1);I(cid:17) +Xc;p

tr Z(4)
c;p

(cid:1);IcnI(cid:17)

(17)

separately, because in every iteration c, all of the Z (5)

The computation starts with a loop over c = 1; : : : ; C in which the accumulator matrices
are computed. Here, the dominant matrices Z (2)
overwrite the bu(cid:11)ers used for M (3;c). We
c
maintain Z (5)
c0 are modi(cid:12)ed, and we
c
cannot do these modi(cid:12)cations on the Z (2)
c0 because they only come available once M (3;c0)
is not required anymore. The gradient is computed in subsequent loops over c and p. In
the following we sketch how the accumulators are updated during a (cid:12)xed iteration c of the
(cid:12)rst loop. Operations marked with ((cid:3)) are done only once, say for c = 1. If p appears, the
operation is done for every p = 1; : : : ; P (if nothing else is said).
From IVM learning in the single process case, we know how to formulate dh(c); da(c) in
terms of d(cid:6)(c)
(cid:1);Ic; d diag (cid:6)(c) and d(cid:22)(c). Thus dG1 can be written as linear expression in these
terms, and the same is true for a part of dG2 as we show now. Our (cid:12)rst goal is to write
dG1 + dG2;part =Xc (cid:16)gT

(cid:1);Ic(cid:17) + f (c)T (cid:16)d(cid:22)(c)(cid:17)(cid:17) :

c (cid:16)d diag (cid:6)(c)(cid:17) + tr ~Z

c (cid:16)d(cid:6)(c)

(18)

T

with ~Z c; f (c) to be determined. We then compute the remaining part of dG2, and (cid:12)nally
deal with the principal part (Eq. 18). Let

(cid:13)(c) = D(c)1=2L(3;c)(cid:0)T (cid:12)(3;c);
f (c) = cc (cid:0) I (cid:1);Ic

(3;c)T

~M

cc

(3;c)

~M

= M (3;c)L(3;c)(cid:0)1D(c)1=2;

(19)

where we overwrite M (3;c) by ~M

(3;c)

. We have

dM (3;c)M (3;c)T = H(cid:16)d(cid:6)(c)
dM (3;c)(cid:12)(3;c) = H(cid:16)d(cid:6)(c)

+ ~M

(cid:1);Ic(cid:17) ~M
(cid:1);Ic(cid:17) (cid:13)(c) + (H (cid:0) I)(cid:16)d(cid:22)(c)(cid:17) ; H = I (cid:0) ~M

(cid:1);Ic(cid:17)T
(3;c)(cid:16)d(cid:6)(c)

(3;c) T

;

therefore

(3;c)

I Ic;(cid:1);

dh(c) = H(cid:16)d(cid:22)(c)(cid:17) + H(cid:16)d(cid:6)(c)
da(c) =(cid:16)d diag (cid:6)(c)(cid:17) (cid:0) diag (I + H )(cid:16)d(cid:6)(c)

(cid:1);Ic(cid:17) (cid:13)(c);

(cid:1);Ic(cid:17) ~M

Noting that f (c) = H T cc and gT

c diag B = tr(diag gc)B, we have

(3;c) T

:

~Z c = (cid:0)2(diag gc) ~M

(3;c)

+ I (cid:1);Ic

~M

(3;c)T

(diag gc) ~M

(3;c)

+ f (c)(cid:13)(c)T

(20)

which overwrites ~M

(3;c)

. The cost is O(n d2

c) for each c. As for G2(c), let

T (c) = ~K

(c)
Ic + ((cid:30)T

c (cid:10) I)K Ic((cid:30)c (cid:10) I) 2 Rdc;dc

and denote

Ic (cid:0) M (3;c)
We have that P (vI;c) = N (0; T (c)) and Q(vI;c) = N (h(c)
we have

A(c) = EQ[vI;c] = (cid:6)(c)

Ic;(cid:1)

T :

Ic;(cid:1) M (3;c)
Ic ; A(c)). If R(c) = T (c)(cid:0)1A(c), then

G2(c) =

1

2(cid:16)(cid:0) log jR(c)j + tr R(c) (cid:0) dc + h(c)

Ic

T T (c)(cid:0)1h(c)

Ic (cid:17) :

We make use of the Cholesky decomposition of T (c) to compute expressions involving T (c)(cid:0)1.
We have

and

d log jR(c)j = tr A(c)(cid:0)1(cid:16)dA(c)(cid:17) (cid:0) tr T (c)(cid:0)1(cid:16)dT (c)(cid:17) ;
d tr R(c) = (cid:0) tr T (c)(cid:0)1A(c)T (c)(cid:0)1(cid:16)dT (c)(cid:17) + tr T (c)(cid:0)1(cid:16)dA(c)(cid:17)

dT (c) = d ~K

(c)

Ic +Xp

(cid:30)2

c;p(cid:16)dK (p)

Ic (cid:17) + 2Xp

(cid:30)c;pK (p)

Ic (d(cid:30)c;p):

And with t(c) = T (c)(cid:0)1h(c)

Ic we have

dh(c)
Ic

therefore

T T (c)(cid:0)1h(c)

Ic = 2t(c)T (cid:16)dh(c)

Ic (cid:17) (cid:0) t(c)T (cid:16)dT (c)(cid:17) t(c);

Ic (cid:17) ;
dG2(c) = tr J (1;c)T (cid:16)dA(c)(cid:17) + tr J (2;c)T (cid:16)dT (c)(cid:17) + t(c)T (cid:16)dh(c)

1

1

J (1;c) =

Also,

2(cid:16)T (c)(cid:0)1 (cid:0) A(c)(cid:0)1(cid:17) ; J (2;c) =
dA(c) =(cid:16)d(cid:6)(c)

Ic (cid:17) (cid:0) 2 sym ~M

(3;c)

2(cid:16)T (c)(cid:0)1 (cid:0) T (c)(cid:0)1A(c)T (c)(cid:0)1 (cid:0) t(c)t(c)T(cid:17) :
Ic (cid:17) + ~M

Ic;(cid:1) (cid:16)d(cid:6)(c)

Ic (cid:17) ~M

(3;c)T
Ic;(cid:1)

(3;c)

:

Ic;(cid:1) (cid:16)d(cid:6)(c)

We can incorporate the dh(c)
the computation of f (c) (Eq. 19) and ~Z c (Eq. 20). The dA(c) is then incorporated by

Ic part by replacing cc in Eq. 16 by cc + I (cid:1);Ict(c) which a(cid:11)ects

~Z c   ~Z c + I (cid:1);Ic(cid:16) ~M

(3;c)

Ic;(cid:1) (cid:0) I(cid:17)T

J (1;c)(cid:16) ~M

(3;c)

Ic;(cid:1) (cid:0) I(cid:17) :

The dT (c) part results in the following direct gradient contributions:

Z(5)

c + = J (2;c); Z (3)

p + = (cid:30)2

c;pI (cid:1);IcJ (2;c)

(cid:1);1:::d; Z (4)

c;p+ = (cid:30)2

c;pI (cid:1);IcJ (2;c)

(cid:1);d+1:::dc;

Z(1)+ =(cid:16)2(cid:30)c;p tr J (2;c)K(p)

Ic (cid:17)c;p

:

Denote the parts in Eq. 18 by dG1;1(c); dG1;2(c); dG1;3(c) respectively. Let

(21)

(cid:6)(c) = ~(cid:6)

(c)

+ M (2;c)M (2;c)T ;

(c)

~(cid:6)

= ~K

(c)

+ ((cid:30)T

c (cid:10) I)(cid:16)K (cid:0) M (4)M (4)T(cid:17) ((cid:30)c (cid:10) I):

(cid:22)(c) does not depend on ~(cid:6)

(c)

. Consider only the ~(cid:6)

(c)

variation for the moment. Let

(4)

~M

= M (4)L(4)(cid:0)1 = diag(cid:16)M (4)

p L(4)
p

(cid:0)1(cid:17)p

which overwrites M (4) once not needed anymore. The cost is O(n P d2). From dG1;1(c) we
have

(cid:30)2
c;pgc;

z(1)

c + = gc;

z(2)
p + = ~g p ((cid:3));

~g p =Xc
Z (1)+ =(cid:18)2(cid:30)c;p(cid:16)diag K (p) (cid:0) diag M (4)
p + = (cid:0)2(diag ~g p) ~M
Z (3)

(4)
p + I (cid:1);I ~M

(4)T
p

p M (4)
p

T(cid:17)T

;

gc(cid:19)c;p
((cid:3));

(4)
p

(diag ~g p) ~M

and from dG1;2(c) we have

Z (2)

c + = ~Z c; Z(4)

c;p+ = (cid:30)2

T

Z (1)+ =(cid:16)2(cid:30)c;p tr ~Z

c (cid:16)K (p)
c;p(cid:18)( ~Z c)(cid:1);1:::d (cid:0) ~Z c

p + = (cid:30)2

Z (3)

c;p( ~Z c)(cid:1);d+1:::dc;
p M (4)
(cid:1);Ic (cid:0) M (4)

Ic;p

~M

(4)

Ic;p (cid:0) I (cid:1);Ic

T(cid:17)(cid:17)c;p

~Z

T
c

~M

;

(4)

p + I (cid:1);I(cid:16) ~Z

T
c

~M

(4)

p (cid:17)T

~M

(4)

Ic;p(cid:19) :

(22)

(23)

Here and elsewhere, if (say) M (4) = (mi;(j;p))i;(j;p), then M (4)
Z (2)
c
the ~Z c in the c loop and do the d ~(cid:6)

Ic;p = (mi;(j;p))i2Ic;j 2 Rdc;d.
accumulator is accessed only here, so it is easiest to compute
updates in one batch after this loop. The total

overwrites ~Z c. The Z(2)
c

cost of the batch (given ~Z c) is O(n P d Pc dc). Note that we need the kernel matrix K (p)

(cid:1);Ic
here. Except in cases where kernel evaluations are very expensive, these matrices should be
recomputed here on the (cid:13)y in order to save memory (part of the kernel matrix is required
in Eq. 24 as well).
Next the dM (2;c)M (2;c)T part. Since

(c)

d((cid:30)T

c (cid:10) I)K (cid:1);I =(cid:0)d(cid:30)T

c (cid:10) I(cid:1) K (cid:1);I + ((cid:30)T

c (cid:10) I) (dK (cid:1);I) ;

we see that for a d((cid:30)T
(tr BT K(p)

c (cid:10) I)K (cid:1);I part, a contribution Z (3)

p + = (cid:30)c;pB is paired with Z (1)+ =

(cid:1);I )c;p, so we only need to deal with the dK (cid:1);I part explicitly. Let

(2;c)

~M

= M (2;c)L(2;c)(cid:0)1(cid:5)T 2 Rn;P d

which overwrites M (2;c), the cost is O(n P 2 d2). Then,

dM (2;c)M (2;c)T =Xp

2(cid:30)c;p sym(cid:16)dK (p)

(cid:1);I(cid:17) ~M

(2;c)T
p

(cid:0) ~M

(2;c)(cid:16)d(cid:5)A(2;c)(cid:5)T(cid:17) ~M

(2;c)T

(ignoring d(cid:30)c). We deal with the dA(2;c) part later. Let

Sc = ~M

(2;c)T

(diag gc) ~M

(2;c)

F (c) = ~Z

T
c

~M

(2;c)

2 Rdc;P d;

=(cid:16)S(p;p0)

c

(cid:17)p;p0 2 RP d;P d;

which comes at cost O(n P 2 d2) for Sc and O(n P d dc) for F (c). The contribution of dA(2;c)
(through dG1;1(c); dG1;2(c)) is

dG1;1(c) + dG1;2(c) = tr(cid:16)(cid:0)Sc (cid:0) ~M

Ignoring dA(2;c) we have the contributions

(2;c)T

Ic;(cid:1) F (c)(cid:17)(cid:16)d(cid:5)A(2;c)(cid:5)T(cid:17) :

Z (3)

p + = (cid:30)c;pB; B = 2(diag gc) ~M

(2;c)
p + ~Z c

~M

(2;c)
Ic;p + I (cid:1);IcF (c)
p ;

Z (1)+ =(cid:16)tr BT K(p)

(cid:1);I(cid:17)c;p

(24)

at cost O(n P d dc). The relationship through B may be exploited in an implementation. B
occurs as intermediate in the computation of S c.
Next we look at dG1;3(c). If

then

"(c) = (cid:5)L(2;c)(cid:0)T (cid:12)(2;c) 2 RP d;

d(cid:22)(c) =(cid:0)d((cid:30)T

c (cid:10) I)K (cid:1);I(cid:1) "(c) (cid:0) ~M

Again, dA(2;c) is dealt with later. If

(2;c)(cid:16)d(cid:5)A(2;c)(cid:5)T(cid:17) "(c) + ~M

(2;c)(cid:16)dK I(cid:5) ^P

(nc) ^(cid:12)

(nc)(cid:17) :

(25)

q(c) = ~M

(2;c)T

f (c) 2 RP d;

the contribution is

For the (cid:12)rst part in Eq. 25 we have

dG1;3(c) = tr(cid:16)(cid:0)"(c)q(c)T(cid:17)(cid:16)d(cid:5)A(2;c)(cid:5)T(cid:17) :

Z(3)

p + = (cid:30)c;pB; B = f (c)"(c)
p

T ; Z (1)+ =(cid:16)tr BT K (p)

(cid:1);I(cid:17)c;p

;

(26)

where again we use the pairing of Z (3)
p and Z(1) contributions. Note that this contribution
can be fused with Eq. 24 by adding the corresponding B matrices. For the third part in
Eq. 25 we note that in ^P

, the rows are in uI ordering, so that

(nc)

(cid:5) ^P

(nc) ^(cid:12)

(nc)

((cid:30)c0 (cid:10) I)E(c0)

1:::d;(cid:1)(cid:12)(1;c0):

= Xc06=c

De(cid:12)ne

B(c) = E(c)

1:::d;(cid:1)E(c)T 2 Rd;dc; (cid:20)(c) = E(c)(cid:12)(1;c) 2 Rdc:

Then, dK I gives the contribution

Z(3)

p + = I (cid:1);Iq(c)

p 0
@Xc06=c

T

:

1:::d1
(cid:30)c0;p(cid:20)(c0)
A

The contribution for d(cid:5) ^P

(nc) ^(cid:12)

(nc)

makes use of Eq. 27:

p

T K(p)

I (cid:20)(c0)

Z (1)+ =(cid:16)q(c)
c0 + = (cid:0)B(c0)T  Xp

Z (5)

1:::d(cid:17)c06=c;p

;

(cid:30)c0;pK (p)

I q(c)

p ! (cid:20)(c0)T! (c0 6= c):

(27)

(28)

(29)

Here, only the rows c0 6= c of Z(1) are updated.
It remains to deal with the dA(2;c) part which is

dG1(c) = tr U (c)(cid:16)d(cid:5)A(2;c)(cid:5)T(cid:17) ; U (c) = (cid:0)Sc (cid:0) ~M

Here, U (c) can be replaced by sym U (c) which we do. We have

(2;c)T

Ic;(cid:1) F (c) (cid:0) "(c)q(c)T 2 RP d;P d:

d(cid:5)A(2;c)(cid:5)T = (dK I) + 2 sym (dK I) (cid:5) ^P

(nc) ^P

(nc)T

If

then ignoring d ^P

(nc) ^P

Q(c) = U (c)K I(cid:5) ^P
(nc)T

, the contribution is

(nc) ^P

(nc)T

(cid:5)T KI + K I(cid:16)d(cid:5) ^P
(cid:5)T 2 RP d;P d;

(nc) ^P

(nc)T

(cid:5)T(cid:17) K I:

where we used that U (c) is symmetric. Recall that U (c)
(p; p) in U (c). The remaining term is

Z (3)

p + = I (cid:1);I(cid:16)U (c)

p + 2Q(c)

p (cid:17) ;
p denotes the d (cid:2) d block at position

(30)

and

d(cid:5) ^P

(nc) ^P

(nc)T

(cid:5)T(cid:17) ;

(nc)T

(nc) ^P

tr K I U (c)KI(cid:16)d(cid:5) ^P
(cid:5)T = Xc06=c(cid:16)2 sym (d(cid:30)c0 (cid:10) I) B(c0)
(cid:0) ((cid:30)c0 (cid:10) I)B(c0)(cid:16)d ~K

(cid:1);1:::d((cid:30)T

c0 (cid:10) I)
Ic0 (cid:17) B(c0)T ((cid:30)T

(c0)

c0 (cid:10) I)(cid:17):

If

W (c) = K I((cid:30)c (cid:10) I)B(c) =(cid:16)(cid:30)c;pK(p)

I B(c)(cid:17)p 2 RP d;dc;

then the contribution is

Z (5)

c0 + = (cid:0)W (c0)T U (c)W (c0); Z(1)+ =(cid:16)2 tr W (c0)

(cid:1);1:::d

T U (c)

(cid:1);p K (p)

I (cid:17)c06=c;p

:

(31)

This completes the description of dG w.r.t. parameters of the covariance functions. The
computational cost is estimated under the assumption that n (cid:29) dc for all c and P < C.
The dominating operations are the computation of ~M
c), the
and Sc at O(n C P 2 d2), and the computation of the ~(cid:6) batch and
computation of ~M

and ~Z c at O(n Pc d2

(3;c)

(2;c)

of the F (c) at O(n P d Pc dc). Thus, the complexity is
c + P d  C P d +Xc

O n  Xc

d2

dc!!!

which is the same as cost as computing the representation without selecting the active sets
(see Section 2.5.5).
The gradient w.r.t. likelihood parameters depends on the details. For example, if the P (y cjvc)
have intercept parameters (cid:27)c in the sense that P (ycjvc) = f (yc; vc + (cid:27)c), it is easy to see
that

dG1 = Xi

If P (ycjvc) = N (ycjvc; (cid:27)2

c ), then

ci!T

d(cid:27):

dG1 =Xc;i

1

2(cid:18)1 (cid:0)

ai;c + (yi;c (cid:0) hi;c)2

(cid:27)2
c

c d(cid:27)2
c :

(cid:19) (cid:27)(cid:0)2

(32)

We close the Section providing some further details for the gradient computation in our
implementation. First of all, it is actually not sensible to store the accumulators Z (4)
c;p 2
Rn;dc(cid:0)d which would come at a total memory cost of O(n P Pc dc). Fortunately, the Z (4)

c;p
are updated in Eq. 23 and Eq. 21 only, so it is easy to compute the gradient contributions
on the (cid:13)y. In general we separate accumulation from the (cid:12)nal traces with kernel derivative
matrices for the sake of a simple implementation.

D.1 Derivative w.r.t. (cid:27)

2
c for Regression Model

If the SLFM is used for regression with a Gaussian noise model P (ycjvc) = N (ycjvc; (cid:27)2
c ) the
site parameters b(c); D(c) depend on ((cid:27)2
c )c and the (cid:12)xed target values y only. In this case
we can refrain from (cid:12)xing the site parameters during the inner loop optimization, so that
only the active set indicators Ic are kept (cid:12)xed. In this Section we compute the derivative of
G w.r.t. the noise variances.
Denote lc = log (cid:27)2
so that

c . We accumulate @G=@lc in zc. Note that D(c) = (cid:27)(cid:0)2

c I and b(c) = (cid:27)(cid:0)2

c y(c)
Ic ,

dD(c)1=2 = (cid:0)

D(c)1=2(dlc);

dD(c)(cid:0)1=2b(c) = (cid:0)

D(c)(cid:0)1=2b(c)(dlc):

1
2

1
2

In the following we make use of these simple relationships, furthermore of the fact that
D(c) / I and therefore commutes with all matrices. We also concentrate exclusively on the
dlc terms.

Some algebra gives

dM (3;c)M (3;c)T = (cid:0)(cid:27)2

c

~M

(3;c) ~M

(3;c) T

(dlc);

dM (3;c)(cid:12)(3;c) = (cid:0)(cid:27)2

c

~M

(3;c)

(cid:13)(c);

so Eq. 16 results in the contribution

zc+ = (cid:27)2

c(cid:16)tr ~M

(3;c) T

(diag gc) ~M

(3;c)

(3;c)

~M

(cid:0) cT

c

(cid:13)(c)(cid:17) :

Contributions from dG2 are through dA(c); dh(c)
Ic :
(3;c) T
Ic;(cid:1) J (1;c) ~M

zc+ = (cid:27)2

c(cid:16)tr ~M

(3;c)

Ic;(cid:1) (cid:0) t(c)T ~M

(3;c)

Ic;(cid:1) (cid:13)(c)(cid:17) :

The next contribution is through d ^P
in the contribution

(nc) ^(cid:12)

(nc)

. We have d(cid:20)(c)

c B(c)(cid:20)(c)(dlc), resulting

zc0+ =0
@(cid:0)(cid:27)2

c0 Xp

(cid:30)c0;pK (p)

I q(c)

p !T

Finally for the d ^P

(nc) ^P

(nc)T

part we note that

1;:::;d = (cid:0)(cid:27)2
B(c0)(cid:20)(c0)1

A ; c0 6= c:

dE(c)

1;:::;d;(cid:1)E(c)

1;:::;d;(cid:1)

T = (cid:0)(cid:27)2

c B(c)B(c)T ;

resulting in the contribution

zc0+ =(cid:16)(cid:0)(cid:27)2

c0 tr W (c0)T U (c)W (c0)(cid:17) ; c0 6= c:

Note all of the zc accumulations use terms which have already been computed for the main
gradient. The contribution through the likelihood factors (Eq. 32) has to be added to zc as
well.

