ABSTRACT

In this paper we compare three frameworks for discriminative train-
ing of continuous-density hidden Markov models (CD-HMMs). Spe-
cically, we compare two popular frameworks, based on conditional
maximum likelihood (CML) and minimum classication error (MCE),
to a new framework based on margin maximization. Unlike CML
and MCE, our formulation of large margin training explicitly penal-
izes incorrect decodings by an amount proportional to the number
of mislabeled hidden states. It also leads to a convex optimization
over the parameter space of CD-HMMs, thus avoiding the problem
of spurious local minima. We used discriminatively trained CD-
HMMs from all three frameworks to build phonetic recognizers on
the TIMIT speech corpus. The different recognizers employed ex-
actly the same acoustic front end and hidden state space, thus en-
abling us to isolate the effect of different cost functions, parameter-
izations, and numerical optimizations. Experimentally, we nd that
our framework for large margin training yields signicantly lower
error rates than both CML and MCE training.

Index Terms speech recognition, discriminative training, MMI,

MCE, large margin, phoneme recognition

1. INTRODUCTION

Most modern speech recognizers are built from continuous-density
hidden Markov models (CD-HMMs). The hidden states in these CD-
HMMs model different phonemes or sub-phonetic elements, while
the observations model cepstral feature vectors. Distributions of cep-
stral feature vectors are most often represented by Gaussian mixture
models (GMMs). The accuracy of the recognizer depends critically
on the careful estimation of GMM parameters.

In this paper, we present a systematic comparison of several
leading frameworks for parameter estimation in CD-HMMs. These
frameworks include a recently proposed scheme based on the goal
of margin maximization [1, 2], an idea that has been widely applied
in the eld of machine learning. We compare the objective func-
tion and learning algorithm in this framework for large margin train-
ing to those of other traditional approaches for parameter estimation
in CD-HMMs. The most basic of these traditional approaches in-
volves maximum likelihood (ML) estimation. Mainly, however, we
focus on competing discriminative methods in which parameters are

Part of the work was performed at University of Pennsylvania.
This work is supported by the National Science Foundation under Grant
Number 0238323 and the UCSD FWGrid Project, NSF Research Infrastruc-
ture Grant number NSF EIA-0303622.

estimated directly to maximize the conditional likelihood [3, 4] or
minimize the classication error rate [5]. Though not as straightfor-
ward to implement as ML estimation, discriminative methods yield
much lower error rates on most tasks in automatic speech recogni-
tion (ASR).

We investigate salient differences between CML, MCE, and large
margin training through carefully designed experiments on the TIMIT
speech corpus [6]. Though much smaller than typical corpora used
for large vocabulary ASR, the TIMIT corpus provides an apt bench-
mark for evaluating the intrinsic merits of different frameworks for
discriminative training. We compare the phonetic error rates on the
TIMIT corpus from multiple systems trained with different param-
eterizations, initial conditions, and learning algorithms. All other
aspects of these systems, however, were held xed. In particular, the
different systems employed exactly the same acoustic front end and
model architectures (e.g., monophone CD-HMMs with full Gaussian
covariance matrices). From the results of these experiments, we are
able to tease apart the signicant factors that differentiate competing
methods for discriminative training.

The paper is organized as follows. In section 2, we review CD-
HMMs as well as several different methods for parameter estimation,
including our own recent formulation of large margin training [1, 2].
In section 3, we compare the performance of phonetic recognizers
trained in all these different frameworks. Finally, in section 4, we
conclude with a brief discussion of future directions for research.

2. PARAMETER ESTIMATION IN HMMS

CD-HMMs dene a joint probability distribution over a hidden state
sequence S = {s1, s2, . . . , sT} and an observed output sequence
X = {x1, x2, . . . , xT}, given by

log p(X, S) =

[log p(st|st1) + log p(xt|st)] .

(1)

X

t

For ASR, the hidden states st and observed outputs xt denote pho-
netic labels and acoustic feature vectors, respectively, and the distri-
butions p(xt|st) are typically modeled by multivariate GMMs:

MX

p(xt|st = j) =

jmN (xt; jm, jm).

(2)

m=1

In eq. (2), we have used N (x; , ) to denote the Gaussian dis-
tribution with mean vector  and covariance matrix , while the
constant M denotes the number of mixture components per GMM.
The mixture weights jm in eq. (2) are constrained to be nonnega-

tive and normalized:P

m jm = 1 for all states j.

Let  denote all the model parameters including transition prob-
abilities, mixture weights, mean vectors, and covariance matrices.
The goal of parameter estimation in CD-HMMs is to compute the
optimal  (with respect to a particular measure of optimality), given
N pairs of observation and target label sequences {Xn, Yn}N

In what follows, we review the optimizations for well-known
frameworks based on maximum likelihood (ML), conditional max-
imum likelihood (CML), and minimum classication error (MCE).
We also review our most recently proposed framework for large mar-
gin training [2].

n=1.

X

2.1. Maximum likelihood estimation
The simplest approach to parameter estimation in CD-HMMs maxi-
mizes the joint likelihood of output and label sequences. The corre-
sponding estimator is given by

ML = arg max

log p(Xn, Yn)

(3)



n

For transition probabilities, ML estimates in this setting are obtained
from simple counts (assuming the training corpus provides phonetic
label sequences). For GMM parameters, the EM algorithm provides
iterative update rules that converge monotonically to local stationary
points of the likelihood. The main attraction of the EM algorithm is
that no free parameters need to be tuned for its convergence.

2.2. Conditional maximum likelihood
CD-HMMs provide transcriptions of unlabeled speech by inferring
the hidden label sequence Y with the highest posterior probability:
Y = arg maxS p(S|X). The CML estimator in CD-HMMs directly
attempts to maximize the probability that this inference returns the
correct transcription. Thus, it optimizes the conditional likelihood:

CML = arg max

log p(Yn|Xn).

(4)

X



n

In CML training, the parameters must be adjusted to increase the
likelihood gap between correct labelings Yn and incorrect labelings S.
This can be seen more explicitly by rewriting eq. (4) as:

"

X

#

between the correct labeling and all competing labelings. Unlike
CML training, however, the size of the gap in eq. (7) does not matter,
as long as it is nite.

The nondifferentiability of the sign and max functions in eq. (7)
makes it difcult to minimize the misclassication error directly.
Thus, MCE training [9] adopts the surrogate cost function:

Nerr X



n

0B@ log p(Xn, Yn) + log

24 1

C

X

S6=Yn

35 1



1CA .

e log p(Xn,S)

(8)
In this approximation, a sigmoid function (z) = (1 + ez)1 re-
places the sign function sign[z], and a softmax function (parameter-
ized by ) replaces the original max. The parameters  and  in this
approximation must be set by heuristics. The sum in the second term
is taken over the top C competing label sequences.

2.4. Large margin training
Recently, we proposed a new framework for discriminative train-
ing of CD-HMMS based on the idea of margin maximization [1, 2].
Our framework has two salient features: (i) it attempts to separate
the likelihoods of correct versus incorrect label sequences by mar-
gins proportional to the number of mislabeled states [10]; (ii) the
required optimization is convex, thus avoiding the pitfall of spurious
local minima. These features also distinguish our approach to large
margin training of CD-HMMs from other recent formulations [11].
We start by reviewing the discriminant functions in large margin
CD-HMMs [1, 2]. These parameterized functions of observations X
and states S take a form analogous to the log-probability in eq. (1).
In particular, we dene
D(X, S) =

[(st1, st) + (xt, st)]

X

(9)

t

in terms of state-state transition scores (st1, st) and state-output
emission scores (xt, st). Unlike eq. (1), however, eq. (9) does
not assume that the transition scores (st1, st) are derived from
the logarithm of normalized probabilities. Likewise, the emission
scores (xt, st) in eq. (9) are parameterized by sums of unnormal-
ized Gaussian distributions:

CML = arg max



log p(Xn, Yn)  log

p(Xn, S)

.

(5)

(xt, st = j) = log

S

m

(xtjm)T
e

1
jm(xtjm)jm ,

(10)

X

The CML estimator in eq. (4) is closely related to the maximum
mutual information (MMI) estimator [7, 8], given by:

MMI = arg max



n

log

p(Xn, Yn)
p(Xn)p(Yn)

.

(6)

Note that eqs. (4) and (6) yield identical estimators in the setting
where the (language model) probabilities p(Yn) are held xed.

2.3. Minimum classication error
MCE training is based on minimizing the number of sequence mis-
classications. The number of such misclassications is given by:

Nerr =

sign [ log p(Xn, Yn) + max
S6=Yn

log p(Xn, S)]

(7)

X

n

where the nonnegative scalar parameter jm  0 is entirely inde-
pendent of jm (as opposed to being related to its log-determinant).
To obtain a convex optimization for large margin training, we
further reparameterize the emission score in eq. (10). In particular,
we express each mixture components parameters {jm, jm, jm}
as elements of the following matrix:

jm =

1
jm
T

jm

1
jm



1
jm jm
1
jmjm + jm

T

jm

.

(11)

 



Our framework for large margin training optimizes the matrices jm,
as opposed to the conventional GMM parameters {jm, jm, jm}.
Since the matrix jm is positive denite and the scalar jm is non-
negative, we also require the matrix jm to be positive semidenite
(as denoted by the constraint jm (cid:31) 0). With this reparameteriza-
tion, the emission score in eq. (10) can be written as:

where sign[z] = 1 for z > 0 and sign[z] = 0 for z  0. To minimize
eq. (7), the parameters must be adjusted to maintain a likelihood gap

(xt, st = j) = log

zT
e

t jmzt where zt =

. (12)

X

X

m



 xt

1

Note that this score is convex in the elements of the matrices jm.
For large margin training of CD-HMMs, we seek parameters
that separate the discriminant functions for correct and incorrect la-
bel sequences. Specically, for each joint observation-label sequence
(Xn, Yn) in the training set, we seek parameters such that:
D(Xn, Yn)  D(Xn, S)  H(Yn, S),  S 6= Yn

(13)
where H(Yn, S) denotes the Hamming distance between the two
label sequences [10]. Note how this constraint requires the log-
likelihood gap between the target sequence Yn and each incorrect
decoding S to scale in proportion to the number of mislabeled states.
Eq. (13) actually species an exponentially large number of con-
straints, one for each alternative label sequence S. We can fold all
these constraints into a single constraint by writing:

D(Xn, Yn) + max
S6=Yn

{H(Yn, S) + D(Xn, S)}  0.

(14)

In the same spirit as the MCE derivation for eq. (8), we obtain a
more tractable (i.e., differentiable) expression by replacing the max
function in eq. (14) with a softmax upper bound:

D(Xn, Yn) + log

H(Yn,S)+D(Xn,S)  0.
e

(15)

X

S6=Yn

The exponential terms in eq. (15) can be summed efciently using a
modication of the standard forward-backward procedure.
While we would like to nd parameters that satisfy the large
margin constraint in eq. (15) for all training sequences {Xn, Yn}N
n=1,
in general this is not possible. For such infeasible scenarios, we
instead compute the parameters that minimize the total amount by
which these constraints are violated:

X

n

min

24D(Xn, Yn) + log

X

S6=Yn

H(Yn,S)+D(Xn,S)
e

35

. (16)

+

The + subscript in eq. (16) denotes the hinge function: [z]+ = z
if z > 0 and [z]+ = 0 if z  0. The optimization of eq. (16) is per-
formed subject to the positive semidenite constraints jm (cid:31) 0. We
can further simplify the optimization by assuming that each emission
score (xt, yt) in the rst term is dominated by the contribution
from a single (pre-specied) Gaussian mixture component. In this
case, the overall optimization is convex; see [2] for further details.

3. EMPIRICAL COMPARISON OF METHODS

We experimented with the discriminative frameworks in section 2 (as
well as several variants of these frameworks) to explore the effects
of different parameterizations, initializations, and cost functions.

3.1. Setup
CD-HMMs were evaluated on the task of phonetic recognition [12]
namely, mapping speech utterances to sequences of phonemes, as
opposed to higher-level units, such as words. Phonetic label se-
quences of test utterances were inferred using Viterbi decoding. Note
that for the Viterbi decoding of test utterances, we did not make any
use of manually time-aligned phonetic transcriptions; in particular,
we did not assume that the boundaries between phonetic segments
were correctly located prior to decoding. This distinguishes the task
of phonetic recognition, considered in this paper, from the simpler
task of phonetic classication [13] considered in our earlier work [1].

M
1
2
4
8

CML MCE Margin
ML
40.1% 36.4% 35.6% 31.2%
36.5% 34.6% 34.5% 30.8%
34.7% 32.8% 32.4% 29.8%
32.7% 31.5% 30.9% 28.2%

Table 1. Phonetic error rates from differently trained CD-HMMs,
with M mixture components per GMM. See text for details.

For each test utterance, Viterbi decoding yielded a frame-by-frame
phonetic transcription with one label for each analysis window of
speech. We matched the label sequences from Viterbi decoding
against ground truth phonetic transcriptions (obtained manually) and
used dynamic programming to compute the minimum string edit dis-
tance for each utterance. We report error rates as the number of in-
sertion, deletion, and substitution errors over the entire corpus, nor-
malized by the total string length.

All CD-HMMs were trained and tested on utterances from the
TIMIT speech corpus [6]. We followed standard practices in prepar-
ing the training, development, and test sets [12, 13]. Our recogniz-
ers employed standard front ends and model architectures. Acous-
tic feature vectors were derived from 13-dimensional mel-frequency
cepstral coefcients (MFCCs) and their rst and second derivatives.
MFCCs were computed on 25 ms analysis windows with 10 ms of
overlap between consecutive windows. Each CD-HMM recognizer
had 48 states, one for each of 48 broad phonetic categories and tran-
scription markers (e.g., silence). For evaluation, these 48 labels were
further simplied to 39 phonetic categories, following the conven-
tion in [12]. To vary the model size, we experimented with differ-
ent numbers of mixture components per state, ranging from 1 to 8 in
each GMM. For each mixture component, we estimated a full covari-
ance matrix from the training corpus; there was no parameter-tying
across different states or mixture components.

We used gradient-based numerical optimizations for CML, MCE,
and large margin training. For CML training, we used conjugate gra-
dient descent; for MCE training, we used steepest gradient descent
(which worked better); for margin maximization, we used a combi-
nation of conjugate gradient and projected subgradient descent, as
described in previous work [1, 2]. For CML training, we obtained
competitive results from conjugate gradient descent and did not ex-
periment with the extended Baum-Welch algorithm [14].

3.2. Experimental results
Table 1 shows the error rates of different CD-HMMs trained by
ML, CML, MCE, and margin maximization. Here, M denotes the
number of mixture components per state (in each GMM). As ex-
pected, all the discriminatively trained CD-HMMs yield signicant
improvements over the baseline CD-HMMs trained by ML. On this
particular task, the results show that MCE does slightly better than
CML, while the largest relative improvements are obtained by large
margin training (by a factor of two or more). Using MMI on this
task, Kapadia et al [14] reported larger relative reductions in error
rates than we have observed for CML (though not better performance
in absolute terms). It is difcult to compare our ndings directly to
theirs, however, since their ML and MMI recognizers used different
front ends and numerical optimizations than those in our work.

Several possible factors might explain the better performance of
CD-HMMs trained by margin maximization. These include: (i) the
relaxation of Gaussian normalization constraints by the parameteri-
zation in eq. (11), yielding more exible models, (ii) the convexity
of our margin-based cost function eq. (16), which ensures that its

m CML
36.4%
1
34.6%
2
32.8%
4
8
31.5%

Unnormalized Reinitialized Reweighted

36.0%
36.3%
33.6%
31.6%

32.6%
31.7%
31.2%
28.9%

33.6%
32.8%
32.8%
31.0%

Table 2. Phonetic error rates from CD-HMMs trained by CML and
three variants of CML. See text for details.

optimization (unlike those for CML and MCE) does not suffer from
spurious local minima, and (iii) the closer tracking of phonetic error
rates by the margin-based cost function, which penalizes incorrect
decodings in direct proportion to their Hamming distance from the
target label sequence. We conducted several experiments with vari-
ants of CML and MCE training in an attempt to determine which of
these factors (if any) played a signicant role.

Some preliminary results are reported in Table 2, for CD-HMMs
trained by three variants of CML. In the rst variant, we relaxed the
normalization constraints on the mixture weights and log-determinant
prefactors of the GMMs. In the second variant, we initialized the
GMMs from a different starting point; in particular, instead of base-
line GMMs trained by ML estimation, we used large margin GMMs
that had been trained for segment-based phonetic classication [1].
Finally, in the third variant, we maximized a reweighted version of
the conditional likelihood:

log p(Xn, Yn)  log

H(Yn,S)+log p(Xn,S).
e

(17)

X

max

X

n

S

The reweighting in eq. (17) penalizes incorrect decodings in pro-
portion to their Hamming distance from the target label sequence,
analogous to the cost function of eq. (16) for large margin training.
The experimental results on these variants of CML training re-
veal several interesting ndings. First, the unnormalized GMMs per-
formed slightly worse than the normalized GMMs, possibly due to
overtting.
It seems that in the absence of margin-based criteria,
the extra degrees of freedom in unnormalized GMMs help to maxi-
mize the conditional likelihood in ways that are not correlated with
the phonetic error rate. Second, with better initializations, the CD-
HMMs trained by CML approached the performance of CD-HMMs
trained by margin maximization. This result highlights a signicant
drawback of optimizations, such as CML and MCE, that are not con-
vex and depend on initial conditions. Third, we observed that the
reweighted conditional likelihood in eq. (17) led to improved per-
formance for smaller models. This positive effect diminished for
larger models, however, perhaps due to the increased difculty of
non-convex global optimization in larger parameter spaces. Finally,
though not reported here, we also experimented with corresponding
variants of MCE training, obtaining similar results.

4. CONCLUSION

In this paper we have compared large margin training in CD-HMMs
to two other leading frameworks for discriminative training. On the
task of phonetic recognition, we observed that our formulation of
large margin training achieved signicantly better performance than
either CML or MCE training. Follow-up experiments suggested two
possible reasons for this better performance: the convexity of the op-
timization for large margin training (versus those for CML and MCE
training), and the penalizing of incorrect decodings in direct propor-
tion to the number of mislabeled states. In future research, we are
interested in applying large margin training to large vocabulary ASR,

where both CML and MCE training have already demonstrated sig-
nicant reductions in word error rates [7, 8, 9].

