Abstract. We study Markov models whose state spaces arise from the Cartesian product of two or more
discrete random variables. We show how to parameterize the transition matrices of these models as a convex
combinationor mixtureof simpler dynamical models. The parameters in these models admit a simple
probabilistic interpretation and can be tted iteratively by an Expectation-Maximization (EM) procedure. We
derive a set of generalized Baum-Welch updates for factorial hidden Markov models that make use of this
parameterization. We also describe a simple iterative procedure for approximately computing the statistics of the
hidden states. Throughout, we give examples where mixed memory models provide a useful representation of
complex stochastic processes.

Keywords: Markov models, mixture models, discrete time series

1.

Introduction

The modeling of discrete time series is a fundamental problem in machine learning, with
widespread applications. These include speech recognition (Rabiner, 1989), natural lan-
guage processing (Nadas, 1984), protein modeling (Haussler, Krogh, Mian, & Sjolander,
1993), musical analysis/synthesis (Dirst & Weigend, 1993), and numerous others.

Probabilistic models of discrete time series typically start from some form of Markov
assumptionnamely, that the future is independent of the past given the present. For the
purpose of statistical estimation, problems arise if either: (i) the system possesses a large
number of degrees of freedom, or (ii) the window of present knowledge required to predict
the future extends over several time steps. In these cases, the number of parameters to
specify the Markov model can overwhelm the amount of available data. In particular, for a
system with n possible states and memory length k, the number of free parameters scales
exponentially as nk+1.

The difculties are compounded for latent variable models in which the Markov assump-
tion applies to the hidden state space. In this case, it may be computationally intractable
to infer values for the hidden states. For example, in rst-order hidden Markov models
(HMMs), computing the likelihood of a sequence of observations scales as n2, where n is
the number of hidden states (Rabiner, 1989). In practice, exact probabilistic inference is
therefore limited to HMMs with relatively small (or tightly constrained) state spaces.

76

SAUL AND JORDAN

In this technical note, we propose a principled way to investigate Markov models
with large state spaces. This is done by representing the transition matrix as a convex
combinationormixtureofsimpler dynamical models. We refer to the resulting models
as mixed memory Markov models. While the use of mixture distributions to parameterize
higher-order Markov models is well known (Raftery, 1985; Ney, Essen, & Kneser, 1994;
MacDonald & Zucchini, 1997), here we apply this methodology more broadly to factorial
modelsmodelsin which large state spaces are represented via the Cartesian product of
smaller ones.

Our note builds on earlier work describing factorial HMMs (Ghahramani & Jordan, 1997)
and dynamic probabilistic networks (Binder, Koller, Russell, & Kanazawa, 1997). These
papers show that complex stochastic processes can be graphically represented by sets of
Markov chains connected (via directed links) to a common set of observable nodes. Such
models arise naturally in the study of coupled time series, where the observations have an a
priori decomposition as the Cartesian product of two or more random variables. Factorial
HMMs aim to combine the power of latent, distributed representations with the richness
of probabilistic semantics (Williams & Hinton, 1990). Capturing this type of probabilistic
reasoning is a fundamental problem in articial intelligence (Dean & Kanazawa, 1989).

We believe that mixed memory Markov models have several advantages for representing
complex stochastic processes and learning from examples. The parameters in these models
admit a simple probabilistic interpretation and can be tted iteratively by an Expectation-
Maximization (EM) procedure (Dempster, Laird, & Rubin, 1977). The EM algorithm has
several desirable properties, including monotone convergence in log-likelihood, lack of step
size parameters, and naturalness at handling probabilistic constraints. In many situations,
it provides a compelling alternative to gradient-based learning methods (Baldi & Chauvin,
1996; Binder et al., 1997).

Mixed memory models can also express a rich set of probabilistic dependencies, making
them appropriate for modeling complex stochastic processes. Applied to factorial HMMs,
they generalize the work by Ghahramani & Jordan (1997) in two important directions: by
introducing coupled dynamics, and by considering non-Gaussian observations. They also
give rise to a simple iterative procedure for making inferences about the hidden states.
We describe this procedure not only for its practical value, but also because it very cleanly
illustrates the idea of exploiting tractable substructures in intractable probabilistic networks
(Saul & Jordan, 1997).

The main signicance of this work lies in its application to factorial HMMs and the
modeling of coupled time series. In principle, though, mixed memory Markov models can
be applied wherever large state spaces arise as the Cartesian product of two or more random
variables. We will take advantage of this generality to present mixed memory models in a
number of different settings. In doing this, our goal is to build upina gradual waythe
somewhat involved notation needed to describe factorial HMMs.

The organization of this note is therefore as follows. In order of increasing complexity, we
consider: (1) higher-order Markov models, where large state spaces arise as the Cartesian
product of several time slices; (2) factorial Markov models, where the dynamics are rst
order but the observations have a componential structure; and (3) factorial HMMs, where
the Markov dynamics apply to hidden states, as opposed to the observations themselves.

MIXED MEMORY MARKOV MODELS

77

We conclude that mixed memory models provide a valuable tool for understanding complex
dynamical systems.

2. Higher order Markov models

Let it 2 f1; 2; : : : ; ng denote a discrete random variable that can take on n possible values.
A kth order Markov model is specied by the transition matrix P (itjit(cid:0)1; it(cid:0)2; : : : ; it(cid:0)k).
To avoid having to specify the O(nk+1) elements of this matrix, we consider parameterizing
the model by the convex combination (Raftery, 1985; Ney et al., 1994):

P (itjit(cid:0)1; it(cid:0)2; : : : ; it(cid:0)k) =

k

X

(cid:22)=1

((cid:22)) a(cid:22)(itjit(cid:0)(cid:22));

(1)

where  ((cid:22)) (cid:21) 0, P(cid:22)  ((cid:22)) = 1, and a(cid:22)(i0ji) are k elementary n (cid:2) n transition matrices.
The model in eq. (1) is specied by O(kn2) parameters, as opposed to O(nk+1) for the
full memory model. Note how  ((cid:22)) is used to weight the inuence of past observations on
the distribution over it. This type of weighted sum is the dening characteristic of mixed
memory models.

The mixture model in eq. (1) is to be distinguished from models that approximate higher-
order Markov models by n-gram smoothing; that is, by employing a linear combination
of nth order transition matrices (Chen & Goodman, 1996). Our model is not an n-
gram smoother; rather it approximates a higher-order Markov model by taking a linear
combination of non-adjacent bigram models. The model in eq. (1) also differs from
mixture-of-experts models as applied to continuous time series (Zeevi, Meir, & Adler,
1996), in which the predictions of different nth order regressors are combined by the
weights of a softmax gating function.

For the purpose of parameter estimation, it is convenient to interpret the index (cid:22) in eq. (1)
as the value of a latent variable. We denote this latent variable (at each time step) by xt and
consider the joint probability distribution:

P (it; xt = (cid:22)jit(cid:0)1; : : : ; it(cid:0)k) =  ((cid:22)) a(cid:22)(itjit(cid:0)(cid:22)):

(2)

Note that marginalizing out xt (i.e., summing over (cid:22)) recovers the previous model for the
transition matrix, eq. (1). Thus we have expressed the dynamics as a mixture model, in
which the parameters  ((cid:22)) are the prior probabilities, P (xt = (cid:22)). Likewise, we can view
the parameters a(cid:22)(i0ji) as the conditional probabilities, P (it = i0jit(cid:0)1; : : : ; it(cid:0)k; xt = (cid:22)).
Let I = fi1; i2; : : : ; iLg denote an observed time series of length L. The sufcient
statistics for a full memory Markov model are the transition frequencies. To t the mixed
memory Markov model we avail ourselves of the EM procedure (Dempster et al., 1977). In
general terms the EM algorithm calculates expected sufcient statistics and sets them equal
to the observed sufcient statistics. The procedure iterates and is guaranteed to increase
the likelihood at each step. For the model in eq. (2), the EM updates are (Ney et al, 1994):

((cid:22))   Pt P (xt = (cid:22)jI)
Pt;(cid:23) P (xt = (cid:23)jI)

;

a(cid:22)(i0ji)   Pt P (xt = (cid:22); it(cid:0)(cid:22) = i; it = i0jI)

Pt P (xt = (cid:22); it(cid:0)(cid:22) = ijI)

:

(3)

(4)

78

SAUL AND JORDAN

Table 1. Entropy per character, computed from various Markov models.

order

0th
1st
2nd
2nd

memory

none
full
mixed
full

English

0.900
0.776
0.754
0.689

Italian

0.844
0.696
0.678
0.622

Finnish

0.840
0.707
0.679
0.607

In the case where multiple time series are available as training data, the sums over t should
be interpreted as sums over series as well. The EM updates for this model are easy to
understand; at each iteration, the model parameters are adjusted so that the statistics of
the joint distribution match the statistics of the posterior distribution. The expectations in
eqs. (3) and (4) may be straightforwardly computed from Bayes rule:

P (xt = (cid:22)jI) =

((cid:22)) a(cid:22)(itjit(cid:0)(cid:22))

P(cid:23)  ((cid:23)) a(cid:23)(itjit(cid:0)(cid:23))

:

(5)

Note that this algorithm requires no ne-tuning of step sizes, as does gradient descent.

In terms of representational power, the model of eq. (1) lies somewhere in between a rst
order Markov model and a kth order Markov model. To demonstrate this point, we tted
various Markov models to word spellings in English, Italian, and Finnish. The state space
for these models was the alphabet (e.g., A to Z for English), and the training data came from
very long lists of words with four or more letters. The matrices a(cid:23)(i0ji) were initialized
by count-based bigram models predicting each letter by the (cid:22)th preceding one. (This type
of initialization, in which the component sub-models are rst trained independently of one
another, is useful to avoid poor local maxima in the learning procedure.) In table 1, we give
the results measured in entropy per character. The results show that the mixed memory
model does noticeably better than the rst-order model. Of course, it cannot capture all
the structure of the full second-order model, which has over ten times as many parameters.
The mixture model should accordingly be viewed as an intermediate step between rst and
higher-order models.

We envision two situations in which the model of eq. (1) may be gainfully applied. The
rst is when the dynamics of the process generating the data are faithfully described by
a mixture model. In this case, one would expect the mixture model to perform as well
as the (full) higher-order model while requiring substantially less data for its parameter
estimation. A real-world example might be the modeling of web sites visited during
a session on the World Wide Web. The modeling of these sequences has applications
to web page prefetching and resource management on the Internet (Bestavros & Cunha,
1995; Cunha, Bestavros, & Crovella, 1995). Typically, the choice of the next web page is
conditioned on a previous site, but not necessarily the last one that has been visited. (Recall
how often it is necessary to retrace ones steps, using the back option.) The model in
eq. (1) captures this type of conditioning explicitly. Here, the states of the Markov model
would correspond to web pages; the matrices a(cid:22)(i0ji), to links from web page i to web
page j; and the index (cid:22), to the number of backward or retraced steps taken before activating
a new link.

The second situation in which this model may be appropriate is when the amount of
In this case,

training data is extremely sparse relative to the size of the state space.

MIXED MEMORY MARKOV MODELS

79

the parameterization in eq. (1), though a poor approximation to the true model, may be
desirable to avoid overtting. Ney et al (1994) have investigated models of this form for
large vocabulary language modeling. The ability to discern likely sequences of words
from unlikely sequences is an important component of automated speech recognition. For
large vocabulariesinthe tens of thousands of wordsthereis never sufcient data to
estimate (robustly) the statistics of second or higher order Markov models. In practice,
therefore, these models are smoothed or interpolated (Chen & Goodman, 1996) with
lower order models. The interpolation with lower order models is forced on practitioners
by the enormous size of the state space (e.g., 104 words) and the small (in relative terms)
amount of training data (e.g., 108 words). Recently, one of us applied a more sophisticated
version of eq. (1) to large vocabulary language modeling (Saul & Pereira, 1997). In only
a few CPU hours, it was possible to t over ten million parameters to the statistics of
an eighty million word corpus. Moreover, the smoothed combination of mixed and full
memory Markov models led to signicantly lower entropies on out-of-sample predictions.

3. Factorial Markov models

In the last section, we saw how large state spaces arose as the result of higher order
dynamics.
In this section, we consider another source of large state spacesnamely,
factorial representations. Many time series have a natural componential structure. Consider
for example the four voicessoprano(S), alto (A), tenor (T), and bass (B)ofa Bach
fugue (Dirst & Weigend, 1993). We can model each voice by a separate Markov model,
but this will not capture the correlations due to harmony. The most straightforward way
to model the coupling between voices is to write down a Markov model whose dynamical
state is the Cartesian product of the four voices. But the combinatorial structure of this
state space leads to an explosion in the number of free parameters; thus it is imperative to
provide a compact representation of the transition matrix.

Mixed memory models are especially geared to these sorts of situations. Let It denote
the tth element of a vector time series, and i(cid:22)
t the (cid:22)th component of It. If each vector has
k components, and each component can take on n values, then the overall state space has
size nk. To model the coupling between these components in a compact way, we make two
simplifying assumptions: (i) that the components i(cid:23)
t at time t are conditionally independent
given the vector It(cid:0)1, or

P (ItjIt(cid:0)1) =

k

Y

(cid:23)=1

P (i(cid:23)

t jIt(cid:0)1);

(6)

and (ii) that the conditional probabilities P (i(cid:23)
of cross-transition matrices:

t jIt(cid:0)1) can be expressed as a weighted sum

P (i(cid:23)

t jIt(cid:0)1) =

k

X

(cid:22)=1

(cid:23) ((cid:22)) a(cid:23)(cid:22)(i(cid:23)

t ji(cid:22)

t(cid:0)1):

(7)

Here again, the parameters a(cid:23)(cid:22)(i0ji) are k2 elementary n (cid:2) n transition matrices, while
the parameters  (cid:23)((cid:22)) are positive numbers that satisfy P(cid:22)  (cid:23)((cid:22)) = 1. The number of

80

SAUL AND JORDAN

Table 2. Portion of the four-component time series generated by Bachs last fugue.

soprano

alto
tenor
bass

61
54
49
46

61
54
49
44

61
54
49
44

66
54
49
46

66
54
49
46

66
54
49
46

66
54
49
46

66
54
51
46

66
54
51
46

66
54
52
46

66
54
52
46

66
54
51
48

66
56
51
48

66
56
51
48

66
56
51
48

66
56
51
48

free parameters in eq. (7) is therefore O(k 2n2), as opposed to O(n2k) for the full memory
model. (By allowing non-square transition matrices, this model can also be generalized to
the case where the different components take on different numbers of values.)

The parameters  (cid:23) ((cid:22)) measure the amount of correlation between the different compo-
nents of the time series. In particular, if there is no correlation, then   (cid:23)((cid:22)) is the identity
matrix, and the (cid:23)th component is independent of all the rest. On the other hand, for non-
zero  (cid:23)((cid:22)), all the components at one time step inuence the (cid:23)th component at the next.
The matrices a(cid:23)(cid:22)(i0ji) provide a compact way to parameterize these inuences.
As in the previous section, it is convenient to introduce latent variables x(cid:23)

t and view

t is to select which component of It(cid:0)1 determines the transition matrix
t . As before, we can derive an EM algorithm to t the parameters of this model. In

t = (cid:22); i(cid:22)

t(cid:0)1 = i; i(cid:23)

t = i0jI)

Pt P (x(cid:23)

t = (cid:22); i(cid:22)

t(cid:0)1 = ijI)

where I stands for the observed time series. Naturally, the structure of these updates is
quite similar to the model of the previous section.

To test this algorithm, we learned a model of the four-component time series generated
by Bachs last fugue. This fugue has a rich history (Dirst & Weigend, 1993). The time
series (3284 beats long) was made public following the Santa Fe competition on time series
prediction. Table 2 shows a portion of this time series: here, each element represents a
sixteenth note, while the numerical value codes the pitch. To help avoid poor local maxima
in the learning procedure, the transition matrices a(cid:23)(cid:22)(i0ji) were initialized by count-based
bigram models predicting the (cid:23)th voice at time t from the (cid:22)th voice at the previous time
step.

By examining the parameters of the tted model, we can see to what extent each voice
enables one to make predictions about the others. In general, we observed that the mixture
coefcients  (cid:23) ((cid:22)) were very close to zero or one. The reason for this is that the voices do
not typically change pitch with every sixteenth note. Hence, for each voice the note at the
previous beat is a very good predictor of the note at the current one.

eq. (7) as a mixture model. Thus we may write:

P (i(cid:23)

t ; x(cid:23)
P (It; XtjIt(cid:0)1) = Y

t = (cid:22)jIt(cid:0)1) =  (cid:23)((cid:22)) a(cid:23)(cid:22)(i(cid:23)
t ; x(cid:23)

P (i(cid:23)

t(cid:0)1);

t ji(cid:22)
t jIt(cid:0)1):

(cid:23)

Here, the role of x(cid:23)
for i(cid:23)
this case, the EM updates are:
(cid:23)((cid:22))   Pt P (x(cid:23)
Pt;(cid:22)0 P (x(cid:23)

a(cid:23)(cid:22)(i0ji)   Pt P (x(cid:23)

t = (cid:22)jI)
t = (cid:22)0jI)

(8)

(9)

(10)

(11)

MIXED MEMORY MARKOV MODELS

81

1

0.8

y
t
i
l
i

0.6

b
a
b
o
r
p

0.4

0.2

0
0

1000

2000
time

3000

4000

Figure 1. Plot of soprano-tenor correlations versus time, as measured by the posterior probabilities of a mixed
memory Markov model.

When the voices do make a transition (i.e., move up or down in pitch), however, the
coupling between voices becomes evident. To see this, we can look at the posterior
probabilities of the latent variables, x(cid:22)
t , which reveal the extent to which the voices interact at
t = T jI),
specic moments in time. Figure 1 shows a plot of the posterior probabilities, P (xS
versus time calculated from the tted model. Within the framework of the mixture model,
these probabilities measure the relative degree to which the sopranos note at time t can
be predicted from the tenors note at the previous time step. The moments at which this
probability acquires a non-zero value indicate times when the tenor and soprano are tightly
coupled. Not surprisingly, these pulses of coupling (when viewed as a time series) have a
discernible local rhythm and regularity of their own.

4. Factorial HMMs

Building on the results of the last section, we now consider the generalization to factorial
hidden Markov models (HMMs). These are HMMs whose states and observations have an
internal, combinatorial structure (Ghahramani & Jordan, 1997; Binder et al., 1997). How
might such structure arise? Suppose we are trying to model the processes that give rise to
a speech signal. A number of unobserved variables interact to generate the signal that we
ultimately observe. In an articulatory model of speech production, these variables might
encode the positions of various organs, such as the lip, tongue, and jaw. In a recognizer,
these variables might encode the current phonemic context, the speaker accent and gender,
and the presence of background noise. In either case, the hidden state for these models is
naturally decomposed as the Cartesian product of several random variables.

Another motivation for factorial representations is that in many applications, the ob-
servations have an a priori componential structure. This is the case, for example, in
audiovisual speech recognition (Bregler & Omohundro, 1995), where information from
different modalities is being combined and presented to the recognizer. It is also the case in

82

SAUL AND JORDAN

frequency subband-based speech recognition (Bourlard & Dupont, 1996), where different
recognizers are trained on sub-bands of the speech signal and then combined to make a
global decision. Simple ways to integrate these different components are: (a) collapsing
the data into a single time series or (b) reweighting and combining the likelihood scores
of independent HMMs. One might hope for a more sophisticated integration, however, by
building a joint model that looks for correlations on the actual time scale of the observations.
Whatever the manner in which they arise, factorial HMMs pose two concrete problems.
The rst is representation.
In most applications, there is not sufcient data to estimate
the elements of the full transition and emission matrices formed by taking the Cartesian
product of the individual factors. How should one parameterize these matrices without
making restrictive or inelegant assumptions? Ideally, the representation should not make
unjustied assumptions of conditional independence, nor should it force us to give up
desirable properties of the EM algorithm, such as monotone convergence in log-likelihood.
The second problem in factorial HMMs is one of computational complexity. The Baum-
Welch algorithm for parameter estimation scales as O(N 2), where N is the number of
hidden states. If the hidden state is a Cartesian product of k random variables, each of
degree n, then the effective number of hidden states is N = nk. Even for small k, this may
be prohibitively large to calculate the statistics in the E-step of the EM algorithm. Hence,
one is naturally led to consider approximations to these statistics.

Let us now return to our development of factorial HMMs with these issues in mind.
We will see that mixture models provide a good compromise to the problem of represen-
tation, and that efcient deterministic approximations exist for the problem of parameter
estimation.

For concreteness, suppose that we have trained k simple HMMs on separate time series
of length L. Now we wish to combine these HMMs into a single model in order to capture
(what may be) useful correlations between the different time series.
If each individual
HMM had n hidden states and m types of observations, then the hidden state space of the
combined model has size nk; likewise, the observation space of the combined model has
size mk. At each time step, we denote these spaces by the Cartesian products:

It = i1
Jt = j 1

t (cid:10) i2
t (cid:10) j 2

t (cid:10) (cid:1) (cid:1) (cid:1) (cid:10) ik
t (cid:10) (cid:1) (cid:1) (cid:1) (cid:10) jk

t

t

(hidden),
(observed).

(12)
(13)

In an HMM, it is the hidden states (as opposed to the observations) that have a Markov
dynamics. Accordingly, in this setting, we use eq. (67) to model the hidden state transition
matrix. By analogy to eqs. (67), we parameterize the emission probabilities by:

P (JtjIt) = Y

X

(cid:23)

(cid:22)

(cid:30)(cid:23) ((cid:22))b(cid:23)(cid:22)(j(cid:23)

t ji(cid:22)

t );

(14)

where b(cid:23)(cid:22)(jji) are k2 elementary n(cid:2)m emission matrices. Note that this model can capture
correlations between the hidden states of the (cid:22)th Markov chain and the observations in the
(cid:23)th time series.

For the purposes of parameter estimation, it is again convenient to introduce latent
variables that encode the mixture components in eq. (14). By analogy to eqs. (8) and (9),
we have:

P (j(cid:23)

t ; y(cid:23)

t = (cid:22)jIt) = (cid:30)(cid:23)((cid:22))b(cid:22)(cid:23) (i(cid:22)

t ; j(cid:23)

t );

(15)

MIXED MEMORY MARKOV MODELS

P (Jt; YtjIt) = Y

(cid:23)

P (j(cid:23)

t ; y(cid:23)

t jIt):

83

(16)

Having encoded the mixture components as hidden variables, we can now apply an EM
algorithm to estimate the model parameters. In this case, the updates have the form:

(cid:23)((cid:22))   Pt P (x(cid:23)
Pt;(cid:22)0 P (x(cid:23)

t = (cid:22)jJ)
t = (cid:22)0jJ)

;

a(cid:23)(cid:22)(i0ji)   Pt P (x(cid:23)

t = (cid:22); i(cid:22)

t(cid:0)1 = i; i(cid:23)

t = i0jJ)

Pt P (x(cid:23)

t = (cid:22); i(cid:22)

t(cid:0)1 = ijJ)

(cid:30)(cid:23)((cid:22))   Pt P (y(cid:23)
Pt;(cid:22)0 P (y(cid:23)

t = (cid:22)jJ)
t = (cid:22)0jJ)

;

b(cid:23)(cid:22)(jji)   Pt P (y(cid:23)

t = (cid:22); i(cid:22)

t = i; j(cid:23)

t = jjJ)

Pt P (y(cid:23)

t = (cid:22); i(cid:22)

t = ijJ)

;

(17)

(18)

(19)

(20)

where J denotes the observed time series. A Viterbi approximation is obtained by con-
ditioning not only on J, but also on the most probable sequence of hidden states, I (cid:3),
where

I (cid:3) = arg max

I

Y

t

P (ItjIt(cid:0)1)P (JtjIt):

(21)

Note that computing the posterior probabilities in these updates requires O(Ln2k) opera-
tions; the same is true for computing the Viterbi path. To avoid this computational burden,
we have used an approximation for estimating the statistics in factorial HMMs, rst outlined
in Saul & Jordan (1996). The basic idea behind our approach is simple: the structure of the
factorial HMM, though intractable as a whole, gives rise to efcient approximations that
exploit the tractability of its underlying components. In this note, we discuss how these
approximations can be used to estimate the Viterbi path. In general, these ideas may be
extended to approximate the full statistics of the posterior distribution, as for example in
Ghahramani & Jordan (1997).

In the factorial HMM, dynamic programming procedures to compute the Viterbi path
algorithm require O(Ln2k) steps. As a practical alternative, we consider an iterative
procedure that returns a (possibly sub-optimal) path in polynomial time. Our iteration is
based on a subroutine that nds the optimal path of hidden states through the (cid:22)th chain
given xed values for the hidden states of the others. Note that when we instantiate the
hidden variables in all but one of the chains, the effective size of the hidden state space
collapses from nk to n, and we can perform the optimization with respect to the remaining
hidden states in O(Ln2) steps. A factor of k2 is picked up when converting the right hand
side of eq. (21) into a form for which the standard Viterbi algorithm can be applied; thus
this elementary chainwise Viterbi operation requires O(Lk 2n2) steps.

The algorithm for approximately computing the full Viterbi path of the factorial HMM is
obtained by piecing these subroutines together in the obvious way. First, an initial guess is
made for the Viterbi path of each component HMM. (Typically, this is done by ignoring the
intercomponent correlations and computing a separate Viterbi path for each chain.) Then,
the chainwise Viterbi algorithm is applied, in turn, to each of the component HMMs. After

84

SAUL AND JORDAN

1

0.8

y
t
i
l
i

0.6

b
a
b
o
r
p

0.4

0.2

0
0

1000

2000
time

3000

4000

Figure 2. Plot of soprano-tenor correlations versus time, as measured by the posterior probabilities of a mixed
memory HMM.

the Viterbi algorithm has been applied k times, or once to each chain, the cycle repeats;
each iteration of this process therefore involves O(Lk 3n2) steps.

Note that each iteration results in a sequence of hidden states that is more probable than
the preceding one; hence, this process is guaranteed to converge to a nal (though possibly
suboptimal) path.
In practice, we have found that this process typically converges to a
stable path in three or four iterations.

The chainwise Viterbi algorithm is not guaranteed to nd the truly optimal sequence of
hidden states for the factorial HMM. The success of the algorithm depends on the quality
of the initial guess and, as always, the good judgment of the modeler. The approximation is
premised on the assumption that the model describes a set of weakly coupled time series
in particular, that the auto-correlations within each time series are as strong or stronger
than the cross-correlations between them. We view the approximation as a computationally
cheap way of integrating HMMs that have been trained on parallel data streams. Its main
virtue is that it exploits the modelers prior knowledge that these separate HMMs should
be weakly coupled. When this assumption holds, the approximation is quite accurate.

To test these ideas, we tted a mixed memory HMM to the Bach fugue from section 3.
One hopes in this model that the hidden states will reect musical structure over longer
time scales than a single note. In our experiments, each voice had a component HMM
with six hidden states; thus, in our previous notation, n = 6 and k = 4. We employed a
Viterbi approximation to the full EM algorithm, meaning that the posterior probabilities
in eqs. (17)(20) were conditioned not only on the observations J, but also on the Viterbi
path, I (cid:3). The most probable sequence of hidden states I (cid:3) was estimated by the iterative
procedure described above. Again it was interesting to see how this model discovered
correlations between the different voices of the fugue. Figure 2 shows a plot of the
posterior probabilities P (xS
t = T jI (cid:3); J) versus time, calculated from the factorial HMM
(after training). The frequent pulses indicate (within the framework of this model) moments
of strong coupling between the soprano and tenor themes of the fugue.

MIXED MEMORY MARKOV MODELS

85

5. Discussion

Many parameterizations have been proposed for probabilistic models of time series. The
mixed memory models in this note have three distinguishing features. First, they can
express a rich set of probabilistic dependencies, including coupled dynamics in factorial
models. Second, they can be tted by EM algorithms, thus avoiding potential drawbacks
of gradient descent. Third, they are compact and easy to interpret; notably, as in ordinary
Markov models, every parameter denes a simple conditional probability. All these features
should enable researchers to build more sophisticated models of dynamical systems.

Acknowledgments

We thank Marney Smyth for retrieving the word lists, Tommi Jaakkola for helping us with
Finnish, and Fernando Pereira for pointing out the application to web page prefetching.
We also acknowledge useful discussions with Zoubin Ghahramani and Yoram Singer.
This work was initiated while the authors were afliated with the Center for Biological
and Computational Learning at MIT. During that time, it was supported by NSF grant
CDA-9404932 and ONR grant N00014-94-1-0777.

