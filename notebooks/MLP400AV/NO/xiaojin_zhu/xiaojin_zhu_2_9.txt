ABSTRACT

1.

INTRODUCTION

We compare two recently proposed frameworks for combin-
ing generative and discriminative probabilistic classiers and
apply them to semi-supervised classication. In both cases
we explore the tradeo between maximizing a discriminative
likelihood of labeled data and a generative likelihood of la-
beled and unlabeled data. While prominent semi-supervised
learning methods assume low density regions between classes
or are subject to generative modeling assumptions, we con-
jecture that hybrid generative/discriminative methods allow
semi-supervised learning in the presence of strongly overlap-
ping classes and reduce the risk of modeling structure in the
unlabeled data that is irrelevant for the specic classication
task of interest. We apply both hybrid approaches within
naively structured Markov random eld models and pro-
vide a thorough empirical comparison with two well-known
semi-supervised learning methods on six text classication
tasks. A semi-supervised hybrid generative/discriminative
method provides the best accuracy in 75% of the experi-
ments, and the multi-conditional learning hybrid approach
achieves the highest overall mean accuracy across all tasks.

Categories and Subject Descriptors

I.2 [Articial Intelligence]: Learning

General Terms

Algorithms, Experimentation, Performance

Keywords

Semi-supervised learning, hybrid generative/discriminative
methods, text classication

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
KDD07, August 1215, 2007, San Jose, California, USA.
Copyright 2007 ACM 978-1-59593-609-7/07/0008 ...$5.00.

Most machine learning methods rely on the availability of
large labeled datasets. However, human annotation is time-
consuming, making labeled data costly to obtain in practice.
Motivated by this problem, researchers have proposed semi-
supervised learning methods that leverage large amounts
of relatively inexpensive unlabeled data along with small
amounts of labeled data. The increasing interest in apply-
ing machine learning to new domains and the vast availabil-
ity of unlabled data from the web and elsewhere are driving
interest in semi-supervised learning.

Semi-supervised learning is especially relevant for applica-
tions in data mining, as when initially analyzing data from
a new domain, obtaining any labeled data requires labori-
ous human annotation. The lowest eort approach to data
mining would use unsupervised learning. However, super-
vised learning methods typically provide better, more task-
focused results.

For example, consider the problem of classifying messages
as belonging to a mac hardware or pc hardware newsgroup.
Although there are word features in the data relevant to this
task (such as powerbook indicating mac, or dell indicat-
ing pc), because mac and pc postings have high word overlap,
an unsupervised clustering algorithm could discover many
dierent ways to partition this data. For example, messages
about hard drives or networking may appear as clusters, but
these clusters may not be directly relevant to the classica-
tion task of interest. If posed as a supervised classication
task, however, with labeled examples from each newsgroup,
the classier will learn to focus on the features relevant to
the mac  pc task, and make the desired separation.

Training methods for machine learning classiers are often
characterized as being generative or discriminative. Gen-
erative training estimates the joint distribution of all vari-
ables, both the classes and the input data, while discrim-
inative training is concerned only with the decision bound-
ary. Classiers trained discriminatively seem to have lower
asymptotic error than analogous generatively-trained classi-
ers because, intuitively, they are able to focus limited rep-
resentational capacity on predicting just the class variable.
However, discriminative approaches do not always provide
the highest accuracy. When the amount of training data is
small, generative training can provide better accuracy even
when the model is not a very good t to the data. Ng and
Jordan demonstrate this, comparing naive Bayes and logistic
regression [15].

Motivated by these observations, several researchers have
proposed hybrid methods that combine generative and dis-
criminative training. These hybrid methods have delivered
promising results in the domains of text classication [13,
18], pixel classication [10], and object recognition [21, 11],
among others.

A variety of semi-supervised techniques have been de-
veloped for both generative and discriminative models. A
straightforward, generative semi-supervised method is the
expectation maximization (EM) algorithm. The EM ap-
proach for naive Bayes text classication models is discussed
by Nigam et al. in [17]. Generative semi-supervised meth-
ods rely on a model for the distribution of the input data,
and can fail either when this model is wrong, or when the
structure of the input data is not correlated with the clas-
sication task (as illustrated in the mac-pc example above).
Discriminative semi-supervised methods, including proba-
bilistic and non-probabilistic approaches, such as transduc-
tive or semi-supervised support vector machines (TSVMs,
S3VMs) [8, 7] and a variety of other graph based meth-
ods [22, 1] assume high density within class and low density
between classes, and can fail when the classes are strongly
overlapping. Hence, these approaches for semi-supervised
learning in discriminative classiers also use model assump-
tions about the structure of the input data, but typically
do not encode these assumptions as explicit models of input
probability density.

In this paper, we apply hybrid generative/discriminative
methods to semi-supervised learning. We compare two re-
cently proposed approaches to combining generative and dis-
criminative methods in detail. The rst is multi-conditional
learning [13], a class of training objective functions com-
posed of the product of multiple likelihoods that share one
set of parameters and are derived from an underlying joint
model. We formulate the semi-supervised training problem
in terms of the optimization of a multi-conditional objective
function that is a weighted combination of a discriminative
likelihood of labeled data and a marginal likelihood of both
labeled and unlabeled data. We also consider a framework
proposed by Lasserre et al. [11] which we henceforth refer to
as the parameter coupling prior 1 method. In this approach,
the discriminative and generative components derived from
a common joint model have separate sets of parameters.
These parameters are coupled by a prior distribution that
species how one set of parameters inuences the other.

Both of these hybrid approaches can be interpreted as dis-
criminative classiers trained using the marginal likelihood
of the input data as parameter regularization. We conjec-
ture that for many problems this form of regularization is
more helpful than typical discriminative regularization ap-
proaches penalizing decision boundaries passing through re-
gions of high marginal density.
In contrast, these gener-
ative/discriminative hybrids are not constrained to avoid
low-density regions between classes when placing decision
boundaries. Additionally, they are able to balance between
leveraging innate clusters in the input data (which may or
may not be useful) and task-specic evidence from the la-
beled data (which may or may not be representative). Hy-
brid methods can avoid relying on generative modeling as-

1We note that parameter coupling prior is a short name we
devised to refer to the work of Lasserre, Bishop, and Minka.
This term is not used in their paper.

sumptions by emphasizing the discriminative likelihood dur-
ing maximization. In summary, these methods allow us to
avoid the assumptions of discriminative semi-supervised ap-
proaches and mitigate the assumptions of generative semi-
supervised methods. By emphasizing each component of the
objective function appropriately, they allow semi-supervised
learning in cases that other methods fail.

In addition to the motivation provided above, the contri-

butions of this paper are:

 We apply multi-conditional learning to semi-supervised

learning.

 We compare the multi-conditional learning approach
with a framework recently proposed by Lasserre, et al.
in [11]. We subject this method to much more thor-
ough evaluation than was provided in [11] (only one
dataset, no comparisons to other methods).

 We implement these model-independent approaches in
a naively structured Markov random eld model and
derive the appropriate gradients.

 We provide an empirical comparison of the two ap-

proaches along with two other prominent semi-supervised
methods for classication. A hybrid method outper-
forms other methods in 75% of the experiments and the
multi-conditional learning approach gives the highest
overall mean accuracy.

2. TWO GENERAL GENERATIVE-
DISCRIMINATIVE APROACHES

First, we dene the learning problem. Suppose we have
data D = DL DU , where DL and DU represent the labeled
and unlabeled data, respectively. Each example in DL is
a pair (y, x), where the vector y has length equal to the
number of classes and a 1 in the position corresponding to
the index of the correct class (other entries are 0). Vector x
has length equal to the number of features of the input and
each position contains the value of a particular feature for
this example. In DU , each example is only (x), as the value
of y is hidden. In the case of document classication, for
example, each example corresponds to a document, and each
position in x might contain the number of times a particular
word occurs.

Notice that x can be decomposed into PN
N = P|x|

i wi, where
i xi, so that each wi corresponds to the event of
observing a feature. Vector wi has a single 1 in one position
and 0 elsewhere. For example, in document classication
wi represents a word occurrence. Another occurrence of the
same word in the document would correspond to separate
event wk. This decomposition of x into individual events is
useful for understanding the graphical model introduced in
Section 3. First, we discuss two model-independent hybrid
approaches.
2.1 Multi-Conditonal Learning

Multi-conditional learning [13] is a class of training objec-
tive functions composed of the product of multiple weighted
likelihoods, each with parameters derived from the same un-
derlying joint model. An advantage of the multi-conditional

framework is the exibility it allows to craft an objective
function for a specic learning task. For example, an objec-
tive function composed of the product of weighted discrimi-
native likelihoods for multiple tasks is a natural framework
for transfer learning or multitask learning [5].

McCallum et al. [13] combine discriminative and genera-
tive likelihoods using the multi-conditional objective func-
tion:

P (Y |X; )P (X|Y ; )

Training text classication models with this objective func-
tion was found to produce improvements in classication ac-
curacy. Here, we express semi-supervised training in terms
of a multi-conditional objective function by combining the
weighted discriminative likelihood of the labeled data and
the weighted marginal likelihood of labeled and unlabeled
data. This objective function is:

O() = P (YL|XL; )P (X; ),

where XL and YL denote the labeled data and the term
P (X; ) includes both labeled and unlabeled data.
It is convenient to maximize the natural log of O:
lnO() =  ln P (YL|XL; ) +  ln P (X; )

(1)

We choose the model parameters  that maximize O:

 = arg max

(lnO()).



In equation (1), increasing  gives more weight to the dis-
criminative component during maximization, while increas-
ing  gives more weight to the generative component.

A practical concern is that each component and its gra-
dient may be dierent in scale. Notice that P (YL|XL; )
is a distribution is over the number of labels, and P (X; )
is a distribution over the number of features. This means
that if the distributions were uniform the magnitude of the
log-likelihood for the generative component would be much
smaller than that of the discriminative component. Addi-
tionally, in semi-supervised learning the number of labeled
examples is typically much smaller than the number of un-
labeled examples, so the sums inside each likelihood calcu-
lation have a dierent number of terms. This makes it di-
cult to choose values of  in an interpretable way. Choosing
 = 0.6 and  = 0.4 does not correspond to maximizing
with 60% of the weight on the discriminative component,
as the discriminative gradient magnitudes tend to be larger
than those of the generative component.

One potential solution to this problem is to normalize
each of the components so that they have the same mag-
nitude, and weight the normalized componenets. In non-log
space, normalizing each component corresponds to raising
each component to a power x. If x > 1, then this makes the
probability distribution more peaked, whereas if x < 1, the
probability distribution is attened. Since P (YL|XL; ) is
convex, stretching or attening it should make little dier-
ence in terms of the ability of a gradient-based optimizer to
nd the maximum. However, P (X; ) is not convex, and
consequently attening it could actually change the maxi-
mum found by the maximizer, if x is small enough to su-
ciently smooth the distribution. Because the generative like-
lihood is smaller in magnitude and attening it by raising it

to a power x < 1 may be detrimental, we avoid normaliza-
tion and set  = 1 and  >> .

The dierence in the magnitude of the likelihoods could
also cause maximization to appear to converge when one
component conceals the changes in the other. To deal with
this issue, we adapt convergence criteria so that training
converges only when both components, considered indepen-
dently, have converged.

In addition to the terms above, we use a standard zero-

mean Gaussian prior over parameters:

ln P ()  ||||2

2

.

2.2 Parameter Coupling Prior

In the approach of Lasserre, et al. [11], which again we
refer to as the parameter coupling prior approach, the gen-
erative and discriminative components have separate sets of
parameters. The two sets of parameters are jointly trained
and are coupled using a prior distribution. Following [11],
we dene the joint distribution of features X, classes Y , and
parameters D and G (for the discriminative and genera-
tive models, respectively) as:

P (X, Y, D, G) = P (D, G)P (Y |X; D)P (X; G)

Let us consider two special cases of priors.
If the prior
P (D, G) constrains D = G, then we have a genera-
tive model based on the joint distribution.

P (X, Y, G) = P (G)P (X, Y ; G)

If the prior assumes that the two sets of parameters are
independent P (D, G) = P (D)P (G), then we have:
P (X, Y, D, G) = P (D)P (Y |X; D) [P (G)P (X; G)]
In other words, if the underlying joint model is such that
the parameters of the marginal model and the conditional
model are completely independent, then the terms inside the
brackets are constant with respect to D, and hence play
no role in classication. Therefore, this is a discriminative
model.

A prior that imposes a soft constraint that the parameters
must be similar allows blending of the generative and dis-
criminative. As in [11], we couple the two sets of parameters
by a prior of the form:

ln P (D, G) = ||D  G||2

22

(2)

Lasserre et al. noted that the generative component P (X; G)
can make use of unlabeled data, and can hence be used
for semi-supervised learning. Experimental results on one
dataset using the above prior demonstrated the potential
for semi-supervised learning using this method.

3. MODEL

It is important to note that the two approaches described
above are model-independent because they only specify the
form of the objective function. We can derive concrete
versions of the objective functions for a specic graphical
model. Here, we apply them to a Markov random eld

x. The gradient of Ly|x with respect to parameters y is
similar. The log marginal likelihood for P (x) is

!
!
!

and has gradient

X
X

y

y

ln

(x)D


X

X
P
P

ln

(x)DU

Lx =



xy

X
h

=

(x)DU

 Ex

Ey|x[xyT ]

(exp(yy + yT T

xy x))  ln Z

(exp(yy + yT T

xy x))  ln Z

y (exp(yy + yT T

xy x))xyT

y (exp(yy + yT T

xy x))

i  Ex,y[xyT ]

  ln Z
xy

Figure 1: A factor graph for a naively-structured
Makrov random eld model.

(MRF) model, an undirected graphical model. The model is
structured so that the input variables wi are conditionally
independent given the class y. This can be interpreted as
an undirected analog to naive Bayes models. For this rea-
son we refer to this specic structure as a naively structured
MRF. The factor graph for this model is shown in Figure 1.
We could also use these training objective functions in more
complicated models with hidden topic variables or models
for sequences.

3.1 Training

We estimate the parameters of the model by nding the
parameters that maximize the objective functions dened
in Section 2. We use gradient methods to nd the max-
imum. Below we dene these objective functions for the
naively-structed MRF model, and compute the gradients.
The components of each objective are derived from the joint
distribution of the model given by:

Where Z = P

P

P (y, x) =

1
Z

exp(T

y y + yT T

xyx)

(3)

y

x exp(T

y y + yT T

xyx) is a normalizing
factor that ensures a true probability distribution. First,
we derive the gradient for the discriminative component of
the objective function, the conditional log-likelihood Ly|x =
log P (y|x), where y and x denote observations and

xy x  ln Z(x),

y y + yT T

Ly|x =

where

X

(x,y)DL

`T
X

y

Z(x) =

exp(yy + yT T

xy x).

The gradient is then computed as



X

(x,y)DL

xyT 

h

P
P

 Ex,y[xyT ]  Ex

Ey|x[xyT ]

Ly|x
xy

=

y exp(yy + yT T

xy x)xyT

y exp(yy + yT T

xy x)

i

!

where Ex[] denotes the expectation under p(x) and Ex[]
denotes an expectation under the empirical distribution of

For the multi-conditional objective function, the parameter
matrices xy and y are the same for both the discrimina-
tive and generative components. For the parameter coupling
prior method, xy and y are dierent for each component
and are coupled by (2). The derivative of this prior with
respect to each set of parameters is:

ln P (D, G)  ||D  G||2
ln P (D, G) =  D  G
ln P (D, G) =  G  D

22

2

2



D



G

If  = 0 in the multi-conditional approach or  =  in
the parameter coupling priors approach, then the Lx term
drops out of the objective function and we only maximize
Ly|x which uses no unlabeled data. Maximizing only Ly|x in
the naive MRF model yields a maximum entropy classier
[16]. We use maximum entropy classiers as the supervised
counterpart to our hybrid semi-supervised methods.

We use Limited-Memory-BFGS, a quasi-Newton optimiza-
tion method that has been shown to work well for maxi-
mum entropy models [12], in conjunction with the adapted
converge criteria discussed in Section 2.1. Note that the
marginal density P (x) is not convex, which means neither
the multi-conditional nor the parameter coupling prior ob-
jective functions are convex. Because L-BFGS is a convex
optimizer, we converge to a local maximum. Empirically, we
have found that L-BFGS requires fewer training iterations
converges to better maxima than other convex optimizers.
Using a method to specically address the lack of convex-
ity may be benecial, but is an issue we leave for future
research.

4. RELATED WORK

4.1 Semi-Supervised Learning

Although many semi-supervised methods have been pro-
posed, they each fall into one of a small number of classes
of methods, each of which make certain assumptions.

Co-training [2] and related multi-view learning methods [4,
20] assume that multiple classiers are trained over multiple
feature views (splits) of the same labeled examples. As ca-
pacity control, these classiers are encouraged to make the

yw1w2wn. . .same prediction on any unlabeled example. However, multi-
ple feature views often do not naturally exist in practice, and
these methods resort to articially creating random feature
splits.

Graph based semi-supervised learning [22, 1] assumes that
labeled and unlabeled examples are connected by a graph,
where edges represent similarity between examples. The dis-
criminant function is encouraged to vary smoothly with re-
spect to the graph. As a result, connected nodes tend to
have the same label. One interpretation of this it is that
labels propagate through unlabeled examples via graph
edges. However, the graph is usually constructed from the
distances in feature space, and is susceptible to overlap-
ping classes. Indeed if unlabeled data from dierent classes
strongly overlap, the graph will be wrong, and the method
can be expected to be inferior to supervised learning.

Semi-supervised or transductive support vector machines
(S3VMs, TSVMs) [8, 7] also assume that there is a wide
margin in kernel induced feature space between unlabeled
data from dierent classes. The margin may be dierent
than the traditional margin of labeled examples. S3VMs
attempt to place the decision boundary in the unlabeled
margin. However, there are two issues: First, such margin
may not exist if the classes strongly overlap even in the
kernel induced feature space. Second, S3VMs involves a
highly non-convex optimization problem which is dicult
to solve [6].

Generative semi-supervised methods based on the expec-
tation maximization (EM) algorithm assume a model for
the input distribution. In [17] Nigam, et al. use the EM al-
gorithm in conjunction with a mixture of multinomials, or
naive Bayes, generative model. In the E step, each unlabeled
example is assigned a label distribution according to its ex-
pected value under the current model. In the M step, the
multinomial parameters are re-estimated.
In practice this
model can fail when the assumption of independent input
features is violated or when the best generative structure
does not correspond to the decision boundary.

4.2 Generative Discriminative Hybrids

There have been several successful applications of hybrid
generative/discriminative methods in addition to the two
approaches that are the focus of this paper. In many ap-
proaches, parameters are separated into two subsets, one of
which is trained discriminatively and the other generatively.

Raina et al. [18], present a model for document classi-
cation in which documents are split into multiple regions.
For a newsgroup message, regions might include the header
and the body.
In this model, each region has its own set
parameters that are trained generatively, while the param-
eters that weight the importance of each region in the nal
classication are trained discriminatively. Experimental re-
sults show that this hybrid algorithm gives better classica-
tion accuracy than either naive Bayes or logistic regression
(a generative/discriminative pair [15]) alone. Additionally,
Raina, et al. show that because the number of discrimina-
tive parameters in the model is small, only a small amount
of training data is required to estimate these parameters.

Kang and Tian [9] extend naive Bayes by splitting fea-
tures into two sets X1 and X2. The directed graphical model

for this approach has nodes X1 as the parents of the class
variable Y , and parameters X2 as the children of Y . Param-
eters of this model are estimated by maximizing P (Y |X1)
(the discriminative component), and P (X2|Y ) (the genera-
tive component). An iterative algorithm based on classica-
tion accuracy is used to decide which features go in X1.

Bouchard and Triggs [3] propose a method to trade-o
generative and discriminative modeling that is similar to
multi-conditional learning because it maximizes a weighted
combination of two likelihood terms using one set of param-
eters. Dening Lgen = ln P (Y, X) and Ldisc = ln P (Y |X),
Bouchard and Triggs present a combined objective function
L =  ln P (X, Y ) + (1  ) ln P (Y |X). A subtle dierence
between this objective function and those presented here is
that in their approach the generative component is the full
joint distribution. As in other related work, experimental
results show that highest accuracy is obtained somewhere
between fully generative (in this case  = 1) and fully dis-
criminative ( = 0).

5. EXPERIMENTS

Semi-supervised learning methods are rarely applied in
practice, in part because there are few empirical compar-
isons of multiple semi-supervised methods. Additionally,
many semi-supervised experiments use binary datasets that
have few features and are easily separable. Here we pro-
vide a substantial comparison of two semi-supervised hy-
brid generative/discriminative methods and two prominent
semi-supervised learning methods, as well as their super-
vised counterparts, on multi-class datasets with large num-
bers of features.

5.1 Setup

We rst discuss the supervised/semi-supervised pairs of

methods we use in the experiments.

Naive Bayes / EM Naive Bayes (nb,emnb)
We use the naive Bayes implementation in the Mallet toolkit
[14]. As in Nigam et.al [17], we use Laplace (plus-1) smooth-
ing, so that unseen events do not get zero probability.

SVM / Transductive SVM (svm,tsvm)
In our experiments we use Universvm, an SVM implementa-
tion introduced in [7] that uses the Concave-Convex Proce-
dure (CCCP) to optimize transductive SVMs. Collobert et
al. show that optimizing TSVMs with CCCP improves ac-
curacy and decreases training time when compared to other
heuristic methods. The TSVM introduces several hyperpa-
rameters that need to be tuned. In our experiments, we tune
C  {105, 103, 101, 10, 103, 105} and
C  {105, 103, 101, 0, 10}, the cost parameters for the
labeled and unlabeled data, respectively. Collobert et al.
also tune the symmetric ramp loss for the unlabeled data,
but doing a grid search over three parameters, each with
several possible values, was not practical for large-scale com-
parison. We set the symmetric ramp loss parameter to 0.5
in all experiments and use a linear kernel. Although we do
not perform normalization of feature counts for other meth-
ods, we nd that this is very important to achieve reason-

able TSVM results and therefore normalize feature vectors
to have Euclidean length 1 for the SVM and TSVM experi-
ments.

Max Entropy / Multi-Conditional Method (me,mcl )
We use the implementation of maximum entropy models in
Mallet [14] and also use this framework to implement the
multi-conditional method. For both we tune the Gaussian
prior variance 2  {0.01, 0.1, 1, 10, 100}. For the multi-
conditional method, we also tune the relative weighting of
the discriminative component   [102, 107] at every order
of magnitude. We use  = 1 for all experiments. See Section
2.1 for some discussion of settings for  and .

Parameter Coupling Prior (pcp)
We implement this model in Mallet [14] as well. Following
Lasserre et al. [11], we tune the parameter , which is trans-
lated into a value for , the strength of the coupling prior,
1 )2. We use  = [0.1, 0.9] in intervals of
using () = ( 
0.1. We use 2 = 1 for the Gaussian prior on parameters,
because we nd that tuning this value provides little benet
when compared to the extra time it requires (we later show
that pcp requires more time to train than the other meth-
ods). As above, the supervised counterpart for this method
is the maximum entropy classier.

We run experiments on ve text classication datasets and
one sliding-window sequence labeling classication dataset.
For the text classication datasets, features correspond to
word occurrence counts. For the NER task, features are bi-
nary word occurrences and properties of those words (such as
capitalization) within three time steps. Stopwords, HTML,
message headers (where appropriate), and features that only
occurred once are removed from all datasets. Where noted,
low frequency features are also removed.

Datasets

 movie (24,841 features, 2 classes) Classify the senti-
ment of movie reviews from IMDB as positive or neg-
ative.

 webkb (22,824 features, 4 classes) Classify university

webpages as student, course, faculty, or project.

 sraa (77,494 features, 4 labels) Classify messages by
the newsgroup to which they were posted: simulated-
aviation, real-aviation, simulated-autoracing, realauto.

 sector (22,835 features 38 labels) Classify webpages

into specic industry sectors.

 blogs (95,583 features, 4 labels) Classify the age of a
blogger given blog posts. Due to the large number of
features in this dataset, those that occur less than 10
times are removed. This dataset was introduced in
[19].

 ner (60,502 features, 9 labels) Sliding-window named-

entity recognition using the CoNLL 2003 dataset.

Although all datasets are labeled, we simulate unlabeled
data by ignoring the labels for some examples. Specically,

we choose examples to remain labeled randomly, but en-
sure that the number of labeled examples is the same for
each class as in [17]. We treat the remaining examples as
unlabeled, up to a maximum of 5,000 unlabeled examples.
The success of semi-supervised learning is dependent on the
quality of the seed set of labeled examples. Therefore, we
average results over ve random labeled sets. We report
accuracy on a held-out test data, rather than reporting ac-
curacy on the unlabeled data, as is done with TSVMs [7].

Many semi-supervised methods introduce hyperparame-
ters including graph methods [22, 1] and TSVMs [7]. We
discuss the issue of choosing hyperparameters and provide
some heuristics that reduce the need for hyperparameter
tuning for generative/discriminative methods in Section 5.3.
For these experiments we use a grid search to nd the best
settings, and use parameters settings that give the best test
set accuracy, as also in [7]. Therefore, these results can
therefore be interpreted as an indication of the potential of
these methods, though further research is needed for prac-
tical parameter tuning.

5.2 Results and Discussion

Classication accuracy results are presented in Table 1.
Either MCL or PCP achieves the highest accuracy in 75%
of the experiments, and MCL and PCP achieve the largest
mean accuracy improvements over their supervised counter-
part at 5.2% and 3.1%, respectively. Additionally, MCL is
the only method to show semi-supervised improvements on
every dataset. Figure 2 illustrates that for both MCL and
PCP the best accuracies are attained in the space between
purely generative and purely discriminative.

We argue in the introduction that hybrid approaches are
able to avoid or mitigate the assumptions of other semi-
supervised methods. We would like to determine if the
cases in which the hybrid methods perform well empirically
match our intuitive justications. It is dicult to quanti-
ably compare the degree of overlapping classes or the degree
to which model assumptions fail across text datasets, but we
can gain some insight by considering the datasets on which
hybrid semi-supervised methods do well and other methods
do poorly.

First notice that naive Bayes performs worst on datasets
sector and ner, in which EM naive Bayes gives lower ac-
curacy than supervised naive Bayes. Additionally, despite
semi-supervised improvements, EM naive Bayes gives much
lower accuracy on blogs than other methods. These three
datasets have the largest number of classes, most compli-
cated and correlated features, and greatest number of fea-
tures, respectively. Highly correlated features clearly violate
the naive assumption of feature independence. We also ex-
pect that for these tasks the natural clusters in the data will
not necessarily be correlated with the decision boundary.
For example, both ner and blog involve a very specic task
on text that includes a wide variety of words and topics.
The hybrid methods are able to mitigate these generative
assumptions using the discriminative component.

Relative to the other methods, TSVMs perform poorly on
ner and sraa, cases in which we expect classes to be strongly
overlapping. The sraa dataset contains messages from news-
groups on simulated aviation, real aviation, simulated auto

dataset
movie (10)
movie (25)
webkb (10)
webkb (25)
sector (5)
sector (10)
ner (10)
ner (25)
sraa (10)
sraa (25)
blogs (10)
blogs (25)
mean

nb
59.7
62.0
60.4
68.2
30.7
41.2
26.1
25.4
63.1
74.5
30.6
32.3
47.9

emnb
62.1
61.2
65.6
73.4
13.2
21.6
2.6
3.0
70.6
78.5
38.8
35.7
43.9

svm tsvm me mcl
59.0
57.9
63.5
64.0
67.6
66.8
76.9
76.3
37.6
42.7
50.4
54.9
87.4
88.6
90.1
90.1
72.7
64.3
78.6
71.6
47.5
49.8
57.6
52.6
64.6
66.1

58.5
62.6
66.2
75.9
45.5
56.8
26.9
27.7
67.6
74.3
48.0
53.7
55.3

58.0
63.0
64.9
76.5
35.8
48.6
77.7
84.6
61.9
69.3
42.2
48.1
60.9

pcp
64.6
68.6
72.5
76.7
28.1
42.7
75.8
84.3
81.6
84.1
41.1
47.8
64.0

Table 1: Classication accuracy results. Parenthesized values indicate the number of labeled documents per
class.

feature
im
dont
lol
today
haha
yeah
good
fun
school
pretty
home
gonna
day
kinda
guess

di me mcl
.298
.039
.277
.021
.020
.278
.291
.020
.271
.019
.270
.016
.016
.275
.273
.014
.276
.013
.264
.010
.258
.010
.010
.269
.283
.010
.261
.009
.009
.258

.259
.255
.258
.271
.252
.254
.259
.260
.263
.254
.248
.259
.273
.251
.249

feature
nbsp
urllink
arianna
years
wife
frienz
couple
moved
town
bar
renee
world
doesn
city
ago

di
-.022
-.012
-.007
-.007
-.006
-.005
-.005
-.005
-.005
-.004
-.004
-.004
-.004
-.004
-.004

me mcl
.215
.237
.234
.246
.249
.243
.234
.240
.235
.242
.256
.261
.259
.254
.239
.245
.248
.252
.243
.247
.241
.245
.249
.244
.251
.256
.249
.253
.254
.250

Table 2: Parameters with largest positive and negative dierences in discriminative power for the blog dataset
with 10 labeled documents per class for label age 10-19 between supervised and semi-supervised with the
multi-conditional method. The Gaussian prior variance for the maximum entropy classier is 1.0. For the
MCL method,  = 105 and the Gaussian prior variance is 1.0, corresponding to a case when the testing
accuracy was 55.6.

feature
airspeed
altitude
pitch
preight
rudder
stephenames
downwind
ppl
ames
prop
garden
afm
ew
crab
climb

di me
.252
.122
.280
.109
.103
.252
.263
.103
.284
.103
.261
.102
.251
.101
.100
.282
.267
.098
.278
.098
.257
.096
.250
.095
.095
.251
.250
.095
.094
.251

pcp
.374
.390
.355
.366
.387
.362
.352
.381
.365
.375
.353
.345
.346
.345
.345

feature
font
autos
div
nextpart
iso
voodoo
meta
px
printable
mb
racing
gb
ns
cars
version

di
-.092
-.089
-.079
-.069
-.066
-.066
-.064
-.063
-.063
-.062
-.061
-.061
-.061
-.061
-.006

me
.250
.249
.250
.250
.250
.244
.250
.250
.250
.246
.233
.250
.248
.238
.232

pcp
.158
.160
.171
.181
.184
.178
.186
.187
.187
.184
.172
.189
.187
.177
.172

Table 3: Parameters with largest positive and negative dierences in discriminative power for the sraa dataset
with 25 labeled documents per class for label real aviation between supervised and semi-supervised with the
parameter coupling prior method. The Gaussian prior variance for the maximum entropy classier is 0.1.
For the PCP method,  = 0.6, corresponding to a case when the testing accuracy was 87.5.

Figure 2: Mean accuracies vs.  for hybrid methods. The dashed line represents the supervised accuracy.
The best accuracies are obtained in the region between purely generative and purely discriminative.

racing, and real automobiles. These classes have high word
overlap, as both simulated and real automobile newsgroups
will include words like tire and engine, for example. The
ner dataset contains classes for the start of a token type,
such as location, and the continuation of a token type. We
expect that the begin and continuation classes for a to-
ken type will not have a low density region between them,
and we see in the results that the TSVM chooses poor lo-
cations for the decision boundaries as the accuracy is much
worse than supervised.

We note that we originally conducted the SVM/TSVM
experiments without normalizing feature vectors, and the
TSVM only improved accuracy over the supervised SVM in
two cases (blog (10,25)). Interestingly, normalization seems
to have a much larger impact on semi-supervised learning
than on supervised learning. Excluding the ner dataset,
the mean SVM accuracy increases by 1.8% using normaliza-
tion, whereas the mean TSVM accuracy increases by 5.2%.
Even when using normalization, TSVMs do not always give
improvements over supervised SVMs. One possible explana-
tion is that most published evaluations of TSVMs use sim-
ple, two-class datasets. Additionally, TSVM experiments
typically use the eventual test data as unlabeled data dur-
ing training, whereas here we use a held-out test set that is
separate from the unlabeled training data. A more compli-
cated kernel may improve accuracy, but note that the other
methods only use linear combinations of features.

Comparing the two hybrid generative/discriminative meth-
ods, MCL achieves higher mean accuracy than PCP on seven
of the twelve experiments, as well as higher overall mean ac-
curacy across all tasks. Notice that the two methods rarely
perform well on the same dataset. The fundamental dier-
ence between the two methods is that MCL has one shared
set of parameters while PCP has two coupled sets of parame-
ters. An advantage of the PCP method is that the gradients
of the two components do not directly compete to modify
parameter values. This allows each component of the PCP
objective to have more freedom in modeling the data.
If
some generative parameter needs to be set in a way which
does not correspond to a good discriminative setting, this

penalty can absorbed if it leads to an improved model. Ad-
ditionally, the PCP method allows the inclusion of a prior
that only eects the discriminative parameters. With MCL,
a Gaussian prior on parameters helps prevent the discrimi-
native component from overtting, but because the param-
eters are shared, this prior has a negative impact on the
generative component. Namely, we observe that the penalty
on large parameter values tends to prevent large changes in
parameter value due to the generative component, as em-
pirically the generative parameters need to be more spread
out than their discriminative counterparts. However, it ap-
pears that the extra modeling power aorded by the PCP
method sometimes makes it more dicult to nd good pa-
rameter settings during training. We note that the cases in
which MCL gives improved accuracy over PCP tend to be
more complicated datasets in terms of the number of labels
or features. This suggests that PCP may be preferable for
less complicated datasets and MCL for more complicated
datasets.

In Tables 2 and 3 we show how parameter values change
after introducing the generative component that uses unla-
beled data. These tables show dierences in the discrimi-
native power of features in a supervised maximum entropy
model and a semi-supervised hybrid. The values for each
feature can be interpreted as the probability that a single
word document containing that word feature belongs to the
class listed in the table caption.
In Table 2, we see that
for the blogs dataset and label age 10-19, the discriminative
power of im, lol, school and home increases, and the
discriminative power of wife, couple, and bar decreases.
In Table 3, we see that for dataset sraa and label real avia-
tion, the discriminative power of airspeed, altitude, and
preight increases, while the power of autos, racing,
and cars decreases. Intuitively, we see that the addition
of the generative component helps to boost the discrimina-
tive power of features that the supervised model could not
discover with limited labeled data.

2345670.360.380.40.420.440.460.480.50.520.54Log AlphaMean AccuracyMean Accuracy vs. Alpha for Multi!Criteria Method: blogs (10)  semi!supervised hybridsupervised discriminative0.10.20.30.40.50.60.70.80.90.50.550.60.650.70.750.80.850.9AlphaMean AccuracyMean Accuracy vs. Alpha for Parameter Coupling Prior Method: sraa (25)  semi!supervised hybridsupervised discriminativemethod movie (10)
emnb
svm
tsvm
me
mcl
pcp
pcp-init

0.8
0.8
30.9
2.0
41.8
124.2
70.1

sraa (10)

ner (10)

2.3
8.5

796.4
12.2
745.1
3090.4
1618.3

4.8
63.4

1052.4

25.8

1535.3
2885.2
2182.6

Table 4: Mean training time in seconds.

5.3 Practical Considerations

Hyperparameter values are often important to the suc-
cess of semi-supervised learning methods, but tuning hy-
perparameters can be dicult in practice.
In supervised
learning, hyperparameter values are typically chosen using
cross-validation. If we apply this method for semi-supervised
learning, each fold test set ends up with a very small number
of labeled examples, since the total number of labeled ex-
amples is small. This means that the performance estimate
obtained from this test set may be inaccurate. Addition-
ally, optimal hyperparameter values may depend upon the
specic set of labeled examples. Training with a subset of
those labeled examples could result in drastically dierent
ideal settings for the the hyper-parameters, as, for example,
a poor seed set of labeled examples may require a hybrid
classier to put more weight on the generative component
and rely on unlabeled data. Another option is to choose
the hyperparameters that give the highest likelihood on the
training data. However, we have found that these hyperpa-
rameters produce the highest likelihood models because of
overtting. Finally, choosing hyperparameters using a vali-
dation set is not practical, as if more labeled data is avail-
able, it would almost certainly be more benecial to use that
data during training than to use it to tune hyperparameters.

Another practical issue in semi-supervised learning is that
the added complexity of semi-supervised training algorithms
increases the overall model training time, as shown in Ta-
ble 4. On average, the PCP method takes the longest to
train. In addition to having twice as many parameters as
the MCL method, it also seem to take more iterations to
converge. The EM naive Bayes semi-supervised learning is
extremely fast because the maximum likelihood estimates
can be computed in closed form.

We propose a heuristic for hybrid models to simultane-
ously reduce training time and reliance on hyperparameter
settings that involves ensuring reasonable initial parameter
settings. For the PCP method, we initialize both sets of pa-
rameters to the results of supervised training on the labeled
data. We present results using supervised initialization of
the PCP method in Table 5. This also reduces training
time, as shown by the entry pcp-init in Table 4. For the
MCL method, we propose to use the discriminative compo-
nent exclusively in the rst few iterations of training. Since
the discriminative component is convex, this guarantees that
we move into a reasonable region parameter space. At the
same time, by avoiding going the whole way to the global
maximum, we give the parameters room to move away from
the discriminative maximum.

dataset
movie (10)
movie (25)
webkb (10)
webkb (25)
sector (5)
sector (10)
ner (10)
ner (25)
sraa (10)
sraa (25)
blogs (10)
blogs (25)

accuracy

59.4
67.7
67.9
78.4
37.9
51.0
75.6
84.5
72.3
77.8
46.0
49.3

Table 5: Classication results when using supervised
initialization for parameter coupling prior model.
Parenthesized values indicate the number of labeled
documents per class.

These heuristics seem to make the algorithms more con-
sistent, as the mean accuracy across all hyperparameter set-
tings is higher. However, the maximum accuracies attained
are sometimes lower. Intuitively, one of the real benets of
these methods is that the generative component can pull the
discriminative component into regions of parameter space
very dierent from those chosen by supervised discrimina-
tive learning on the limited labeled data. With these initial-
izations, we ensure that we do not converge to a poor local
maximum, but sacrice the potential to nd drastically dif-
ferent parameter settings.

6. CONCLUSION AND FUTURE WORK

We have considered hybrid generative/discriminative ap-
proaches to semi-supervised classication in which the gen-
erative component includes unlabeled data. We compare
two methods for combining generative and discriminative
likelihood in detail: a multi-conditional learning method and
a method where each component has its own set of parame-
ters that are coupled by a prior distribution. In a substantial
empirical comparison, a hybrid method provides the best
accuracy in eight of the twelve experiments. Intuitively, we
conjecture that they perform well by mitigating the model-
ing assumptions of generative semi-supervised methods and
avoiding the low-density-between-class assumptions of dis-
criminative semi-supervised methods.

In future work, we would like to apply hybrid genera-

tive/discriminative approaches to transfer or multi-task learn-
ing, consider heuristic and probabilistic methods to learn
hyperparameters from data, and research alternative opti-
mization techniques utilizing ideas from the multi-criteria
optimization literature.

7. ACKNOWLEDGMENTS

This work was supported in part by the Center for Intel-
ligent Information Retrieval, in part by The Central Intel-
ligence Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249, in part
by NSF Nano #DMI-0531171, and in part by the Defense
Advanced Research Projects Agency (DARPA), through the

Department of the Interior, NBC, Acquisition Services Di-
vision, under contract number NBCHD030010. CP appre-
ciates support by Microsoft Research under the Memex and
eScience funding programs and support from Kodak Re-
search. Any opinions, ndings and conclusions or recom-
mendations expressed in this material are the authors and
do not necessarily reect those of the sponsor.

