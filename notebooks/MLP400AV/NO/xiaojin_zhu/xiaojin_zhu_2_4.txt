Abstract

We introduce Time-Sensitive Dirichlet Process Mixture models for clustering. The
models allow innite mixture components just like standard Dirichlet process mixture
models. However they also have the ability to model time correlations between in-
stances.

Research supported in part by NSF grants NSF-CCR 0122481, NSF-IIS 0312814, and NSF-
IIS 0427206. Zoubin Ghahramani was supported at CMU by DARPA under the CALO project.

Report Documentation Page

Form Approved

OMB No. 0704-0188

Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and
maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information,
including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington
VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it
does not display a currently valid OMB control number.

1. REPORT DATE
MAY 2005

2. REPORT TYPE

4. TITLE AND SUBTITLE
Time-Sensitive Dirichlet Process Mixture Models

6. AUTHOR(S)

7. PERFORMING ORGANIZATION NAME(S) AND ADDRESS(ES)
Carnegie Mellon University,School of Computer
Science,Pittsburgh,PA,15213

3. DATES COVERED
00-05-2005 to 00-05-2005

5a. CONTRACT NUMBER
5b. GRANT NUMBER
5c. PROGRAM ELEMENT NUMBER
5d. PROJECT NUMBER
5e. TASK NUMBER
5f. WORK UNIT NUMBER

8. PERFORMING ORGANIZATION
REPORT NUMBER

9. SPONSORING/MONITORING AGENCY NAME(S) AND ADDRESS(ES)

10. SPONSOR/MONITORS ACRONYM(S)

11. SPONSOR/MONITORS REPORT
NUMBER(S)

12. DISTRIBUTION/AVAILABILITY STATEMENT
Approved for public release; distribution unlimited

13. SUPPLEMENTARY NOTES
The original document contains color images.

14. ABSTRACT
15. SUBJECT TERMS
16. SECURITY CLASSIFICATION OF:

a. REPORT

unclassified

b. ABSTRACT

unclassified

c. THIS PAGE

unclassified

17. LIMITATION OF

ABSTRACT

18. NUMBER
OF PAGES

19a. NAME OF
RESPONSIBLE PERSON

14

Standard Form 298 (Rev. 8-98)
Prescribed by ANSI Std Z39-18

Keywords: I.2.6 [Articial Intelligence]:Learning; I.5.1 [Pattern Recognition]:Models

Statistical, I.5.2 [Pattern Recognition]:Design MethodologyClassier design and eval-
uation; General Terms: Algorithms; Additional Key Words: Dirichlet process mixture
models, MCMC, time

1 Introduction

Traditional clustering algorithms make two assumptions that are often false in practice:
1. that the number of clusters is known; 2. that the data points are independent. We
propose a model that allows innite number of clusters, and cluster members may have
certain dependency in time.

Consider emails received by a user over a period of time. Suppose we want to

cluster the emails by topic thread. There are several ways to do this:

 We can sort emails by the subject line. However it is unreliable and we want a

more exible probabilistic model based on email content.

 We can model each thread with a multinomial distribution over the vocabulary,
and treat each email as a bag of words. The whole email collection can be
modeled as a mixture of multinomial. The problem is that we do not know the
number of threads (mixing components). Fixing the number, which is a common
practice, seems arbitrary.

 We can model the collection as a Dirichlet process mixture model (DPM) [1].
DPMs allow potentially innite number of components. Nonetheless DPMs are
exchangeable. When applied to emails, this means that old threads never die
down. This is undesirable because we want the emails from years ago to have
less inuence than those from this morning in predicting the next email.

We therefore would like to introduce the concept of time into DPMs, while keeping
the ability to model unlimited number of clusters. This is achieved with the proposed
Time-Sensitive Dirichlet Process Mixture (tDPM) models.

2 The tDPM Framework

Consider a sequence of input d with time stamp t: (d1, t1), . . . , (dn, tn), where the time
monotonically increases. For concreteness let us assume the ds are email documents,
each represented as a bag-of-word vector. Let si  {1, 2, . . .} be the true cluster
membership (email thread) of di. Notice we do not set the number of clusters a priori.
There could potentially be an unlimited number of clusters as the number of documents
n grows.

Without loss of generality we assume that each cluster j is represented by a multi-
nomial distribution j over the vocabulary. The probability for cluster j to generate
document di is then

(1)

P (di|j) = Yvvocabulary

j(v)di(v)

Since past email threads can inuence the current email, we want si to depend on
the history s1, . . . , si1. We also want such dependency to vary with time: older emails
should have less inuence. We introduce a weight function w(t, j) which summarizes

1

2

1.5

1

0.5

w(t,1)
w(t,2)

2

1.5

1

0.5

0
5

0

5

t
(a)

10

15

20

0
5

0

5

10

15

20

t
(b)

Figure 1: (a) The time kernel with  = 0.5. (b) The weight functions with data from
two clusters, marked as star or circle respectively.

the history at time t. It gives the weight (or inuence) of cluster j at time t, given the
history so far s1, . . . , si : ti < t,

w(t, j) = X{i|ti<t,si=j}

k(t  ti)

(2)

Note the weight function is the sum of some time kernel k. In the email example we
can use a kernel like k(t) = exp(t) if t  0, and k(t) = 0 if t < 0. This kernel
stipulates that an email will boost the probability of the same thread in later emails, but
the boost decreases exponentially as specied by the parameter . Figure 1(a) shows
an example time kernel with  = 0.5, while (b) shows two weight functions built upon
the kernel. In the example there are documents from cluster 1 at time 0,2,6, and from
cluster 2 at time 3,4. Other forms of the time kernel are possible too.

We dene the prior probability of assigning cluster j to di, given the history s1, . . . , si1,

to be

P (si = j|s1, . . . , si1)

= P (si = j|w(ti, ))

= (

w(ti,j)

Pj 0 w(ti,j 0)+



Pj 0 w(ti,j 0)+

if j is in history
if j is new

(3)
(4)

(5)

where  is a concentration parameter. We call this a time-sensitive Dirichlet process
mixture (tDPM) model. Intuitively if there has been many recent emails from cluster
j, the new email will have a large probability also from j. In addition, there is always
a possibility that the new email is from a new cluster not seen so far.

tDPM is very similar to the standard Dirichlet process mixture (DPM) models.
In fact, it can be shown that if the time kernel k is a step function, then we recover

2

w

s

d

t

w

w

t

s

d

t

s

d



Figure 2: The graphical model for Time-sensitive Dirichlet Process Mixture models. d
is the feature (e.g. words of an email), t is the time stamp, s is the cluster label, and w
is the sufcient statistic that summarizes the history. Shaded nodes are observed.

the standard DPMs. It is the decaying of k over time that allows us to include time
information in to the process. The graphical model representation of tDPM is given in
Figure 2.

3 Inference

Given d and t, we would like to infer s. We use a Markov Chain Monte Carlo method.
Notice w is a deterministic function of s and t and does not need to be sampled. As
shown later if we used conjugate priors, we do not need to actually sample  but can
analytically integrate it out. Therefore we only need to sample s.
In Gibbs sampling, we need to sample si from the distribution

P (si = j|si, d1, . . . , dn)  P (si = j|si)P (di|di:si=j)

(6)

where di:si=j is the set of documents in cluster si = j, excluding di.
The prior P (si = j|si) in (6) involves all nodes before and after si:

P (si = j|si)

P (sm|s1, . . . , sm1)! P (si = j|s1, . . . , si1)  n
Ym=i+1

   i1
Ym=1
 P (si = j|s1, . . . , si1)  n
Ym=i+1

P (sm|s1, . . . , sm1)!

P (sm|s1, . . . , sm1)!

(7)

Substituting in the denition (5), it is easy to show that the denominators are the same
for different values of j, and the only difference is in the numerator.

3

The likelihood term p(di|di:si=j) in (6) is domain-specic. For the email task,

a Dirichlet-multinomial [2] is the natural choice:

p(di|di:si=j) = Z p(di|)p(|di:si=j)d

(8)

where p(|di:si=j) is a posterior Dirichlet distribution. The posterior is derived
from a prior (base) Dirichlet distribution G0, and the observed data di:si=j. Let the
Dirichlet prior G0 be parameterized by m, where m is a vector over the vocabulary
and m sums to 1, and  is the strength of the prior:

p(|m) =

mv1
v

(9)

()

Qv (mv)Yv

Treating the document collection di:si=j as a single, large document, the Dirichlet
posterior after observing counts fv for word v in di:si=j is

p(|di:si=j) = p(|f, m) =

And the Dirichlet-multinomial is

(Pv fv + )
Qv (fv + mv)Yv

fv+mv1
v

(10)

(11)

(12)

P (di|di:si=j) = Z p(di|)p(|di:si=j)d

=

(Pv fv + )
Qv (fv + mv) Qv (di(v) + fv + mv)
(Pv di(v) +Pv fv + )

Putting everything together for (6), we can x all other s and sample for si. A single
Gibbs sampling iteration consists of looping through i = 1 . . . n and sample si in turn.
The algorithm is given in Figure 3. The time complexity is O(n2) for each iteration
of the Gibbs sampler. If k has limited support, the complexity reduces O(n) but we
lose the ability to model long range correlations. Finally we run the Gibbs sampler for
many iterations to get the marginals on s.

Some readers may be disturbed by the apparent double counting in Figure 3 when
we assign u(c) =  to not only the brand new state cnew, but also to states not in {s<i}
but in {s>i}. We assure the readers that it is merely an artifact of numbering. If we
were to renumber the states at each iteration, we can recover (5) exactly.

4 Parameter Learning

The parameters of the model include the base Dirichlet distribution G0, the concentra-
tion parameter , and the time kernel parameter . We x the base Dirichlet G0. For
the time being let us assume that all clusters share the same kernel parameter . The
free parameters are  = {, }.

We learn the parameters by evidence maximization. Since our model is conditioned

on time, the evidence is dened as

P (D|T, ) =XS

P (D|S)P (S|T, )

(13)

4

for position i = 1 to n

/* C is the candidate states for si, */
/* where {si} is the set of current states at positions other than i, */
/* and cnew / {si} is a new state, represented by an arbitrary new number. */
C = {si}  {cnew}

/* Compute the unnormalized probability p(si = c|si) for all candidate c */
for c  C

/* evaluate candidate si = c */
si  c
/* Prior: the history part. {s<i} is the set of states before position i */
if c  {s<i} then u(c) = wc(ti)
else u(c) = 
/* Prior: the future part. */
for j = i + 1 to n

if sj  {s<j} then u(c) = u(c)  wsj (tj)
else u(c) = u(c)  

end
/* Likelihood. */
u(c) = u(c)  P (di|di:si=c)

end

/* pick the state si with probability proportional to u() */
si  u(C)

end

Figure 3: A single Gibbs sampling iteration for tDPM

5

where D is the set of all documents, T is the corresponding set of time stamps, and S
is the set of cluster assignments. We want to nd the best parameters  that maximize
the evidence:

 = arg max



P (D|T, )

= arg max

 XS

P (D|S)P (S|T, )

(14)

(15)

We nd the parameters with a stochastic EM algorithm. The cluster labels S are
hidden variables. Let 0 be the current parameters. We can sample S(1) . . . S(M ) from
the posterior distribution P (S|D, T, 0), as detailed in section 3. In generalized EM
algorithm, we seek a new parameter  which increases the expected log likelihood of
the complete data

Q(0, ) = EP (S|D,T,0) [log P (S, D|T, )]

= EP (S|D,T,0) [log P (D|S) + log P (S|T, )]

(16)
(17)

Notice log P (D|S) does not depend on , . We approximate the expectation by sam-
ple average

Q(0, ) = Const() + EP (S|D,T,0) [log P (S|T, )]

 Const() +

1
M

M

Xm=1

log P (S(m)|T, )

And we nd the gradients w.r.t.  for parameter update

Q




=




1
M

1
M

M

Xm=1

M

N

Xm=1
Xi=1

log P (S(m)|T, )




log P (s(m)

i

|s(m)

1

. . . s(m)

i1, T, )

where P (s(m)

i

|s(m)

1

. . . s(m)

i1, T, ) is dened in (5). The gradients are:

(18)

(19)

(20)

(21)




log P (si|s1 . . . si1, T, ) = ( 

1
 

Pc w(ti,c)+

1

Pc w(ti,c)+

1

if si in history
if si new

(22)

log P (si|s1 . . . si1, T, )


w(ti,si)  Pc
 w(ti,si)
 Pc


 w(ti,c)
Pc w(ti,c)+


 w(ti,c)
Pc w(ti,c)+




= 


6

if si in history
if si new

(23)

where

w(t, c) = Xi:ti<t,si=c
w(t, c) = Xi:ti<t,si=c




k(t  ti) =X e(tti)

(t  ti)e(tti)

(24)

(25)

We then take a gradient step in the M-step of the generalized EM algorithm to improve
the log likelihood.

5 Experiments

We create synthetic datasets which have explicit time dependency between instances,
and use them to illustrate the time sensitivity of tDPM models.

All synthetic datasets have n = 100 instances. We rst create the time stamps of
each instances by sampling from a Poisson process. In particular, the interval between
two consecutive time stamps has an exponential distribution with mean 1/ = 1:

p(ti+1  ti) = e(ti+1ti)

(26)

For the instance di at time ti, its state si is sampled from the conditional distribution
(5). We use an exponential function as the kernel k,

k(t) = e0.5t, t  0

(27)

and the concentration parameter  is set to 0.2. This emulates the situation where new
clusters are created from time to time, and a cluster stays alive better if many preceding
instances are from the cluster.

If a new cluster c is created, we sample its multinomial distribution c from the
base distribution G0. The base distribution G0 is a at Dirichlet on a vocabulary of
size three: G0  Dir(1, 1, 1), so that all multinomials are equally likely. Finally docu-
ments are sampled from their corresponding multinomial , where all documents have
the same length |d|. We create two datasets with document length |d| equals 20 and
50 respectively, with everything else being the same. Given that the vocabulary size is
3, they correspond to somewhat hard (less words) and easy (more words) datasets re-
spectively. Figure 4 shows time vs. cluster plots of the two datasets. Notice documents
from the same cluster tend to group together in time, which ts our intuition on real
world problems like emails.

During evaluation, the input to various algorithms are the documents di and their
time stamps ti, and the goal is to infer the clustering si. Notice the true number of
clusters is not given to the algorithms.

For the tDPM model, we assume we know the true base distribution G0  Dir(1, 1, 1),

concentration parameter  = 0.2, and the kernel k(t) = e0.5t. We run the Gibbs sam-
pler with initial states s1 = . . . = sn = 1. Each MCMC iteration updates s1, . . . , sn
once, and thus consists of n Gibbs steps. We ignore the burn-in period of the rst 100
MCMC iterations, and then take out a sample of s1, . . . , sn every 11 iterations. In this

7

experiment we take out 109 samples altogether. We evaluate the performance of tDPM
by three measures:

1. Number of clusters discovered. Notice each sample s1, . . . , sn is a clustering of
the data, and different samples may have different number of clusters. In fact
Figure 5(a,b) shows the distribution of number of clusters in the 109 samples,
on the hard (|d| = 20) and easy (|d| = 50) synthetic datasets respectively. The
modes are at 12 and 15, very close to the true values of 12 and 14 respectively.

2. Confusion matrix. One way to combine the samples with possibly different num-
ber of clusters is to compute the n  n confusion matrix M, where Mij is the
probability that i, j are in same cluster. This can be easily estimated from the
109 samples by the frequency of i, j in the same cluster. Ideally M should be
similar to the true confusion matrix M , dened by M 
ij = 1 if the true cluster
has label si = sj, and 0 otherwise. In Figure 5(c,d) we plot the true confusion
matrices M . Notice we sort the instances by their true cluster for better visu-
alization. In Figure 5(e,f) we plot the tDPM confusion matrices, using the same
order. They are reasonably similar.

3. Variation of Information. We compute the variation of information measure [3]
between the true clustering and each sample clustering. We list the mean and
standard deviation for the two synthetic datasets: (hard) 0.9272  0.1718, (easy)
0.1245  0.0911.

We compare tDPM to a standard DPM model, by using a step function as the ker-
nel k. Again we assume we know the true base distribution G0  Dir(1, 1, 1), and
concentration parameter  = 0.2. The Gibbs sampling is done exactly the same as in
tDPM. We nd that

1. Number of clusters discovered. Figure 6(a,b) shows the distribution of number
of clusters with DPM. DPM discovers fewer clusters than tDPM. The modes are
at 6(or 7) and 9 respectively, and the true values are 12 and 14.

2. Confusion matrix. In Figure 6(c,d) we plot the DPM confusion matrices. Notice

they are much less similar to the true matrices.

3. Variation of Information. With DPM we have (hard) 1.8627  0.1753, (easy)
0.6630  0.1144. This means the sample clusterings are signicantly farther
away from the true clustering, compared to tDPM.

To summarize, tDPM is better than the standard DPM model, when the instances

have a time dependency.

6 Discussions

The tDPM model is a way to take time into consideration. Notice it is different than
simply adding time as a new feature for cluster.

8

The tDPM is not time reversible nor exchangeable in general. This is different from
the standard DPM. It is both a blessing and curse. It allows for the modeling of time,
but at the expense of computation.

There are many ways one can extend the tDPM model proposed here:

 The time kernel k can have different forms. For example, different clusters can
have different decay rate . More interestingly, k can even be periodic to model
repetitive emails like weekly meeting announcements.

 Currently the models for each cluster are stationary and do not evolve over time.

This can potentially be relaxed.

 One can have a generative model on time dependencies. For example one can as-
sume a Poisson process on cluster, and then a non-homogeneous Poisson process
on the documents within the cluster.

