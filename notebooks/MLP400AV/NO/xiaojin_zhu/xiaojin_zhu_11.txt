Abstract

Kernel conditional random elds (KCRFs) are
introduced as a framework for discriminative
modeling of graph-structured data. A repre-
senter theorem for conditional graphical mod-
els is given which shows how kernel condi-
tional random elds arise from risk minimization
procedures dened using Mercer kernels on la-
beled graphs. A procedure for greedily select-
ing cliques in the dual representation is then pro-
posed, which allows sparse representations. By
incorporating kernels and implicit feature spaces
into conditional graphical models, the framework
enables semi-supervised learning algorithms for
structured data through the use of graph kernels.
The framework and clique selection methods are
demonstrated in synthetic data experiments, and
are also applied to the problem of protein sec-
ondary structure prediction.

1. Introduction

Many classication problems involve the annotation of data
items having multiple components, with each component
requiring a classication label. Such problems are chal-
lenging because the interaction between the components
can be rich and complex. In text, speech, and image pro-
cessing, for example, it is often useful to label individual
words, sounds, or image patches with categories to enable
higher level processing; but these labels can depend on one
another in a highly complex manner. For biological se-
quence annotation, it is desirable to annotate each amino
acid in a protein with a label, with the collection of labels
representing the global geometric structure of the molecule.
Here the labels in principle depend on the physical char-
acteristics of the molecule and its ambient chemical envi-

Appearing in Proceedings of the 21 st International Conference
on Machine Learning, Banff, Canada, 2004. Copyright 2004 by
the authors.

ronment. In each case, classication tasks naturally arise
which clearly violate the assumption of independent and
identically distributed instances that is made in the majority
of classication procedures in statistics and machine learn-
ing. It is therefore of central importance to extend recent
advances in classication theory and practice to structured,
non-independent data classication problems.

Conditional random elds (Lafferty et al., 2001) have been
proposed as an approach to modeling the interactions be-
tween labels in such problems using the tools of graphical
models. A conditional random eld (CRF) is a model that
assigns a joint probability distribution over labels condi-
tional on the input, where the distribution respects the in-
dependence relations encoded in a graph. In general, the
labels are not assumed to be independent, nor are the ob-
servations conditionally independent given the labels, as
is assumed in generative models such as hidden Markov
models. The CRF framework has already been used to ob-
tain promising results in a number of domains where there
is interaction between labels, including tagging, parsing
and information extraction in natural language processing
(Collins, 2002; Sha & Pereira, 2003; Pinto et al., 2003) and
the modeling of spatial dependencies in image processing
(Kumar & Hebert, 2003).
In related work, Taskar et al.
(2003) have studied random elds (also known as Markov
networks) t using loss functions that incorporate a gener-
alized notion of margin, and have observed how the kernel
trick applies to this family of models.

We present an extension of conditional random elds that
permits the use of implicit features spaces through Mercer
kernels, using the framework of regularization theory. Such
an extension is motivated by the signicant body of recent
work that has shown kernel methods to be extremely effec-
tive in a wide variety of machine learning techniques; for
example, they enable the integration of multiple sources of
information in a principled manner. Our introduction of
Mercer kernels into conditional graphical models is also
motivated by the problem of semi-supervised learning. In
many domains, the collection of annotated training data is
difcult and costly, as it requires the efforts of expert hu-
man annotators, while the collection of unlabeled data may

be relatively easy and inexpensive. The emerging theme in
recent research in semi-supervised learning is that kernel
methods, in particular those based on graphical representa-
tions of unlabeled data, form a theoretically attractive and
empirically promising set of techniques for combining la-
beled and unlabeled data (Belkin & Niyogi, 2002; Chapelle
et al., 2002; Smola & Kondor, 2003; Zhu et al., 2003).

In Section 2 we formalize the learning problem and present
a version of the classical representer theorem of Kimeldorf
and Wahba (1971). Unlike the classical result, for kernel
conditional random elds the dual parameters depend on
all potential assignments of labels to cliques in the graph,
not only the observed labels. This motivates the need for
algorithms to derive sparse representations, since the full
representation has parameters for each labeled clique in
the graphs appearing in the training data. In Section 3 we
present a greedy algorithm for selecting a small number of
representative cliques. This clique selection algorithm
parallels the import vector selection algorithms of kernel
logistic regression (Zhu & Hastie, 2001), and the feature
selection methods that have been previously proposed for
random elds and conditional random elds using explicit
features (McCallum, 2003).

In Section 4 the ideas and methods are demonstrated on
two synthetic data sets, where the effects of the underly-
ing graph kernels, clique selection, and sequential model-
ing can be clearly seen. In Section 5 we report the results
of experiments using kernel CRFs for protein secondary
structure prediction. This is the task of mapping primary
sequences of amino acids onto a string of secondary struc-
ture assignments, such as helix, sheet, or coil. It is widely
believed that secondary structure can contribute valuable
information to discerning how proteins fold in three dimen-
sions. We compare kernel conditional random elds, esti-
mated using clique selection, against support vector ma-
chine classiers, with both methods using kernels derived
from position-specic scoring matrices (PSI-BLAST pro-
les) as input features. In addition, we give results for the
use of graph kernels derived from the PSI-BLAST proles
in a transductive, semi-supervised framework for estimat-
ing the kernel CRFs. The paper concludes with a brief dis-
cussion in Section 6.

2. Representation

Before proceeding with formalism, we give some intuition
for what our framework is intended to capture. Our goal
is to annotate structured data, where the structure is repre-
sented by a graph. Labels are to be assigned to the nodes
in the graph in order to minimize some loss function, such
as 0-1 error; the labels come from a small set Y, for ex-
ample, Y = {red, blue, green}. Each vertex in the
In
graph is associated with a feature vector xv  X .

image processing, the feature vector at a node might in-
clude a pixel intensity, as well as average pixel intensities
smoothed over neighboring regions using wavelets. In pro-
tein secondary structure prediction, each node might corre-
spond to an amino acid in the protein, and the feature vector
at a node may include an amino acid histogram of all pro-
tein fragments in a database which closely match the given
protein at that node. In the following section we present
our notation and formal framework for such problems.

2.1. Cliques and labeled graphs

Let G denote a collection of nite graphs. For example, G
might be the set of nite chains, appropriate for sequence
modeling, or the rectangular 2-dimensional grids, appro-
priate for some image processing tasks. The set of ver-
tices of a graph g  G is denoted by V (g), and size of the
graph is the number of vertices, denoted |g| = |V (g)|. A
clique is a subset of the vertices which is fully connected,
with any pair of vertices joined by an edge; we denote
the set of cliques in the graph by C(g). The number of
vertices in a clique is denoted by |c|. Similarly, we de-
note by C(G) = {(g, c) | g  G, c  C(g)} the collection
of cliques across varying graphs. In other words, a mem-
ber of C(G) consists of a graph and a distinguished clique
of that graph. We will work with kernels that compare
components of different graphs. For example, we could
consider a kernel K : C(G)  C(G)  {0, 1} given by
K((g, c), (g0, c0)) = (|c|, |c0|).

We next consider labelings of a graph. Let Y be a -
nite set of labels;
innite Y is also possible in a re-
gression framework, but we restrict to nite Y for sim-
plicity. The set of Y-labelings of a graph g is denoted

labeled graphs is Y(G) = {(g, x) | g  G, y  Y(g)}.
Similarly, let X be an input feature space; for example,

Y(g) = (cid:8)y | y  Y |g|(cid:9), and the collection of all Y-
X = Rn. The set X (g) = (cid:8)x | x  X |g|(cid:9) denotes the
YC(g) = (cid:8)(c, yc) | c  C(g), yc  Y |c|(cid:9) be the set of Y-

set of assignments of a feature vector to each vertex of
the graph g; X (G) = {(g, x) | g  G, x  X (g)} is the
collection of all such annotated graphs.
Finally, let

labeled cliques in a graph. As above, we similarly dene
X YC(g) = {(x, c, yc) | x  X (g), (c, yc)  YC(g)} and
X YC(G) = {(g, x, c, yc) | (x, c, yc)  X YC(g)}.

2.2. Representer Theorem

The prediction task for conditional graphical models is to
learn a function h : X (G)  Y(G) where h(g, x)  Y(g)
is a labeling of g, with the goal of minimizing a suitably
dened loss function. The classier h = hn is chosen
, with each
(i)) being a labeled graph, the graph possibly

based on a labeled sample(cid:8)(g(i), x

(g(i), x
changing from example to example.

(i))(cid:9)n

(i), y

(i), y

i=1

To limit the complexity of the hypothesis, we will as-
sume that it is determined completely by a function f :
X YC(G)  R. Let f (g, x) denote the collection of values
{f (g, x, c, yc)}, with c  C(g) varying over the cliques
of g and yc  Y |c| varying over all possible labelings of
that clique. We assume that a loss function (y, f (g, x)) is
given. As an important example, and the loss function used
in this paper, consider the negative log loss

exp XcC(g)

fc(x, y

(1)

0

c)

(y, f (g, x)) =

 XcC(g)

fc(x, yc) + log Xy0Y(g)

where fc(x, yc) is shorthand for f (g, x, c, yc). The neg-
ative log marginal loss could also be considered for mini-
mizing the per-node error. The negative log loss function
corresponds to a conditional random eld given by

p(y | g, x) = Z 1(g, x, f ) exp Xc

fc(x, yc)!

(2)

We now discuss how the representer theorem of kernel
machines (Kimeldorf & Wahba, 1971) applies to condi-
tional graphical models. While this is a simple extension,
were not aware of an analogous formulation in the statis-
tics or machine learning literature.

Let K be a Mercer kernel on X YC(G); thus

K((g, x, c, yc), (g0, x

0, c0, y

0
c0))  R

0, c0, y

for each (x, c, yc)  X YC(g) and (x
c0)  X YC(g0).
0
Intuitively, this assigns a measure of similarity between a
labeled clique in one graph and a labeled clique in a (pos-
sibly) different graph. We denote by HK the associated re-
producing kernel Hilbert space, and by kkK the associated
norm on L2(X YC(G)).
Consider a regularized loss function of the form

Rf =

nXi=1

(cid:16)y

(i), f (g(i), x

(i))(cid:17) +  (kf kK)

It is important to note that the loss depends on all possi-
ble assignments yc of labels to each clique, not just those
observed in the labeled data y
(i). Suppressing the depen-
dence on the graph g in the notation, let Kc(x, yc; , ) =
K((g, x, c, yc), ). Following the argument for the stan-
dard representer theorem, it can easily be shown that the
minimizer of a regularized loss function of the above form
can be expressed in terms of the basis functions f () =
K(, (g(i), x

(i), c, yc)).

kkK, and let  : R+  R+ be strictly increasing. Then
theminimizerf ? of

Rf =

nXi=1

(cid:16)y

(i), f (g(i), x

(i))(cid:17) +  (kf kK)

ifitexists,hastheform

f ?() =

nXi=1 XcC(g(i)) XycY |c|

(i)

c (yc) Kc(x

(i), yc; )

The key property distinguishing this result from the stan-
dard representer theorem is that the dual parameters
(i)
c (yc) now depend on all assignments of labels.

2.3. Two special cases

Thus,

Let K be a Mercer kernel on Z = X  Y 
Y.
the kernel is dened in terms of the ma-
0) where z = (x, y1, y2). Using
trix entries K(z, z
K we can dene a kernel on edges in X YC(G) by
K ((g, x, (v1, v2), (y1, y2)), (g0, x
2))) =
2)). For the regularized risk
0
K((xv1 , y1, y2), (x
v0
1
minimization problem

2), (y0

1, y0

1, v0

0, (v0

, y0

1, y0

min
f HK

R(x, f, ) = min
f HK

nXi=1

(y

(i), f (x

(i))) +  kf kK

where f  HK, the CRF representer theorem implies that
the solution f ? has the form

f ?
(v1,v2)(x, y1, y2) =

nXi=1Xy,y0 X(v,v0)

(i)
(v,v0)(y, y0) K((xv1, y1, y2), (x

(i)
v , y, y0))

In the special case of kernel K(z, z
it follows that

0) = K(x, x0) (y1, y0
1)

f ?
(v1,v2)(x, y1, y2) =

nXi=1 XvV (g(i))

(i)

v (y1) K(xv1 , x

(i)
v )

Under the probabilistic model (2), this is simply kernel
logistic regression.
0) =
K(x, x0) (y1, y0

In the special case of K(z, z

2) we get that

1) + (y1, y0

1) (y2, y0

f ?

(v1,v2)(x, y1, y2) =Xi,v

(i)

v (y1) K(x

(i)
v , xv1 ) + (y1, y2)

and we recover a simple type of semiparametric CRF.

3. Clique Selection

Proposition (Representer theorem for CRFs). LetK be
a Mercerkernelon X YC(G) withassociated RKHS norm

The representer theorem shows that the minimizing func-
tion f is supported by labeled cliques over the training

examples; however, this may result in an extremely large
number of parameters. We therefore pursue a strategy of
incrementally selecting cliques in order to greedily reduce
the regularized risk. The resulting procedure is parallel to
forward stepwise logistic regression, and to related meth-
ods for kernel logistic regression (Zhu & Hastie, 2001),
as well as to the greedy selection procedure presented in
(Della Pietra et al., 1997).

Our algorithm will maintain an active set A =

(cid:8)(g(i), c, yc)(cid:9)  YC(G) of labeled cliques, where the la-

belings are not restricted to those appearing in the training
data. Each such candidate clique can be represented by a
(i), c, yc), )  HK, and
basis function h() = K((g(i), x
is assigned a parameter h = (i)
c (yc). We work with the
regularized risk

R(f ) =Xi

(cid:16)y

(i), f (g(i), x

(i))(cid:17) +


2

kf k2

K

(3)

where  is the log-loss of equation (1). To evaluate a can-
didate h, one strategy is to compute the gain sup R(f )
R(f + h), and to choose the candidate h having the
largest gain. This presents an apparent difculty, since the
optimal parameter  cannot be computed in closed form,
and must be evaluated numerically. For sequence models
this involves forward-backward calculations for each can-
didate h, the cost of which is prohibitive.

As an alternative, we adopt the functional gradient descent
approach, which evaluates a small change to the current
function. For a given candidate h, consider adding h to
the current model with small weight ; thus f 7 f + h.
Then R(f + h) = R(f ) + dR(f, h) + O(2), where
the functional derivative of R at f in the direction h is
computed as

(i), y

(i), f ) h(x

dR(f, h) = Ef [h]  eE[h] +  hf, hiK

where eE[h] = Pi h(x
tion and Ef [h] = PiPy p(y | x

(i)) is the empirical expecta-
(i), y) is the
model expectation conditioned on x, combined with the
empirical distribution on x. The idea is that in directions h
where the functional gradient dR(f, h) is large, the model
is mismatched with the labeled data; this direction should
be added to the model to make a correction. This results in
the greedy clique selection algorithm summarized in Fig-
ure 1.

Following our earlier notation,

h(x

(i), y) = XcC(g(i))

h(g(i), x

(i), c, yc)

is the sum over all cliques. The candidate functions h might
include functions of the form

h() = K((g(i), x

(i), c, yc), )

Initialize with f = 0, and iterate:

1. For each candidate h  HK, supported by a sin-
gle labeled clique, calculate the functional derivative
dR(f, h).

2. Select the candidate h = arg maxh |dR(f, h)| hav-
ing the largest gradient direction. Set f 7 f + hh.

3. Estimate parameters f for each active f by minimiz-

ing R(f ).

Figure1.Greedy Clique Selection. Labeled cliques encode basis
functions h which are greedily added to the model, using a form
of functional gradient descent.

where i is a specic instance, c is a particular clique of
g(i), and yc is a labeling of that clique. Alternatively, in
a slightly less greedy manner, at each step in the selection
procedure a specic instance and clique may be selected,
and functions for each clique labeling may be added.

In the experiments reported below for sequences,
the
marginal probabilities p(yt = y | x) and expected counts
for the state transitions are required; these are computed
using the forward-backward algorithm, with log domain
arithmetic to avoid underow. A quasi-Newton method
(BFGS, cubic-polynomial line search) is used to estimate
the parameters in step 3. Prediction is carried out using the
forward-backward algorithm to compute marginals rather
than using the Viterbi algorithm.

3.1. Combining multiple kernels

The above use of kernels enables semi-supervised learn-
ing for structured prediction problems. One of the emerg-
ing themes in semi-supervised learning is that graph ker-
nels can provide a useful framework for combining labeled
and unlabeled data. Here an undirected graph is dened
over labeled and unlabeled data instances, and generally
the assumption is that labels vary smoothly over the graph.
The graph is represented by the weight matrix W , and one
can construct a kernel from the graph Laplacian, substitut-
ing eigenvalues  by r(), where r is a non-negative and
(typically) decreasing function. This regularizes high fre-
quency components and encourages smooth functions on
the graph; see (Smola & Kondor, 2003) for a description of
this unifying view of graph kernels.

It is important to note that such a use of a graph kernel for
semi-supervised learning introduces an additional graphi-
cal structure, which should not be confused with the graph
representing the explicit dependencies between labels in a
CRF. For example, when modeling sequences, the natural
CRF graph structure is a chain. By incorporating unla-
beled data through the use of a graph kernel, an additional

graph that will generally have many cycles is implicitly in-
troduced. However, the graph kernel and a more standard
kernel may be naturally combined as a linear combination;
see, for example, (Lanckriet et al., 2004).

4. Synthetic Data Experiments

To demonstrate the properties and advantages of KCRFs,
we prepared two synthetic datasets: a galaxy dataset to
investigate the relation to semi-supervised and sequential
learning, and an HMM with Gaussian mixture emission
probabilities to demonstrate the properties of clique selec-
tion and the advantages of incorporating kernels.

Galaxy. The galaxy dataset is a variant of two spirals;
see Figure 2 (left). Note the dense core of points from both
classes. The sequences are generated from a 2-state hidden
Markov model (HMM), where each state emits instances
uniformly from one of the classes. There is a 90% chance
of staying in the same state. The idea is that under a se-
quence model, an example from the core will have a better
than random chance to be labeled correctly based on the
context. This is not true under a non-sequence model, and
the dataset as a whole will thus have about a 20% Bayes
error rate under the iid assumption. We sample 100 se-
quences of length 20. Note the choice of semi-supervised
vs. standard kernels and sequence vs. non-sequence mod-
els are orthogonal; the four combinations are all tested
on. We construct a semi-supervised graph kernel by rst
creating an unweighted 10-nearest neighbor graph. We
then compute the graph Laplacian L, and form the kernel

K = 10(cid:0)L + 106I(cid:1)1. This corresponds to a function

r() = 1/( + 106) on Ls eigenvalues. The standard
kernel is the radial basis function (RBF) kernel with band-
width  = 0.35. All parameters here and below are tuned
by cross validation.

Figure 2 (center) shows the results of using kernel logis-
tic regression with the semi-supervised kernel and with
the RBF kernel; here the sequence structure is ignored.
For each training set size, which ranges from 20 to 400
points, 10 random trials were performed. The error inter-
vals shown are one standard error. When the labeled set
size is small, the graph kernel is much better than the RBF
kernel. However both kernels saturate at the 20% Bayes
error rate.

Next we apply both kernels to the semiparametric KCRF
model in section 2.3; see Figure 2 (right). Note the x-axis
is the number of training sequencessince each sequence
has 20 instances, the range is the same as Figure 2 (center).
The kernel CRF is capable of getting under the 20% Bayes
error oor of the non-sequence model, with both kernels
and sufcient labeled data. However, the graph kernel is
able to learn the structure much faster than the RBF ker-
nel. Evidently, the high error rate for low label data sizes

prevents the RBF model from effectively using the context.

HMM with Gaussian mixtures. This more difcult
dataset is generated from a 3-state HMM. Each state is a
mixture of 2 Gaussians with random mean and covariance.
The Gaussians strongly overlap; see Figure 3 (left). The
transition probabilities favor remaining in the state, with
a probability of 0.8, and to transition to each of the other
two states with equal probability 0.1; we generate 100 se-
quences of length 30. We use an RBF kernel with  = 0.5.
(A graph kernel is slightly worse than the RBF kernel on
this dataset, and is not shown.) We perform 20 trials for
each training set size, and in each trial we perform clique
selection to select the top 20 vertices. The center and right
plots in Figure 3 show that the semiparametric KCRF again
outperforms kernel logistic regression with the same RBF
kernel.

Figure 4 shows clique selection, with a training size 20 se-
quences, averaged over 20 random trials. The regularized
risk (left), which is training set likelihood plus regularizor,
always decreases as we select more vertices into the KCRF.
On the other hand, the test set likelihood (center) and ac-
curacy (right) saturate and even worsen slightly, showing
signs of overtting. All curves change dramatically at rst,
demonstrating the effectiveness of the clique selection al-
gorithm. In fact, fewer than 10 vertex cliques are sufcient
for this problem.

5. Protein Secondary Structure Prediction

For the protein secondary structure prediction task, we used
the RS126 dataset, on which many current methods have
been developed and tested (Cuff & Barton, 1999).
It is
a non-homologous dataset, since among the 126 protein
chains, no two proteins share more than 25% sequence
identity over a length of more than 80 residues (Cuff & Bar-
ton, 1999). The dataset can be downloaded from http:
//barton.ebi.ac.uk/.

We adopt the DSSP denition of protein secondary struc-
ture (Kabsch & Sander, 1983), which is based on hydrogen
bonding patterns and geometric constraints. Following the
discussion in (Cuff & Barton, 1999), the 8 DSSP labels are
reduced to a 3 state model as follows: H & G map to helix
(H), E & B to sheets (E), and all other states to coil (C).

The state-of-the-art performance for secondary structure
prediction is achieved by window-base methods, using the
position-specic scoring matrices (PSSM) as input fea-
tures, i.e., PSI-BLAST proles, together with Support Vec-
tor Machines (SVMs) as the underlying learning algorithm
(Jones, 1999; Kim & Park, 2003). Finally, the raw predic-
tions are fed into a second layer SVM to lter out physi-
cally unrealistic predictions, such as one sheet residue sur-
rounded by helix residues (Jones, 1999).

e

t

a
r

r
o
r
r
e


t
s
e
T

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

semisupervised
RBF

50

100

150

200

250

300

350

400

Training set size

e
t
a
r

r
o
r
r
e

t
s
e
T

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

semisupervised
RBF

2

4

6

8

10

12

14

16

18

20

Training set size

Figure2.Left: The galaxy data. Center: Kernel logistic regression, comparing two kernels: RBF and a graph kernel using the unlabeled
data. Right: Kernel conditional random elds, which take into account the sequential structure of the data.

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

t

e
a
r

r
o
r
r
e


t
s
e
T

0.4

0.2

0

0.2

0.4

0.6

0.8

1

1.2

0

0

2

4

6

1

0.8

0.6

0.4

0.2

0

0.2

8

10

12

Training sequences

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

t

e
a
r

r
o
r
r
e


t
s
e
T

14

16

18

20

0

0

2

4

6

8

Training sequences

10

12

14

16

18

20

Figure3.Left: The Gaussian mixture data (only a few data points are shown). Center: Kernel logistic regression with an RBF kernel.
Right: Kernel CRF with the same kernel.

2 + x

In our experiments, we apply a linear transformation L to
the PSSM matrix elements according to L(x) = 0 for x 
5, L(x) = 1
10 for 5  x  5, and L(x) = 1 for x 
5. This is the same transform used by Kim and Park (2003),
which achieved one of the best results in the recent CASP
(Critical Assessment of Structure Predictions) competition.
The window size is set to 13 by cross-validation. Therefore
the number of features per position is 13  21 (the number
of amino acids plus gap).

0) = K(x, x0) (y1, y0

Clique selection. We use an RBF kernel with bandwidth
 = 0.1 chosen by cross-validation. Figure 5 (left) shows
the kernel CRF risk reduction as clique selection proceeds,
when only vertex clique candidates are allowed (note there
are always position independent edge parameters in the
KCRF models, to prevent the models from degrading into
kernel logistic regression), and when both vertex and edge
cliques are allowed. (The kernel between vertex cliques is
1), and between edge cliques
K(z, z
2).) The total
it is K(z, z
number of clique candidates is about 4800 (vertex only)
and 20000 (vertex and edge). The rapid reduction in risk
indicates sparse training of kernel CRFs is successful. Also
when more exibility is allowed by including edge cliques,
the risk reduction is much faster. The more exible model
also has higher test set log likelihood (center) although this
does not improve the test set accuracy too much (right).
These observations are generally true for other trials too.

0) = K(x, x0) (y1, y0

1) (y2, y0

Per-residue accuracy. To evaluate prediction performance,
we use the overall per-residue accuracy (also known as
Q3). We experiment with training set size of 5 and 10 se-
quences respectively. For each size we perform 10 trials
where the training sequences are randomly sampled, and
the remaining proteins are used as the test set. For ker-
nel CRF we select 300 cliques, again from either vertex
candidates alone or vertex and edge candidates. We com-
pare them with the SVM-light package (Joachims, 1998)
for SVM classier. All methods use the same RBF kernel.
See Table 1. KCRFs and SVMs have comparable perfor-
mance.

Transition accuracy. Further information can be obtained
by studying transition boundaries, for example, the tran-
sition from coil to sheet. From the point of view of
structural biology, these transition boundaries may provide
important information about how proteins fold in three di-
mension. On the other hand, those are the positions where
most secondary structure prediction systems will fail. The
transition boundary is dened as a pair of adjacent positions
(i, i + 1) whose true labels differ. It is classied correctly
only if both labels are correct. This is a very hard problem,
as can be seen in Table 2 and KCRFs are able to achieve a
considerable improvement over SVM.

Semi-supervised learning. We start with an unweighted 10
nearest neighbor graph over positions in both training and

450

400

350

300

250

200

150

100

k
s
i
r

d
e
z
i
r
a
u
g
e
r

l

50

0

5

10

400

600

800

1000

1200

1400

1600

d
o
o
h

i
l

e
k

i
l


g
o

l

t
s
e

t

40

45

50

1800

0

5

10

20

15
35
number of selected vertices

25

30

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
a


t
s
e

t

40

45

50

0.3

0

5

10

20

15
35
number of selected vertices

25

30

40

45

50

20

15
35
number of selected vertices

25

30

Figure4.Clique selection on the Gaussian mixture data. Left: regularized risk; Center: test set log likelihood; Right: test set accuracy.

1000

900

800

700

600

500

k
s
i
r


d
e
z
i
r
a
u
g
e
r

l

400

0

vertex only
vertex and edge

x 104

0.85

d
o
o
h

i
l

e
k

i
l


g
o

l

t
s
e

t

0.9

0.95

1

1.05

1.1

vertex only
vertex and edge

500

1000

number of cliques

1500

2000

1.15

0

500

1000

1500

2000

number of cliques

0.75

0.7

y
c
a
r
u
c
c
a


t
s
e

t

0.65

0.6

0.55

0.5

0.45

0

vertex only
vertex and edge

500

1000

1500

2000

number of cliques

Figure5.Clique selection for KCRFs on the protein data. Left: regularized risk; Center: test set log likelihood; Right: test set accuracy.
The two curves represent the cases where only vertex cliques are selected (dashed) vs. both vertex and edge cliques are selected (solid).

1

test sequences, with the metric being Euclidean distance in
the feature space. Then the eigensystem of the normalized
Laplacian is computed. The semi-supervised graph kernel
is obtained with the function r(i) =
i+0.01 on the rst
i  200 eigenvalues. The rest eigenvalues are set to zero.
We use the graph kernel together with the RBF kernel in
KCRF. As a clique candidate is associated with a kernel,
we now select two best candidates per iteration, one with
the graph kernel and the other with the RBF kernel. We still
run for 300 iterations for all trials. We also report the re-
sults using Transductive SVMs (TSVMs) (Joachims, 1999)
with the RBF kernel. From the results in Table 3, we can
see that the semi-supervised graph kernel is signicantly
better than TSVMs on the 5-protein dataset while achieves
no improvement on the other one. To diagnose the cause,
we look at the graph together with all the test labels. We
nd that the labels are not smooth w.r.t. the graph: on aver-
age only 54.5% of a nodes neighbors have the same label
as that node. Detecting faulty graphs without using large
amount of labels, and constructing better graphs remain fu-
ture research.

The approximate average running time of each trial, in-
cluding both training and testing, is 30 minutes for KCRFs,
7 minutes for SVMs, and 16 hours for TSVMs. For KCRFs
the majority of the time is spent on clique selection.

6. Conclusion

Kernel conditional random elds have been introduced as
a framework for approaching graph-structured classica-
tion problems. A representer theorem was derived which
shows how KCRFs can be motivated by regularization the-
ory. The resulting techniques combine the strengths of hid-
den Markov models, or more general Bayesian networks,
kernel machines, and standard discriminative linear classi-
ers including logistic regression and SVMs. The formal-
ism presented is quite general, and should apply naturally
to a wide range of problems.

Our experimental results on synthetic data, while care-
fully controlled to be simple, clearly indicate how sequence
modeling, graph kernels for semi-supervised learning, and
clique selection for sparse representations work together
within this framework. The success of these methods in
real problems will depend on the choice of suitable kernels
that capture the structure of the data.

For protein secondary structure prediction, our results are
only suggestive. Secondary structure prediction is a prob-
lem that has been extensively studied for more than 20
years; yet the task remains difcult, with prediction accura-
cies remaining low. The major bottleneck lies in beta-sheet
prediction, where there are long range interactions between
regions of the protein chain that are not necessarily consec-
utive in the primary sequence. Our experimental results
indicate that KCRFs and semi-supervised kernels have the

5 protein set

Method
KCRF (v)
KCRF (v+e)

SVM

Accuracy
0.6625
0.6562
0.6509

std

0.0224
0.0202
0.0307

10 protein set
std

Accuracy
0.6933
0.6933
0.6875

0.0276
0.0272
0.0235

Table1.Per-residue accuracy of different methods for secondary
structure prediction, with the RBF kernel. KCRF (v) uses vertex
cliques only; KCRF (v+e) uses vertex and edge cliques.

5 protein set

Method
KCRF (v)
KCRF (v+e)

SVM

Accuracy
0.1097
0.1114
0.0667

std

0.0271
0.0250
0.0313

10 protein set
std

Accuracy
0.1462
0.1522
0.1066

0.0235
0.0214
0.0311

Table2. Transition accuracy with different methods.

5 protein set

Method
KCRF (v)
KCRF (v+e)
Trans. SVM

Accuracy
0.6722
0.6674
0.6480

std

0.0194
0.0201
0.0276

10 protein set
std

Accuracy
0.6854
0.6819
0.6813

0.0190
0.0194
0.0210

Table3.Per-residue accuracy with semi-supervised methods.

potential to lead to progress on this problem, where the
state of the art has been based on heuristic sliding win-
dow methods. However, our results also suggest that the
improvement due to semi-supervised learning is hindered
by the lack of a good similarity measure with which to con-
struct the graph. The construction of an effective graph is
a challenge that may best be tackled by biologists and ma-
chine learning researchers working together.

Acknowledgments

This work was supported in part by NSF ITR grants CCR-
0122581, IIS-0205456 and IIS-0312814.

