Abstract

We investigate the use of unlabeled data to help labeled data in classi-
cation. We propose a simple iterative algorithm, label propagation, to
propagate labels through the dataset along high density areas dened by
unlabeled data. We analyze the algorithm, show its solution, and its con-
nection to several other algorithms. We also show how to learn parame-
ters by minimum spanning tree heuristic and entropy minimization, and
the algorithms ability to perform feature selection. Experiment results
are promising.

1 Introduction

Labeled data are essential for supervised learning. However they are often available only
in small quantities, while unlabeled data may be abundant. Using unlabeled data together
with labeled data is of both theoretical and practical interest. Recently many approaches
have been proposed for combining unlabeled and labeled data [1] [2]. Among them there
is a promising family of methods which assume that closer data points tend to have similar
class labels in a manner analogous to k-Nearest-Neighbor (kNN) in traditional supervised
learning. As a result, these methods propagate labels through dense unlabeled data regions.

We propose a new algorithm to propagate labels. We formulate the problem as a particular
form of label propagation, where a nodes labels propagate to all nodes according to their
proximity. Meanwhile we x the labels on the labeled data. Thus labeled data act like
sources that push out labels through unlabeled data. We prove the convergence of the
algorithm, nd a closed form solution for the xed point, and analyze its behavior on several
datasets. We also propose a minimum spanning tree heuristic and an entropy minimization
criterion to learn the parameters, and show our algorithm learns to detect irrelevant features.

Letx1;y1:::x;y be labeled data, whereY=fy1:::yg2f1:::Cg are the
class labels. We assume the number of classesC is known, and all classes are present
in the labeled data. Letx1;y1:::x	;y	 be unlabeled data whereYU=

2 Label propagation

2.1 Problem setup

. The

for example, positive or discrete. We have chosen to focus on Euclidean distance in this

propagation.

All nodes have soft labels which can be interpreted as distributions over labels. We let the
labels of a node propagate to all nodes through the edges. Larger edge weights allow labels

Intuitively, we want data points that are close to have similar labels. We create a fully
connected graph where the nodes are all data points, both labeled and unlabeled. The

fy1:::y	g are unobserved; usually(cid:28)	. LetX=fx1:::x	g2RD
problem is to estimateYU fromX andY.
edge between any nodesi;j is weighted so that the closer the nodes are in local Euclidean
distance,dij, the larger the weightwij. The weights are controlled by a parameter(cid:27):
wij=ex d2ij(cid:27)2!=ex Dd=1xdixdj2
!
(cid:27)2
Other choices of distance metric are possible, and may be more appropriate if thex are,
paper, later allowing different(cid:27)s for each dimension, corresponding to length scales for
to travel through more easily. Dene a		 probabilistic transition matrixT
wij	k=1wkj
Tij=j!i=
whereTij is the probability of jumping from nodej toi. Also dene a	C label
matrixY , whoseith row representing the label probabilities of nodeyi. The initialization
of the rows ofY corresponding to unlabeled data points is not important. We are now ready
1. All nodes propagate labels for one step:Y TY
2. Row-normalizeY to maintain the class probability interpretation.
3. Clamp the labeled data. Repeat from step 2 untilY converges.
distributions toYi
=yi;
, so the probability mass is concentrated on the given class.
Y (cid:22)TY with(cid:22)T being the row-normalized matrix ofT , i.e.(cid:22)Tij=Tij=kTik. LetY
be the top rows ofY (the labeled data) andYU the remaining	 rows. Notice thatY
never really changes since it is clamped in step 3, and we are solely interested inYU . We
split(cid:22)T after the-th row and the-th column into 4 sub-matrices
(cid:22)T=(cid:20)(cid:22)T
(cid:22)T		(cid:21)
(cid:22)T
(cid:22)T	
It can be shown that our algorithm isYU (cid:22)T		YU(cid:22)T	Y, which leads toYU=
i!1(cid:22)T		Y0[i=1(cid:22)Ti1
(cid:22)T	Y, whereY0
is the initialY . We need to show

(cid:22)T		Y0!0. By construction, all elements in(cid:22)T are greater than zero. Since(cid:22)T is row
	j=1(cid:22)T		ij(cid:20)
normalized, and(cid:22)T		 is a sub-matrix of(cid:22)T , it follows that9(cid:13)<1; such that
ikj(cid:22)T		kj(cid:20)
ik(cid:22)T		kj=k(cid:22)T1
j(cid:22)T		ij=jk(cid:22)T1
(cid:13);8i=1:::	. Therefore

The intuition is that with this constant push from labeled nodes, the class boundaries will
be pushed through high density data regions and settle in low density gaps. If this structure
of data ts the classication goal, our algorithm can use unlabeled data to help learning.

The algorithm converges to a simple solution. First, step 1 and 2 can be combined into

(1)

(2)

(3)

to present the algorithm.

2.2 The algorithm

The label propagation algorithm is as follows:

Step 3 is critical: Instead of letting the labeled data points fade away, we clamp their class

2.3 Parameter setting

2.4 Rebalancing class proportions

is inconsequential. Obviously

(4)

is a xed point. Therefore it is the unique xed point and the solution to our iterative
algorithm.

no node is connected. During tree growth, the edges are examined one by one from short
to long. An edge is added to the tree if it connects two separate components. The process
repeats until the whole graph is connected. We nd the rst tree edge that connects two
as

of this (and longer) edge is close to 0, with the hope that local propagation is then mostly

k(cid:22)T1
. So the row sums of(cid:22)T		 converge to zero, which means(cid:22)T		Y0!0.
ik(cid:13)(cid:20)(cid:13)
Thus the initial pointY0
YU=(cid:22)T		1(cid:22)T	Y
We set the parameter(cid:27) with a heuristic. We nd a minimum spanning tree (MST) over all
data points under Euclidean distancesdij, with Kruskals Algorithm [3]. In the beginning
components with different labeled points in them. We regard the length of this edged0
a heuristic of the minimum distance between classes. We set(cid:27)=d0=3 so that the weight
within classes. Later we will propose an entropy-based criterion to learn the(cid:27) parameters.
For classication purposes, onceYU is computed, we can take the most likely (ML) class of


=1) are either estimated from labeled data or known a priori (i.e. from
1:::C (
(cid:15) Class Mass Normalization: Find coefcients(cid:21)
to scale columns ofYU s.t.
(cid:21)1YU:1:::::(cid:21)CYU:C=1:::::C. Once decisions are made for
equal to1:::C.
(cid:15) Label Bidding: We have	
class
labels for sale. Each pointi bids $YUi
for
class
. Bids are processed from high to low. AssumeYUi
is currently the highest
bid. If class
labels remain, a
label is sold to pointi, who then quits the bidding.
BandsC=3;=3;	=178;(cid:27)=0:22; for SpringsC=2;=2;	=184;(cid:27)=0:43.
Both(cid:27)s are from the MST heuristic. Simple ML classication is used. Here, obviously
reduce the size of each image down to1616 by down-sampling and Gaussian smoothing,

each unlabeled point as its label. However, this procedure does not provide any control over
the nal proportions of the classes, which are implicitly determined by the distribution of
data. If classes are not well separated and labeled data is scarce, incorporating constraints
on class proportions can improve nal classication. We assume the class proportions

To demonstrate properties of this algorithm we investigated both synthetic datasets and
a real-world classication problem. Figure 1 shows label propagation on two synthetic
datasets. Large symbols are labeled data, other points are originally unlabeled. For 3-

kNN would fail to follow the structure of data.

For a real world example we test label propagation on a handwritten digits dataset, origi-
nally from the Cedar Buffalo binary digits database [4]. The digits were preprocessed to

with pixel values ranging from 0 to 255 [5]. We use digits 1, 2 and 3 in our experi-
ment as three classes, with 1100 images in each class. Each image is represented by a 256

Otherwise the bid is ignored and the second highest bid is processed, and so on.
Label bidding guarantees that strict class proportions will be met.

3 Experimental results

an oracle). We propose two post-processing alternatives to ML class assignment:

each point, this procedure does not guarantee that class proportion will be exactly

3.5

3

2.5

2

4

3

2

1.5

1

0.5

0
0

that it can work very well; in this section we propose a criterion for learning the model
parameters that can be applied in more general settings. Data label likelihood does not

1

0
2

2

0

2

1
(a) 3-Bands

3

0

2

2

(b) Springs

Figure 1: Label propagation on two synthetic datasets.

dimensional vector. Figure 2(a) shows a random sample of 100 images from the dataset.

(5) LBo: Label bidding post processing, with oracle class proportions. We use two algo-

trial we randomly sample labeled data from the whole dataset, and use the rest of images as
unlabeled data. If any class is absent from the sampled labeled set, we redo the sampling.

(see section 2.4): (1) ML: The most likely labels; (2) CNe: Class mass normalization, with
maximum likelihood estimate of class proportions from labeled data; (3) LBe: Label bid-
ding, with maximum likelihood estimate of class proportions from labeled data; (4) CNo:

We vary labeled data size from 3 up to 100. For each size, we perform 20 trials. In each
Thus labeled and unlabeled data are approximatelyiid. We nd(cid:27) by the MST heuristic
(all trials have(cid:27) close to 340). To speed up computation, only the top 150 neighbors of
each image are considered to makeT sparse. We measure 5 error rates on unlabeled data
Class mass normalization, with knowledge of the oracle (true) class proportions (i.e.1=3);
rithms as baselines. The rst one is standardkNN. We report 1NN results since it is the
best amongk=1:::11. The second baseline algorithm is propagating 1NN (p1NN):
Among all unlabeled data, nd the pointx	 closest to a labeled point (call itx). Label
x	 withxs label, addx	 to the labeled set, and repeat. p1NN is a crude version of label
Figure 2(b)(f) shows the results. ML labeling is better than 1NN when(cid:21)40 (b). But
performance when is small (c). If we know the true class proportion, the performance is
LBo. Each entry is averaged over 20 trials. All differences are statistically signicant at(cid:11)
level 0.05 ( test), except for the pairs in thin face.
When(cid:27)!0, the label propagation result approaches p1NN, because under the exponential
weights (1) the inuence of the nearest point dominates. When(cid:27)!1, the whole dataset
all labeled points, resulting in equal class probabilities. The appropriate(cid:27) is in between.
We used MST as a heuristic to set the parameter(cid:27) and have shown in the previous section

even better (e,f), with label bidding being slightly superior to class mass normalization. On
the other hand since label bidding requires exact proportions, its performance is bad when
the class proportions are estimated (d). To summarize, label bidding is the best when exact
proportions are known, otherwise class mass normalization is the best. p1NN consistently
performs no better than 1NN. Table 1 lists the error rates for p1NN, 1NN, ML, CNe and

if we rebalance class proportions, we can do much better. If we use class frequency of
labeled data as class proportions and perform class mass normalization, we improve the

propagation. It performs well on the two synthetic datasets, with the same results as in
Figure 1.

effectively shrinks to a single point. All unlabeled points receive the same inuence from

4 Parameter learning by entropy minimization

70

60

50

40

30

20

10

0

%


r
o
r
r
e

1NN
ML

70

60

50

40

30

20

10

0

%


r
o
r
r
e

1NN
CNe

10
0

20

40

l

60

80

100

10
0

20

40

l

60

80

100

(a) A sample

(b) ML

(c) CNe

70

60

1NN
LBe

70

60

1NN
CNo

70

60

1NN
LBo

0

l

10
0

20

l

l

0

10
0

20

40

60

10
0

20

40

60

80

100

40

60

80

100

80

100

%


r
o
r
r
e

%


r
o
r
r
e

%


r
o
r
r
e

50

40

30

20

10

0

50

40

30

20

10

50

40

30

20

10

(f) LBo

(d) LBe

(e) CNo

Figure 2: The digits dataset. Each point is an average of 20 random trials. The error bars

make sense as a criterion in our setting, especially with very few labeled points, since
intuitively the quality of a solution depends on how unlabeled data are assigned labels.

points condently. There are many arbitrary labelings of the unlabeled data that have low
entropy, which might suggest that this criterion would not work. However, it is important
to point out that most of these arbitrary low entropy labelings cannot be achieved by prop-
agating labels using our algorithm. In fact, we nd that the space of low entropy labelings

are1 standard deviation.
We propose to minimize the entropy=ijYijgYij, which is the sum of the en-
tropy on individual data points. This captures the intuition that good(cid:27) should label all
achievable by label propagation is small and lends itself well to tuning the(cid:27) parameters.
One complication remains, which is that has a minimum 0 at(cid:27)!0 (notice p1NN gives



Table 1: Digits: error rate of different post processing methods.
30
14.3
12.1
7.0
1.6
0.5
100
7.4
6.0
1.0
0.7
0.5

each point a hard label), but this (p1NN) is not always desirable. Figure 3(a,b) shows the
problem on the Bridge dataset, where the upper grid is slightly tighter than the lower grid.

3
p1NN 46.1
1NN 36.4
49.6
ML
6.9
CNe
2.3
LBo
35
p1NN 12.9
1NN 11.9
5.0
ML
1.1
CNe
0.5
LBo

9
35.0
27.8
33.5
10.6
0.8
50
11.7
9.1
3.4
1.0
0.5

20
17.7
15.4
12.6
3.4
0.5
80
8.8
7.1
1.2
0.7
0.5

6
34.2
28.3
35.0
12.3
2.3
40
11.5
10.7
2.4
1.1
0.5

12
29.2
23.7
26.6
7.0
0.6
60
10.1
8.6
2.0
0.8
0.5

15
23.2
19.0
20.7
5.4
0.6
70
9.6
7.7
1.5
0.8
0.5

25
15.0
13.1
9.3
2.0
0.5
90
8.2
6.6
1.1
0.7
0.5

0.5

4

2

40

2

4

2

0

2

1

0

2

0

30

25

35

H

1

2

1

2

5

4

3

2

1

0

5

4

3

2

3
4

3
4

(5)

=0.05
=0.005
=0.0005
=0.00005
=0.000005
unsmoothed
1.5

This can be xed by smoothingT . Inspired by the analysis on the PageRank algorithm [6],
we smoothT with a uniform transition matrixU, whereUij=1=	;8i;j:
~T=(cid:15)U1(cid:15)T
~T is then used in place ofT in the algorithm. Figure 3(c) shows vs.(cid:27) before and
after smoothing on the Bridge dataset, with different(cid:15) values. Smoothing helps to get
rid of the nuisance minimum at(cid:27)!0. In the following we use the value(cid:15)=0:0005.
Although we have to add one more parameter(cid:15) to learn(cid:27), the advantage is apparent when
(a) p1NN or(cid:27)!0
(b) optimal(cid:27)=0:72
Figure 3: The Bridge dataset. p1NN (or(cid:27)!0) result is undesirable. SmoothingT helps
to remove the minimum of at(cid:27)!0.
we introduce multiple parameters(cid:27)1:::(cid:27)D, one for each dimension. Now the weights are
wij=exDd=1xdixdj2=(cid:27)2d. The(cid:27)ds are analogous to the relevance or length
scales in Gaussian process. We use gradient descent to nd the parameters(cid:27)1:::(cid:27)D that
minimizes. Readers are referred to [7] for a derivation of=(cid:27)d.
The learned single(cid:27) is 0.26 and 0.43 for the 3-Bands and springs datasets respectively,
very close to the MST heuristic. Classication remains the same. With multiple(cid:27)ds, our
algorithm can detect irrelevant dimensions. For the Bridge dataset(cid:27)1 keeps increasing
while(cid:27)2 and asymptote during learning, meaning the algorithm thinks the horizontal
dimension (corresponding to(cid:27)1) is irrelevant to classication (large(cid:27)d allows labels to
4-dimensional unit hypersphere with gaps (Figure 4). There are two45
are(cid:27)1=0:18,(cid:27)2=0:19,(cid:27)3=14:8, and(cid:27)4=13:3, i.e.
to compute the-step ancestorhood of any nodei, that is, given that the random walk is at
nodei, what is the probability that it was at some nodej at steps before. To understand

freely propagate along that dimension). Classication is the same as Figure 3(b). Let us
look at another synthetic dataset, Ball, with 400 data points uniformly sampled within a
gaps when the
dataset is projected onto dimensions 1-2 and 3-4 respectively, but no gap in dimensions
1-3, 1-4 or 2-3. The gap in dimensions 1-2 is related to classication while the one in 3-4
is not. But this information is only hinted by the 4 labeled points. The learned parameters
the algorithm learns that
dimensions 3, 4 are irrelevant to classication, even though the data are clustered along
those dimensions. Classication follows the gap in dimensions 1, 2.

The proposed label propagation algorithm is closely related to the Markov random walks
algorithm [8]. Both utilize the manifold structure dened by large amount of unlabeled
data, and assume the structure is correlated to the goal of classication. Both dene a
probabilistic process for labels to transit between nodes. But the Markov random walks al-
gorithm approaches the problem from a different perspective. It uses the transition process

5 Related work

(c) smoothing

the algorithm, it is helpful to imagine that each node has two separate sets of labels, one

1



1

0.5

1

0.5

0

0.5

1
1

1

0

0.5

1
1

0.5

0

0.5

0.5

0

0.5

1

Figure 4: The Ball dataset. The algorithm learns that dimensions 3, 4 are irrelevant.

labels, weighted by their ancestorhood. This is in fact kernel regression, with the kernel

will be the same. In our algorithm, labeled data are constant sources that push out labels,

margin of the observed labels are optimized. The algorithm is sensitive to the time scale

There seems to be a resemblance between label propagation and mean eld approximation
[9] [10]. In label propagation, upon convergence we have the equations (for unlabeled data)

hidden and one observable. A nodeis observable label is the average of all nodes hidden
being the-step ancestorhood. The hidden labels are learned such that the likelihood or
, since when!1 every node looks equally like an ancestor, and all observable labels
and the system achieves equilibrium when!1.
Yi
= jTijYj

0jTijYj
0
Consider the labeled / unlabeled data graph as a conditional Markov random eldF with
pairwise interactionwij between nodesi;j, and with labeled nodes clamped. Each un-
clamped (unlabeled) nodei inF can be in one ofC states, denoted by a vector also called
Yi=yi;1;:::;yi;C. The probability of a particular congurationY inF is
FY=1Zex[ijwijYiYj>. We now show label propagation (6) is approximately
a mean eld solution to a Markov random eldF0
that approximatesF.F0
asF0Y=1Zex[gijwijYiYj>, which is the same asF up to the rst order:
F0Y(cid:25)1Zex[ijwijYiYj>1=FY. The mean eld solution toF0
hYi
i= jwijhYj
i

0jwijhYj
i
wherehi denotes the mean. Equation (6) is an approximation to (7) in the sense that if we
kwik are the same for alli, we can replaceTij withwij in (6). Therefore we
nd that label propagation is approximately the mean eld approximation toF.
Markov random eldF, since the minimum cut corresponds to minimum energy. There is
random eldF to learn from labeled and unlabeled data, optimizing the length scale pa-

a subtle difference: assume the middle band in Figure 1(a) has no labeled point. Mincut
will classify the middle band as either all o or all +, since these are the two most likely
state congurations [12]. But label propagation, being more in the spirit of a mean eld
approximation, splits the middle band, classifying points in the upper half as o and lower
half as + (with low condence though). In addition, label propagation is not limited to
binary classication.

In related work, we have also attempted to use Boltzmann machine learning on the Markov

The graph mincut algorithm [11] nds the most likely state conguration of the same

assume

rameters using the likelihood criterion on the labeled points [13].

(6)

is dened

is :

(7)

6 Summary

We proposed a label propagation algorithm to learn from both labeled and unlabeled data.
Labels were propagated with a combination of random walk and clamping. We showed the
solution to the process, and its connection to other methods. We also showed how to learn
the parameters. As with various semi-supervised learning algorithms of its kind, label prop-
agation works only if the structure of the data distribution, revealed by abundant unlabeled
data, ts the classication goal. In the future we will investigate better ways to rebalance
class proportions, applications of the entropy minimization criterion to learn propagation
parameters from real datasets, and possible connections to the diffusion kernel [14].

Acknowledgments

We thank Sam Roweis, Roni Rosenfeld, Teddy Seidenfeld, Guy Lebanon, Jin Rong and
Jing Liu for helpful discussions. Sam Roweis provided the handwritten digits dataset. The
rst author is supported in part by a Microsoft Research Graduate Fellowship.

