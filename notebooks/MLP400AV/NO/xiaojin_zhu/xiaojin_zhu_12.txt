Abstract

We present an algorithm based on convex optimization for constructing
kernels for semi-supervised learning. The kernel matrices are derived
from the spectral decomposition of graph Laplacians, and combine la-
beled and unlabeled data in a systematic fashion. Unlike previous work
using diffusion kernels and Gaussian random eld kernels, a nonpara-
metric kernel approach is presented that incorporates order constraints
during optimization. This results in exible kernels and avoids the need
to choose among different parametric forms. Our approach relies on
a quadratically constrained quadratic program (QCQP), and is compu-
tationally feasible for large datasets. We evaluate the kernels on real
datasets using support vector machines, with encouraging results.

Introduction

1
Semi-supervised learning has been the focus of considerable recent research. In this learn-
ing problem the data consist of a set of points, with some of the points labeled and the
remaining points unlabeled. The task is to use the unlabeled data to improve classication
performance. Semi-supervised methods have the potential to improve many real-world
problems, since unlabeled data are often far easier to obtain than labeled data.

Kernel-based methods are increasingly being used for data modeling and prediction be-
cause of their conceptual simplicity and good performance on many tasks. A promising
family of semi-supervised learning methods can be viewed as constructing kernels by trans-
forming the spectrum of a local similarity graph over labeled and unlabeled data. These
kernels, or regularizers, penalize functions that are not smooth over the graph [7]. Infor-
mally, a smooth eigenvector has the property that two elements of the vector have similar
values if there are many large weight paths between them on the graph. This results in the
desirable behavior of the labels varying smoothly over the graph, as sought by, e.g., spectral
clustering approaches [2], diffusion kernels [5], and the Gaussian random eld approach
[9]. However, the modication to the spectrum, called a spectral transformation, is often
a function chosen from some parameterized family. As examples, for the diffusion kernel
the spectral transformation is an exponential function, and for the Gaussian eld kernel the
transformation is a smoothed inverse function.

In using a parametric approach one faces the difcult problem of choosing an appropriate
family of spectral transformations. For many familes the number of degrees of freedom
in the parameterization may be insufcient to accurately model the data.
In this paper

we propose an effective nonparametric method to nd an optimal spectral transformation
using kernel alignment. The main advantage of using kernel alignment is that it gives
us a convex optimization problem, and does not suffer from poor convergence to local
minima. A key assumption of a spectral transformation is monotonicity, so that unsmooth
functions over the data graph are penalized more severly. We realize this property by
imposing order constraints. The optimization problem in general is solved using semi-
denite programming (SDP) [1]; however, in our approach the problem can be formulated
in terms of quadratically constrained quadratic programming (QCQP), which can be solved
more efciently than a general SDP.

This paper is structured as follows. In Section 2 we review some graph theoretic concepts
and relate them to the construction of kernels for semi-supervised learning. In Section 3
we introduce convex optimization via QCQP and relate it to the more familiar linear and
quadratic programming commonly used in machine learning. Section 4 poses the problem
of kernel based semi-supervised learning as a QCQP problem with order constraints. Ex-
perimental results using the proposed optimization framework are presented in Section 5.
The results indicate that the semi-supervised kernels constructed from the learned spectral
transformations perform well in practice.

2 Semi-supervised Kernels from Graph Spectra
We are given a labeled dataset consisting of input-output pairs {(x1, y1), . . . , (xl, yl)} and
a (typically much larger) unlabeled dataset {xl+1, . . . , xn} where x is in some general
input space and y is potentially from multiple classes. Our objective is to construct a kernel
that is appropriate for the classication task. Since our methods use both the labeled and
unlabeled data, we will refer to the resulting kernels as semi-supervised kernels. More
specically, we restrict ourselves to the transductive setting where the unlabeled data also
serve as the test data. As such, we only need to nd a good Gram matrix on the points
{x1, . . . , xn}. For this approach to be effective such kernel matrices must also take into
account the distribution of unlabeled data, in order that the unlabeled data can aid in the
classication task. Once these kernel matrices have been constructed, they can be deployed
in standard kernel methods, for example support vector machines.

In this paper we motivate the construction of semi-supervised kernel matrices from a
graph theoretic perspective. A graph is constructed where the nodes are the data instances
{1, . . . , n} and an edge connects nodes i and j if a local similarity measure between xi
and xj suggests they may have the same label. For example, the local similarity measure
can be the Euclidean distance between feature vectors if x  Rm, and each node can con-
nect to its k nearest neighbors with weight value equal to 1. The intuition underlying the
graph is that even if two nodes are not directly connected, they should be considered similar
as long as there are many paths between them. Several semi-supervised learning algorithms
have been proposed under the general graph theoretic theme, based on techniques such as
random walks [8], diffusion kernels [5], and Gaussian elds [9]. Many of these methods
can be unied into the regularization framework proposed by [7], which forms the basis of
this paper.
The graph can be represented by an n  n weight matrix W = [wij] where wij is the edge
weight between nodes i and j, with wij = 0 if there is no edge. We require the entries of W
to be non-negative, and assume that it forms a symmetric matrix; it is not necessary for W
itself to be positive semi-denite. In semi-supervised learning W is an essential quantity;
we assume it is provided by domain experts, and hence do not study its construction. Let
D be a diagonal matrix where dii = Pj wij is the degree of node i. This allows us
to dene the combinatorial graph Laplacian as L = D  W (the normalized Laplacian
L = D1/2LD1/2 can be used as well). We denote Ls eigensystem by {i, i}, so
that L = Pn
i where we assume the eigenvalues are sorted in non-decreasing
order. The matrix L has many interesting properties [3]; for instance, it is always positive

i=1 ii>

semi-denite, even if W is not. Perhaps the most important property of the Laplacian
related to semi-supervised learning is the following: a smaller eigenvalue  corresponds
to a smoother eigenvector  over the graph; that is, the value Pij wij((i)  (j))2 is
small. In a physical system the smoother eigenvectors correspond to the major vibration
modes. Assuming the graph structure is correct, from a regularization perspective we want
to encourage smooth functions, to reect our belief that labels should vary slowly over the
graph. Specically, [2] and [7] suggest a general principle for creating a semi-supervised
kernel K from the graph Laplacian L: transform the eigenvalues  into r(), where the
spectral transformation r is a non-negative and decreasing function1

K =

n

X

i=1

r(i) i>
i

(1)

K ) = (P c2

Note that it may be that r reverses the order of the eigenvalues, so that smooth is have
larger eigenvalues in K. A soft labeling function f = P cii in a kernel machine has
a penalty term in the RKHS norm given by (||f ||2
i /r(i)). Since r is de-
creasing, a greater penality is incurred for those terms of f corresponding to eigenfunctions
that are less smooth. In previous work r has often been chosen from a parametric family.
For example, the diffusion kernel [5] corresponds to r() = exp( 2
2 ) and the Gaussian
eld kernel [10] corresponds to r() = 1
+ . Cross validation has been used to nd the
hyperparameters  or  for these spectral transformations. Although the general principle
of equation (1) is appealing, it does not address question of which parametric family to use
for r. Moreover, the number of degrees of freedom (or the number of hyperparameters)
may not suit the task at hand, resulting in overly constrained kernels. The contribution of
the current paper is to address these limitations using a convex optimization approach by
imposing an ordering constraint on r but otherwise not assuming any parametric form for
the kernels.

3 Convex Optimization using QCQP
i , i = 1    n be the outer product matrices of the eigenvectors. The semi-
Let Ki = i>
supervised kernel K is a linear combination K = Pn
i=1 iKi, where i  0. We formulate
the problem of nding the spectral transformation as one that nds the interpolation coef-
cients {r(i) = i} by optimizing some convex objective function on K. To maintain the
positive semi-deniteness constraint on K, one in general needs to invoke SDPs [1]. Semi-
denite optimization can be described as the problem of optimizing a linear function of a
symmetric matrix subject to linear equality constraints and the condition that the matrix be
positive semi-denite. The well-known linear programming problem can be generalized
to a semi-denite optimization by replacing the vector of variables with a symmetric ma-
trix, and replacing the non-negativity constraints with a positive semi-denite constraints.
This generalization inherits several properties: it is convex, has a rich duality theory and
allows theoretically efcient solution algorithms based on iterating interior point methods
to either follow a central path or decrease a potential function. However, a limitation of
SDPs is their computational complexity [1], which has restricted their application to small
scale problems [6]. However, an important special case of SDPs are quadratically con-
strained quadratic programs (QCQP) which are computationally more efcient. Here both
the objective function and the constraints are quadratic as illustrated below,

minimize

subject to

1
2
1
2

x>P0x + q>

0 x + r0

x>Pix + q>

i x + ri  0

i = 1    m

Ax = b

(2)

(3)

(4)

1We use a slightly different notation where r is the inverse of that in [7].

+, i = 1, . . . , m, where S n

+ denes the set of square symmetric positive
where Pi  S n
semi-denite matrices. In a QCQP, we minimize a convex quadratic function over a feasible
region that is the intersection of ellipsoids. The number of iterations required to reach the
solution is comparable to the number required for linear programs, making the approach
feasible for large datasets. However, as observed in [1], not all SDPs can be relaxed to
QCQPs. For the semi-supervised kernel learning task presented here solving an SDP would
be computationally infeasible.

Recent work [4, 6] has proposed kernel target alignment that can be used not only to assess
the relationship between the feature spaces generated by two different kernels, but also to
assess the similarity between spaces induced by a kernel and that induced by the labels
themselves. Desirable properties of the alignment measure can be found in [4]. The cru-
cial aspect of alignnement for our purposes is that its optimization can be formulated as a
QCQP. The objective function is the empirical kernel alignment score:

A(Ktr, T ) =

hKtr, T iF

phKtr, KtriF hT, T iF

(5)

where Ktr is the kernel matrix restricted to the training points, hM, N iF denotes the Frobe-
nius product between two square matrices hM, N iF = Pij mijnij = T r(M N >), and T
is the target matrix on training data, with entry Tij set to +1 if yi = yj and 1 otherwise.
Note for binary {+1, 1} training labels y this is simply the rank one matrix T = yy
>. K
is guaranteed to be positive semi-denite by constraining i  0. Previous work using ker-
nel alignment did not take into account that the Kis were derived from the graph Laplacian
with the goal of semi-supervised learning. As such, the is can take arbitrary values and
there is no preference to penalize components that do not vary smoothly over the graph.
This can be rectied by requiring smoother eigenvectors to receive larger coefcients, as
shown in the next section.

4 Semi-Supervised Kernels with Order Constraints
As stated above, we would like to maintain a decreasing order on the spectral transforma-
tion i = r(i) to encourage smooth functions over the graph. This motivates the set of
order constraints

i  i+1,

i = 1    n  1

(6)

And we can specify the desired semi-supervised kernel as follows.

Denition 1 Anorder constrained semi-supervised kernel K isthesolutiontothefollow-
ingconvexoptimizationproblem:

maxK
subjectto

A(Ktr, T )

K = Pn

i=1 iKi

i  0

trace(K) = 1

i  i+1,

i = 1    n  1

(7)
(8)
(9)
(10)
(11)

whereT isthetrainingtargetmatrix,Ki = i>
Laplacian.

i andisaretheeigenvectorsofthegraph

The formulation is an extension to [6] with order constraints, and with special components
Kis from the graph Laplacian. Since i  0 and Kis are outer products, K will auto-
matically be positive semi-denite and hence a valid kernel matrix. The trace constraint is
needed to x the scale invariance of kernel alignment. It is important to notice the order
constraints are convex, and as such the whole problem is convex. Let vec(A) be the column

vectorization of a matrix A. Dening M = (cid:2)vec(K1,tr)    vec(Km,tr)(cid:3), it is not hard to
show that the problem can then be expressed as

max
subject to

(12)
(13)
(14)
(15)
The objective function is linear in , and there is a simple cone constraint, making it a
quadratically constrained quadratic program (QCQP).

vec(T )>M 
||M ||  1

i  i+1,

i = 1    n  1

i  0

An improvement of the above order constrained semi-supervised kernel can be obtained
by studying the Laplacian eigenvectors with zero eigenvalues. For a graph Laplacian there
will be k zero eigenvalues if the graph has k connected subgraphs. The k eigenvectors are
piecewise constant over individual subgraphs, and zero elsewhere. This is desirable when
k > 1, with the hope that subgraphs correspond to different classes. However if k = 1, the
graph is connected. The rst eigenvector 1 is a constant vector. The corresponding K1 is
a constant matrix, and acts as a bias term. In this situation we do not want to impose the
order constraint 1  2 on the constant bias term. Instead we let 1 vary freely during
optimization.

Denition 2 An improved order constrained semi-supervised kernel K is the solution to
thesameprobleminDenition1,buttheorderconstraints(11)applyonlytonon-constant
eigenvectors:

i  i+1,

i = 1    n  1, and i notconstant

(16)

In practice we do not need all n eigenvectors of the graph Laplacian, or equivalently all n
Kis. The rst m < n eigenvectors with the smallest eigenvalues work well empirically.
Also note we could have used the fact that Kis are from orthogonal eigenvectors i to
further simplify the expression. However we neglect this observation, making it easier to
incorporate other kernel components if necessary.

It is illustrative to compare and contrast the order constrained semi-supervised kernels to
other semi-supervised kernels with different spectral transformation. We call the original
kernel alignment solution in [6] a maximal-alignment kernel. It is the solution to Deni-
tion 1 without the order constraints (11). Because it does not have the additional constraints,
it maximizes kernel alignment among all spectral transformation. The hyperparameters 
and  of the Diffusion kernel and Gaussian elds kernel (described earlier) can be learned
by maximizing the alignment score also, although the optimization problem is not neces-
sarily convex. These kernels use different information from the original Laplacian eigen-
values i. The maximal-alignment kernels ignore i altogether. The order constrained
semi-supervised kernels only use the order of i and ignore their actual values. The diffu-
sion and Gaussian eld kernels use the actual values. In terms of the degree of freedom in
choosing the spectral transformation is, the maximal-alignment kernels are completely
free. The diffusion and Gaussian eld kernels are restrictive since they have an implicit
parametric form and only one free parameter. The order constrained semi-supervised ker-
nels incorporates desirable features from both approaches.

5 Experimental Results
We evaluate the order constrained kernels on seven datasets. baseball-hockey (1993 in-
stances / 2 classes), pc-mac (1943/2) and religion-atheism (1427/2) are document catego-
rization tasks taken from the 20-newsgroups dataset. The distance measure is the standard
cosine similarity between tf.idf vectors. one-two (2200/2), odd-even (4000/2) and ten
digits (4000/10) are handwritten digits recognition tasks. one-two is digits 1 vs. 2;
odd-even is the articial task of classifying odd 1, 3, 5, 7, 9 vs. even 0, 2, 4, 6, 8 digits,

such that each class has several well dened internal clusters; ten digits is 10-way clas-
sication. isolet (7797/26) is isolated spoken English alphabet recognition from the UCI
repository. For these datasets we use Euclidean distance on raw features. We use 10NN
unweighted graphs on all datasets except isolet which is 100NN. For all datasets, we use
the smallest m = 200 eigenvalue and eigenvector pairs from the graph Laplacian. These
values are set arbitrarily without optimizing and do not create a unfair advantage to the
proposed kernels. For each dataset we test on ve different labeled set sizes. For a given
labeled set size, we perform 30 random trials in which a labeled set is randomly sampled
from the whole dataset. All classes must be present in the labeled set. The rest is used as
unlabeled (test) set in that trial. We compare 5 semi-supervised kernels (improved order
constrained kernel, order constrained kernel, Gaussian eld kernel, diffusion kernel2 and
maximal-alignment kernel), and 3 standard supervised kernels (RBF (bandwidth learned
using 5-fold cross validation),linear and quadratic). We compute the spectral transforma-
tion for order constrained kernels and maximal-alignment kernels by solving the QCQP
using standard solvers (SeDuMi/YALMIP). To compute accuracy we use a standard SVM.
We choose the the bound on slack variables C with cross validation for all tasks and ker-
nels. For multiclass classication we perform one-against-all and pick the class with the
largest margin.
The results3 are shown in Table 1, which has two rows for each cell: The upper row is
the average test set accuracy with one standard deviation; The lower row is the average
training set kernel alignment, and in parenthesis the average run time in seconds for Se-
DuMi/YALMIP on a 3GHz Linux computer. Each number is averaged over 30 random
trials. To assess the statistical signicance of the results, we perform paired t-test on test
accuracy. We highlight the best accuracy in each row, and those that can not be determined
as different from the best, with paired t-test at signicance level 0.05. The semi-supervised
kernels tend to outperform standard supervised kernels. The improved order constrained
kernels are consistently among the best. Figure 1 shows the spectral transformation i of
the semi-supervised kernels for different tasks. These are for the 30 trials with the largest
labeled set size in each task. The x-axis is in increasing order of i (the original eigenvalues
of the Laplacian). The mean (thick lines) and 1 standard deviation (dotted lines) of only
the top 50 is are plotted for clarity. The i values are scaled vertically for easy compari-
son among kernels. As expected the maximal-alignment kernels spectral transformation is
zigzagged, diffusion and Gaussian elds are very smooth, while order constrained kernels
are in between. The order constrained kernels (green) have large 1 because of the order
constraint. This seems to be disadvantageous  the spectral transformation tries to balance
it out by increasing the value of other is so that the constant K1s relative inuence is
smaller. On the other hand the improved order constrained kernels (black) allow 1 to be
small. As a result the rest is decay fast, which is desirable.

6 Conclusions

We have proposed and evaluated a novel approach for semi-supervised kernel construction
using convex optimization. The method incorporates order constraints, and the resulting
convex optimization problem can be solved efciently using a QCQP. In this work the base
kernels were derived from the graph Laplacian, and no parametric form for the spectral
transformation was imposed, making the approach more general than previous approaches.
Experiments show that the method is both computationally feasible and results in improve-
ments to classication performance when used with support vector machines.

2The hyperparameters 

2 and  are learned with the fminbnd() function in Matlab to maximize

kernel alignment.

3Results on baseball-hockey and odd-even are similar and omitted for space. Full results can be

found at http://www.cs.cmu.edu/zhuxj/pub/ocssk.pdf

PC vs. MAC

Religion vs. Atheism

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

0

0

5

10

15

20

25

rank

30

35

40

45

50

0

0

5

10

15

20

25

rank

30

35

40

45

50

Ten Digits (10 classes)

ISOLET (26 classes)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

0

0

5

10

15

20

25

rank

30

35

40

45

50

0

0

5

10

15

20

25

rank

30

35

40

45

50

Figure 1: Comparison of spectral transformation for the 5 semi-supervised kernels.

