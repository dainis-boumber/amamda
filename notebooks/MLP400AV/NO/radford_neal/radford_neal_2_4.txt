Abstract. Ratios of normalizing constants for two distributions are needed in both Bayesian statistics,
where they are used to compare models, and in statistical physics, where they correspond to dierences
in free energy. Two approaches have long been used to estimate ratios of normalizing constants.
The simple importance sampling (SIS) or free energy perturbation method uses a sample drawn
from just one of the two distributions. The bridge sampling or acceptance ratio estimate can be
viewed as the ratio of two SIS estimates involving a bridge distribution. For both methods, dicult
problems must be handled by introducing a sequence of intermediate distributions linking the two
distributions of interest, with the nal ratio of normalizing constants being estimated by the product
of estimates of ratios for adjacent distributions in this sequence. Recently, work by Jarzynski, and
independently by Neal, has shown how one can view such a product of estimates, each based on
simple importance sampling using a single point, as an SIS estimate on an extended state space. This
Annealed Importance Sampling (AIS) method produces an exactly unbiased estimate for the ratio
of normalizing constants even when the Markov transitions used do not reach equilibrium. In this
paper, I show how a corresponding Linked Importance Sampling (LIS) method can be constructed
in which the estimates for individual ratios are similar to bridge sampling estimates. As a further
elaboration, bridge sampling rather than simple importance sampling can be employed at the top
level for both AIS and LIS, which sometimes produces further improvement. I show empirically that
for some problems, LIS estimates are much more accurate than AIS estimates found using the same
computation time, although for other problems the two methods have similar performance. Like AIS,
LIS can also produce estimates for expectations, even when the distribution contains multiple isolated
modes. AIS is related to the tempered transition method for handling isolated modes, and to a
method for dragging fast variables. Linked sampling methods similar to LIS can be constructed
that are analogous to tempered transitions and to this method for dragging fast variables, which may
sometimes work better than those analogous to AIS.

1

1 Introduction

Consider two distributions on the same space, with probability mass or density functions 0(x) =
p0(x)/Z0 and 1(x) = p1(x)/Z1. Suppose that we are not able to directly compute 0 and 1, but only
p0 and p1, since we do not know the normalizing constants, Z0 and Z1. We wish to nd a Monte Carlo
estimate for the ratio of these normalizing constants, Z1/Z0, which we sometimes denote by r, using
samples of values drawn (at least approximately) from 0 and from 1. Sometimes, we may know Z0,
in which case we can arrange for it to be one, so that estimation of this ratio will give the numerical
value of Z1. Other times, we will be able to obtain only the ratio of normalizing constants, but this
may be sucient for our purposes.

In statistical physics, x represents the state of some physical system, and the distributions are

typically canonical distributions having the following form (for j = 0, 1):

pj(x) = exp(jU (x, j ))

(1)

where U (x, j) is an energy function, which may depend on the parameter j, and j is the inverse
temperature of system j. Many interesting properties of the systems are related to the free energy,
dened as  log(Zj) / j . Often, only the dierence in free energy between systems 0 and 1 is relevant,
and this is determined by the ratio Z1/Z0.

In Bayesian statistics, x comprises the parameters and latent variables for some statistical model,
0 is the prior distribution for these quantities (for which the normalizing constant is usually known),
and 1 is the posterior distribution given the observed data. We can compute p1(x) as the product
of the prior density for x and the probability of the data given x, but the normalizing constant, Z1,
is dicult to compute. We can interpret Z1 as the marginal likelihood  the probability of the
observed data under this model, integrating over possible values of the models parameters and latent
variables. The marginal likelihood for a model indicates how well it is supported by the data.

Although I will use simple distributions as illustrations in this paper, in real applications, x is
usually high dimensional, and at least one of 0 and 1 is usually quite complex. Accordingly, sam-
pling from these distributions generally requires use of Markov chain methods, such as the venerable
Metropolis algorithm (Metropolis, et al 1953). See (Neal 1993) for a review of Markov chain sampling
methods. Sometimes, however, 0 will be relatively simple, and independent points drawn from it can
be generated eciently, as would often be the case with the prior distribution for a Bayesian model,
or for a physical system at innite temperature (0 = 0).

Many methods for estimating ratios of normalizing constants from Monte Carlo data have been
investigated in the physics literature (for a review, see (Neal 1993, Section 6.2)), and later rediscov-
ered in the statistics literature (Gelman and Meng 1998). A logical method to start with is simple
importance sampling (SIS), also called free energy perturbation, based on the following identity,
which can easily be proved on the assumption that no region having zero probability under 0 has

2

non-zero probability under 1:

Z1
Z0

= E0(cid:20) p1(X)

p0(X)(cid:21) 

1
N

N

Xi=1

p1(x(i))
p0(x(i))

=

1
N

N

Xi=1

r(i)
SIS = rSIS

(2)

In the above equation, E0 denotes an expectation with respect to the distribution 0, which is
estimated by a Monte Carlo average over points x(i), . . . , x(N ) drawn from 0 (either independently,
or using a Markov chain sampler). Here and later, rM will denote an estimate of r = Z1/Z0, found by
method M. If this estimate is an average of unbiased estimates based on a number of samples, these
individual estimates will be denoted by r(i)
M .

The simple importance sampling estimate, rSIS, will be poor if 0 and 1 are not close enough 
in particular, if any region with non-negligible probability under 1 has very small probability under
0. Such a region would have an important eect on the value of r, but very little information about
it would be contained in the sample from 0. In such a situation, it may be possible to obtain a good
estimate by introducing intermediate distributions. Parameterizing these distributions in some way
using , we can dene a sequence of distributions, 0, . . . , n, with 0 = 0 and n = 1 so that the rst
and last distributions in the sequence are 0 and 1, with the intermediate distributions interpolating
between them. We can then write

Z1
Z0

=

Zj+1
Zj

n1

Yj=0

(3)

Provided that j+1 and j are close enough, we can estimate each of the factors Zj+1 /Zj using
simple importance sampling, and from these estimates obtain an estimate for Z1/Z0.

We can obtain good estimates in a wider range of situations, or using fewer intermediate distributions
(sometimes none), by applying a technique introduced by Bennett (1976), who called it the acceptance
ratio method. This method was later rediscovered by Meng and Wong (1996), who called it bridge
sampling. Lu, Singh, and Kofke (2003) provide a recent review and assessment. One way of viewing
this method is that it replaces the simple importance sampling estimate for Z1/Z0 by a ratio of
estimates for Z/Z0 and Z/Z1, where Z is the normalizing constant for a bridge distribution,
(x) = p(x)/Z, which is chosen so that it is overlapped by both 0 and 1. Using simple importance
sampling estimates for Z/Z0 and Z/Z1, we can obtain the estimate

Z1
Z0

= E0(cid:20) p(X)

p0(X)(cid:21) . E1(cid:20) p(X)

p1(X)(cid:21) 

1
N0

N0

Xk=1

p(x0,k)

p0(x0,k) . 1

N1

p(x1,k)
p1(x1,k)

N1

Xk=1

= rbridge

(4)

where x0,1, . . . , x0,N0 are drawn from 0 and x1,1, . . . , x1,N1 are drawn from 1.

One simple choice for the bridge distribution is the geometric bridge:

pgeo

 (x) = pp0(x)p1(x)

3

(5)

which is in a sense half-way between 0 and 1. As discussed by Bennett (1976) and by Meng and
Wong (1996), the asymptotically optimal choice of bridge distribution is

popt
 (x) =

p0(x)p1(x)

r(N0/N1)p0(x) + p1(x)

(6)

where r = Z1/Z0. Of course, we cannot use this bridge distribution in practice, since we do not know
r. We can use a preliminary guess at r to dene an initial bridge distribution, however, which will give
us a bridge sampling estimate for Z1/Z0. Using this estimate as the new value of r, we can rene our
bridge distribution, iterating this process as many times as desired. The result of this iteration can
also be viewed as a maximum likelihood estimate for r, as discussed by Shirts, et al (2003), who argues
on this basis that it is asymptotically as good as any estimate for r. I have found that estimates with
r set iteratively are often better than those found with the true value of r (which does not contradict
optimality of the true value for a xed choice of bridge distribution).

If 0 and 1 do not overlap suciently, no bridge distribution will produce good estimates, and
we will have to introduce intermediate distributions as in equation (3). Note, however, that the
bridge sampling estimate with either of the above bridge distributions converges to the correct ratio
asymptotically as long there is some region that has non-zero probability under both 0 and 1, a
much weaker requirement than that for simple importance sampling.

This advantage of bridge sampling over SIS can be seen in a simple example involving distributions
that are uniform over an interval of the reals. Let p0(x) = I(0,3)(x) and p1(x) = I(2,4)(x), so that
Z0 = 3 and Z1 = 2. The simple importance sampling estimate of equation (2) does not work, as it
converges to 1/3 rather than 2/3. However, using a bridge distribution with p(x) = I(2,3), which is
eectively what both popt
 will be in this example, the bridge sampling estimate of equation (4)
converges to the correct value, since the numerator converges to 1/3 and the denominator to 1/2.

 and pgeo

Although both simple importance sampling and bridge sampling have been successfully used in many
applications, they have some deciencies. One issue is that although the SIS estimate of equation (2)
is unbiased for Z1/Z0, the bridge sampling estimate of equation (4) is not, and the same would
appear to be the case for an estimate using intermediate distributions (via equation (3)). This is of
no direct importance, particularly since we are often more interested in log(Z1/Z0) than in Z1/Z0
itself. However, it does preclude averaging independent replications of the bridge sampling estimate
to obtain a better estimate, since the bias would prevent convergence to the correct value as the
number of replications increases. A more vexing diculty is that, except sometimes for 0, sampling
from the distributions  must usually be done by Markov chain methods, which approach the desired
distribution only asymptotically. To speed convergence, the Markov chain for sampling j is often
started from the last state sampled for j1 , but it is unclear how many iterations should then be
discarded before an adequate approximation to the correct distribution is reached.

Surprisingly, these diculties can be completely overcome when using simple importance sampling
with a single point. As shown by Jarzynski (1997, 2001), and later independently by myself (Neal 2001),
an estimate for Z1/Z0 using intermediate distributions as in equation (3) will be exactly unbiased if

4

each of the ratios Zj+1/Zj is estimated using the simple importance sampling estimate of equation (2)
with N = 1, sampling each distribution with a Markov chain update starting with the point for the
previous distribution. Averaging the estimates obtained from M independent replications of this
process (called runs) produces the following estimate:

Z1
Z0 

1
M

M

Xi=1

n1

Yj=0

pj+1 (x(i)
j )
pj (x(i)
j )

=

1
M

M

Xi=1

r(i)
AIS = rAIS

(7)

0 , . . . , x(M )

0

invariant to x(i)

are drawn independently from 0, and each x(i)
j

Here, x(1)
for j > 0 is generated by applying
a Markov chain transition that leaves j
j1. This single Markov transition (which
could, however, consist of several Metropolis or other updates if we so choose), will usually not be
enough to reach equilibrium, but the estimate rAIS is nevertheless exactly unbiased, and will converge
to the true value as M increases, provided that no region having zero probability under j has non-
zero probability under j+1. This can be proved by showing how the estimate above can be seen as
a simple importance sampling estimate on an extended state space that includes the values sampled
for the intermediate distributions.

I call this method Annealed Importance Sampling (AIS), since the sequence of distributions used
often corresponds to an annealing procedure, in which the temperature is gradually decreased. As I
discuss in (Neal 2001), this allows the procedure to sample dierent isolated modes of the distribution
on dierent runs, properly weighting the points obtained from each of these runs to produce the correct
probability for each mode. AIS is related to an earlier method for moving between isolated modes
that I call tempered transitions (Neal 1996). In a recent paper (Neal 2004), I show how tempered
transitions can be modied to produce a method for ecient Markov chain sampling when some of
the state variables are fast  ie, when it is possible to more quickly recompute the probability of a
state when only these fast variables change than when the other slow variables change as well. In this
method, the fast variables are dragged through intermediate distributions in order to produce more
appropriate values to go with a proposed change to the slow variables. Deciding whether to accept
the nal proposal involves what is in eect an estimate of the ratio of normalizing constants for the
conditional distributions of the fast variables.

In this paper, I show how the ideas behind Annealed Importance Sampling and bridge sampling
can be combined.
I call the resulting method Linked Importance Sampling (LIS), since the two
samples needed for bridge sampling are linked by a single state that is used in both. Intermediate
distributions can be used, with each distribution being linked by a single state to the next distribution.
In contrast to bridge sampling, LIS estimates are unbiased, and as is the case for AIS, they remain
exactly unbiased even when intermediate distributions are used, and when sampling is done using
Markov chain transitions that have not converged to their equilibrium distributions.

Crooks (2000) mentions a dierent way of combining AIS with bridge sampling  since AIS esti-
mates are simple importance sampling estimates on an extended state space, we can combine forward
and reverse estimates to produce a bridge sampling estimate that may be superior. I will call this

5

method bridged AIS. Similarly, such a top-level application of bridge sampling can be combined with
the low-level application of bridge sampling in LIS, giving what I call bridged LIS.

Using tests on sequences of one-dimensional distributions, I demonstrate that for some problems
LIS is much more ecient than AIS  a result that should be expected, since in extreme cases, such
as for the uniform distributions discussed above, the simple importance sampling estimates underlying
AIS do not converge to the correct answer even asymptotically, whereas bridge sampling estimates do.
For some other problems, however, AIS and LIS perform about equally well. The bridged version of
AIS sometimes performs much better than the unbridged version, but still performs less well than LIS
and its bridged version on some problems. I also analyse the asymptotic properties of AIS and LIS
for some types of distribution, providing additional insight into their behaviour.

Variants of tempered transitions and of my method for dragging fast variables can be constructed
that are analogous to LIS rather than to AIS. I discuss the linked variant of tempered transitions
briey, and include a more detailed description of a linked version of dragging, which may sometimes be
better than the version related to AIS. I conclude by discussing some possibilities for future research.

2 The Linked Importance Sampling procedure

Assume that we can evaluate the unnormalized probability or density functions p(x), for any value
of the parameter , with the normalized form of such a distribution being denoted by . The values
 = 0 and  = 1 dene the two distributions we are interested in, for which the normalizing constants
are Z0 and Z1. A sequence of n1 intermediate values for  dene distributions that will assist in
estimating the ratio of these normalizing constants, r = Z1/Z0. We denote the values of  for the
distributions used by 0, . . . , n, with 0 = 0 and n = 1. Typically, j < j+1 for all j.

For problems in statistical physics,  might be proportional to the inverse temperature, , of
equation (1), or might map to a value for . For a Bayesian inference problem,  might be a power
that the likelihood is raised to, so that  = 0 causes the data to be ignored, and  = 1 gives full
weight to the data; the ratio Z1/Z0 will then be the marginal likelihood. In both of these examples,
progressing in small steps from  = 0 to  = 1 is not only useful in estimating Z1/Z0, but also often
has an annealing eect, which helps avoid being trapped in a local mode of the distribution.

2.1 Details of the LIS procedure

For each distribution, , assume we have a pair of Markov chain transition probability (or density)

functions, denoted by T(x, x) and T (x, x), satisfying R T(x, x)dx = 1 and R T (x, x)dx = 1, for

which the following mutual reversibility relationship holds:

(x) T(x, x) = (x) T (x, x),

for all x and x

(8)

From this relationship, one can easily show that both T and T  leave  invariant  ie, that

R (x)T(x, x)dx = (x), and the same for T . If T is reversible (ie, satises detailed balance),

6

then T  will be the same as T. Non-reversible transitions often arise when components of state are
updated in some predetermined order, in which case the reverse transition simply updates components
in the opposite order. As a special case, T might draw the next state from  independently of the
current state. Such independent sampling may often be possible for T0.

These Markov chain transitions are used to obtain samples that are approximately drawn from each
of the n+1 distributions, 0, . . . , n . We assume that we can begin sampling from 0 by drawing
a single point independently from 0. For j > 0, we begin sampling from j by selecting a link
state, xj1j, from the sample associated with j1. For all j, we produce a sample of Kj +1 states
from this starting point by applying a total of Kj forward (Tj ) or reversed (T j ) Markov transitions.
Link states are selected using bridge distributions, pj j+1, which are dened in terms of pj and pj+1,
perhaps using the form of equation (5) or (6), with p0 replaced by pj and p1 by pj+1.
In detail, the Linked Importance Sampling procedure produces M estimates, r(1)

LIS, . . . , r(M )

LIS , that

are averaged to produce the nal estimate, rLIS. Each r(i)

LIS is obtained by performing the following:

The LIS Procedure

1) Pick an integer 0 uniformly at random from {0, . . . , K0}, and then set x0,0 to a value drawn

from 0.

2) For j = 0, . . . , n, sample Kj +1 states drawn (at least approximately) from j as follows:

a) If j > 0: Pick an integer j uniformly at random from {0, . . . , Kj}, and then set xj,j to

xj1j.

b) For k = j + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tj (xj,k1, xj,k). (If j = Kj, do nothing in this step.)

c) For k = j  1, . . . , 0, draw xj,k according to the reverse Markov chain transition probabil-

ities T j (xj,k+1, xj,k). (If j = 0, do nothing in this step.)

d) If j < n: Pick a value for j from {0, . . . , Kj} according to the following probabilities:

0(j | xj) =

pj j+1(xj,j )

pj (xj,j ) .

and then set xj j+1 to xj,j .

Kj

Xk=0

pj j+1(xj,k)

pj (xj,k)

(9)

3) Set n to a value chosen uniformly at random from {0, . . . , Kn}. (This selection has no eect on

the estimate, but is used in the proof of correctness.)

4) Compute the estimate from this run as follows:

r(i)
LIS =

n1

Yj=0

1

Kj + 1

Kj

Xk=0

pj j+1(xj,k)

pj (xj,k) .

1

Kj+1 + 1

Kj+1

Xk=0




pj j+1(xj+1,k)

pj+1(xj+1,k) 


(10)

(Note that most of the factors of 1/(Kj + 1) and 1/(Kj+1 + 1) cancel, giving a nal result of
(Kn +1) / (K0 +1), but the redundant factors are retained above for clarity of meaning.)

7


1/2


0


1

Figure 1: An illustration of Linked Importance Sampling. One intermediate distribution is used, with
1 = 1/2. The distributions 0, 1/2, and 1 are represented by ovals enclosing the regions of high
probability under each distribution. Nine Markov chain transitions are performed at each stage. The
two link states are shown as black dots. The initial and nal states (indexed by 0 and n) are shown
as gray dots. Other states generated by the forward and reverse Markov chain transitions are shown
as empty dots. For this run, 0 = 4, 0 = 9, 1 = 1, 1 = 8, 2 = 3, and 2 = 7.

The result of performing steps (1) through (3) is illustrated in Figure 1. After M runs of this procedure,
the nal estimate is computed as

rLIS =

r(i)

LIS

1
M

M

Xi=1

(11)

The crucial aspect of Linked Importance Sampling is that when moving from distribution j to
j+1, a link state, xj j+1, is randomly selected from among the sample of points xj,1, . . . , xj,Kj+1 that
are associated with j . We can view the link state as part of the sample associated with j+1 as well
as that associated with j . Accordingly, when using the optimal bridge of equation (6), I will set
N0/N1 to (Kj +1)/(Kj+1 +1), though the proof of optimality for bridge sampling does not guarantee
that this is an optimal choice when using this bridge distribution for LIS.

2.2 Proof that LIS estimates are unbiased

In order to prove that r(i)
LIS is an unbiased estimate of r = Z1/Z0, we can regard steps (1) through (3)
above as dening a distribution, 0, over all the quantities involved in the procedure  namely, xj,
j, and j, for j = 0, . . . , n, with xj representing xj,0, . . . , xj,Kj . We then consider the procedure for
generating these same quantities in reverse, which operates as follows:

8

The Reverse LIS Procedure

1) Pick an integer n uniformly at random from {0, . . . , Kn}, and then set xn,n to a value drawn

from n.

2) For j = n, . . . , 0, sample Kj +1 states drawn (at least approximately) from j as follows:

a) If j < n: Pick an integer j uniformly at random from {0, . . . , Kj}, and then set xj,j to

xj j+1.

b) For k = j + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tj (xj,k1, xj,k). (If j = Kj, do nothing in this step.)

c) For k = j  1, . . . , 0, draw xj,k according to the reverse Markov chain transition proba-

bilities T j (xj,k+1, xj,k). (If j = 0, do nothing in this step.)

d) If j > 0: Pick a value for j from {0, . . . , Kj} according to the following probabilities:

1(j | xj) =

pj1j(xj,j )

pj (xj,j ) .

Kj

Xk=0

pj1j(xj,k)

pj (xj,k)

(12)

and then set xj1j to xj,j .

3) Set 0 to a value chosen uniformly at random from {0, . . . , K0}.

This reverse procedure also denes a distribution over all the quantities generated (xj, j, and j for
j = 0, . . . , n), which will be denoted by 1.

We now dene the unnormalized probability (density) functions P0(x, , ) = Z00(x, , ) and
P1(x, , ) = Z11(x, , ). The ratio of normalizing constants for these distributions is obviously
r = Z1/Z0. We can estimate this ratio by simple importance sampling, using the ratios

P1(x, , )
P0(x, , )

=

Z1 1(n) n(xn,n)

Z0 0(0) 0(x0,0)

n1

n

Qj=0
Qj=1

n

1(j)

0(j)

n

n

1(j | xj) 1(0)

Qj=0
Qj=0
0(xj | j, xj,j )

Qj=1
1(xj | j, xj,j )
Qj=0
0(j | xj) 0(n)

n1

From Steps (2b) and (2c) of the forward and reverse procedures, along with the mutual reversibility

(13)

(14)

(15)

(16)

relationship of equation (8), we see that

n

0(xj | j, xj,j ) =

=

=

Yk=j+1
Yk=j+1

n

Tj (xj,k1, xj,k) 

Tj (xj,k1, xj,k) 

j 1

Yk=0
Yk=0

j 1

T j (xj,k+1, xj,k)

Tj (xj,k, xj,k+1)

j (xj,k)
j (xj,k+1)

j (xj,0)
j (xj,j )

n

Yk=1

Tj (xj,k1, xj,k)

9

and similarly,

1(xj | j, xj,j ) =

j (xj,0)
j (xj,j )

n

Yk=1

Tj (xj,k1, xj,k)

(17)

From this, we see that parts of the ratio in equation (13) can be written as

n

n

Qj=0
Qj=0

Z1 n(xn,n)

Z0 0(x0,0)

1(xj | j, xj,j )

0(xj | j, xj,j )

=

pn(xn,n)
p0(x0,0)

j (xj,j )
j (xj,j )

=

n

Yj=0

n1

Yj=0

pj+1(xj,j )
pj (xj,j )

(18)

The last step uses the fact that for j = 1, . . . , n, xj,j = xj1j = xj1,j1.

From Steps (1) and (2a), we see that 0(j) = 1 / (Kj +1) and 1(j) = 1 / (Kj +1). Using this,

and again using xj,j = xj1,j1, we get that

n1

n

Qj=0
Qj=1

1(j)

0(j)

n

n1

Qj=1
Qj=0

n1

0(j | xj)

1(j | xj)

=

n1

Qj=0

1(j+1 | xj+1) (Kj+1 +1)
Qj=0
pj+1(xj+1,j+1) .

0(j | xj) (Kj +1)

pj j+1(xj+1,j+1)

Kj+1+1

Kj+1

1

=

=

n1

Yj=0

n1

Yj=0

pj j+1(xj,j )

Kj +1

pj (xj,j ) . 1
Xk=0

Yj=0

Kj +1

n1

Kj

1




pj (xj,j )
pj+1(xj,j )

pj j+1(xj+1,k)
pj+1 (xj+1,k)

pj j+1(xj,k)

Kj

Xk=0
Xk=0
pj (xj,k) .

pj (xj,k)

pj j+1(xj,k)

1

Kj+1+1

Kj+1

Xk=0

pj j+1(xj+1,k)

pj+1(xj+1,k) 


(19)

(20)

(21)

From Steps (1) and (3), we see that 0(0) = 1(0) = 1 / (K0 + 1) and 1(n) = 0(n) =
1 / (Kn+1), so these factors cancel in equation (13). The factors in equation (18) cancel with the rst
part of equation (21). The nal result is that the simple importance sampling estimate based on a
single LIS run is as shown in equation (10), demonstrating that rLIS is indeed an unbiased estimate of
r = Z1/Z0.

2.3 Bridged LIS estimates

Since the LIS estimate can be viewed as a simple importance sampling estimate on an extended space,
we can consider a bridged LIS estimate in which this top-level SIS estimate is replaced by a bridge
sampling estimate. This will require that we actually perform the reverse LIS procedure described
above, from which an LIS estimate for the reverse ratio, r = Z0/Z1, can be computed:

r(i)
LIS =

n

Yj=1




1

Kj + 1

Kj

Xk=0

pj1j(xj,k)

pj (xj,k) .

1

Kj1 + 1

Kj1

Xk=0

pj1j(xj1,k)

pj1 (xj1,k) 


(22)

10

The reversed procedure requires independent sampling from 1. This will usually not be possible
directly, but well-separated states from a Markov chain sampler with 1 as its invariant distribution
will provide a good approximation, provided that this sampler moves around the whole distribution,
without being trapped in an isolated mode. Indeed, the entire sample of Kn+1 states from 1 that is
needed at the start of the reverse procedure can be obtained by taking consecutive states from such a
Markov chain sampler.

For the bridged form of LIS, we also need a suitable bridge distribution, P, for which we must be
able to evaluate the ratios P/P0 and P/P1. (Note that this choice of a top-level bridge distribution
is separate from the choices of low-level bridge distributions, pj j+1, though we might use the same
form for both.) With the optimal bridge of equation (6), these ratios can be written as follows, if the
forward procedure is performed M times and the reverse procedure M times:

P opt
 (x, , )
P0(x, , )

P opt
 (x, , )
P1(x, , )

+ 1#1
= " r (M/M ) (cid:18) P1(x, , )
P0(x, , )(cid:19)1
P1(x, , )(cid:19)1#1
= " r (M/M ) + (cid:18)P0(x, , )

The geometric bridge of equation (5) results in

P geo
 (x, , )
P0(x, , )

P geo
 (x, , )
P1(x, , )

P0(x, , )

= s P1(x, , )
= s P0(x, , )

P1(x, , )

(23)

(24)

(25)

(26)

These expressions allow us to express bridged LIS estimates in terms of the simple LIS estimate of
equation (10), and its reverse version of equation (22). For the optimal bridge, we get

ropt
LIS-bridged =

1
M

M

Xi=1

r (M/M ) / r(i)

1

LIS + 1 . 1

M

M

Xi=1

1

r (M/M ) + 1/r(i)

LIS

Similarly, for the geometric bridge, we get

rgeo
LIS-bridged =

1
M

M

Xi=1 qr(i)

LIS . 1

M

M

Xi=1 qr(i)

LIS

(27)

(28)

2.4 LIS estimates with independent sampling with no intermediate distributions

It is interesting to look at the special case of Linked Importance Sampling with n = 1  ie, in which
the are no intermediate distributions between 0 and 1  in which the points from both 0 and
1 are sampled independently. The LIS procedure can then be simplied somewhat, and it is also
possible to improve the LIS estimate by averaging over the choice of link state. Such averaging is not

11

feasible when Markov chain sampling is used, since choosing a dierent link state would require a new
simulation of the Markov transitions.

Since we will sample points independently, there is no need to decide how many points will be
sampled by the forward transitions and how many by the reverse transitions in Steps (2a) and (2b)
of the LIS procedure. We simply obtain a pair of samples consisting of points x0,0, . . . , x0,K0 drawn
independently from 0, and points x1,1, . . . , x1,K1 drawn independently from 1. We then randomly
select a link state, indexed by , from among x0,0, . . . , x0,K0 according to the following probabilities,
which depend on the choice of a single bridge distribution, denoted by p(x):

0(| x0) =

p(x0,)

p0(x0,) .

K0

Xk=0

p(x0,k)
p0(x0,k)

The LIS estimate for r = Z1/Z0 based on this pair of samples from 0 and 1 is

r(i)
LIS =

1

K0 +1

K0

Xk=0

p(x0,k)

p0(x0,k) . 1

K1+1" p(x0,)

p1(x0,)

+

K1

Xk=1

p(x1,k)

p1(x1,k)#

(29)

(30)

The superscript i is used here to indicate that this estimate is based on the ith pair of samples. We
can see that it is very similar to the bridge sampling estimate of equation (4), except that the link
state is included in both samples. Since these LIS estimates are unbiased, we can average M of them
to obtain a nal LIS estimate.

We can also average the estimate of equation (30) over the random choice of link state, which is
guaranteed to produce an estimate (also unbiased) with smaller mean-squared-error (see Schervish
1995, Section 3.2). The result is

r(i)
LIS-ave =

0(| x0)

1

K0 +1

K0

X=0

K0

Xk=0

=

K1+1
K0+1

K0

X=0

p(x0,)

p0(x0,) . " p(x0,)

p(x0,k)

p1(x0,)

p0(x0,k) . 1
Xk=1

K1 +1" p(x0,)
p1(x1,k)#

p1(x0,)

p(x1,k)

K1

+

+

K1

Xk=1

p(x1,k)

p1(x1,k)#

(31)

(32)

Averaging these estimates over M pairs of samples produces a nal estimate denoted by rLIS-ave.

To use bridged LIS in this context, we need to nd reverse estimates as well, but these reverse
estimates neednt be independent of the forward estimates, since the asymptotic validity of the bridge
sampling estimate of equation (4) does not depend on the samples x0 and x1 being independent.
Accordingly, we can use the same samples from 0 and 1 for the forward and the reverse operations.
However, to perform reverse sampling, we need to have a sample of K1 +1 points drawn from 1, the
rst of which is ignored when performing forward sampling. Conversely, the rst of the K0 +1 points
drawn from 0 is ignored when performing the reverse sampling.

We can improve the bridged LIS estimates by averaging the numerator and the denominator of
equation (27) or (28) with respect to the random choice of link state. We can also average with

12

respect to the omission of one of the points from one of the samples  ie, rather than omitting
the rst of K1 + 1 points in the sample from 1 when computing a forward estimate, we average
with respect to a random choice of point to omit, and similarly for reverse estimates. Note that the
averaging should be done over the sums in the numerator and denominator, not with respect to the
entire estimate, nor with respect to the values of r(i)
LIS appearing inside the summands. The
eective sample size after this additional averaging of dependent points is unclear, so it is not obvious
what the ratio of sample sizes in equation (6) should be, but using (K0 + 1)/(K1 + 1) is probably
adequate.

LIS and r(i)

3 Analytical comparisons of AIS and LIS

In this section, I analyse (somewhat informally) the performance of AIS and LIS asymptotically, and
in other situations where analytical results are possible.

3.1 Asymptotic properties of AIS and LIS estimates

I begin by analysing the asymptotic performance of AIS and LIS when the sequence of distributions
is dened by an unnormalized density function of the following form:

p(x) = p0(x) exp(U (x))

(33)

This class includes sequences of canonical distributions dened by equation (1) in which the inverse
temperature varies, as well as sequences that can be used for Bayesian analysis, in which p0 denes the
prior and  is a power that the likelihood (expressed as exp(U (x))) is raised to, with  = 1 giving the
posterior distribution. For these distributions, we can express r using the well-known thermodynamic
integration formula as follows:

r = log(Z1/Z0) = Z 1

0

E (U ) d

(34)

The analysis here is asymptotic, as the number of intermediate distributions used, given by n1,
goes to innity. I will assume the j dening these distributions are chosen according to a scheme in
which for any a  (0, 1), the spacing j+1  j when j = a n is asymptotically proportional to 1/n
 in other words, the relative density of intermediate distributions in the neighborhood of dierent
values of  stays the same as the overall density increases. The simplest such scheme is to let j = j/n,
though other schemes may sometimes be better.

With the above form for p, the AIS estimate from a single run (from equation (7)) can be written

as follows:

log r(i)

AIS =

n1

Xj=0

log(cid:16)pj+1 (x(i)

j ). pj (x(i)

j )(cid:17) =

n1

Xj=0

j (cid:17)
(j+1  j) U(cid:16)x(i)

(35)

13

When j = j/n, this can be seen as a stochastic form of Riemanns Rule for numerically integrating
equation (34), though one dierence is that log rAIS converges to the correct value as M goes to innity
even if n stays xed.

Provided that there is some nite bound on the variance of U under all the distributions , and
that the Markov transitions used mix well, a Central Limit Theorem will apply, allowing us to conclude
that the distribution of n = log r(i)
AIS becomes Gaussian as n goes to innity. Let the mean of n be
n, and let the variance of n asymptotically be 2/n, where  is determined by details of the spacing
of intermediate distributions and of the degree of autocorrelation in the Markov transitions. Note
that E[Y q] = exp(q + q2 2/2) when Y = exp(X) and X is Gaussian with mean  and variance  2.
Using this, the mean of exp(n) is exp(n + 2/2n). This must equal r, since rAIS is unbiased, so
n = log(r)  2/2n. Using this, we can see that the variance of r(i)
AIS = exp(n) is r [exp(2/2n)  1],
which for large n will be approximately r2/2n. The variance of rAIS will therefore be r2/2nM .
Asymptotically, the total computational eort, which will generally be proportional to nM , can be
divided in any way between more intermediate distributions (n) or more runs (M ) without aecting
the accuracy of estimation of r, provided that n is kept large enough that these asymptotic results
apply  a fact noted by Hendrix and Jarzynski (2001). We can therefore use a value of M greater
than one without penalty, in order to obtain an error estimate from the degree of variation over the
M runs.

For LIS, we can write the log of the estimate from one run (equation (10)) as follows:

n1

Kj

1

Kj+1

1

log



pj j+1(xj,k)

pj (xj,k) 

log r(i)

LIS =

Kj + 1

Xj=0

Xk=0
Suppose that we let Kj = mK 0
j  for all j and some set of K 0
j , and that we then let m go to
innity. Assuming that the variances of the ratios of probabilities are nite, and that the Markov
chain transitions used mix suciently well, a Central Limit Theorem will again apply, and we can
conclude that all of the n terms in the sum above, and therefore also the sum itself, will approach
Gaussian distributions, with variances proportional to 1/m.

Xk=0

Kj+1 + 1

pj j+1(xj+1,k)

pj+1 (xj+1,k) 



 (36)

  log


To analyse the LIS estimate in more detail, we need to assume a form of bridge distribution, as well
as a form for p. If p has the form of equation (33) and we use the geometric bridge of equation (5),
we can write

log r(i)

LIS =

n1

Xj=0

 log


log


1

Kj + 1

1

Kj+1 + 1

Kj

Xk=0
Xk=0

exp((j+1j) U (xj,k) / 2)
 
exp((jj+1) U (xj+1,k) / 2)


Kj




(37)

Since exp(z)  1 + z and log(1 + z)  z when z is small, we can rewrite this when n is large (and

14

hence j+1j is small) as
Xj=0

LIS 

log r(i)

n1

n1

 log

1 
log
1 +



j+1j

2

j+1j

2

1

Kj + 1

j+1j

2

1

Kj+1 + 1

U (xj,k) +

Kj

Xk=0

1

Kj + 1

K0

Kj

Xk=0
Xk=0

Kj+1

U (xj,k)
 

U (xj+1,k)


Xk=0
Xk=0

Kj+1 + 1

Kn + 1

Kj+1

Kn

1

1





Xj=0
10

2

= 



n1

Xj=1

U (x0,k) 

nn1

2

1

K0 + 1

Xk=0
j+1j1

1

2

Kj + 1

Kj

Xk=0

U (xj,k)

U (xj+1,k)


U (xn,k)

(38)

(39)

(40)

When j = j/n, this looks like a stochastic form of the Trapezoidal Rule for numerically integrating
equation (34). Since the Trapezoidal Rule converges faster than Reimanns Rule, one might expect
LIS to perform better than AIS asymptotically, but this is not so in this stochastic situation. Suppose
for simplicity that we set all Kj = m. The variance of log r(i)
LIS will be dominated by the variance of the
last sum above, which will be proportional to 1/nm, assuming that m is large, so that the dependence
between terms (from sharing link states) is negligible. Using the same argument as for AIS above,
the variance of log rLIS will be proportional to 1/nmM . Considering that the computation time for an
LIS run will be proportional to nm, versus n for AIS, we see that the variances of the AIS and LIS
estimates go down the same way in proportion to computation time, asymptotically as n and m go to
innity.

Furthermore, the proportionality constant should be the same for AIS and LIS, assuming that the
overhead of the two procedures is negligible compared to the time spent performing Markov transitions,
so that the proportionality constants for computation time are the same for AIS (multiplying n) and
for LIS (multiplying nm). The proportionality constants for variance for AIS (multiplying 1/nM )
and for LIS (multiplying 1/nmM ) depend in a complex way on the form of the density of j values
and on the mixing properties of the Markov transitions, but the result should be the same for AIS
and LIS, provided the same scheme is used for choosing j values, and the same Markov transitions
are used, parameterized smoothly in terms of . A dierence that might appear signicant is that
for AIS only one Markov transition is done for each j, whereas for LIS, m such transitions are
done. However, as n goes to innity, nearby distributions become more similar, so transitions for m
consecutive distributions become similar to m transitions for one of these distributions.

The apparently pessimistic conclusion from this is that when both n and m (and hence the Kj)
are large, the performance of LIS should be about the same as that of AIS (with n for AIS chosen to

15

equalize the computation time), assuming that the distributions used have the form of equation (33),
that the variance of U is nite under all of the distributions , and that the Markov transitions used
mix well enough. Fortunately, however, there is no reason to make both m and n large with LIS. For
good performance, n must be large enough that j and j+1 overlap signicantly, but there is no
reason to make n much larger than this. The accuracy of the estimates can be improved as desired
by increasing m and/or M while keeping n xed. The results below show that LIS estimates with n
xed are sometimes much better than AIS estimates.

Finally, let us consider the asymptotic performance of the bridged versions of AIS and LIS, assuming
that the variance of U is nite, so that the distribution of the estimates from individual runs becomes
Gaussian as n (for AIS) or m (for LIS) goes to innity. Looking at equations (27) and (28), which
also are applicable to bridged AIS estimates, we see that the log of r(i)
LIS-bridged can for both optimal and
geometric bridges be expressed as the dierence of the log of the numerator, which is the mean of a
function of the forward estimates, r(i)
LIS, and the log of the denominator, which is the mean of a function
of the reverse estimates, r(i)
LIS. If these forward and reverse estimates have Gaussian distributions with
small variances, 2 and 2, then r(i)
LIS-bridged will also be Gaussian, with a variance that can be computed
in terms of the derivatives of the summands in the numerator and the denominator, with respect to
r(i)
LIS and r(i)
LIS, evaluated at the true values of r and 1/r. I will assume that r = 1 below, as can be
done without loss of generality.

LIS = r = 1 and r(i)

For the geometric bridge, these derivatives are both 1/2, from which it follows that the variance of
the numerator in equation (28) is 2/4M and that of the denominator is 2/4M . Since the numerator
and denominator evaluate to one for r(i)
LIS = 1/r = 1, the sum of the variances of
the logs of the numerator and denominator is 2/4M + 2/4M . If 2 = 2 and M = M , this reduces
to 2/2M . The variance of an unbridged LIS estimate will be 2/M . However, the bridged estimate
requires time proportional to M + M , compared to just M for the unbridged estimate. The value of
M for the unbridged method can therefore be twice as large as for the bridged method, with the result
that bridged and unbridged estimates perform equally well asymptotically (assuming the variance of
U is nite).

For the optimal bridge, the derivatives of the summands in the numerator and denominator are
both 1/4, when evaluated at r(i)
LIS = 1/r = 1, and assuming that M = M . The
numerator and denominator both evaluate to 1/2, with the result that asymptotically the variance of
the bridged estimate, assuming 2 = 2, is 2/2M , the same as for the geometric bridge.

LIS = r = 1 and r(i)

In conclusion, bridged AIS and LIS estimates asymptotically have the same performance as the
corresponding unbridged estimates (with twice the value of M ), for both the optimal and geometric
bridges, assuming U has nite variance. This conclusion applies more generally, as long as a Central
Limit Theorem holds for the individual estimates, r(i)
LIS. However, the bridged methods may
be much better when the variance of U is innite, or for classes of distributions other than that of
equation (33). The bridged methods may also provide improvement when the values of n or m are
not large enough for the asymptotic results to apply.

LIS and r(i)

16

3.2 Properties of AIS and LIS when sampling from uniform distributions

In this section, I will demonstrate that when n is kept suitably small, LIS can perform much better
than AIS when these methods are applied to sequences of uniform distributions.

As a rst example, consider the class of nested uniform distributions with unnormalized densities

given by

p(x) = ( 1 if s < x < s

0 otherwise

(41)

for which the normalizing constants are Z = 2s, so that r = Z1/Z0 = s. The results concerning
this class of distributions can easily be extended to any class of uniform distributions, in any number
of dimensions, that have nested regions of support. For both AIS and LIS, I will assume that the
intermediate distributions are dened by j = j/n. With this choice, the probability that a point, x,
randomly sampled from j will have pj+1(x) = 1 is s1/n, for any j.

During an AIS run, only a single point is sampled from each distribution. An AIS run will produce
an estimate for r of zero if any of the ratios pj+1(x(i)
j ) in equation (7) are zero, which
happens with probability 1  (s1/n)n = 1  s, and will otherwise produce an estimate of one. Note
that the distribution of estimates is independent of n. AIS is therefore not a useful technique for
nested uniform distributions  simple importance sampling (ie, AIS with n = 1) would work just as
well (or just as poorly, if s is very small). Bridged AIS produces no improvement in this context.

j ) / pj (x(i)

Suppose instead we use LIS with all Kj = m, and suppose that the Markov transitions, Tj, produce
points that are almost independent of the previous point. For this problem, both the geometric and
optimal forms of the bridge distribution result in pj j+1(x) = pj+1(x). If m + 1 points are sampled
independently from j , the fraction of these points for which pj+1(x) is one will have variance s1/n (1
s1/n) / (m+1). For suciently large m, the variance of the log of this fraction will be approximately
(s1/n (1s1/n) / (m+1)) / s2/n, which simplies to (s1/n1) / (m+1). For this approximation to be
useful, the probability that none of the m + 1 points sampled from j lie in the region where pj+1 is
one, equal to (1  s1/n)m+1, must be negligible. This probability must be fairly small anyway, if LIS
is to perform well.

Suppose that the computational cost of an LIS run is proportional to the sum of the number of
If we x this cost, the
points sampled from 0 and the number of Markov transitions performed.
number of intermediate distributions, n, and the number of transitions for each distribution, m, will
be related by m(n + 1) = C, for some constant C. Assume for the moment that both n and m
are large. The probability of a run producing a zero estimate will then be negligible, and we can
assess the accuracy of the estimate for one run by the variance of log r(i)
LIS (modied in some way to
eliminate the innity resulting from the negligible, but non-zero, probability that r(i)
LIS is zero). Looking
at equation (36), we see that for these nested uniform distributions, the second log term vanishes 
pj j+1(xj+1,k) / pj+1 (xj+1,k) is always one, since pj j+1 is the same as pj+1 . When m is large, the
dependence between terms with dierent values of j will be negligible, so we can add the variances of

17

the terms to get the variance of the estimate, obtaining the result that

Var(cid:16) log r(i)

LIS(cid:17)  n (s1/n1) / (m+1)

(42)

(43)

When n is large, s1/n = exp(log(1/s)/n) is approximately 1 + log(1/s)/n, and hence the variance
above is approximately log(1/s) / (m+1). So it seems that the larger the value of m, the better 
until we reach a value of m for which the corresponding value of n, equal to C/m  1, is small enough
that this result no longer applies.

Best performance will therefore come using a fairly small value of n, but a large value of m.

Substituting m = C/(n+1) into equation (42), and assuming m/(m+1)  1, we get
LIS(cid:17)  n (s1/n1) / (C/(n+1)) = n(n+1) (s1/n1) / C

Var(cid:16) log r(i)

The value of n that minimizes this depends only on s, not on C. The optimal choice of n increases
slowly as s gets smaller: s = 0.1 gives n = 2, s = 0.05 gives n = 3, s = 0.01 gives n = 4, and
s = 0.0001 gives n = 7.

As a second example, consider the class of non-nested uniform distributions with unnormalized

densities given by

p(x) = ( 1 if t  1 < x < t + 1

0 otherwise

(44)

For this class, Z = 2 for all , so r = Z1/Z0 = 1.
I will again assume that the intermediate
distributions are dened by j = j/n, and that all Kj = m. Assuming that n is greater than t/2, the
probability that a point, x, randomly sampled from j will have pj+1(x) = 1 is 1  t/2n, for any j.
For this example, AIS estimates do not converge to the true value of r as M increases, regardless
of the value of n. To see this, note that the ratios in equation (7) will all be either zero or one, and
that the estimate from one run, r(i)
AIS, will be one if all of these ratios are one, and zero otherwise. The
probability of a particular ratio being one is 1 t/2n, so the probability that all are one (assuming the
T produce points independent of the current point) is (1  t/2n)n, which approaches exp(t/2) as n
goes to innity. The AIS estimate, averaging over M runs, will have mean exp(t/2), rather than the
correct value of one.

In contrast, bridged AIS estimates will converge to the true value as M increases, as long as n is at
least t/2, so that there is overlap between successive distributions in the sequence. However, when t
is large, the overlap between the distributions over paths produced by forward and reverse AIS runs,
given by exp(t/2), will be very small, and the procedure will be very inecient.
To see how well LIS performs, recall the formula for log rLIS from equation (36):

log r(i)

LIS =

n1

Xj=0


log


1

Kj + 1

Kj

Xk=0

pj j+1(xj,k)

pj (xj,k) 

  log


1

Kj+1 + 1

Kj+1

Xk=0

pj j+1(xj+1,k)

pj+1 (xj+1,k) 



 (45)

18

Due to symmetry, the two log terms above have the same distribution, for all j. The variance of
one of these log terms (for large m) is ((t/2n) (1 t/2n) / (m + 1)) / (1 t/2n)2, which simplies to
1 / ((2n/t1) (m+1)). The second log term in equation (36) for one j will involve the same points, xj+1,k,
as the rst log term for the next j. The eect of this is that these terms will be negatively correlated,
with correlation of 1 if n = t. However, since the two terms occur with opposite signs, the eect on the
nal sum is that n1 pairs of terms (out of 2n terms total) are positively correlated. Straightforward
calculations show that this correlation is 2n/t  1 for t/2 < n  t and 1 / (2n/t  1) for n  t. Using
the fact that when X and Y have the same distribution, Var(X + Y ) = 2 Var(X) [1 + Cor(X, Y )], we
obtain the result that, for large m,

Var(cid:16) log r(i)

LIS(cid:17) 

2

(2n/t1) (m+1)( n + (n1) (2n/t  1)
n + (n1) / (2n/t  1)

if t/2 < n  t
if n  t

)

Setting m = C/(n+1), and assuming m/(m+1)  1, gives

Var(cid:16) log r(i)

LIS(cid:17) 

2(n+1)

C(2n/t1)( n + (n1) (2n/t  1)
n + (n1) / (2n/t  1)

if t/2 < n  t
if n  t

)

(46)

(47)

Numerical investigation shows that the global minimum of the variance occurs where n is near (3/2) t.
A second local minimum where n is near (3/4) t also exists. The two minima are nearly equally good
when t is large. There is a local maximum where n is near t, with the variance there being about
19% greater than at the global minimum. The variance is much larger for very large and very small
values of n. We therefore see that for this example too, the best results are obtained by xing n to a
moderate value; any desired level of accuracy can then be obtained by increasing m and/or M .

4 Empirical comparisons of AIS and LIS

The analytical results of the previous section indicate that LIS can sometimes perform much better
than AIS, but that the benets of LIS may only be seen when the number of intermediate distributions
used is kept suitably small (but not so small that they do not overlap). In this section, I investigate
the performance of AIS and LIS (and their bridged versions) empirically. The programs used for these
tests (written in R) are available from my web page.

These tests were done using sequences of one-dimensional distributions having unnormalized density

functions of the following form:

p(x) = exp(cid:16)(cid:12)(cid:12)(cid:12)

(xt) / s(cid:12)(cid:12)(cid:12)

where s, t, and q are xed constants. As  moves from 0 to 1, the centre of this distribution shifts
by t, and changes width by the factor s. The power q controls how thick the tails of the distributions
are. When q = 2, the distributions are Gaussian; a larger value produces lighter tails. Note that Z
is proportional to s, and hence r = Z1/Z0 is equal to s.

q(cid:17)

(48)

19

.

8
0

4

.

0

0
0

.

8
0

.

4
0

.

0
.
0

.

8
0

4

.

0

0
0

.

.

8
0

4

.

0

0
0

.

2

0

2

4

6

3

2

1

0

1

2

3

3

2

1

0

1

2

3

q = 2,  t = 4,  s = 1

q = 2,  t = 0,  s = 0.05

q = 2,  t = 2,  s = 0.3

8
0

.

4
0

.

0
.
0

8
0

.

4
0

.

0
.
0

2

0

2

4

6

3

2

1

0

1

2

3

3

2

1

0

1

2

3

q = 10,  t = 4,  s = 1

q = 10,  t = 0,  s = 0.05

q = 10,  t = 2,  s = 0.3

Figure 2: The sequences of unnormalized density functions used for the tests. The plots show the
unnormalized density functions for  = 0, 1/4, 2/4, 3/4, 1, for six combinations of s, t, and q.

If t = 0, the distributions can be written in the form of equation (33), after reparameterizing in
terms of  = 1/sq, so that p(x) = exp(|x|q). In this case, we expect the asymptotic behaviour
to be as discussed in Section 3.1, but the behaviour with samples of practical size may be dierent.
As q goes to innity, the distributions converge to uniform distributions over (ts, t+s), and the
results of Section 3.2 become relevant.

I did an initial set of tests using six sequences of distributions. Three of these sequences were of
Gaussian distributions, with q = 2. The rst of these used s = 1 and t = 4, producing a shift with no
change in scale as  increases from 0 to 1. The second used s = 0.05 and t = 0, producing a contraction
with no shift. The last used s = 0.3 and t = 2, combining a shift with a contraction. A second set of
three sequences used the same values of s and t, but with q = 10, which produces more rectangular
distributions with lighter tails. The six sequences are shown in Figure 2. Each sequence in these plots
consists of ve distributions, corresponding to  = 0, 1/4, 2/4, 3/4, 1. These were the sequences used
for the LIS runs (hence n = 4 for these runs). The AIS runs used more distributions, spaced more nely
with respect to , so as to produce the same number of Markov transitions and sampling operations
as in the LIS runs.

These distributions (for any ) can easily be sampled from using rejection sampling. Samples from
0 and 1 were used to initialize forward and reverse runs of AIS and LIS. For this test, we pretend
that sampling for other  must be done using Markov chain methods. The transition used for , T,
was a random-walk Metropolis update, using a Gaussian proposal distribution with mean equal to the

20

current point and standard deviation s. Since Metropolis updates are reversible, T  was the same.

Two sets of forward and reverse LIS runs were done with n = 4, all Kj = 50, and M = 20, one
set using the geometric bridge, the other using the optimal bridge with the true value of r. The
forward estimates were computed from equation (10); the reverse estimates from equation (22), which
is equivalent to using the forward procedure with the reverse sequence of distributions. Bridged LIS
estimates were also found using equation (27), with the value of r found by iteration. To make the
comparison with forward and reverse estimates fair, the bridged LIS estimates used M = 10  ie, only
half of the forward and half of the reverse runs were used, for a total of 20 runs.

A corresponding set of forward, reverse, and bridged AIS runs were also done, with n = 250 and
M = 20 (M = 10 for the bridged estimates). If sampling a point from 0 or 1 takes about the same
computation time as a Metropolis update, these AIS runs will take about the same time as the LIS
runs. (This assumes that sampling and Markov transitions dominate the time, which is typically true
for real problems but perhaps not for this simple test problem.)

Sets of longer LIS and AIS runs were also done, which were the same as the sets above except that

for LIS, Kj = 200 for all j, and for AIS, n = 1000, which again equalizes the computation time.

Experience, together with the asymptotic results of Section 3.1, shows that estimates produced
using a small value of M are better than, or at least as good as, those produced with larger M . I
chose M = 20 (M = 10 for bridged estimates) since this is about the smallest value that allows reliable
estimation of standard errors, which would usually be needed in practice.

The standard errors for AIS and LIS estimates of r were estimated by the sample standard deviation
of the r(i) divided by M . When comparing the methods, I looked primarily at the mean squared
error when estimating log(r) (rather than when estimating r). The estimate I used was log(r), and the
standard error for this estimate was estimated by the standard error for r divided by r. For the reverse
runs, log(r) was estimated by  log(r). For bridged AIS and LIS, the standard errors for the log of
the numerator and the log of the denominator of equation (27) were found, and the overall standard
error was computed as the square root of the sum of the squares of these two standard errors. This
method of converting estimates and standard errors for r to those for log(r) is valid asymptotically.
It might be improved upon for nite samples, but such improvements would probably not aect the
relative merits of the methods compared here.

Figures 3 through 8 plot the mean squared errors of estimates for log(r) for the six sets of runs.
Results are shown for AIS, for LIS using the geometric bridge, and for LIS using the optimal bridge,
with the true value of r. Results for both the forward and reverse versions of each method are shown,
together with the bridged version, using the optimal bridge, with r obtained by iteration. Results
for the short runs (n = 4, Kj = 50 for LIS, n = 250 for AIS) are on the left, and for the long runs
(n = 4, Kj = 200 for LIS, n = 2000 for AIS) on the right. The mean squared error for each method was
estimated by simulating each method 2000 times, and comparing the estimates with the true value of
log(r). The bars in the plots are dark up to the estimated mean squared error minus twice its standard

21

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

7
0
0

.

6
0
0

.

5
0
0

.

4
0

.

0

3
0
0

.

2
0
.
0

1
0
.
0

0
0
.
0

for

rev bri

for

rev bri

for

rev bri

2
1
0
0

.

0
1
0
0

.

8
0
0
0

.

6
0
0

.

0

4
0
0
0

.

2
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 3: Results of short and long runs on the distribution sequence with s = 1, t = 4, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

r
o
r
r

E

d
e
r
a
u
q
S
n
a
e
M



5
1
.
0

0
1
.
0

5
0

.

0

0
0

.

0

0.26 0.29

for

rev bri

for

rev bri

for

rev bri

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0.04 0.04

for

rev bri

for

rev bri

for

rev bri

Figure 4: Results of short and long runs on the distribution sequence with s = 1, t = 4, and q = 10.

22

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

5
2
0
0

.

0
2
0
0

.

5
1
0

.

0

0
1
0

.

0

5
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

5
0
0
0

.

4
0
0
0

.

3
0
0

.

0

2
0
0
0

.

1
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 5: Results of short and long runs on the distribution sequence with s = 0.05, t = 0, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

AIS

0.13

for

rev bri

for

rev bri
Short Runs

for

rev bri

Figure 6: Results of short and long runs on the distribution sequence with s = 0.05, t = 0, and q = 10.

23

0
1
.
0

8
0
.
0

6
0
.
0

4
0

.

0

2
0

.

0

0
0

.

0

r
o
r
r

E

d
e
r
a
u
q
S
n
a
e
M



5

0
2
0
.
0

5
1
0
.
0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

for

rev bri

for

rev bri
Long Runs

for

rev bri

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

7
0

.

0

6
0

.

0

5
0

.

0

4
0
0

.

3
0
0

.

2
0
.
0

1
0
.
0

0
0
.
0

for

rev bri

for

rev bri

for

rev bri

2
1
0
0

.

0
1
0
0

.

8
0
0
0

.

6
0
0

.

0

4
0
0
0

.

2
0
0
.
0

0
0
0
.
0

for

rev bri

for

rev bri

for

rev bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 7: Results of short and long runs on the distribution sequence with s = 0.3, t = 2, and q = 2.

AIS: n=250,  LIS: n=4 m=50

AIS: n=1000,  LIS: n=4 m=200

AIS

LISgeometric

LISoptimal

0.24 0.37

for

rev bri

for

rev bri

for

rev bri

r
o
r
r

E

d
e
r
a
u
q
S
n
a
e
M



0
2
.
0

5
1
.
0

0
1

.

0

5
0

.

0

0
0

.

0

6
0
.
0

5
0
.
0

4
0
.
0

3
0

.

0

2
0

.

0

1
0

.

0

0
0

.

0

AIS

LISgeometric

LISoptimal

for

rev bri

for

rev bri

for

rev bri

Figure 8: Results of short and long runs on the distribution sequence with s = 0.3, t = 2, and q = 10.

24

error, and are then light up to the estimated mean squared error plus twice its standard error. For
bars that extend above the plot the estimated mean squared error is shown at the top of the bar.

The results for translated sequences of distributions (t = 4 and s = 1) are shown in Figures 3 and 4.
When the distributions are Gaussian (q = 2), no advantage is seen for LIS  if anything, LIS performs
slightly worse than AIS, particularly when the geometric bridge is used. The forward and reverse forms
of AIS and LIS should have identical performance for these distribution sequences, due to symmetry;
any dierences seen result from random variation. The bridged forms of both AIS and LIS perform
better than the unbridged forward and reverse forms. The advantage of bridging is less for the longer
runs, however, as expected from the analysis at the end of Section 3.1.

When q = 10, the distributions have much lighter tails than the Gaussian, more closely resembling
the uniform distributions analysed in Section 3.2. For these sequences of distributions, LIS performs
substantially better than AIS. The unbridged version of AIS does particularly badly. The mean
squared error for the bridged version of AIS is about 2.5 times greater than for the bridged version of
LIS. It makes little dierence whether the geometric or optimal bridge is used for LIS.

Figures 5 and 6 show the results for sequences of distributions with the same mean (t = 0) but
decreasing width (s = 0.05). For these sequences, a modest advantage of LIS over AIS is apparent
for the sequence of Gaussian distributions (q = 2), with the variance for AIS estimates being about a
factor of 1.3 greater than for LIS estimates with the geometric bridge, and about a factor of 1.7 greater
than for LIS estimates with the optimal bridge. The reversed AIS and LIS estimates are somewhat
worse than the forward estimates for this sequence of distributions. No advantage is seen for bridged
AIS or LIS estimates.

The results for the sequence of distributions with q = 10 is similar, except that the advantage of LIS

over AIS is much greater  about a factor of 6.

Results for the last type of sequence, with s = 0.3 and t = 2, are shown in Figures 7 and 8. This
problem is a hybrid of the previous two, with both translation and change in width, producing results
intermediate between those for the previous two problems. No dierence in performance between AIS
and LIS is apparent for the Gaussian distributions (q = 2), but the bridged forms of both perform
slightly better. For the sequence of distributions with q = 10, a clear advantage of LIS over AIS can
be seen, but this advantage is not as great as for the sequence with t = 0 and s = 0.05. The bridged
forms of both AIS and LIS are again better, more so for the short runs than for the long runs.

In addition to looking at the mean squared error of estimates found with these methods, I also
looked at the fraction of times that the estimate for log(r) diered from the true value by more
than twice the standard error estimated using the M runs. This should be approximately 5% if the
distribution of estimates is Gaussian, and the standard errors are accurate. For the longer runs, this
fraction was indeed near or only slightly above 5% for all methods, except for the unbridged AIS runs
when these performed very poorly. For the shorter runs, however, the unbridged AIS and LIS methods
produced estimates more than two standard errors from the mean around 10% of the time (sometimes

25

much more often, when unbridged AIS performed poorly). Both the bridged AIS and the bridged LIS
methods gave more reliable standard errors. However, it is possible that better standard errors for
the unbridged methods might be obtained with a more sophisticated approach than I used.

I performed additional runs to verify and extend some of the analytic results from Section 3.
Figures 9 and 10 show results obtained using LIS with increasing numbers of intermediate distributions,
starting with the value of n = 4 used for the tests above, and continuing to n = 9, n = 19, and
n = 39, while keeping the computation time constant by decreasing m in proportion to n+1. The two
distribution sequences with s = 1 and t = 4 and with s = 0.05 and t = 0 were used, in both cases with
q = 10. The sequence with t = 0 and s = 0.05 has the form of equation (33), so in accordance with the
analysis of Section 3.1, we expect that asymptotically, as n increases, LIS and AIS should have the
same performance. This is indeed what we see in Figure 9. We also see the same behaviour for the
sequence with t = 4 and s = 1 in Figure 10.

As q increases, the distributions become close to uniform, and the results of Section 3.2 should
apply. To test this, I tried values of q = 2, q = 10, q = 20, and q = 30 for the distribution sequence with
s = 1 and t = 4 and the sequence with s = 0.05 and t = 0. Results are shown in Figures 11 and 12. (The
results for q = 2 and q = 10 are the same as on the left in Figures 3 to 6, though the scale diers.)

For the sequences with s = 1 and t = 4, the limiting uniform distributions have the form of the
second example in Section 3.2. As noted there, AIS estimates do not converge to the correct value of
r for this distribution sequence; bridged AIS estimates do converge, but may be rather inecient. We
see analogous behaviour in Figure 11 when q is large. The mean squared error of the AIS estimates
increases approximately linearly with q over the range q = 10 to q = 30. The bridged AIS estimates also
get worse as q increases, but more slowly. In contrast, the mean squared error of the LIS estimates
changes hardly at all as q increases.

The story is similar for sequences with s = 0.05 and t = 1, for which the limiting uniform distributions
correspond to those in the rst example of Section 3.2. The LIS estimates perform about equally well
for all values of q, but the AIS estimates are dramatically worse for large values of q. For this sequence,
reverse AIS estimates are much worse than forward AIS estimates, and bridging does not help.

According to the analysis of Section 3.1, the choice of choice of n = 4 for LIS used above is not
optimal for either of these distribution sequences when q is large. For the sequence with s = 1 and
t = 4, using n = 6 should be better by a factor of 1.176. However, in LIS runs with q = 30, the mean
squared error using n == 4 and m = 200 is indistinguishable from that using n = 6 and m = 143,
given the standard errors (a factor of 1.09 or more should have been detectable). Of course, q = 30
does not give exactly uniform distributions, and these values of m may not be large enough for the
asymptotic results to apply, especially since the Markov transitions do not sample independently. For
the sequence with s = 0.05 and t = 0, the results in Section 3.1 indicate that using n = 3 should be
better by a factor of 1.084. In this case, LIS runs with q = 30 using n = 3 and m = 250 are better than
runs using n = 4 and m = 200 by a factor of 1.16, signicantly greater than one given the standard
errors, but not signicantly dierent from the expected ratio of 1.084.

26

AIS: n=1000,  LIS: n=4 m=200
LISoptimal

LISgeometric

AIS

4
0

.

0

3
0

.

0

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



2
0

.

0

1
0

.

0

0
0

.

0

4
0

.

0

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=19 m=50
LISoptimal

LISgeometric

AIS

3
0

.

0

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



2
0

.

0

1
0
0

.

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=9 m=100
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=39 m=25
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

4
0

.

0

3
0

.

0

2
0

.

0

1
0

.

0

0
0

.

0

4
0

.

0

3
0

.

0

2
0

.

0

1
0
0

.

0
0
0

.

Figure 9: Results using increasing values of n for LIS, while keeping computation time constant, for
the distribution sequence with s = 1, t = 4, and q = 10. The same AIS procedure was used for all plots,
but results vary randomly.

27

AIS: n=1000,  LIS: n=4 m=200
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=19 m=50
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0
0

.

0
0
0
0

.

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0

.

0

0
0
0

.

0

0
2
0

.

0

5
1
0

.

0

0
1
0

.

0

5
0
0
0

.

0
0
0
0

.

AIS: n=1000,  LIS: n=9 m=100
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

AIS: n=1000,  LIS: n=39 m=25
LISoptimal

LISgeometric

AIS

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 10: Results using increasing values of n for LIS, while keeping computation time constant, for
the distribution sequence with s = 0.05, t = 0, and q = 10. The same AIS procedure was used for all
plots, but results vary randomly.

28

q=2

q=10

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

7

.

0

6

.

0

5

.

0

4

.

0

3
.

0

2

.

0

1

.

0

0

.

0

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
0

.

0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=20

AIS

LISgeometric

LISoptimal

for

rev

bri

for

rev

bri

for

rev

bri

7

.

0

6

.

0

5

.

0

4

.

0

3
.

0

2

.

0

1

.

0

0

.

0

7

.

0

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1
0

.

0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=30

AIS

LISgeometric

LISoptimal

0.94 0.95

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 11: Results with increasing values of q, for sequences of distributions with s = 1 and t = 4. The
AIS runs used n = 250; the LIS runs used n = 4 and m = 50, requiring the same amount of computation.

29

q=2

q=10

AIS

LISgeometric

LISoptimal

AIS

LISgeometric

LISoptimal

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0
0

.

0
0

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=20

LISgeometric

LISoptimal

AIS

0.34

for

rev

bri

for

rev

bri

for

rev

bri

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0
0

.

0
0

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0
0
0

.

for

rev

bri

for

rev

bri

for

rev

bri

q=30

LISgeometric

LISoptimal

AIS

0.59

for

rev

bri

for

rev

bri

for

rev

bri

r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



r
o
r
r



E
d
e
r
a
u
q
S
n
a
e
M



Figure 12: Results with increasing values of q, for sequences of distributions with s = 0.05 and t = 1.
The AIS runs used n = 250; the LIS runs used n = 4 and m = 50, requiring the same amount of
computation.

30

5 Other applications of linked sampling

So far in this paper, I have focused on how Linked Importance Sampling can be used to estimate ratios
of normalizing constants. LIS can also be used to estimate expectations with respect to 1, however,
and in some applications, this may be its most important use. Linked sampling methods related to
LIS can also be applied in other ways. I briey described these other applications here, outlining the
use of linked sampling for dragging fast variables in some detail.

5.1 Estimating expectations

The expectation of some function, a(x), with respect to 1 can be estimated using simple importance
sampling, with points drawn from 0, as follows:

E1[a(X)] = E0(cid:20)a(X)

p1(X)

p0(X)(cid:21) . Z1

Z0 

1
N

N

Xi=1

a(x(i))

p1(x(i))

p0(x(i)) . 1

N

p1(x(i))
p0(x(i))

N

Xi=1

(49)

where x(i), . . . , x(N ) are drawn from 0. Like equation (2), this estimate is valid only if no region
having zero probability under 0 has non-zero probability under 1. The two factors of 1/N of course
cancel, but are included to emphasize the connection with the estimate for r = Z1/Z0, which is simply
the denominator of the estimate above.

Since LIS can be viewed as simple importance sampling on an extended state space, with distribu-
tions 0 and 1 dened by the forward and reverse procedures of Section 2, we can use equation (49)
to estimate any quantity that can be expressed as an expectation with respect ot 1. Step (1) of the
reverse procedure dening 1 sets xn,n to a value randomly chosen from n = 1. Step (2) then
sets the other xn,k to values obtained from xn,n by applying Markov chain transitions that leave 1
invariant. It follows that under 1, all the points xn,k have marginal distribution 1 (though they
may not be independent). Accordingly,

E1[a(X)] = E 1"

1

Kn +1

a(Xn,k)#

Kn

Xk=0

(50)

Estimating the right side as in equation (49), and using the fact that the ratio of probabilities under
1 over those under 0 is given by r(i)

LIS in equation (10), we get the estimate

E1[a(X)] 

LIS

r(i)
Kn +1

M

Xi=1

Kn

Xk=0

a(x(i)

n,k) .

M

Xi=1

r(i)

LIS

(51)

If the M runs of LIS are started by sampling independently from 0 (as will often be possible), the
standard error of this estimate can be assessed in the usual fashion for importance sampling, as I have
discussed for the analogous AIS estimates in (Neal 2001). This error assessment can be dicult, since
when some r(i)
LIS is hard to estimate. Note, however,
that the degree to which the Markov chain transitions used have converged need not be assessed, a

LIS are much larger than others, the variance of r(i)

31

possible advantage compared with simple MCMC estimates. The estimate of equation (51) will be
asymptotically correct (as M  ) regardless of how far these Markov chain transitions are from
convergence.

The primary reason one might wish to use LIS to estimate expectations is that going through
the sequence of distributions parameterized by 0, . . . , n may produce an annealing eect, which
prevents the Markov chain sampler from being trapped in a local mode of the distribution. Compared
with the analogous AIS procedure, LIS may perform better for some forms of distributions, for the
same reasons as were discussed in Sections 3 and 4. One should also note that LIS estimates for
expectations with respect to j for all j can easily be obtained from a single set of runs, by simply
considering the results of each LIS run up to the point where the sample for j is obtained.

5.2 A linked form of tempered transitions

My tempered transition method (Neal 1996) is another approach to sampling from distributions
with isolated modes, between which movement is dicult for Markov chain transitions such as simple
Metropolis updates.
In this approach, such simple Markov chain transitions are supplemented by
occasional complex tempered transitions, composed of many simple Markov chain transitions. A
tempered transition consists of several stages, which proceed through a sequence of distributions, from
the distribution being sampled, to a higher temperature distribution in which movement between
modes is easier, and then back down to the distribution being sampled. At each stage of a tempered
transition, we generate a single new state by applying a Markov chain transition to the current state,
after which we switch to the next distribution in the sequence. The second half of a tempered transition
is similar to an Annealed Importance Sampling run, while the rst half is similar to an AIS run with
the reversed sequence of distributions.

A similar linked procedure can be dened, in which at each stage we generate a chain of states by
applying a Markov chain transition. We then select a link state from this sequence (using a suitable
bridge distribution) which serves as the starting point for the chain of states generated in the next
stage. In the nal stage, a chain of states is produced using a Markov chain transition that leaves the
distribution being sampled invariant, and a candidate state is selected uniformly at random from this
chain. The appropriate probability for accepting this candidate state is computed using ratios similar
to those going into the LIS estimate of equation (10).

As discussed in Section 4, for AIS to work well, all distributions in the sequence must assign
reasonably high probability to regions of the space that have non-negligible probability under the next
distribution in the sequence. One would expect tempered transitions to work well only when this
holds for both the sequence and its reversal.
In contrast, one would expect the linked version of
tempered transitions to work well as long as the sequence satises the weaker condition that there be
some overlap between adjacent distributions (assuming a suitable bridge distribution is used).

32

5.3 Dragging fast variables using linked chains

A slight modication of the tempered transition method can be applied to problems in which the state
is composed of both fast and slow variables. We will write the distribution of interest for such a
problem as

(x, y) = (1/Z) exp(U (x, y))

(52)

where x denotes the fast variables and y the slow variables. We assume that the computation
is dominated by the time required to evaluate U (x, y), but that once U (x, y) has been evaluated,
with relevant intermediate quantities saved, evaluating U (x, y) for any new x is much faster than
evaluating U (x, y) for some y not previously encountered. One example of such a problem is inference
for Gaussian process classication models (Neal 1999), in which y consists of the hyperparameters
dening the covariance function used, and x consists of the latent variables associated with the n
observations. After a change to y, we must recompute the Cholesky decomposition of an n  n
covariance matrix, which takes time proportional to n3, whereas after a change to x only, U (x, y) can
be re-computed in time proportional to n2, assuming the Cholesky decomposition for this value of y
has been saved.

In my method for dragging fast variables (Neal 2004), the ability to quickly re-evaluate U (x, y)
when only x changes is exploited to allow larger changes to be made to y than would be possible
if x were kept xed, or were given a new value from some simple proposal distribution. From the
state (x0, y0), a dragging update proposes a new value y1, drawn from some symmetrical proposal
distribution, in conjunction with a new value x1 that is found by applying a succession of Markov
chain updates that leave invariant distributions in the series, j (x), for j = 1, . . . , n 1, with 0 <
j < j+1 < 1. The proposed state, (x1, y1), is then accepted or rejected in a fashion analogous to
tempered transitions.

The distributions in the sequence used are dened by the following unnormalized probability or

density function, which depends on the current and proposed values for y:

p(x) = exp ( ((1) U (x, y0) +  U (x, y1)))

(53)

The corresponding normalized probability or density function will be written as . Note that 0(x) =
(x|y0) and 1(x) = (x|y1). Crucially, after U (x, y0) and U (x, y1) have been evaluated once (for
any x), we can evaluate p(x) for any  and any x without any further slow computations. Indeed,
since U (x0, y0) will usually have already been evaluated as part of the previous Markov chain transition,
only one slow computation will be required to evaluate p(x) for any number of values of  and x.

A linked dragging update can be dened as follows. Given the sequence of distributions dened
by 0, . . . , n, with 0 = 0 and n = 1, the numbers of transitions (T or T ) to perform for each
distribution over x, denoted by K0, . . . , Kn, and a set of bridge distributions, denoted by pj j+1, for
j = 0, . . . , n1, an update from the current state (x0, y0) is done as follows:

33

The Linked Dragging Procedure

1) Propose a new value, y1, from some proposal distribution S(y1|y0), which satises the symmetry

condition that S(y1|y0) = S(y0|y1).

2) Pick an integer 0 uniformly at random from {0, . . . , K0}, and then set x0,0 to the current values

of the fast variables, x0.

3) For j = 0, . . . , n, create a chain of values for x associated with j as follows:

a) If j > 0: Pick an integer j uniformly at random from {0, . . . , Kj}, and then set xj,j to

xj1j.

b) For k = j + 1, . . . , Kj, draw xj,k according to the forward Markov chain transition prob-

abilities Tj (xj,k1, xj,k). (If j = Kj, do nothing in this step.)

c) For k = j  1, . . . , 0, draw xj,k according to the reverse Markov chain transition probabil-

ities T j (xj,k+1, xj,k). (If j = 0, do nothing in this step.)

d) If j < n: Pick a value for j from {0, . . . , Kj} according to the following probabilities

0(j | xj) =

pj j+1(xj,j )

pj (xj,j ) .

Kj

Xk=0

pj j+1(xj,k)

pj (xj,k)

(54)

and then set xj j+1 to xj,j .

3) Set n to a value chosen uniformly at random from {0, . . . , Kn}, and let the proposed new values

for the fast variables, x1, be equal to xn,n.

4) Accept (x1, y1) as the new state with probability

min


1,

n1

Yj=0




1

Kj + 1

Kj

Xk=0

pj j+1(xj,k)

pj (xj,k) .

1

Kj+1 + 1

Kj+1

Xk=0

pj j+1(xj+1,k)

pj+1(xj+1,k) 


(55)




If (x1, y1) is not accepted, the new state is the same as the old state, (x0, y0).

One can show that this update leaves (x, y) invariant by showing that it satises detailed balance,
which in turns follows from the stronger property that the probability of starting at (x0, y0), assuming
this start state comes from (x, y), then generating the various quantities produced by the above
procedure, and nally accepting (x1, y1) as the new state, is the same as the probability of starting
this procedure at (x1, y1), generating the same quantities in reverse, and nally accepting (x0, y0).
The proof of this is analogous to the derivation of LIS in Section 2.

To use the linked dragging procedure, we need to select suitable bridge distributions. Since the
characteristics of (x) will depend on y0 and y1, and of course , we may not know enough to select
good estimates for the values of r needed to use the optimal bridge of equation (6), though we might

34

try just setting r to one. This is not a problem for the geometric bridge of equation (5), for which the
acceptance probability above can be written as

1

Kj + 1

Kj

Xk=0 s pj+1(xj,k)
pj (xj,k) .

1

Kj+1 + 1

Kj+1

pj+1 (xj+1,k)
Xk=0 s pj (xj+1,k)





From equation (53), we see that

min


1,

n1

Yj=0




pj+1(xj,k)
pj (xj,k)

pj (xj+1,k)
pj+1(xj+1,k)

= exp ( (j+1j) (U (xj,k, y1)U (xj,k, y0)))

= exp ( (j+1j) (U (xj+1,k, y0)U (xj+1,k, y1)))

(56)

(57)

(58)

For the simplest case with no intermediate distributions (ie, with n = 1), the acceptance probability
simplies to

min

1,




1

K0 + 1

1

K1 + 1

K0

K1

Xk=0
Xk=0

exp ( (U (xj,k, y1)U (xj,k, y0)) / 2)

exp ( (U (xj,k, y0)U (xj,k, y1)) / 2)




(59)

6 Conclusions and Future work

In this paper, I have demonstrated that in some situations Linked Importance Sampling is substan-
tially more ecient than Annealed Importance Sampling, provided a suitable number of intermediate
distributions are used. However, in other situations, where the tails of the distributions involved are
suciently heavy, the two methods are about equally ecient. More research is therefore needed to
determine for which problems of practical interest LIS, and related linked sampling methods, will be
useful.

In tests on multivariate Gaussian distributions, I have not seen an advantage for LIS over AIS. Both
perform about equally well on a sequence of 100-dimensional spherical Gaussian distributions with
variances changing by a factor of two, so that log(r) = 100. This is in accord with the results in
Section 4, where LIS had little or no advantage over AIS when the distributions were Gaussian. LIS
is more likely to be useful for problems involving continuous distributions with lighter tails.

One problem that may benet from LIS is that of computing the probability of a very rare event,
which can be cast as computing the normalizing constant for a distribution with the constraint that
the state be in the set corresponding to this event. Intermediate distributions might use looser forms
of this constraint. If, in all these distributions, states violating the constraints have zero probability,
AIS will tend to have the same bad behaviour seen with uniform distributions in Section 3.2, while
LIS may work much better.

35

Another context where LIS may outperform AIS is when only a xed number of intermediate
distributions are available  ie, only a nite number of values are allowed for . This is the situation
for the sequential importance sampler of MacEachern, Clyde, and Liu (1999), which can be seen as
an instance of AIS (Neal 2001). Here, the intermediate distributions use only a fraction of the n items
in the data set; such a fraction can only have the form j/n with j an integer. The distance between
successive distributions for this problem may sometimes be too great for AIS to work well, but their
overlap might nevertheless be sucient for LIS.

It may be possible to improve LIS by reducing the variance in how well it samples at each stage.
Instead of performing a predetermined number, Kj, of Markov transitions at stage j, we might instead
perform as many transitions as are necessary to obtain a good sample. Dene a tour to be a
sequence of transitions that moves from a high value of some key quantity (eg, U (x) for the canonical
distributions of equation (1)) to a low value of this quantity, or vice versa. Good sampling might
be ensured by performing some predetermined number of tours, with the number of these tours that
occur before and after the link state being chosen at random. Suitable high and low values would
probably need to be found using preliminary runs.

More speculatively, it seems as if there should be some method that has the advantages of LIS
over AIS, but that like AIS uses many intermediate distributions, performing only a single Markov
transition for each. Intuitively, it seems that such a smooth method that does not abruptly change
 should be more ecient. One can use LIS with all Kj set to one, but this will produce good results
only if n is large, which we saw in the analysis of Section 3.1 does not lead to an advantage over
AIS. Perhaps some way could be found of using states associated with all values of  when estimating
each of the ratios Zj+1 /Zj , while still producing an estimate that is exactly unbiased even when the
Markov transitions do not reach equilibrium.

Acknowledgements

This research was supported by the Natural Sciences and Engineering Research Council of Canada. I
hold a Canada Research Chair in Statistics and Machine Learning.

