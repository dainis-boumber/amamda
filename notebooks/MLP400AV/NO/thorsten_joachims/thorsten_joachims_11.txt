Abstract

Learning general functional dependencies is
one of the main goals in machine learning.
Recent progress in kernel-based methods has
focused on designing (cid:176)exible and powerful in-
put representations. This paper addresses
the complementary issue of problems involv-
ing complex outputs such as multiple depen-
dent output variables and structured output
spaces. We propose to generalize multiclass
Support Vector Machine learning in a formu-
lation that involves features extracted jointly
from inputs and outputs. The resulting op-
timization problem is solved eciently by
a cutting plane algorithm that exploits the
sparseness and structural decomposition of
the problem. We demonstrate the versatility
and eectiveness of our method on problems
ranging from supervised grammar learning
and named-entity recognition, to taxonomic
text classication and sequence alignment.

1. Introduction

This paper deals with the general problem of learn-
ing a mapping from inputs x 2 X to discrete outputs
y 2 Y based on a training sample of input-output pairs
(x1; y1); : : : ; (xn; yn) 2 X  Y drawn from some xed
but unknown probability distribution. Unlike the case
of multiclass classication where Y = f1; :::; kg with
interchangeable, arbitrarily numbered labels, we con-
sider structured output spaces Y. Elements y 2 Y
may be, for instance, sequences, strings, labeled trees,

Appearing in Proceedings of the 21 st International Confer-
ence on Machine Learning, Ban, Canada, 2004. Copyright
by the authors.

lattices, or graphs. Such problems arise in a variety of
applications, ranging from multilabel classication and
classication with class taxonomies, to label sequence
learning, sequence alignment learning, and supervised
grammar learning, to name just a few.

We approach these problems by generalizing large
margin methods, more specically multi-class Support
Vector Machines (SVMs) (Weston & Watkins, 1998;
Crammer & Singer, 2001), to the broader problem of
learning structured responses. The naive approach of
treating each structure as a separate class is often in-
tractable, since it leads to a multiclass problem with a
very large number of classes. We overcome this prob-
lem by specifying discriminant functions that exploit
the structure and dependencies within Y. In that re-
spect, our approach follows the work of Collins (2002;
2004) on perceptron learning with a similar class of
discriminant functions. However, the maximum mar-
gin algorithm we propose has advantages in terms of
accuracy and tunability to specic loss functions. A
similar philosophy of using kernel methods for learning
general dependencies was pursued in Kernel Depen-
dency Estimation (KDE) (Weston et al., 2003). Yet,
the use of separate kernels for inputs and outputs and
the use of kernel PCA with standard regression tech-
niques signicantly diers from our formulation, which
is a more straightforward and natural generalization of
multiclass SVMs.

2. Discriminants and Loss Functions

We are interested in the general problem of learning
functions f : X ! Y based on a training sample of
input-output pairs. As an illustrating example, con-
sider the case of natural language parsing, where the
function f maps a given sentence x to a parse tree

should be treated dierently from a parse tree that
is radically dierent. Typically, the correctness of a
predicted parse tree is measured by its F1 score (see
e.g. Johnson (1999)), the harmonic mean of precision
of recall as calculated based on the overlap of nodes
between the trees. We thus assume the availability of
a bounded loss function 4 : Y Y ! < where 4(y; ^y)
quanties the loss associated with a prediction ^y, if the
true output value is y. If P (x; y) denotes the data gen-
erating distribution, then the goal is to nd a function
f within a given hypothesis class such that the risk

P (f ) =ZX Y 4(y; f (x)) dP (x; y) :
R4

(3)

is minimized. We assume that P is unknown, but that
a nite training set of pairs S = f(xi; yi) 2 X Y : i =
1; : : : ; ng generated i.i.d. according to P is given. The
performance of a function f on the training sample
S is described by the empirical risk R4
S (f ). For w-
parameterized hypothesis classes, we will also write
R4
P (w)  R4
P (f (; w)) and similarly for the empirical
risk.

3. Margins and Margin Maximization

First, we consider the separable case in which there
exists a function f parameterized by w such that the
empirical risk is zero. If we assume that 4(y; y0) > 0
for y 6= y0 and 4(y; y) = 0, then the condition of zero
training error can then be compactly written as a set
of non-linear constraints

8i :

y2Ynyifhw; (xi; y)ig < hw; (xi; yi)i :
max

(4)

Each nonlinear inequality in (4) can be equivalently
replaced by jYj  1 linear inequalities, resulting in a
total of njYj  n linear constraints,

8i; 8y 2 Y n yi : hw; i(y)i > 0 ;

(5)

where we have dened the shorthand i(y) 
(xi; yi)  (xi; y).
If the set of inequalities in (5) is feasible, there will
typically be more than one solution w. To specify
a unique solution, we propose to select the w with
kwk  1 for which the score of the correct label yi
is uniformly most dierent from the closest runner-
up ^yi(w) = argmaxy6=yihw; (xi; y)i. This general-
izes the maximum-margin principle employed in SVMs
(Vapnik, 1998) to the more general case considered in
this paper. The resulting hard-margin optimization

Figure 1. Illustration of natural language parsing model.

y. This is depicted graphically in Figure 1. The ap-
proach we pursue is to learn a discriminant function
F : X  Y ! < over input/output pairs from which
we can derive a prediction by maximizing F over the
response variable for a specic given input x. Hence,
the general form of our hypotheses f is

f (x; w) = argmax

F (x; y; w) ;

(1)

y2Y

where w denotes a parameter vector. It might be use-
ful to think of F as a w-parameterized family of cost
functions, which we try to design in such a way that
the minimum of F (x;; w) is at the desired output y
for inputs x of interest. Throughout this paper, we
assume F to be linear in some combined feature repre-
sentation of inputs and outputs (x; y),

F (x; y; w) = hw; (x; y)i :

(2)

The specic form of  depends on the nature of the
problem and special cases will be discussed subse-
quently.

Using again natural language parsing as an illustrative
example, we can chose F such that we get a model that
is isomorphic to a Probabilistic Context Free Grammar
(PCFG). Each node in a parse tree y for a sentence
x corresponds to grammar rule gj, which in turn has
a score wj. All valid parse trees y (i.e. trees with a
designated start symbol S as the root and the words in
the sentence x as the leaves) for a sentence x are scored
by the sum of the wj of their nodes. This score can
thus be written as F (x; y; w) = hw; (x; y)i, where
(x; y) is a histogram vector counting how often each
grammar rule gj occurs in the tree y. f (x; w) can
be eciently computed by nding the structure y 2 Y
that maximizes F (x; y; w) via the CKY algorithm (see
Manning and Schuetze (1999)).
Learning over structured output spaces Y inevitably
involves loss functions other than the standard zero-
one classication loss (cf. Weston et al. (2003)). For
example,
in natural language parsing, a parse tree
that diers from the correct parse in a few nodes only

problem is

SVM0 : min

w

1
2kwk2

8i; 8y 2 Y n yi : hw; i(y)i  1 :

(6a)

(6b)

To allow errors in the training set, we introduce slack
variables and propose to optimize a soft-margin crite-
rion. While there are several ways of doing this, we
follow Crammer and Singer (2001) and introduce one
slack variable for every non-linear constraint (4), which
will result in an upper bound on the empirical risk and
oers some additional algorithmic advantages. Adding
a penalty term that is linear in the slack variables to
the objective results in the quadratic program

n

Xi=1

1
2kwk2 +

C
n

i; s.t. 8i; i  0 (7a)
SVM1 : min
w; 
8i; 8y 2 Y n yi : hw; i(y)i  1  i : (7b)
Alternatively, we can also penalize margin violations
by a quadratic term C
leading to an analogue
optimization problem which we refer to as SVM2 . In
both cases, C > 0 is a constant that controls the trade-
o between training error minimization and margin
maximization.

2nPi 2

i

SVM1 implicitly considers the zero-one classication
loss. As argued above, this is inappropriate for prob-
lems like natural language parsing, where jYj is large.
We now propose two approaches that generalize the
above formulations to the case of arbitrary loss func-
tions 4. Our rst approach is to re-scale the slack vari-
ables according to the loss incurred in each of the linear
constraints. Intuitively, violating a margin constraint
involving a y 6= yi with high loss 4(yi; y) should be
penalized more severely than a violation involving an
output value with smaller loss. This can be accom-
plished by multiplying the violation by the loss, or
equivalently, by scaling slack variables with the inverse
loss, which yields the problem

SVM4s

1

: min
w; 

1
2kwk2 +

C
n

n

Xi=1

i; s.t. 8i; i  0

(8)

8i;8y2Ynyi : hw; i(y)i 1

i

4(yi;y)

: (9)

A justication for this formulation is given by the sub-
sequent proposition (proof omitted).
Proposition 1. Denote by (w; ) the optimal solu-
tion to SVM4s
. Then 1
is an upper bound
on the empirical risk R4
The optimization problem SVM4s

nPn

can be derived

S (w).

i=1 

1

i

analogously, where 4(yi; y) is replaced by p4(yi; y)

2

in order to obtain an upper bound on the empirical
risk.

A second way to include loss functions is to re-scale
the margin as proposed by Taskar et al.
(2004) for
the special case of the Hamming loss. The margin
constraints in this setting take the following form:

8i; 8y 2 Y n yi : hw; i(y)i  4(yi; y)  i

(10)

This set of constraints yield an optimization prob-
lem SVM4m
1 which also results in an upper bound on
R4
S (w). In our opinion, a potential disadvantage of
the margin scaling approach is that it may give signif-
icant weight to output values y 2 Y that are not even
close to being confusable with the target values yi, be-
cause every increase in the loss increases the required
margin.

4. Support Vector Machine Learning

The key challenge in solving the QPs for the gener-
alized SVM learning is the large number of margin
constraints; more specically the total number of con-
straints is njYj. In many cases, jYj may be extremely
large, in particular, if Y is a product space of some
sort (e.g. in grammar learning, label sequence learn-
ing, etc.). This makes standard quadratic program-
ming solvers unsuitable for this type of problem.

In the following, we propose an algorithm that exploits
the special structure of the maximum-margin problem,
so that only a much smaller subset of constraints needs
to be explicitly examined. The algorithm is a general-
ization of the SVM algorithm for label sequence learn-
ing (Hofmann et al., 2002; Altun et al., 2003) and the
algorithm for inverse sequence alignment (Joachims,
2003). We will show how to compute arbitrarily close
approximations to all of the above SVM optimization
problems in polynomial time for a large range of struc-
tures and loss functions. Since the algorithm operates
on the dual program, we will rst derive the Wolfe dual
for the various soft margin formulations.

4.1. Dual Programs

We will denote by iy the Lagrange multiplier enforc-
ing the margin constraint for label y 6= yi and exam-
ple (xi; yi). Using standard Lagrangian duality tech-
niques, one arrives at the following dual QP for the
hard margin case SVM0

1

max

iy

2 Xi;y6=yi
 Xi;y6=yi
s.t. 8i;8y 6= Y n yi :

j; y6=yj

iyj y hi(y); j(y)i (11a)

iy  0 :

(11b)

A kernel K((x; y); (x0; y0)) can be used to replace the
inner products, since inner products in  can be
easily expressed as inner products of the original -
vectors.

For soft-margin optimization with slack re-scaling and
linear penalties (SVM4s
), additional box constraints

1

n Xy6=yi

iy

4(yi; y)  C; 8i

(12)

are added to the dual.
Quadratic slack penal-
ties (SVM2 ) lead to the same dual as SVM0 after
altering the inner product to hi(y); j(y)i +
ij

. ij = 1, if i = j, else 0.

n

Cp4(yi;y)p4(yj ;y)

Finally, in the case of margin re-scaling, the loss func-
tion aects the linear part of the objective function

part Q is unchanged from (11a)) and introduces stan-

maxPi;y iy4(yi; y)  Q() (where the quadratic
dard box constraints nPy6=yi

iy  C.

4.2. Algorithm

1 and SVM4s

Algorithm 1 Algorithm for solving SVM0 and the loss
re-scaling formulations SVM4s
1: Input: (x1; y1); : : : ; (xn; yn), C, 
2: Si  ; for all i = 1; : : : ; n
3: repeat
4:
5:

for i = 1; : : : ; n do

2

2

1

set up cost function
SVM4s
SVM4s
SVM4m
SVM4m

: H(y)  (1  hi(y); wi)4(yi; y)
: H(y)  (1hi(y); wi)p4(yi; y)
: H(y)  4(yi; y)  hi(y); wi
: H(y) p4(yi; y)  hi(y); wi

jy

1

2

02Sj

0 j(y0).

where w PjPy
compute ^y = arg maxy2Y H(y)
compute i = maxf0; maxy2Si H(y)g
if H(^y) > i +  then

6:
7:
8:
9:
10:
11:
12:
13: until no Si has changed during iteration

Si  Si [ f^yg
S  optimize dual over S, S = [iSi.

end if
end for

The algorithm we propose aims at nding a small set
of active constraints that ensures a suciently accu-
rate solution. More precisely, it creates a nested se-
quence of successively tighter relaxations of the origi-
nal problem using a cutting plane method. The latter
is implemented as a variable selection approach in the
dual formulation. We will show that this is a valid
strategy, since there always exists a polynomially-sized
subset of constraints so that the corresponding solu-
tion fullls all constraints with a precision of at least .
This means, the remaining { potentially exponentially
many { constraints are guaranteed to be violated by
no more than , without the need for explicitly adding
them to the optimization problem.

We will base the optimization on the dual program
formulation which has two important advantages over
the primal QP. First, it only depends on inner prod-
ucts in the joint feature space dened by , hence
allowing the use of kernel functions. Second, the con-
straint matrix of the dual program (for the L1-SVMs)
supports a natural problem decomposition, since it is
block diagonal, where each block corresponds to a spe-
cic training instance.

Pseudocode of the algorithm is depicted in Algo-
rithm 1. The algorithm applies to all SVM formula-
tions discussed above. The only dierence is in the way
the cost function gets set up in step 5. The algorithm
maintains a working set Si for each training example
(xi; yi) to keep track of the selected constraints which
dene the current relaxation.
Iterating through the
training examples (xi; yi), the algorithm proceeds by

nding the (potentially) \most violated" constraint,
involving some output value ^y (line 6).
If the (ap-
propriately scaled) margin violation of this constraint
exceeds the current value of i by more than  (line 8),
the dual variable corresponding to ^y is added to the
working set (line 9). This variable selection process in
the dual program corresponds to a successive strength-
ening of the primal problem by a cutting plane that
cuts o the current primal solution from the feasible
set. The chosen cutting plane corresponds to the con-
straint that determines the lowest feasible value for i.
Once a constraint has been added, the solution is re-
computed wrt. S (line 10). Alternatively, we have also
devised a scheme where the optimization is restricted
to Si only, and where optimization over the full S is
performed much less frequently. This can be benecial
due to the block diagonal structure of the optimization
problems, which implies that variables jy with j 6= i,
y 2 Sj can simply be \frozen" at their current val-
ues. Notice that all variables not included in their
respective working set are implicitly treated as 0. The
algorithm stops, if no constraint is violated by more
than . The presented algorithm is implemented and
available1 as part of SVMlight. Note that the SVM
optimization problems from iteration to iteration dif-
fer only by a single constraint. We therefore restart
the SVM optimizer from the current solution, which
greatly reduces the runtime. A convenient property of
both algorithms is that they have a very general and
well-dened interface independent of the choice of 

1http://svmlight.joachims.org/

and 4. To apply the algorithm, it is sucient to im-
plement the feature mapping (x; y) (either explicit or
via a joint kernel function), the loss function 4(yi; y),
as well as the maximization in step 6. All of those,
in particular the constraint/cut selection method, are
treated as black boxes. While the modeling of (x; y)
and 4(yi; y) is more or less straightforward, solving
the maximization problem for constraint selection typ-
ically requires exploiting the structure of .

4.3. Analysis

1

It is straightforward to show that the algorithm nds a
solution that is close to optimal (e.g. for the SVM4s
,
adding  to each i is a feasible point of the primal at
most C from the maximum). However, it is not im-
mediately obvious how fast the algorithm converges.
We will show in the following that the algorithm con-
verges in polynomial time for a large class of problems,
despite a possibly exponential or innite jYj.
Let us begin with an elementary Lemma that will be
helpful for proving subsequent results.
It quanties
how the dual objective changes, if one optimizes over
a single variable.

Lemma 1. Let J be a positive denite matrix and let
us dene a concave quadratic program

W () = 

1
2

0J  + hh; i s.t.   0

and assume   0 is given with r = 0. Then max-
imizing W with respect to r while keeping all other
components xed will increase the objective by

(hr Ps sJrs)2

2Jrr

provided that hr Ps sJrs.
Proof. Denote by [r  ] the solution  with the
r-th coecient changed to , then

W ([r  ])  W () = hr Xs
hr Ps sJrs

The dierence is maximized for

 =

Jrr

sJrs! 

2
2

Jrr

Notice that   0, since hr  Ps sJrs and Jrr >

0.

Using this Lemma, we can lower bound the improve-
ment of the dual objective in step 10 of Algorithm 1.
For brevity, let us focus on the case of SVM4s
. Simi-
lar results can be derived also for the other variants.

2

Proposition 2. Dene 4i = maxy 4(yi; y) and
Ri = maxy ki(y)k. Then step 10 in Algorithm
1, improves the dual objective for SVM4s
2 at least by
1
2 2(4iR2

i + n=C)1.

n

Proof. Using the notation in Algorithm 1 one
can apply Lemma 1 with r = (i; ^y) denoting
the newly added constraint, hr = 1, Jrr =
ki(^y)k2 +
Py6=yi
iy
ing the fact that Py6=yi

C4(yi;^y) and Ps sJrs = hw; i(^y)i +
Cp4(yi;^y)p4(yi;y)

shows the following increase of the objective function
when optimizing over r alone:

. Note that r = 0. Us-
niy

Cp4(yi;y)

= i, Lemma 1

n

n

n

2



iy

Cp4(yi;^y)p4(yi;y)2
(cid:181)1  hw; i(^y)i Py6=yi
2ki(^y)k2 +
C4(yi;^y)
2ki(^y)k24(yi; ^y) + n
C

The step follows from the fact that i  0 and
p4(yi; ^y)(1hw; i(^y)i) > i + , which is the con-
dition of step 8. Replacing the quantities in the de-
nominator by their upper limit proves the claim, since
jointly optimizing over more variables than just r can
only further increase the dual objective.

2

This leads to the following polynomial bound on the
maximum size of S.
Theorem 1. With R = maxi Ri, 4 = maxi 4i
and for a given  > 0, Algorithm 1 for the
SVM4s
terminates after incrementally adding at most
2(C 42 R2 + n 4) constraints to the working set S.
Proof. With S = ; the optimal value of the dual is
0. In each iteration a constraint (i; y) is added that
is violated by at least , provided such a constraint
exists. After solving the S-relaxed QP in step 10, the
2 2( 4 R2 + n=C)1
objective will increase by at least 1
according to Proposition 2. Hence after t constraints,
the dual objective will be at least t times this amount.
The result follows from the fact that the dual objective
is upper bounded by the minimum of the primal, which
in turn can be bounded by 1

2 C 4.

Note that the number of constraints in S does not de-
pend on jYj. This is crucial, since jYj is exponential or
innite for many interesting problems. For problems
where step 6 can be computed in polynomial time, the
overall algorithm has a runtime polynomial in n; R; 4,
1=, since at least one constraint will be added while

cycling through all n instances and since step 10 is
polynomial.

5. Applications and Experiments

To demonstrate the eectiveness and versatility of our
approach, we report results on a number of dierent
tasks To adapt the algorithm to a new problem, it is
sucient to implement the feature mapping (x; y),
the loss function 4(yi; y), as well as the maximization
in step 6.

5.1. Multiclass Classication

1; : : : ; v0

Our algorithm can implement the conventional winner-
takes-all (WTA) multiclass classication (Crammer &
Singer, 2001) as follows. Let Y = fy1; : : : ; yKg and
w = (v0
K)0 is a stack of vectors, vk being a
weight vector associated with the k-th class yk. Fol-
lowing Crammer and Singer (2001) one can then dene
F (x; yk; w) = hvk; '(x)i, where '(x) 2 <D denotes
an arbitrary input representation. These discriminant
functions can be equivalently represented in the pro-
posed framework by dening a joint feature map as
follows (x; y)  '(x)  c(y). Here c refers to the
orthogonal (binary) encoding of the label y and  is
the tensor product which forms all products between
coecients of the two argument vectors.

5.2. Classication with Taxonomies

The rst generalization we propose is to make use of
more interesting output features  than the orthogonal
representation c. As an exemplary application of this
kind, we show how to take advantage of known class
taxonomies. Here a taxonomy is treated as a lattice
in which the classes y 2 Y are the minimal elements.
For every node z in the lattice (corresponding to a
super-class or class) we introduce a binary attribute
z(y) indicating whether or not z is a predecessor of
y. Notice that h(y); (y0)i will count the number of
common predecessors.

We have performed experiments using a document
collection released by the World Intellectual Prop-
erty Organization (WIPO), which uses the Interna-
tional Patent Classication (IPC) scheme. We have
restricted ourselves to one of the 8 sections, namely
section D, consisting of 1,710 documents in the WIPO-
alpha collection. For our experiments, we have indexed
the title and claim tags. We have furthermore sub-
sampled the training data to investigate the eect of
the training set size. Document parsing, tokenization
and term normalization have been performed with the

Table 1. Results on the WIPO-alpha corpus, section D
with 160 groups using 3-fold and 5-fold cross validation, re-
spectively. (cid:176)t is a standard ((cid:176)at) SVM multiclass model,
tax the hierarchical architecture.
0/1 denotes training
based on the classication loss, 4 refers to training based
on the tree loss.

(cid:176)t 0/1

tax 0/1

(cid:176)t 4 tax 4

4 training instances per class
acc
4-loss
2 training instances per class
acc
4-loss

28.32
1.32

20.46
1.51

28.32
1.36

20.20
1.54

27.47
1.30

20.20
1.39

29.74

+5.01 %
1.21 +12.40 %

21.73

+7.57 %
1.33 +13.67 %

MindServer retrieval engine.2 As a suitable loss func-
tion 4, we have used a tree loss function which denes
the loss between two classes y and y0 as the height of
the rst common ancestor of y and y0 in the taxon-
omy. The results are summarized in Table 1 and show
that the proposed hierarchical SVM learning architec-
ture improves performance over the standard multi-
class SVM in terms of classication accuracy as well
as in terms of the tree loss.

5.3. Label Sequence Learning

Label sequence learning deals with the problem of pre-
dicting a sequence of labels y = (y1; : : : ; ym), yk 2 ,
from a given sequence of inputs x = (x1; : : : ; xm).
It subsumes problems like segmenting or annotat-
ing observation sequences and has widespread appli-
cations in optical character recognition, natural lan-
guage processing, information extraction, and compu-
tational biology. In the following, we study our algo-
rithm on a named entity recognition (NER) problem.
More specically, we consider a sub-corpus consisting
of 300 sentences from the Spanish news wire article
corpus which was provided for the special session of
CoNLL2002 devoted to NER. The label set in this
corpus consists of non-name and the beginning and
continuation of person names, organizations, locations
and miscellaneous names, resulting in a total of jj = 9
dierent labels. In the setup followed in Altun et al.
(2003), the joint feature map (x; y) is the histogram
of state transition plus a set of features describing the
emissions. An adapted version of the Viterbi algorithm
is used to solve the argmax in line 6. For both per-
ceptron and SVM a second degree polynomial kernel
was used.

The results given in Table 2 for the zero-one loss,
compare the generative HMM with Conditional Ran-
dom Fields (CRF) (Laerty et al., 2001), Collins per-

2http://www.recommind.com

Table 2. Results of various algorithms on the Named En-
tity Recognition task (Altun et al., 2003).

Table 4. Error rates and number of constraints jSj depend-
ing on the number of training examples ( = 0:1, C = 0:01).

Method HMM CRF Perceptron SVM
Error
5.08

9.36

5.17

5.94

Table 3. Results for various SVM formulations on the
Named Entity Recognition task ( = 0:01, C = 1).

Method Train Err Test Err
5.10.6
SVM2
SVM4s
5.10.8
SVM4m
5.10.7

0.20.1
0.40.4
0.30.2

2

2

Const

Avg Loss
2824106 1.020.01
2626225 1.100.08
2628119 1.170.12

Train Error

Test Error

SVM2

SVM2 GenMod

Const
n GenMod
1 20.013.3 0.00.0 74.32.7 47.04.6
7.80.3
2 20.08.2 0.00.0 54.53.3 34.34.3 13.90.8
4 10.05.5 2.02.0 28.02.3 14.41.4 31.90.9
58.91.2
0.00.0 10.20.7 7.11.6
5.20.5
1.00.7 3.40.7
95.22.3
3.00.3 157.22.4
1.00.4 2.30.5
2.00.5 1.90.4
2.80.6 252.72.1

2.01.3
2.50.8
2.01.0
2.80.5

10
20
40
80

ceptron and the SVM algorithm. All discriminative
learning methods substantially outperform the stan-
dard HMM. In addition, the SVM performs slightly
better than the perceptron and CRFs, demonstrating
the benet of a large-margin approach. Table 3 shows
that all SVM formulations perform comparably, prob-
ably due to the fact the vast majority of the support
label sequences end up having Hamming distance 1 to
the correct label sequence (notice that for loss equal
to 1 all SVM formulations are equivalent).

5.4. Sequence Alignment

Next we show how to apply the proposed algorithm
to the problem of learning how to align sequences
x 2 X = . For a given pair of sequences x and
z, alignment methods like the Smith-Waterman algo-
rithm select the sequence of operations (e.g. insertion,
substitution) ^a(x; z) = argmaxa2A hw; (x; z; a)i that
transforms x into y and that maximizes a linear ob-
jective function derived from the (negative) operation
costs w. (x; z; a) is the histogram of alignment op-
erations. We use the value of hw; (x; z; ^a(x; z))i as a
measure of similarity.

In order to learn the cost vector w we use training data
of the following type. For each native sequence xi there
is a most similar homologue sequence zi along with
what is believed to be the (close to) optimal alignment
ai. In addition we are given a set of decoy sequences
zt
i, t = 1; : : : ; k with unknown alignments. The goal is
to nd a cost vector w so that homologue sequences
are close to the native sequence, and so that decoy
sequences are further away. With Yi = fzi; z1
i ; :::; zk
i g
as the output space for the i-th example, we seek a w so
that hw; (xi; zi; ai)i exceeds hw; (xi; zt
i; a)i for all t
and a. This implies a zero-one loss and hypotheses of
the form f (xi; w) = argmaxy2Yi maxa hw; (x; z; a)i.
We use the Smith-Waterman algorithm to implement
the maxa.

Table 4 shows the test error rates (i.e. fraction of times
the homolog is not selected) on the synthetic dataset

described in Joachims (2003). The results are aver-
aged over 10 train/test samples. The model contains
400 parameters in the substitution matrix  and a
cost  for \insert/delete". We train this model using
the SVM2 and compare against a generative sequence
alignment model, where the substitution matrix is

computed as ij = log P (xi;zj )
P (xi)P (zj ) using Laplace esti-
mates. For the generative model, we report the results
for  = 0:2, which performs best on the test set. De-
spite this unfair advantage, the SVM performs better
for low training set sizes. For larger training sets, both
methods perform similarly, with a small preference for
the generative model. However, an advantage of the
SVM model is that it is straightforward to train gap
penalties. As predicted by Theorem 1, the number of
constraints jSj is low. It appears to grows sub-linearly
with the number of examples.

5.5. Natural Language Parsing

We test the feasibility of our approach for learning
a weighted context-free grammar (see Figure 1) on a
subset of the Penn Treebank Wall Street Journal cor-
pus. We consider the 4098 sentences of length at most
10 from sections F2-21 as the training set, and the 163
sentences of length at most 10 from F22 as the test set.
Following the setup in Johnson (1999), we start based
on the part-of-speech tags and learn a weighted gram-
mar consisting of all rules that occur in the training
data. To solve the argmax in line 6 of the algorithm,
we use a modied version of the CKY parser of Mark
Johnson3 and incorporated it into SVMlight.

The results are given in Table 5. They show accu-
racy and micro-averaged F1 for the training and the
test set. The rst line shows the performance for gen-
erative PCFG model using the maximum likelihood
estimate (MLE) as computed by Johnsons implemen-
tation. The second line show the SVM2 with zero-one
loss, while the following lines give the results for the
F1-loss 4(yi; y) = (1  F1(yi; y)) using SVM4s
2 and
3At http://www.cog.brown.edu/mj/Software.htm

Table 5. Results for learning a weighted context-free gram-
mar on the Penn Treebank. CPU time measured in hours.

Train

F1 Acc

Test

Training Eciency
F1 Const CPU(%QP)

Method Acc
PCFG
SVM2
SVM4s
SVM4m

2

2

61.4 90.4 55.2 86.0 N/A
66.3 92.0 58.9 86.2
7494
8043
62.2 92.1 58.9 88.5
63.5 92.3 58.3 88.4
7117

0

1.2 (81.6%)
3.4 (10.5%)
3.5 (18.0%)

2

SVM4m
. All results are for C = 1 and  = 0:01. All
values of C between 101 to 102 gave comparable re-
sults. While the zero-one loss achieves better accuracy
(i.e. predicting the complete tree correctly), the F1-
score is only marginally better. Using the F1-loss gives
substantially better F1-scores, outperforming the MLE
substantially. The dierence is signicant according to
a McNemar test on the F1-scores. We conjecture that
we can achieve further gains by incorporating more
complex features into the grammar, which would be
impossible or at best awkward to use in a generative
PCFG model. Note that our approach can handle ar-
bitrary models (e.g. with kernels and overlapping fea-
tures) for which the argmax in line 6 can be computed.

In terms of training time, Table 5 shows that the to-
tal number of constraints added to the working set is
small. It is roughly twice the number of training ex-
amples in all cases. While the training is faster for the
zero-one loss, the time for solving the QPs remains
roughly comparable. The re-scaling formulations lose
time mostly on the argmax in line 6. This might be
sped up, since we were using a rather naive algorithm
in the experiments.

6. Conclusions

We formulated a Support Vector Method for super-
vised learning with structured and interdependent out-
puts.
It is based on a joint feature map over in-
put/output pairs, which covers a large class of interest-
ing models including weighted context-free grammars,
hidden Markov models, and sequence alignment. Fur-
thermore, the approach is very (cid:176)exible in its ability to
handle application specic loss functions. To solve the
resulting optimization problems, we proposed a simple
and general algorithm for which we prove convergence
bounds. Our empirical results verify that the algo-
rithm is indeed tractable. Furthermore, we show that
the generalization accuracy of our method is at least
comparable or often exceeds conventional approaches
for a wide range of problems. A promising property
of our method is that it can be used to train com-
plex models, which would be dicult to handle in a
generative setting.

Acknowledgments

The authors would like to thank Lijuan Cai for con-
ducting the experiments on classication with tax-
onomies. This work was supported by the Kanellakis
Dissertation Fellowship, NSF-ITR Grant IIS-0312401,
and NSF CAREER Award 0237381.

