Abstract

Memory-based  classification  algorithms such  as  radial  basis  func(cid:173)
tions or K-nearest  neighbors typically rely on simple distances (Eu(cid:173)
clidean,  dot  product ... ),  which  are  not  particularly  meaningful  on
pattern vectors.  More complex, better suited distance measures are
often  expensive  and  rather  ad-hoc  (elastic  matching,  deformable
templates).  We  propose  a  new  distance  measure which  (a)  can  be
made  locally  invariant to  any  set  of transformations  of the  input
and  (b)  can  be  computed  efficiently.  We  tested  the  method  on
large handwritten character  databases  provided by the  Post  Office
and the NIST.  Using invariances with  respect  to translation, rota(cid:173)
tion,  scaling,  shearing and line  thickness,  the  method  consistently
outperformed  all other systems  tested  on  the same databases.

1

INTRODUCTION

Distance-based  classification  algorithms such  as radial basis functions  or K-nearest
neighbors  often  rely  on  simple  distances  (such  as  Euclidean  distance,  Hamming
distance,  etc.).  As  a  result,  they  suffer  from  a  very  high  sensitivity  to  simple
transformations of the input patterns that should leave the classification unchanged
(e.g.  translation  or scaling for  2D  images).  This is  illustrated  in  Fig.  1 where  an
unlabeled  image of a  "9"  must  be classified  by finding  the  closest  prototype image
out  of  two  images  representing  respectively  a  "9"  and  a  "4".  According  to  the
Euclidean  distance  (sum  of  the  squares  of the  pixel  to  pixel  differences),  the  "4"
is  closer  even  though  the  "9"  is  much  more  similar  once  it has  been  rotated  and
thickened.  The result  is  an  incorrect  classification.  The key  idea is  to  construct  a
distance  measure  which  is  invariant  with  respect  to  some  chosen  transformations
such  as  translation, rotation and others.  The special  case  of linear  transformations
has been well studied in statistics and is sometimes referred to as Procrustes analysis

50

Efficient  Pattern  Recognition  Using  a  New Transformation  Distance

51

Pattern to
be classified

prototype A

Prototype B

Figure  1:  What is  a  good similarity measure?  According  to the Euclidean distance
the pattern to be classified is more similar to prototype B. A better distance measure
would find  that prototype  A  is  closer  because  it differs  mainly by a  rotation and  a
thickness  transformation, two  transformations which  should leave the classification
invariant.

(Sibson,  1978).  It has  been  applied  to  on-line  character  recognition  (Sinden  and
Wilfong,  1992).
This  paper  considers  the  more  general  case  of non-linear  transformations  such  as
geometric  transformations  of  gray-level  images.  Remember  that  even  a  simple
image  translation  corresponds  to  a  highly  non-linear  transformation  in  the  high(cid:173)
dimensional pixel space l .  In  previous work  (Simard et al.,  1992b),  we  showed  how
a  neural  network could  be trained to be invariant with respect  to selected  transfor(cid:173)
mations of the input.  VVe  now  apply similar ideas  to distance-based  classifiers.
''''hen a pattern P is transformed (e.g. rotated) with a transformation s that depends
on  one  parameter  a  (e.g.  the  angle of the  rotation),  the  set  of all  the  transformed
patterns  Sp  =  {x  I 35 such  that  x  =  s(5, P)}  is  a  one-dimensional  curve  in  the
vector  space  of the  inputs  (see  Fig.  2).
In  certain  cases,  such  as  rotations  of
digitized  images,  this  curve  must  be  made  continuous  using smoothing  techniques
(see  (Simard et  al.,  1992b)).  When the set of transformations is  parameterized  by
n  parameters ai  (rotation, translation, scaling, etc.),  Sp  is  a  manifold of at most n
dimensions.  The patterns  in  Sp  that  are  obtained  through  small transformations
of  P,  i.e.
the  part  of Sp  that  is  close  to  P,  can  be  approximated  by  a  plane
tangent  to  the  manifold  Sp  at  the  point  P.  Small  transformations  of P  can  be
obtained  by  adding  to  P  a  linear  combination  of vectors  that  span  the  tangent
plane (tangent  vectors).  The images at the bottom of Fig.  2 were  obtained by  that
procedure.  Tangent vectors for  a  transformation s  can easily be computed by finite
difference  (evaluating os(a, P)/oa);  more  details  can  be  found  in  (Simard  et  al.,
1992b; Simard et al.,  1992a).
As  we  mentioned  earlier,  the  Euclidean  distance  between  two  patterns  P  and  E
is  in  general  not  appropriate  because  it  is  sensitive  to  irrelevant  transformations
of P  and  of E.  In  contrast,  the  distance  V(E, P)  defined  to  be  the  minimal dis(cid:173)
tance  between  the  two  manifolds Sp  and  SE  is  truly  invariant with  respect  to  the
transformation  used  to  generate  Sp  and  SE.  Unfortunately,  these  manifolds have
no  analytic expression  in general,  and finding  the distance  between  them is  a  hard
optimization problem  with  multiple  local  minima.  Besides,  t.rue  invariance  is  not

1 If the ima.ge of a  "3"  is  translated vertica.lly  upward,  the middle top pixel  will oscillate

from  black  to white three  times.

52

Simard, Cun, and Denker

[3]

  -15 

-7.5-

p

.7.5 -

True rotations of P

Transformations at p

1/
II

_  _ ........... _  .Y

Pixel space

a=-O.2

a=-Q.l

p

a=O.l

a=O.2

p

T.  V.

Figure  2:  Top:  Small  rotations  of  an  original  digitized  image  of  the  digit  "3".
Middle:  Representation  of the  effect  of the  rotation  in  pixel  space  (if there  were
only  3  pixels).  Bottom:  Images  obtained  by  moving  along  the  tangent  to  the
transformation  curve  for  the  same  original  digitized  image  P  by  adding  various
amounts (a) of the tangent vector  (T.V.).

necessarily desirable since a rotation of a  "6"  into a  "9"  does not preserve the correct
classification.
Our  approach  consists  of approximating  the  non-linear  manifold  Sp  and  SE  by
linear  surfaces  and  computing  the  distance  D( E, P)  defined  to  be  the  minimum
distance  between  them.  This solves  three  problems  at  once:  1)  linear  manifolds
have  simple  analytical  expressions  which  can  be  easily  computed  and  stored,  2)
finding  the  minimum  distance  between  linear  manifolds  is  a  simple  least  squares
problem which can be solved efficiently  and, 3) this  distance is  locally invaria.nt but
not globally invariant.  Thus the  distance  between  a  "6"  and  a slightly rota.ted  "6"
is  small but the  distance  between  a  "6"  and  a  "9"  is  la.rge.  The  different.  distan ces
between  P  and E  are represented  schematically in  Fig.  3.
The figure  represents  two patterns P  and E  in 3-dimensional space.  The ma.nifolds
generated  by  s  are represented  by  one-dimensional curves  going  through  E  and  P
respectively.  The  linear  approximations to  the  manifolds  are  represented  by  lines
tangent  to  the curves at E  and P.  These lines do not intersect in 3 dimensions and
the  shortest  distance  between  them  (uniquely  defined)  is  D(E, P).  The  distance
between  the  two  non-linear  transformation  curves  VeE, P)  is  also  shown  on  the
figure.
An  efficient  implementation  of the  tangent  distance  D(E, P)  will  be  given  in  the

Efficient Pattern  Recognition  Using  a  New Transformation Distance

53

Figure  3:  Illustration  of the  Euclidean  distance  and  the  tangent  distance  between
P  and E

next  section.  Although  the  tangent  distance  can  be  applied  to  any  kind  of pat(cid:173)
terns  represented  as  vectors,  we  have  concentrated  our  efforts  on  applications  to
image recognition.  Comparison of tangent distance with the best known competing
method will be  described.  Finally we  will discuss possible variations on the tangent
distance and how  it can  be generalized  to problems other than pattern recognition.

2

IMPLEMENTATION

In  this  section  we  describe  formally  the  computation of the  tangent  distance.  Let
the function  s  which  map  u, a  to  s(a, u)  be  a  differentiable  transformation  of the
input space,  depending  on  a  vector a of parameter,  verifying s(O, u) = 'It.
If u  is  a  2  dimensional  image  for  instance,  s(a, u)  could  be  a  rotation  of u  by
the  angle  &.  If we  are  interested  in  all  transformations  of images  which  conserve
distances  (isometry),  8(a, u)  would  be  a  rotation  by  a r  followed  by  a  translation
by  ax, a y  of the image u.  In  this case  & =  (ar , ax, a y) is  a  vector  of parameters of
dimension 3.  In general,  & =  (ao, .. " am-d is  of dimension m.
Since  8 is  differentiable,  the set  Stl.  =  {x  I 3a for  which  x  = 8( a, 'It)}  is  a  differen(cid:173)
tiable manifold  which  can  be  approximated  to  the first  order  by  a  hyperplane Ttl..
This hyperplane is  tangent  to Stl.  at u  and is  generated  by the  columns of matrix

Ltl.  = 08(&~ 'It) I  = [08(&, u), ... , 08(a, U)]

aa

d=cf

oao

aam-l  d=O

which  are  vectors  tangent  to  the  manifold.  If E  and  P  are  two  patterns  to  be
compared,  the  respective  tangent  planes  TE  and  Tp  can  be  used  to  define  a  new
distance  D  between  these  two  patterns.  The tangent  distance  D(E, P)  between  E
and  P  is  defined  by

D(E, P) =  min

xETE,yETp

IIx - yW

The equation of the  tangent planes TE  and Tp  is  given  by:

(1)

(2)

(3)
(4)

54

Simard, Cun,  and  Denker

where  LE  and  Lp  are the matrices  containing the tangent  vectors  (see  Eq.  1)  and
the vectors a E and ap are the coordinates of E' and P' in the corresponding tangent
planes.  The quantities  LE  and  Lp  are  attributes of the  patterns  so  in  many  cases
they  can  be  precomputed  and stored.
Computing the  tangent  distance

(5)

amounts to solving a linear least squares problem.  The optimality condition is  that
the partial derivatives of D(E, P)  with respect  to  a p  and aE should be  zero:

oD(~, P) = 2(E'(aE) _  p'(ap  T LE = 0
oD(~,P) = 2(p'(ap) _  E'(aET Lp = 0

oaE

oap

(6)

(7)

Substituting E'  and  P' by  their expressions yields to the following  linear system of
equations,  which  we  must solve  for  ap and ilE:

L;(E - P  - Lpilp + LEaE) = 0
Lf(E - P  - Lpap + LEilE) = 0

(8)
(9)

The solution of this system is

(LPEL"E1L~ - L;)(E - P) = (LPEL"E1LEP  - Lpp )ap
(LEPLp~L; - L~)(E - P) = (LEE  - LEPLp~LpE)aE

(10)
(11)
where  LEE  =  LfT LE ,  LpE  =  L~LE' L EP  =  L~Lp and  Lpp  = L~Lp.  LU
decompositions  0  LEE  and  Lpp  can  be precomputed.  The most expenSIve  part in
solving this system is evaluating LEP  (LPE  can  be obtained by  transposing  LEP).
It requires mE x mp dot products, where mE is the number of tangent vectors for  E
and mp  is  the number of tangent vectors for  P.  Once  LEP  has been  computed,  ilp
and ilE can be computed by solving two (small) linear system of respectively mE and
mp equations.  The tangent distance  is obtained by computing  IIE'(aE) - p'(ap )11
using the value of a p  and ilE in equations 3 and 4.  If n  is the length of vector E  (or
P), the algorithm described above requires roughly n(mE+l)(mp+l)+3(m~+m~)
multiply-adds.  Approximations  to  the  tangent  distance  can  be  computed  more
efficiently.

3  RESULTS

Before  giving  the  results  of handwritten  digit  recognition  experiments,  we  would
like to demonstrate the property of "local invariance"  of tangent  distance.  A  16  by
16  pixel  image similar to  the  "3"  in  Fig 2  was  translated  by  various  amounts.  The
tangent distance (using the tangent vector corresponding to horizonta.l translations)
and  the  Euclidean  Distance  between  the  original  image  and  its  translated  version
were  measured  as a  function  of the size  k  (in  pixels)  of the translation.  The result
is  plotted in  Fig. 4.  It is  clear that the Euclidean  Distance starts increasing linearly
with  k  while  the  tangent  distance  remains  very  small for  translations  as  large  as
two  pixels.  This  indicates that,  while  Euclidean Distance is  not invariant to trans(cid:173)
lation,  tangent  distance  is  locally  invariant.  The  extent  of the  invariance  can  be

Efficient Pattern  Recognition  Using a  New Transformation  Distance

55

10

8

6

4

2

Distance

Tangent
Distance

o~--.---~~~~--~~

8
~ ~ ~ ~  0
# of pixels by which image is translated

2

4

6

Figure 4:  Euclidean and tangent distances between a  16x16 handwritten digit image
and  its  translated  version  as  a  function  of the  amount  of translation  measured  in
pixels.

increased  by  smoothing the original image,  but significant features  may be blurred
away, leading to confusion errors.  The figure  is not symmetric for large translations
because  the translated  image is  truncated  to the  16  by  16  pixel field  of the original
image.  In  the  following  experiments,  smoothing  was  done  by  convolution  with  a
Gaussian of standard deviation u  =  0.75.  This value, which  was estimated visually,
turned  out  to be  nearly optimal (but not  critical).

3.1  Handwritten Digit  Recognition

Experiments  were  conducted  to  evaluate  the  performance  of tangent  distance  for
handwritten  digit  recognition.  An interesting  characteristic  of digit images is  that
we can readily identify a set of local transformations which do not affect the identity
of the character,  while covering a large portion of the set of possible  instances of the
character.  Seven such image transformations were identified:  X and Y  translations,
rotation, scaling, two  hyperbolic transformations (which  can  generate shearing and
squeezing),  and  line  thickening  or  thinning.  The  first  six  transformations  were
chosen  to  span  the  set  of all  possible  linear  coordinate  transforms  in  the  imn~e
plane (nevertheless,  they correspond to highly non-linear transforms in pixel space).
Additional transformations  have been  tried  with less  success.
The simplest possible use  of tangent distance  is in a  Nearest  Neighbor  classifier.  A
set  of prototypes  is  selected  from  a  training  set,  and  stored  in  memory.  When  a
test  pattern  is  to  be  classified,  the  J(  nearest  prototypes  (in  terms  of tangent  dis(cid:173)
tance) are found,  and the pattern is given the class that has the majority among the
neighbors.  In  our applications,  the size  of the prototype set is  in  the neighborhood
of 10,000.  In  principle,  classifying  a  pattern  would  require  computing  10,000  tan(cid:173)
gent distances,  leading to excessive  classification times,  despite  the efficiency  of the
tangent distance  computation.  Fortunately,  two patterns  that are  very  far  apart in
terms of Euclidean  Distance are likely to be far  apart in  terms of tangent  distance.
Therefore  we  can  use  Euclidean  distance  as  a  "prefilter" , and eliminate prototypes
that are  unlikely  to  be  among the nearest  neighbors.  V'le  used  the  following 4-step
classification  procedure:  1)  the  Euclidean  distance  is  computed  between  the  test
pattern  and  all  the  prototypes,  2)  The  closest  100  prototypes  are  selected,  3)  the
tangent  distance  between  these  100  prototypes  and  the  test  pattern  is  computed

56

Simard, Cun, and Denker

error (%)

6
5
4

3

2
1
0

USPS

Human  T-Dlst  NNet

K-NN

5
4
3

2
1
o

NIST

Human  T -Dlst  NNet

Figure  5:  Comparison  of  the  error  rate  of tangent  nearest  neighbors  and  other
methods on  two handwritten  digit databases

and 4)  the most represented  label among the J( closest  prototype is outputed.  This
procedure  is  two  orders  of magnitude  faster  than  computing  all  10,000  tangent
distances,  and yields  the same performance.
US  Postal Service database:  In  the first  experiment,  the  database consisted  of
16  by  16  pixel size-normalized images of handwritten  digits,  coming from  US  mail
envelopes.  The entire  training set  of 9709  examples of was  used  as  the  prototype
set.  The test set contained 2007 patterns.  The best performance was obtained with
the  "one  nearest  neig~bor" rule.  The results  are  plotted in  Fig.  5.  The error  rate
of the  method  is  2.6%.  Two  members  of our  group  labeled  the  test  set  by  hand
with an error rate of 2.5% (using one of their labelings as  the truth to test  the other
also  yielded  2.5% error  rate).  This is  a  good  indicator  of the  level  of difficulty  of
this  task2 .  The  performance of our  best  neural  network  (Le  Cun et  al.,  1990)  was
3.3%.  The  performance  of one  nearest  neighbor  with  the  Euclidean  distance  was
5.9%.  These  results show  that tangent  distance performs substantially better  than
both standard K-nearest  neighbor  and neural networks.
NIST  database:  The second experiment  was  a  competition organized by  the N 8,(cid:173)
tional Institute  of Standards  and  Technology.  The  object  of the  competition  was
to  classify  a  test  set  of 59,000  handwritten  digits,  given  a  training  set  of 223,000
patterns.  A total of 45  algorithms were submitted from 26  companies from 7 differ(cid:173)
ent  countries.  Since  the  training set  was  so  big,  a  very  simple procedure  was  used
to select  about  12,000  patterns  as  prototypes.  The  procedure  consists  of creating
a new  database (empty  at the beginning), and  classifying each  pattern of the large
database  using  the  new  database as  a  prototype set.  Each  time  an  error  is  made,
the pattern is added to the new  database.  More than one pass may have to be made
before  the  new  database  is  stable.  Since  this filtering  process  would  take  too  long
with  223,000  prototypes,  we  split  the  large  database  into  22  smaller  databases  of
10,000  patterns  each,  filtered  those  (to  about  550  patterns)  and  concatenated  the
result,  yielding  a  database  of roughly  12,000  patterns.  This  procedure  has  many
drawbacks,  and  in  particular,  it  is  very  good  at  picking  up  mislabeled  characters
in  the  training  set.  To  counteract  this  unfortunate  effect,  a  3  nearest  neighbors
procedure  was  used  with  tangent  distance.  The  organizers  decided  to  collect  the

2This is  an extremely difficult  test set.  Procedures that achieve less than 0.5% error on

other handwritten  digit  tasks  barely achieve  less  than 4%  on  this one

Efficient Pattern  Recognition  Using a  New Transformation Distance

57

training set  and  the  test  set  among two  very  different  populations (census  bureau
workers  for  the training set,  high-school students for  the test set),  we  therefore  re(cid:173)
port results on  the official  NIST test set  (named  "hard  test  set"),  and  on  a subset
of the  official  training set,  which  we  kept  aside  for  test  purposes  (the  "easy  test
set").  The  results  are  shown  in  Fig.  5.  The  performance  is  much  worse  on  the
hard test set since the distribution was very  different  from  that of the  training set.
Out of the  25  participants  who  used  the  NIST training  database,  tangent  distance
finished first.  The overall winner did not use  the training set  provided by  NIST (he
used  a  much larger proprietary training set),  and therefore  was not  affected  by  the
different  distributions in the training set  and  test set.

4  DISCUSSION

The  tangent  distance  algorithm  described  in  the  implementation  section  can  be
improved/adjusted  in  at  least  four  different  ways:  1)  approximating  the  tangent
distance  for  better  speed  2)  modifying the tangent  distance  itself,  3)  changing  the
set of transformations/tangent vectors and 4)  using the tangent distance with  clas(cid:173)
sification  algorithms  other  than  K-nearest  neighbors,  perhaps  in  combination,  to
minimize the number of prototypes.  We  will discuss each  of these  aspects  in  turn.
Approximation:  The distance between two hyperplanes TE  and Tp going through
P  and  E  can  be  approximated  by  computing  the  projection  PEep)  of Ponto TE
and  Pp(E)  of E  onto  Tp.  The  distance  IIPE(P) - Pp(E)1I  can  be  computed  in
O(n(mE + mp  multiply-adds  and  is  a  fairly  good  approximation  of  D(E, P).
This  approximation  can  be  improved  at  very  low  cost  by  computing  the  closest
points between the lines defined by (E, PEep~ and (P, Pp(E.  This approximation
was  used  with  no  loss  of performance  to  reduce  the number  of computed  tangent
distance  from  100  to  20  (this  involves  an  additional  "prefilter").  In  the  case  of
images,  another  time-saving  idea  is  to  compute  tangent  distance  on  progressively
smaller sets of progressively higher resolution  images.
Changing  the  distance:  One  may  worry  that  the  tangent  planes  of  E  and  P
may  be  parallel  and  be  very  close  at a  very  distant region  (a bad side effect  of the
linear  a.pproximation).  This effect  can  be  limited  by  imposing  a  constraint  of the
form  IlaEIi <  f{E  and  lIapli  < f{p.  This constraint  was  implemented  but did  not
yield  better  results.  The  reason  is  that  tangent  planes  are  mostly  orthogonal  in
high  dimensional space  and  the norms of [[aEIi  and  !lapll  are  already small.
The tangent  distance  can  be  normalized  by  dividing it by  the norm of the vectors.
This improves  the results slightly because  it offsets  side  effects  introduced  in some
transformations such  as  scaling.  Indeed,  if scaling  is  a  transformation  of interest,
there  is  a  potential  danger  of finding  the  minimum  distance  between  two  images
after  they  have  been  scaled  down  to  a  single  point.  The  linear  approximation  of
the  scaling  transformation  does  not  reach  this  extreme,  but  still  yields  a  slight
degradation of the performance.  The error rate reported on the USPS database can
be improved  to 2.4%  using  this normalization (which  was  not  tried on  NIST).
Tangent distance can  be viewed  as one iteration of a  Newton-type algorithm which
finds  the  points  of minimum  distance  on  the  true  transformation  manifolds.  The
vectors  aE  and  ap  are  the  coordinates of the two  closest  points  in  the  respective
tangent  spaces,  but  they  can  also  be  interpreted  for  real  (non-linear)  transforma(cid:173)
tions.  If ae; is  the  amount  of the  translation  tangent  vector  that must  be  added
to  E  to  make it  as  close  as  possible  to  P,  we  can  compute  the  true  translation  of
image  E  by  ae,;  pixels.  In  other  words,  E'(aE)  and  pl(ap)  are  projected  onto

58

Simard, Cun, and  Denker

close  points of SE  and  Sp.  This involves a  resampling but  can  be  done  efficiently.
Once  this  new  image  has  been  computed,  the  corresponding  tangent  vectors  can
be  computed  for  this new  image and  the  process  can  be  repeated.  Eventually  this
will  converge  to  a  local  minimum in  the  distance  between  the  two  transformation
manifold of P  and E.  The tangent distance needs to be normalized for  this iteration
process  to work.
A  priori knowledge:  The  a  priori knowledge  used  for  tangent  vectors  depends
greatly  on  the  application.  For  character  recognition,  thickness  was  one  of  the
most important transformations,  reducing  the error  rate from  3.3%  to  2.6%.  Such
a  transformation  would  be  meaningless  in,  say,  speech  or face  recognition.  Other
transformations  such  as  local  rubber  sheet  deformations  may  be  interesting  for
character  recognition.  Transformations can  be known  a priori or learned  from  the
data.
Other algorithms, reducing the number of prototypes:  Tangent distance is
a  general  method  that  can  be  applied  to  problems  other  than  image  recognition,
with  classification  methods other  than  K-nearest  neighbors.  Many  distance- ba.sed
classification schemes  could  be  used  in  conjunction  with  tangent  distance,  among
them  LVQ  (Kohonen,  1984),  and radial basis functions.  Since  all  the operators  in(cid:173)
volved in the tangent distance are differentiable, it is possible to compute the partial
derivative of the tangent distance (between  an object and a prototype) with respect
to  the  tangent  vectors,  or  with  respect  to  the  prototype.  Therefore  the  tangent
distance  operators can  be inserted in gradient-descent  based  adaptive machines (of
which  LVQ  and  REF  are  particular  cases).  The  main  advantage  of learning  the
prototypes or  the tangent  vectors is  that fewer  prototypes may  be  needed  to reach
the same (or superior)  level  of performance as,  say,  regular  K-nearest  neighbors.
In  conclusion,  tangent  distance  can  greatly  improve  many  of  the  distance-based
algorithms.  We  have  used  tangent  distance  in  the  simple  K-nearest  neighbor  al(cid:173)
gorithm  and  outperformed  all  existing  techniques  on standard  classification  tasks.
This surprising success  is probably due the fact  that  a  priori knowledge can  be  very
effectively  expressed  in  the  form of tangent  vectors.  Fortuna.tely,  many algorithms
are based on computing distances and can be adapted to express  a  priori knowledge
in  a similar fashion.  Promising candidates include Parzen  windows,  learning vector
quantization  and radial basis functions.

