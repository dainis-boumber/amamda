Abstract

We assess the applicability of several popular learning
methods for the problem of recognizing generic visual cat-
egories with invariance to pose, lighting, and surrounding
clutter. A large dataset comprising stereo image pairs of 50
uniform-colored toys under 36 azimuths, 9 elevations, and 6
lighting conditions was collected (for a total of 194,400 in-
dividual images). The objects were 10 instances of 5 generic
categories: four-legged animals, human gures, airplanes,
trucks, and cars. Five instances of each category were used
for training, and the other ve for testing. Low-resolution
grayscale images of the objects with various amounts of
variability and surrounding clutter were used for training
and testing. Nearest Neighbor methods, Support Vector Ma-
chines, and Convolutional Networks, operating on raw pix-
els or on PCA-derived features were tested. Test error rates
for unseen object instances placed on uniform backgrounds
were around 13% for SVM and 7% for Convolutional Nets.
On a segmentation/recognition task with highly cluttered
images, SVM proved impractical, while Convolutional nets
yielded 16/7% error. A real-time version of the system was
implemented that can detect and classify objects in natu-
ral scenes at around 10 frames per second.

1. Introduction

The recognition of generic object categories with invari-
ance to pose, lighting, diverse backgrounds, and the pres-
ence of clutter is one of the major challenges of Computer
Vision. While there have been attempts to detect and recog-
nize objects in natural scenes using a variety of clues, such
as color, texture, the detection of distinctive local features,
and the use of separately acquired 3D models, very few au-
thors have attacked the problem of detecting and recogniz-
ing 3D objects in images primarily from the shape informa-
tion.

Even fewer authors have attacked the problem of rec-
ognizing generic categories, such as cars, trucks, airplanes,
human gures, or four-legged animals purely from shape in-
formation. The dearth of work in this area is due in part to
the difculty of the problem, and in large part to the non-

availability of a dataset with sufcient size and diversity to
carry out meaningful experiments.

The rst part of this paper describes the NORB dataset,
a large image dataset comprising 97,200 stereo image pairs
of 50 objects belonging to 5 generic categories (four-legged
animals, human gures, airplanes, trucks, and cars) under 9
different elevations, 36 azimuths, and 6 lighting conditions.
The raw images were used to generate very large sets of
greyscale stereo pairs where the objects appear at variable
location, scale, image-plane angles, brightness, and con-
trast, on top of background clutter, and distractor objects.

The second part of the paper reports results of generic
shape recognition using popular image classication meth-
ods operating on various input representations. The classi-
ers were trained on ve instances of each category (for all
azimuths, elevations, and lightings) and tested on the ve
remaining instances. Results of simultaneous detection and
recognition with Convolutional Nets are also reported.

The main purpose of this paper is not to introduce new
recognition methods, but rather to (1) describe the largest
publicly available dataset for generic object recognition; (2)
report baseline performance with standard method on this
dataset; (3) explore how different classes of methods fare
when the number of input variables is in the tens of thou-
sands, and the number of examples in the hundreds of thou-
sands; (4) compare the performance of methods based on
global template matching such as K-Nearest Neighbors and
Support Vector Machines, and those based on local fea-
ture extraction such as Convolutional Nets, when intra-class
variabilities involve highly complex transformations (pose
and lighting); (5) assess the performance of template-based
methods when the size of the problem is at the upper limit
of their practicality; (6) measure to what extent the vari-
ous learning architectures can learn invariance to 3D pose
and lighting, and can deal with the variabilities of natural
images; (7) determine whether trainable classiers can take
advantage of binocular inputs.

2. The NORB Dataset

Many object detection and recognition systems de-
scribed in the literature have (wisely) relied on many
different non-shape related clues and various assump-
tions to achieve their goal. Authors have advocated the
use of color,
texture, and contours for image index-

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

ing applications [8],
the detection of distinctive local
features [20, 26, 25, 23], the use of global appearance tem-
plates [11, 10, 19], the extraction of silhouettes and edge in-
formation [14, 22, 8, 4, 19] and the use of pose-invariant
feature histograms [9, 5, 1]. Conversely, learning-based
methods operating on raw pixels or low-level local fea-
tures have been quite successful for such applications as
face detection [24, 18, 12, 7, 25, 21], but they have yet to
be applied successfully to shape-based, pose-invariant ob-
ject recognition. One of the central questions addressed
in this paper is how methods based on global templates
and methods based on local features compare on invari-
ant shape classication tasks.

In the NORB dataset, the only useful and reliable clue
is the shape of the object, while all the other parameters
that affect the appearance are subject to variation, or are de-
signed to contain no useful clue. Parameters that are subject
to variation are: viewing angles (pose), lighting condition,
position in the image plane, scale, image-plane rotation,
surrounding objects, background texture, contrast, lumi-
nance, and camera settings (gain and white balance). Poten-
tial clues whose impact was eliminated include: color (all
images were grayscale), and object texture (objects were
painted with a uniform color). For specic object recogni-
tion tasks, the color and texture information may be help-
ful, but for generic shape recognition tasks the color and
texture information are distractions rather than useful clues.
The image acquisition setup was deliberately designed to
reect real imaging situations. By preserving natural vari-
abilities and eliminating irrelevant clues and systematic bi-
ases, our aim was to produce a benchmark in which no hid-
den regularity can be used, which would unfairly advantage
some methods over others.

While several datasets of object images have been made
available in the past [11, 22, 19], NORB is considerably
larger than those datasets, and offers more variability, stereo
pairs, and the ability to composite the objects and their cast
shadows onto diverse backgrounds.

Ultimately, practical object recognition systems will
have to be trained on natural images. The value of the
present approach is to allow systematic objective compar-
isons shape classication methods, as well as a way of
assessing their invariant properties, and the number of ex-
amples required to train them.

2.1. Data Collection

The image acquisition system was composed of a
turntable on which object were placed, two Hitachi KP-
D20AU CCD cameras mounted on a swiveling arm, and
four studio lights with bounce umbrellas. The angle of the
turntable, the elevations of the camera arm, and the inten-
sity of the lights were all under computer control. The cam-
eras were 41cm away from the objects (roughly arm length)
and 7.5cm apart from each other (roughly the distance be-
tween the two eyes in humans). The lenses focal length
was set around 16mm. The turntable was 70cm in diame-
ter and had a uniform medium gray color. The lights were
placed at various xed locations and distances around the
object.

We collected images of 50 different toys shown in g-
ure 1. The collection consists of 10 instances of 5 generic
categories: four-legged animals, human gures, airplanes,
trucks, and cars. All the objects were painted with a uni-
form bright green. The uniform color ensured that all irrel-
evant color and texture information was eliminated. 1,944
stereo pairs were collected for each object instance: 9 eleva-
tions (30, 35, 40, 45, 50, 55, 60, 65, and 70 degrees from the
horizontal), 36 azimuths (from 0 to 350
), and 6
lighting conditions (various on-off combinations of the four
lights). A total of 194,400 RGB images at 640480 resolu-
tion were collected (5 categories, 10 instances, 9 elevations,
36 azimuths, 6 lightings, 2 cameras) for a total of 179GB
of raw data. Note that each object instance was placed in a
different initial pose, therefore 0 degree angle may mean
facing left for one instance of an animal, and facing 30
degree right for another instance.

every 10

2.2. Processing

Training and testing samples were generated so as to
carefully remove (or avoid) any potential bias in the data
that might make the task easier than it would be in real-
istic situations. The object masks and their cast shadows
were extracted from the raw images. A scaling factor was
determined for each of the 50 object instances by comput-
ing the bounding box of the union of all the object masks
for all the images of that instance. The scaling factor was
chosen such that the largest dimension of the bounding box
was 80 pixels. This removed the most obvious systematic
bias caused by the variety of sizes of the objects (e.g. most
airplanes were larger than most human gures in absolute
terms). The segmented and normalized objects were then
composited (with their cast shadows) in the center of var-
ious 96  96 pixel background images. In some experi-
ments, the locations, scales, image-plane angle, brightness,
and contrast were randomly perturbed during the composit-
ing process.

2.3. Datasets

Experiments were conducted with four datasets gen-
erated from the normalized object images. The rst two
datasets were for pure categorization experiments (a some-
what unrealistic task), while the last two were for simulta-
neous detection/segmentation/recognition experiments.

every 20

All datasets used 5 instances of each category for train-
ing and the 5 remaining instances for testing. In the normal-
ized dataset, 972 images of each instance were used: 9 ele-
vations, 18 azimuths (0 to 360
), and 6 illumina-
tions, for a total of 24,300 training samples and 24,300 test
samples. In the various jittered datasets, each of the 972 im-
ages of each instance were used to generate additional ex-
amples by randomly perturbing the position ([-3, +3] pix-
els), scale (ratio in [0.8, 1.1]), image-plane angle ([-5, 5] de-
grees), brightness ([-20, 20] shifts of gray levels), contrast
([0.8, 1.3] gain) of the objects during the compositing pro-
cess. Ten drawings of these random parameters were drawn
to generate training sets, and one or two drawings to gener-
ate test sets.

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

Figure 1. The 50 object instances in the NORB dataset. The left side contains the training instances
and the right side the testing instances for each of the 5 categories.

In the textured and cluttered datasets, the objects were
placed on randomly picked background images. In those ex-
periments, a 6-th category was added: background images
with no objects (results are reported for this 6-way classi-
cation). In the textured set, the backgrounds were placed at a
xed disparity, akin to a back wall orthogonal to the camera
axis at a xed distance. In the cluttered datasets, the dispar-
ities were adjusted and randomly picked so that the objects
appeared placed on highly textured horizontal surfaces at
small random distance from that surface. In addition, a ran-
domly picked distractor object from the training set was
placed at the periphery of the image.
 normalized-uniform set: 5 classes, centered, unper-
turbed objects on uniform backgrounds. 24,300 train-
ing samples, 24,300 testing samples. See gure 1.
 jittered-uniform set: 5 classes, random perturbations,
uniform backgrounds. 243,000 training samples (10
drawings) and 24,300 test samples (1 drawing)
 jittered-textured set: 6 classes (including one back-
ground class)
random perturbation, natural back-
ground textures at xed disparity. 291,600 train-
ing samples (10 drawings), 58,320 testing samples (2
drawings). See gure 2.
 jittered-cluttered set: 6 classes (including one back-
ground class), random perturbation, highly cluttered
background images at random disparities, and ran-
domly placed distractor objects around the periphery.
291,600 training samples (10 drawings), 58,320 test-
ing samples (2 drawings). See gure 2.

Occlusions of the central object by the distractor occur oc-
casionally, as can be seen in gure 2. Most experiments
were performed in binocular mode (using left and right im-
ages), but some were performed in monocular mode. In
monocular experiments, the training set and test set were
composed of all left and right images used in the corre-
sponding binocular experiment. Therefore, while the num-
ber of training samples was twice higher, the total amount

of training data was identical. Examples from the jittered-
textured and jittered-cluttered training set are shown in g-
ure 2.

3. Experiments

The following classiers were tested on raw image pairs
from the normalized-uniform dataset: linear classier, K-
Nearest Neighbor (Euclidean distance), pairwise Support
Vector Machines with Gaussian kernels, and Convolutional
Networks [7]. With 18,432 input variables and 24,300 sam-
ples, this dataset is at the upper limit of practicality for
template-matching-based methods such as K-NN and SVM
(in fact, special algorithms had to be implemented to make
them practical). The K-Nearest Neighbor and SVM meth-
ods were also applied to 95-dimensional vectors of PCA
coefcients extracted from the 2  96  96 binocular train-
ing images. All the methods were also applied to Laplacian-
ltered versions of the images, but the results were uni-
formly worse than with raw images and are not reported.

The Convolutional Network was trained and tested on
the normalized-uniform dataset, as well as on the jittered-
uniform and jittered-textured datasets. The jittered training
sets were much too large to be handled by the K-NN and
SVM methods within reasonable limits of CPU time and
memory requirements. In the following sections, we give
brief descriptions of the methods employed. All the algo-
rithms were implemented with the Lush environment [3].
The SVM implementation used the Torch library [6] in-
terfaced to Lush, while the Convolutional Net implementa-
tion used Lushs gblearn2 package.

3.1. Principal Component Analysis

Computing the Principal Components of the dataset for
the PCA-based K-NN and SVM was a major challenge be-
cause it was impossible to manipulate (let alone diagonal-
ize) the 18,43218,432 covariance matrix (2  96  96

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

Figure 2. Some of the 291,600 examples from the jittered-texturedtraining set (top 4 rows), and from
the jittered-clutteredtraining set (bottom 4 rows). (left camera images).

(cid:2)

i min

(cid:3)

(cid:1)

(xi  u)2, (xi + u)2

squared). Fortunately, following [13], we can compute the
principal direction of a centered cloud of points (xi) by
nding two cluster centroids that are symmetric with re-
spect to the origin: we must nd a vector u that mini-
mizes
. A quick solution is
obtained with online (stochastic) algorithms as discussed in
[2] in the context of the K-Means algorithm. Repeated ap-
plications of this method, with projections on the comple-
mentary space spanned by the previously obtained direc-
tions, yield the rst 100 principal components in a few CPU
hours. The rst 29 components thus obtained (the left cam-
era portion) are shown in gure 3. The rst 95 principal
components were used in the experiments.

3.2. K-Nearest Neighbors (with Euclidean Dis-

tance)

Because running the K-Nearest Neighbors algorithm
with 24,300 reference images in dimension 18,432 is pro-
hibitively expensive, we precomputed the distances of
a few representative images Ak to all the other refer-
ence images Xi. By triangular inequality, the distances be-
tween a query image X and all the reference image Xi is
bounded below by Maxk |d(X, Ak)  d(Ak, Xi)|. These
can be used to choose which distances should be com-

puted rst, and to avoid computing distances that are
known to be higher than those of the currently selected ref-
erence points [17]. Experiments were conducted for val-
ues of K up to 18, but the best results were obtained for
K = 1. We also applied K-NN to the 95-dimensional
PCA-derived feature vectors.

3.3. Pairwise Support Vector Machine (SVM)

We applied the SVM method with Gaussian kernels
to the raw images of the normalized-uniform dataset, but
failed to obtain convergence in manageable time due to
the overwhelming dimension, the number of training sam-
ples, and the task complexity. We resorted to using the 95-
dimensional, PCA-derived feature vectors, as well as sub-
sampled, monocular versions of the images at 4848 pixels
and 3232 resolutions.

Ten SVMs were independently trained to classify one
class versus one other class (pairwise classiers). This
greatly reduces the number of samples that must be ex-
amined by each SVM over the more traditional approach
of classifying one class versus all others. During test-
ing, the sample is sent to all 10 classiers. Each classier
votes for one of its attributed categories. The cate-
gory with the largest number of votes wins. The num-
ber of support vectors per classier were between 800 and

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

layers of trainable convolutions and spatial subsampling in-
terspersed with sigmoid non-linearities to extract features
of increasingly large receptive elds, increasing complex-
ity, and increasing robustness to irrelevant variabilities of
the inputs.

A six-layer net, shown in gure 5, was used in the exper-
iments reported here. The layers are respectively named C1,
S2, C3, S4, C5, and output. The C letter indicates a convo-
lutional layer, and the S layer a subsampling layer. C1 has
8 feature maps and uses 55 convolution kernels. The rst
2 maps take input from the left image, the next two from
the right image, and the last 4 from both. S2 is a 44 sub-
sampling layer. C3 has 24 feature maps that use 96 convo-
lution kernels of size 66. Each C3 map takes input from
2 monocular maps and 2 binocular maps on S2, each with
a different combination. S4 is a 33 subsampling layer. C5
has a variable number of maps (80 and 100 in the reported
results) that combine inputs from all map in S4 through
66 kernels. Finally the output layer takes inputs from all
C5 maps. The network has a total of 90,575 trainable pa-
rameters. A full propagation through the network requires
3,896,920 multiply-adds.

The network was trained to minimize the mean squared
error with a set of target outputs. For 5-class recognition
tasks, we used a traditional place code (one unit active, the
other inactive), for 6-class detection/recognition tasks, we
added a 6-th target conguration with all output units inac-
tive for the background class (no object in the center of the
image).

We used a stochastic version of

the Levenberg-
Marquardt algorithm with diagonal approximation of the
Hessian [7], for approximately 250,000 online updates. No
signicant over-training was observed, and no early stop-
ping was performed. For experiments with monocular
data,
im-
age, or vice versa with equal probability.

image was duplicated into the right

the left

4. Results and Discussion

4.1. Results on the normalized-uniform and

jittered-uniform datasets

The results are shown in table 1. To our knowledge, these
are the rst systematic experiments that apply machine
learning to shape-based generic object recognition with in-
variance to pose and lighting. These results are intended as
a baseline for future work with the NORB datasets.

The rst section of the table gives results on the
normalized-uniform database, a somewhat unrealistic set-
ting that assumes that objects can be isolated from their
surroundings and have been size-normalized prior to recog-
nition.

The biggest surprise is that brute-force Nearest Neigh-
bor with Euclidean distance on raw pixels works at all, de-
spite the complex variabilities in the data (see lines 1.1 and
1.2 in the table). Naturally, the classication is horribly ex-
pensive in memory and CPU time.

Another important lesson is that Gaussian SVM becomes
impractical with very large and complex datasets such as
NORB. The Gaussian SVM architecture consists of a layer

Figure 3. The average image and the rst
29 principal eigenvectors of the normalized-
uniformtraining set (only the left camera por-
tions of the vectors are shown).

Classication

exp#
1.0
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
2.0
2.1

exp#
5.1
6.0
6.2

Classier

Linear

K-NN (K=1)
K-NN (K=1)
SVM Gauss
SVM Gauss
SVM Gauss
SVM Gauss
Conv Net 80
Conv Net 100

Input

PCA 95

PCA 95

raw 2x96x96
raw 2x96x96

raw 2x96x96
raw 1x48x48
raw 1x32x32

Dataset
norm-unif
norm-unif
norm-unif
norm-unif
norm-unif
norm-unif
norm-unif
norm-unif
norm-unif
jitt-unif
jitt-unif
Detection/Segmentation/Recognition
Dataset
jitt-text
jitt-clutt
jitt-clutt

raw 2x96x96
raw 2x96x96
raw 2x96x96
raw 2x96x96

raw 2x96x96
raw 2x96x96
raw 1x96x96

Input

Conv Net 100
Conv Net 100
Conv Net 100

Linear

Conv Net 100

Classier

Test Error
30.2%
18.4 %
16.6%
N.C.
13.9%
12.6%
13.3%
6.6%
6.8%
30.6%
7.1%

Test Error
10.6%
16.7%
39.9%

images,

raw binocular
indicates raw monocular

Table 1. Recognition results. raw 2x96x96
raw
indicates
1x96x96
im-
ages, PCA-95 indicates a vector of 95
PCA-derived features. norm-unif refers to
the normalized-uniform dataset, jitt-unif to
the jittered-uniform dataset, jitt-text to the
jittered-textured dataset, and jitt-clutt to
the jittered-cluttered dataset.

2000 on PCA-derived inputs (roughly 2  106 ops to clas-
sify one sample), and between 2000 and 3000 on 3232
raw images (roughly 30  106 ops to classify one sam-
ple). SVMs could not be trained on the jittered datasets
because of the prohibitive size of the training set.

3.4. Convolutional Network

Convolutional Networks [7] have been used with great
success in various image recognition applications, such as
handwriting recognition and face detection. The reader is
refered to the above reference for a general discussion of
Convolutional Nets. Convolutional Nets use a succession of

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

of template matchers, whose prototypes are a subset of the
training samples (i.e. a Gaussian bump is placed around
each training sample), followed by a layer of linear com-
binations with learned weights. Since the supervised learn-
ing only takes place in the linear layer, the objective func-
tion can be made convex (quadratic with box constraints in
the case of traditional SVMs). In fact, an often-stated ad-
vantage of SVMs is the convexity of their objective func-
tion. That property seems to be of little help in our case be-
cause of the difculty of the task (which increases the num-
ber of support vectors), the large number of training sam-
ples, and the fact that the size of the quadratic program to
be solved grows with the square of the number of training
samples. We did not obtain convergence on the raw binoc-
ular data after several days of CPU time using one of the
fastest known implementations of SVMs (from the Torch
package [6]). Experiments with reduced resolution monoc-
ular images yielded decent results around 13% (see lines 1.4
and 1.5 in table 1). Working from the PCA features yielded
similar results (see line 1.6).

Unfortunately, those complexity reductions were still in-
sufcient to allow us experiments with the much larger
jittered-textured and jittered-cluttered trainings set. The per-
formance of SVMs on the jittered test set after training on
the unjittered training set was predictably abysmal (48%
error on PCA features and 34% on raw 14848) PCA-
derived features, which has met with some success in face
recognition, only brought marginal improvements over us-
ing raw pixels with K-NN (from 18.4 to 16.6% error), and
no improvement with SVM.

One should not be misled by the surprisingly good per-
formance of template-based methods on the normalized-
uniform dataset. This dataset is unrealistically favorable to
template-based methods because the lighting conditions are
in small numbers (6) and are exactly identical in the train-
ing set and the test set. Furthermore, the perfectly uniform
backgrounds, perfect object registration, and perfect size
normalization are not likely to be possible in realistic ob-
ject recognition settings.

On the normalized-uniform set, convolutional nets
reached error rates below 7% with binocular inputs (lines
1.7, and 1.8). The error rate was only mildly affected by jit-
tering the training and test samples (7.1% versus 6.8% for
non-jittered). The size of the jittered database was too large
to carry out experiments with the template-based meth-
ods that would result in meaningful comparisons.

4.2. Results on the jittered-textured and jittered-

cluttered datasets

The most challenging task by far was the jittered-
cluttered dataset, and the less challenging jittered-textured
dataset, where the classier must simultaneously de-
tect and recognize objects. The shear size and complex-
ity of these datasets place them above the practical limits
of template-based methods, therefore we only report re-
sults with Convolutional Nets (lines 5.x and 6.x).

A test error rate of 10.6% on the 6 classes (5 objects plus
background) was obtained on the jittered-textured dataset.
A large proportion of errors were objects classied as back-

class
animal
human
plane
truck
car
junk

animal
0.85
0.01
0.01
0.03
0.00
0.01

human
0.02
0.89
0.00
0.00
0.00
0.02

plane
0.01
0.00
0.77
0.00
0.01
0.00

truck
0.00
0.00
0.02
0.84
0.20
0.00

car
0.00
0.00
0.06
0.05
0.69
0.00

junk
0.11
0.10
0.14
0.07
0.09
0.96

Table 2. Confusion matrix on the test set
for the binocular convolutional net on the
jittered-cluttered database (line 6.0 in the re-
sults table). Each row indicates the probabil-
ity that the system will classify an object of
the given category into each of the 6 cate-
gories. Most errors are false negatives (ob-
jects classied as junk), or cars being classi-
ed as trucks.

ground, and cars and space shuttles classied as trucks. A
test error rate of 16.7% was obtained on the highly challeng-
ing jittered-cluttered dataset in binocular mode. An exam-
ple of the internal state of this network is shown in gure 5.
Typical examples of images from the test set and the corre-
sponding answers produced by the system are shown in g-
ure 6.

One signicant surprise is the comparatively poor perfor-
mance of Convolutional Net on the jittered-cluttered dataset
with monocular inputs (line 6.2): the error rate is 39.9%
compared with 16.7% for binocular inputs. This suggests
that the binocular network is able to take advantage of the
disparity information to help locate the outline of the object
and disambiguate the segmentation/classication. In fact, it
can be observed on gure 5 that the last 4 feature maps
in the rst and second layers, which take inputs from both
cameras, seem to be estimating features akin to disparity.

5. Conclusion and Outlook

An important goal of this work is to point out the lim-
itations of popular template-based approaches (including
SVMs) for classication over very large datasets with com-
plex variabilities. Our results emphasize the crucial impor-
tance of trainable local feature extractors for robust and in-
variant recognition.

A real-time portable demo system was implemented us-
ing USB cameras connected to a laptop computer. Convo-
lutional Nets can be scanned over large images very ef-
ciently [7]. Taking advantage of this property, the network
is scanned over input images at multiple scales producing
likelihood maps for each category. The system can spot and
recognize animals, human gures, planes, cars and trucks
in natural scenes with high accuracy at a rate of several
frames per second. By presenting the input image at mul-
tiple scales, the system can detect those objects over a wide
range of scales. Examples of output of this system with nat-
ural images are shown in gure 4. This gure was gener-
ated using the monocular convolutional net trained on the
jitterd-cluttered database (line 6.2 on the results table). Al-

Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR04)
1063-6919/04 $20.00  2004 IEEE

Figure 4. Examples of results on natural images. The list of objects found by the monocular convo-
lutional net is displayed above each sample.

though the raw performance of this network on the database
was quite poor, and despite the fact that it was trained only
with semi-articial data, the system can spot most objects in
the scenes. The network is applied to the image at two dif-
ferent scales, and is scanned over multiple positions at the
large scale. The scores for each class at all scales and posi-
tions are combined to produce an overall likelihood of nd-
ing an object of the class anywhere in the image. The list
of classes whose likelihood exceeds a threshold are shown
above each image in the gure. The gray level of the la-
bel word is indicative of the likelihood.

The NORB dataset opens the door to large-scale exper-
iments with learning-based approaches to invariant object
recognition. This is the rst installment in what promises to
be a long series of works on the subject. Future work will
use trainable classiers that incorporate explicit models of
image formation and geometry.

Acknowledgments We thank Margarita Osadchy, and
David Jacobs for useful comments. This work was sup-
ported by NSF under ITR grant CCR-0325463.

