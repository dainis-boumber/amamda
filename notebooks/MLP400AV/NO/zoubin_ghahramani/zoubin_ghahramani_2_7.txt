Abstract

We present a novel algorithm for agglomer-
ative hierarchical clustering based on evalu-
ating marginal likelihoods of a probabilistic
model. This algorithm has several advan-
tages over traditional distance-based agglom-
erative clustering algorithms. (1) It de(cid:12)nes
a probabilistic model of the data which can
be used to compute the predictive distribu-
tion of a test point and the probability of it
belonging to any of the existing clusters in
the tree. (2) It uses a model-based criterion
to decide on merging clusters rather than an
ad-hoc distance metric. (3) Bayesian hypoth-
esis testing is used to decide which merges
are advantageous and to output the recom-
mended depth of the tree. (4) The algorithm
can be interpreted as a novel fast bottom-up
approximate inference method for a Dirich-
let process (i.e. countably in(cid:12)nite) mixture
model (DPM). It provides a new lower bound
on the marginal likelihood of a DPM by sum-
ming over exponentially many clusterings of
the data in polynomial time. We describe
procedures for learning the model hyperpa-
rameters, computing the predictive distribu-
tion, and extensions to the algorithm. Exper-
imental results on synthetic and real-world
data sets demonstrate useful properties of the
algorithm.

1. Introduction

Hierarchical clustering is one of the most frequently
used methods in unsupervised learning. Given a set of
data points, the output is a binary tree (dendrogram)
whose leaves are the data points and whose internal
nodes represent nested clusters of various sizes. The
tree organizes these clusters hierarchically, where the
hope is that this hierarchy agrees with the intuitive

Appearing in Proceedings of the 22 nd International Confer-
ence on Machine Learning, Bonn, Germany, 2005. Copy-
right 2005 by the author(s)/owner(s).

organization of real-world data. Hierarchical struc-
tures are ubiquitous in the natural world. For ex-
ample, the evolutionary tree of living organisms (and
consequently features of these organisms such as the
sequences of homologous genes) is a natural hierarchy.
Hierarchical structures are also a natural representa-
tion for data which was not generated by evolutionary
processes. For example, internet newsgroups, emails,
or documents from a newswire, can be organized in
increasingly broad topic domains.

The traditional method for hierarchically clustering
data as given in (Duda & Hart, 1973) is a bottom-
up agglomerative algorithm. It starts with each data
point assigned to its own cluster and iteratively merges
the two closest clusters together until all the data be-
longs to a single cluster. The nearest pair of clusters
is chosen based on a given distance measure (e.g. Eu-
clidean distance between cluster means, or distance
between nearest points).

There are several limitations to the traditional hier-
archical clustering algorithm. The algorithm provides
no guide to choosing the \correct" number of clusters
or the level at which to prune the tree. It is often dif-
(cid:12)cult to know which distance metric to choose, espe-
cially for structured data such as images or sequences.
The traditional algorithm does not de(cid:12)ne a probabilis-
tic model of the data, so it is hard to ask how \good"
a clustering is, to compare to other models, to make
predictions and cluster new data into an existing hier-
archy. We use statistical inference to overcome these
limitations. Previous work which uses probabilistic
methods to perform hierarchical clustering is discussed
in section 6.

Our Bayesian hierarchical clustering algorithm uses
marginal likelihoods to decide which clusters to merge
and to avoid over(cid:12)tting. Basically it asks what the
probability is that all the data in a potential merge
were generated from the same mixture component,
and compares this to exponentially many hypotheses
at lower levels of the tree (section 2).

The generative model for our algorithm is a Dirichlet
process mixture model (i.e. a countably in(cid:12)nite mix-

Bayesian Hierarchical Clustering

ture model), and the algorithm can be viewed as a fast
bottom-up agglomerative way of performing approxi-
mate inference in a DPM. Instead of giving weight to
all possible partitions of the data into clusters, which
is intractable and would require the use of sampling
methods, the algorithm e(cid:14)ciently computes the weight
of exponentially many partitions which are consistent
with the tree structure (section 3).

2. Algorithm

Our Bayesian hierarchical clustering algorithm is sim-
ilar to traditional agglomerative clustering in that it is
a one-pass, bottom-up method which initializes each
data point in its own cluster and iteratively merges
pairs of clusters. As we will see, the main di(cid:11)erence is
that our algorithm uses a statistical hypothesis test to
choose which clusters to merge.

Let D = fx(1); : : : ; x(n)g denote the entire data set,
and Di (cid:26) D the set of data points at the leaves of the
subtree Ti. The algorithm is initialized with n trivial
trees, fTi : i = 1 : : : ng each containing a single data
point Di = fx(i)g. At each stage the algorithm consid-
ers merging all pairs of existing trees. For example, if
Ti and Tj are merged into some new tree Tk then the
associated set of data is Dk = Di [Dj (see (cid:12)gure 1(a)).

In considering each merge, two hypotheses are com-
pared. The (cid:12)rst hypothesis, which we will denote Hk
1
is that all the data in Dk were in fact generated in-
dependently and identically from the same probabilis-
tic model, p(xj(cid:18)) with unknown parameters (cid:18). Let us
imagine that this probabilistic model is a multivariate
Gaussian, with parameters (cid:18) = ((cid:22); (cid:6)), although it is
crucial to emphasize that for di(cid:11)erent types of data,
di(cid:11)erent probabilistic models may be appropriate. To
evaluate the probability of the data under this hypoth-
esis we need to specify some prior over the parameters
of the model, p((cid:18)j(cid:12)) with hyperparameters (cid:12). We now
have the ingredients to compute the probability of the
data Dk under Hk
1 :

p(DkjHk

1 ) = Z p(Dkj(cid:18))p((cid:18)j(cid:12))d(cid:18)

= Z h Yx(i)2Dk

p(x(i)j(cid:18))ip((cid:18)j(cid:12))d(cid:18)

(1)

This calculates the probability that all the data in Dk
were generated from the same parameter values as-
suming a model of the form p(xj(cid:18)). This is a natural
model-based criterion for measuring how well the data
(cid:12)t into one cluster. If we choose models with conjugate
priors (e.g. Normal-Inverse-Wishart priors for Normal
continuous data or Dirichlet priors for Multinomial

kD

iT

iD

kT

jT

jD

1

2

3

4

Figure 1. (a) Schematic of a portion of a tree where Ti and Tj
are merged into Tk, and the associated data sets Di and Dj
are merged into Dk. (b) An example tree with 4 data points.
The clusterings (1 2 3)(4) and (1 2)(3)(4) are tree-consistent par-
titions of this data. The clustering (1)(2 3)(4) is not a tree-
consistent partition.

discrete data) this integral is tractable. Throughout
this paper we use such conjugate priors so the inte-
grals are simple functions of su(cid:14)cient statistics of Dk.
For example, in the case of Gaussians, (1) is a function
of the sample mean and covariance of the data in Dk.

The alternative hypothesis to Hk
1 would be that the
data in Dk has two or more clusters in it. Summing
over the exponentially many possible ways of dividing
Dk into two or more clusters is intractable. However,
if we restrict ourselves to clusterings that partition the
data in a manner that is consistent with the subtrees
Ti and Tj, we can compute the sum e(cid:14)ciently using re-
cursion. (We elaborate on the notion of tree-consistent
partitions in section 3 and (cid:12)gure 1(b)). The probabil-
ity of the data under this restricted alternative hy-
pothesis, Hk
2 , is simply a product over the subtrees
p(DkjHk
2 ) = p(DijTi)p(DjjTj) where the probability of
a data set under a tree (e.g. p(DijTi)) is de(cid:12)ned below.

1 and Hk

Combining the probability of the data under hypothe-
ses Hk
2 , weighted by the prior that all points
in Dk belong to one cluster, (cid:25)k
1 ), we obtain
the marginal probability of the data in tree Tk:

def= p(Hk

p(DkjTk) = (cid:25)kp(DkjHk

1 ) + (1 (cid:0) (cid:25)k)p(DijTi)p(DjjTj) (2)

This equation is de(cid:12)ned recursively, there the (cid:12)rst
term considers the hypothesis that there is a single
cluster in Dk and the second term e(cid:14)ciently sums over
all other clusterings of the data in Dk which are con-
sistent with the tree structure (see (cid:12)gure 1(a)).
In
section 3 we show that equation 2 can be used to de-
rive an approximation to the marginal likelihood of a
Dirichlet Process mixture model, and in fact provides
a new lower bound on this marginal likelihood.1 We

1It is important not to confuse the marginal likelihood in
equation 1, which integrates over the parameters of one cluster,
and the marginal likelihood of a DPM, which integrates over all
clusterings and their parameters.

Bayesian Hierarchical Clustering

also show that the prior for the merged hypothesis, (cid:25)k,
can be computed bottom-up in a DPM. The posterior
1 jDk)
probability of the merged hypothesis rk
is obtained using Bayes rule:

def= p(Hk

rk =

(cid:25)kp(DkjHk
1 )

(cid:25)kp(DkjHk

1 ) + (1 (cid:0) (cid:25)k)p(DijTi)p(DjjTj)

(3)

This quantity is used to decide greedily which two trees
to merge, and is also used to determine which merges
in the (cid:12)nal hierarchy structure were justi(cid:12)ed. The
general algorithm is very simple (see (cid:12)gure 2).

input: data D = fx(1) : : : x(n)g, model p(xj(cid:18)),

prior p((cid:18)j(cid:12))

initialize: number of clusters c = n, and

Di = fx(i)g for i = 1 : : : n

while c > 1 do

Find the pair Di and Dj with the highest
probability of the merged hypothesis:

rk =

(cid:25)kp(DkjHk
1 )
p(DkjTk)

Merge Dk   Di [ Dj, Tk   (Ti; Tj)
Delete Di and Dj, c   c (cid:0) 1

end while
output: Bayesian mixture model where each

tree node is a mixture component

The tree can be cut at points where rk < 0:5

Figure 2. Bayesian Hierarchical Clustering Algorithm

Our Bayesian hierarchical clustering algorithm has
many desirable properties which are absent in tradi-
tional hierarchical clustering. For example, it allows us
to de(cid:12)ne predictive distributions for new data points,
it decides which merges are advantageous and suggests
natural places to cut the tree using a statistical model
comparison criterion (via rk), and it can be customized
to di(cid:11)erent kinds of data by choosing appropriate mod-
els for the mixture components.

3. Approximate Inference in a Dirichlet

Process Mixture Model

The above algorithm is an approximate inference
method for Dirichlet Process mixture models (DPM).
Dirichlet Process mixture models consider the limit of
in(cid:12)nitely many components of a (cid:12)nite mixture model.
Allowing in(cid:12)nitely many components makes it possible
to more realistically model the kinds of complicated
distributions which we expect in real problems. We
brie(cid:13)y review DPMs here, starting from (cid:12)nite mixture
models.

Consider a (cid:12)nite mixture model with C components

p(x(i)j(cid:30)) =

C

Xj=1

p(x(i)j(cid:18)j)p(ci = jjp)

(4)

where ci 2 f1; : : : ; Cg is a cluster indicator variable for
data point i, p are the parameters of a multinomial dis-
tribution with p(ci = jjp) = pj, (cid:18)j are the parameters
of the jth component, and (cid:30) = ((cid:18)1; : : : ; (cid:18)C; p). Let the
parameters of each component have conjugate priors
p((cid:18)j(cid:12)) as in section 2, and the multinomial parameters
also have a conjugate Dirichlet prior

p(pj(cid:11)) =

(cid:0)((cid:11))

(cid:0)((cid:11)=C)C

C

Yj=1

p(cid:11)=C(cid:0)1
j

:

(5)

Given a data set D = fx(1) : : : ; x(n)g, the marginal
likelihood for this mixture model is

p(x(i)j(cid:30))# p((cid:30)j(cid:11); (cid:12))d(cid:30)

(6)

p(Dj(cid:11); (cid:12)) =Z " n
Yi=1
where p((cid:30)j(cid:11); (cid:12)) = p(pj(cid:11))QC
p(Dj(cid:11); (cid:12)) =Xc

likelihood can be re-written as

j=1 p((cid:18)jj(cid:12)). This marginal

p(cj(cid:11))p(Djc; (cid:12))

(7)

where c = (c1; : : : ; cn) and p(cj(cid:11)) =R p(cjp)p(pj(cid:11))dp

is a standard Dirichlet integral. The quantity (7) is
well-de(cid:12)ned even in the limit C ! 1. Although the
number of possible settings of c grows as C n and there-
fore diverges as C ! 1, the number of possible ways
of partitioning the n points remains (cid:12)nite (roughly
O(nn)). Using V to denote the set of all possible par-
titioning of n data points, we can re-write (7) as:

p(Dj(cid:11); (cid:12)) = Xv2V

p(vj(cid:11))p(Djv; (cid:12))

(8)

Rasmussen (2000) provides a thorough analysis of
DPMs with Gaussian components, and a Markov chain
Monte Carlo (MCMC) algorithm for sampling from
the partitionings v. DPMs have the interesting prop-
erty that the probability of a new data point belonging
to a cluster is propotional to the number of points al-
ready in that cluster (Blackwell & MacQueen, 1973),
where (cid:11) controls the probability of the new point cre-
ating a new cluster.

For an n point data set, each possible clustering is
a di(cid:11)erent partition of the data, which we can de-
note by placing brackets around data point indices:
e.g. (1 2)(3)(4). Each individual cluster, e.g. (1 2),
is a nonempty subset of data, yielding 2n (cid:0) 1 pos-
sible clusters, which can be combined in many ways

Bayesian Hierarchical Clustering

to form clusterings (i.e. partitions) of the whole data
set. We can organize a subset of these clusters into
a tree. Combining these clusters one can obtain all
tree-consistent partitions of the data (see (cid:12)gure 1(b)).
Rather than summing over all possible partitions of
the data using MCMC, our algorithm computes the
sum over all exponentially many tree-consistent par-
titions for a particular tree built greedily bottom-up.
This can be seen as a fast and deterministic alternative
to MCMC approximations.

Returning to our algorithm, since a DPM with concen-
tration hyperparameter (cid:11) de(cid:12)nes a prior on all parti-
tions of the nk data points in Dk (the value of (cid:11) is
directly related to the expected number of clusters),
the prior on the merged hypothesis is the relative mass
of all nk points belonging to one cluster versus all the
other partitions of those nk data points consistent with
the tree structure. This can be computed bottom-up
as the tree is being built ((cid:12)gure 3).

initialize each leaf i to have di = (cid:11), (cid:25)i = 1
for each internal node k do
dk = (cid:11)(cid:0)(nk) + dleftk drightk
(cid:25)k = (cid:11)(cid:0)(nk)

dk

end for

Figure 3. Algorithm for computing prior on merging, where
rightk (leftk) indexes the right (left) subtree of Tk and
drightk (dleftk ) is the value of d computed for the right (left)
child of internal node k.

Lemma 1 The marginal likelihood of a DPM is:

p(Dk) = Xv2V

=1 (cid:0)(nv
 )

(cid:11)mvQmv
h (cid:0)(nk+(cid:11))
(cid:0)((cid:11)) i

p(Dv
 )

mv

Y=1

where V is the set of all possible partitionings of Dk,
mv is the number of clusters in partitioning v, and nv

is the number of points in cluster  of partitioning v.

This follows from equation (8) where the (cid:12)rst (frac-
tional) term in the sum is p(v), the second (product)
term is p(Dkjv), and the explicit dependence on (cid:11) and
(cid:12) has been dropped.

Theorem 1 The
Bayesian Hierarchical Clustering algorithm is:

quantity (2)

computed by the

Proof Rewriting equation (2) using algorithm 3 to
substitute in for (cid:25)k we obtain:

p(DkjTk) = p(DkjHk
1 )

(cid:11)(cid:0)(nk)

dk

+ p(DijTi)p(DjjTj)

didj
dk

We will proceed to give a proof by induction. In the
base case, at a leaf node, the second term in this equa-
tion drops out since there are no subtrees. (cid:0)(nk =
1) = 1 and dk = (cid:11) yielding p(DkjTk) = p(DkjHk
1 ) as
we should expect at a leaf node.

For the inductive step, we note that the (cid:12)rst term is al-
ways just the trivial partition with all nk points into a
single cluster. According to our inductive hypothesis:

p(DijTi) = Xv0 2VTi

(cid:11)m

v

0 Q

0

v

m
0 =1
di

0

(cid:0)(nv

0 )

0

p(Dv

0 )

m

0

v

Y0 =1

and similarly for p(DjjTj), where VTi (VTj ) is the set of
all tree-consistent partitionings of Di (Dj). Combining
terms we obtain:

p(DijTi)p(DjjTj)

didj
dk

m

0

v

Y0 =1
Y00 =1

m

v

00

0

v

00

1

=

(cid:11)m

dk 0
@ Xv0 2VTi
(cid:2)0
B@ Xv00 2VTj
(cid:11)mvQmv
= Xv2VNTT

(cid:11)m

v

=1 (cid:0)(nv
 )
dk

p(Dv
 )

mv

Y=1

0

(cid:0)(nv

0 )

p(Dv

00

(cid:0)(nv

00 )

p(Dv

m

0

v

Y0 =1
Y00 =1

m

v

00

0

0 )1
A
00 )1
CA

00

where VNTT is the set of all non-trivial tree-consistent
partionings of Dk. For the trivial partition, mv = 1
and nv
1 = nk. By combining the trivial and non-trivial
terms we get a sum over all tree-consistent partitions
yielding the result in Theorem 1. This completes the
proof. Another way to get this result is to expand out
p(DjT ) and substitute for (cid:25) using algorithm 3.

Corollary 1 For any binary tree Tk with the data
points in Dk at its leaves, the following is a lower
bound on the marginal likelihood of a DPM:

dk(cid:0)((cid:11))

(cid:0)(nk + (cid:11))

p(DkjTk) (cid:20) p(Dk)

p(DkjTk) = Xv2VT

(cid:11)mvQmv

=1 (cid:0)(nv
 )
dk

mv

Y=1

p(Dv
 )

where VT is the set of all tree-consistent partitionings
of Dk.

Proof Proof of Corollary 1 follows trivially by multi-
plying p(DkjTk) by a ratio of its denominator and the
dk(cid:0)((cid:11))
denominator from p(Dk) from lemma 1 (i.e.
(cid:0)(nk+(cid:11)) ),
and from the fact that tree-consistent partitions are a
subset of all partitions of the data.

Bayesian Hierarchical Clustering

Proposition 1 The number of tree-consistent parti-
tions is exponential in the number of data points for
balanced binary trees.

Proof If Ti has Ci tree-consistent partitions of Di and
Tj has Cj tree-consistent partitions of Dj, then Tk =
(Ti; Tj) merging the two has CiCj + 1 tree-consistent
partitions of Dk = Di [ Dj, obtained by combining all
partitions and adding the partition where all data in
Dk are in one cluster. At the leaves Ci = 1. Therefore,
for a balanced binary tree of depth  the number of
tree-consistent partitions grows as O(22
) whereas the
number of data points n grows as O(2)

In summary, p(DjT ) sums the probabilities for all tree-
consistent partitions, weighted by the prior mass as-
signed to each partition by the DPM. The computa-
tional complexity of constructing the tree is O(n2),
the complexity of computing the marginal likelihood
is O(n log n) , and the complexity of computing the
predictive distribution (see section 4) is O(n).

4. Learning and Prediction

@(cid:12)

Learning Hyperparameters. For any given set-
ting of the hyperparameters, the root node of the tree
approximates the probability of the data given those
particular hyperparameters. In our model the hyper-
paramters are the concentration parameter (cid:11) from the
DPM, and the hyperparameters (cid:12) of the probabilistic
model de(cid:12)ning each component of the mixture. We
can use the root node marginal likelihood p(DjT ) to
do model comparison between di(cid:11)erent settings of the
hyperparameters. For a (cid:12)xed tree we can optimize over
the hyperparameters by taking gradients. The gradi-
ent @p(DkjTk)
combines the results of this computation
at the subtrees of Tk with @p(DkjHk
1 )
. These gradients
can be computed bottom-up as the tree is being built.
Similarly, @p(DkjTk)
@(cid:11) , which in turn de-
pends on @di
@(cid:11) , which can be propagated up from the
the subtrees. This allows us to construct an EM-like
algorithm where we (cid:12)nd the best tree structure in the
(Viterbi-like) E step and then optimize over the hy-
perparameters in the M step. In our experiments we
have only optimized one of the hyperparameters with
a simple line search for Gaussian components. A sim-
ple empirical approach is to set the hyperparameters (cid:12)
by (cid:12)tting a single model to the whole data set. Details
on hyperparameter optimization can be found in our
tech report (Heller & Ghahramani, 2005).

depends on @(cid:25)k

@(cid:11)

@(cid:12)

Predictive Distribution. For any tree, the prob-
ability of a new test point given the data can be
computed by recursing through the tree starting at

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

y
t
i
r
u
P

0.05
3800

3700

3600

3500

3300

3400
3200
Marginal Likelihood

3100

3000

2900

2800

Figure 4. Log marginal likelihood (evidence) vs. purity over 50
iterations of hyperparameter optimization

the root node. Each node k represents a cluster,
with an associated predictive distribution p(xjDk) =

R p(xj(cid:18))p((cid:18)jDk; (cid:12))d(cid:18). The overall predictive distribu-

tion sums over all nodes weighted by their posterior
probabilities:

p(xjD) = Xk2N

!k p(xjDk)

(9)

rkQi2Nk

where N is the set of all nodes in the tree, !k

def=
(1 (cid:0) ri) is the weight on cluster k, and Nk is
the set of nodes on the path from the root node to
the parent of node k. This expression can be derived
by rearranging the sum over all tree-consistent parti-
tionings into a sum over all clusters in the tree (noting
that a cluster can appear in many partitionings).

For Gaussian components with conjugate priors, this
results in a predictive distribution which is a mixture
of multivariate t distributions. We show some exam-
ples of this in the Results section.

5. Results

We compared Bayesian hierarchical clustering to tra-
ditional hierarchical clustering using average, single,
and complete linkage, using a euclidean distance met-
ric, over 5 datasets (4 real and 1 synthetic). We also
compared our algorithm to average linkage hierarchi-
cal clustering on several toy 2D problems ((cid:12)gure 5).
On these problems we were able to compare the di(cid:11)er-
ent hierarchies generated by the two algorithms, and
visualize clusterings and predictive distributions.

The 4 real datasets we used are the spambase (100 ran-
dom examples from each class, 2 classes, 57 attributes)
and glass (214 examples, 7 classes, 9 attributes)
datasets from the UCI repository, the CEDAR Buf-
falo digits (20 random examples from each class, 10
classes, 64 attributes), and the CMU 20Newsgroups
dataset (120 examples, 4 classes - rec.sport.baseball,

Bayesian Hierarchical Clustering

Data Set

Single Linkage Complete Linkage Average Linkage

BHC

Synthetic
Newsgroups
Spambase
3Digits
10Digits
Glass

0:599 (cid:6) 0:033
0:275 (cid:6) 0:001
0:598 (cid:6) 0:017
0:545 (cid:6) 0:015
0:224 (cid:6) 0:004
0:478 (cid:6) 0:009

0:634 (cid:6) 0:024
0:315 (cid:6) 0:008
0:699 (cid:6) 0:017
0:654 (cid:6) 0:013
0:299 (cid:6) 0:006
0:476 (cid:6) 0:009

0:668 (cid:6) 0:040
0:282 (cid:6) 0:002
0:668 (cid:6) 0:019
0:742 (cid:6) 0:018
0:342 (cid:6) 0:005
0:491 (cid:6) 0:009

0:828 (cid:6) 0:025
0:465 (cid:6) 0:016
0:728 (cid:6) 0:029
0:807 (cid:6) 0:022
0:393 (cid:6) 0:015
0:467 (cid:6) 0:011

Table 1. Purity scores for 3 kinds of traditional agglomerative clustering, and Bayesian hierarchical clustering. The mean scores
over 5-fold cross-validation along with standard errors are shown.

rec.sport.hockey, rec.autos, and sci.space, 500 at-
tributes). We also used synthetic data generated from
a mixture of Gaussians (200 examples, 4 classes, 2
attributes). The synthetic, glass and toy datasets
were modeled using Gaussians, while the digits, spam-
base, and newsgroup datasets were binarized and mod-
eled using Bernoullis. We binarized the digits dataset
by thresholding at a greyscale value of 128 out of 0
through 255, and the spambase dataset by whether
each attribute value was zero or non-zero. We ran
the algorithms on 3 digits (0,2,4), and all 10 digits.
The newsgroup dataset was constucted using Rain-
bow (McCallum, 1996), where a stop list was used
and words appearing fewer than 5 times were ignored.
The dataset was then binarized based on word pres-
ence/absence in a document. For these classi(cid:12)cation
datasets, where labels for the data points are known,
we computed a measure between 0 and 1 of how well a
dendrogram clusters the known labels called the den-
drogram purity.2 We found that the marginal like-
lihood of the tree structure for the data was highly
correlated with the purity. Over 50 iterations with dif-
ferent hyperparameters this correlation was 0:888 (see
(cid:12)gure 4). Table 1 shows the results on these datasets.

On all datasets except Glass BHC found the highest
purity trees. For Glass, the Gaussian assumption may
have been poor. This highlights the importance of
model choice. Similarly, for classical distance-based
hierarchical clustering methods, a poor choice of dis-
tance metric may result in poor clusterings.

Another advantage of the BHC algorithm, not fully
addressed by purity scores alone, is that it tends to
create hierarchies with good structure, particularly at
high levels. Figure 6 compares the top three levels (last

2Let T be a tree with leaves 1; : : : ; n and c1; : : : ; cn be the
known discrete class labels for the data points at the leaves. Pick
a leaf  uniformly at random; pick another leaf j uniformly in
the same class, i.e. c = cj . Find the smallest subtree containing
 and j. Measure the fraction of leaves in that subtree which are
in the same class (c). The expected value of this fraction is the
dendrogram purity, and can be computed exactly in a bottom
up recursion on the dendrogram. The purity is 1 i(cid:11) all leaves in
each class are contained in some pure subtree.

All Data

354

Game
Team
Play

446

Car
Space
NASA

205

149

284

162

Baseball
Pitch
Hit

NHL
Hockey
Round

Car
Dealer
Drive

Space
NASA
Orbit

All Data

1

Quebec
Jet
Boston

799

Car
Baseball
Engine

2

797

Pitcher
Boston
Ball

Car
Player
Space

1

Vehicle
Dealer
Driver

796

Team
Game
Hockey

Figure 6. Top level structure, of BHC (left) vs. Average Linkage
HC, for the newsgroup dataset. The 3 words shown at each
node have the highest mutual information between the cluster
of documents at that node versus its sibling, and occur with
higher frequency in that cluster. The number of documents at
each cluster is also given.

three merges) of the newsgroups hierarchy (using 800
examples and the 50 words with highest information
gain) from BHC and ALHC. Continuing to look at
lower levels does not improve ALHC. CLHC and SLHC
perform similarly to ALHC. Full dendrograms for this
dataset and dendrograms of the 3digits dataset are
available in the tech report (Heller & Ghahramani,
2005).

6. Related Work

The work in this paper is related to and inspired by
several previous probabilistic approaches to cluster-
ing3, which we brie(cid:13)y review here. Stolcke and Omo-
hundro (1993) described an algorithm for agglomera-
tive model merging based on marginal likelihoods in
the context of hidden Markov model structure induc-
tion . Williams (2000) and Neal (2003) describe Gaus-
sian and di(cid:11)usion-based hierarchical generative mod-
els, respectively, for which inference can be done us-
ing MCMC methods. Similarly, Kemp et al. (2004)
present a hierarchical generative model for data based
on a mutation process.

Ban(cid:12)eld and Raftery (1993) present an approximate

3There has also been a considerable amount of decision tree
based work on Bayesian tree structures for classi(cid:12)cation and
regression, but this is not closely related to work presented here.

Bayesian Hierarchical Clustering

method based on the likelihood ratio test statistic
to compute the marginal likelihood for c and c (cid:0) 1
clusters and use this in an agglomerative algorithm.
Vaithyanathan and Dom (2000) perform hierarchical
clustering of multinomial data consisting of a vector of
features. The clusters are speci(cid:12)ed in terms of which
subset of features have common distributions.

Segal et al. (2002) present probabilistic abstraction
hierarchies (PAH) which learn a hierarchical model
in which each node contains a probabilistic model
and the hierarchy favors placing similar models at
neighboring nodes in the tree (as measured by a dis-
tance function between probabilstic models).
Ra-
moni et al. (2002) present an agglomerative algorithm
for merging time series based on greedily maximizing
marginal likelihood. Friedman (2003) has also recently
proposed a greedy agglomerative algorithm based on
marginal likelihood which simultaneously clusters rows
and columns of gene expression data.

The algorithm in our paper is di(cid:11)erent from the above
algorithms in several ways. First, unlike (Williams,
2000; Neal, 2003; Kemp et al., 2004) it is not in fact a
hierarchical generative model of the data, but rather
a hierarchical way of organizing nested clusters. Sec-
ond our algorithm is derived from Dirichlet process
mixtures. Third the hypothesis test at the core of
our algorithm tests between a single merged hypoth-
esis and the alternative is exponentially many other
clusterings of the same data (not one vs two clusters
at each stage). Lastly, our algorithm does not use any
iterative method, like EM, or require sampling, like
MCMC, and is therefore signi(cid:12)cantly faster than most
of the above algorithms.

7. Discussion

We have presented a novel algorithm for Bayesian hier-
archical clustering based on Dirichlet process mixtures.
This algorithm has several advantages over traditional
approaches, which we have highlighted throughout the
paper. We have presented prediction and hyperparam-
eter optimization procedures and shown that the al-
gorithm provides competitive clusterings of real-world
data as measured by purity with respect to known la-
bels. The algorithm can also be seen as an extremely
fast alternative to MCMC inference in DPMs.

The limitations of our algorithm include its inher-
ent greediness, a computational complexity which is
quadratic in the number of data points, and the lack
of any incorporation of tree uncertainty.

In future work, we plan to try BHC on more com-
plex component models for other realistic data|this

is likely to require approximations of the component
marginal likelihoods (1). We also plan to extend BHC
to systematically incorporate hyperparameter opti-
mization and improve the running time to O(n log n)
by exploiting a randomized version of the algorithm.
We need to compare this novel, fast inference algo-
rithm for DPMs to other inference algorithms such
as MCMC (Rasmussen, 2000), EP (Minka & Ghahra-
mani, 2003) and Variational Bayes (Blei & Jordan,
2004). We also hope to explore the idea of computing
several alternative tree structures in order to create
a manipulable tradeo(cid:11) between computation time and
tightness of our lower bound. There are many exciting
avenues for further work in this area.

Acknowledgements: Thanks to David MacKay,
members of the Gatsby Unit, and members of CALD
at CMU for useful comments. This project was par-
tially supported by the EU PASCAL Network of Ex-
cellence and ZG was partially supported at CMU by
DARPA under the CALO project.

