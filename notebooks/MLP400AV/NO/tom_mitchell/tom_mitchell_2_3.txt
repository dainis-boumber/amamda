Abstract. Learning provides a useful tool for the automatic design of autonomous robots.
Recent research on learning robot control has predominantly focussed on learning single tasks
that were studied in isolation. If robots encounter a multitude of control learning tasks over
their entire lifetime, however, there is an opportunity to transfer knowledge between them. In
order to do so, robots may learn the invariants of the individual tasks and environments. This
task-independent knowledge can be employed to bias generalization when learning control,
which reduces the need for real-world experimentation. We argue that knowledge transfer is
essential if robots are to learn control with moderate learning times in complex scenarios. Two
approaches to lifelong robot learning which both capture invariant knowledge about the robot
and its environments are presented. Both approaches have been evaluated using a HERO-
2000 mobile robot. Learning tasks included navigation in unknown indoor environments and
a simple nd-and-fetch task.

1 Why Learning?

Traditionally, it has often been assumed in robotics research that accurate a priori
knowledge about the robot, its sensors, and most important its environment is avail-
able. By assuming the availability of accurate models of both the world and the robot
itself, the kind of environments such robots can operate in, and consequently the
kind of tasks such robots can solve, is limited. Limitations especially arise from four
factors:

 Knowledge bottleneck. A human designer had to provide accurate models of

the world and the robot.

 Engineering bottleneck. Even if sufciently detailed knowledge is available,
making it computer-accessible,i.e., hand-coding explicit models of robot hard-
ware, sensors and environments, has often been found to require unreasonable
amounts of programming time.

 Tractability bottleneck. It was early recognized that many realistic robot do-
mains are too complex to be handled efciently [Schwartz et al., 1987], [Canny,

1 This paper is also available as Technical Report IAI-TR-93-7, University of Bonn, Dept.

of Computer Science III, March 1993.

1987], [Bylander, 1991]. Computational tractability turned out to be a severe ob-
stacle for designing control structures for complex robots in complex domains,
and robots were far from being reactive.

 Precision bottleneck. The robot device must be precise enough to accurately

execute plans that were generated using the internal models of the world.

Recent research on autonomous robots has changed the design of autonomous agents
and pointed out a promising direction for future research in robotics (see for exam-
ple [Brooks, 1989] and several papers in [Maes, 1991]). Reactivity and real-time
operation have received considerably more attention than, for example, optimal-
ity and completeness. Many approaches have dropped the assumption that perfect
world knowledge is availablesome systems even operate in the extreme where no
domain-specic initial knowledge is available at all. Consequently, todays robots
are facing gradually unknown, hostile environments, they have to orient themselves,
explore their environments autonomously, recover from failures, and they have to
solve whole families of tasks.

If robots lack initial knowledge about themselves and their environments, learning
becomes inevitable. The term learning refers to a variety of algorithms that are
characterized by their ability to replace missing or incorrect world knowledge by
experimentation, observation, and generalization. Learning robots thus collect parts
of their domain knowledge themselves and improve over time. They are less depen-
dent on a human instructor to provide this knowledge beforehand. A learning robot
architecture is typically exible enough to deal with a whole class of environments,
robots and/or tasks (goals). Consequently the internal prior knowledge, if available
at all, is often too weak to solve a concrete problem off-line. In order to reach a
goal, learning robots rely on the interaction with their environment to extract in-
formation. Different learning strategies differ mainly in three aspects: their way to
do experimentation (exploration), their way to generalize from the observed experi-
ence, and the type and amount of prior knowledge that constrains the space of their
internal hypothesizes about the world. There will be no optimal, general learning
technique for autonomous robots, since learning techniques are characterized by a
trade-off between the degree of exibility, given by the size of the gaps in the
domain knowledge, and the amount of observations required for lling these gaps.
Generally speaking, the more universal a robot learning architecture, the more exper-
imentation we expect the robot to take to learn successfully, and vice versa. However,
the advantage of applying learning strategies to autonomous robot agents is obvious:
Learning robots can operate in a whole class of initially unknown environments,
and they can compensate for changes, since they can re-adjust their internal belief
about the environment and themselves. Moreover, all empirically learned knowledge
is grounded in the real world. It has long been recognized in AI that learning is a
key feature for making autonomous agents capable of solving more complex tasks
in more realistic environments. Recent research has produced a variety of rigorous
learning techniques that allow a robot to acquire huge chunks of knowledge by it-
self. See for example [Mel, 1989], [Moore, 1990], [Pomerleau, 1989], [Tan, 1991],

Figure 1 The robot control learning problem. A robot agent is able to perceive the
state of its environment by its sensors, and change it using its effectors (actions). A
reward function that maps sensations to rewards measures the performance of the
robot. The control learning problem is the problem of nding a control policy that
generates actions such that the reward is maximized over time.

[Mahadevan and Connell, 1991], [Lin, 1992b], and [Kuipers and Byun, 1990].

What exactly is the concrete problem addressed by robot learning? Let us outline a
general denition of learning robot control. Assume the robot acts in an environment
W (the world). Each time step, the environment is in a certain state z  Z. By state
we mean the sum of all quantities in the environment that may change during
the lifetime of the robot. The robot is able to (partially) perceive the state of its
environment through its sensors. It is also able to act using its effectors, as shown
in Figure 1. Let S denes the set of all possible sensations, and A the set of all
actions that the robot can execute. Actions (including zero-actions, if the robot does
nothing) change the state of the world. Hence, the environment can be understood as
a mapping W : Z  A (cid:0)! Z from states and actions to states. For example, imagine
an autonomous mobile robot whose task it is to keep the oor clean. The worlds such
a robot will be acting in are buildings, including the obstacles surrounding the robot,
the oors, the dirt on the oors, humans that walk by, and so on. Appropriate sensors
might include a camera mounted on the robots arm, and the set of all sensations
might be the space of all camera images. Actions could be go forward, turn,
switch on/off vacuum, and lift arm.

In order to dene the goals of the robot, we assume that the robot is given a reward
function R : S (cid:0)! IR, that maps sensations to scalar reward values. The reward
evaluates the success of the robot to solve its tasks: In the most simple form the
reward is positive (say 100), if the robot reaches its goals, it is negative (-100) if the
robot fails, and zero otherwise. Positive reward corresponds to pleasure, and negative
reward represents pain. In the mobile robot domain, for example, the reward may be
positive if there is no dirt on the oor, and negative reward may be received if the
robot collides with the furniture or runs out of battery power. The control learning

problem is the problem of nding a control function F that generates actions such
that the reward is maximized over time. More formally, a control learning problem
can be described in the following way:

Control learning problem:
hS; A; W; Ri (cid:0)! F : S  ! A
such that F maximizes R over time

This formulation of the robot control learning problem has been extensively studied
in the eld of Reinforcement Learning. [Sutton, 1984], [Barto et al., 1991]. Thus
far, most approaches that emerged from this eld have studied robot learning with
a minimal set of assumptions: The robot is able to sense, it is able to execute
actions, actions have an effect on future sensations, and there is a pre-given reward
function that denes the goals of the robot. The goal of reinforcement learning
is to maximize the reward over time. Note that this general denition lacks any
specications about the robot at hand, its environment, or the kind of reward functions
the robot is expected to face. Hence, approaches that are able to adaptively solve such
a robot control learning problem are general learning techniques. Perhaps the most
restricting assumption found in most approaches to reinforcement learning is that
the robot is able to sense the state of the world reliably. If this is the case, it sufces
to learn the policy as a function of the most recent sensation to action: F : S ! A,
i.e., the control policy is purely reactive. As Barto et al. pointed out [Barto et al.,
1991], the problem of learning a control policy can then be attacked by Dynamic
Programming techniques [Bellman, 1957].

As it turns out, even if robots have access to complete state descriptions of their
environments, learning control in complex robot worlds with large state spaces is
practically not feasible. This is because it takes often too much experimentation
to acquire the knowledge required for maximizing reward, i.e., to ll the huge
knowledge gaps, and robot hardware is slow. One might argue that better learning
techniques have to be invented that decrease the learning complexity, while still
being general. Althoughthere is certainly space for better learning and generalization
techniques that gradually decrease the amount of experimentation required, it seems
unlikely that such techniques will ever be applicable to complex, real-world robot
environments with sparse reward and difcult tasks. The complexity of knowledge
acquisition is inherent in the formulation of the problem. Real-world experimentation
will be the central bottleneck of any general learning technique that does not utilize
prior knowledge about the robot and its environment.

Why do natural agents such as humans learn so much better than articial agents?
Maybe a better question to ask is: Is the learning problem faced by natural agents
any simpler than that of articial ones? We will not attempt to give general answers
to these general questions. We will, however, point out the importance of knowledge
transfer and lifelong learning problems in order to make robots to learn more complex
control.

2 The Necessity of Lifelong Agent Learning

Humans typically encounter a multitude of control learning problems over their
entire lifetime, and so do robots. Henceforth, in this paper we will be interested
in the lifelong learning problem faced by a robot agent, in which it must learn a
collection of control policies for a variety of related control tasks. Each of these
control problems, hS; A; Wi; Rii involves the same robot with the same set of
sensors, effectors, and may vary only in the particular environment Wi, and in the
reward function Ri that denes the goal states for this problem. For example, an
industrial mobile robot might face multiplelearning tasks such as shipping packages,
delivering mail, supervising critical processes, guarding its work-place at night, and
so on. In this scenario the environment will be the same for all tasks, but the
reward function varies. Alternatively, a window-cleaning robot that is able to climb
fronts of buildings and move arbitrarily on walls and windows might have the very
single task of cleaning windows. It will, however, face multiple fronts and windows
(environments) over its lifetime.

In the lifelong learning problem, for each different environment and each reward
function the robot agent must nd a different control policy, Fi. The lifelong learning
problem of the agent therefore corresponds to a set of control learning problems:

Lifelong learning problem:
fhS; A; Wi; Rii (cid:0)! FijFi : S ! Ag
such that Fi maximizes Ri over time

Of course the agent could approach the lifelong learning problem by handling each
control learning problem independently. However, there is an opportunity for the
agent to do considerably better. Because these control learning problems are dened
in terms of the same S, A, and potentially the same W , the agent should be able to
reduce the difculty of solving the i-th control learning problem by using knowledge
it acquired from solving earlier control learning problems. For example, a robot that
must learn to deliver laser printer output and to collect trash in the same environment
should be able to use much of what it has learned from the rst task to simplify
learning the second. The problem of lifelong learning offers the opportunity for
synergy among the different control learning problems, which can speed-up learning
over the lifetime of the agent.

Viewing robot learning as a lifelong learning problem motivates research on boot-
strapping learning algorithms that transfer learned knowledge from one learning
task to another. Bootstrapping learning algorithms might start with low-complexity
learning problems, and gradually increase the problem solving power to harder prob-
lems. Similar to humans, future robots might rst have to learn simple tasks (such
as low-level navigation, hand-eye coordination), and, once successful, draw their at-
tention to increasingly more difcult and complex learning tasks (such as picking up

Figure 2 The robot we used in all experiments described in this paper is a wheeled
HERO-2000 robot with a manipulator and a gripper. It is equipped with two sonar
sensors, one on the top of the robot that can be directed by a rotating mirror to
give a full 360 sweep (24 values), and one on the wrist that can be rotated by the
manipulator to give a 90 sweep. Sonar sensors return approximate echo distances.
Such sensors are inexpensive but very noisy.

and delivering laser printer output). As, for example, Singh [Singh, 1992b] and Lin
[Lin, 1992b] demonstrate, learning related control tasks with increasing complexity
can result in a remarkable synergy between these control learning tasks, and an im-
proved problem solving power. In their experiments, simulated mobile robots were
able to solve more complex navigation tasks when the robots were given simpler,
related learning tasks beforehand. They also report that their systems were unable to
learn the same complex tasks in isolation, pointing out the importance of knowledge
transfer for robot learning.

The reminder of the paper is organized as follows. In the next two sections, we will
briey present two approaches to the lifelong learning problem. Both approaches
have been implemented and evaluated using a HERO-2000 mobile robot with a ma-
nipulator shown in Figure 2. In the rst approach, called explanation-based neural
network learning (EBNN), we will assume that the environment of the agent stays the
same for all control learning tasks. This allows the robot to learn task-independent
action models. Once learned, these action models provide a means of transferring
knowledge across control learning tasks. In EBNN, such action models are used to
explain and analyze observations. In Section 4, we drop the assumption of static
environments. Instead, we describe a mobile autonomous robot that has to solve the

Figure 3 Episode: Starting with the initial state s1,
the action sequence
a1; a2; : : : ; an(cid:0)1 was observed to produce the nal reward Rn. The robot agent
uses its action models, which capture previous experiences in the same environment,
to explain the observed episode and thus bias learning. See text for an explanation.

same task in different, related environments. We demonstrate how this robot might
learn and transfer environment-independent knowledge that captures the characteris-
tics of its sensors, as well as invariant characteristics of the environments. In Section
5 we will review some related approaches to robot learning which also utilize previ-
ously learned knowledge. As we will see, there are several types of techniques that
can be grouped into categories. The paper is concluded by Section 6

3 Explaining Observations in Terms of Prior Experiences

As dened in the previous section, the lifelong learning problem faced by a learning
robot agent is the problem of learning collections of tasks in families of environments
over the entire lifetime. In this section we will draw our attention to a particular sub-
type of lifelong learning problems, namely to the lifelong learning problem of robots
that spend their whole life exclusively in the same environment. This restricted
class of lifelong learning scenarios plays an important role in autonomous agent
research. Many prospective robot agents, such as housekeeping robots, industrial
robot arms or articial insects, face a variety of learning problems in the very
same environment. In such scenarios, knowledge about the environment can be
reused, since the environment stays the same for each task. The explanation-based
neural network learning algorithm (EBNN), which will be presented in this section,
transfers task-independent knowledge via learned action models which are learned
empirically during problem solving.

3.1 Learning Action Models

Robots observe their environments (and themselves) by the effects of their actions.
In other words, each time an action is performed, the robot may use its sensors

to sense the way the world has changed. Let us assume for simplicity that the
robot is able to accurately sense the state of the world. As pointed out in the
previous section, learning control reduces to learning a reactive controller4. If the
environment is sufciently predictable, neural network learning techniques such
as the Back-propagation training procedure [Rumelhart et al., 1986] can be used
to model the environment. More specically, each time an action is performed, the
previous state denoted by s, the action a and the next state s form a training example
for the action model network, denoted by M :

h s; a i (cid:0)! s

Action models5 are thus functions of the type M : S  A (cid:0)! S. Since we assume
that the environment is the same for all tasks, action models capture invariants that
allow for transferring knowledge from one task to another.

Each individual control learning problem requires a different policy, i.e., learning
a control function Fi : S (cid:0)! A that, when employed by the agent, maximizes
the corresponding reward Ri. In general, Fi is difcult to learn directly. Following
the ideas of reinforcement learning ([Samuel, 1959], [Sutton, 1988], [Barto et al.,
1990], [Watkins, 1989]), we decompose the problem of learning Fi into the problem
of learning an evaluation function, Qi, dened over states and actions.

Qi : S A (cid:0)! IR, where Qis; a is the expected future cumulative reward
achieved after executing action a in state s.

Assume for a moment the agent had learned an accurate evaluation function Qi.
Then it can easily use this function to select optimal actions that maximize reward
over time. Given some state, s, that the agent nds itself in, it computes its control
action a by considering its available actions to determine which of them produces
the highest Qi value:

a = argmaxaA Qis; a

Since Qis; a measures the future cumulative reward, a is the optimal action if
Qis; a is correct. The problem of learning a policy is thus reduced to the problem
of learning an evaluation function.

How can an agent learn the evaluation function Qi? To see how training data for
learning Qi might be obtained, consider the scenario depicted in Figure 3. The robot

4 We intentionally avoid the complex problem of incomplete and noisy perception perception,
since the algorithm presented in this section is kind of orthogonal to research on these issues.
See [Bachrach and Mozer, 1991], [Chrisman, 1992], [Lin and Mitchell, 1992], [Mozer and
Bachrach, 1989], [Rivest and Schapire, 1987], [Tan, 1991], [Whitehead and Ballard, 1991]
for approaches to learning with incomplete perception.

5 See for example [Barto et al., 1989], [Jordan, 1989], [Munro, 1987], [Thrun, 1992] for

more approaches to learning action models with neural networks.

agent begins at the initial state s1 and performs the action sequence a1; a2; : : : ; an.
After action an it receives the reward rn = 92:3 which, in this example, indicates
that the action sequence was considerably successful. At a rst glance, this episode
can be used by the agent to derive training examples for the evaluation function Qi
by associating the nal reward6 with each state-action pair:

h s1; a1 i (cid:0)! rn = 92:3
h s2; a2 i (cid:0)! rn = 92:3

...

h sn(cid:0)1; an(cid:0)1 i (cid:0)! rn = 92:3

(1)

An inductive learning method, such as the Back-propagation training procedure, can
use such training examples to learn Qi. As the number of training examples grows,
the agents internal version of Qi will improve, resulting in it choosing increasingly
effective actions.7 Notice that for each control learning problem hS; A; Wi; Rii, the
agent must learn a distinct Qi, since the reward differs for different tasks.

3.2 The Explanation-Based Neural Network Learning Algorithm

How can the agent use its previously learned knowledge, namely the neural network
action models, to guide learning of the evaluation function Qi? As pointed out
earlier in this paper, we are interested in the lifelong learning problem. Since we
assume throughout this section that the environment is the same for all individual
control learning problems, neural network action models capture important domain
knowledge that is independent of the particular control learning problem at hand.
In the explanation-based neural network learning algorithm (EBNN), the agent uses
these action models to bias learning of the control functions.

6 For simplication of the notation, we assume that reward will only be received at the end of
an episode. EBNN can be applied to arbitrary reward functions. See [Mitchell and Thrun,
1993b] for more details.

7 Note that more sophisticated learning schemes for learning evaluation functions have
been developed. In his dissertation, Watkins [Watkins, 1989] describes Q-Learning, a
scheme for learning evaluation function Qisk; ak  recursively. In Q-Learning train-
ing patterns are derived based on the maximum possible Q value at the next state:
hsk; aki (cid:0)! maxa Qsk+1; a. Indeed, in all our experiments we applied a linear combi-
nation of Watkins recursive scheme and the non-recursive scheme described in the paper.
This combination is strongly related to Suttons T D algorithms [Sutton, 1988]. Since
the exact procedure is not essential for the ideas presented in this paper, we will omit any
details. A second extension, also used widely, is to discount reward over time. If actions
are to be chosen such that the number of actions is minimal, reward is typically discounted
with a discount factor   1. The resulting control policy consequently prefers sooner
reward to more distant reward. See [Mitchell and Thrun, 1993b] or [Thrun and Mitchell,
1993] for a more detailed description of these issues.

Figure 4 Fitting slopes: Let f be a target function for which three examples
hx1; fx1i, hx2; fx2i, and hx3; fx3i are known. Based on these points the
learner might generate the hypothesis g. If the output-input derivatives are also
known, the learner can do much better: h.

EBNN works as follows. Suppose the robot faces the control learning problem num-
ber i, i.e., it has to learn the evaluation function Qi. The learning scheme described
above provides training values for the desired evaluation function. Repetitive exper-
imentation allows the robot to collect enough data to learn the desired Qi. However,
this process does not utilize the knowledge represented in the action models. As-
sume the agent has learned already accurate action models that model the effect of
actions on the state of the environment. Of course, these action models will only
be approximately correct, since they are learned inductively from a nite amount of
training data. In EBNN, the agent employs these action models to explain, analyze
and generalize better from the observed episodes. This is done in the following three
steps:

1. Explain. An explanation is a post-facto prediction of the observed state-action
sequence [DeJong and Mooney, 1986], [Mitchell et al., 1986], [Mitchell and
Thrun, 1993a]. Starting with the initial state-action pair s1; a1, the agent post-
facto predicts subsequent states up to the nal state sn using its neural network
action models. Since the action models are only approximately correct, predic-
tions will deviate from the observed states.

2. Analyze. Having explained the whole episode, the explanation is analyzed to
extract further information that is useful for learning the evaluation function Qi.
In particular, the agent analyzes how a small change of the states features will
affect the nal reward, and thus the value of the evaluation function. This is done
by extracting partial derivatives (slopes) of the target function Qi with respect to
the observed states in the episode: First, the agent computes the partial derivative
of the nal reward with respect to the nal state sn. Notice that the reward
function Rs, including its derivative, is assumed to be given to the agent.
These slopes are now propagated backwards through the chain of action model
inferences. Neural network action models represent differentiable functions.
Using the chain rule of differentiation, the agent computes the partial derivative
of the nal reward with respect to the preceding state sn(cid:0)1 by multiplying the
partial derivative of the reward with the derivative of the neural network action

model. This process is iterated, yielding all partial derivatives of the nal reward
along the whole episode:

@Qi
@sn

sn(cid:0)1; an(cid:0)1 
...
s1; a1 

@Qi
@s1

@R
@s
@M

@s

   

@R
@s

sn 

@M
@s

sn(cid:0)1; an(cid:0)1

sn 

@M
@s

sn(cid:0)1; an(cid:0)1 

s2; a2 

@M

@s

s1; a1

Here M : S  A (cid:0)! S denotes the neural network action model. The reward-
state slopes analyze the importance of the state features for the nal reward.
State features believed (by the action models) to be irrelevant to achieving the
nal reward will have partial derivatives of zero, whereas large derivative values
indicate the presence of strongly relevant features.

3. Learn. The analytically extracted slopes approximate the slopes of the target
evaluation function Qi. Figure 4 illustrates the importance of the slope informa-
tion of the target function. Suppose the unknown target function is the function
f depicted in Figure 4a, and suppose that three training examples are given: x1,
x2 and x3. An arbitrary continuous function approximator, for example a neural
network, might hypothesize the function g shown in Figure 4b. If the slopes at
these points are known as well, then the resulting function might be much better,
as illustrated in Figure 4c.
EBNN uses a combined learning scheme utilizing both types of training infor-
mation. The target values (c.f. Equation 2) for learning Qi are generated from
observation, whereas the target slopes, given by

r h sn(cid:0)1; an(cid:0)1 i (cid:0)!
...

r h s1; a1 i (cid:0)!

@R

@s

sn 

@M

@s

sn(cid:0)1; an(cid:0)1

@R

@s

sn 

n(cid:0)1

Y

k=1

@M

@s

sk; ak  ;

are extracted from analyzing the observations using domain knowledge acquired
in previous control learning tasks. Both sources of training information, the
target values and the target slopes, are used to update the weights and biases
of the target network.8 Consequently, the domain knowledge is used to bias
the generalization. Since this bias is knowledgeable, it will partially replace the
need for real-world experimentation, hence accelerate learning.

8 As Simard and colleagues pointed out, the Back-propagation algorithm can be extended to
t target slopes as well as target values [Simard et al., 1992]. Their algorithm tangent prop
incrementally updates weights and biases of a neural network such that both the value and
the slope error are simultaneously minimized.

3.3 Accommodating Imperfect Action Models

Initial experiments with EBNN on a simulated robot navigation task showed a
signicant speedup in learning when the robot agent had access to highly accurate
action models [Mitchell and Thrun, 1993b]. If the action models are not sufciently
accurate, however, the robot performance can seriously suffer from the analysis. This
is because the extracted slopes might very well be wrong and mislead generalization.
This observations raises an essential question for research on lifelong agent learning
and knowledge transfer: How can a robot agent deal with incorrect prior knowledge?
Clearly, if the agent lacks training experience, any inductively learned bias might be
poor and misleading. But even in the worst case, a learning mechanism that employs
previously learned domain knowledge should not take more time for learning control
than a learning mechanism that does not utilize prior knowledge at all. How can the
learner avoid the damaging effects arising from poor prior knowledge?

In EBNN, malicious slopes are identied and their inuence is gradually reduced.
More specically, the accuracy of the extracted slopes is estimated based upon the
observed prediction error of the action models. For example, if the action models
perfectly post-facto predict the observed episode, the estimated accuracy of all slopes
will be 1. Likewise, if for some of the action models in the chain of model derivatives
have produced inaccurate state predictions, the corresponding estimated accuracy
will be close to zero. The accuracies of the slopes are now used when training the
target network. Since tangent prop allows to weight each training pattern individually,
the estimated accuracies can be used to determine the ratio with which value learning
and slope learning are weighted when learning the target concept. More specically,
in EBNN the step-size for weight updates is multiplied by the estimated slope
accuracy when learning slopes.

As illustrated elsewhere [Mitchell and Thrun, 1993b], weighting slope training by
their accuracies was found to successfully reduce the impact of malicious slopes
resulting from inaccurate action models. We evaluated EBNN using nine different
sets of action models that were trained with different amounts of training data.
With well-trained action models in the simulated robot domain, the same speedup
was observed as before. With increasing inaccurate action models, the performance
of EBNN approached that of standard reinforcement learning without knowledge
transfer. In these experiments, EBNN degraded gracefully with increasing errors in
the action models. These results are intriguing since they indicate the feasibility of
lifelong learning algorithms that are able to benet from previously learned bias,
even if this bias is poor.

3.4 A Concrete Example: Learning to Pick up a Cup

We will present some initial results obtained with our HERO-2000 robot. Thus far,
we predominantly investigated the effect of previously learned knowledge on the

Figure 5 The robot uses its manipulator and a sonar sensor to sense the distance to
a cup and to pick it up.

learning speed for new control learning tasks. We therefore trained the action mod-
els beforehand with manually collected training data. The robot agent had to learn
a policy for approaching and grasping a cup. The robot at hand, shown again in
Figure 5, used a hand-mounted sonar sensor to observe its environment. Sonar data
was preprocessed to estimate the direction and distance to the closest object, which
was used as the world state description. Robot actions were forward(inches),
turn(degrees), and grab. Positive reward was received for successful grasps,
and negative reward for unsuccessful grasps as well as for losing sight of the
cup. In this experiment, actions were modeled by three separate networks, one
for each action. The networks for the parameterized actions forward(inches)
and turn(degrees) predicted the distance and the orientation to the cup (one
hidden layer with 6 units), whereas the model for grab predicted the probability
that a pre-given, open-loop grasping routing would manage to pick up the cup (four
hidden units). All action models were pre-learned from approximately two hundred
training episodes containing an average of ve steps each, which were manually
collected beforehand. Figure 6 shows as an example the action model for the grab
action. Since this particular action model modeled the probability of success of the
grasping routine, the reward function was simply the identity mapping Rs = s
(with the constant derivative 1). The evaluation function Q was also modeled by
three distinct networks, one for each action (8 hidden units each). After learning the
action models, the six training episodes for the evaluation networks shown in Figure
7 were provided by a human teacher who controlled the robot. We applied a version
of T D [Sutton, 1988] with  = 0:7 and Watkins Q-Learning [Watkins, 1989]

GRAB: model

1
1

0.5
0.5

0
0

-0.5
-0.5

-1
-1
-45 deg
5 deg

3
3

13
13

0 deg
0 deg

45 deg 127
45 deg 127

Figure 6 Action model for the action model grab. The x and y axis measures again
the angle and distance to the cup. The z axis plots the expected success of the grasp
action, i.e., the probability that the grasping succeeds.

with experience replay [Lin, 1992a] for learning control.

Figure 8 illustrates the learned Q function for the grab action with (right row) and
without (left row) employing the action models and EBNN. In this initial stage of
learning, when little data is yet available, the generalization bias from the pre-learned
action models is apparent. Although none of the Q functions has yet converged, the
Q functions learned using EBNN have a shape that is more correct, and which is
unlikely to be guessed based solely on the few observed training points with no
initial knowledge. For example, even after presenting six episodes the plain learning
procedure predicts positive reward solely based upon the angle of the cup, whereas
the combined EBNN method has already learned that grasping will fail if the cup is
too far away. This information, however, is not represented in the training episodes
(Figure 7), since there is no single example of an attempt to grasp a cup far away.
It rather seems that the slopes of the model were copied into the target evaluation
function. This illustrates the effect of the slopes in EBNN: The evaluation functions
learned with EBNN discovered the correlation of the distance of the cup and the
success of the grab action from the neural network action model. If these action
models were learned in an earlier control learning tasks, there would be a signicant
synergy effect between them.

The EBNN results in this section are initial. They are presented because they indicate
that task-independent knowledge, once learned, can be successfully transferred by
EBNN when learning the grasping task. They are also presented since they evaluated
EBNN in a real robot domain, unlike the results presented in [Mitchell and Thrun,
1993b], [Thrun and Mitchell, 1993]. However, thus far we did not collect enough

3

Training episodes

6

6

4

4

2+

2

3

3+

5+

5

1

1

e
c
n
a
t
s
d

i

13

turn

forward

127

-45 deg

0 deg

angle

45 deg

Figure 7 Six training episodes for learning control, labeled by 1 to 6. The horizontal
axis measures the angle of the cup, relative to the robots body, and the vertical
axis measures the distance to the cup in a logarithmic scale. Successful grasps are
labeled with +, unsuccessful with . Notice that some of the episodes included
forwarding and turning.

training data to learn a complete policy for approaching and grasping the cup with
either method. Future research will characterize the synergy during the full course
of learning until convergence. It will also experiment with families of related tasks,
such as approaching and grasping different objects, including cups that lie on the
side.

3.5 EBNN and Lifelong Robot Learning

What lesson does EBNN teach us in the context of lifelong agent learning? In this
section we made the restricting assumption that all control learning problems of the
robot agent play in the very same environment. If this is the case, any type of models
of the robot and its environment are promising candidates for transferring task-
invariant knowledge between the individual control learning problems. In EBNN,
task-independent information is represented by neural network action models. These
models bias generalization when learning control via the process of explaining and
analyzing observed episodes. EBNN is a method for lifelong agent learning, since
it learns and re-uses learned knowledge that is independent of the particular control
learning problem at hand. Although the initial experiments described in this paper
do not fully demonstrate this point, EBNN is able to efciently replace real-world

GRAB: Q-function

without analytical training, 2 episodes

with analytical training, 2 episodes

1
1
0.5
5
0
0
-0.5
.5
-1
-1
-45 deg
eg

1
1
0.5
5
0
0
-0.5
.5
-1
-1
-45 deg
eg

3
3

13
13

1
1
0.5
5
0
0
-0.5
.5
-1
-1
-45 deg
eg

0 deg
0 deg

0 deg
0 deg

45 deg127
45 deg127

45 deg127
45 deg127

6 episodes

6 episodes

3
3

13
13

1
1
0.5
5
0
0
-0.5
.5
-1
-1
-45 deg
eg

0 deg
0 deg

0 deg
0 deg

45 deg127
45 deg127

45 deg127
45 deg127

3
3

13
13

3
3

13
13

Figure 8 Evaluation functions for the action grab after presenting 2 (upper row) and
6 (lower row) training episodes, and without (left column) and with (right column)
using the action model for biasing the generalization.

experimentation by previously learned bias. In related experiments with a simulated
robot presented elsewhere [Mitchell and Thrun, 1993b], we observed a signicant
speed-up by a factor of 3 to 4 when pre-learned action models were employed.

It is important to mention that we expect EBNN to be even more efcient if the
dimension of the state space is higher. This is because for each single observation
EBNN extracts a d-dimensional slope vector from the environment, if d if the
dimension of the input space. Our conjecture is that with accurate domain theories
the generalization improvement scales linearly with the number of instance features
(e.g., we would expect a three order of magnitude improvement for a network with
1000 input features), since each derivative extracted by EBNN can be viewed roughly
as summarizing information equivalent to one new training example for each of the
d input dimensions. This conjecture is roughly consistent with our experiments in

the simulated robot domain. The relative performance improvement might increase
even more if higher-order derivatives are extracted, which is not yet the case in
EBNN. It is important to notice, however, that the quality of the action models is
crucial for the performance of EBNN. With inaccurate action models, EBNN will not
perform better than a purely inductive learning procedure. This does not surprise.
Approaches to the lifelong learning problem will always be at most as efcient
as non-transferring approaches the robot solves its rst control learning problem
in its life. The desired synergy effect occurs later, when the agent has learned an
appropriate domain-specic bias, when it is more mature.

4 Lifelong Learning in Multiple Environments

In the previous section we focussed on a certain type of lifelong robot learning
problems, namely problems which deal with a single environment. We will now focus
on a more general type of lifelong-learning scenarios, in which the environments
differ for the individual control learning tasks. As pointed out in Section 2 , there
are quite a few robot learning scenarios where a robot has to learn control in whole
families of environments. For example, a vacuum cleaning robot might have to learn
to clean different buildings. Alternatively, an autonomous vehicle might have to
learn navigation in several types of terrain.

Multi-environment scenarios provide less possibilities for transferring knowledge.
At a rst glance, transferring knowledge seems hard, if the environment is not
the same for all control learning tasks. But even in this type problems there are
invariants that may be learned and used as a bias. The key observation is that all
lifelong learning scenarios involve the same robot, the same effectors, the same
sensors, although they might have to deal with a variety of environments and control
learning tasks therein. Approaches to this type of lifelong learning problems thus
aim at learning the characteristics of the sensors and effectors of the robot, as well
as invariants in the environments, if there are any. The principle of learning and
transferring task-independent knowledge is the same as in the previous sectionjust
the type of knowledge that is transferred differs.

So far, there has been little systematic research on this general type of lifelong robot
learning scenarios. In the remainder of this section we will not describe a general
learning mechanism, but a particular approach to learning the characteristics of the
robots sensors and the environments. Using the HERO-2000 robot as an example,
we will demonstrate how inverse sensor models can represent a knowledgeable bias
which is independent of the particular environment at hand.

Figure 9 The robot explores an unknown environment. Note the obstacle in the
middle of the laboratory. Our lab causes many malicious sonar values, and is a hard
testbed for sonar-based navigation. For example, some of the chairs absorb sound
almost completely, and are thus hard to detect by sonar.

4.1 Learning to Interpret Sensations

What kind of invariants can be learned across multiple environments? Two observa-
tions are crucial for the approach described here. First, the robot and its sensors are
the same for each environment. Second, there might be regularities in the environ-
ments that can be learned as well. In this section we will describe a neural network
approach to learning the characteristics of the robots sensors, as well as those of
typical indoor environments.

The task of the mobile HERO-2000 robot is to explore unknown buildings [Thrun,
1993]. Facing a new indoor environment such as the laboratory environment depicted
in Figure 9, the robot has to wander around and to use its sensors to avoid collisions.
In the exploration task the robot uses two of its sensors: A rotating sonar sensor is
mounted on the head of the robot, as shown in Figure 2. The robot also monitors
its wheels encoders to detect stuck or slipping wheels. Negative reward is received
for collision which can be detected using the wheel encoders. Positive reward is
received for entering regions where the robot has not been before. Initially, the robot
does not possess any knowledge about its sensors and the environments it will face
throughout its life. Sensations are uninterpreted 24-dimensional vectors of oats
(sonar scans), along with a single bit that encodes the state of the wheels. In order
to simplify learning, we assume that the robot has access to its x-y- coordinates

(b)

Figure 10 Task-independent knowledge: (a) sensor interpretation network R and
(b) condence network C.

in a global reference frame ( measures the orientation of the robot).9 Navigation
in unknown environments is clearly a lifelong robot learning problem. Initially, the
robot has to experience collisions, since the initial knowledge does not sufce to
prevent from them. Collisions will be penalized by negative reward. In order to
transfer knowledge, the robot then has to learn how to interpret its sensors in order
to prevent collisions. This knowledge is re-used for each environment the robot will
face over its lifetime. After some initial experimentation, the robot should be able
to maneuver in new, unknown world while successfully avoiding collisions with
obstacles.

We will now describe a pair of networks which learn sensor-specic knowledge
that can be re-used across multiple environment. The sensor interpretation function,
denoted by R, maps sensor informationin our case a sonar scanto reward infor-
mation. More specically, R evaluates for arbitrary locations close to the robot the
probability for a collision, based on a single sonar scan. Figure 10a shows R. Input
to the network is a vector of sonar values, together with the coordinates of the query
point x; y relative to the robots local coordinate frame. The output of the net-
work is 1, if the interpretation predicts a collision for this point, and 0, if the network
predicts free-space. This function can be learned by standard supervised learning
algorithms, if the robot keeps track of all sensor readings and of all locations where
it collided (and where it did not collide). As the robot operates, it constructs maps
that label occupied regions and free-space, which is used to form training examples
for the sensor interpretation network R. As usual, the robot uses Back-propagation
to learn R.

In order to prevent the precious robot hardware from real-world collisions, we

9 If the robot wheels are perfect, this x-y- position can be calculated internally by dead
reckoning. The robot at hand, however, is not precise enough, and after 10 to 20 minutes
of operation the real coordinated usually deviate signicantly from the internal estimates.
An approach to compensating such control errors is briey described in [Thrun, 1993].

(a) sensor interpretation

1

4

(b) condence

1

4

2

5

2

5

3

6

3

6

10.5 feet

-

Figure 11 Capturing domain knowledge in the networks R and C. (a) Sensor
interpretations and (b) condence values are shown for the following examples: 1.
hallway, 2. hallway with open door. 3. hallway with human walking by, 4. corner of
a room, 5. corner with obstacle, 6. several obstacles. Lines indicate sonar measure-
ments (distances), and the region darkness represents in (a) the expected collision
reward for surrounding areas (dark values indicate negative reward), and in (b) the
condence level (dark values indicate low condence).

designed a robot simulator and used it for generating the training patterns for the
interpretation network. In the simulator the whole robot environment is known, and
training examples that map sensations to occupancy information can be extracted
easily. In a few minutes the simulated robot explored its simulated world, collecting

225

200

175

150

125

100

75

175

150

125

100

75

50

25

0

80

100

120

140

160

180

200

0

20

40

60

80

100

120

140

Figure 12 Map building: (a) Raw, noisy sensor input on an exploration path with
27 measurements. (b) Resulting model, corresponding to the lab shown in Figure 1b
(lab doorway is toward bottom left). The path of the robot is also plotted (from right
to left), demonstrating the exploration of the initially unknown lab. In the next steps,
the robot will pass the door of the lab and explore the hallway.

a total of 8 192 training examples. Six examples for sensor interpretation using R
are shown in Figure 11a. The circle in the center represents the robot, and the lines
orthogonal to the robot represent distances sensed by the sonars. The probability of
negative reward, as predicted by the R, is displayed in the circular regions around
the robot: The darker the region, the higher the probability of collision. As can
be seen from this gure, the network R has successfully learned how to interpret
sonar signals. If sonar values are small (meaning that the sonar signal bounced back
early), the network predicts an obstacle nearby. Likewise, large value readings are
interpreted as free-space. The network has also learned invariants in the training
environments. For example, typical sizes of walls are known. In Figure 11a-1,
for example, the robot predicts a long obstacle of a certain width, but behind this
obstacle it predicts a considerably low probability for collision. This prediction is
surprising, given that sonar sensors cannot see through obstacles. In the training
environments, however, regions behind walls happened to be free fairly often, which
explains the X-ray predictions by the network. This provides clear evidence that the
sensor interpretation network does not only represent knowledge about the robots
sensors, but also knowledge about certain invariants of the environments at hand.

Figure 13 Lab and oor: (a) model, and (b) condence map.

4.2 Building Maps

We will now motivate the need for a second network, the so-called condence
network C, which is related to R. As can bee seen from Figure 11a, a single sonar
scan can be used to build a small local map around the robot. If the robot moves
around and takes several readings, the resulting interpretations can be combined to
form a map10 of the world. However, there will be points in the world where the
interpretations from multiple sensations will disagree. There are two main reasons for
conicting interpretations: First, sonar sensors, like any sensor, are noisy devices.
They often fail to detect certain objects such as smooth surfaces or objects with
absorbing surfaces like some of our ofce chairs. Second, sonars are blind for
areas behind obstacles. Hence interpretations for regions behind an obstacle will
be inaccuratethey truly reect the average probability of occupancy (the prior).
This observation makes it necessary to design a mechanism that resolves conicts
between different predictions.

We will approach this problem by explicitly modeling the reliability of the inter-
pretations, and using the reliability as weighting factor when combining multiple
interpretations. More specically, consider a testing phase for the interpretation net-
work R. For some of the testing examples, R will manage to predict the target
value closely. For others, however, there will be a signicant residual error for the

10 See [Moravec, 1988] and [Elfes, 1987] for related approaches to map building and robot

navigation.

Figure 14 Map compiled during the rst AAAI autonomous robot competition in
San Jose, July 1992. The map reects the knowledge of the robot after Stage 3 of
the competition. In this and the previous stage, the robot had to nd and approach
ten visually marked poles, while avoiding collision with obstacles. Lines mark the
boundary of the competition area. The map is somehow inaccurate since it was built
on two separate days, and the locations of the robot and the obstacles were not quite
identical. We compensated for such inaccuracies by decaying the condence over
time.

reasons given above. This error is used to train a second network C, which is called
condence network. The input to C is the same as the input to R. The target output is
the (normalized) prediction error of R. After training, C thus predicts the expected
deviation of the sensor interpretations, denoted by Cs; x; y. The condence
into these interpretation Rs; x; y is thus given by (cid:0) ln Cs; x; y. When
multiple sensor interpretations are combined, the individual interpretation values are
averaged, weighted by their condence value. Figure 11b shows condence values
for the interpretations showed in Figure 11a. Here dark regions correspond to low
condence. Likewise, light regions indicate high condence. As can be seen from
these examples, the condence in regions behind obstacles is generally low. Low
condence is also predicted for boundary regions between free-space and obstacles,
This does not surprise, since sonar values detect objects in a 15 cone. They do
not tell where in the cone an object was found. Consequently, the ne-structure of
objects is hard to predict. Similar to the sensor interpretation network, the con-

dence network represents knowledge about the sensors and the environments of the
robot that can be transferred across environments. It is important to notice that both
networks, R and C, represent learned knowledge that is independent of the particular
environment at hand. These networks act as a bias in the modeling process, when
the robot constructs an internal model of a new environment.

The Figures 12 to 14 show some example maps that were obtained for different
environments. We used the networks R and C to nd models of our lab (9). A simple
non-adaptive algorithm was used for exploration that basically planned minimal-
cost plans to the closest unexplored region. Models of the lab and the hallway are
shown in Figures 12 and 13. Although both networks were trained in simulation, they
successfully prevented the robot from collidingwith obstacles. The same networks R
and C were used on a autonomous robot competition, that was held during the AAAI
conference in July 1992 in San Jose. Here the task and environment differed from
the environments the robot had seen previously: The environment was a large arena
lled with paper boxes, and the task was to nd and to navigate to 10 visually marked
poles. The robot had no information about the location of the obstacles and the poles.
It had to explore and model the environment by itself. The nal implementation (we
named the robot Odysseus) was far more complex than what is described here.
Odysseus navigation was map-based, and the adaptive map building procedure was
a component of Odysseus control. The robot employed the same networks that were
generated by the simulator and tested in our experiments in the lab. After a total of 40
minutes operation during two separate stages of the competition the robot produced
the map shown in Figure 14. The networks produced maps that were accurate enough
to protect the robot from any collision with an obstacle. They also allowed the robot
to nd close-to-optimal paths to the goal locations.

4.3 Sensor Interpretation and Lifelong Robot Learning

What is the contribution of this approach to the problem of lifelong agent learning?
Of course, learning sensor interpretation does not necessarily have to be viewed as a
method for learning bias, and the presented method is by no means a general learning
scheme for lifelong learning problems. However, for the purpose of this paper we
will characterize map building in terms of lifelong robot learning. Obviously in each
new environment the robot clearly has to learn control. This is because initially,
when the robot faces a new, unknown environment, its knowledge does clearly not
sufce to generate actions that maximize reward. The robot then gradually learns a
policy for action generation step-by-step, and the internal maps provide the freedom
for learning, the knowledge gaps lled during the course of interaction with
the world. Building internal two-dimensional occupancy maps, together with the
static planning routine that generates action using these maps, is learning control.
Thus, learning neural network sensor interpretations offers a promising perspective
for research in lifelong robot learning in multiple environments. Although both
the environments and the goals differed for the individual robot control tasks in

our experiments, we have demonstrated that knowledge transfer could drastically
reduce real-world experimentation. The robot did know about collisions in both real-
world environments without ever having experienced one, solely based on previously
learned knowledge. We believe that methods which acquire and utilize models of the
sensors, effectors (not demonstrated here), and invariants in the environments are
promising candidates for efcient robot learning in more complex lifelong learning
scenarios.

5 Related Work

Approaches to knowledge transfer in the Lifelong Learning Problems can be viewed
as techniques for acquiring function approximation bias. Various researchers have
noted the importance of learning bias and transferring knowledge across multiple
robot control learning tasks. They can roughly be grouped into the following cate-
gories:

1. Learning models. Action models are perhaps the most straightforward way to
learn and transfer task-independent knowledge, if all individual control learning
tasks deal with a single environment. Approaches that utilize action models dif-
fer in the type of action models they employ, and the way the action models are
used to bias learning control. Sutton [Sutton, 1990] presents a system that learns
action models, like EBNN. He uses these models for synthesizing hypothetical
experiences that rene the control policy. In his experiments he found a tremen-
dous speedup in learning control when using these action models. EBNN differs
from this approach in that it uses its action models for explaining real-world ob-
servations, and in that provides a mechanism to recover from errors in the action
models. Lin [Lin, 1992a] describes a mechanism where past experience is mem-
orized and repeatedly replayed when learning control. The collection of past
experience forms a non-generalizing action model. Lin also reports signicant
speedups when using his replay mechanism. As mentioned above, experience
replay was also used for neural network training in EBNN. Thus far, there has
been little research on learning models that can act as a bias in lifelong learning
problems with multiple robot environments.

2. Learning behaviors and abstractions. A second way of learning and transfer-
ring knowledge across tasks are behaviors. Behaviors are controllers (policies)
with low complexityOften the term behavior refers to reactive controllers. Re-
inforcement learning, for example, can be viewed as a technique for learning
behaviors. Behaviors form abstract action spaces, since the basic actions of the
robot might be replaced by the action of invoking a behavior. Thus, with appro-
priate behaviors abstract action spaces can be formed, and hierarchies of actions
can be identied. Learning behaviors accelerates learning control by restricting
the search space of all possible policies, mainly for two reasons. First, the num-
ber of behaviors is often smaller than the number of actions. Second, behaviors

typically are selected for longer periods of time. The latter argument is usually
more important and provides a stronger bias for learning control.
Singh [Singh, 1992b], [Singh, 1992a] reports a technique to learn complex tasks
by rst learning controllers for simple tasks based on reinforcement learning.
These controllers represent reactive behaviors. The high-level policy is learned
in the abstract action space formed by the low level behaviors. In order to rep-
resent even the high-level controller as a purely reactive function, Singh makes
several restricting assumptions on the type of tasks and sensor information. In
his doctoral thesis, Lin [Lin, 1992b] describes a related scheme for learning be-
haviors, action hierarchies and abstraction. He assumes that a human instructor
teaches a robot a set of elemental behaviors which sufce for all tasks which the
robot will face over its lifetime. Unlike Singh, his approach does not guarantee
that optimal controller can be learned in the limit. Recently, Dayan and Hinton
[Dayan and Hinton, 1993] proposed a system that uses a pre-given hierarchical
decomposition to learn control on different levels of abstraction. Each level of
abstraction differs in the grain-size of the sensory information, resulting in dif-
ferently specialized controllers. Since their system learns reactive controllers on
each level using reinforcement learning algorithms, it is unclear for what type
of problems this procedure will learn successfully, since essential information
may be missed when providing incomplete sensor information to purely reactive
controllers.

3. Learning inductive function approximation bias. Another, more straightfor-
ward approach to learning and transferring knowledge is to learn the inductive
bias of the function approximators used for learning control directly. Atkeson
[Atkeson, 1991] presents a scheme for learning distance measures for instance-
based, local approximation schemes. In his algorithm, scaling factors are learned
that allows to weight different input features differently. Sutton [Sutton, 1992]
reports a family of learning schemes that allow to learn inductive bias similar
to Kalman lters [Kalman, 1960]. Although he did not describe his methods in
the context of learning control, his research has been motivated by transferring
knowledge across multiple control learning tasks.

4. Learning representations. Representations, together with inductive bias, de-
termine the way a function approximator generalizes from examples. Many re-
searchers have focussed on learning appropriate representations in order to learn
bias. For example, Pratt [Pratt, 1993] describes several approaches that allow to
re-use learned representations in hidden units of neural networks. Although she
could empirically demonstrate that this transfer could signicantly reduce the
number of training epochs required for the convergence of the Back-propagation
algorithm, she only found occasional improvements in the generalization. A sim-
ilar technique is reported by Sharkey and Sharkey [Sharkey and Sharkey, 1992].
Some researchers have studied knowledge transfer if several tasks are learned
simultaneously. For example, Suddarth and Kergosien [Suddarth and Kergosien,
1990] demonstrated that multiple learning tasks of certain types can success-
fully guide and improve generalization. In his approach, he gives hints to neural

networks in form of additional output units that learn a closely related task.
These hints constrain the internal representation developed by the network. In a
more general way, Caruana [Caruana, 1993] recently proposed to learn whole
collections of tasks in parallel, using a shared internal representation. He con-
jectures that multi-task learning will make neural network learning algorithms
scale to more complex learning tasks.

Both approaches to the lifelong learning problem described in this paper fall into the
rst category. In EBNN, the robot learns action models which bias generalization
in the evaluation functions. Sensor interpretation functions are inverse models of
the sensors and the environments of the robot. In the robot exploration tasks, the
robot thus learns models of itself and typical aspects of its environments, which
bias the construction of the individual maps. As pointed out earlier, action models
are well-suited if the environment is the same for all learning tasks, whereas sensor
models are appropriate for lifelong agent learning in multiple environments.

6 Conclusion

In this paper we have presented a lifelong learning perspective for autonomous
robots. We propose not to study robot learning problems in isolation, but in the
context of the multitude of learning problems that a robot will face over its lifetime.
Lifelong robot learning opens the opportunity for transfer of learned knowledge.
This knowledge may be used as a bias when learning control. Although control
learning methods that allow the transfer of knowledge are more complex as most
algorithms that solve isolated control learning problems, robot learning itself be-
comes easier. Robots that memorize and transfer knowledge rely less on real-world
experimentation and thus learn faster. This is because previously learned knowledge
may act as a knowledgeable bias that may partially replace the pure syntactic bias
of inductive learning algorithms.

We have demonstrated with two concrete approaches the potential synergy effect of
knowledge transfer. These approaches addressed two main types of lifelong agent
learning scenarios, namely those that are dened in a single environment, and those
that are not. We strongly believe that knowledge transfer is essential for scaling robot
learning algorithms to more realistic and complex domains. Exploiting previously
learned knowledge simplies learning control. These results support our fundamental
claim that learning becomes easier, if it is embedded into a lifelong learning context.

Acknowledgment

We thank the CMU Robot Learning Group and the Odysseus team at CMU for invalu-
able discussion that contributed to this research. We also thank Ryusuke Masuoka

Figure 15 (a) The University of Bonn robot Rhino (manufactured by Real World
Interface, Inc.), (b) Map (approximately 20  30 meters) constructed by Rhino at
the AAAI-94 robot competition, using the technique described in this paper.

for his invaluable help in rening EBNN.

This research was sponsored in part by the Avionics Lab, Wright Research and De-
velopment Center, Aeronautical Systems Division (AFSC), U. S. Air Force, Wright-
Patterson AFB, OH 45433-6543 under Contract F33615-90-C-1465, Arpa Order
No. 7597 and by a grant from Siemens Corporation. The views and conclusions
contained in this document are those of the authors and should not be interpreted as
representing the ofcial policies, either expressed or implied, of the U.S. Government
or Siemens Corp.

Addendum

Since this paper was submitted, EBNN was successfully applied to a variety of real-
world learning tasks. In [Mitchell and Thrun, 1995, Thrun, 1994, Thrun, 1995a],
result of applying EBNN to mobile robot navigation using the CMU Xavier robot
are reported. EBNN has also been applied to robot perception [Mitchell et al., 1994],

[OSullivan et al., to appear], object recognition [Thrun and Mitchell, 1994] and the
game of chess [Thrun, 1995b]. In [Thrun and Mitchell, 1994], a denition of the
lifelong learning problem in the context of supervised learning can be found.

The approach to interpreting sonar sensors for building occupancy maps reported in
Sect. 4 has been, with slight modications, successfully employed in the University
of Bonns entry Rhino at the 1994 AAAI mobile robot competition [Buhmann et
al., to appear]. Currently, maps are routinely built for large indoor areas.

