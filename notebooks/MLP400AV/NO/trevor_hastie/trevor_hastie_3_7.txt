Abstract

Sentiment classication is the task of labeling a re-
view document according to the polarity of its pre-
vailing opinion (favorable or unfavorable).
In ap-
proaching this problem, a model builder often has
three sources of information available: a small col-
lection of labeled documents, a large collection of
unlabeled documents, and human understanding of
language. Ideally, a learning method will utilize all
three sources. To accomplish this goal, we general-
ize an existing procedure that uses the latter two.

We extend this procedure by re-interpreting it
as a Naive Bayes model for document sentiment.
Viewed as such, it can also be seen to extract a
pair of derived features that are linearly combined
to predict sentiment. This perspective allows us to
improve upon previous methods, primarily through
two strategies: incorporating additional derived fea-
tures into the model and, where possible, using la-
beled data to estimate their relative inuence.

1 Introduction
Text documents are available in ever-increasing
numbers, making automated techniques for infor-
mation extraction increasingly useful. Traditionally,
most research effort has been directed towards ob-
jective information, such as classication accord-
ing to topic; however, interest is growing in produc-
ing information about the opinions that a document
contains; for instance, Morinaga et al. (2002). In
March, 2004, the American Association for Arti-
cial Intelligence held a symposium in this area, en-
titled Exploring Affect and Attitude in Text.

One task in opinion extraction is to label a re-
view document d according to its prevailing senti-
ment s 2 f(cid:0)1; 1g (unfavorable or favorable). Sev-
eral previous papers have addressed this problem
by building models that rely exclusively upon la-
beled documents, e.g. Pang et al.
(2002), Dave
(2003). By learning models from labeled
et al.
data, one can apply familiar, powerful techniques
directly; however, in practice it may be difcult to

obtain enough labeled reviews to learn model pa-
rameters accurately.

A contrasting approach (Turney, 2002) relies only
upon documents whose labels are unknown. This
makes it possible to use a large underlying corpus 
in this case, the entire Internet as seen through the
AltaVista search engine. As a result, estimates for
model parameters are subject to a relatively small
amount of random variation. The corresponding
drawback to such an approach is that its predictions
are not validated on actual documents.

In machine learning,

it has often been effec-
tive to use labeled and unlabeled examples in tan-
dem, e.g. Nigam et al.
(2000). Turneys model
introduces the further consideration of incorporat-
ing human-provided knowledge about language. In
this paper we build models that utilize all three
sources: labeled documents, unlabeled documents,
and human-provided information.

The basic concept behind Turneys model is quite
simple.
The sentiment orientation (Hatzivas-
siloglou and McKeown, 1997) of a pair of words
is taken to be known. These words serve as an-
chors for positive and negative sentiment. Words
that co-occur more frequently with one anchor than
the other are themselves taken to be predictive of
sentiment. As a result, information about a pair of
words is generalized to many words, and then to
documents.

In the following section, we relate this model
with Naive Bayes classication, showing that Tur-
neys classier is a pseudo-supervised approach:
it effectively generates a new corpus of labeled doc-
uments, upon which it ts a Naive Bayes classier.
This insight allows the procedure to be represented
as a probability model that is linear on the logistic
scale, which in turn suggests generalizations that are
developed in subsequent sections.

2 A Logistic Model for Sentiment
2.1 Turneys Sentiment Classier
In Turneys model, the sentiment orientation (cid:27) of
word w is estimated as follows.

2.2 Naive Bayes Classication
Bayes Theorem provides a convenient framework
for predicting a binary response s 2 f(cid:0)1; 1g from a
feature vector x:

^(cid:27)(w) = log

N(w;excellent)=Nexcellent

N(w;poor)=Npoor

(1)

Pr(s = 1jx) =

Here, Na is the total number of sites on the Internet
that contain an occurrence of a  a feature that can
be a word type or a phrase. N(w;a) is the number of
sites in which features w and a appear near each
other, i.e. in the same passage of text, within a span
of ten words. Both numbers are obtained from the
hit count that results from a query of the AltaVista
search engine. The rationale for this estimate is that
words that express similar sentiment often co-occur,
while words that express conicting sentiment co-
occur more rarely. Thus, a word that co-occurs more
frequently with excellent than poor is estimated to
have a positive sentiment orientation.

To extrapolate from words to documents, the esti-
mated sentiment ^s 2 f(cid:0)1; 1g of a review document
d is the sign of the average sentiment orientation of
its constituent features.1 To represent this estimate
formally, we introduce the following notation: W
is a dictionary of features: (w1; : : : ; wp). Each
features respective sentiment orientation is repre-
sented as an entry in the vector ^(cid:27) of length p:

^(cid:27)j = ^(cid:27)(wj)

(2)

Given a collection of n review documents, the i-th
each di is also represented as a vector of length p,
with dij equal to the number of times that feature wj
occurs in di. The length of a document is its total

Turneys classier for the i-th documents senti-

j=1 dij.

ment si can now be written:

number of features, jdij =Pp
^si = sign Pp

j=1 ^(cid:27)jdij
jdij

!

(3)

Using a carefully chosen collection of features,
this classier produces correct results on 65.8% of
a collection of 120 movie reviews, where 60 are
labeled positive and 60 negative. Although this is
not a particularly encouraging result, movie reviews
tend to be a difcult domain. Accuracy on senti-
ment classication in other domains exceeds 80%
(Turney, 2002).

1Note that not all words or phrases need to be considered as
features. In Turney (2002), features are selected according to
part-of-speech labels.

Pr(xjs = 1)(cid:25)1

Pk2f(cid:0)1;1g Pr(xjs = k)(cid:25)k

(4)

For a labeled sample of data (xi; si); i = 1; :::; n,
a classs marginal probability (cid:25)k can be estimated
trivially as the proportion of training samples be-
longing to the class. Thus the critical aspect of clas-
sication by Bayes Theorem is to estimate the con-
ditional distribution of x given s. Naive Bayes sim-
plies this problem by making a naive assump-
tion: within a class, the different feature values are
taken to be independent of one another.

Pr(xjs) =Yj

Pr(xjjs)

(5)

As a result, the estimation problem is reduced to
univariate distributions.

(cid:15) Naive Bayes for a Multinomial Distribution

We consider a bag of words model for a docu-
ment that belongs to class k, where features are as-
sumed to result from a sequence of jdij independent
multinomial draws with outcome probability vector
qk = (qk1; : : : ; qkp).

Given a collection of documents with labels,
(di; si); i = 1; : : : ; n, a natural estimate for qkj is
the fraction of all features in documents of class k
that equal wj:

^qkj = Pi:si=k dij
Pi:si=k jdij

(6)

In the two-class case, the logit transformation
provides a revealing representation of the class pos-
terior probabilities of the Naive Bayes model.

dlogit(sjd) , log cPr(s = 1jd)
cPr(s = (cid:0)1jd)
pXj=1
pXj=1

= ^(cid:11)0 +

^(cid:25)1
^(cid:25)(cid:0)1

= log

dj ^(cid:11)j

+

dj log

where ^(cid:11)0 = log

^(cid:11)j = log

^(cid:25)1
^(cid:25)(cid:0)1
^q1j
^q(cid:0)1j

^q1j
^q(cid:0)1j

(7)

(8)

(9)

(10)

(11)

Observe that the estimate for the logit in Equation
9 has a simple structure: it is a linear function of
d. Models that take this form are commonplace in
classication.
2.3 Turneys Classier as Naive Bayes
Although Naive Bayes classication requires a la-
beled corpus of documents, we show in this sec-
tion that Turneys approach corresponds to a Naive
Bayes model. The necessary documents and their
corresponding labels are built from the spans of text
that surround the anchor words excellent and poor.
More formally, a labeled corpus may be produced

by the following procedure:

1. For a particular anchor ak, locate all of the sites

on the Internet where it occurs.

2. From all of the pages within a site, gather the
features that occur within ten words of an oc-
currence of ak, with any particular feature in-
cluded at most once. This list comprises a new
document, representing that site.2

3. Label this document +1 if ak = excellent, -1

if ak = poor.

When a Naive Bayes model is t to the corpus
described above, it results in a vector ^(cid:11) of length
p, consisting of coefcient estimates for all fea-
tures. In Propositions 1 and 2 below, we show that
Turneys estimates of sentiment orientation ^(cid:27) are
closely related to ^(cid:11), and that both estimates produce
identical classiers.

Proposition 1

Proposition 2 Turneys classier is identical to a
Naive Bayes classier t on this corpus, with (cid:25)1 =
(cid:25)(cid:0)1 = 0:5.
Proof: A Naive Bayes classier typically assigns an
observation to its most probable class. This is equiv-
alent to classifying according to the sign of the es-
timated logit. So for any document, we must show
that both the logit estimate and the average senti-
ment orientation are identical in sign.

When (cid:25)1 = 0:5, (cid:11)0 = 0. Thus the estimated logit

is

dlogit(sjd) =

^(cid:11)jdj

pXj=1
pXj=1

= C1

^(cid:27)jdj

(18)

(19)

This is a positive multiple of Turneys classier
(Equation 3), so they clearly match in sign.
2

3 A More Versatile Model
3.1 Desired Extensions
By understanding Turneys model within a Naive
Bayes framework, we are able to interpret its out-
put as a probability model for document classes. In
the presence of labeled examples, this insight also
makes it possible to estimate the intercept term (cid:11)0.
Further, we are able to view this model as a mem-
ber of a broad class: linear estimates for the logit.
This understanding facilitates further extensions, in
particular, utilizing the following:

^(cid:11) = C1 ^(cid:27)

where C1 =

Nexc:=Pi:si=1 jdij
Npoor=Pi:si=(cid:0)1 jdij

(12)

(13)

1. Labeled documents

2. More anchor words

Proof: Because a feature is restricted to at most one
occurrence in a document,

Xi:si=k

dij = N(w;ak)

Then from Equations 6 and 11:

^q1j
^q(cid:0)1j

^(cid:11)j = log

= log

N(w;exc:)=Pi:si=1 jdij
N(w;poor)=Pi:si=(cid:0)1 jdij

= C1^(cid:27)j

(14)

(15)

(16)

(17)

2

2If both anchors occur on a site, then there will actually be

two documents, one for each sentiment

The reason for using labeled documents is
straightforward; labels offer validation for any cho-
sen model. Using additional anchors is desirable
in part because it is inexpensive to produce lists of
words that are believed to reect positive sentiment,
perhaps by reference to a thesaurus. In addition, a
single anchor may be at once too general and too
specic.

An anchor may be too general in the sense that
many common words have multiple meanings, and
not all of them reect a chosen sentiment orien-
tation. For example, poor can refer to an objec-
tive economic state that does not necessarily express
negative sentiment. As a result, a word such as
income appears 4.18 times as frequently with poor
as excellent, even though it does not convey nega-
tive sentiment. Similarly, excellent has a technical

meaning in antiquity trading, which causes it to ap-
pear 3.34 times as frequently with f urniture.

An anchor may also be too specic, in the sense
that there are a variety of different ways to express
sentiment, and a single anchor may not capture them
all. So a word like pretentious carries a strong
negative sentiment but co-occurs only slightly more
frequently (1.23 times) with excellent than poor.
Likewise, f ascination generally reects a positive
sentiment, yet it appears slightly more frequently
(1.06 times) with poor than excellent.

3.2 Other Sources of Unlabeled Data
The use of additional anchors has a drawback in
terms of being resource-intensive. A feature set may
contain many words and phrases, and each of them
requires a separate AltaVista query for every chosen
anchor word. In the case of 30,000 features and ten
queries per minute, downloads for a single anchor
word require over two days of data collection.

An alternative approach is to access a large
collection of documents directly.
Then all co-
occurrences can be counted in a single pass.
Although this approach dramatically reduces the
amount of data available, it does offer several ad-
vantages.

(cid:15) Increased Query Options Search engine
queries of the form phrase NEAR anchor
may not produce all of
the desired co-
occurrence counts. For instance, one may wish
to run queries that use stemmed words, hy-
phenated words, or punctuation marks. One
may also wish to modify the denition of
NEAR, or to count individual co-occurrences,
rather than counting sites that contain at least
one co-occurrence.

(cid:15) Topic Matching Across the Internet as a
whole, features may not exhibit the same cor-
relation structure as they do within a specic
domain. By restricting attention to documents
within a domain, one may hope to avoid co-
occurrences that are primarily relevant to other
subjects.

(cid:15) Reproducibility On a xed corpus, counts of
word occurrences produce consistent results.
Due to the dynamic nature of the Internet,
numbers may uctuate.

3.3 Co-Occurrences and Derived Features
The Naive Bayes coefcient estimate ^(cid:11)j may itself
be interpreted as an intercept term plus a linear com-
bination of features of the form log N(wj ;ak).

Num. of Labeled Occurrences Correlation

1 - 5
6 - 10
11 - 25
26 - 50
51 - 75
76 - 100

0.022
0.082
0.113
0.183
0.283
0.316

Figure 1: Correlation between Supervised and Un-
supervised Coefcient Estimates

^(cid:11)j = log

N(j;exc:)=Pi:si=1 jdij
N(j;pr:)=Pi:si=(cid:0)1 jdij

= log C1 + log N(j;exc:) (cid:0) log N(j;pr:)

(20)

(21)

We generalize this estimate as follows: for a col-
lection of K different anchor words, we consider a
general linear combination of logged co-occurrence
counts.

^(cid:11)j =

KXk=1

(cid:13)k log N(wj ;ak)

(22)

In the special case of a Naive Bayes model, (cid:13)k =
1 when the k-th anchor word ak conveys positive
sentiment, (cid:0)1 when it conveys negative sentiment.
Replacing the logit estimate in Equation 9 with

an estimate of this form, the model becomes:

dlogit(sjd) = ^(cid:11)0 +

= ^(cid:11)0 +

= (cid:13)0 +

pXj=1
pXj=1
KXk=1

KXk=1
pXj=1

(cid:13)k

dj ^(cid:11)j

(23)

dj(cid:13)k log N(wj ;ak)

(24)

dj log N(wj ;ak)

(25)
(26)

This model has only K + 1 parameters:
(cid:13)0; (cid:13)1; : : : ; (cid:13)K. These can be learned straightfor-
wardly from labeled documents by a method such
as logistic regression.

Observe that a document receives a score for each
j=1 dj log N(wj ;ak). Effectively, the
predictor variables in this model are no longer
counts of the original features dj. Rather, they are

anchor wordPp

Unsupervised vs. Supervised Coefficients

4

3

2

1

0

1


2


3


.
s
f



e
o
C

s
e
y
a
B
e
v
a
N

y
e
n
r
u
T

i

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

Traditional Naive Bayes Coefs.

Figure 2: Unsupervised versus Supervised Coef-
cient Estimates

inner products between the entire feature vector d
and the logged co-occurence vector N(w;ak). In this
respect, the vector of logged co-occurrences is used
to produce derived feature.

4 Data Analysis
4.1 Accuracy of Unsupervised Coefcients
that uses the Lynx
By means of a Perl script
browser, Version 2.8.3rel.1, we download AltaVista
hit counts for queries of the form target NEAR
anchor. The initial list of targets consists of
44,321 word types extracted from the Pang cor-
pus of 1400 labeled movie reviews. After pre-
processing, this number is reduced to 28,629.3

In Figure 1, we compare estimates produced by
two Naive Bayes procedures. For each feature wj,
we estimate (cid:11)j by using Turneys procedure, and
by tting a traditional Naive Bayes model to the
labeled documents. The traditional estimates are
smoothed by assuming a Beta prior distribution that
is equivalent to having four previous observations of
wj in documents of each class.

^q1j
^q(cid:0)1j

= C2

where C2 =

4 +Pi:si=1 dij
4 +Pi:si=(cid:0)1 dij
4p +Pi:si=1 jdij
4p +Pi:si=(cid:0)1 jdij

(27)

(28)

(29)

Here, dij is used to indicate feature presence:

dij =(cid:26) 1 if wj appears in di

0 otherwise

3We eliminate extremely rare words by requiring each target
to co-occur at least once with each anchor. In addition, certain
types, such as words containing hyphens, apostrophes, or other
punctuation marks, do not appear to produce valid counts, so
they are discarded.

Positive

best

brilliant
excellent
spectacular
wonderful

Negative

awful
bad

pathetic

poor
worst

Figure 3: Selected Anchor Words

We choose this tting procedure among several can-
didates because it performs well in classifying test
documents.

In Figure 1, each entry in the right-hand col-
umn is the observed correlation between these two
estimates over a subset of features. For features
that occur in ve documents or fewer, the corre-
lation is very weak (0.022). This is not surpris-
ing, as it is difcult to estimate a coefcient from
such a small number of labeled examples. Corre-
lations are stronger for more common features, but
never strong. As a baseline for comparison, Naive
Bayes coefcients can be estimated using a subset
of their labeled occurrences. With two independent
sets of 51-75 occurrences, Naive Bayes coefcient
estimates had a correlation of 0.475.

Figure 2 is a scatterplot of the same coefcient
estimates for word types that appear in 51 to 100
documents. The great majority of features do not
have large coefcients, but even for the ones that
do, there is not a tight correlation.

4.2 Additional Anchors
We wish to learn how our model performance de-
pends on the choice and number of anchor words.
Selecting from WordNet synonym lists (Fellbaum,
1998), we choose ve positive anchor words and
ve negative (Figure 3). This produces a total of
25 different possible pairs for use in producing co-
efcient estimates.

Figure 4 shows the classication performance
of unsupervised procedures using the 1400 labeled
Pang documents as test data. Coefcients ^(cid:11)j are es-
timated as described in Equation 22. Several differ-
ent experimental conditions are applied. The meth-
ods labeled Count use the original un-normalized
coefcients, while those labeled Norm. have been
normalized so that the number of co-occurrences
with each anchor have identical variance. Results
are shown when rare words (with three or fewer oc-
currences in the labeled corpus) are included and
omitted. The methods pair and 10 describe
whether all ten anchor coefcients are used at once,
or just the ones that correspond to a single pair of

Method

Count Pair >3
Norm. Pair >3
all
Count Pair
Norm. Pair
all
Count 10
> 3
Norm. 10
> 3
all
Count 10
Norm. 10
all

Feat. Misclass. St.Dev
2.9%
3.0%
3.1%
3.0%

39.6%
38.4%
37.4%
37.3%
36.4%
35.4%
34.6%
34.1%

Misclassification versus Sample Size

r
o
r
r

E


.
f
i
s
s
a
C

l

0
4

.

0

8
3

.

0

6
3

.

0

4
3

.

0

2
3

.

0

0
3

.

0






Figure 4: Classication Error Rates for Different
Unsupervised Approaches

100

200

300

400

500

600

Num. of Labeled Documents

anchor words. For anchor pairs, the mean error
across all 25 pairs is reported, along with its stan-
dard deviation.

Patterns are consistent across the different condi-
tions. A relatively large improvement comes from
using all ten anchor words. Smaller benets arise
from including rare words and from normalizing
model coefcients.

Models that use the original pair of anchor words,
excellent and poor, perform slightly better than the
average pair. Whereas mean performance ranges
from 37.3% to 39.6%, misclassication rates for
this pair of anchors ranges from 37.4% to 38.1%.

4.3 A Smaller Unlabeled Corpus
As described in Section 3.2, there are several rea-
sons to explore the use of a smaller unlabeled cor-
pus, rather than the entire Internet. In our experi-
ments, we use additional movie reviews as our doc-
uments. For this domain, Pang makes available
27,886 reviews.4

Because this corpus offers dramatically fewer in-
stances of anchor words, we modify our estimation
procedure. Rather than discarding words that rarely
co-occur with anchors, we use the same feature set
as before and regularize estimates by the same pro-
cedure used in the Naive Bayes procedure described
earlier.

Using all features, and ten anchor words with nor-
malized scores, test error is 35.0%. This suggests
that comparable results can be attained while re-
ferring to a considerably smaller unlabeled corpus.
Rather than requiring several days of downloads,
the count of nearby co-occurrences was completed
in under ten minutes.

Because this procedure enables fast access to
counts, we explore the possibility of dramatically
enlarging our collection of anchor words. We col-

Figure 5: Misclassication with Labeled Docu-
ments. The solid curve represents a latent fac-
tor model with estimated coefcients. The dashed
curve uses a Naive Bayes classier. The two hor-
izontal lines represent unsupervised estimates; the
upper one is for the original unsupervised classier,
and the lower is for the most successful unsuper-
vised method.

lect data for the complete set of WordNet syn-
onyms for the words good, best, bad, boring, and
dreadf ul. This yields a total of 83 anchor words,
35 positive and 48 negative. When all of these an-
chors are used in conjunction, test error increases to
38.3%. One possible difculty in using this auto-
mated procedure is that some synonyms for a word
do not carry the same sentiment orientation. For in-
stance, intense is listed as a synonym for bad, even
though its presence in a movie review is a strongly
positive indication.5

4.4 Methods with Supervision
As demonstrated in Section 3.3, each anchor word
ak is associated with a coefcient (cid:13)k.
In unsu-
pervised models, these coefcients are assumed to
be known. However, when labeled documents are
available, it may be advantageous to estimate them.
Figure 5 compares the performance of a model
with estimated coefcient vector (cid:13), as opposed to
unsupervised models and a traditional supervised
approach. When a moderate number of labeled doc-
uments are available, it offers a noticeable improve-
ment.

The supervised method used for reference in this
case is the Naive Bayes model that is described in
section 4.1. Naive Bayes classication is of partic-
ular interest here because it converges faster to its
asymptotic optimum than do discriminative meth-
ods (Ng, A. Y. and Jordan, M., 2002). Further, with

4This corpus is freely available on the following website:

5In the labeled Pang corpus, intense appears in 38 positive

http://www.cs.cornell.edu/people/pabo/movie-review-data/.

reviews and only 6 negative ones.

Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web.

Ng, A. Y. and Jordan, M. 2002. On discriminative
vs. generative classiers: A comparison of logis-
tic regression and naive bayes. Advances in Neu-
ral Information Processing Systems, 14.

Bo

Pang,

and

Kamal Nigam, Andrew K. McCallum, Sebastian
Thrun, and Tom M. Mitchell. 2000. Text clas-
sication from labeled and unlabeled documents
using EM. Machine Learning, 39(2/3):103134.
Shivakumar
senti-
Vaithyanathan.
ment
classication using machine learning
techniques. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP).

2002. Thumbs up?

Lillian Lee,

P.D. Turney and M.L. Littman. 2002. Unsupervised
learning of semantic orientation from a hundred-
billion-word corpus.

Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised
classication of reviews. In Proceedings of the
40th Annual Meeting of
the Association for
Computational Linguistics (ACL02), pages 417
424, Philadelphia, Pennsylvania. Association for
Computational Linguistics.

Janyce Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proc. 17th National Con-
ference on Articial Intelligence (AAAI-2000),
Austin, Texas.

Jian Zhang and Yiming Yang. 2003. robustness of
regularized linear classication methods in text
categorization. In Proceedings of the 26th An-
nual International ACM SIGIR Conference (SI-
GIR 2003).

a larger number of labeled documents, its perfor-
mance on this corpus is comparable to that of Sup-
port Vector Machines and Maximum Entropy mod-
els (Pang et al., 2002).

The coefcient vector (cid:13) is estimated by regular-
ized logistic regression. This method has been used
in other text classication problems, as in Zhang
and Yang (2003). In our case, the regularization6
is introduced in order to enforce the beliefs that:

(cid:13)1 (cid:25) (cid:13)2, if a1, a2 synonyms
(cid:13)1 (cid:25) (cid:0)(cid:13)2, if a1, a2 antonyms

(30)
(31)

For further information on regularized model tting,
see for instance, Hastie et al. (2001).

5 Conclusion
In business settings, there is growing interest in
learning product reputations from the Internet. For
such problems, it is often difcult or expensive to
obtain labeled data. As a result, a change in mod-
eling strategies is needed, towards approaches that
require less supervision.
In this paper we pro-
vide a framework for allowing human-provided in-
formation to be combined with unlabeled docu-
ments and labeled documents. We have found that
this framework enables improvements over existing
techniques, both in terms of the speed of model es-
timation and in classication accuracy. As a result,
we believe that this is a promising new approach to
problems of practical importance.

