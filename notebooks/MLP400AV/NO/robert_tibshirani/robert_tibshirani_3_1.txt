Abstract

In microarray studies, an important problem is to compare a predictor of disease outcome
derived from gene expression levels to standard clinical predictors. Comparing them on the same
dataset that was used to derive the microarray predictor can lead to results strongly biased in
favor of the microarray predictor. We propose a new technique called pre-validation for making
a fairer comparison between the two sets of predictors. We study the method analytically and
explore its application in a recent study on breast cancer.

Tibshirani and Efron: Pre-validation and inference in  microarrays

1 Introduction

A DNA microarray dataset typically consists of expression measurements on
a large number of genes (say 10,000) over a small set of cases (say 100). In
addition, a true class label is often available for each case, and the objective
is to nd a function of the gene expression values that accurately predicts
the class membership. A number of techniques have been proposed for this
[see e.g. Dudoit et al. (2001)]. Having formed a prediction function from the
microarray values, the following problem often arises in a medical context:
how do we compare the microarray predictor to an existing clinical predictor
of the class membership. Does the new microarray predictor add anything
to the clinical predictor?

An example of this problem arose in the paper of vant Veer et al. (2002).
Their microarray data has 4918 genes measured over 78 cases, taken from a
study of breast cancer. There are 44 cases in the good prognosis group and
34 in the poor prognosis group. The microarray predictor was constructed
as follows:

1. 70 genes were selected, having largest absolute correlation with the 78

class labels

2. Using these 70 genes, a nearest centroid classier (described in detail

in Section 6) was constructed.

3. Applying the classier to the 78 microarrays gave a dichotomous pre-

dictor zj for each case j.

It was of interest to compare this predictor to a number of clinical pre-
dictors including tumor grade, estrogen receptor (ER) status, progesteron
receptor (PR) status, tumor size, patient age, and angioinvasion. The top
part of Table 1, labelled re-use, shows the result when a multivariate logis-
tic regression was t to the microarray predictor and clinical predictors. The
microarray predictor looks very strong, but this is not surprising as it was
derived using the same set of cases that are used in the logistic regression.
In the bottom half of the table, we have derived a pre-validated version
of the microarray predictor, and used it in the logistic regression. Now the
microarray predictor is much less signicant, and the clinical predictors have
strengthened.

The same idea behind pre-validation was used in the supplementary
material for vant Veer et al. (2002), and is the topic of this paper. It is also

Published by The Berkeley Electronic Press, 2002

1

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

Table 1: Results of model tting to breast cancer data. Top panel: re-using the
microarray scores zj in the logistic model; bottom panel: using pre-validated
scores zj. The last column is the change in misclassication rate, when the
given predictor is deleted from the full model. The full-model misclassication
rates are 0.14 and 0.21 for the re-use and pre-validated models respectively.

Model

Coef Stand. Err. Z score p-value Odds ratio MR

microarray
angio
er
grade
pr
age
size

4.096
1.208
-0.554
-0.697
1.214
-1.593
1.483

Re-use

1.092
0.816
1.044
1.003
1.057
0.911
0.732

microarray
angio
er
grade
pr
age
size

Pre-validated
0.675
0.682
0.894
0.720
0.863
0.701
0.594

1.549
1.589
-0.617
0.719
0.537
-1.471
0.998

3.753
1.482
-0.530
-0.695
1.149
-1.748
2.026

2.296
2.329
-0.690
0.999
0.622
-2.099
1.681

0.000
0.069
0.298
0.243
0.125
0.040
0.021

0.011
0.010
0.245
0.159
0.267
0.018
0.046

60.105
3.348
0.575
0.498
3.367
0.203
4.406

4.706
4.898
0.540
2.053
1.710
0.230
2.714

0.12
0.01
0.01
0.01
-0.01
0.01
0.00

0.05
0.01
0.01
0.00
0.01
0.03
0.04

similar to the method of stacking due to Wolpert (1992), in the area of
machine learning.

2 Pre-validation

In order to avoid the overtting problem apparent in the top half of Table 1,
we might try to use some sort of cross-validation:

1. Divide the cases up into say K approximately equal-sized parts
2. Set aside one of parts. Using the other K  1 parts, select the 70 genes

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

2

Tibshirani and Efron: Pre-validation and inference in  microarrays

having the largest absolute correlation with the class labels, and form
a nearest centroid classier.

3. Fit a logistic model to the kth part, using the microarray class predictor

and clinical predictors

4. Do steps 2 and 3 for each of the k = 1, 2, . . . K parts, and average the

results from the K resulting logistic models.

The main problem with this idea is step 3, where there will typically be too
few cases to t the model. In the above example, with K = 10, the 10th
part would consist of only 7 or 8 cases. Using a smaller value of K (say 5)
would yield a larger number of cases, but then might make the training sets
too small in step 2. Use of multiple random splits can help cross-validation
a little in this case.

Pre-validation is a variation on cross-validation that avoids these prob-
lems. It derives a fairer version of the microarray predictor, and then this
predictor is t along side the clinical predictors in the usual way. Here is how
pre-validation was used in the bottom half of Table 1:

1. Divide the cases up into K = 13 equal-sized parts of 6 cases each.

2. Set aside one of parts. Using only the data from the other 12 parts,
select the genes having absolute correlation at least .3 with the class
labels, and form a nearest centroid classication rule.

3. Use the rule to the predict the class labels for the 13th part

4. Do steps 2 and 3 for each of the 13 parts, yielding a pre-validated

microarray predictor zj for each of the 78 cases.

5. Fit a logistic regression model to the pre-validated microarray predic-
tor and the 6 clinical predictors. Figure 1 illustrates the logic of this
computation.

Notice that the cross-validation in steps 13 deals only with the microarray
predictor: it creates a fairer version of this predictor, in which the predictor
for case j has not seen the true class label for case j. This pre-validated
predictor is then compared to the clinical predictor in the standard way at
step 5. The issues behind the choice of K are similar to those in cross-

Published by The Berkeley Electronic Press, 2002

3

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

outcome

cases

y

omitted part

Expression data

X

genes

pre-validated

predictor
clinical
predictors

~
z

logistic
regression

T

c

Figure 1: Schematic of pre-validation process. The cases are divided up into
(say) 10 equal-sized groups. The cases in one group are left out, and a mi-
croarray predictor is derived from the expression data of the remaining cases.
Evaluating this predictor on the left-out cases yields the pre-validated predic-
tor z for those cases. This is done for each of the 10 groups in turn, producing
the pre-validated predictor z for all cases. Finally, z can be included along
with clinical predictors in a logistic regression model, in order to assess its
relative strength in predicting the outcome.

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

4

Tibshirani and Efron: Pre-validation and inference in  microarrays

validation. The choice K = N (leave-one-out) doesnt perturb the data
enough and results in higher variance estimates. With K = 2 the training
sets are too small relative to the full training set. The values K = 5 or 10
are a good compromise.

3 Pre-validation in detail
We have microarray data X, a p n matrix of measurements on p genes over
n cases. 1 The outcome is an n-vector y, and suppose we also have set of k
clinical predictors, represented by a n  k matrix c, for predicting y. Let xj

denote the jth column of X.

An expression predictor z = (z1, z2, . . . zn) is adaptively chosen from the

training data

zj = fX,y(xj).

(1)

The notation indicates that zj is a function of the data X and y, and is
evaluated at xj. In our motivating example earlier, the function f () consisted

of nding the 70 top correlated genes, and using them to form a nearest
centroid classier. Our challenge is to compare c to z, in terms of their
strength in predicting y. The re-use method uses the predictor z as is,
and then ts a model to examine the relative contributions of z and c in
predicting y. In our motivating example, we used a logistic model. Clearly
this comparison is biased in favor of z, since the outcomes y were already
used in the construction of z

It is helpful to consider what we would do given an independent test set
(X 0, y0) with corresponding clinical predictors c0. We could use the test set
to derive the predictor z0 = (z0
j ) and then use
this to t a model to predict y0 from z0 and c0. This would allow us to
directly examine the relative contributions of z0 and c0 in predicting y0.

n) where z0

j = fX,y(x0

1, z0

2, . . . z0

K-fold pre-validation tries to mimic this, without use of a test set. We
divide the cases into K roughly equal-sized groups, and let g(k) be the cases

1It is conventional in the microarray area to arrange the data matrix X with genes
(predictors) in the rows and cases in the columns. Although this is the transpose of the
usual statistical convention, we adopt it here. Accordingly, we index the cases by the
subscript j.

Published by The Berkeley Electronic Press, 2002

5

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

composing each part k. For k = 1, 2, . . . K, we form the pre-validated pre-
dictor

zg(k) = fX

g(k),yg(k)(xg(k)); for k = 1, 2, . . . K

(2)

the notation indicating that cases g(k) have been removed from X and y.
Finally, we t the model to predict y from z and c, and compare their
contributions in this prediction.

4 Does pre-validation work?

The goal of pre-validation is to construct a fairer version of the microarray
predictor that acts as if it hasnt seen the response y. One way to quantify
this goal is as follows. When the pre-validated predictor z is included with
clinical predictors in a linear or linear logistic model for predicting y, it should
spend one degree of freedom. For the usual (non pre-validated) predictor z,
we expect more than one degree of freedom to be spent. In this section we
will make these notions precise and see if pre-validation works in this sense.

As before we dene a microarray predictor as

z(x) = fX,y(x),

(3)

let z be the vector of values (z(x1), z(x2), . . . z(xn)) and let c be an n by k
matrix of k clinical predictors. Using these we t a linear or linear logistic
model to predict y, with predicted values

(x, c; z, c, y).

(4)

The rst two arguments indicate the predicted values are evaluated at x (a
p-vector of expression values) and c (a k-vector of clinical predictors).

Let j = (xj, cj; z, c, y), the predicted values for the training data, and
let 2 be the variance of each yj. Following Efron et al. (2002) (see also Stein
(1981),Ye (1998)), we dene the degrees of freedom in the predicted values
j to be

n(cid:1)

n(cid:1)

j=1

df() = E(

j=1

 j
yj

)/2 =

cov(j, yj)/2

(5)

The leftmost relation denes degrees of freedom by the total change in the
predicted values as each yj is perturbed. On the right, it is dened as the

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

6

Tibshirani and Efron: Pre-validation and inference in  microarrays

total self-inuence of each observation on its predicted value. These two
notions are equivalent, as shown in Efron (1986). He proves that (5) holds
exactly for the Gaussian model, and approximately when  is an expectation
parameter in an exponential family model.

In the special case that f is a linear function of y and the model giving
(x, c; z, c, y) is linear least squares, we can derive an explicit expression for
df().
Let z = Ay and let M be the n  (k + 1) matrix {z, c}. Let P project
onto the column space of M and Pc project onto the column space of c
= (I  P )A, zc = (I  Pc)z and  = P y. Then we have
n(cid:1)
alone. Dene A
the following results:







)  I)zc

n(cid:1)

1

1
 j
yj

= (k + 1) +

 j
yj
|y= = (k + 1) +

yT (A

+ tr(A

||zc||2
)  I)zc

yT (tr(A
||zc||2

(6)

The proof is given in the Appendix.

The term (k + 1) is the degrees of freedom if z were the usual kind of
predictor, constructed independently of the training data. The second term
is the additional degrees of freedom due to the possible dependence of z on
y.

If A is a least squares projection matrix of rank p and there are no clinical
predictors (k = 0), then one can show that the second term in expression (6)
is p  1, so that the total degrees of freedom is (0 + 1) + (p  1) = p.

It turns out that leave-one-out pre-validation can also be expressed as a
linear operator. Let H be the projection matrix onto the row space of X
(recall that X is p  n, with genes in the rows). Let D be a diagonal matrix
consisting of the diagonal of H, and let I be the n n identity matrix. Then
the pre-validated predictors have the form
z = Ay; with A = (I  D)

1(H  D)

(7)

[using the Sherman-Morrison Woodbury identity; see e.g. Hastie & Tibshi-
rani (1990), chapter 3]. The matrix A has some interesting properties, for
example tr(A) = 0.

Our hope here is that with A dened as in (7), the additional degrees of
freedom [second term in (6)] will be zero. This is dicult to study analyti-
cally, so we turn to a numerical study in the next section.

Published by The Berkeley Electronic Press, 2002

7

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

5 Numerical study of degrees of freedom

In this section we carry out some simulations to study the degrees of freedom
in pre-validated predictors.

In our rst study, we generated standard independent normal expression
measurements on n = 50 or 200 cases and p genes, p ranging from 2 to 20. The
outcome y was generated y = xT  +  where   N (0, .04). The coecient
vector  was set to zero in the null case, and   N (0, 1) in the non-null
case. Finally, a samples of independent clinical predictors c  N (0, 1) was
generated.

The linear pre-validation t (7) was computed from the expression data,
and included along with c in a linear least-squares model to predict y. Note
that for simplicity, the outcome y was centered and no intercept was included
in the linear least squares model. The mean and standard error of the total
degrees of freedom [formula (6)] over 50 simulations is shown in Table 2.

While the degrees of freedom tends to be less than p+1 (the value without
pre-validation), we see it exceeds the ideal value of 2 in the null case, and is
less than 2 in the non-null case.

The null case is most bothersome, for in that case treating the microarray
predictor as a one degree of freedom predictor will cause us to overestimate
its eect (we have no explanation for this). In the null setting with p = 2,
it is remarkable that the degrees of freedom is actually greater than p + 1,
which is the value for the non pre-validated predictor.

We think of this increase in degrees of freedom as leakage. While
pre-validation makes each value zj independent of its outcome value yj, the
outcome yj does inuence other pre-validated values zk, causing some degrees
of freedom to leak into the nal t. One might guess that leakage will
tend to disappear as the sample size n gets large. But that is not borne out
in the results for n = 200.


j with 


j


j

 j + 

The rightmost column of Table 2 gives a parametric bootstrap estimate
of degrees of freedom. Fixing the expression data, new response values were
 N (0, 2), 2 being the usual unbiased
generated as y
estimate of variance. Using ve such sets of bootstrap values, the covariance
expression in (5) was estimated. We see that the bootstrap does a reasonable
job of estimating the degrees of freedom, although sometimes it underesti-
mates the actual value. The bootstrap method can be applied to general
situations where the actual value of degrees of freedom is not available. This
estimate would be used to modify the assessment of the likelihood ratio test

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

8

Tibshirani and Efron: Pre-validation and inference in  microarrays

Table 2: Simulation results. Degrees of freedom of pre-validated predictor
from formula (6) and parametric bootstrap estimate. Ideal value is 2.

p

Formula (se) Parametric bootstrap (se)

Null case, n = 50

2
5
10
20

2
5
10
20

2
5
10
20

2
5
10
20

4.04 (0.21)
3.40 (0.09)
2.95 (0.05)
2.89 (0.02)

Null case, n = 200

6.44 (0.63)
3.95 (0.22)
3.39 (0.09)
3.09 (0.06)

Non-null case, n = 50

1.35 (0.09)
1.19 (0.03)
1.15 (0.01)
1.30 (0.02)

Non-null case, n = 200

1.61 (0.19)
1.17 (0.02)
1.12 (0.01)
1.11 (0.01)

3.25 (0.18)
2.72 (0.12)
2.73 (0.10)
2.64 (0.09)

3.74 (0.31)
2.98 (0.16)
2.73 (0.14)
2.64 (0.10)

1.32 (0.13)
0.98 (0.09)
0.68 (0.14)
0.56 (0.10)

1.64 (0.19)
1.07 (0.12)
1.01 (0.13)
0.79 (0.19)

Table 3: Simulation results- large example. As in Table 2, with the ideal
value for degrees of freedom equal to 2.

Formula (se) Parametric bootstrap (se)

Null case

Non-Null case

6.48 0.4

2.70 (.34)

7.26 0.47

3.65 (.50)

Published by The Berkeley Electronic Press, 2002

9

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

to compare the full model to that containing just the clinical predictors.

In Table 3, we generated a scenario closer to actual microarray experi-
ments. Here n = 50 and p = 1000, with the expression values again being
standard independent Gaussian. Since p > n we cannot use least squares
regression to construct the microarray predictor z, so we used pre-validated
ridge regression. As before, the outcome was generated as y = xT  +  and
linear least squares was used for the nal model to predict y from z and c,
with   N (0, .052) in the non-null case.

In this setup the mapping fX,y(x) is not linear, so it is not convenient
to use formula (6). Hence we computed the covariance expression on the
right-hand side of (5) directly. The results in Table 3 show that leakage is
again a problem, in the null case.

Our conclusion from these studies is that pre-validation greatly reduces
the degrees of freedom of the microarray predictor, but does not reliably
reduce it to the ideal value of one. Hence we recommend that for each
application of pre-validation, a parametric bootstrap be used to estimate the
degrees of freedom. This is illustrated in the breast cancer example in the
next section.

6 Further analysis of the breast cancer data

We re-analyzed the breast cancer data from vant Veer et al. (2002). The
authors use the following steps:

1. Starting with 24,881 genes, they apply ltering based on fold-change
and a p-value criterion, to yield 4936 genes (personal communication
from authors).

2. They select the genes have absolute correlation  .3 with the class

labels, giving 231 genes

3. They nd the 231-dimensional centroid vector for the 44 good-prognosis

cases

4. They compute the correlation of each case with this centroid and choose
a cuto value so that exactly 3 of the poor groups are misclassied. This
value turned out to be .38. Finally they classify to the good prognosis
group if the correlation with the normal centroid is  .38, otherwise
they classify to the poor prognosis group.

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

10

Tibshirani and Efron: Pre-validation and inference in  microarrays

Table 4: Odds ratios from pre-validated data

Predictor
microarray
angio
ER
grade
PR
age
size

Friend et. al. Our analysis
4.7
4.9
1.9
2.1
1.7
4.3
2.7

17.6
4.7
1.7
1.1
2.1
4.0
3.5

5. Starting with the top 5, 10, 15 . . . genes, they carried out this classica-
tion procedure with leave-one-out cross-validation, to pick the optimal
number of genes. reporting an optimal number of 70

Even with some help from the authors, we were unable to exactly reproduce
this analysis. At stages 2 and 3, we obtained 4918 and 235 genes, respectively.
The authors told us they could not release their list of 4936 genes for legal
reasons. We xed the number of genes (70) in step 5.

The authors carry out what we call a pre-validation analysis in the sup-
plementary material to their paper. Table 4 shows the odds ratios from the
pre-validated data, both from their analysis and our. The odds ratios for the
microarray predictor dier greatly, and we were unable to reproduce their
results.

We estimated the degrees of freedom for the full model in Table 4 nu-
merically, using the bootstrap method of the previous section, obtaining an
estimate of 9.0, with a standard error of .71. The nominal value would be 8
(7 predictors plus the intercept), so about one degree of freedom has leaked.
In the remainder of this section we apply full cross-validation to this
dataset Pre-validation is a partial form of cross-validation that is especially
convenient when the microarray score z might be applied in a wide variety
of possible models.

The model illustrated in Figure 1 can be described as follows: for each

case j, a score zj is constructed from the 78  4936 microarray data matrix

X and the 78 dimensional response vector y, say

Published by The Berkeley Electronic Press, 2002

11

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

zj = f (xj|X, y)

(8)

according to algorithm (1)(5) at the beginning of this Section. The nota-
tion in (8) indicates that (X, y) determines the form of the rule f , which is
then evaluated at the 4936-vector xj for that cases microarray. The nal
prediction for case j is

(cid:2)j = g(cj, zj|c, z).

(9)
Here g is the logistic regression rule based on the 78 6 matrix of covariates
c and the vector z of 78 z scores, then evaluated at (cj, zj), the vector of 6

covariates and zj (the top panel of Table 1 was based on the predictions(cid:2)j).
Full cross-validation modies (8)(9) so that the data for case j is not
likewise c(j), y(j) etc. The cross-validated predictor(cid:3)j is obtained as
involved in constructing the form of the rules for its own prediction. Let X(j)
indicate the 77  4937 matrix obtained by deleting column xj from X, and

(10)

(11)

(cid:3)zj = f (xj|X(j), y(j))
(cid:3)j = g(cj,(cid:3)zj|c(j),(cid:3)z(j)).

n(cid:1)

(cid:4)Err =

1
n

Q(yj, j)

and

then

(By contrast, the pre-validated predictors used in the bottom of Table 1

employed g(cj,(cid:3)zj|c,(cid:3)z).)
set. If Q(yj, j) is a measure of error for predicting that outcome yj by (cid:2)j,

Full cross-validation permits an almost unbiased estimate of the predic-
tion error we would obtain if rule (9) were applied to an independent test

is nearly unbiased for the expected error rate of rule (cid:2)j applied to an inde-

i=1

(12)

pendent set of test cases, see Efron & Tibshirani (1997) and Efron (1983).

Table 5 refers to the error function

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

12

Tibshirani and Efron: Pre-validation and inference in  microarrays

If yj = 1 and(cid:2)j  34/78
or yj = 0 and(cid:2)j > 34/78
then Q(yj,(cid:2)j) = 1

and Q(yj,(cid:2)yj) = 0 otherwise. In other words, (cid:4)Err is the proportion of pre-

(13)

diction errors (with the prediction threshold set at 34/78 rather than 1/2 to
reect the 44/34 division of cases in the training cases.)

Table 5: Estimates of prediction error for two logistic regression models: c
alone (only the 6 covariates) and c plus z (6 covariates plus the microarray
predictor z.) Naive reuse method suggests that adding z cuts the error rate
nearly in half, from 26.9% to 14.1%. Most of the apparent improvement
disappears under full cross-validation, where now the comparison is 29.5%
versus 28.2%. Bootstrap methods give similar results. The standard errors
were obtained from jackknife calculations, and show that this experiment was
too small to detect genuine dierences of less than about 10%.

Model Re-use

cross-val

(sterr)

zero-boot

(sterr)

632+ boot

(sterr)

c alone:
c plus z:
dierence:

0.269
0.141
0.128

0.295
0.282
0.013

(0.052)
(0.076)
(0.091)

0.341
0.342
-0.001

(0.078)
(0.067)
(0.072)

0.320
0.301
0.019

(0.062)
(0.068)
(0.066)

Table 5 compares the prediction error rates (13) from two logistic re-
gression models: one based on just the six covariates in c, the other using
these plus the microarray predictor z. The naive reuse error rates make z

look enormously helpful, reducing (cid:4)Err from 26.9% to 14.1%. Most of zs
advantage disappears under cross-validation, giving (cid:4)Err(c) = 29.5% versus
(cid:4)Err(c, z) = 28.2%, for a dierence of only 1.3%.

Another important point is clear from Table 2: the cross-validated dier-
ence of 1.3% has an estimated standard error of 9.1%. In other words there
is not enough data in the vant Veer et al. study to establish a prediction
advantage for z of less than say 10% even if it exists (which does not appear
to be the case.)

Published by The Berkeley Electronic Press, 2002

13

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1

cases each, so that calculations (11)-(12) produced(cid:3)zj and (cid:3)j values six at a

The cross-validation in Table 5 grouped the 78 cases into 13 groups of 6

time. The jackknife was used to calculate standard errors: a typical jackknife
pseudo-value deleted one of the 13 groups and repeated the cross-validation
calculations using just the data from the other 12 groups, nally obtaining
the standard error as in (11.5) or (11.15) of Efron & Tibshirani (1993).

Cross-validation can be overly variable for estimating prediction error
(13), see Efron (1983). Table 2 also reports two bootstrap estimates de-
scribed in Efron & Tibshirani (1997): zero-boot and the 632+ rule,

equations 17 and 27-29 of that paper, with(cid:2) = .50), the latter having a par-

ticularly good track record. The story here does not change much, though
632+ gives a slightly larger dierence estimate, 1.9%, with a smaller stan-
dard error, 6.6%. Bootstrap estimates were based on B = 600 bootstrap
replications, with standard errors estimated by the jackknife-after-bootstrap
computations of Efron (1992).

7 Discussion

In this paper we have analyzed pre-validation, a technique for deriving a fairer
version of an adaptively chosen predictor.
It seems especially well-suited
to microarray problems. The promise of pre-validation is that the resulting
predictor will act similarly to one that has been derived from an independent
dataset. Hence when included in a model with clinical predictors, it should
have have one degree of freedom. This is contrast to the usual (non pre-
validated) predictor, which has degrees of freedom equal to the total number
of parameters t in each of the two stages of the analysis.

We have found that pre-validation is only partially successful in achieving
its goal. Generally it controls the degrees of freedom of the predictor, as
compared to the non pre-validated version. However in null situations where
the microarray predictor is independent of the response, degrees of freedom
can leak from one case to another, so that the total degrees of freedom of
the pre-validated predictor is more than the ideal value of one. Conversely, in
non-null settings, the total degrees of freedom of the pre-validated predictor
can be less than expected. Overall we recommend use of the parametric
bootstrap, to estimate the degrees of freedom of the pre-validated predictor.
With the estimated value of degrees of freedom in hand, one can use the
pre-validated predictor along with clinical predictors, in a model to compare

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

14

Tibshirani and Efron: Pre-validation and inference in  microarrays

their predictive accuracy.

Finally, while pre-validation is a promising method for building and as-
sessing an adaptive predictor on the same set of data, it is no substitute
for full cross-validation or test set validation, in situations where there is
sucient data to use these techniques.

Acknowledgments: Tibshirani was partially supported by NIH grant 2
R01 CA72028, and NSF grant DMS-9971405. Efron was partially supported
by NIH grant 2R01 CA59039 and NSF grant DMS-0072360.

Appendix: proof of formula (6)

z = Ay for a xed n  n matrix A, and then (cid:2) = P y where P is the

Formula (6) concerns pre-validation in linear model situations. We compute
n  n projection matrix into the linear space spanned by the columns of the
n  (k + 1) matrix M = (c, z), c being the n  k matrix of xed covariates:

P = M G

1M T

(G = M T M ).

(A1)

Notice that P = P (y) is not xed, being a function of y though z.

Dene P



= I  P, A




= P

An innitesimal change in the response vector, y  y + dy, changes z by
amount dz = Ady, which we can write as

1)z.

A,

and z(c) = (I  c(cT c)
dz  d(cid:2)z + dz



.

dz = P dz + P



(A2)

The resulting change in P is calculated as follows.
Lemma


dP = (dz

T )/(cid:6)z(c)(cid:6)2.

component d(cid:2)z has no eect on the projection matrix P since it preserves the

Proof. Changes in y aect P only through changes in z. Moreover, the

(A3)

zT
(c) + z(c)dz

linear space M , so we can consider dP to be a function of only dz
change in G = M T M is zero to rst order,


G + dG = (c, z + dz


)T (c, z + dz

) = G,

. The

(A4)



Published by The Berkeley Electronic Press, 2002

15

Statistical Applications in Genetics and Molecular Biology, Vol. 1 [2002], Iss. 1, Art. 1


since cT dz


= 0 = zT dz

. Thus

P + dP

=


[(c, z) + (0, dz

)]G

= P + (0, dz

or


dP = dz

(G21, G22)

Here we have partitioned G

(cid:6)

1 into

(cid:7)

1 =

G

G11 G12
G21 G22

(cid:5)(cid:6)
(cid:7)

1

(cid:7)

cT
zT

+

(cid:6)
(cid:6)

0T

dz

(cid:7)(cid:8)
(cid:7)

0
T
dz

,

+ (c, z)G

(cid:6)

1

(cid:7)

(cid:6)
(cid:7)

cT
zT

1



)G

(cid:6)

cT
zT

+ (c, z)

G12
G22

T

dz

(with G21 = G21T ).

(cid:6)

(cid:7)

(A5)

(A6)

Let

Then, also partitioning G,

(cid:6)

(cid:7)

v = (c, z)

G12
G22

.

(cid:6)

(cid:7)

G12
G22

cT v = (G11, G12)

(A7) following from(cid:6)

= 0 and zT v = (G21, G22)

(cid:7)(cid:6)

(cid:7)

(cid:6)

(cid:7)

G11 G22
G21 G22

G11 G12
G21 G22

=

I, 0
0, 1

.

G12
G22

= 1,

(A7)

Since v is a linear combination of z and the columns of c, cT v = 0 shows
that v must lie along the projection of z orthogonal to c, that is v = z(c).
Also zT v = 1 implies  = 1/(cid:6)z(c)(cid:6)2, or v = z(c)/(cid:6)z(c)(cid:6)2. The Lemma follows

from (A5).


Lemma (A3) combined with dz


= A

dy gives


ii z(c)j + A

ijz(c)i)/(cid:6)z(c)(cid:6)2.


= (A

Pij
yi

http://www.bepress.com/sagmb/vol1/iss1/art1
DOI: 10.2202/1544-6115.1000

(A7)

16

Tibshirani and Efron: Pre-validation and inference in  microarrays

(cid:1)

j

Pij
yi

yj

(cid:2)i

yi


yi

=

(cid:1)

=

i

Pii +

(cid:1)
(cid:1)
(cid:1)

j

j

Pijyj = Pii +

Pij
yi

(cid:1)

yj

Finally

so (cid:1)

i

(cid:2)i

yj

= (k + 1) +

(A

i

j

ijz(c)i)yj/(cid:6)z(c)(cid:6)2.


ii z(c)j + A

which is the top version of (6). The bottom version follows since

(cid:2)T A



z(c) = ((cid:2)T P



)(Az(c)) = 0T (Az(c)) = 0

Note:

it is not necessary for the rst mapping z = f (y) to be linear.
Result (6) holds in the nonlinear case if A is dened to be the matrix of
partial derivatives

A = (zi/yj).

