1. Introduction. Automatic model-building algorithms are familiar, and
sometimes notorious, in the linear model literature: Forward Selection, Backward
Elimination, All Subsets regression and various combinations are used to auto-
matically produce good linear models for predicting a response y on the basis
of some measured covariates x1, x2, . . . , xm. Goodness is often dened in terms
of prediction accuracy, but parsimony is another important criterion: simpler mod-
els are preferred for the sake of scientic insight into the x  y relationship. Two
promising recent model-building algorithms, the Lasso and Forward Stagewise lin-

Received March 2002; revised January 2003.
1Supported in part by NSF Grant DMS-00-72360 and NIH Grant 8R01-EB002784.
2Supported in part by NSF Grant DMS-02-04162 and NIH Grant R01-EB0011988-08.
3Supported in part by NSF Grant DMS-00-72661 and NIH Grant R01-EB001988-08.
4Supported in part by NSF Grant DMS-99-71405 and NIH Grant 2R01-CA72028.
AMS 2000 subject classication. 62J07.
Key words and phrases. Lasso, boosting, linear regression, coefcient paths, variable selection.

407

408

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

ear regression, will be discussed here, and motivated in terms of a computationally
simpler method called Least Angle Regression.

Least Angle Regression (LARS) relates to the classic model-selection method
known as Forward Selection, or forward stepwise regression, described in
Weisberg [(1980), Section 8.5]: given a collection of possible predictors, we
select the one having largest absolute correlation with the response y, say xj1,
and perform simple linear regression of y on xj1. This leaves a residual vector
orthogonal to xj1, now considered to be the response. We project the other
predictors orthogonally to xj1 and repeat the selection process. After k steps this
results in a set of predictors xj1, xj2 , . . . , xjk that are then used in the usual way
to construct a k-parameter linear model. Forward Selection is an aggressive tting
technique that can be overly greedy, perhaps eliminating at the second step useful
predictors that happen to be correlated with xj1.

Forward Stagewise, as described below, is a much more cautious version of
Forward Selection, which may take thousands of tiny steps as it moves toward
a nal model. It turns out, and this was the original motivation for the LARS
algorithm, that a simple formula allows Forward Stagewise to be implemented
using fairly large steps, though not as large as a classic Forward Selection, greatly
reducing the computational burden. The geometry of the algorithm, described in
Section 2, suggests the name Least Angle Regression. It then happens that this
same geometry applies to another, seemingly quite different, selection method
called the Lasso [Tibshirani (1996)]. The LARSLassoStagewise connection is
conceptually as well as computationally useful. The Lasso is described next, in
terms of the main example used in this paper.

Table 1 shows a small part of the data for our main example.
Ten baseline variables, age, sex, body mass index, average blood pressure
and six blood serum measurements, were obtained for each of n = 442 diabetes

Diabetes study: 442 diabetes patients were measured on 10 baseline variables; a prediction model

was desired for the response variable, a measure of disease progression one year after baseline

TABLE 1

Patient

AGE
x1

SEX BMI
x2
x3

1
2
3
4
5
6
...
441
442

59
48
72
24
50
23
...
36
36

2
1
2
1
1
1
...
1
1

32.1
21.6
30.5
25.3
23.0
22.6
...
30.0
19.6

BP
x4

101
87
93
84
101
89
...
95
71

x5

157
183
156
198
192
139
...
201
250

Serum measurements
x6
x9

x7

x8

93.2
103.2
93.6
131.4
125.4
64.8
...

125.2
133.2

38
70
41
40
52
61
...
42
97

4
3
4
5
4
2
...
5
3

4.9
3.9
4.7
4.9
4.3
4.2
...
5.1
4.6

Response

y

151
75
141
206
135
97
...
220
57

x10

87
69
85
89
80
68
...
85
92

LEAST ANGLE REGRESSION

409

(1.1)

yi = 0,

xij = 0,

n(cid:1)

i=1

n(cid:1)

i=1

n(cid:1)

i=1

patients, as well as the response of interest, a quantitative measure of disease
progression one year after baseline. The statisticians were asked to construct
a model that predicted response y from covariates x1, x2, . . . , x10. Two hopes
were evident here, that the model would produce accurate baseline predictions
of response for future patients and that the form of the model would suggest which
covariates were important factors in disease progression.
The Lasso is a constrained version of ordinary least squares (OLS). Let x1, x2,
. . . , xm be n-vectors representing the covariates, m = 10 and n = 442 in the
diabetes study, and let y be the vector of responses for the n cases. By location
and scale transformations we can always assume that the covariates have been
standardized to have mean 0 and unit length, and that the response has mean 0,
for j = 1, 2, . . . , m.
(cid:2)
(cid:2)

= 1
A candidate vector of regression coefcients (cid:2)
prediction vector(cid:2),(cid:2) = m(cid:1)
 = (

This is assumed to be the case in the theory which follows, except that numerical
results are expressed in the original units of the diabetes example.

(cid:2)
j = X
(cid:2)) = (cid:2)y (cid:2)(cid:2)2 = n(cid:1)
(yi (cid:2)i )2.
(cid:2)) be the absolute norm of(cid:2),
) = m(cid:1)
(cid:2)
|(cid:2)
j|.
(cid:2)) subject to a bound t on T (
The Lasso chooses(cid:2) by minimizing S(
(cid:2)),
(cid:2))
(cid:2))  t.

j=1
with total squared error

[Xnm = (x1, x2, . . . , xm)]

Lasso: minimize S(

(cid:1)
m)

gives

subject to T (

(1.3)

Let T (

(1.4)

T (

j=1

(cid:2)

1,

2, . . . ,

x2
ij

i=1

(1.2)

(1.5)

(cid:2)



xj

S(

Quadratic programming techniques can be used to solve (1.5) though we will
present an easier method here, closely related to the homotopy method of
Osborne, Presnell and Turlach (2000a).

The left panel of Figure 1 shows all Lasso solutions(cid:2)
as t increases from 0, where (cid:2)

(t) for the diabetes study,
 equals the OLS
regression vector, the constraint in (1.5) no longer binding. We see that the Lasso
tends to shrink the OLS coefcients toward 0, more so for small values of t.
Shrinkage often improves prediction accuracy, trading off decreased variance for
increased bias as discussed in Hastie, Tibshirani and Friedman (2001).

 = 0, to t = 3460.00, where (cid:2)

410

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

FIG. 1. Estimates of regression coefcients (cid:2)
panel) Lasso estimates, as a function of t =(cid:3)
|(cid:2)
j , j = 1, 2, . . . , 10, for the diabetes study. (Left
j|. The covariates enter the regression equation
sequentially as t increases, in order j = 3, 9, 4, 7, . . . , 1. (Right panel) The same plot for Forward
Stagewise Linear Regression. The two plots are nearly identical, but differ slightly for large t as
shown in the track of covariate 8.

j

a subset of the covariates have nonzero values of (cid:2)

Forward Stagewise Linear Regression, henceforth called Stagewise, is an

The Lasso also has a parsimony property: for any given constraint value t, only
j . At t = 1000, for example,
only variables 3, 9, 4 and 7 enter the Lasso regression model (1.2). If this model
provides adequate predictions, a crucial question considered in Section 4, the
statisticians could report these four variables as the important ones.

iterative technique that begins with (cid:2) = 0 and builds up the regression function
in successive small steps. If (cid:2) is the current Stagewise estimate, let c((cid:2)) be the
vector of current correlations(cid:2)c = c((cid:2)) = X
so that(cid:2)cj is proportional to the correlation between covariate xj and the current

(y (cid:2)),

(cid:1)

(1.6)

(cid:2)
j = arg max|(cid:2)cj|

and (cid:2) (cid:2) +   sign((cid:2)c j )  x j ,

residual vector. The next step of the Stagewise algorithm is taken in the direction
of the greatest current correlation,

(1.7)

with  some small constant. Small is important here: the big choice  = |(cid:2)c j

|
leads to the classic Forward Selection technique, which can be overly greedy,
impulsively eliminating covariates which are correlated with x j . The Stagewise
procedure is related to boosting and also to Friedmans MART algorithm

LEAST ANGLE REGRESSION

411

[Friedman (2001)]; see Section 8, as well as Hastie, Tibshirani and Friedman
[(2001), Chapter 10 and Algorithm 10.4].

The right panel of Figure 1 shows the coefcient plot for Stagewise applied to
the diabetes data. The estimates were built up in 6000 Stagewise steps [making
 in (1.7) small enough to conceal the Etch-a-Sketch staircase seen in Figure 2,
Section 2]. The striking fact is the similarity between the Lasso and Stagewise
estimates. Although their denitions look completely different, the results are
nearly, but not exactly, identical.

The main point of this paper is that both Lasso and Stagewise are variants of
a basic procedure called Least Angle Regression, abbreviated LARS (the S
suggesting Lasso and Stagewise). Section 2 describes the LARS algorithm
while Section 3 discusses modications that turn LARS into Lasso or Stagewise,
reducing the computational burden by at least an order of magnitude for either one.
Sections 5 and 6 verify the connections stated in Section 3.

Least Angle Regression is interesting in its own right, its simple structure
lending itself to inferential analysis. Section 4 analyzes the degrees of freedom
of a LARS regression estimate. This leads to a Cp type statistic that suggests which
estimate we should prefer among a collection of possibilities like those in Figure 1.
A particularly simple Cp approximation, requiring no additional computation

beyond that for the(cid:2)

 vectors, is available for LARS.

Section 7 briey discusses computational questions. An efcient S program for
all three methods, LARS, Lasso and Stagewise, is available. Section 8 elaborates
on the connections with boosting.

2. The LARS algorithm. Least Angle Regression is a stylized version of the
Stagewise procedure that uses a simple mathematical formula to accelerate
the computations. Only m steps are required for the full set of solutions, where
m is the number of covariates: m = 10 in the diabetes example compared to the
6000 steps used in the right panel of Figure 1. This section describes the LARS
algorithm. Modications of LARS that produce Lasso and Stagewise solutions are
discussed in Section 3, and veried in Sections 5 and 6. Section 4 uses the simple
structure of LARS to help analyze its estimation properties.

The LARS procedure works roughly as follows. As with classic Forward
Selection, we start with all coefcients equal to zero, and nd the predictor
most correlated with the response, say xj1. We take the largest step possible in
the direction of this predictor until some other predictor, say xj2, has as much
correlation with the current residual. At this point LARS parts company with
Forward Selection. Instead of continuing along xj1, LARS proceeds in a direction
equiangular between the two predictors until a third variable xj3 earns its way
into the most correlated set. LARS then proceeds equiangularly between xj1, xj2
and xj3, that is, along the least angle direction, until a fourth variable enters, and
so on.

412

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

c((cid:2)) = X

(cid:1)

(y2 (cid:2)).

(y (cid:2)) = X

(cid:1)

The remainder of this section describes the algebra necessary to execute the
equiangular strategy. As usual the algebraic details look more complicated than
the simple underlying geometry, but they lead to the highly efcient computational
algorithm described in Section 7.

(cid:2), (1.2), in successive steps, each step adding
LARS builds up estimates(cid:2) = X
one covariate to the model, so that after k steps just k of the (cid:2)
j s are nonzero.
Figure 2 illustrates the algorithm in the situation with m = 2 covariates, X =
(x1, x2). In this case the current correlations (1.6) depend only on the projection y2
of y into the linear space L(X) spanned by x1 and x2,
The algorithm begins at (cid:2)0 = 0 [remembering that the response has had its
(2.1)
mean subtracted off, as in (1.1)]. Figure 2 has y2 (cid:2)0 making a smaller angle
with x1 than x2, that is, c1((cid:2)0) > c2((cid:2)0). LARS then augments(cid:2)0 in the direction
Stagewise would choose (cid:2)1 equal to some small value , and then repeat the
process many times. Classic Forward Selection would take (cid:2)1 large enough to
make (cid:2)1 equal y1, the projection of y into L(x1). LARS uses an intermediate
value of(cid:2)1, the value that makes y2 (cid:2), equally correlated with x1 and x2; that is,
y2 (cid:2)1 bisects the angle between x1 and x2, so c1((cid:2)1) = c2((cid:2)1).

(cid:2)1 =(cid:2)0 +(cid:2)1x1.

of x1, to
(2.2)

L(x1, x2). Beginning at(cid:2)0 = 0, the residual vector y2 (cid:2)0 has greater correlation with x1 than x2;
FIG. 2. The LARS algorithm in the case of m = 2 covariates; y2 is the projection of y into
the next LARS estimate is(cid:2)1 =(cid:2)0 +(cid:2)1x1, where(cid:2)1 is chosen such that y2 (cid:2)1 bisects the angle
between x1 and x2; then(cid:2)2 =(cid:2)1 +(cid:2)2u2, where u2 is the unit bisector;(cid:2)2 = y2 in the case m = 2,
but not for the case m > 2; see Figure 4. The staircase indicates a typical Stagewise path. Here LARS
gives the Stagewise track as   0, but a modication is necessary to guarantee agreement in higher
dimensions; see Section 3.2.

LEAST ANGLE REGRESSION

413

(cid:2)2 =(cid:2)1 +(cid:2)2u2,

Let u2 be the unit vector lying along the bisector. The next LARS estimate is

(2.3)

with (cid:2)2 chosen to make (cid:2)2 = y2 in the case m = 2. With m > 2 covariates,
(cid:2)2 would be smaller, leading to another change of direction, as illustrated in
is motivated by the fact that it is easy to calculate the step sizes (cid:2)1,(cid:2)2, . . .

Figure 4. The staircase in Figure 2 indicates a typical Stagewise path. LARS

theoretically, short-circuiting the small Stagewise steps.

XA = ( sj xj )jA,

Subsequent LARS steps, beyond two covariates, are taken along equiangular
vectors, generalizing the bisector u2 in Figure 2. We assume that the covariate
vectors x1, x2, . . . , xm are linearly independent. For A a subset of the indices
{1, 2, . . . , m}, dene the matrix
(2.4)
where the signs sj equal 1. Let
1/2,
(2.5)
1A being a vector of 1s of length equaling |A|, the size of A. The
where wA = AAG
(2.6)


equiangular vector uA = XAwA

AXA and AA = (1
(cid:1)

GA = X

1
A 1A)

1
A 1A,

is the unit vector making equal angles, less than 90

, with the columns of XA,

X

(2.7)

AuA = AA1A and (cid:2)uA(cid:2)2 = 1.
(cid:1)
procedure we begin at(cid:2)0 = 0 and build up(cid:2) by steps, larger steps in the LARS
case. Suppose that(cid:2)A is the current LARS estimate and that

We can now fully describe the LARS algorithm. As with the Stagewise

(cid:1)
AG

is the vector of current correlations (1.6). The active set A is the set of indices
corresponding to covariates with the greatest absolute current correlations,

we compute XA, AA and uA as in (2.4)(2.6), and also the inner product vector
(2.11)

Then the next step of the LARS algorithm updates(cid:2)A, say to

uA.

(2.8)

(2.9)

Letting

(2.10)

(2.12)

(cid:2)
C = max

j

(cid:1)

(cid:2)c = X
(y (cid:2)A)
and A = {j :|(cid:2)cj| =(cid:2)
{|(cid:2)cj|}
C}.
sj = sign{(cid:2)cj}
a  X
(cid:2)A+ =(cid:2)A

+(cid:2) uA,

for j  A,

(cid:1)

414

where

(2.13)
+

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

(cid:2)
C +(cid:2)cj
AA + aj

,

(cid:5)

;

(cid:2) = min

+
jAc

(cid:4) (cid:2)
C (cid:2)cj
AA  aj
( ) =(cid:2)A
(cid:6)
y  ( )
|cj ( )| =(cid:2)

(cid:1)

j

 indicates that the minimum is taken over only positive components within

min
each choice of j in (2.13).

+  uA,

Formulas (2.12) and (2.13) have the following interpretation: dene

cj ( ) = x

(cid:7) =(cid:2)cj   aj .

(2.14)
for  > 0, so that the current correlation
(2.15)
For j  A, (2.7)(2.9) yield
C   AA,
(2.16)
(cid:2)
C (cid:2)cj )/(AA aj ). Likewise cj ( ), the current correlation for the
showing that all of the maximal absolute current correlations decline equally.
For j  Ac, equating (2.15) with (2.16) shows that cj ( ) equals the maximal
(cid:2)
C +(cid:2)cj )/(AA + aj ). Therefore(cid:2)
value at  = (
in (2.13) is the smallest positive value of  such that some new index (cid:2)
reversed covariate xj , achieves maximality at (
the active set; (cid:2)
C+ =(cid:2)
j}; the new maximum absolute correlation is (cid:2)
A  {(cid:2)
Figure 3 concerns the LARS analysis of the diabetes data. The complete
algorithm required only m = 10 steps of procedure (2.8)(2.13), with the variables

j joins
j is the minimizing index in (2.13), and the new active set A+ is

C (cid:2) AA.

FIG. 3. LARS analysis of the diabetes study: (left) estimates of regression coefcients (cid:2)
(cid:3)|(cid:2)
j ,
j|; plot is slightly different than either Lasso or Stagewise,
j = 1, 2, . . . , 10; plotted versus
set (2.9) in order 3, 9, 4, 7, . . . , 1; heavy curve shows maximum current correlation (cid:2)
Figure 1; (right) absolute current correlations as function of LARS step; variables enter active
Ck declining

with k.

LEAST ANGLE REGRESSION

of the regression coefcients (cid:2)

joining the active set A in the same order as for the Lasso: 3, 9, 4, 7, . . . , 1. Tracks
j are nearly but not exactly the same as either the

415

active set increases.

Lasso or Stagewise tracks of Figure 1.

The right panel shows the absolute current correlations

(cid:1)

j (y (cid:2)k1)|
Ck1 (cid:2)k1Ak1

Section 4 makes use of the relationship between Least Angle Regression and
Ordinary Least Squares illustrated in Figure 4. Suppose LARS has just completed

|(cid:2)ckj| = |x
(2.17)
for variables j = 1, 2, . . . , 10, as a function of the LARS step k. The maximum
(cid:2)
Ck = max{|(cid:2)ckj|} =(cid:2)
correlation
(2.18)
henceforth having |(cid:2)ckj| =(cid:2)
declines with k, as it must. At each step a new variable j joins the active set,
Ck. The sign sj of each xj in (2.4) stays constant as the
step k  1, giving(cid:2)k1, and is embarking upon step k. The active set Ak, (2.9),
(cid:2)k1  L(Xk1), is
will have k members, giving Xk, Gk, Ak and uk as in (2.4)(2.6) (here replacing
subscript A with k). Let yk indicate the projection of y into L(Xk), which, since
yk =(cid:2)k1 + XkG
correlations in Ak all equal (cid:2)
Since uk is a unit vector, (2.19) says that yk (cid:2)k1 has length
Comparison with (2.12) shows that the LARS estimate (cid:2)k lies on the line

k(y (cid:2)k1) =(cid:2)
(cid:2)

k(y (cid:2)k1) =(cid:2)k1 +

(cid:1)

the last equality following from (2.6) and the fact

that

the signed current

(cid:2)

Ck
Ak

k 

Ck
Ak

.

1
k X

Ck,
(cid:1)
X

Ck1A.

uk,

(2.19)

(2.20)

(2.21)

FIG. 4. At each stage the LARS estimate (cid:2)k approaches, but does not reach, the corresponding
OLS estimate yk.

416

(2.22)

(cid:2)k

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

(yk (cid:2)k1).

(cid:2)k1 = (cid:2)kk

from(cid:2)k1 to yk,
It is easy to see that(cid:2)k, (2.12), is always less than k, so that(cid:2)k lies closer than yk
to (cid:2)k1. Figure 4 shows the successive LARS estimates (cid:2)k always approaching
but never reaching the OLS estimates yk.
dened. By convention the algorithm takes(cid:2)m = m =(cid:2)
and(cid:2)
The exception is at the last stage: since Am contains all covariates, (2.13) is not
= ym

m equal the OLS estimate for the full set of m covariates.
The LARS algorithm is computationally thrifty. Organizing the calculations
correctly, the computational cost for the entire m steps is of the same order as
that required for the usual Least Squares solution for the full set of m covariates.
Section 7 describes an efcient LARS program available from the authors.
With the modications described in the next section, this program also provides
economical Lasso and Stagewise solutions.

Cm/Am, making(cid:2)m

3. Modied versions of Least Angle Regression. Figures 1 and 3 show
Lasso, Stagewise and LARS yielding remarkably similar estimates for the diabetes
data. The similarity is no coincidence. This section describes simple modications
of the LARS algorithm that produce Lasso or Stagewise estimates. Besides
improved computational efciency, these relationships elucidate the methods
rationale: all three algorithms can be viewed as moderately greedy forward
stepwise procedures whose forward progress is determined by compromise among
the currently most correlated covariates. LARS moves along the most obvious
compromise direction, the equiangular vector (2.6), while Lasso and Stagewise
put some restrictions on the equiangular strategy.

3.1. The LARSLasso relationship. The full set of Lasso solutions, as shown
for the diabetes study in Figure 1, can be generated by a minor modication of
the LARS algorithm (2.8)(2.13). Our main result is described here and veried
in Section 5. It closely parallels the homotopy method in the papers by Osborne,
Presnell and Turlach (2000a, b), though the LARS approach is somewhat more
direct.

Let (cid:2)
(cid:2)
 be a Lasso solution (1.5), with (cid:2) = X
the sign of any nonzero coordinate (cid:2)
correlation(cid:2)cj = x
(cid:2)
j ) = sign((cid:2)cj ) = sj;

j (y (cid:2)),

sign(

(3.1)

(cid:1)

. Then it is easy to show that
j must agree with the sign sj of the current

see Lemma 8 of Section 5. The LARS algorithm does not enforce restriction (3.1),
but it can easily be modied to do so.

LEAST ANGLE REGRESSION

417

(cid:2)

dj

where j ( ) =(cid:2)
j + 

(cid:2)

Suppose we have just completed a LARS step, giving a new active set A as

in (2.9), and that the corresponding LARS estimate (cid:2)A corresponds to a Lasso
solution(cid:2) = X
dene(cid:2)d to be the m-vector equaling sj wAj for j  A and zero elsewhere. Moving

(3.2)
a vector of length the size of A, and (somewhat abusing subscript notation)

wA = AAG

1
A 1A,

. Let

(3.5)

( ) = X( ),

in the positive  direction along the LARS line (2.14), we see that
(3.3)
(cid:2)
j = (cid:2)
for j  A. Therefore j ( ) will change sign at
(3.4)
the rst such change occurring at (cid:8) = min
{j},
say for covariate x j ;(cid:8) equals innity by denition if there is no j > 0.
If(cid:8) is less than(cid:2) , (2.13), then j ( ) cannot be a Lasso solution for  >(cid:8) since
since |c j ( )| =(cid:2)
remove(cid:8)

the sign restriction (3.1) must be violated:  j ( ) has changed sign while c j ( ) has
not. [The continuous function c j ( ) cannot change sign within a single LARS step

If(cid:8) <(cid:2) , stop the ongoing LARS step at  =(cid:8) and
+(cid:8) uA and A+ = A  {(cid:8)
j}

C   AA > 0, (2.16).]
(cid:2)A+ =(cid:2)A

j from the calculation of the next equiangular direction. That is,

LASSO MODIFICATION.

j /

dj ,

j >0

(3.6)
rather than (2.12).

THEOREM 1. Under the Lasso modication, and assuming the one at a time

condition discussed below, the LARS algorithm yields all Lasso solutions.

The active sets A grow monotonically larger as the original LARS algorithm
progresses, but the Lasso modication allows A to decrease. One at a time
means that the increases and decreases never involve more than a single index j .
This is the usual case for quantitative data and can always be realized by adding a
little jitter to the y values. Section 5 discusses tied situations.

The Lasso diagram in Figure 1 was actually calculated using the modied LARS
algorithm. Modication (3.6) came into play only once, at the arrowed point in
the left panel. There A contained all 10 indices while A+ = A  {7}. Variable 7
taking (cid:2)
had an effect on the tracks of the others, noticeably (cid:2)
was restored to the active set one LARS step later, the next and last step then
 all the way to the full OLS solution. The brief absence of variable 7
8. The price of using Lasso
instead of unmodied LARS comes in the form of added steps, 12 instead of 10
in this example. For the more complicated quadratic model of Section 4, the
comparison was 103 Lasso steps versus 64 for LARS.

418

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

3.2. The LARSStagewise relationship. The staircase in Figure 2 indicates

how the Stagewise algorithm might proceed forward from (cid:2)1, a point of equal
current correlations(cid:2)c1 =(cid:2)c2, (2.8). The rst small step has (randomly) selected
index j = 1, taking us to(cid:2)1 + x1. Now variable 2 is more correlated,
1(y (cid:2)1  x1),
(3.7)
forcing j = 2 to be the next Stagewise choice and so on.

2(y (cid:2)1  x1) > x

x

(cid:1)

(cid:1)

Suppose that the Stagewise procedure has taken N steps of innitesimal size

We will consider an idealized Stagewise procedure in which the step size  goes
to zero. This collapses the staircase along the direction of the bisector u2 in
Figure 2, making the Stagewise and LARS estimates agree. They always agree
for m = 2 covariates, but another modication is necessary for LARS to produce
 from some previous estimate(cid:2), with
Stagewise estimates in general. Section 6 veries the main result described next.
(3.8)
It is easy to show, as in Lemma 11 of Section 6, that Nj = 0 for j not in the active
set A dened by the current correlations x

j = 1, 2, . . . , m.
j (y (cid:2)), (2.9). Letting

Nj  #{steps with selected index j},

(cid:1)

(3.9)
with PA indicating the coordinates of P for j  A, the new estimate is
(3.10)

P  (N1, N2, . . . , Nm)/N ,
 =(cid:2) + N XAPA

[(2.4)].

(Notice that the Stagewise steps are taken along the directions sj xj .)

The LARS algorithm (2.14) progresses along
where wA = AAG

+  XAwA,

A

(3.11)

1
A 1A

[(2.6)(3.2)].

Comparing (3.10) with (3.11) shows that LARS cannot agree with Stagewise if
wA has negative components, since PA is nonnegative. To put it another way, the
direction of Stagewise progress XAPA must lie in the convex cone generated by
the columns of XA,

(cid:10)

(3.12)

CA =

sj xj Pj , Pj  0

.

(cid:9)
v = (cid:1)

jA

If uA  CA then there is no contradiction between (3.12) and (3.13). If not it
seems natural to replace uA with its projection into CA, that is, the nearest point
in the convex cone.

STAGEWISE MODIFICATION. Proceed as in (2.8)(2.13), except with uA
replaced by u B, the unit vector lying along the projection of uA into CA. (See
Figure 9 in Section 6.)

LEAST ANGLE REGRESSION

419

THEOREM 2. Under the Stagewise modication, the LARS algorithm yields

all Stagewise solutions.

The vector u B in the Stagewise modication is the equiangular vector (2.6) for

the subset (cid:2)B  A corresponding to the face of CA into which the projection falls.
there the set A = {3, 9, 4, 7, 2, 10, 5, 8} was decreased to (cid:2)B = A{3, 7}. It took a
total of 13 modied LARS steps to reach the full OLS solution m
1X

Stagewise is a LARS type algorithm that allows the active set to decrease by one
or more indices. This happened at the arrowed point in the right panel of Figure 1:

y.
The three methods, LARS, Lasso and Stagewise, always reach OLS eventually,
but LARS does so in only m steps while Lasso and, especially, Stagewise can take
longer. For the m = 64 quadratic model of Section 4, Stagewise took 255 steps.

= (X

X)

(cid:1)

(cid:1)

According to Theorem 2 the difference between successive Stagewisemodied

LARS estimates is

(cid:2)A+ (cid:2)A

=(cid:2) u B

=(cid:2) X B w B,

(cid:1)

sign(

j ) = sj ,

We can now make a useful comparison of the three methods:

(3.13)
as in (3.13). Since u B exists in the convex cone CA, w B must have nonnegative
components. This says that the difference of successive coefcient estimates for

coordinate j  (cid:2)B satises
(cid:2)
+j (cid:2)
j (y (cid:2))}.
(3.14)
where sj = sign{x
1. Stagewisesuccessive differences of (cid:2)
j (y (cid:2));
correlation(cid:2)cj = x
2. Lasso(cid:2)
The successive difference property (3.14) makes the Stagewise (cid:2)
move monotonically away from 0. Reversals are possible only if(cid:2)cj changes sign
while (cid:2)

From this point of view, Lasso is intermediate between the LARS and Stagewise
methods.

j agrees in sign with(cid:2)cj ;

j is resting between two periods of change. This happened to variable 7

3. LARSno sign restrictions (but see Lemma 4 of Section 5).

j agree in sign with the current

j estimates

(cid:1)

in Figure 1 between the 8th and 10th Stagewise-modied LARS steps.

3.3. Simulation study. A small simulation study was carried out comparing
the LARS, Lasso and Stagewise algorithms. The X matrix for the simulation was
based on the diabetes example of Table 1, but now using a Quadratic Model
having m = 64 predictors, including interactions and squares of the 10 original
covariates:

(3.15)

Quadratic Model 10 main effects, 45 interactions, 9 squares,

420

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

the last being the squares of each xj except the dichotomous variable x2. The true
mean vector  for the simulation was  = X, where  was obtained by running
LARS for 10 steps on the original (X, y) diabetes data (agreeing in this case with
the 10-step Lasso or Stagewise analysis). Subtracting  from a centered version of
the original y vector of Table 1 gave a vector  = y   of n = 442 residuals. The
true R2 for this model, (cid:2)(cid:2)2/((cid:2)(cid:2)2 + (cid:2)(cid:2)2), equaled 0.416.

100 simulated response vectors y
y


were generated from the model
 =  + 



,

 = (


1 , 



2 , . . . , 

Figure 5 compares the LARS, Lasso and Stagewise estimates. For a given

(3.16)

n) a random sample, with replacement, from the compo-
with 
), yielding a sequence of estimates(cid:2)(k)
nents of . The LARS algorithm with K = 40 steps was run for each simulated
, k = 1, 2, . . . , 40, and like-
data set (X, y
estimate(cid:2) dene the proportion explained pe((cid:2)) to be
wise using the Lasso and Stagewise algorithms.
pe((cid:2)) = 1  (cid:2)(cid:2)  (cid:2)2/(cid:2)(cid:2)2,
so pe(0) = 0 and pe() = 1. The solid curve graphs the average of pe((cid:2)(k)
(3.17)
over the 100 simulations, versus step number k for LARS, k = 1, 2, . . . , 40.
horizontal axis is now the average number of nonzero (cid:2)
j terms composing(cid:2)(k)
The corresponding curves are graphed for Lasso and Stagewise, except that the
For example,(cid:2)(40)
.
averaged 33.23 nonzero terms with Stagewise, compared to





)

35.83 for Lasso and 40 for LARS.

Figure 5s most striking message is that the three algorithms performed almost
identically, and rather well. The average proportion explained rises quickly,

FIG. 5.
Simulation study comparing LARS, Lasso and Stagewise algorithms; 100 replications of
model (3.15)(3.16). Solid curve shows average proportion explained, (3.17), for LARS estimates
as function of number of steps k = 1, 2, . . . , 40; Lasso and Stagewise give nearly identical results;
small dots indicate plus or minus one standard deviation over the 100 simulations. Classic Forward
Selection (heavy dashed curve) rises and falls more abruptly.

LEAST ANGLE REGRESSION

421

to 40. The light dots display the small standard deviation of pe((cid:2)(k)
reaching a maximum of 0.963 at k = 10, and then declines slowly as k grows
gave a(cid:2)(k)
) over the 100
simulations, roughly 0.02. Stopping at any point between k = 5 and 25 typically
with true predictive R2 about 0.40, compared to the ideal value 0.416
for .
The dashed curve in Figure 5 tracks the average proportion explained by classic
Forward Selection. It rises very quickly, to a maximum of 0.950 after k = 3 steps,
and then falls back more abruptly than the LARSLassoStagewise curves. This
behavior agrees with the characterization of Forward Selection as a dangerously
greedy algorithm.

3.4. Other LARS modications. Here are a few more examples of LARS type

model-building algorithms.

POSITIVE LASSO. Constraint (1.5) can be strengthened to

(3.18)

minimize S(

)

subject to T (

(cid:2)

(cid:2)
)  t and all (cid:2)

j  0.

This would be appropriate if the statisticians or scientists believed that the
variables xj must enter the prediction equation in their dened directions.
Situation (3.18) is a more difcult quadratic programming problem than (1.5),
but it can be solved by a further modication of the Lasso-modied LARS

algorithm: change |(cid:2)cj| to(cid:2)cj at both places in (2.9), set sj = 1 instead of (2.10)

and change (2.13) to

(3.19)

(cid:2) = min

+
jAc

(cid:4) (cid:2)
C (cid:2)cj
AA  aj

(cid:5)

.

very large choices of t.

The positive Lasso usually does not converge to the full OLS solution m, even for
in the same way, and has the property that the(cid:2)
The changes above amount to considering the xj as generating half-lines rather
than full one-dimensional spaces. A positive Stagewise version can be developed
of covariates, for example, A4 = {3, 9, 4, 7} in the diabetes study. Instead of(cid:2)
LARSOLS hybrid. After k steps the LARS algorithm has identied a set Ak
might prefer  k, the OLS coefcients based on the linear model with covariates
 k we
in Akusing LARS to nd the model but not to estimate the coefcients. Besides
looking more familiar, this will always increase the usual empirical R2 measure of
(cid:2) k)  R2(
t (though not necessarily the true tting accuracy),
[R2(

j tracks are always monotone.

(cid:2)k) = 1  2
k(2  k)

(cid:2) k1)],

(3.20)

R2(k)  R2(
where k =(cid:2)k/ k as in (2.22).

k

422

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

The increases in R2 were small in the diabetes example, on the order of 0.01
for k  4 compared with R2 = 0.50, which is expected from (3.20) since we
reason k and (cid:2)
 k1) was small. For the same
would usually continue LARS until R2(
 k are likely to lie near each other as they did in the diabetes

(cid:2)
(cid:2)
k)  R2(

example.

Main effects rst.

It is straightforward to restrict the order in which variables
are allowed to enter the LARS algorithm. For example, having obtained A4 =
do this we begin LARS again, replacing y with y(cid:2)4 and x with the n 6 matrix
{3, 9, 4, 7} for the diabetes study, we might then wish to check for interactions. To
whose columns represent the interactions x3:9, x3:4, . . . , x4:7.

Backward Lasso. The Lassomodied LARS algorithm can be run backward,

are nonzero, their signs must agree with the signs sj that the current correlations
had during the nal LARS step. This allows us to calculate the last equiangular

starting from the full OLS solution m. Assuming that all the coordinates of  m
direction uA, (2.4)(2.6). Moving backward from (cid:2)m
  uA, we eliminate from the active set the index of the rst (cid:2)
( ) =(cid:2)m
 m along the line
= X
that becomes zero. Continuing backward, we keep track of all coefcients (cid:2)
current correlations(cid:2)cj , following essentially the same rules for changing A as in
Section 3.1. As in (2.3), (3.5) the calculation of(cid:8) and(cid:2) is easy.

j
j and

The crucial property of the Lasso that makes backward navigation possible
is (3.1), which permits calculation of the correct equiangular direction uA at each
step. In this sense Lasso can be just as well thought of as a backward-moving
algorithm. This is not the case for LARS or Stagewise, both of which are inherently
forward-moving algorithms.

4. Degrees of freedom and Cp estimates. Figures 1 and 3 show all possible
Lasso, Stagewise or LARS estimates of the vector  for the diabetes data. The
 of course, so we need some rule for selecting among
the possibilities. This section concerns a Cp-type selection criterion, especially as
it applies to the choice of LARS estimate.

scientists want just a single(cid:2)
Let (cid:2) = g(y) represent a formula for estimating  from the data vector y.

Here, as usual in regression situations, we are considering the covariate vectors
x1, x2, . . . , xm xed at their observed values. We assume that given the xs, y is
generated according to an homoskedastic model
y  (,  2I),

(4.1)
meaning that the components yi are uncorrelated, with mean i and variance  2.
Taking expectations in the identity

((cid:2)i  i)2 = (yi (cid:2)i )2  (yi  i)2 + 2((cid:2)i  i )(yi  i),

(4.2)

LEAST ANGLE REGRESSION

(cid:4)(cid:2)y (cid:2)(cid:2)2

 2

(cid:5)

 n

= E

n(cid:1)

i=1

cov((cid:2)i , yi)

 2

.

+ 2

423

The last term of (4.3) leads to a convenient denition of the degrees of freedom

(4.3)

and summing over i, yields

(cid:4)(cid:2)(cid:2)  (cid:2)2
(cid:5)
for an estimator(cid:2) = g(y),

 2

E

(4.4)

i=1
and a Cp-type risk estimation formula,

cov((cid:2)i, yi )/ 2,

df, 2 = n(cid:1)
Cp((cid:2)) = (cid:2)y (cid:2)(cid:2)2

 n + 2df, 2.

 2

(4.5)

If  2 and df, 2 are known, Cp((cid:2)) is an unbiased estimator of the true risk
E{(cid:2)(cid:2)  (cid:2)2/ 2}. For linear estimators (cid:2) = My, model (4.1) makes df, 2 =

trace(M), equaling the usual denition of degrees of freedom for OLS, and
coinciding with the proposal of Mallows (1973). Section 6 of Efron and Tibshirani
(1997) and Section 7 of Efron (1986) discuss formulas (4.4) and (4.5) and their
role in Cp, Akaike information criterion (AIC) and Steins unbiased risk estimated
(SURE) estimation theory, a more recent reference being Ye (1998).

Practical use of Cp formula (4.5) requires preliminary estimates of ,  2 and
df, 2. In the numerical results below, the usual OLS estimates  and  2 from
the full OLS model were used to calculate bootstrap estimates of df, 2; bootstrap
samples y

and replications(cid:2)

were then generated according to





(4.6)

y

  N (,  2)

and (cid:2)

 = g(y



).

(cid:3)

B

(cid:3)

B

,





(b)

(4.8)

(4.7)

where y

and then

() =

b=1 y
B

(cid:11)covi =

Independently repeating (4.6) say B times gives straightforward estimates for the
covariances in (4.4),
b=1

(cid:2)
i ()]
i (b)[ y
i (b)  y



B  1
df = n(cid:1)
(cid:12)
(cid:11)covi / 2.
The left panel of Figure 6 shows (cid:12)
Normality is not crucial in (4.6). Nearly the same results were obtained using
were resampled from e = y  .
 = 
 + e

mates(cid:2)k, k = 1, 2, . . . , m = 10. It portrays a startlingly simple situation that we
y
dfk for the diabetes data LARS esti-
df ((cid:2)k) = k.

will call the simple approximation,

, where the components of e

(4.9)

i=1



,

424

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

FIG. 6. Degrees of freedom for LARS estimates (cid:2)k: (left) diabetes study, Table 1, k = 1,
2, . . . , m = 10; (right) quadratic model (3.15) for the diabetes data, m = 64. Solid line is simple
approximation dfk = k. Dashed lines are approximate 95% condence intervals for the bootstrap
estimates. Each panel based on B = 500 bootstrap replications.

The right panel also applies to the diabetes data, but this time with the quadratic
model (3.15), having m = 64 predictors. We see that the simple approxima-
tion (4.9) is again accurate within the limits of the bootstrap computation (4.8),
where B = 500 replications were divided into 10 groups of 50 each in order to
estimate the risk of a k-step LARS estimator(cid:2)k by
calculate Student-t condence intervals.

If (4.9) can be believed, and we will offer some evidence in its behalf, we can

(4.10)

Cp((cid:2)k) = (cid:2)y (cid:2)k

(cid:2)2/ 2  n + 2k.

The formula, which is the same as the Cp estimate of risk for an OLS estimator
based on a subset of k preselected predictor vectors, has the great advantage of not
Figure 7 displays Cp((cid:2)k) as a function of k for the two situations of Figure 6.
requiring any further calculations beyond those for the original LARS estimates.
The formula applies only to LARS, and not to Lasso or Stagewise.
Minimum Cp was achieved at steps k = 7 and k = 16, respectively. Both of the
minimum Cp models looked sensible, their rst several selections of important
covariates agreeing with an earlier model based on a detailed inspection of the data
assisted by medical expertise.

The simple approximation becomes a theorem in two cases.

THEOREM 3.

then the k-step LARS estimate k has df (k) = k.

If the covariate vectors x1, x2, . . . , xm are mutually orthogonal,

To state the second more general setting we introduce the following condition.

LEAST ANGLE REGRESSION

425

FIG. 7. Cp estimates of risk (4.10) for the two situations of Figure 6: (left) m = 10 model has
smallest Cp at k = 7; (right) m = 64 model has smallest Cp at k = 16.

POSITIVE CONE CONDITION. For all possible subsets XA of the full design

matrix X,

(4.11)

1
A 1A > 0,

G

where the inequality is taken element-wise.

The positive cone condition holds if X is orthogonal. It is strictly more general
than orthogonality, but counterexamples (such as the diabetes data) show that not
all design matrices X satisfy it.

It is also easy to show that LARS, Lasso and Stagewise all coincide under the
positive cone condition, so the degrees-of-freedom formula applies to them too in
this case.

THEOREM 4. Under the positive cone condition, df (k) = k.
differentiable (see Remark A.1 in the Appendix) and set   g =(cid:3)
The proof, which appears later in this section, is an application of Steins
unbiased risk estimate (SURE) [Stein (1981)]. Suppose that g : Rn  Rn is almost
i=1 gi/xi .
If y  Nn(,  2I), then Steins formula states that

n

(4.12)

cov(gi , yi )/ 2 = E[  g(y)].

n(cid:1)

i=1

The left-hand side is df (g) for the general estimator g(y). Focusing specically
on LARS, it will turn out that   k(y) = k in all situations with probability 1,
but that the continuity assumptions underlying (4.12) and SURE can fail in certain
nonorthogonal cases where the positive cone condition does not hold.

426

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

effort at pathology to make df ((cid:2)k) much different than k.
A range of simulations suggested that the simple approximation is quite
accurate even when the xj s are highly correlated and that it requires concerted
Steins formula assumes normality, y  N (,  2I). A cruder delta method
rationale for the simple approximation requires only homoskedasticity, (4.1). The
geometry of Figure 4 implies(cid:2)k
cotk =

(4.13)
where cotk is the cotangent of the angle between uk and uk+1,

= yk  cotk  (cid:2)yk+1  yk(cid:2),

(4.14)

(cid:1)
kuk+1
u
kuk+1)2]1/2 .
[1  (u
(cid:1)



k

(cid:1)
k,

  y)

+ Mk(y

=(cid:2)k

Let vk be the unit vector orthogonal to L(Xb), the linear space spanned by the rst
k covariates selected by LARS, and pointing into L(Xk+1) along the direction of
yk+1  yk. For y
(cid:2)
near y we can reexpress (4.13) as a locally linear transformation,

(4.15)
Pk being the usual projection matrix from Rn into L(Xk); (4.15) holds within a
neighborhood of y such that the LARS choices L(Xk) and vk remain the same.
The matrix Mk has trace(Mk) = k. Since the trace equals the degrees of freedom
It is clear that (4.9) df ((cid:2)k) = k cannot hold for the Lasso, since the degrees of
for linear estimators, the simple approximation (4.9) is seen to be a delta method
approximation to the bootstrap estimates (4.6) and (4.7).

with Mk = Pk  cotk  ukv

freedom is m for the full model but the total number of steps taken can exceed m.
However, we have found empirically that an intuitively plausible result holds: the
degrees of freedom is well approximated by the number of nonzero predictors in
the model. Specically, starting at step 0, let (cid:7)(k) be the index of the last model

in the Lasso sequence containing k predictors. Then df ((cid:2)(cid:7)(k)) = k. We do not yet
In the orthogonal case, we assume that xj = ej
for j = 1, . . . , m. The LARS algorithm then has a particularly simple form,
reducing to soft thresholding at the order statistics of the data.

have any mathematical support for this claim.

4.1. Orthogonal designs.

To be specic, dene the soft thresholding operation on a scalar y1 at threshold t

by

 y1  t,

0,
y1 + t,

(y1; t) =

if y1 > t,
if |y1|  t,
if y1 < t.

The order statistics of the absolute values of the data are denoted by

|y|(1)  |y|(2)    |y|(n)  |y|(n+1) := 0.

(4.16)
We note that ym+1, . . . , yn do not enter into the estimation procedure, and so we
may as well assume that m = n.

LEAST ANGLE REGRESSION

427

LEMMA 1. For an orthogonal design with xj = ej , j = 1, . . . , n, the kth LARS

 yi  |y|(k+1),
estimate (0  k  n) is given by
(cid:6)
(cid:7)
0,
yi + |y|(k+1),
yi;|y|(k+1)

k,i(y) =
= 

(4.17)

(4.18)

.

if yi > |y|(k+1),
if |yi|  |y|(k+1),
if yi < |y|(k+1),

AA = |A|1/2,

uA = |A|1/21A,

PROOF. The proof is by induction, stepping through the LARS sequence. First
note that the LARS parameters take a simple form in the orthogonal setting:
j / Ak.
GA = IA,
We assume for the moment that there are no ties in the order statistics (4.16), so
that the variables enter one at a time. Let j (l) be the index corresponding to the
lth order statistic, |y|(l) = slyj (l): we will see that Ak = {j (1), . . . , j (k)}.
C1 = |y|(1). It is easily seen that
1 = min
j(cid:14)=j (1)

j y = yj , and so at the rst step LARS picks variable j (1) and sets
(cid:1)

ak,j = 0,

We have x

(cid:16)|y|(1)  |yj|(cid:17) = |y|(1)  |y|(2)
1 =(cid:18)|y|(1)  |y|(2)

ej (1),

(cid:19)

which is precisely (4.17) for k = 1.
Suppose now that step k  1 has been completed, so that Ak = {j (1), . . . , j (k)}
and (4.17) holds for k1. The current correlations Ck = |y|(k) and ck,j = yj
for j / Ak. Since Ak  ak,j = k

1/2, we have

and so

(cid:16)|y|(k)  |yj|(cid:17)

k = min
j /Ak

k1/2

and

(cid:19)
kuk =(cid:18)|y|(k)  |y|(k+1)
1{j  Ak}.
Adding this term to k1 yields (4.17) for step k.
The argument clearly extends to the case in which there are ties in the order
statistics (4.16): if |y|(k+1) =  = |y|(k+r), then Ak(y) expands by r variables at
step k + 1 and k+ (y),  = 1, . . . , r, are all determined at the same time and are
equal to k+1(y). (cid:1)

PROOF OF THEOREM 4 (Orthogonal case). The argument is particularly
simple in this setting, and so worth giving separately. First we note from (4.17)
that k is continuous and Lipschitz(1) and so certainly almost differentiable.

428

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

Hence (4.12) shows that we simply have to calculate   k. Inspection of (4.17)

shows that

  k

=(cid:1)
=(cid:1)

i

 k,i

yi

(y)

(cid:16)|yi| > |y|(k+1)

(cid:17) = k

I

almost surely, that is, except for ties. This completes the proof. (cid:1)

i

4.2. The divergence formula. While for the most general design matrices X, it
can happen that k fails to be almost differentiable, we will see that the divergence
formula

  k(y) = k

(4.19)

does hold almost everywhere. Indeed, certain authors [e.g., Meyer and Woodroofe

(2000)] have argued that the divergence  (cid:2) of an estimator provides itself a
useful measure of the effective dimension of a model.
Turning to LARS, we shall say that (y) is locally linear at a data point y0
if there is some small open neighborhood of y0 on which (y) = My is exactly
linear. Of course, the matrix M = M(y0) can depend on y0in the case of LARS,
it will be seen to be constant on the interior of polygonal regions, with jumps
across the boundaries. We say that a set G has full measure if its complement has
Lebesgue measure zero.

LEMMA 2. There is an open set Gk of full measure such that, at all y  Gk,

k(y) is locally linear and   k(y) = k.

PROOF. We give here only the part of the proof that relates to actual
calculation of the divergence in (4.19). The arguments establishing continuity and
local linearity are delayed to the Appendix.

So, let us x a point y in the interior of Gk. From Lemma 13 in the Appendix,
this means that near y the active set Ak(y) is locally constant, that a single variable
enters at the next step, this variable being the same near y. In addition, k(y) is
locally linear, and hence in particular differentiable. Since Gk  Gl for l < k, the
same story applies at all previous steps and we have

(4.20)
Differentiating the j th component of vector k(y) yields

l(y)ul.

k(y) = k(cid:1)
(y) = k(cid:1)

l=1

l=1

 k,j

yi

l(y)

yi

ul,j .

LEAST ANGLE REGRESSION

429

In particular, for the divergence

  k(y) = n(cid:1)

= k(cid:1)

 k,i

(cid:16)l, ul(cid:17),

(4.21)

yi

l=1

i=1
the brackets indicating inner product.
The active set is Ak = {1, 2, . . . , k} and xk+1 is the variable to enter next.
For k  2, write k = xlxk for any choice l < kas remarked in the Conventions
in the Appendix, the choice of l is immaterial (e.g., l = 1 for deniteness).
Let bk+1 = (cid:16)k+1, uk(cid:17), which is nonzero, as argued in the proof of Lemma 13.
As shown in (A.4) in the Appendix, (2.13) can be rewritten

k(y) = b

1
k+1

(cid:16)k+1, y  k1(cid:17).

(4.22)
(cid:17)
For k  2, dene the linear space of vectors equiangular with the active set
u :(cid:16)x1, u(cid:17) =  = (cid:16)xk, u(cid:17) for xl with l  Ak(y)

Lk = Lk(y) =(cid:16)

.

[We may drop the dependence on y since Ak(y) is locally xed.] Clearly dim Lk =
n  k + 1 and
(4.23)

Lk+1  Lk.

uk  Lk,

We shall now verify that, for each k  1,

(4.24)

(cid:16)k, uk(cid:17) = 1

and
First, for k = 1 we have 1(y) = b

(cid:16)k, u(cid:17) = 0
1
2

Formula (4.21) shows that this sufces to prove Lemma 2.

for u  Lk+1.
1
2

(cid:16)2, u(cid:17), and that

b2,
0,
Now, for general k, combine (4.22) and (4.20):

(cid:16)2, u(cid:17) = (cid:16)x1  x2, u(cid:17) =

(cid:4)
(cid:16)2, y(cid:17) and (cid:16)1, u(cid:17) = b
if u = u1,
if u  L2.
bk+1k(y) = (cid:16)k+1, y(cid:17)  k1(cid:1)
bk+1(cid:16)k, u(cid:17) = (cid:16)k+1, u(cid:17)  k1(cid:1)
(cid:4)

(cid:16)k+1, ul(cid:17)l(y),

l=1

(cid:16)k+1, ul(cid:17)(cid:16)l, u(cid:17).

l=1
From the denitions of bk+1 and Lk+1 we have
bk+1,
0,

(cid:16)k+1, u(cid:17) = (cid:16)xl  xk+1(cid:17) =

if u = uk,
if u  Lk+1.

and hence

Hence the truth of (4.24) for step k follows from its truth at step k  1 because of
the containment properties (4.23). (cid:1)

430

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

4.3. Proof of Theorem 4. To complete the proof of Theorem 4, we state the

following regularity result, proved in the Appendix.

LEMMA 3. Under the positive cone condition, k(y) is continuous and almost

differentiable.

This guarantees that Steins formula (4.12) is valid for k under the positive

cone condition, so the divergence formula of Lemma 2 then immediately yields
Theorem 4.

5. LARS and Lasso properties. The LARS and Lasso algorithms are
described more carefully in this section, with an eye toward fully understanding
their relationship. Theorem 1 of Section 3 will be veried. The latter material
overlaps results in Osborne, Presnell and Turlach (2000a), particularly in their
Section 4. Our point of view here allows the Lasso to be described as a quite
simple modication of LARS, itself a variation of traditional Forward Selection
methodology, and in this sense should be more accessible to statistical audiences.
In any case we will stick to the language of regression and correlation rather
than convex optimization, though some of the techniques are familiar from the
optimization literature.

The results will be developed in a series of lemmas, eventually lending to a proof
of Theorem 1 and its generalizations. The rst three lemmas refer to attributes of
giving estimate (cid:2)k1 and active set Ak for step k, with covariate xk the newest
the LARS procedure that are not specic to its Lasso modication.
Using notation as in (2.17)(2.20), suppose LARS has completed step k  1,

addition to the active set.

LEMMA 4.

If xk is the only addition to the active set at

the end of
step k  1, then the coefcient vector wk = AkG
1
(cid:2)
k 1k for the equiangular vector
uk = Xkwk, (2.6), has its kth component wkk agreeing in sign with the current
has its kth component(cid:2)
correlation ckk = x
 k

k(y(cid:2)k1). Moreover, the regression vector(cid:2)

 k for(cid:2)k

kk agreeing in sign with ckk.

= X

(cid:1)

Lemma 4 says that new variables enter the LARS active set in the correct
direction, a weakened version of the Lasso requirement (3.1). This will turn out to
be a crucial connection for the LARSLasso relationship.

PROOF OF LEMMA 4. The case k = 1 is apparent. Note that since

(2.20), from (2.6) we have
1

wk = Ak

(5.1)

C
k

(cid:2)

[(X

X

(cid:1)

k(y (cid:2)k1) =(cid:2)
k(y (cid:2)k1)] := Ak
1X

Ck1k,

(cid:1)
kXk)

(cid:1)

(cid:2)

C

1
k w


k .

LEAST ANGLE REGRESSION

431

The term in square braces is the least squares coefcient vector in the regression
of the current residual on Xk, and the term preceding it is positive.

Note also that

(5.2)

k(y  yk1) = (0, )
(cid:1)
(cid:1)

X

with  > 0,

k1(y  yk1) = 0 by denition (this 0 has k  1 elements), and ck( ) =
(cid:1)

(5.3)

ck( )

 < cj ( ),
since X
k(y   uk1) decreases more slowly in  than cj ( ) for j  Ak1:
(cid:1)
for  <(cid:2)k1,
x
= cj ( ) =(cid:2)
for  =(cid:2)k1,
for(cid:2)k1 <  < k1.
k(y  yk1 + yk1 (cid:2)k1)
(cid:21)
(cid:20)
1X
+ (X
1

> cj ( ),

1X

(cid:2)w

Thus

Ck,



(cid:1)

(cid:1)

k

k

0


(5.5)

(5.4)

(cid:1)
kXk)

(cid:1)
kXk)
(cid:1)
kXk)

k is positive, because it is in the rst term in (5.5) [(X

= (X
[( k1 (cid:2)k1)uk1].
= (X
The kth element of (cid:2)w
positive denite], and in the second term it is 0 since uk1  L(Xk1).
k1,k +(cid:2)kwkk,
This proves the rst statement in Lemma 4. The second follows from
and(cid:2)
k1,k = 0, xk not being active before step k. (cid:1)
Our second lemma interprets the quantity AA = (1
(cid:1)
(cid:1)

Let SA indicate the extended simplex generated by the columns of XA,

(cid:2)
kk =(cid:2)

1
A 1)

(5.6)

(cid:10)

G

(cid:9)
v = (cid:1)

jA

SA =

Pj = 1

,

sj xj Pj :

jA

(5.7)

(cid:1)
kXk) is

1/2, (2.4) and (2.5).

extended meaning that the coefcients Pj are allowed to be negative.

LEMMA 5. The point in SA nearest the origin is

vA = AAuA = AAXAwA

where wA = AAG

1
A 1A,

(5.8)
with length (cid:2)vA(cid:2) = AA. If A  B, then AA  AB, the largest possible value
being AA = 1 for A a singleton.

PROOF. For any v  SA, the squared distance to the origin is (cid:2)XAP(cid:2)2 =
GAP . Introducing a Lagrange multiplier to enforce the summation constraint,

(cid:1)

P
we differentiate

(5.9)

(cid:1)

GAP  (1

AP  1),
(cid:1)

P

432

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

PA = A2
AG

(cid:1)
AG

A 1A = 1,
1

and nd that the minimizing PA = G
1
A 1A. Summing, we get 1
and hence
A 1A = AAwA.
1

(5.10)
Hence vA = XAPA  SA and
(cid:2)vA(cid:2)2 = P
(5.11)
verifying (5.8). If A  B, then SA  SB, so the nearest distance AB must be
equal to or less than the nearest distance AA. AA obviously equals 1 if and only
if A has only one member. (cid:1)

A PA = A4
1
A1

A 1A = A2
1

(cid:1)
AG

(cid:1)
AG

A,

(cid:2),

 and d, let

 +  d and S( ) = (cid:2)y  X( )(cid:2)2.

The LARS algorithm and its various modications proceed in piecewise linear

steps. For m-vectors(cid:2)
( ) =(cid:2)
(cid:2)
LEMMA 6. Letting (cid:2)c = X
at(cid:2) = X
(y  X
) be the current correlation vector
S( )  S(0) = 2(cid:2)c
d + d
S(0) = 2(cid:2)c
S(0) = 2d
(cid:2)
) are both convex functions of(cid:2)

PROOF. S( ) is a quadratic function of  , with rst two derivatives at  = 0,
(cid:1)
(cid:2)
 =(cid:2)
(cid:2)
standard results show that(cid:2)(t) and(cid:2)(t) are unique and continuous functions of t.

The remainder of this section concerns the LARSLasso relationship. Now
(t).
, with S strictly convex,

(t) will indicate a Lasso solution (1.5), and likewise (cid:2) =(cid:2)(t) = X

Because S(

) and T (

Xd 2.

d and

X

Xd.

(5.12)

(5.13)

(5.14)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

(cid:1)

X

(cid:2)

(cid:1)

(cid:1)

For a given value of t let

(5.15)

A = {j :(cid:2)

j (t) (cid:14)= 0}.

interval of the t axis, with inmum t0, within which the set A of nonzero Lasso

We will show later that A is also the active set that determines the equiangular
direction uA, (2.6), for the LARSLasso computations.

We wish to characterize the track of the Lasso solutions (cid:2)
of (cid:2)(t) as t increases from 0 to its maximum effective value. Let T be an open
coefcients(cid:2)
LEMMA 7. The Lasso estimates(cid:2)(t) satisfy
(5.16)
for t  T , where uA is the equiangular vector XAwA, wA = AAG

(cid:2)(t) =(cid:2)(t0) + AA(t  t0)uA

j (t) remains constant.

(t) or equivalently

1
A 1A, (2.7).

433

(5.18)

A(t),

LEAST ANGLE REGRESSION

equiangular vector uA determined by A. We can also state this in terms of

PROOF. The lemma says that, for t in T , (cid:2)(t) moves linearly along the
the nonzero regression coefcients(cid:2)
(cid:2)
A(t) =(cid:2)
A(t0) + SAAA(t  t0)wA,
(5.17)
(cid:2)
in (5.17) because denitions (2.4), (2.10) require(cid:2)(t) = X
where SA is the diagonal matrix with diagonal elements sj , j  A. [SA is needed
Since(cid:2)(t) satises (1.5) and has nonzero set A, it also minimizes
(t) = XASA
A(t).]
subject to (cid:1)
(cid:3)| j| for the full m-variable OLS solution  m.] Moreover, the fact that the
minimizing point (cid:2)
so that (cid:2)

(cid:2)
j ) = sj
(cid:2)
) = t as long as t is less than

A(t) occurs strictly inside the simplex (5.19), combined with
A), implies we can drop the second condition in (5.19)
{S(

(cid:2)
A) = (cid:2)y  XASA

[The inequality in (1.5) can be replaced by T (

the strict convexity of S(

(cid:2)
j = t

for j  A.

(cid:2)
A(cid:2)2

A(t) solves

subject to

minimize

(cid:2)
A)}

(5.19)

(5.20)

sign(

(cid:2)

(cid:2)

and

sj

A

S(

(cid:2)
Introducing a Lagrange multiplier, (5.20) becomes
A(cid:2)2 + 

minimize

(5.21)

1
2

(cid:2)y  XASA
(cid:2)
A) + SA1A = 0.

sj

A

A(y  XASA
(cid:1)

SAX

Differentiating we get

(5.22)

(cid:1)
(cid:1)

A

sj

(cid:2)
j = t.
(cid:2)

j .

(cid:2)
A(t1) and (cid:2)

Consider two values t1 and t2 in T with t0 < t1 < t2. Corresponding to each
of these are values for the Lagrange multiplier  such that 1 > 2, and solutions
A(t2). Inserting these into (5.22), differencing and premultiplying

X

A(t1)

(cid:1)
AXASA

(cid:7) = (1  2)1A.

(cid:6)(cid:2)
A(t2) (cid:2)
(cid:2)
A(t2) (cid:2)
(cid:2)
A(t2) (cid:2)
A(t1) = (1  2)SAG
A(t1)] = t2  t1 according to the Lasso denition, so
[(
(cid:1)
A
t2  t1 = (1  2)s
A 1A = (1  2)A
2
1
(cid:2)
A(t2) (cid:2)

A 1A = (1  2)1
1
A(t2  t1)G

A 1A = SAAA(t  t1)wA.
1

A(t1) = SAA2

(cid:1)
ASAG

1
A 1A.

(cid:1)
AG

A

by SA we get
(5.23)

However, s

Hence

(5.24)

(5.25)

and

(5.26)

434

polytope

D (t) =

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

of the Lasso parameter t. (cid:1)

(cid:1)|j|  t

C(t)
C(t) is a piecewise linear decreasing function

A(t  t0), so that (cid:2)
(cid:22)

Letting t2 = t and t1  t0 gives (5.17) by the continuity of (cid:2)(t), and
nally (5.16). Note that (5.16) implies that the maximum absolute correlation(cid:2)
equals (cid:2)
C(t0)  A2
The Lasso solution(cid:2)(t) occurs on the surface of the diamond-shaped convex
(cid:23)
D (t) increasing with t. Lemma 7 says that, for t  T ,(cid:2)
(5.27)
estimates (cid:2)(t) move in the LARS equiangular direction uA, (2.6). It remains to
(t) moves linearly along
edge A of the polytope, the edge having j = 0 for j / A. Moreover the regression
show that A changes according to the rules of Theorem 1, which is the purpose
LEMMA 8. A Lasso solution(cid:2) has
of the next three lemmas.
(cid:2)cj =(cid:2)
(cid:2)
C  sign(
where(cid:2)cj equals the current correlation x
(cid:2)
j ) = sign((cid:2)cj )

for j  A,
j (y (cid:2)) = x
j (y  X
(cid:1)
for j  A.

this implies that
(5.29)

). In particular,

(5.28)

sign(

(cid:2)

j )

 :

,

(cid:1)

C. (cid:1)

and
(5.30)

|cj (t)| =(cid:2)

(cid:2)
of the left-hand side is (cid:2)cj , and the right-hand side is   sign(
Likewise  = |(cid:2)cj| =(cid:2)
PROOF. This follows immediately from (5.22) by noting that the j th element
j ) for j  A.
j (y (cid:2)(t)) satisfy
LEMMA 9. Within an interval T of constant nonzero set A, and also at t0 =
inf(T ), the Lasso current correlations cj (t) = x
(cid:1)
C(t)  max{|c(cid:7)(t)|}
for j  A
|cj (t)| (cid:2)
PROOF. Equation (5.28) says that the |cj (t)| have identical values, say (cid:2)
for j  A. It remains to show that (cid:2)
in (5.30). For an m-vector d we dene ( ) =(cid:2)
Ct ,
likewise T ( ) =(cid:3)|j ( )|, and
Ct has the extremum properties indicated
(t) +  d and S( ) as in (5.12),
Rt (d) = S(0)/ T (0).
Again assuming (cid:2)
(cid:25)(cid:26)(cid:24)(cid:1)
dj +(cid:1)
(cid:1)
Rt (d) = 2

j > 0 for j  A, by redenition of xj if necessary, (5.14) and

dj +(cid:1)

(cid:25)
|dj|

for j / A.

(cid:24)(cid:2)

(5.28) yield

cj (t)dj

(5.31)

(5.32)

C(t)

Ct

.

A

Ac

A

Ac

(cid:3)

If dj = 0 for j / A, and
(5.33)

dj (cid:14)= 0,

Rt (d) = 2(cid:2)
Rt (d) = 2|cj (t)|.

Ct ,

LEAST ANGLE REGRESSION

435

while if d has only component j nonzero we can make

Rt  Rt (wA)

(5.34)
According to Lemma 7 the Lasso solutions for t  T use dA proportional to wA
with dj = 0 for j / A, so
(5.35)
the Lasso must maximize Rt (d). This shows that (cid:2)
is the downward slope of the curve (T , S(T )) at T = t, and by the denition of
C(t), and veries (5.30),
which also holds at t0 = inf(T ) by the continuity of the current correlations. (cid:1)
We note that Lemmas 79 follow relatively easily from the KarushKuhn
Tucker conditions for optimality for the quadratic programming Lasso problem
[Osborne, Presnell and Turlach (2000a)]; we have chosen a more geometrical
argument here to demonstrate the nature of the Lasso path.
Figure 8 shows the (T , S) curve corresponding to the Lasso estimates in
Figure 1. The arrow indicates the tangent to the curve at t = 1000, which has

Ct = (cid:2)

FIG. 8. Plot of S versus T for Lasso applied to diabetes data; points indicate the 12 modied LARS
steps of Figure 1; triangle is (T , S) boundary point at t = 1000; dashed arrow is tangent at t = 1000,
negative slope Rt , (5.31). The (T , S) curve is a decreasing, convex, quadratic spline.

436

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

(cid:1)

Dene

A1 = {j :(cid:2)

downward slope R1000. The argument above relies on the fact that Rt (d) cannot be
greater than Rt , or else there would be (T , S) values lying below the optimal curve.
Using Lemmas 3 and 4 it can be shown that the (T , S) curve is always convex, as

in Figure 8, being a quadratic spline with S(T ) = 2(cid:2)
C(T ) and S(T ) = 2A2
A.
(cid:2)
Lemma 9, with Lasso regression vector(cid:2)
, prediction estimate (cid:2) = X
We now consider in detail the choice of active set at a breakpoint of the
piecewise linear Lasso path. Let t = t0 indicate such a point, t0 = inf(T ) as in
(y (cid:2)), sj = sign((cid:2)cj ) and maximum absolute correlation (cid:2)
correlations(cid:2)c = X
, current
C.
j = 0 and |(cid:2)cj| =(cid:2)
A0 = {j :(cid:2)
C},
j (cid:14)= 0},
10, and take ( ) =(cid:2) +  d for some m-vector d;
(5.36)
also S( ) = (cid:2)y  X( )(cid:2)2 and T ( ) =(cid:3)|j ( )|.
A10 = A1  A0 and A2 = Ac
LEMMA 10. The negative slope (5.31) at t0 is bounded by 2(cid:2)
R(d) = S(0)/ T (0)  2(cid:2)
(5.37)
with equality only if dj = 0 for j  A2. If so, the differences S = S( )  S(0)
and T = T ( )  T (0) satisfy
S = 2(cid:2)
(5.38)

CT + L(d)2  (T )2,

C,

C,

where

(cid:2)

C)dj

A10

A2

(5.39)

according to Lemma 8. Proceeding as in (5.32),

PROOF. We can assume(cid:2)cj  0 for all j , by redenition if necessary, so(cid:2)
j  0

L(d) = (cid:2)Xd/d+(cid:2).
(cid:25)(cid:26)(cid:24)(cid:1)
(cid:25)(cid:26)(cid:24)(cid:1)
(cid:1)

(cid:24)(cid:1)
dj +(cid:1)
R(d) = 2(cid:2)
((cid:2)cj /
(cid:24)(cid:1)
dj +(cid:1)
R(d) = 2(cid:2)
((cid:2)cj /
C unless dj = 0 for j  A2, verifying (5.37), and also implying

(5.40)
We need dj  0 for j  A0  A2 in order to maximize (5.40), in which case
This is < 2(cid:2)
The rst term on the right-hand side of (5.13) is then 2(cid:2)

dj + (cid:1)
dj +(cid:1)

(cid:25)
|dj|
(cid:25)

T ( ) = T (0) + 

C(T ), while the second

A0A2

(5.41)

(cid:1)
term equals (d/d+)

X

(cid:1)

X(d/d+)(T )2 = L(d)2. (cid:1)

A10

A2

A10

A2

dj .

A10

(cid:2)

C)dj

C

C

(5.42)

A1

.

dj

.

LEAST ANGLE REGRESSION

437

A

 (T )2,

Lemma 10 has an important consequence. Suppose that A is the current active
set for the Lasso, as in (5.17), and that A  A10. Then Lemma 5 says that L(d)
is  AA, and (5.38) gives
S  2(cid:2)
(5.43)
with equality if d is chosen to give the equiangular vector uA, dA = SAwA,
dAc = 0. The Lasso operates to minimize S(T ) so we want S to be as negative
S(0) exceeds the optimum value 2(cid:2)
as possible. Lemma 10 says that if the support of d is not conned to A10, then
C but S(0)
Suppose that(cid:2)
exceeds the minimum value 2AA unless dA is proportional to SAwA as in (5.17).
 obtained from the Lasso-
modied LARS algorithm, henceforth called LARSLasso, as at t = 1000 in
follow a linear track determined by some subset A, ( ) =(cid:2) +  uA, and so will
Figures 1 and 3. We know from Lemma 7 that subsequent Lasso estimates will

C  T + A2
C; if it is conned, then S(0) = 2(cid:2)

, a Lasso solution, exactly equals a(cid:2)

the LARSLasso estimates, but to verify Theorem 1 we need to show that A is
the same set in both cases.

Lemmas 47 put four constraints on the Lasso choice of A. Dene A1, A0 and

A10 as at (5.36).

(5.44)

A +  SAwA,

small  the subsequent Lasso coefcients (5.17),

A( ) =(cid:2)
(cid:2)
j ( ) (cid:14)= 0, j  A1.

CONSTRAINT 1. A1  A. This follows from Lemma 7 since for sufciently
will have(cid:2)
CONSTRAINT 2. A  A10. Lemma 10, (5.37) shows that the Lasso choice(cid:2)d
in ( ) =(cid:2) + 
(cid:2)d must have its nonzero support in A10, or equivalently that
(cid:2)( ) =(cid:2) +  uA must have uA  L(XA10 ). (It is possible that uA happens to
equal uB for some B  A10, but that does not affect the argument below.)
A 1A cannot have sign(wj ) (cid:14)= sign((cid:2)cj ) for any
(cid:2)
j ( )) (cid:14)= sign((cid:2)cj ( )) for sufciently
CONSTRAINT 3. wA = AAG

coordinate j  A0. If it does, then sign(
small  , violating Lemma 8.

1

CONSTRAINT 4. Subject to Constraints 13, A must minimize AA. This
the Lasso

follows from Lemma 10 as in (5.43), and the requirement
curve S(T ) declines at the fastest possible rate.

Theorem 1 follows by induction: beginning at(cid:2)0 = 0, we follow the LARS
with the Lasso denition (1.5). First of all, suppose that(cid:2), our hypothesized Lasso

Lasso algorithm and show that at every succeeding step it must continue to agree

that

and LARSLasso solution, has occurred strictly within a LARSLasso step. Then

438

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

A0 is empty so that Constraints 1 and 2 imply that A cannot change its current
value: the equivalence between Lasso and LARSLasso must continue at least to
the end of the step.

There are two cases: if j0 has just been added to the set {|(cid:2)cj| =(cid:2)
The one-at-a-time assumption of Theorem 1 says that at a LARSLasso
says that sign(wj0 ) = sign((cid:2)cj0), so that Constraint 3 is not violated; the other
breakpoint, A0 has exactly one member, say j0, so A must equal A1 or A10.
C}, then Lemma 4
three constraints and Lemma 5 imply that the Lasso choice A = A10 agrees with
the LARSLasso algorithm. The other case has j0 deleted from the active set as
in (3.6). Now the choice A = A10 is ruled out by Constraint 3: it would keep wA
the same as in the previous LARSLasso step, and we know that that was stopped
in (3.6) to prevent a sign contradiction at coordinate j0. In other words, A = A1,
in accordance with the Lasso modication of LARS. This completes the proof
of Theorem 1.

two new members j1 and j2 are added to the set {|(cid:2)cj| = (cid:2)
A LARSLasso algorithm is available even if the one-at-a-time condition does
not hold, but at the expense of additional computation. Suppose, for example,
C}, so A0 = {j1, j2}.
It is possible but not certain that A10 does not violate Constraint 3, in which
case A = A10. However, if it does violate Constraint 3, then both possibilities
A = A1  {j1} and A = A1  {j2} must be examined to see which one gives
the smaller value of AA. Since one-at-a-time computations, perhaps with some
added y jitter, apply to all practical situations, the LARS algorithm described in
Section 7 is not equipped to handle many-at-a-time problems.

coefcients, for example, as indicated at

and Stagewise procedures. Assume that(cid:2)
6. Stagewise properties. The main goal of this section is to verify Theorem 2.
(cid:3)|(cid:2)
Doing so also gives us a chance to make a more detailed comparison of the LARS
(cid:2), current correlations(cid:2)c = X
Figure 1, with prediction vector (cid:2) = X
(y (cid:2)),
 is a Stagewise estimate of the regression
j| = 2000 in the right panel of
C = max{|(cid:2)cj|} and maximal set A = {j :|(cid:2)cj| =(cid:2)
(cid:2)
C}. We must show that successive
by redenition of xj as xj, if necessary, that the signs sj = sign((cid:2)cj ) are all non-
Stagewise estimates of  develop according to the modied LARS algorithm of
Theorem 2, henceforth called LARSStagewise. For convenience we can assume,
(cid:2)
, giving new prediction vector(cid:2)(N ).
N additional -steps forward from(cid:2) = X
LEMMA 11. For sufciently small , only j  A can have Pj = Nj /N > 0.
PROOF. Letting N    , (cid:2)(cid:2)(N ) (cid:2)(cid:2)   so that(cid:2)c(N ) = X
(y (cid:2)(N ))
(cid:7)(cid:27)(cid:27)  (cid:2)xj(cid:2)  (cid:2)(cid:2)(N )(cid:2)(cid:2)   .

As in (3.8)(3.10) we suppose that the Stagewise procedure (1.7) has taken

|(cid:2)cj (N ) (cid:2)cj| =(cid:27)(cid:27)x

(cid:6)(cid:2)(N )(cid:2)

satises

(6.1)

(cid:1)

(cid:1)

negative.

(cid:1)

j

[(cid:2)
C  maxAc{(cid:2)cj}], j in Ac cannot have maximal current correlation and

LEAST ANGLE REGRESSION

439

For  < 1
2
can never be involved in the N steps. (cid:1)

(cid:2)( ) =(cid:2) +  v,

Lemma 11 says that we can write the developing Stagewise prediction vector as

(6.2)
PA a vector of length |A|, with components Nj /N for j  A. The nature of the
Stagewise procedure puts three constraints on v, the most obvious of which is
the following.

where v = XAPA,

CONSTRAINT I. The vector v  S

(cid:9)

v : v = (cid:1)

+
A, the nonnegative simplex
xj Pj , Pj  0,

Pj = 1

(cid:1)

.

(cid:10)

=

+
A

S

(6.3)
Equivalently,  v  CA, the convex cone (3.12).

jA

jA

The Stagewise procedure, unlike LARS, is not required to use all of the maximal
set A as the active set, and can instead restrict the nonzero coordinates Pj to a
subset B  A. Then v  L(XB ), the linear space spanned by the columns of XB,
but not all such vectors v are allowable Stagewise forward directions.

CONSTRAINT II. The vector v must be proportional to the equiangular vector

uB, (2.6), that is, v = vB, (5.8),
vB = A2
(6.4)

BXBG

Constraint II amounts to requiring that the current correlations in B decline at

B 1B = ABuB.
1
j (y (cid:2)   v) =(cid:2)cj   x
Bv = 1B for some  > 0, implying v = G
(cid:1)

an equal rate: since (cid:2)cj ( ) = x
(cid:1)
jv,
satises Constraint II. Violating Constraint II makes the current correlations(cid:2)cj ( )
B 1B; choosing  = A2
1

we need X

(6.5)

unequal so that the Stagewise algorithm as dened at (1.7) could not proceed in
direction v.

B

(cid:1)

BvB = A2
(cid:1)
j vB = A2
(cid:1)

x

B1B, or

Equation (6.4) gives X

(6.6)

for j  B.
CONSTRAINT III. The vector v = vB must satisfy
for j  A  B.

j vB  A2
(cid:1)

x

B

B

(6.7)

440

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

j vB = A2
(cid:1)

B. It is easy to show that vBBo

members of A = {j :|(cid:2)cj| =(cid:2)
Constraint III follows from (6.5). It says that the current correlations for
C} not in B must decline at least as quickly as those
in B. If this were not true, then vB would not be an allowable direction for
Stagewise development since variables in A B would immediately reenter (1.7).
To obtain strict inequality in (6.7), let B0  A  B be the set of indices for
= vB. In other words, if we
which x
take B to be the largest set having a given vB proportional to its equiangular
Writing (cid:2)( ) =(cid:2) +  v as in (6.2) presupposes that the Stagewise solutions
vector, then x
always express the family of Stagewise solutions as(cid:2)(z), where the real-valued
value as(cid:2)(z) goes from 0 to the full OLS estimate. [The choice Z = T used in
(cid:2)
parameter Z plays the role of T for the Lasso, increasing from 0 to some maximum
Figure 1 may not necessarily yield a one-to-one mapping; Z = S(0)  S(
estimate(cid:2)
), the
reduction in residual squared error, always does.] We suppose that the Stagewise
(z) is everywhere right differentiable with respect to z. Then the right

follow a piecewise linear track. However, the presupposition can be reduced
to one of piecewise differentiability by taking  innitesimally small. We can

B for j  A  B.

(cid:1)
j vB > A2

derivative

(6.8)

(cid:2)v = d

(cid:2)(z)/dz

If uA  CA,

must obey the three constraints.
The denition of the idealized Stagewise procedure in Section 3.2, in which
  0 in rule (1.7), is somewhat vague but the three constraints apply to any
reasonable interpretation. It turns out that the LARSStagewise algorithm satises
the constraints and is unique in doing so. This is the meaning of Theorem 2.
[Of course the LARSStagewise algorithm is also supported by direct numerical
comparisons with (1.7), as in Figure 1s right panel.]
then v = vA obviously satises the three constraints. The
interesting situation for Theorem 2 is uA / CA, which we now assume to be the
case. Any subset B  A determines a face of the convex cone of dimension |B|,
the face having Pj > 0 in (3.12) for j  B and Pj = 0 for j  A  B. The
orthogonal projection of uA into the linear subspace L(XB ), say ProjB (uA), is
proportional to Bs equiangular vector uB: using (2.7),
(6.9)

1
B X
ProjB (vA) = (AA/AB)2vB.
The nearest point to uA in CA, say(cid:2)uA, is of the form Axj
Therefore(cid:2)uA exists strictly within face (cid:2)B, where (cid:2)B = {j :(cid:2)
Pj  0.
Proj B (uA). According to (6.9),(cid:2)uA is proportional to (cid:2)Bs equiangular vector u B,
Pj > 0}, and must equal
= ABuB. In other words v B satises Constraint II, and it obviously

B AA1B = (AA/AB )  uB ,
1
Pj with (cid:2)
(cid:2)

and also to v B
also satises Constraint I. Figure 9 schematically illustrates the geometry.

ProjB (uA) = XBG

BuA = XBG
(cid:1)

or equivalently

(6.10)

LEAST ANGLE REGRESSION

441

FIG. 9. The geometry of the LARSStagewise modication.

LEMMA 12. The vector v B satises Constraints IIII, and conversely if v
satises the three constraints, then v = v B.
PROOF. Let Cos  AA/AB and Sin = [1  Cos2]1/2, the latter being greater

than zero by Lemma 5. For any face B  A, (6.9) implies
(6.11)
where zB is a unit vector orthogonal to L(XB ), pointing away from CA. By
an n-dimensional coordinate rotation we can make L(XB ) = L(c1, c2, . . . , cJ ),
J = |B|, the space of n-vectors with last n  J coordinates zero, and also
(6.12)
the rst 0 having length J  1, the second 0 length n  J  1. Then we can write
(6.13)

uA = (Cos, 0, Sin, 0),
(cid:7)

uA = CosuB + SinzB,

uB = (1, 0, 0, 0),

xj =(cid:6)

(6.14)

so (2.7) yields

(6.15)

x(cid:7) =(cid:6)
(cid:7)uA = Cosx(cid:7)1
(cid:1)

AA = x

x(cid:7)1 , x(cid:7)2 , x(cid:7)3, x(cid:7)4

,

+ Sinx(cid:7)3 .

AB, xj2, 0, 0

the rst coordinate AB being required since x
j uA = CosAB = AA, as also required by (2.7).
(cid:1)
x
For (cid:7)  A  B denote x(cid:7) as

for j  B,
j uB = AB, (2.7). Notice that
(cid:1)
(cid:7)

442

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

Now assume B = (cid:2)B. In this case a separating hyperplane H orthogonal to
z B in (6.11) passes between the convex cone CA and uA, through(cid:2)uA = Cosu B,
 0 [i.e., x(cid:7) and uA are on opposite sides of H, x(cid:7)3 being
implying x(cid:7)3
negative since the corresponding coordinate of uA, Sin in (6.12), is positive].
Equation (6.15) gives Cosx(cid:7)1
 AA = CosA B or
= x
(cid:7)(A Bu B) = A B x(cid:7)1
(cid:1)
(cid:1)
(cid:7)v B

(6.16)

 A2B

x

,

verifying that Constraint III is satised.
A and v = vB
Conversely suppose that v satises Constraints IIII so that v  S
+
for the nonzero coefcient set B: vB = Bxj Pj , Pj > 0. Let H be the hyperplane
passing through Cos  uB orthogonally to zB, (6.9), (6.11). If vB (cid:14)= v B, then at
least one of the vectors x(cid:7), (cid:7)  A  B, must lie on the same side of H as uA, so
and vB would be proportional to(cid:2)uA, the nearest point to uA in CA, implying
that x(cid:7)3 > 0 (or else H would be a separating hyperplane between uA and CA,
vB = v B). Now (6.15) gives Cos  x(cid:7)1 < AA = Cos  AB, or
(cid:7)(ABuB) = ABx(cid:7)1 < A2
(cid:1)

(6.17)
This violates Constraint III, showing that v must equal v B. (cid:1)
Notice that the direction of advance (cid:2)v = v B of the idealized Stagewise
procedure is a function only of the current maximal set (cid:2)A = {j :|(cid:2)cj| = (cid:2)
say(cid:2)v =  (
C},

(cid:2)A ). In the language of (6.7),
(cid:2)

(cid:7)vB = x
(cid:1)

B.

x

(cid:2)A ).
=  (

d

(z)

(6.18)

dz

The LARSStagewise algorithm of Theorem 2 produces an evolving family of
 that everywhere satises (6.18). This is true at every LARSStagewise
breakpoint by the denition of the Stagewise modication. It is also true between

estimates(cid:2)
breakpoints. Let (cid:2)A be the maximal set at the breakpoint, giving(cid:2)v = v B
(cid:2)A).
In the succeeding LARSStagewise interval(cid:2)( ) =(cid:2) +  v B, the maximal set is
=  (
immediately reduced to (cid:2)B, according to properties (6.6), (6.7) of v B, at which it
(cid:2)A ) = v B since v B
so the LARSStagewise procedure, which continues in the direction (cid:2)v until a
 C B,

stays during the entire interval. However,  (

(cid:2)B ) =  (

new member is added to the active set, continues to obey the idealized Stagewise
equation (6.18).

All of this shows that the LARSStagewise algorithm produces a legitimate
version of the idealized Stagewise track. The converse of Lemma 12 says that
there are no other versions, verifying Theorem 2.

The Stagewise procedure has its potential generality as an advantage over LARS
and Lasso: it is easy to dene forward Stagewise methods for a wide variety
of nonlinear tting problems, as in Hastie, Tibshirani and Friedman [(2001),
Chapter 10, which begins with a Stagewise analysis of boosting]. Comparisons

LEAST ANGLE REGRESSION

443

with LARS and Lasso within the linear model framework, as at the end of
Section 3.2, help us better understand Stagewise methodology. This sections
results permit further comparisons.

Consider proceeding forward from(cid:2) along unit vector u,(cid:2)( ) =(cid:2) +  u, two
interesting choices being the LARS direction u A and the Stagewise direction(cid:2) B.
For u  L(X A), the rate of change of S( ) = (cid:2)y (cid:2)( )(cid:2)2 is

= 2(cid:2)
C  u

 u
(cid:1)
A
A A

,

(cid:27)(cid:27)(cid:27)(cid:27)
 S( )
(cid:28)



0

(cid:27)(cid:27)(cid:27)(cid:27)

(cid:2)

C( )



0

(cid:27)(cid:27)(cid:27)(cid:27)
(cid:28)

(cid:27)(cid:27)(cid:27)(cid:27)

(6.19)

(6.20)

(cid:27)(cid:27)(cid:27)(cid:27)

(cid:27)(cid:27)(cid:27)(cid:27)

(6.19) following quickly from (5.14). This shows that the LARS direction u A
maximizes the instantaneous decrease in S. The ratio

SStage( )

SLARS( )

= A A
A B

,

equaling the quantity Cos in (6.15).

The comparison goes the other way for the maximum absolute correlation(cid:2)





0

0

C( ).

Proceeding as in (2.15),

(6.21)

 

{|x

j u|}.
(cid:1)

= minA
(cid:2)

= A A
A B

.

0

The argument for Lemma 12, using Constraints II and III, shows that u B maxi-
(cid:2)
mizes (6.21) at A B, and that



CLARS( )



CStage( )

(6.22)



0



The original motivation for the Stagewise procedure was to minimize residual
squared error within a framework of parsimonious forward search. However, (6.20)
shows that Stagewise is less greedy than LARS in this regard, it being more
accurate to describe Stagewise as striving to minimize the maximum absolute
residual correlation.

7. Computations. The entire sequence of steps in the LARS algorithm
with m < n variables requires O(m3 + nm2) computationsthe cost of a least
squares t on m variables.
In detail, at the kth of m steps, we compute m  k inner products cj k of the
nonactive xj with the current residuals to identify the next active variable, and then
invert the k  k matrix Gk = X
(cid:1)
kXk to nd the next LARS direction. We do this
by updating the Cholesky factorization Rk1 of Gk1 found at the previous step
[Golub and Van Loan (1983)]. At the nal step m, we have computed the Cholesky
R = Rm for the full cross-product matrix, which is the dominant calculation for a
least squares t. Hence the LARS sequence can be seen as a Cholesky factorization
with a guided ordering of the variables.

444

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

(cid:1)

The computations can be reduced further by recognizing that the inner products
X and the
above can be updated at each iteration using the cross-product matrix X
current directions. For m (cid:19) n, this strategy is counterproductive and is not used.
For the lasso modication, the computations are similar, except that occasion-
ally one has to drop a variable, and hence downdate Rk [costing at most O(m2)
operations per downdate]. For the stagewise modication of LARS, we need to
check at each iteration that the components of w are all positive. If not, one or
more variables are dropped [using the inner loop of the NNLS algorithm described
in Lawson and Hanson (1974)], again requiring downdating of Rk. With many
correlated variables, the stagewise version can take many more steps than LARS
because of frequent dropping and adding of variables, increasing the computations
by a factor up to 5 or more in extreme cases.
The LARS algorithm (in any of the three states above) works gracefully for the
case where there are many more variables than observations: m (cid:19) n. In this case
LARS terminates at the saturated least squares t after n 1 variables have entered
the active set [at a cost of O(n3) operations]. (This number is n  1 rather than n,
because the columns of X have been mean centered, and hence it has row-rank
n  1.) We make a few more remarks about the m (cid:19) n case in the lasso state:
1. The LARS algorithm continues to provide Lasso solutions along the way, and
the nal solution highlights the fact that a Lasso t can have no more than n 1
(mean centered) variables with nonzero coefcients.
2. Although the model involves no more than n  1 variables at any time, the
number of different variables ever to have entered the model during the entire
sequence can beand typically isgreater than n  1.

3. The model sequence, particularly near the saturated end, tends to be quite

variable with respect to small changes in y.

4. The estimation of  2 may have to depend on an auxiliary method such as
nearest neighbors (since the nal model is saturated). We have not investigated
the accuracy of the simple approximation formula (4.12) for the case m > n.
Documented S-PLUS implementations of LARS and associated functions
are available from www-stat.stanford.edu/hastie/Papers/; the diabetes data also
appears there.

8. Boosting procedures. One motivation for studying the Forward Stagewise
algorithm is its usefulness in adaptive tting for data mining. In particular, Forward
Stagewise ideas are used in boosting, an important class of tting methods for
data mining introduced by Freund and Schapire (1997). These methods are one of
the hottest topics in the area of machine learning, and one of the most effective
prediction methods in current use. Boosting can use any adaptive tting procedure
as its base learner (model tter): trees are a popular choice, as implemented in
CART [Breiman, Friedman, Olshen and Stone (1984)].

LEAST ANGLE REGRESSION

445

Friedman, Hastie and Tibshirani (2000) and Friedman (2001) studied boosting
and proposed a number of procedures, the most relevant to this discussion being
least squares boosting. This procedure works by successive tting of regression
trees to the current residuals. Specically we start with the residual r = y and the
t y = 0. We t a tree in x1, x2, . . . , xm to the response y giving a tted tree t1
(an n-vector of tted values). Then we update y to y +   t1, r to y  y and
continue for many iterations. Here  is a small positive constant. Empirical studies
show that small values of  work better than  = 1: in fact, for prediction accuracy
the smaller the better. The only drawback in taking very small values of  is
computational slowness.

A major research question has been why boosting works so well, and
specically why is -shrinkage so important? To understand boosted trees in
the present context, we think of our predictors not as our original variables
x1, x2, . . . , xm, but instead as the set of all trees tk that could be tted to our data.
There is a strong similarity between least squares boosting and Forward Stagewise
regression as dened earlier. Fitting a tree to the current residual is a numerical way
of nding the predictor most correlated with the residual. Note, however, that the
greedy algorithms used in CART do not search among all possible trees, but only
a subset of them. In addition the set of all trees, including a parametrization for
the predicted values in the terminal nodes, is innite. Nevertheless one can dene
idealized versions of least-squares boosting that look much like Forward Stagewise
regression.

Hastie, Tibshirani and Friedman (2001) noted the the striking similarity between
Forward Stagewise regression and the Lasso, and conjectured that this may help
explain the success of the Forward Stagewise process used in least squares
boosting. That is, in some sense least squares boosting may be carrying out a Lasso
t on the innite set of tree predictors. Note that direct computation of the Lasso
via the LARS procedure would not be feasible in this setting because the number
of trees is innite and one could not compute the optimal step length. However,
Forward Stagewise regression is feasible because it only need nd the the most
correlated predictor among the innite set, where it approximates by numerical
search.

In this paper we have established the connection between the Lasso and Forward
Stagewise regression. We are now thinking about how these results can help to
understand and improve boosting procedures. One such idea is a modied form of
Forward Stagewise: we nd the best tree as usual, but rather than taking a small
step in only that tree, we take a small least squares step in all trees currently in our
model. One can show that for small step sizes this procedure approximates LARS;
its advantage is that it can be carried out on an innite set of predictors such as
trees.

446

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

APPENDIX

A.1. Local linearity and Lemma 2.

l(y  k1(y)) = Ck(y) and x
(cid:1)

CONVENTIONS. We write xl with subscript l for members of the active
set Ak. Thus xl denotes the lth variable to enter, being an abuse of notation for
slxj (l) = sgn(cj (l))xj (l). Expressions x
luk = Ak
(cid:1)
clearly do not depend on which xl  Ak we choose.
By writing j / Ak, we intend that both xj and xj are candidates for inclusion
at the next step. One could think of negative indices j corresponding to new
variables xj = xj .
a neighborhood of y0, we say that Ak(y) is locally xed [at Ak = Ak(y0)].

The active set Ak(y) depends on the data y. When Ak(y) is the same for all y in

A function g(y) is locally Lipschitz at y if for all sufciently small vectors y,

(A.1)

(cid:2)g(cid:2) = (cid:2)g(y + y)  g(y)(cid:2)  L(cid:2)y(cid:2).

If the constant L applies for all y, we say that g is uniformly locally Lipschitz (L),
and the word locally may be dropped.

LEMMA 13. For each k, 0  k  m, there is an open set Gk of full measure
on which Ak(y) and Ak+1(y) are locally xed and differ by 1, and k(y) is locally
linear. The sets Gk are decreasing as k increases.

PROOF. The argument is by induction. The induction hypothesis states that for
each y0  Gk1 there is a small ball B(y0) on which (a) the active sets Ak1(y)
and Ak(y) are xed and equal to Ak1 and Ak, respectively, (b) |Ak \ Ak1| = 1
so that the same single variable enters locally at stage k  1 and (c) k1(y) = My
is linear. We construct a set Gk with the same property.
Fix a point y0 and the corresponding ball B(y0)  Gk1, on which y 
k1(y) = y  My = Ry, say. For indices j1, j2 / A, let N (j1, j2) be the set of y

for which there exists a  such that
(Ry   uk) = x
j1(Ry   uk) = x
(cid:1)

(A.2)

w

(cid:1)

j2(Ry   uk).
(cid:1)

Setting 1 = xl  xj1, the rst equality may be written 

1Ry =  
(cid:1)

(cid:1)
1uk and so

when 

1uk (cid:14)= 0 determines
(cid:1)

 = 

(cid:1)
1Ry/

1uk =: 
(cid:1)

(cid:1)
1y.

[If 

1uk = 0, there are no qualifying y, and N (j1, j2) is empty.] Now using
(cid:1)
the second equality and setting 2 = xl  xj2, we see that N (j1, j2) is contained
in the set of y for which

2Ry = 
(cid:1)

(cid:1)
1y 

(cid:1)
2uk.



LEAST ANGLE REGRESSION

447

In other words, setting 2 = R

2  (

(cid:1)
N (j1, j2)  {y : 

(cid:1)
2uk)1, we have
2y = 0}.
(cid:1)

N (y0) =(cid:29){N (j1, j2) : j1, j2 / A, j1 (cid:14)= j2},

If we dene

it is evident that N (y0) is a nite union of hyperplanes and hence closed. For
y  B(y0) \ N (y0), a unique new variable joins the active set at step k. Near each
such y the joining variable is locally the same and k(y)uk is locally linear.
We then dene Gk  Gk1 as the union of such sets B(y)\ N (y) over y  Gk1.
Thus Gk is open and, on Gk, Ak+1(y) is locally constant and k(y) is locally
linear. Thus properties (a)(c) hold for Gk.
The same argument works for the initial case k = 0: since 0 = 0, there is no
Finally, since the intersection of Gk with any compact set is covered by a nite

circularity.
number of B(yi) \ N (yi), it is clear that Gk has full measure. (cid:1)

LEMMA 14.

continuous (resp. linear) and uniformly Lipschitz.

Suppose that, for y near y0, k1(y) is continuous (resp. linear)
and that Ak(y) = Ak. Suppose also that, at y0, Ak+1(y0) = A  {k + 1}.
Then for y near y0, Ak+1(y) = Ak  {k + 1} and k(y) and hence k(y) are
Ck and (cid:2)ckj dened in
(2.18) and (2.17), respectively. Since k + 1 / Ak, we have | Ck(y0)| > ck,k+1(y0),
and k(y0) > 0 satises

the situation at y0, with (cid:2)
(cid:5)
(cid:4)=

PROOF. Consider rst

(cid:4)

j = k + 1
j > k + 1 .

as

ck,j (y0)  k(y0)ak,j

Ck(y0)  k(y0)Ak

(A.3)
In particular, it must be that Ak (cid:14)= ak,k+1, and hence
k(y0) = Ck(y0)  ck,k+1(y0)

>

Ak  ak,k+1

> 0.

Call an index j admissible if j / Ak and ak,j (cid:14)= Ak. For y near y0, this property

is independent of y. For admissible j , dene

which is continuous (resp. linear) near y0 from the assumption on k1. By

denition,

Rk,j (y) = Ck(y)  ck,j (y)
Ak  ak,j

,

k(y) = min
jPk(y)

Rk,j (y),

Pk(y) = {j admissible and Rk,j (y) > 0}.

For admissible j , Rk,j (y0) (cid:14)= 0, and near y0 the functions y  Rk,j (y) are
continuous and of xed sign. Thus, near y0 the set Pk(y) stays xed at Pk(y0)
and (A.3) implies that

Rk,k+1(y) < Rk,j (y),

j > k + 1, j  Pk(y).

Consequently, for y near y0, only variable k + 1 joins the active set, and so
Ak+1(y) = Ak  {k + 1}, and

(A.4)

k(y) = Rk,k+1(y) = (xl  xk+1)

(cid:1)

(y  k1(y))

(xl  xk+1)

(cid:1)uk

.

This representation shows that both k(y) and hence k(y) = k1(y) + k(y)uk
are continuous (resp. linear) near y0.
To show that k is locally Lipschitz at y, we set  = w  xk+1 and write, using

notation from (A.1),

(cid:1)

 k = 

(y  k1)

.

(cid:1)



uk

448

where

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

As y varies, there is a nite list of vectors (xl, xk+1, uk) that can occur in
uk, and since all such terms are positive [as observed
the denominator term 
below (A.3)], they have a uniform positive lower bound, amin say. Since (cid:2)(cid:2)  2
and k1 is Lipschitz (Lk1) by assumption, we conclude that

(cid:1)

| k|
(cid:2)y(cid:2)  2a

min(1 + Lk1) =: Lk.
1

(cid:1)

A.2. Consequences of the positive cone condition.

LEMMA 15.
x+ = sj xj for some j / A). Let PA = XAG
so that a = x

Suppose that |A+| = |A| + 1 and that XA+ = [XA x+] (where
1
(cid:1)
A denote projection on span(XA),
(cid:20)
(cid:21)
A X
(cid:1)+PAx+ < 1. The +-component of G
1
A+1A+ is
(cid:1)+uA
1  x

A+1A+)+ = (1  a)
1
1

(A.5)

(G

.

AA

Consequently, under the positive cone condition (4.11),

(A.6)

(cid:1)+uA < AA.
(cid:21)
PROOF. Write GA+ as a partitioned matrix
=

X X

(cid:20)

GA+ =

x

(cid:1)
(cid:1)+X x

X
x

(cid:1)
x+
(cid:1)+x+

(cid:20)

(cid:21)

A B
(cid:1)
B
D

.

LEAST ANGLE REGRESSION

449

Applying the formula for the inverse of a partitioned matrix [e.g., Rao (1973), page
33],

A+1A+)+ = E
1

1F

(cid:1)

1 + E

1,

(G

where

(cid:1)

E = D  B
F = A

1B = G

A

1B = 1  x
1
x+,
A X

(cid:1)

(cid:1)+PAx+,

from which (A.5) follows. The positive cone condition implies that G
and so (A.6) is immediate. (cid:1)

1
A+1A+ > 0,

A.3. Global continuity and Lemma 3. We shall call y0 a multiple point at
step k if two or more variables enter at the same time. Lemma 14 shows that
such points form a set of measure zero, but they can and do cause discontinuities

in k+1 at y0 in general. We will see, however, that the positive cone condition

prevents such discontinuities.

We conne our discussion to double points, hoping that these arguments will
be sufcient to establish the same pattern of behavior at points of multiplicity 3
or higher. In addition, by renumbering, we shall suppose that indices k + 1 and
k + 2 are those that are added at double point y0. Similarly, for convenience only,
we assume that Ak(y) is constant near y0. Our task then is to show that, for y near
a double point y0, both k(y) and k+1(y) are continuous and uniformly locally

Lipschitz.

Suppose that Ak(y) = Ak

LEMMA 16.

is constant near y0 and that
Ak+(y0) = Ak  {k + 1, k + 2}. Then for y near y0, Ak+(y) \ Ak can only be
one of three possibilities, namely {k + 1},{k + 2} or {k + 1, k + 2}. In all cases
k(y) = k1(y) + k(y)uk as usual, and both k(y) and k(y) are continuous

and locally Lipschitz.

double point and the positivity set Pk(y) = Pk near y0, we have

PROOF. We use notation and tools from the proof of Lemma 14. Since y0 is a
0 < Rk,k+1(y0) = Rk,k+2(y0) < Rk,j (y0)

for j  Pk \ {k + 1, k + 2}.

Continuity of Rk,j implies that near y0 we still have

0 < Rk,k+1(y), Rk,k+2(y) < min

(cid:16)

Rk,j (y); j  Pk \ {k + 1, k + 2}(cid:17)

.

Hence Ak+ \ Ak must equal {k + 1} or {k + 2} or {k + 1, k + 2} according
as Rk,k+1(y) is less than, greater than or equal to Rk,k+2(y). The continuity of

k(y) = min{Rk,k+1(y), Rk,k+2(y)}

is immediate, and the local Lipschitz property follows from the arguments of
Lemma 14. (cid:1)

450

EFRON, HASTIE, JOHNSTONE AND TIBSHIRANI

LEMMA 17. Assume the conditions of Lemma 16 and in addition that the

positive cone condition (4.11) holds. Then k+1(y) is continuous and locally
Lipschitz near y0.

PROOF. Since y0 is a double point, property (A.3) holds, but now with equality
when j = k+ 1 or k+ 2 and strict inequality otherwise. In other words, there exists
0 > 0 for which

Ck+1(y0)  ck+1,j (y0)

(cid:4)= 0,
 0,

if j = k + 2,
if j > k + 2.

Consider a neighborhood B(y0) of y0 and let N (y0) be the set of double points
in B(y0), that is, those for which Ak+1(y)\ Ak = {k + 1, k + 2}. We establish the
convention that at such double points k+1(y) = k(y); at other points y in B(y0),
k+1(y) is dened by k(y) + k+1(y)uk+1 as usual.
Now consider those y near y0 for which Ak+1(y) \ Ak = {k + 1}, and so, from
the previous lemma, Ak+2(y) \ Ak+1 = {k + 2}. For such y, continuity and the
(cid:4)= O((cid:2)y  y0(cid:2)),
local Lipschitz property for k imply that

Ck+1(y)  ck+1,j (y)

if j = k + 2,
if j > k + 2.

> 0/2,

It is at this point that we use the positive cone condition (via Lemma 15) to
guarantee that Ak+1 > ak+1,k+2. Also, since Ak+1(y) \ Ak = {k + 1}, we have

Ck+1(y) > ck+1,k+2(y).

These two facts together show that k + 2  Pk+1(y) and hence that
= O((cid:2)y  y0(cid:2))

k+1(y) = Ck+1(y)  ck+1,k+2(y)
Ak+1  ak+1,k+2

is continuous and locally Lipschitz. In particular, as y approaches N (y0), we have
k+1(y)  0. (cid:1)

REMARK A.1. We say that a function g : Rn  R is almost differentiable if it
is absolutely continuous on almost all line segments parallel to the coordinate axes,
and its partial derivatives (which consequently exist a.e.) are locally integrable.
This denition of almost differentiability appears supercially to be weaker than
that given by Stein, but it is in fact precisely the property used in his proof.
Furthermore, this denition is equivalent to the standard denition of weak
differentiability used in analysis.

PROOF OF LEMMA 3. We have shown explicitly that k(y) is continuous
and uniformly locally Lipschitz near single and double points. Similar arguments

LEAST ANGLE REGRESSION

451

extend the property to points of multiplicity 3 and higher, and so all points y are
covered. Finally, absolute continuity of y  k(y) on line segments is a simple
consequence of the uniform Lipschitz property, and so k is almost differentiable.
(cid:1)

Acknowledgments. The authors thank Jerome Friedman, Bogdan Popescu,

Saharon Rosset and Ji Zhu for helpful discussions.

