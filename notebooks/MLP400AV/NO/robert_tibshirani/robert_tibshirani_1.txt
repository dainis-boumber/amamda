Abstract

The standard 2-norm SVM is known for its good performance in two-
In this paper, we consider the 1-norm SVM. We
class classication.
argue that the 1-norm SVM may have some advantage over the standard
2-norm SVM, especially when there are redundant noise features. We
also propose an efcient algorithm that computes the whole solution path
of the 1-norm SVM, hence facilitates adaptive selection of the tuning
parameter for the 1-norm SVM.

1

Introduction

In standard two-class classication problems, we are given a set of training data (x1, y1),
. . . (xn, yn), where the input xi  Rp, and the output yi  {1,1} is binary. We wish to
nd a classcation rule from the training data, so that when given a new input x, we can
assign a class y from {1,1} to it.

To handle this problem, we consider the 1-norm support vector machine (SVM):
n(cid:1)
1  yi
(cid:2)(cid:2)1 = |1| +  + |q|  s,

(2)
where D = {h1(x), . . . hq(x)} is a dictionary of basis functions, and s is a tuning parame-
ter. The solution is denoted as 0(s) and (s); the tted model is


0 +







q(cid:1)

j=1

min
0,

s.t.

jhj(xi)

(1)

i=1

+

f(x) = 0 +

jhj(x).

(3)

j=1

The classication rule is given by sign[ f(x)]. The 1-norm SVM has been successfully
used in [1] and [9]. We argue in this paper that the 1-norm SVM may have some advantage
over the standard 2-norm SVM, especially when there are redundant noise features.
To get a good tted model f(x) that performs well on future data, we also need to select
an appropriate tuning parameter s. In practice, people usually pre-specify a nite set of
values for s that covers a wide range, then either use a separate validation data set or use

q(cid:1)

cross-validation to select a value for s that gives the best performance among the given set.
In this paper, we illustrate that the solution path (s) is piece-wise linear as a function of
s (in the Rq space); we also propose an efcient algorithm to compute the exact whole
solution path { (s), 0  s  }, hence help us understand how the solution changes
with s and facilitate the adaptive selection of the tuning parameter s. Under some mild
assumptions, we show that the computational cost to compute the whole solution path (s)
is O(nq min(n, q)2) in the worst case and O(nq) in the best case.
Before delving into the technical details, we illustrate the concept of piece-wise linearity
of the solution path (s) with a simple example. We generate 10 training data in each of
two classes. The rst class has two standard normal independent inputs x1, x2. The second
class also has two standard normal independent inputs, but conditioned on 4.5  x2

8. The dictionary of basis functions is D = {
}. The solution
path (s) as a function of s is shown in Figure 1. Any segment between two adjacent
vertical lines is linear. Hence the right derivative of (s) with respect to s is piece-wise
constant (in Rq). The two solid paths are for x2
2, which are the two relevant features.

2x1x2, x2

1 and x2

1 +x2

2

1, x2
2

2x1,

2x2,





8

.

0

6

.

0

4

.

0



2

.

0

0

.

0

0.0

0.5

s

1.0

1.5

Figure 1: The solution path (s) as a function of s.

In section 2, we motivate why we are interested in the 1-norm SVM. In section 3, we
describe the algorithm that computes the whole solution path (s). In section 4, we show
some numerical results on both simulation data and real world data.

2 Regularized support vector machines

The standard 2-norm SVM is equivalent to t a model that

n(cid:1)


1  yi


0 +

i=1

j=1

min
0,j

q(cid:1)

jhj(xi)







+

+ (cid:2)(cid:2)2
2,

(4)

where  is a tuning parameter. In practice, people usually choose hj(x)s to be the basis
functions of a reproducing kernel Hilbert space. Then a kernel trick allows the dimension
of the transformed feature space to be very large, even innite in some cases (i.e. q = ),
without causing extra computational burden ([2] and [12]). In this paper, however, we will
concentrate on the basis representation (3) rather than a kernel representation.
Notice that (4) has the form loss + penalty, and  is the tuning parameter that controls
the tradeoff between loss and penalty. The loss (1  yf)+ is called the hinge loss, and

the penalty is called the ridge penalty. The idea of penalizing by the sum-of-squares of the
parameters is also used in neural networks, where it is known as weight decay. The ridge
penalty shrinks the tted coefcients  towards zero. It is well known that this shrinkage
has the effect of controlling the variances of , hence possibly improves the tted models
prediction accuracy, especially when there are many highly correlated features [6]. So from
a statistical function estimation point of view, the ridge penalty could possibly explain the
success of the SVM ([6] and [12]). On the other hand, computational learning theory has
associated the good performance of the SVM to its margin maximizing property [11], a
property of the hinge loss. [8] makes some effort to build a connection between these two
different views.
In this paper, we replace the ridge penalty in (4) with the L1-norm of , i.e.
penalty [10], and consider the 1-norm SVM problem:

the lasso

q(cid:1)







jhj(xi)

+ (cid:2)(cid:2)1,

(5)

n(cid:1)


1  yi


0 +

min
0,

i=1

j=1

+

which is an equivalent Lagrange version of the optimization problem (1)-(2).

The lasso penalty was rst proposed in [10] for regression problems, where the response y
is continuous rather than categorical. It has also been used in [1] and [9] for classication
problems under the framework of SVMs. Similar to the ridge penalty, the lasso penalty also
shrinks the tted coefcients s towards zero, hence (5) also benets from the reduction
in tted coefcients variances. Another property of the lasso penalty is that because of the
L1 nature of the penalty, making  sufciently large, or equivalently s sufciently small,
will cause some of the coefcients js to be exactly zero. For example, when s = 1 in
Figure 1, only three tted coefcients are non-zero. Thus the lasso penalty does a kind of
continuous feature selection, while this is not the case for the ridge penalty. In (4), none of
the js will be equal to zero.
It is interesting to note that the ridge penalty corresponds to a Gaussian prior for the js,
while the lasso penalty corresponds to a double-exponential prior. The double-exponential
density has heavier tails than the Gaussian density. This reects the greater tendency of
the lasso to produce some large tted coefcients and leave others at 0, especially in high
dimensional problems. Recently, [3] consider a situation where we have a small number of
training data, e.g. n = 100, and a large number of basis functions, e.g. q = 10, 000. [3]
argue that in the sparse scenario, i.e. only a small number of true coefcients js are non-
zero, the lasso penalty works better than the ridge penalty; while in the non-sparse scenario,
e.g.
the true coefcients js have a Gaussian distribution, neither the lasso penalty nor
the ridge penalty will t the coefcients well, since there is too little data from which to
estimate these non-zero coefcients. This is the curse of dimensionality taking its toll.
Based on these observations, [3] further propose the bet on sparsity principle for high-
dimensional problems, which encourages using lasso penalty.

3 Algorithm

Section 2 gives the motivation why we are interested in the 1-norm SVM. To solve the
1-norm SVM for a xed value of s, we can transform (1)-(2) into a linear programming
problem and use standard software packages; but to get a good tted model f(x) that
performs well on future data, we need to select an appropriate value for the tuning paramter
s. In this section, we propose an efcient algorithm that computes the whole solution path
(s), hence facilitates adaptive selection of s.

3.1 Piece-wise linearity

(cid:10)
If we follow the solution path (s) of (1)-(2) as s increases, we will notice that since both
i(1  yi fi)+ and (cid:2)(cid:2)1 are piece-wise linear, the Karush-Kuhn-Tucker conditions will
not change when s increases unless a residual (1  yi fi) changes from non-zero to zero,
(cid:10)
or a tted coefcient j(s) changes from non-zero to zero, which correspond to the non-
i(1  yi fi)+ and (cid:2)(cid:2)1. This implies that the derivative of (s) with
smooth points of
respect to s is piece-wise constant, because when the Karush-Kuhn-Tucker conditions do
not change, the derivative of (s) will not change either. Hence it indicates that the whole
solution path (s) is piece-wise linear. See [13] for details.
Thus to compute the whole solution path (s), all we need to do is to nd the joints, i.e.
the asterisk points in Figure 1, on this piece-wise linear path, then use straight lines to
interpolate them, or equivalently, to start at (0) = 0, nd the right derivative of (s), let
s increase and only change the derivative when (s) gets to a joint.

Initial solution (i.e. s = 0)

3.2
The following notation is used. Let V = {j : j(s) (cid:6)= 0}, E = {i : 1  yi fi = 0},
L = {i : 1  yi fi > 0} and u for the right derivative of V(s): (cid:2)u(cid:2)1 = 1 and V(s)
denotes the components of (s) with indices in V. Without loss of generality, we assume
#{yi = 1}  #{yi = 1}; then 0(0) = 1, j(0) = 0. To compute the path that (s)
follows, we need to compute the derivative of (s) at 0. We consider a modied problem:

(cid:1)

(cid:1)

min
0,j

s.t.

(1  yifi)+ +

yi=1

yi=1
(cid:2)(cid:2)1  s, fi = 0 +

(1  yifi)
q(cid:1)

jhj(xi).

(6)

(7)

j=1

Notice that if yi = 1, the loss is still (1  yifi)+; but if yi = 1, the loss becomes
(1  yifi). In this setup, the derivative of (s) with respect to s is the same no matter
what value s is, and one can show that it coincides with the right derivative of (s)
when s is sufciently small. Hence this setup helps us nd the initial derivative u of (s).
Solving (6)-(7), which can be transformed into a simple linear programming problem, we
get initial V, E and L. |V| should be equal to |E|. We also have:

(cid:11)

(cid:12)

(cid:11)

(cid:12)

=

1
0

0(s)
V(s)

(cid:11)

(cid:12)

+ s 

u0
u

.

(8)

s starts at 0 and increases.

3.3 Main algorithm

The main algorithm that computes the whole solution path (s) proceeds as following:

1. Increase s until one of the following two events happens:

 A training point hits E, i.e. 1  yifi (cid:6)= 0 becomes 1  yifi = 0 for some i.
 A basis function in V leaves V, i.e. j (cid:6)= 0 becomes j = 0 for some j.
Let the current 0,  and s be denoted by old

0 , old and sold.

2. For each j

where u0, uj and uj are the unknowns. We then compute:

j



= 1

lossj

)uj + |uj|
(cid:1)

(cid:13)
/ V, we solve:
u0 +

(cid:10)
(cid:10)
V ujhj(xi) + ujhj(xi) = 0 for i  E
V sign( old
(cid:14)
(cid:15)
(cid:1)
=
L
(cid:10)
(cid:2)  E, we solve:
u0 +
V sign( old
(cid:1)

V ujhj(xi) = 0 for i  E\{i
(cid:2)}
(cid:15)

ujhj(xi) + ujhj(xi)

)uj = 1

(cid:1)

(cid:10)

u0 +

(cid:14)

(cid:13)

s

yi

V

j

(9)

.

(10)

(11)

3. For each i

where u0 and uj are the unknowns. We then compute:

lossi(cid:2)

s

=

yi

L

ujhj(xi)

u0 +
(12)
from step 2 and step 3. There are q|V|+

V

.

4. Compare the computed values of loss
s

|E| = q + 1 such values. Choose the smallest negative loss
(cid:12)

 If the smallest loss
 If the smallest negative loss

s corresponds to a j

(cid:11)

s



is non-negative, the algorithm terminates; else

s . Hence,

in step 2, we update

}, u 

V  V  {j

u
uj
 If the smallest negative loss
(cid:2)
s corresponds to a i
E  E\{i
(cid:2)}, L  L  {i
(cid:12)
(cid:12)
In either of the last two cases, (s) changes as:
old
0oldV

0(sold + s)
V(sold + s)

(cid:11)

(cid:11)

+ s 

=

in step 3, we update u and
(14)

(cid:2)} if necessary.
(cid:12)

(cid:11)

u0
u

,

(15)

.

(13)

and we go back to step 1.

In the end, we get a path (s), which is piece-wise linear.

3.4 Remarks

Due to the page limit, we omit the proof that this algorithm does indeed give the exact
whole solution path (s) of (1)-(2) (see [13] for detailed proof). Instead, we explain a little
what each step of the algorithm tries to do.
Step 1 of the algorithm indicates that (s) gets to a joint on the solution path and the right
derivative of (s) needs to be changed if either a residual (1 yi fi) changes from non-zero
to zero, or the coefcient of a basis function j(s) changes from non-zero to zero, when s
increases. Then there are two possible types of actions that the algorithm can take: (1) add
a basis function into V, or (2) remove a point from E.
Step 2 computes the possible right derivative of (s) if adding each basis function hj(x)
into V. Step 3 computes the possible right derivative of (s) if removing each point i
(cid:2)
from E. The possible right derivative of (s) (determined by either (9) or (11)) is such that
the training points in E are kept in E when s increases, until the next joint (step 1) occurs.
loss/s indicates how fast the loss will decrease if (s) changes according to u. Step 4
takes the action corresponding to the smallest negative loss/s. When the loss can not
be decreased, the algorithm terminates.

1-norm

Test Error (SE)

Table 1: Simulation results of 1-norm and 2-norm SVM
|D|
5
14
27
44
65

No Penalty
0.08 (0.01)
0.12 (0.03)
0.20 (0.05)
0.22 (0.06)
0.22 (0.06)

0.073 (0.010)
0.074 (0.014)
0.074 (0.009)
0.082 (0.009)
0.084 (0.011)

0.08 (0.02)
0.10 (0.02)
0.13 (0.03)
0.15 (0.03)
0.18 (0.03)

2-norm

Simulation

1 No noise input
2 noise inputs
2
4 noise inputs
3
6 noise inputs
4
8 noise inputs
5

# Joints
94 (13)
149 (20)
225 (30)
374 (52)
499 (67)

3.5 Computational cost
We have proposed an algorithm that computes the whole solution path (s). A natural
question is then what is the computational cost of this algorithm? Suppose |E| = m at a
joint on the piece-wise linear solution path, then it takes O(qm2) to compute step 2 and
step 3 of the algorithm through Sherman-Morrison updating formula. If we assume the
training data are separable by the dictionary D, then all the training data are eventually
going to have loss (1  yi fi)+ equal to zero. Hence it is reasonable to assume the number
of joints on the piece-wise linear solution path is O(n). Since the maximum value of m
is min(n, q) and the minimum value of m is 1, we get the worst computational cost is
O(nq min(n, q)2) and the best computational cost is O(nq). Notice that this is a rough
calculation of the computational cost under some mild assumptions. Simulation results
(section 4) actually indicate that the number of joints tends to be O(min(n, q)).

4 Numerical results

In this section, we use both simulation and real data results to illustrate the 1-norm SVM.

4.1 Simulation results

The data generation mechanism is the same as the one described in section 1, except that
we generate 50 training data in each of two classes, and to make harder problems, we
sequentially augment the inputs with additional two, four, six and eight standard normal
noise inputs. Hence the second class almost completely surrounds the rst, like the skin
surrounding the oragne, in a two-dimensional subspace. The Bayes error rate for this prob-
lem is 0.0435, irrespective of dimension. In the original input space, a hyperplane cannot
nomial kernel, hence the dictionary of basis functions is D = {
separate the classes; we use an enlarged feature space corresponding to the 2nd degree poly-
(cid:2) =
1, . . . p}. We generate 1000 test data to compare the 1-norm SVM and the standard 2-norm
SVM. The average test errors over 50 simulations, with different numbers of noise inputs,
are shown in Table 1. For both the 1-norm SVM and the 2-norm SVM, we choose the
tuning parameters to minimize the test error, to be as fair as possible to each method. For
comparison, we also include the results for the non-penalized SVM.

2xjxj(cid:2) , x2

j , j, j

2xj,



From Table 1 we can see that the non-penalized SVM performs signicantly worse than the
penalized ones; the 1-norm SVM and the 2-norm SVM perform similarly when there is no
noise input (line 1), but the 2-norm SVM is adversely affected by noise inputs (line 2 - line
5). Since the 1-norm SVM has the ability to select relevant features and ignore redundant
features, it does not suffer from the noise inputs as much as the 2-norm SVM. Table 1 also
shows the number of basis functions q and the number of joints on the piece-wise linear
solution path. Notice that q < n and there is a striking linear relationship between |D| and
#Joints (Figure 2). Figure 2 also shows the 1-norm SVM result for one simulation.

0
1

.

5
0

.



0
0

.

.

5
0


0
2
0

.

5
1
0

.

r
o
r
r

E


t
s
e
T

0
1
0

.

5
0
0

.

0
0
5

0
0
4

0
0
3

i

s
t
n
o
J

f
o

r
e
b
m
u
N

0
0
2

0
0
1

0

2

4

s

6

0

2

4

s

6

10

20
50
Number of Bases

30

40

60

Figure 2: Left and middle panels: 1-norm SVM when there are 4 noise inputs. The left panel is the
piece-wise linear solution path (s). The two upper paths correspond to x2
2, which are the
relevant features. The middle panel is the test error along the solution path. The dash lines correspond
to the minimum of the test error. The right panel illustrates the linear relationship between the number
of basis functions and the number of joints on the solution path when q < n.

1 and x2

4.2 Real data results

In this section, we apply the 1-norm SVM to classication of gene microarrays. Classi-
cation of patient samples is an important aspect of cancer diagnosis and treatment. The
2-norm SVM has been successfully applied to microarray cancer diagnosis problems ([5]
and [7]). However, one weakness of the 2-norm SVM is that it only predicts a cancer class
label but does not automatically select relevant genes for the classication. Often a primary
goal in microarray cancer diagnosis is to identify the genes responsible for the classica-
tion, rather than class prediction. [4] and [5] have proposed gene selection methods, which
we call univariate ranking (UR) and recursive feature elimination (RFE) (see [14]), that can
be combined with the 2-norm SVM. However, these procedures are two-step procedures
that depend on external gene selection methods. On the other hand, the 1-norm SVM has
an inherent gene (feature) selection property due to the lasso penalty. Hence the 1-norm
SVM achieves the goals of classication of patients and selection of genes simultaneously.
We apply the 1-norm SVM to leukemia data [4]. This data set consists of 38 training data
and 34 test data of two types of acute leukemia, acute myeloid leukemia (AML) and acute
lymphoblastic leukemia (ALL). Each datum is a vector of p = 7, 129 genes. We use the
the jth genes expression level, as the basis function, i.e. q = p.
original input xj, i.e.
The tuning parameter is chosen according to 10-fold cross-validation, then the nal model
is tted on all the training data and evaluated on the test data. The number of joints on
the solution path is 104, which appears to be O(n) (cid:10) O(q). The results are summarized
in Table 2. We can see that the 1-norm SVM performs similarly to the other methods
in classication and it has the advantage of automatically selecting relevant genes. We
should notice that the maximum number of genes that the 1-norm SVM can select is upper
bounded by n, which is usually much less than q in microarray problems.

5 Conclusion

We have considered the 1-norm SVM in this paper. We illustrate that the 1-norm SVM may
have some advantage over the 2-norm SVM, especially when there are redundant features.
The solution path (s) of the 1-norm SVM is a piece-wise linear function in the tuning

Table 2: Results on Microarray Classication

Method
2-norm SVM UR
2-norm SVM RFE
1-norm SVM

CV Error Test Error

# of Genes

2/38
2/38
2/38

3/34
1/34
2/34

22
31
17

parameter s. We have proposed an efcient algorithm to compute the whole solution path
(s) of the 1-norm SVM, and facilitate adaptive selection of the tuning parameter s.

Acknowledgments

Hastie was partially supported by NSF grant DMS-0204162, and NIH grant ROI-CA-
72028-01. Tibshirani was partially supported by NSF grant DMS-9971405, and NIH grant
ROI-CA-72028.

