Abstract

The support vector machine (SVM) is a widely used tool for classication. Many efcient imple-
mentations exist for tting a two-class SVM model. The user has to supply values for the tuning
parameters: the regularization cost parameter, and the kernel parameters. It seems a common prac-
tice is to use a default value for the cost parameter, often leading to the least restrictive model.
In this paper we argue that the choice of the cost parameter can be critical. We then derive an
algorithm that can t the entire path of SVM solutions for every value of the cost parameter, with
essentially the same computational cost as tting one SVM model. We illustrate our algorithm on
some examples, and use our representation to give further insight into the range of SVM solutions.
Keywords: support vector machines, regularization, coefcient path

1. Introduction

In this paper we study the support vector machine (SVM)(Vapnik, 1996; Scholkopf and Smola,
2001) for two-class classication. We have a set of n training pairs xi, yi, where xi  Rp is a p-vector
of real-valued predictors (attributes) for the ith observation, and yi  {1, +1} codes its binary
response. We start off with the simple case of a linear classier, where our goal is to estimate a
linear decision function

f (x) = b 0 + b T x,

(1)

c(cid:13)2004 Trevor Hastie, Saharon Rosset, Robert Tibshirani and Ji Zhu.

HASTIE, ROSSET, TIBSHIRANI AND ZHU

5

.

1

0
1

.

5
0

.

0
0

.

.

5
0


.

0
1


9

11

8

7

f (x) = +1

f (x) = 0

5

10

3

1/||b ||

12

6

f (x) = 1

2

1

0.5

0.0

0.5

1.0

1.5

4

2.0

Figure 1: A simple example shows the elements of a SVM model. The +1 points are solid,
the -1 hollow. C = 2, and the width of the soft margin is 2/||b || = 2  0.587. Two
hollow points {3,5} are misclassied, while the two solid points {10,12} are correctly
classied, but on the wrong side of their margin f (x) = +1; each of these has x
i > 0. The
three square shaped points {2,6,7} are exactly on the margin.

and its associated classier

Class(x) = sign[ f (x)].

(2)

There are many ways to t such a linear classier, including linear regression, Fishers linear
discriminant analysis, and logistic regression (Hastie et al., 2001, Chapter 4). If the training data
are linearly separable, an appealing approach is to ask for the decision boundary {x : f (x) = 0}
that maximizes the margin between the two classes (Vapnik, 1996). Solving such a problem is an
exercise in convex optimization; the popular setup is

1
2

min
b 0,b

||b ||2 subject to, for each i: yi(b 0 + xT

i

b )  1.

(3)

A bit of linear algebra shows that
boundary. When the data are not separable, this criterion is modied to

i

1

||b || (b 0 + xT

b ) is the signed distance from xi to the decision

n(cid:229)
i=1
subject to, for each i: yi(b 0 + xT

||b ||2 +C

min
b 0,b

1
2

i

i,
b )  1  x

i.

(4)

1392

x
SVM REGULARIZATION PATH

Binomial Log-likelihood
Support Vector

s
s
o
L

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

-3

-2

-1

1

2

3

0
y f (x)

Figure 2: The hinge loss penalizes observation margins y f (x) less than +1 linearly, and is indiffer-
ent to margins greater than +1. The negative binomial log-likelihood (deviance) has the
same asymptotes, but operates in a smoother fashion near the elbow at y f (x) = 1.

Here the x
i are non-negative slack variables that allow points to be on the wrong side of their soft
margin ( f (x) = 1), as well as the decision boundary, and C is a cost parameter that controls the
amount of overlap. Figure 1 shows a simple example. If the data are separable, then for sufciently
large C the solutions to (3) and (4) coincide.
If the data are not separable, as C gets large the
solution approaches the minimum overlap solution with largest margin, which is attained for some
nite value of C.

Alternatively, we can formulate the problem using a Loss + Penalty criterion (Wahba et al.,

2000; Hastie et al., 2001):

min
b 0,b

n(cid:229)

[1  yi(b 0 + b T xi)]+ +

i=1

||b ||2.

2

(5)

The regularization parameter l
in (5) corresponds to 1/C, with C in (4). Here the hinge loss
L(y, f (x)) = [1  y f (x)]+ can be compared to the negative binomial log-likelihood L(y, f (x)) =
log[1 + exp(y f (x))] for estimating the linear function f (x) = b 0 + b T x; see Figure 2.

This formulation emphasizes the role of regularization. In many situations we have sufcient
variables (e.g. gene expression arrays) to guarantee separation. We may nevertheless avoid the
maximum margin separator (l  0), which is governed by observations on the boundary, in favor of
a more regularized solution involving more observations.

This formulation also admits a class of more exible, nonlinear generalizations

min
f H

n(cid:229)

i=1

L(yi, f (xi)) + l J( f ),

(6)

where f (x) is an arbitrary function in some Hilbert space H , and J( f ) is a functional that measures
the roughness of f in H .

The nonlinear kernel SVMs arise naturally in this context. In this case f (x) = b 0 + g(x), and
J( f ) = J(g) is a norm in a Reproducing Kernel Hilbert Space of functions HK generated by a

1393

l
HASTIE, ROSSET, TIBSHIRANI AND ZHU

Radial Kernel: C = 2, g = 1

Radial Kernel: C = 10,000, g = 1

o

o
o

o

o

o
o
o
o

o

o

o
o
o

o
o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o

o
o

o
o

o

o

o

o

o
o
o

o
o

o
o
o
o

o

Training Error: 0.160
Test Error:       0.218
Bayes Error:    0.210

o

Training Error: 0.065
Test Error:       0.307
Bayes Error:    0.210

o

o

o
o

o

o

o
o
o
o

o

o

o

o
o
oo

o

o

o
o
o
o
o
o
o
o
o
o

o

o

o
o

o
o

o

o
o
o

o
o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o

o
o

o
o

o

o

o

o

o
o
o
o
o
o
o
o
o
o

o

o

o
o

o
o

o

o

o
o
oo

o

o

o
o
o

o
o

o
o
o
o

o

Figure 3: Simulated data illustrate the need for regularization. The 200 data points are generated
from a pair of mixture densities. The two SVM models used radial kernels with the scale
and cost parameters as indicated at the top of the plots. The thick black curves are the
decision boundaries, the dotted curves the margins. The less regularized t on the right
overts the training data, and suffers dramatically on test error. The broken purple curve
is the optimal Bayes decision boundary.

positive-denite kernel K(x, x0). By the well-studied properties of such spaces (Wahba, 1990; Ev-
geniou et al., 1999), the solution to (6) is nite dimensional (even if HK is innite dimensional), in
this case with a representation f (x) = b 0 + (cid:229) n
iK(x, xi). Consequently (6) reduces to the nite
i=1
form

n(cid:229)

min
b 0,q

L[yi,b 0 +

n(cid:229)

i=1

j=1

iK(xi, x j)] +

2

n(cid:229)

n(cid:229)

j=1

j0=1

jq

j0K(x j, x0

j).

(7)

With L the hinge loss, this is an alternative route to the kernel SVM; see Hastie et al. (2001) for
more details.

It seems that the regularization parameter C (or l ) is often regarded as a genuine nuisance
in the community of SVM users. Software packages, such as the widely used SVMlight (Joachims,
1999), provide default settings for C, which are then used without much further exploration. A
recent introductory document (Hsu et al., 2003) supporting the LIBSVM package does encourage
grid search for C.

Figure 3 shows the results of tting two SVM models to the same simulated data set. The data
are generated from a pair of mixture densities, described in detail in Hastie et al. (2001, Chapter 2).1
The radial kernel function K(x, x0) = exp(g ||x  x0||2) was used, with g = 1. The model on the left
is more regularized than that on the right (C = 2 vs C = 10,000, or l = 0.5 vs l = 0.0001), and

1. The actual training data and test distribution are available from

http:// www-stat.stanford.edu/ElemStatLearn.

1394

q
q
l
q
SVM REGULARIZATION PATH

Test Error Curves  SVM with Radial Kernel

g = 5

g = 1

g = 0.5

g = 0.1

r
o
r
r

E


t
s
e
T

5
3

.

0

0
3
0

.

5
2

.

0

0
2

.

0

1e01

1e+01

1e+03

1e01

1e+01

1e+03

1e01

1e+01

1e+03

1e01

1e+01

1e+03

C = 1/l

Figure 4: Test error curves for the mixture example, using four different values for the radial kernel
parameter g . Small values of C correspond to heavy regularization, large values of C to
light regularization. Depending on the value of g , the optimal C can occur at either end of
the spectrum or anywhere in between, emphasizing the need for careful selection.

performs much better on test data. For these examples we evaluate the test error by integration over
the lattice indicated in the plots.

Figure 4 shows the test error as a function of C for these data, using four different values for the
kernel scale parameter g . Here we see a dramatic range in the correct choice for C (or l = 1/C);
when g = 5, the most regularized model is called for, and we will see in Section 6 that the SVM is
really performing kernel density classication. On the other hand, when g = 0.1, we would want to
choose among the least regularized models.

One of the reasons that investigators avoid extensive exploration of C is the computational
In this paper we develop an algorithm which ts the entire path of SVM solu-
cost involved.
tions [b 0(C),b (C)], for all possible values of C, with essentially the computational cost of tting a
single model for a particular value of C. Our algorithm exploits the fact that the Lagrange multi-
pliers implicit in (4) are piecewise-linear in C. This also means that the coefcients b (C) are also
piecewise-linear in C. This is true for all SVM models, both linear and nonlinear kernel-based
SVMs. Figure 8 on page 1406 shows these Lagrange paths for the mixture example. This work
was inspired by the related Least Angle Regression (LAR) algorithm for tting LASSO models
(Efron et al., 2004), where again the coefcient paths are piecewise linear.

These speedups have a big impact on the estimation of the accuracy of the classier, using a
validation dataset (e.g. as in K-fold cross-validation). We can rapidly compute the t for each test
data point for any and all values of C, and hence the generalization error for the entire validation set
as a function of C.

In the next section we develop our algorithm, and then demonstrate its capabilities on a number
of examples. Apart from offering dramatic computational savings when computing multiple solu-

1395

HASTIE, ROSSET, TIBSHIRANI AND ZHU

tions (Section 4.3), the nature of the path, in particular at the boundaries, sheds light on the action
of the kernel SVM (Section 6).

2. Problem Setup

We use a criterion equivalent to (4), implementing the formulation in (5):

n(cid:229)

i=1

b T b

i +

min
b ,b 0
subject to 1  yi f (xi)  x

2

i; x

i  0; f (x) = b 0 + b T x.

(8)

Initially we consider only linear SVMs to get the intuitive avor of our procedure; we then general-
ize to kernel SVMs.

We construct the Lagrange primal function

LP :

n(cid:229)

i=1

b T b +

i +

2

n(cid:229)

i=1

i(1  yi f (xi)  x

i) 

n(cid:229)

i=1

ix

i

and set the derivatives to zero. This gives

b =

1

n(cid:229)

iyixi,

i=1

i = 0,

n(cid:229)

yia

i=1
i = 1  g

i,

:

:

:

0

i

i(1  yi f (xi)  x

i) = 0,
i = 0.

ix

along with the KKT conditions

(9)

(10)

(11)

(12)

(13)
(14)

We see that 0  a
i = 0 since no cost is incurred, and a

i  1, with a

i = 1 when x

i > 0 (which is when yi f (xi) < 1). Also when yi f (xi) > 1,

i = 0. When yi f (xi) = 1, a

i can lie between 0 and 1.2

We wish to nd the entire solution path for all values of l  0. The basic idea of our algorithm
is as follows. We start with l
large and decrease it toward zero, keeping track of all the events that
occur along the way. As l decreases, ||b || increases, and hence the width of the margin decreases
(see Figure 1). As this width decreases, points move from being inside to outside the margin. Their
corresponding a
i = 0 when
they are outside the margin (yi f (xi) > 1). By continuity, points must linger on the margin (yi f (xi) =
1) while their a
i(l ) trajectories are piecewise-linear
in l
, which affords a great computational savings: as long as we can establish the break points, all

i = 1 when they are inside the margin (yi f (xi) < 1) to a

i decrease from 1 to 0. We will see that the a

i change from a

2. For readers more familiar with the traditional SVM formulation (4), we note that there is a simple connection be-
0
i  [0,C]. We prefer our
i  [0,1], and this simplies the denition of the paths we dene.

tween the corresponding Lagrange multipliers, a
formulation here since our a

i, and hence in that case a

i/l = Ca

i = a

0

1396

x
l
x
l
a
g


b
l
a


b


x
a
a
g
x
SVM REGULARIZATION PATH

values in between can be found by simple linear interpolation. Note that points can return to the
margin, after having passed through it.

It is easy to show that if the a

i(C) and b (C)
are piecewise linear in C. It turns out that b 0(C) is also piecewise linear in C. We will frequently
switch between these two representations.

i(l ) are piecewise linear in l

, then both a

0

i(C) = Ca

We denote by I+ the set of indices corresponding to yi = +1 points, there being n+ = |I+| in
total. Likewise for I and n. Our algorithm keeps track of the following sets (with names inspired
by the hinge loss function in Figure 2):

 E = {i : yi f (xi) = 1, 0  a
 L = {i : yi f (xi) < 1, a
 R = {i : yi f (xi) > 1, a

i  1}, E for Elbow,

i = 1}, L for Left of the elbow,

i = 0}, R for Right of the elbow.

3. Initialization
We need to establish the initial state of the sets dened above. When l
), from (10)
b = 0, and the initial values of b 0 and the a
i depend on whether n = n+ or not. If the classes are
balanced, one can directly nd the initial conguration by nding the most extreme points in each
class. We will see that when n 6= n+, this is no longer the case, and in order to satisfy the constraint
(11), a quadratic programming algorithm is needed to obtain the initial conguration.

is very large (

In fact, our SvmPath algorithm can be started at any intermediate solution of the SVM optimiza-
tion problem (i.e. the solution for any l ), since the values of a
i and f (xi) determine the sets L, E
and R . We will see in Section 6 that if there is no intercept in the model, the initialization is again
trivial, no matter whether the classes are balanced or not. We have prepared some MPEG movies
to illustrate the two special cases detailed below. The movies can be downloaded at the web site
http://www-stat.stanford.edu/hastie/Papers/svm/MOVIE/.

sufciently large, all the a
i = n+ + n.

i = 1. The initial b 0  [1,1]  any value gives the

3.1 Initialization: n = n+
Lemma 1 For l
same loss (cid:229) n
i=1
Proof Our proof relies on the criterion and the KKT conditions in Section 2. Since b = 0, f (x) = b 0.
To minimize (cid:229) n
i = 0
i=1
in (12), and hence a
i = 1, i  I+, and
hence also a
We also have that for these early and large values of l

i, we should clearly restrict b 0 to [1,1]. For b 0  (1,1), all the x
i = 1. Picking one of the endpoints, say b 0 = 1, causes a

i = 1, i  I, for (11) to hold.

i > 0, g

1

b =

b  where b  =

n(cid:229)

i=1

yixi.

(15)

Now in order that (11) remain satised, we need that one or more positive and negative examples
hit the elbow simultaneously. Hence as l decreases, we require that i yi f (xi)  1 or

yi"b T xi

+ b 0#  1

1397

(16)

x
x
l
l
HASTIE, ROSSET, TIBSHIRANI AND ZHU

5

.

0

)
C
(
0

0

.

0

5

.

0


0

.

1


33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333
88888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888888
00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000
44444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444
99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999
11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111
66666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666666
55555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555555
77777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777777
22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222

5

.

0

)
C
(

0

.

0

5

.

0


0

.

1


0.00

0.05

0.10
C = 1/l

0.15

0.20

0.00

0.05

0.15

0.20

0.10
C = 1/l

Figure 5: The initial paths of the coefcients in a small simulated dataset with n = n+. We see
the zone of allowable values for b 0 shrinking toward a xed point (20). The vertical lines
indicate the breakpoints in the piecewise linear coefcient paths.

or

b 0  1 

b T xi

b 0  1 

b T xi

for all i  I+

for all i  I.

(17)

(18)

b T xi and i = argminiI

b T xi (for simplicity we assume that these are
Pick i+ = argmaxiI+
unique). Then at this point of entry and beyond for a while we have a
i, and f (xi+) = 1
and f (xi) = 1. This gives us two equations to solve for the initial point of entry l 0 and b 0, with
solutions

i+ = a

l 0 =

2

b T xi+  b T xi

,

b 0 =  b T xi+ + b T xi
b T xi+  b T xi! .

(19)

(20)

Figure 5 (left panel) shows a trajectory of b 0(C) as a function of C, for a small simulated data
set. These solutions were computed directly using a quadratic-programming algorithm, using a

1398

b
b
l
l
SVM REGULARIZATION PATH

2

.

0

0
0

.

.

2
0


.

4
0


)
C
(

********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************
********************************************************************************************************************************************************************************************************

0.00

0.05

0.10

C

0.15

0.20

0.00

0.05

0.15

0.20

0.10

C

1
1

1

1

1

1
1
1

11
11

s
n
i
g
r
a

M

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0


0

.

1


11
7
12
6

5
3
8910
1

2

4

)
C
(
0

)
C
(

9

.

0

8
0

.

7

.

0

6
0

.

5
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0

.

0

0.00

0.05

0.10

C

0.15

0.20

0.00

0.05

0.15

0.20

0.10

C

Figure 6: The initial paths of the coefcients in a case where n < n+. All the n points are
misclassied, and start off with a margin of 1. The a 
i remain constant until one of
the points in I reaches the margin. The vertical lines indicate the breakpoints in the
piecewise linear b (C) paths. Note that the a
i(C) are not piecewise linear in C, but rather
in l = 1/C.

1399

b
b
a
HASTIE, ROSSET, TIBSHIRANI AND ZHU

predened grid of values for l
nature of this path. The breakpoints were found using our exact-path algorithm.

. The arbitrariness of the initial values is indicated by the zig-zag

3.2 Initialization: n+ > n
In this case, when b = 0, the optimal choice for b 0 is 1, and the loss is (cid:229) n
i=1
also require that (11) holds.
Lemma 2 With b (a ) = (cid:229) n

i=1 yia
i } = argmina

{a 

ixi, let
||b (a )||2

s.t. a

i  [0,1] for i  I+, a

i = n. However, we

iI+

i = n

(21)

(22)

i = 1 for i  I, and (cid:229)
i , and b = b /l

Then for some l 0 we have that for all l > l 0, a
Proof The Lagrange dual corresponding to (9) is obtained by substituting (10)(12) into (9) (Hastie
et al., 2001, Equation 12.13):

, with b  = (cid:229) n

i=1 yia 

i = a 

i xi.

LD =

n(cid:229)

i=1

i 

1
2l

n(cid:229)

n(cid:229)

i=1

i0=1

ia

i0yiyi0xixi0.

(23)

Since we start with b = 0, b 0 = 1, all the I points are misclassied, and hence we will have
i = 1i  I, and hence from (11) (cid:229) n
i = 2n. This latter sum will remain 2n for a while as
i=1
b grows away from zero. This means that during this phase, the rst term in the Lagrange dual is
2l ||b (a )||2, and since we maximize the dual, this proves
constant; the second term is equal to  1
the result.

We now establish the starting point l 0 and b 0 when the a

i start to change. Let b  be the xed

coefcient direction corresponding to a 

i (as in (15)):

There are two possible scenarios:

b  =

n(cid:229)

i=1

a 
i yixi.

(24)

1. There exist two or more elements in I+ with 0 < a 
2. a 

i  {0,1} i  I+.

i < 1, or

Consider the rst scenario (depicted in Figure 6), and suppose a 
i = argminiI
margin, we can nd

i+  (0,1) (on the margin). Let
b T xi. Then since the point i+ remains on the margin until an I point reaches its

l 0 =

b T xi+  b T xi

2

,

(25)

identical in form to to (19), as is the corresponding b 0 to (20).

For the second scenario, it is easy to see that we nd ourselves in the same situation as in
i = 1 must reach the margin
b T xi, where

Section 3.1a point from I and one of the points in I+ with a 
simultaneously. Hence we get an analogous situation, except with i+ = argmaxiI 1
+ is the subset of I+ with a 
I 1

i = 1.

+

1400

x
a
a
a
a
a
SVM REGULARIZATION PATH

3.3 Kernels

The development so far has been in the original feature space, since it is easier to visualize. It is
easy to see that the entire development carries through with kernels as well. In this case f (x) =
b 0 + g(x), and the only change that occurs is that (10) is changed to

g(xi) =

1

n(cid:229)

jy jK(xi, x j), i = 1, . . . , n,

(26)

j=1
jy j/l using the notation in (7).

or q

j(l ) = a
Our initial conditions are dened in terms of expressions b T xi+, for example, and again it is

easy to see that the relevant quantities are

g(xi+) =

n(cid:229)

j=1

a 

jy jK(xi+, x j),

(27)

where the a 

i are all 1 in Section 3.1, and dened by Lemma 2 in Section 3.2.
Hereafter we will develop our algorithm for this more general kernel case.

4. The Path

The algorithm hinges on the set of points E sitting at the elbow of the loss function  i.e on the
margin. These points have yi f (xi) = 1 and a
i  [0,1]. These are distinct from the points R to the
right of the elbow, with yi f (xi) > 1 and a
i = 0, and those points L to the left with yi f (xi) < 1 and
i = 1. We consider this set at the point that an event has occurred. The event can be either:

1. The initial event, which means 2 or more points start at the elbow, with their initial values of

a  [0,1].

2. A point from L has just entered E, with its value of a
3. A point from R has reentered E, with its value of a

i initially 1.

i initially 0.

4. One or more points in E has left the set, to join either R or L.

Whichever the case, for continuity reasons this set will stay stable until the next event occurs,
i must change from 0 to 1 or vice versa. Since all points in E

since to pass through E, a points a
have yi f (xi) = 1, we can establish a path for their a

i.

Event 4 allows for the possibility that E becomes empty while L is not. If this occurs, then
the KKT condition (11) implies that L is balanced w.r.t. +1s and -1s, and we resort to the initial
condition as in Section 3.1.

We use the subscript ` to index the sets above immediately after the `th event has occurred.
0 and l ` be the values of these parameters at the point of entry.
0 = l `b `
0.

Suppose |E`| = m, and let a
Likewise f ` is the function at this point. For convenience we dene a 0 = l

0, and hence a

i , b `

`

`

Since

f (x) =

1

l   n(cid:229)

j=1

y ja

jK(x, x j) + a 0! ,

1401

(28)

l
a
a
b
HASTIE, ROSSET, TIBSHIRANI AND ZHU

for l ` > l > l `+1 we can write

f (x) = (cid:20) f (x) 
l " (cid:229)
(a

jE`

=

1

l `

f `(x)(cid:21) +
j  a

`

l `

f `(x)

j)y jK(x, x j) + (a 0  a

`

0) + l ` f `(x)# .

(29)

The second line follows because all the observations in L` have their a
their a
that

i = 1, and those in R` have
. Since each of the m points xi  E` are to stay at the elbow, we have

i = 0, for this range of l

1

l " (cid:229)
j  a

`

jE`

Writing d

j = a

j, from (30) we have

(a

j  a

`

j)yiy jK(xi, x j) + yi(a 0  a

`

0) + l `# = 1, i  E`.

(30)

(31)

(32)

jyiy jK(xi, x j) + yid 0 = l `  l , i  E`.

jE`

Furthermore, since at all times (cid:229) n

i=1 yia

i = 0, we have that

y jd

j = 0.

jE`

Equations (31) and (32) constitute m + 1 linear equations in m + 1 unknowns d

j, and can be solved.
` the m  m matrix with i jth entry yiy jK(xi, x j) for i and j in E`, we have from

Denoting by K

(31) that

d + d 0y` = (l `  l )1,

K
`

where y` is the m vector with entries yi, i  E`. From (32) we have

d = 0.

yT
`

We can combine these two into one matrix equation as follows. Let

then (34) and (33) can be written

A` =(cid:18) 0

T
y`
y` K

d a =(cid:18)d 0

`(cid:19) ,
d (cid:19) , and
A`d a = (l `  l )1a.

1a =(cid:18)0
1(cid:19) ,

If A` has full rank, then we can write

ba = A`

11a,

and hence

Hence for l `+1 < l < l `, the a

`

j  (l `  l )b j, j  {0}  E`.

j = a
j for points at the elbow proceed linearly in l

l `

f (x) =

h f `(x)  h`(x)i + h`(x),

1402

(33)

(34)

(35)

(36)

(37)

(38)

. From (29) we have

(39)

l
l
(cid:229)
d
(cid:229)
a
l
SVM REGULARIZATION PATH

where

h`(x) = (cid:229)

jE`

y jb jK(x, x j) + b0.

(40)

Thus the function itself changes in a piecewise-inverse manner in l

.

If A` does not have full rank, then the solution paths for some of the a

i are not unique, and
more care has to be taken in solving the system (36). This occurs, for example, when two training
observations are identical (tied in x and y). Other degeneracies can occur, but rarely in practice, such
as three different points on the same margin in R2. These issues and some of the related updating
and downdating schemes are an area we are currently researching, and will be reported elsewhere.

4.1 Finding l `+1
The paths (38)(39) continue until one of the following events occur:

1. One of the a

i for i  E` reaches a boundary (0 or 1). For each i the value of l

for which this

occurs is easily established from (38).

2. One of the points in L ` or R ` attains yi f (xi) = 1. From (39) this occurs for point i at

l = l `(cid:18) f `(xi)  h`(xi)
yi  h`(xi) (cid:19) .

(41)

By examining these conditions, we can establish the largest l < l ` for which an event occurs, and
hence establish l `+1 and update the sets.

One special case not addressed above is when the set E becomes empty during the course of the
algorithm. In this case, we revert to an initialization setup using the points in L. It must be the case
that these points have an equal number of +1s as -1s, and so we are in the balanced situation as in
3.1.

By examining in detail the linear boundary in examples where p = 2, we observed several

different types of behavior:

1. If |E | = 0, than as l decreases, the orientation of the decision boundary stays xed, but the

margin width narrows as l decreases.

2. If |E | = 1 or |E | = 2, but with the pair of points of opposite classes, then the orientation

typically rotates as the margin width gets narrower.

3. If |E | = 2, with both points having the same class, then the orientation remains xed, with

the one margin stuck on the two points as the decision boundary gets shrunk toward it.

4. If |E |  3, then the margins and hence f (x) remains xed, as the a

i(l ) change. This implies

that h` = f ` in (39).

4.2 Termination
In the separable case, we terminate when L becomes empty. At this point, all the x
and further movement increases the norm of b unnecessarily.

In the non-separable case, l

runs all the way down to zero. For this to happen without f
blowing up in (39), we must have f `  h` = 0, and hence the boundary and margins remain

i in (8) are zero,

1403

HASTIE, ROSSET, TIBSHIRANI AND ZHU

xed at a point where (cid:229)
this constraint.

i x

i is as small as possible, and the margin is as wide as possible subject to

4.3 Computational Complexity

At any update event ` along the path of our algorithm, the main computational burden is solving
the system of equations of size m` = |E`|. While this normally involves O(m3
` ) computations, since
E`+1 differs from E` by typically one observation, inverse updating/downdating can reduce the
computations to O(m2
` ). The computation of h`(xi) in (40) requires O(nm`) computations. Beyond
that, several checks of cost O(n) are needed to evaluate the next move.

0
0
1

0
8

0
6

0
4

0
2

0

l

w
o
b
E
e
z
S



i

111111

g = 5

g = 1

1111111111111111111111111111111

11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111
2222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
3333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333

444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444

g = 0.5

g = 0.1

11111
1

444444444444444
44444
4
4

1e04

1e02

1e+00

Figure 7: The elbow sizes |E`| as a function of l

, for different values of the radial-kernel parameter

g . The vertical lines show the positions used to compare the times with libsvm.

We have explored using partitioned inverses for updating/downdating the solutions to the elbow
equations (for the nonsingular case), and our experiences are mixed. In our R implementations,
the computational savings appear negligible for the problems we have tackled, and after repeated
updating, rounding errors can cause drift. At the time of this publication, we in fact do not use
updating at all, and simply solve the system each time. We are currently exploring numerically
stable ways for managing these updates.

Although we have no hard results, our experience so far suggests that the total number L

of
moves is O(k min(n+, n)), for k around 4  6; hence typically some small multiple c of n. If the
average size of E` is m, this suggests the total computational burden is O(cn2m + nm2), which is
similar to that of a single SVM t.

Our R function SvmPath computes all 632 steps in the mixture example (n+ = n = 100, radial
kernel, g = 1) in 1.44(0.02) secs on a Pentium 4, 2Ghz Linux machine; the svm function (using the
optimized code libsvm, from the R library e1071) takes 9.28(0.06) seconds to compute the solution
at 10 points along the path. Hence it takes our procedure about 50% more time to compute the entire
path, than it costs libsvm to compute a typical single solution.

1404

l
SVM REGULARIZATION PATH

We often wish to make predictions at new inputs. We can also do this efciently for all values
, because from (28) we see that (modulo 1/l ), these also change in a piecewise-linear fashion
of l
in l
. Hence we can compute the entire t path for a single input x in O(n) calculations, plus an
additional O(nq) operations to compute the kernel evaluations (assuming it costs O(q) operations
to compute K(x, xi)).

5. Examples

In this section we look at three examples, two synthetic and one real. We examine our running mix-
ture example in some more detail, and expose the nature of quadratic regularization in the kernel
feature space. We then simulate and examine a scaled-down version of the p (cid:29) n problemmany
more inputs than samples. Despite the fact that perfect separation is possible with large margins, a
heavily regularized model is optimal in this case. Finally we t SVM path models to some microar-
ray cancer data.

5.1 Mixture Simulation
In Figure 4 we show the test-error curves for a large number of values of l
, and four different values
for g
for the radial kernel. These l ` are in fact the entire collection of change points as described
in Section 4. For example, for the second panel, with g = 1, there are 623 change points. Figure 8
[upper plot] shows the paths of all the a
i(l ), as well as [lower plot] a few individual examples. An
MPEG movie of the sequence of models can be downloaded from the rst authors website.

We were at rst surprised to discover that not all these sequences achieved zero training errors
on the 200 training data points, at their least regularized t. In fact the minimal training errors, and
the corresponding values for g are summarized in Table 1. It is sometimes argued that the implicit

Training Errors
Effective Rank

5
0
200

1
12
177

0.5
21
143

0.1
33
76

Table 1: The number of minimal training errors for different values of the radial kernel scale pa-
rameter g , for the mixture simulation example. Also shown is the effective rank of the
200  200 Gram matrix Kg .

feature space is innite dimensional for this kernel, which suggests that perfect separation is
always possible. The last row of the table shows the effective rank of the kernel Gram matrix K
(which we dened to be the number of singular values greater than 1012). This 200  200 matrix
has elements Ki, j = K(xi, x j), i, j = 1, . . . , n. In general a full rank K is required to achieve perfect
separation. Similar observations have appeared in the literature (Bach and Jordan, 2002; Williams
and Seeger, 2000).

This emphasizes the fact that not all features in the feature map implied by K are of equal
stature; many of them are shrunk way down to zero. Alternatively, the regularization in (6) and (7)
penalizes unit-norm features by the inverse of their eigenvalues, which effectively annihilates some,
depending on g . Small g
implies wide, at kernels, and a suppression of wiggly, rough functions.

1405

g
HASTIE, ROSSET, TIBSHIRANI AND ZHU

1

)

(l
i

0

1e04

1e02

1e+00

)

(l
i

1

0

0

5

10

15

Figure 8: [Upper plot] The entire collection of piece-wise linear paths a

i(l ), i = 1, . . . , N, for the
is plotted on the log-scale. [Lower plot] Paths for 5 selected

mixture example. Note: l
observations; l

is not on the log scale.

1406

a
l
a
l
SVM REGULARIZATION PATH

Writing (7) in matrix form,

min
b 0,q

L[y,Kq ] +

q T Kq ,

2

(42)

we reparametrize using the eigen-decomposition of K = UDUT . Let Kq = Uq  where q  = DUT q
Then (42) becomes

.

min
b 0,q 

L[y,Uq ] +

q T D1q .

2

(43)

Now the columns of U are unit-norm basis functions (in R2) spanning the column space of K;

g = 5

4
3
4
4
4
4
44
4
4
44
4
4
444
4
44
4444

22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
333333333333333333333333333333
1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111
33333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333333

g = 1
22222222222222222222222222

4
44
444
44
44444
44
44444

g = 0.1
444444
444

222222222222222222

g = 0.5

4444
44
4444
4

4444444444

222

222

222

22222

333

33333

11111111

11111111
111
1
11

444
4

44444444

44444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444444
3333333333333333333333333333333333333333333
4
4
3
2

22222222222222

3333

222222222

22

l

e
u
a
v
n
e
g
E

i

1
0
+
e
1

3
0

e
1

7
0

e
1

1
1

e
1

5
1

e
1

0

50

100

150

200

Eigenvalue Sequence

Figure 9: The eigenvalues (on the log scale) for the kernel matrices Kg corresponding to the four
values of g as in Figure 4. The larger eigenvalues correspond in this case to smoother
eigenfunctions, the small ones to rougher. The rougher eigenfunctions get penalized ex-
ponentially more than the smoother ones. For smaller values of g , the effective dimension
of the space is truncated.

from (43) we see that those members corresponding to near-zero eigenvalues (the elements of the
diagonal matrix D) get heavily penalized and hence ignored. Figure 9 shows the elements of D for
the four values of g . See Hastie et al. (2001, Chapter 5) for more details.

5.2 p(cid:29)n Simulation

The SVM is popular in situations where the number of features exceeds the number of observations.
Gene expression arrays are a leading example, where a typical dataset has p > 10,000 while n < 100.
Here one typically ts a linear classier, and since it is easy to separate the data, the optimal marginal
classier is the de facto choice. We argue here that regularization can play an important role for these
kinds of data.

1407

l
l
HASTIE, ROSSET, TIBSHIRANI AND ZHU

We mimic a simulation found in Marron (2003). We have p = 50 and n = 40, with a 20-20 split
of + and - class members. The xi j are all iid realizations from a N(0,1) distribution, except for
the rst coordinate, which has mean +2 and -2 in the respective classes.3 The Bayes classier in
this case uses only the rst coordinate of x, with a threshold at 0. The Bayes risk is 0.012. Figure 10
summarizes the experiment. We see that the most regularized models do the best here, not the
maximal margin classier.

Although the most regularized linear SVM is the best in this example, we notice a disturbing
aspect of its endpoint behavior in the top-right plot. Although b
is determined by all the points, the
threshold b 0 is determined by the two most extreme points in the two classes (see Section 3.1). This
can lead to irregular behavior, and indeed in some realizations from this model this was the case. For
values of l
larger than the initial value l 1, we saw in Section 3 that the endpoint behavior depends
on whether the classes are balanced or not. In either case, as l
increases, the error converges to the
estimated null error rate nmin/n.

This same objection is often made at the other extreme of the optimal margin; however, it typi-
cally involves more support points (19 points on the margin here), and tends to be more stable (but
still no good in this case). For solutions in the interior of the regularization path, these objections
no longer hold. Here the regularization forces more points to overlap the margin (support points),
and hence determine its orientation.

Included in the gures are regularized linear discriminant analysis and logistic regression mod-
els (using the same l ` sequence as the SVM). Both show similar behavior to the regularized SVM,
having the most regularized solutions perform the best. Logistic regression can be seen to assign
weights pi(1  pi) to observations in the tting of its coefcients b and b 0, where

pi =

1

1 + eb 0b T xi

(44)

is the estimated probability of +1 occurring at xi (Hastie and Tibshirani, 1990, e.g.).

 Since the decision boundary corresponds to p(x) = 0.5, these weights can be seen to die down

in a quadratic fashion from 1/4, as we move away from the boundary.

 The rate at which the weights die down with distance from the boundary depends on ||b ||; the

smaller this norm, the slower the rate.

It can be shown, for separated classes, that the limiting solution (l  0) for the regularized
logistic regression model is identical to the SVM solution: the maximal margin separator (Rosset
et al., 2003).

Not surprisingly, given the similarities in their loss functions (Figure 2), both regularized SVMs
and logistic regression involve more or less observations in determining their solutions, depending
on the amount of regularization. This involvement is achieved in a smoother fashion by logistic
regression.

5.3 Microarray Classication

We illustrate our algorithm on a large cancer expression data set (Ramaswamy et al., 2001). There
are 144 training tumor samples and 54 test tumor samples, spanning 14 common tumor classes that

3. Here we have one important feature; the remaining 49 are noise. With expression arrays, the important features

typically occur in groups, but the total number p is much larger.

1408

SVM REGULARIZATION PATH

Optimal Margin

Extreme Margin

+
+

+

+

+
+
+
+
+
++
++
+
+
+
+

+

+
+

n
o
i
t
c
e
r
i



D
M
V
S

+
+

+

+ +
+
+
+++
+
+
+
+
+
++

+

+

+

2

1

0

1


2


3


o

o

o
o
o
o
o

o
o
o

o

o

o

o
o
o

o

o

o

o

2

4

4

2

0

2

4

o

o
o
o
o
o
oo
o
o
o
o

o

o
o
oo

4

2

o
o

o

0

Optimal Direction

Optimal Direction

RRRR
SSSSS

R
S

R

S

RRRR
SSSS

R

SSSSSSSSSSSSSSSSSSS
RRRRRRRRRRRRRRRRRR

S

RRRRRRRRRRRRRRRRRRRRRRRRRRRRRR
LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL
SSSSSSSSSSSSSS

SSSSSSSSSSS

SS

SS

R

S

RR
RR

SSSS

R
S

R

S

RRRR

R

SSSSSSSSSSSSSSSSSSSSSSS
RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
LLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLLL

r
o
r
r

E


t
s
e
T

5
0
0

.

4
0
0

.

3
0
0

.

2
0
0

.

2

1

0

1


2


3


0
4

5
3

0
3

5
2

n
o
i
t
c
e
r
i



D
M
V
S

)
s
e
e
r
g
e
d
(

e
g
n
A

l

200

300

100
l = 1/C

200

300

100
l = 1/C

Figure 10: p (cid:29) n simulation. [Top Left] The training data projected onto the space spanned by the
(known) optimal coordinate 1, and the optimal margin coefcient vector found by a non-
regularized SVM. We see the large gap in the margin, while the Bayes-optimal classier
(vertical red line) is actually expected to make a small number of errors. [Top Right] The
same as the left panel, except we now project onto the most regularized SVM coefcient
vector. This solution is closer to the Bayes-optimal solution. [Lower Left] The angles
between the Bayes-optimal direction, and the directions found by the SVM (S) along the
regularized path. Included in the gure are the corresponding coefcients for regularized
LDA (R)(Hastie et al., 2001, Chapter 4) and regularized logistic regression (L)(Zhu
and Hastie, 2004), using the same quadratic penalties. [Lower Right] The test errors
corresponding to the three paths. The horizontal line is the estimated Bayes rule using
only the rst coordinate.

1409

HASTIE, ROSSET, TIBSHIRANI AND ZHU

account for 80% of new cancer diagnoses in the U.S.A. There are 16,063 genes for each sample.
Hence p = 16,063 and n = 144. We denote the number of classes by K = 14. A goal is to build a
classier for predicting the cancer class of a new sample, given its expression values.

We used a common approach for extending the SVM from two-class to multi-class classica-

tion:

1. Fit K different SVM models, each one classifying a single cancer class (+1) versus the rest

(-1).
2. Let [ f l

a test observation x.

1 (x), . . . , f l

K(x)] be the vector of evaluations of the tted functions (with parameter l ) at

3. Classify Cl

(x) = argmaxk f l

k (x).

Other, more direct, multi-class generalizations exist (Rosset et al., 2003; Weston and Watkins,
1998); although exact path algorithms are possible here too, we were able to implement our ap-
proach most easily with the one vs all strategy above. Figure 11 shows the results of tting
this family of SVM models. Shown are the training error, test error, as well as 8-fold balanced
cross-validation.4 The training error is zero everywhere, but both the test and CV error increase
sharply when the model is too regularized. The right plot shows similar results using quadratically
regularized multinomial regression (Zhu and Hastie, 2004).

Although the least regularized SVM and multinomial models do the best, this is still not very

good. With fourteen classes, this is a tough classication problem.

It is worth noting that:

 The 14 different classication problems are very lop-sided; in many cases 8 observations
in one class vs the 136 others. This tends to produce solutions with all members of the small
class on the boundary, a somewhat unnatural situation.

 For both the SVM and the quadratically regularized multinomial regression, one can reduce
the logistics by pre-transforming the data. If X is the n  p data matrix, with p (cid:29) n, let its
singular-value decomposition be UDVT . We can replace X by the n n matrix XV = UD = R
and obtain identical results (Hastie and Tibshirani, 2003). The same transformation V is
applied to the test data. This transformation is applied once upfront, and the transformed data
is used in all subsequent analyses (i.e. K-fold cross-validation as well).

6. No Intercept and Kernel Density Classication

Here we consider a simplication of the models (6) and (7) where we leave out the intercept term
b 0. It is easy to show that the solution for g(x) has the identical form as in (26):

g(x) =

1

n(cid:229)

j=1

jy jK(x, x j).

(45)

However, f (x) = g(x) (or f (x) = b T x in the linear case), and we lose the constraint (11) due to the
intercept term.

This also adds considerable simplication to our algorithm, in particular the initial conditions.

4. By balanced we mean the 14 cancer classes were represented equally in each of the folds; 8 folds were used to

accommodate this balance, since the class sizes in the training set were multiples of 8.

1410

l
a
SVM REGULARIZATION PATH

SVM

Train
Test
10fold CV

s
e

t

a
R
n
o



i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

Multinomial Regression

5
0

.

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

5
0

.

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

s
e

t

a
R
n
o



i
t

a
c
i
f
i
s
s
a
c
s
M

i

l

1

10

100

1000

10000

1e02

1e+00

1e+02

1e+04

Figure 11: Misclassication rates for cancer classication by gene expression measurements. The
left panel shows the the training (lower green), cross-validation (middle black, with
standard errors) and test error (upper blue) curves for the entire SVM path. Although the
CV and test error curves appear to have quite different levels, the region of interesting
behavior is the same (with a curious dip at about l = 3000). Seeing the entire path
leaves no guesswork as to where the region of interest might be. The right panel shows
the same for the regularized multiple logistic regression model. Here we do not have an
exact path algorithm, so a grid of 15 values of l

is used (on a log scale).

 It is easy to see that initially a

, and hence all
points are in L. This is true whether or not n = n+, unlike the situation when an intercept is
present (Section 3.2).

i = 1i, since f (x) is close to zero for large l

 With f (x) = (cid:229) n

j=1 y jK(x, x j), the rst element of E is i = argmaxi | f (xi)|, with l 1 =

| f (xi)|. For l  [l 1, ), f (x) = f (x)/l

.

 The linear equations that govern the points in E are similar to (33):

d = (l `  l )1,

K
`

(46)

We now show that in the most regularized case, these no-intercept kernel models are actually
performing kernel density classication. Initially, for l > l 1, we classify to class +1 if f (x)/l > 0,

1411

l
l
HASTIE, ROSSET, TIBSHIRANI AND ZHU

else to class -1. But

f (x) = (cid:229)

K(x, x j)  (cid:229)

K(x, x j)

jI+

jI

= n  n+

n



1
n+

K(x, x j) 

jI+

+h+(x)  p h(x).

n
n



1
n

K(x, x j)!

jI

(47)

(48)

As l

is relaxed, the a

In other words, this is the estimated Bayes decision rule, with h+ the kernel density (Parzen window)
estimate for the + class, p + the sample prior, and likewise for h(x) and p . A similar observation
is made in Scholkopf and Smola (2001), for the model with intercept. So at this end of the regular-
ization scale, the kernel parameter g plays a crucial role, as it does in kernel density classication.
As g
increases, the behavior of the classier approaches that of the 1-nearest neighbor classier. For
very small g , or in fact a linear kernel, this amounts to closest centroid classication.

i(l ) will change, giving ultimately zero weight to points well within their
own class, and sharing the weights among points near the decision boundary. In the context of
nearest neighbor classication, this has the avor of editing, a way of thinning out the training set
retaining only those prototypes essential for classication (Ripley, 1996).

All these interpretations get blurred when the intercept b 0 is present in the model.
For the radial kernel, a constant term is included in span{K(x, xi)}n

1, so it is not strictly necessary
to include one in the model. However, it will get regularized (shrunk toward zero) along with all
the other coefcients, which is usually why these intercept terms are separated out and freed from
regularization. Adding a constant b2 to K(, ) will reduce the amount of shrinking on the intercept
(since the amount of shrinking of an eigenfunction of K is inversely proportional to its eigenvalue;
see Section 5). For the linear SVM, we can augment the xi vectors with a constant element b, and
then t the no-intercept model. The larger b, the closer the solution will be to that of the linear SVM
with intercept.

7. Discussion

Our work on the SVM path algorithm was inspired by earlier work on exact path algorithms in
other settings. Least Angle Regression (Efron et al., 2002) shows that the coefcient path for
the sequence of lasso coefcients (Tibshirani, 1996) is piecewise linear. The lasso solves the
following regularized linear regression problem,

min
b 0,b

(yi  b 0  xT

i

n(cid:229)

i=1

b )2 + l |b |,

(49)

j=1 |b
l ; the larger l

where |b | = (cid:229)
p
j| is the L1 norm of the coefcient vector. This L1 constraint delivers a sparse
solution vector b
l are zero, the remainder shrunk toward zero.
In fact, any model with an L1 constraint and a quadratic, piecewise quadratic, piecewise linear, or
mixed quadratic and linear loss function, will have piecewise linear coefcient paths, which can be
calculated exactly and efciently for all values of l
(Rosset and Zhu, 2003). These models include,
among others,

, the more elements of b

 A robust version of the lasso, using a Huberized loss function.

1412

(cid:229)
(cid:229)
(cid:181)
p
SVM REGULARIZATION PATH

 The L1 constrained support vector machine (Zhu et al., 2003).

The SVM model has a quadratic constraint and a piecewise linear (hinge) loss function. This
i are piecewise

leads to a piecewise linear path in the dual space, hence the Lagrange coefcients a
linear.

Other models that would share this property include
 The e -insensitive SVM regression model

 Quadratically regularized L1 regression, including exible models based on kernels or smooth-

ing splines.

Of course, quadratic criterion + quadratic constraints also lead to exact path solutions, as in the
classic case of ridge regression, since a closed form solution is obtained via the SVD. However,
these paths are nonlinear in the regularization parameter.

For general non-quadratic loss functions and L1 constraints, the solution paths are typically
piecewise non-linear. Logistic regression is a leading example. In this case, approximate path-
following algorithms are possible (Rosset, 2005).

The general techniques employed in this paper are known as parametric programming via active
sets in the convex optimization literature (Allgower and Georg, 1992). The closest we have seen to
our work in the literature employ similar techniques in incremental learning for SVMs (Fine and
Scheinberg, 2002; Cauwenberghs and Poggio, 2001; DeCoste and Wagstaff, 2000). These authors
do not, however, construct exact paths as we do, but rather focus on updating and downdating the
solutions as more (or less) data arises. Diehl and Cauwenberghs (2003) allow for updating the
parameters as well, but again do not construct entire solution paths. The work of Pontil and Verri
(1998) recently came to our notice, who also observed that the lagrange multipliers for the margin
vectors change in a piece-wise linear fashion, while the others remain constant.

The SvmPath has been implemented in the R computing environment (contributed library svmpath

at CRAN), and is available from the rst authors website.

Acknowledgments

The authors thank Jerome Friedman for helpful discussions, and Mee-Young Park for assisting
with some of the computations. They also thank two referees and the associate editor for helpful
comments. Trevor Hastie was partially supported by grant DMS-0204162 from the National Science
Foundation, and grant RO1-EB0011988-08 from the National Institutes of Health. Tibshirani was
partially supported by grant DMS-9971405 from the National Science Foundation and grant RO1-
EB0011988-08 from the National Institutes of Health.

