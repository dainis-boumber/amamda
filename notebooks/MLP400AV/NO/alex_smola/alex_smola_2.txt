Abstract. Wahbas classical representer theorem states that the solu-
tions of certain risk minimization problems involving an empirical risk
term and a quadratic regularizer can be written as expansions in terms
of the training examples. We generalize the theorem to a larger class
of regularizers and empirical risk terms, and give a self-contained proof
utilizing the feature space associated with a kernel. The result shows
that a wide range of problems have optimal solutions that live in the
nite dimensional span of the training examples mapped into feature
space, thus enabling us to carry out kernel algorithms independent of
the (potentially innite) dimensionality of the feature space.

1 Introduction

Following the development of support vector (SV) machines [23], positive denite
kernels have recently attracted considerable attention in the machine learning
community. It turns out that a number of results that have now become popular
were already known in the approximation theory community, as witnessed by the
work of Wahba [24]. The present work brings together tools from both areas. This
allows us to formulate a generalized version of a classical theorem from the latter
eld, and to give a new and simplied proof for it, using the geometrical view
of kernel function classes as corresponding to vectors in linear feature spaces.

The paper is organized as follows. In the present rst section, we review
some basic concepts. The two subsequent sections contain our main result, some
examples and a short discussion.

1.1 Positive Denite Kernels

The question under which conditions kernels correspond to dot products in linear
spaces has been brought to the attention of the machine learning community by
Vapnik and coworkers [1,5,23]. In functional analysis, the same problem has
been studied under the heading of Hilbert space representations of kernels. A
good monograph on the functional analytic theory of kernels is [4]. Most of the
material in the present introductory section is taken from that work. Readers
familiar with the basics of kernels can skip over the remainder of it.

D. Helmbold and B. Williamson (Eds.): COLT/EuroCOLT 2001, LNAI 2111, pp. 416426, 2001.
c(cid:1) Springer-Verlag Berlin Heidelberg 2001

A Generalized Representer Theorem

417

Suppose we are given empirical data

(x1, y1), . . . , (xm, ym)  X  R.

(1)

Here, the target values yi live in R, and the patterns xi are taken from a domain
X . The only thing we assume about X is that is a nonempty set. In order to
study the problem of learning, we need additional structure. In kernel methods,
this is provided by a similarity measure
k : X  X  R,

(x, x(cid:2)) (cid:5) k(x, x(cid:2)).

(2)

The function k is called a kernel [20]. The term stems from the rst use of this
type of function in the study of integral operators, where a function k giving rise
to an operator Tk via

k(x, x(cid:2))f(x(cid:2)) dx(cid:2)

(3)

(cid:1)

(Tkf)(x) =

X

m(cid:2)

m(cid:2)

is called the kernel of Tk. Note that we will state most results for the more
general case of complex-valued kernels;1 they specialize to the real-valued case
in a straightforward manner. Below, unless stated otherwise, indices i and j will
be understood to run over the training set, i.e. i, j = 1, . . . , m.
Denition 1 (Gram matrix). Given a kernel k and patterns x1, . . . , xm  X ,
the m  m matrix

K := (k(xi, xj))ij

(4)

is called the Gram matrix of k with respect to x1, . . . , xm.
Denition 2 (Positive denite matrix). An m m matrix K over the com-
plex numbers C satisfying

cicjKij  0

(5)

i=1

j=1

for all c1, . . . , cm  C is called positive denite.
Denition 3 (Positive denite kernel). Let X be a nonempty set. A function
k : X  X  C which for all m  N, x1, . . . , xm  X gives rise to a positive
denite Gram matrix is called a positive denite (pd) kernel.2
Real-valued kernels are contained in the above denition as a special case. How-
ever, it is not sucient to require that (5) hold for real coecients ci. If we want
to get away with real coecients only, we additionally have to require that the
1 We use the notation c to denote the complex conjugate of c.
2 One might argue that the term positive denite kernel
is slightly misleading. In
matrix theory, the term denite is sometimes used to denote the case where equality
in (5) only occurs if c1 =  = cm = 0. Simply using the term positive kernel, on
the other hand, could be confused with a kernel whose values are positive.

418

B. Scholkopf, R. Herbrich, and A.J. Smola

kernel be symmetric. The complex case is slightly more elegant; in that case, (5)
can be shown to imply symmetry, i.e. k(xi, xj) = k(xj, xi).

Positive denite kernels can be regarded as generalized dot products. Indeed,
any dot product is a pd kernel; however, linearity does not carry over from dot
products to general pd kernels. Another property of dot products, the Cauchy-
Schwarz inequality, does have a natural generalization: if k is a positive denite
kernel, and x1, x2  X , then

|k(x1, x2)|2  k(x1, x1)  k(x2, x2).

(6)

x (cid:5) k(, x).

 : X  CX ,

1.2 ... and Associated Feature Spaces
We dene a map from X into the space of functions mapping X into C, denoted
as CX , via [4]

(7)
Here, (x) = k(, x) denotes the function that assigns the value k(x(cid:2), x) to x(cid:2)  X .
Applying  to x amounts to representing it by its similarity to all other points
in the input domain X . This seems a very rich representation, but it turns out
that the kernel allows the computation of a dot product in that representation.
We shall now construct a dot product space containing the images of the
input patterns under . To this end, we rst need to endow it with the linear
structure of a vector space. This is done by forming linear combinations of the
form

f() =

ik(, xi),

g() =

jk(, x(cid:2)
j).

(8)

m(cid:1)(cid:2)

j=1

m(cid:2)

i=1

Here, m, m(cid:2)  N, 1, . . . , m, 1, . . . , m(cid:1)  C and x1, . . . , xm, x(cid:2)
are arbitrary. A dot product between f and g can be constructed as

1, . . . , x(cid:2)

m(cid:1)  X

m(cid:2)

m(cid:1)(cid:2)

i=1

j=1

(cid:10)f, g(cid:11) :=

ijk(xi, x(cid:2)
j).

(9)

j, xi) = k(xi, x(cid:2)

To see that this is well-dened, although it explicitly contains the expansion
coecients (which need not be unique), note that (cid:10)f, g(cid:11) =
j), using
(cid:3)
k(x(cid:2)
j). The latter, however, does not depend on the particular
expansion of f. Similarly, for g, note that (cid:10)f, g(cid:11) =
i ig(xi). This also shows
that (cid:10),(cid:11) is anti-linear in the rst argument and linear in the second one. It
is symmetric, since (cid:10)f, g(cid:11) = (cid:10)g, f(cid:11). Moreover, given functions f1, . . . , fn, and
coecients 1, . . . , n  C, we have

(cid:3)m(cid:1)
j=1 jf(x(cid:2)

n(cid:2)

n(cid:2)

(cid:4)
n(cid:2)

(cid:5)

n(cid:2)

ij(cid:10)fi, fj(cid:11) =

ifi,

jfj

 0,

(10)

i=1

j=1

i=1

j=1

hence (cid:10),(cid:11) is actually a pd kernel on our function space.

A Generalized Representer Theorem

419

For the last step in proving that it even is a dot product, one uses the
following interesting property of , which follows directly from the denition:
for all functions (8),

(11)
i.e., k is the representer of evaluation. In particular, (cid:10)k(, x), k(, x(cid:2))(cid:11) = k(x, x(cid:2)),
the reproducing kernel property [2,4,24], hence (cf. (7)) we indeed have k(x, x(cid:2)) =
(cid:10)(x), (x(cid:2))(cid:11). Moreover, by (11) and (6) we have

(cid:10)k(, x), f(cid:11) = f(x),

|f(x)|2 = |(cid:10)k(, x), f(cid:11)|2  k(x, x)  (cid:10)f, f(cid:11).

(12)
Therefore, (cid:10)f, f(cid:11) = 0 implies f = 0, which is the last property that was left to
prove in order to establish that (cid:10),(cid:11) is a dot product.

One can complete the space of functions (8) in the norm corresponding to
the dot product, i.e., add the limit points of sequences that are convergent in
that norm, and thus gets a Hilbert space Hk, usually called a reproducing kernel
Hilbert space (RKHS). The case of real-valued kernels is included in the above;
in that case, Hk can be chosen as a real Hilbert space.

2 The Representer Theorem

As a consequence of the last section, one of the crucial properties of kernels is
that even if the input domain X is only a set, we can nevertheless think of the
pair (X , k) as a (subset of a) Hilbert space. From a mathematical point of view,
this is attractive, since we can thus study various data structures (e.g., strings
over discrete alphabets [26,13,18]) in Hilbert spaces, whose theory is very well
developed. From a practical point of view, however, we now face the problem that
for many popular kernels, the Hilbert space is known to be innite-dimensional
[24]. When training a learning machine, however, we do not normally want to
solve an optimization problem in an innite-dimensional space.

This is where the main result of this paper will be useful, showing that
a large class of optimization problems with RKHS regularizers have solutions
that can be expressed as kernel expansions in terms of the training data. These
optimization problems are of great interest for learning theory, both since they
comprise a number of useful algorithms as special cases and since their statistical
performance can be analyzed with tools of learning theory (see [23,3], and, more
specically dealing with regularized risk functionals, [6]).
Theorem 1 (Nonparametric Representer Theorem). Suppose we are
given a nonempty set X , a positive denite real-valued kernel k on X  X , a
training sample (x1, y1), . . . , (xm, ym)  X  R, a strictly monotonically increas-
ing real-valued function g on [0,[, an arbitrary cost function c : (X  R2)m 
R  {}, and a class of functions
(cid:2)

(cid:8)

ik(, zi), i  R, zi  X ,(cid:17)f(cid:17) < 

.

(13)

(cid:6)
f  RX

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)f() =

F =

i=1

B. Scholkopf, R. Herbrich, and A.J. Smola

420
Here, (cid:17)  (cid:17) is the norm in the RKHS Hk associated with k, i.e. for any zi 

X , i  R (i  N), (cid:9)(cid:9)(cid:9)(cid:9)(cid:9) (cid:2)

i=1

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2

(cid:2)

(cid:2)

i=1

j=1

=

ijk(zi, zj).

ik(, zi)

Then any f  F minimizing the regularized risk functional

c ((x1, y1, f(x1)), . . . , (xm, ym, f(xm))) + g ((cid:17)f(cid:17))

admits a representation of the form

f() =

m(cid:2)

i=1

ik(, xi).

(14)

(15)

(16)

m(cid:2)

i=1

Let us give a few remarks before the proof. In its original form, with mean
squared loss

c((x1, y1, f(x1)), . . . , (xm, ym, f(xm))) =

1
m

(yi  f(xi))2,

(17)

or hard constraints on the outputs, and g((cid:17)f(cid:17)) = (cid:17)f(cid:17)2 ( > 0), the theorem
is due to [15]. Note that in our formulation, hard constraints on the solution
are included by the possibility of c taking the value . A generalization to
non-quadratic cost functions was stated by [7], cf. the discussion in [25] (note,
however, that [7] did not yet allow for coupling of losses at dierent points).
The present generalization to g((cid:17)f(cid:17)) is, to our knowledge, new. For a machine
learning point of view on the representer theorem and a variational proof, cf.
[12].

The signicance of the theorem is that it shows that a whole range of learning
algorithms have solutions that can be expressed as expansions in terms of the
training examples. Note that monotonicity of g is necessary to ensure that the
theorem holds. It does not ensure that the regularized risk functional (15) does
not have multiple local minima. For this, we would need to require convexity of
g and of the cost function c. If we discarded the strictness of the monotonicity
of g, it would no longer follow that each minimizer (there might be multiple
ones) of the regularized risk admits an expansion (16); however, it would still
follow that there is always another solution minimizing (15) that does admit the
expansion. In the SV community, (16) is called the SV expansion.
Proof. As we have assumed that k maps into R, we will use (cf. (7))

 : X  RX ,

x (cid:5) k(, x).

(18)
Since k is a reproducing kernel, evaluation of the function (x) on the point x(cid:2)
yields

((x))(x(cid:2)) = k(x(cid:2), x) = (cid:10)(x(cid:2)), (x)(cid:11)

(19)

A Generalized Representer Theorem

421

for all x, x(cid:2)  X . Here, (cid:10),(cid:11) denotes the dot product of Hk.

Given x1, . . . , xm, any f  F can be decomposed into a part that lives in the

span of the (xi) and a part which is orthogonal to it, i.e.

m(cid:2)

i=1

f =

i(xi) + v

(20)

(21)

(22)

(cid:4)
m(cid:2)

for some   Rm and v  F satisfying, for all j,
(cid:10)v, (xj)(cid:11) = 0.
(cid:5)

Using the latter and (19), application of f to an arbitrary training point xj
yields

m(cid:2)
(cid:3)m
independent of v. Consequently, the rst term of (15) is independent of v. As
i=1 i(xi), and g is strictly
for the second term, since v is orthogonal to
(cid:15)(cid:16)(cid:16)(cid:17)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
monotonic, we get
(cid:2)

i(cid:10)(xi), (xj)(cid:11),

i(xi) + v, (xj)

g((cid:17)f(cid:17)) = g

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:11)

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)2

i(xi) + v

= g

+ (cid:17)v(cid:17)2

=

i=1







f(xj) =

i=1

i(xi)

i

(cid:10)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:2)
(cid:10)(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:2)

i

i

 g

(cid:9)(cid:9)(cid:9)(cid:9)(cid:9)
(cid:11)

m(cid:2)

i(xi)

,

(23)

(cid:3)
with equality occuring if and only if v = 0. Setting v = 0 thus does not aect the
rst term of (15), while strictly reducing the second term  hence, any minimizer
must have v = 0. Consequently, any solution takes the form f =
i i(xi),
i.e., using (19),

f() =

ik(, xi).

(24)

The theorem is proven.

i=1

The extension to the case where we also include a parametric part is slightly
more technical but straightforward. We state the corresponding result without
proof:
Theorem 2 (Semiparametric Representer Theorem). Suppose that in ad-
dition to the assumptions of the previous theorem we are given a set of M
real-valued functions {p}M
p=1 on X , with the property that the m  M matrix
(p(xi))ip has rank M. Then any f := f + h, with f  F and h  span{p},
minimizing the regularized risk

(cid:21)

(cid:22)
(x1, y1, f(x1)), . . . , (xm, ym, f(xm))

c

+ g ((cid:17)f(cid:17))

(25)

422

B. Scholkopf, R. Herbrich, and A.J. Smola

admits a representation of the form

m(cid:2)

i=1

f() =

ik(xi,) +

M(cid:2)

p=1

pp(),

(26)

with unique coecients p  R for all p = 1, . . . , M.
Remark 1 (Biased regularization). A straightforward extension of the represen-
ter theorems can be obtained by including a term (cid:10)f0, f(cid:11) into (15) or (25),
respectively, where f0  Hk. In this case, if a solution to the minimization prob-
lem exists, it admits an expansion which diers from the above ones in that it
additionally contains a multiple of f0. To see this, decompose the vector v used
in the proof of Theorem 1 into a part orthogonal to f0 and the remainder.
2(cid:17)f(cid:17)2, this can be seen to correspond to an
2(cid:17)f  f0(cid:17)2. Thus, it is no longer the size

In the case where g((cid:17)f(cid:17)) = 1

eective overall regularizer of the form 1
of (cid:17)f(cid:17) that is penalized, but the dierence to f0.

Some explicit applications of Theorems 1 and 2 are given below.

Example 1 (SV regression). For SV regression with the insensitive loss [23] we
have

c

(xi, yi, f (xi))i=1,...,m

=

max (0,|f (xi)  yi|  )

(27)

(cid:22)

m(cid:2)

i=1

1


and g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2, where  > 0 and   0 are xed parameters which determine
the trade-o between regularization and t to the training set. In addition, a
single (M = 1) constant function 1(x) = b (b  R) is used as an oset that is
not regularized by the algorithm [25].

In [22], a semiparametric extension was proposed which shows how to deal

with the case M > 1 algorithmically. Theorem 2 applies in that case, too.
Example 2 (SV classication). Here, the targets satisfy yi  {1}, and we use

(cid:21)

(cid:21)

(cid:22)

m(cid:2)

i=1

=

1


c

(xi, yi, f (xi))i=1,...,m

max (0, 1  yif (xi)) ,

(28)

the regularizer g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2, and 1(x) = b. For   0, we recover the
hard margin SVM, i.e., the minimizer must correctly classify each training point
(xi, yi). Note that after training, the actual classier will be sgn (f() + b).
Example 3 (SVMs minimizing actual risk bounds). The reason why SVM algo-
rithms such as the ones discussed above use the regularizer g ((cid:17)f(cid:17)) = (cid:17)f(cid:17)2 are
practical ones. It is usually argued that theoretically, we should really be min-
imizing an upper bound on the expected test error, but in practice, we use a
quadratic regularizer, traded o with an empirical risk term via some constant.
One can show that in combination with certain loss functions (hard constraints,
linear loss, quadratic loss, or suitable combinations thereof), this regularizer

A Generalized Representer Theorem

423

leads to a convex quadratic programming problem [5,23]. In that case, the stan-
dard Kuhn-Tucker machinery of optimization theory [19] can be applied to derive
a so-called dual optimization problem, which consists of nding the expansion
coecients 1, . . . , m rather than the solution f in the RKHS.

From the point of view of learning theory, on the other hand, more general
functions g might be preferable, such as ones that are borrowed from uniform
convergence bounds. For instance, we could take inspiration from the basic pat-
tern recognition bounds of [23] and use, for some small  > 0, the (strictly
monotonic) function

(cid:15)(cid:16)(cid:16)(cid:17) R2(cid:17)f(cid:17)2

(cid:21)

log

(cid:22)
R2(cid:5)f(cid:5)2 + 1

2m

m

g((cid:17)f(cid:17)) :=

 log(/4)

.

(29)

More sophisticated functions based on other bounds on the test error are con-
ceivable, as well as variants for regression estimation (e.g., [3,6]).

Unlike Wahbas original version, the generalized representer theorem can help
dealing with these cases. It asserts that the solution still has an expansion in
terms of the training examples. It is thus sucient to minimize the risk bound
over expansions of the form (16) (or (26)). Substituting (16) into (15), we get an
(m-dimensional) problem in coecient representation (without having to appeal
to methods of optimization theory)

+ g

ijk(xi, xj)

i=1

j=1

If g and c are convex, then so will be the dual, thus we can solve it employing
methods of convex optimization (such as interior point approaches often used
even for standard SVMs). If the dual is non-convex, we will typically only be
able to nd local minima.

Independent of the convexity issue, the result lends itself well to gradient-
based on-line algorithms for minimizing RKHS-based risk functionals [10,9,17,
11,8,16]: for the computation of gradients, we only need the objective function to
be dierentiable; convexity is not required. Such algorithms can thus be adapted
to deal with more general regularizers.

Example 4 (Bayesian MAP estimates). The well-known correspondence to
Bayesian methods is established by identifying (15) with the negative log poste-
rior [14,12]. In this case, exp(c((xi, yi, f(xi))i=1,...,m)) is the likelihood of the
data, while exp(g((cid:17)f(cid:17))) is the prior over the set of functions. The well-known
Gaussian process prior (e.g. [24,27]), with covariance function k, is obtained
by using g((cid:17)f(cid:17)) = (cid:17)f(cid:17)2 (here,  > 0, and, as above, (cid:17)  (cid:17) is the norm of
the RKHS associated with k). A Laplacian prior would be obtained by using

min

1,...,mR

c

ik(x1, xi)), . . . , (xm, ym,

ik(xm, xi)))

(cid:10)

(x1, y1,

m(cid:2)
(cid:15)(cid:16)(cid:16)(cid:17) m(cid:2)
m(cid:2)

i=1




m(cid:2)

(cid:11)


 .

i=1

(30)

B. Scholkopf, R. Herbrich, and A.J. Smola

424
g((cid:17)f(cid:17)) = (cid:17)f(cid:17). In all cases, the minimizer of (15) corresponds to a function
with maximal a posteriori probability (MAP).

Example 5 (Kernel PCA). PCA in a kernel feature space can be shown to cor-
respond to the case of

c((xi, yi, f(xi))i=1,...,m) =

f(xj)

= 1

(31)


m(cid:3)
 0
 otherwise

if 1
m

i=1

(cid:10)
f(xi)  1

m

(cid:11)2

m(cid:3)

j=1

with g an arbitrary strictly monotonically increasing function [21]. The con-
straint ensures that we are only considering linear feature extraction functionals
that produce outputs of unit empirical variance. Note that in this case of unsu-
pervised learning, there are no labels yi to consider.

3 Conclusion

We have shown that for a large class of algorithms minimizing a sum of an em-
pirical risk term and a regularization term in a reproducing kernel Hilbert space,
the optimal solutions can be written as kernel expansions in terms of training ex-
amples. This has been known for specic algorithms; e.g., for the SV algorithm,
where it is a direct consequence of the structure of the optimization problem,
but not in more complex cases, such as the direct minimization of some bounds
on the test error (cf. Example 3). The representer theorem puts these individ-
ual ndings into a wider perspective, and we hope that the reader will nd the
present generalization useful by either gaining some insight, or by taking it as
a practical guideline for designing novel kernel algorithms: as long as the objec-
tive function can be cast into the form considered in the generalized representer
theorem, one can recklessly carry out algorithms in innite dimensional spaces,
since the solution will always live in a specic subspace whose dimensionality
equals at most the number of training examples.

Acknowledgements. Thanks to Bob Williamson, Grace Wahba, Jonathan
Baxter, Peter Bartlett, and Nello Cristianini for useful comments. This work
was supported by the Australian Research Council. AS was supported by DFG
grant SM 62/1-1.

