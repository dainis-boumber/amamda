Abstract

The  Support  Vector  (SV)  method  was  recently  proposed  for  es(cid:173)
timating  regressions,  constructing  multidimensional  splines,  and
solving linear operator equations  [Vapnik,  1995].  In  this  presenta(cid:173)
tion we report results of applying the SV method to these problems.

1

Introduction

The Support Vector method is a universal tool for solving multidimensional function
estimation problems.  Initially it was designed to solve pattern recognition problems,
where  in  order  to  find  a  decision  rule  with  good  generalization  ability  one  selects
some (small) subset of the training data, called the Support Vectors (SVs).  Optimal
separation of the SV s  is  equivalent to optimal separation  the entire  data.

This  led  to  a  new  method  of  representing  decision  functions  where  the  decision
functions  are  a  linear expansion  on  a  basis  whose elements  are  nonlinear functions
parameterized  by  the  SVs  (we  need  one  SV  for  each  element  of the  basis).  This
type of function representation is especially useful for high dimensional input space:
the number of free  parameters in  this representation  is  equal to the  number of SVs
but does  not  depend on the  dimensionality of the space.

Later  the  SV  method  was  extended  to  real-valued  functions.  This  allows  us  to
expand high-dimensional functions  using  a small basis  constructed  from  SVs.  This

smola@prosun.first.gmd.de

282

v.  Vapnik,  S.  E.  Golowich and A.  Smola

novel  type  of function  representation  opens  new  opportunities  for  solving  various
problems of function  approximation and estimation.

In  this paper  we  demonstrate that using  the SV technique one  can  solve  problems
that in  classical techniques  would  require estimating a  large number of free  param(cid:173)
eters.  In particular we  construct one and two dimensional splines with an arbitrary
number  of grid  points.  Using  linear  splines  we  approximate non-linear  functions .
We show that by  reducing requirements on the accuracy  of approximation, one  de(cid:173)
creases  the number of SVs which  leads to data compression.  We also show  that the
SV  technique  is  a useful  tool for  regression  estimation.  Lastly we  demonstrate that
using the SV function representation for  solving inverse ill-posed problems provides
an  additional opportunity for  regularization.

2  SV method for  estimation of real functions

Let  x  E  Rn  and Y E Rl.  Consider  the following set  of real functions:  a  vector  x  is
mapped into some a  priori chosen  Hilbert space,  where we  define functions that are
linear in  their parameters

Y = I(x,w) = L  Wi<Pi(X),  W = (WI, ... ,WN, ... ) E n

00

i=1

(1)

In [Vapnik,  1995] the following method for estimating functions  in the set (1)  based
on training data (Xl, Yd, .. . , (Xl, Yl)  was suggested:  find the function  that minimizes
the following functional:

1

l

R(w) =  L
i=1

IYi  - I(Xi, w)lt:  + I(w, w),

where

Iy - I(x, w)lt:  =

{  0

Iy - I(x, w)l-   otherwise,

if  Iy - I(x, w)1  < ,

(2)

(3)

(w, w)  is  the  inner  product  of two  vectors,  and  I  is  some constant .  It was  shown
that the function  minimizing this functional has a  form:

I(x, a, a*)  =  L(a; - ai)(<I>(xi), <I>(x)) + b

l

(4)

;=1

where  ai, ai  2::  0  with  aiai  =  0  and  (<I>(Xi), <I>(x  is  the  inner  product  of two
elements of Hilbert space.

To find  the coefficients a;  and ai  one has to solve the following quadratic optimiza(cid:173)
tion problem:  maximize the functional

i l l

W(a*, a) = - L(a; +ai)+ Ly(a; -ai)-~ L
i,j=1

i=1

;=1

(a; -ai)(aj -aj )(<I>(Xi), <I>(Xj)),

subject  to constraints

l
L(ai-ai)=O,  O~ai,a;~C,  i=l, ... ,f.
i=1

(5)

(6)

SV Method for Function Approximation and Regression Estimation

283

The important feature  of the solution (4)  of this optimization problem is  that only
some of the coefficients (a; - ai) differ from zero.  The corresponding vectors  Xi  are
called Support  Vectors  (SVs).  Therefore  (4)  describes  an expansion on  SVs.

It was  shown  in  [Vapnik,  1995]  that  to  evaluate  the  inner  products  (<1>(Xi)' <1>(x))
both  in  expansion  (4)  and  in  the  objective  function  (5)  one  can  use  the  general
form  of the  inner  product  in  Hilbert  space.  According  to  Hilbert  space  theory,  to
guarantee that  a  symmetric function  K ( u, v)  has an expansion

K(u, v) = L ak1fJk(u)tPk(V)

00

k=l

with  positive coefficients  ak  > 0,  i.e.  to guarantee that K (u, v)  is  an inner product
in some feature  space  <1>,  it  is  necessary  and sufficient  that  the conditions

J K(u, v)g(u)g(v) du dv  > 0

(7)

(8)

be  valid for  any  non-zero function  9  on  the Hilbert space  (Mercer's  theorem).

Therefore,  in  the SV  method, one  can replace  (4)  with

I(x, a, a*) = L(a; - ai)K(x, Xi) + b

l

i=l

where  the inner product  (<1>( Xi),  <1>( x
coefficients  ai  and ai  one  has  to maximize the function

is  defined  through a  kernel  K (Xi,  x).  To find

W(a*, a) = -[ L(a; +ai)+ Ly(a; -ai)- ~ L

(a; -ai)(aj -aj)K(xi, Xj)  (9)

l

l

l

i=l

i=l

i,j=l

subject  to constraints  (6).

3  Constructing kernels for  inner products

To define  a set  of approximating functions  one has  to define  a  kernel  K (Xi, X)  that
generates  the  inner  product  in  some  feature  space  and  solve  the  corresponding
quadratic optimization problem.

3.1  Kernels generating splines

We  start with the spline functions.  According to their  definition, splines  are piece(cid:173)
wise  polynomial functions,  which  we  will  consider on the set  [0,1].  Splines of order
n  have the following representation

n

N

In(x) = L  arxr + L  Wj(x - t~r~.

(10)

r=O

~=l

where  (x - t)+  =  max{(x - t),  O},  tl, ... , tN  E  [0,1]  are  the  nodes,  and  ar , Wj  are
real  values.  One  can  consider  the  spline  function  (10)  as  a  linear function  in  the
n + N  + 1 dimensional feature  space spanned by

1, x, ... , xn, (x - tdf., ... , (x - tN)f..

284

V.  Vapnik,  S.  E.  Golowich and A.  Smola

Therefore  the  inner product  that generates splines  of order  n  in  one  dimension is

n

N

IXi,Xj) = Lx;xj + L(Xi -t3)~(Xj -t3)~'

(11)

r=O

3=1

Two dimensional splines  are linear functions  in the (N + n + 1)2  dimensional space

1, x, ... , xn, y, ... , yn, ... , (x - td~(y - t~)~, ... , (x - tN )~(y - tN )~.

(12)
Let us  denote by Ui  = (Xi ,Yi),  Uj  = (Xi,Yj)  two two-dimensional vectors.  Then the
generating  kernel  for  two  dimensional spline functions  of order  n  is

It is  easy  to  check  that  the  generating kernel  for  the  m-dimensional splines  is  the
product  of m  one-dimensional generating  kernels.

In  applications  of the  SV  method  the  number  of nodes  does  not  play  an  impor(cid:173)
tant  role.  Therefore,  we  introduce  splines  of order  d  with  an  infinite  number  of
nodes  S~oo).  To  do  this  in  the  R1  case,  we  map  any  real  value  Xi  to  the  element
1, Xi,  ... , xi, (Xi  - t)+  of the  Hilbert space.  The inner  product  becomes

IXi,Xj) =  Lx;xj+ 1 (Xi-t)~(Xj -t)~dt

1

n

r=O

0

(13)

For  linear splines  S~oo) we  therefore  have  the following generating kernel:

In  many applications expansions  in  Bn-splines  [Unser  &  Aldroubi,  1992]  are  used,
where

Bn(x) = E (-~y (  n + 1  )  (X + n + 1 _ r)n .

n.

r=O

r

2

+

One  may  use  Bn-splines  to  perform  a  construction  similar  to  the  above,  yielding
the kernel

3.2  Kernels generating Fourier expansions
Lastly,  Fourier  expansion  can  be  considered  as  a  hyperplane  in  following  2N + 1
dimensional feature space

V2 ' cos x, sln x, ... , cos  x, sln  x.
1

N '  N

.

The inner product  in  this  space  is  defined  by  the Dirichlet formula:

SV Method for Function Approximation and Regression Estimation

285

4  Function  estimation and data compression

In this section  we  approximate functions on the basis  of observations at f  points

(16)

We  demonstrate that  to  construct  an  approximation within  an  accuracy  of c  at
the data points, one  can  use  only the subsequence of the data containing the SVs.

We  consider  approximating the one  and two  dimensional functions

smlxl
f(x)  =  smclxl  =  -I-xl-

.

(17)

on the  basis of a  sequence  of measurements  (without  noise)  on  the  uniform lattice
(100 for  the one  dimensional case  and 2,500 for the two-dimensional case).
For different  c  we  approximate this function  by  linear splines from si 00) .

Figure 1:  Approximations with different  levels of accuracy require different numbers
ofSV: 31  SV for  c = 0.02  (left)  and  9 SV for  c =  0.1.  Large  dots indicate SVs .

..,(cid:173)
.. .. -..

  +;.

os

D

....
.. . . - -
+  :  ."'.4\ ,  ~.
. .   ....  " ...... .


,....  +:  :.

  - . :   + 



.+ .+

....  ,+ ::, 

Figure  2:  Approximation  of  f( x, y)
splines  with  accuracy  c =  0.01  (left)  required  157 SV  (right)

sinc vi x 2 + y2  by  two  dimensional  linear

..





~  

0



  


Figure  3:  sincx  function  corrupted  by  different  levels  of noise  (7  = 0.2  left,  0.5
right)  and its regression.  Black  dots  indicate SV, circles  non-SV  data.

o
o

of>

.0

~ .



  0

286

V.  Vapnik,  S.  E.  Golowich and A. Smola

5  Solution  of the linear  operator equations

In  this  section  we  consider  the  problem  of solving  linear  equations  in  the  set  of
functions defined by SVs.  Consider the problem of solving a linear operator equation

Af(t) = F(x),

f(t)  E 2,  F(x)  E  W,

(18)

where  we  are given measurements of the right hand side

(19)
Consider  the  set  of functions  f(t, w)  E  2  linear  in  some  feature  space  {<I>(t)  =
(>o(t),  ... , >N(t), ... )}:

(Xl, FI ), ... , (Xl,  Fl).

00

f(t, w) =  L  wr>r(t)  =  (W, <I>(t .

(20)

r=O

The operator A  maps this set  of functions  into

F(x, w) =  Af(t, w)  =  L  wrA>r(t)  =  L  wrtPr(x)  =  (W, w(x

(21)

00

00

r=O

r=O

where  tPr(x)  = A>r(t),  w(x) = (tPl(X), ... , tPN(X), ... ).  Let  us  define  the  generating
kernel  in  image space

00

K(Xi, Xj)  =  L

tPr(Xi)tPr(Xj)  = (W(Xi)' W(Xj

(22)

and the corresponding  cross-kernel  function

r=O

00

K,(Xi' t) =  L

tPr(xd>r(t)  =  (W(Xi), <I>(t.

(23)

r=O

The problem of solving (18)  in the set  of functions  f(t, w)  E  2  (finding  the vector
W)  is  equivalent to the problem of regression estimation (21)  using data (19).

To  estimate  the  regression  on  the  basis  of the  kernel  K(Xi, Xj)  one  can  use  the
methods  described  in  Section  1.  The  obtained  parameters  (a;  - ai,  i  =  1, ... f)
define  the approximation to the solution of equation  (18)  based on  data (19):

l

f(t, a) = L(ai - ai)K,(xi, t).

i=l

We have applied this  method to solution of the Radon equation

j aCm)

-a(m)

f( m cos tt + u sin tt,  m sin tt - u cos tt )du =  p( m, tt),

-1 ~ m  ~ 1,  0 < tt < 11",

(24)
using  noisy  observations  (ml' ttl, pd, ... , (ml' ttl, Pi)'  where Pi  = p( mi, tti) + ~i  and
{ed  are independent  with Eei  = 0,  Eel  < 00.

a(m) =  -/1 - m 2

sv Method for Function Approximation and Regression Estimation

287

For  two-dimensional linear splines  S~ 00)  we  obtained analytical expressions  for  the
'kernel (22)  and cross-kernel  (23).  We  have  used  these  kernels  for  solving the corre(cid:173)
sponding regression problem and reconstructing images based on data that is similar
to  what  one  might get  from  a  Positron  Emission  Tomography scan  [Shepp,  Vardi
&  Kaufman,  1985].

A  remarkable feature  of this  solution is  that  it  aVOIds  a  pixel  representation  of the
function  which  would  require  the estimation of 10,000  to  60,000  parameters.  The
spline  approximation shown  here  required only  172  SVs.

Figure 4:  Original image (dashed line) and its reconstruction  (solid line) from 2,048
observations (left).  172 SVs  (support lines)  were  used  in the reconstruction  (right).

6  Conclusion

In  this  article  we  present  a  new  method  of function  estimation  that  is  especially
useful  for  solving  multi-dimensional problems.  The  complexity  of the  solution  of
the  function  estimation problem using  the  SV representation  depends  on the  com(cid:173)
plexity of the desired  solution  (i.e.  on the  required  number of SVs for  a  reasonable
approximation  of the  desired  function)  rather  than  on  the  dimensionality of the
space.  Using the SV  method one  can solve various problems of function estimation
both in statistics  and  in  applied  mathematics.

Acknowledgments

We would like to thank Chris Burges (Lucent Technologies) and Bernhard Scholkopf
(MPIK Tiibingen)  for  help  with  the  code  and useful  discussions.

This  work  was  supported  in  part  by  NSF  grant  PHY  95-12729  (Steven  Golowich)
and by ARPA grant N00014-94-C-0186 and the German National Scholarship Foun(cid:173)
dation (Alex Smola).

