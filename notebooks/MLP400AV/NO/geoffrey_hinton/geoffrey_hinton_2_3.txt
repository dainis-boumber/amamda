Abstract

Deep and recurrent neural networks (DNNs
and RNNs respectively) are powerful mod-
els that were considered to be almost impos-
sible to train using stochastic gradient de-
scent with momentum.
In this paper, we
show that when stochastic gradient descent
with momentum uses a well-designed random
initialization and a particular type of slowly
increasing schedule for the momentum pa-
rameter, it can train both DNNs and RNNs
(on datasets with long-term dependencies) to
levels of performance that were previously
achievable only with Hessian-Free optimiza-
tion. We nd that both the initialization
and the momentum are crucial since poorly
initialized networks cannot be trained with
momentum and well-initialized networks per-
form markedly worse when the momentum is
absent or poorly tuned.
Our success training these models suggests
that previous attempts to train deep and re-
current neural networks from random initial-
izations have likely failed due to poor ini-
tialization schemes. Furthermore, carefully
tuned momentum methods suce for dealing
with the curvature issues in deep and recur-
rent network training objectives without the
need for sophisticated second-order methods.

1. Introduction

Deep and recurrent neural networks (DNNs and
RNNs, respectively) are powerful models that achieve
high performance on dicult pattern recognition prob-
lems in vision, and speech (Krizhevsky et al., 2012;
Hinton et al., 2012; Dahl et al., 2012; Graves, 2012).

Although their representational power is appealing,
the diculty of training DNNs has prevented their

Proceedings of the 30 th International Conference on Ma-
chine Learning, Atlanta, Georgia, USA, 2013.
JMLR:
W&CP volume 28. Copyright 2013 by the author(s).

ilyasu@google.com
jmartens@cs.toronto.edu
gdahl@cs.toronto.edu

hinton@cs.toronto.edu

widepread use until fairly recently. DNNs became
the subject of renewed attention following the work
of Hinton et al. (2006) who introduced the idea of
greedy layerwise pre-training. This approach has since
branched into a family of methods (Bengio et al.,
2007), all of which train the layers of the DNN in a
sequence using an auxiliary objective and then ne-
tune the entire network with standard optimization
methods such as stochastic gradient descent (SGD).
More recently, Martens (2010) attracted considerable
attention by showing that a type of truncated-Newton
method called Hessian-free Optimization (HF) is capa-
ble of training DNNs from certain random initializa-
tions without the use of pre-training, and can achieve
lower errors for the various auto-encoding tasks con-
sidered by Hinton & Salakhutdinov (2006).
Recurrent neural networks (RNNs), the temporal ana-
logue of DNNs, are highly expressive sequence mod-
els that can model complex sequence relationships.
They can be viewed as very deep neural networks
that have a layer for each time-step with parame-
ter sharing across the layers and, for this reason, they
are considered to be even harder to train than DNNs.
Recently, Martens & Sutskever (2011) showed that
the HF method of Martens (2010) could eectively
train RNNs on articial problems that exhibit very
long-range dependencies (Hochreiter & Schmidhuber,
1997). Without resorting to special types of memory
units, these problems were considered to be impossi-
bly dicult for rst-order optimization methods due
to the well known vanishing gradient problem (Bengio
et al., 1994). Sutskever et al. (2011) and later Mikolov
et al. (2012) then applied HF to train RNNs to per-
form character-level language modeling and achieved
excellent results.

Recently, several results have appeared to challenge
the commonly held belief that simpler rst-order
methods are incapable of learning deep models from
random initializations. The work of Glorot & Ben-
gio (2010), Mohamed et al. (2012), and Krizhevsky
et al. (2012) reported little diculty training neural
networks with depths up to 8 from certain well-chosen

1Work was done while the author was at the University

of Toronto.

On the importance of initialization and momentum in deep learning

random initializations. Notably, Chapelle & Erhan
(2011) used the random initialization of Glorot & Ben-
gio (2010) and SGD to train the 11-layer autoencoder
of Hinton & Salakhutdinov (2006), and were able to
surpass the results reported by Hinton & Salakhutdi-
nov (2006). While these results still fall short of those
reported in Martens (2010) for the same tasks, they
indicate that learning deep networks is not nearly as
hard as was previously believed.

The rst contribution of this paper is a much more
thorough investigation of the diculty of training deep
and temporal networks than has been previously done.
In particular, we study the eectiveness of SGD when
combined with well-chosen initialization schemes and
various forms of momentum-based acceleration. We
show that while a denite performance gap seems to
exist between plain SGD and HF on certain deep and
temporal learning problems, this gap can be elimi-
nated or nearly eliminated (depending on the prob-
lem) by careful use of classical momentum methods
or Nesterovs accelerated gradient. In particular, we
show how certain carefully designed schedules for the
constant of momentum , which are inspired by var-
ious theoretical convergence-rate theorems (Nesterov,
1983; 2003), produce results that even surpass those re-
ported by Martens (2010) on certain deep-autencoder
training tasks. For the long-term dependency RNN
tasks examined in Martens & Sutskever (2011), which
rst appeared in Hochreiter & Schmidhuber (1997),
we obtain results that fall just short of those reported
in that work, where a considerably more complex ap-
proach was used.

Our results are particularly surprising given that mo-
mentum and its use within neural network optimiza-
tion has been studied extensively before, such as in the
work of Orr (1996), and it was never found to have such
an important role in deep learning. One explanation is
that previous theoretical analyses and practical bench-
marking focused on local convergence in the stochastic
setting, which is more of an estimation problem than
an optimization one (Bottou & LeCun, 2004). In deep
learning problems this nal phase of learning is not
nearly as long or important as the initial transient
phase (Darken & Moody, 1993), where a better ar-
gument can be made for the benecial eects of mo-
mentum.

In addition to the inappropriate focus on purely local
convergence rates, we believe that the use of poorly de-
signed standard random initializations, such as those
in Hinton & Salakhutdinov (2006), and suboptimal
meta-parameter schedules (for the momentum con-
stant in particular) has hampered the discovery of the
true eectiveness of rst-order momentum methods in
deep learning. We carefully avoid both of these pit-
falls in our experiments and provide a simple to under-
stand and easy to use framework for deep learning that

is surprisingly eective and can be naturally combined
with techniques such as those in Raiko et al. (2011).

We will also discuss the links between classical mo-
mentum and Nesterovs accelerated gradient method
(which has been the subject of much recent study in
convex optimization theory), arguing that the latter
can be viewed as a simple modication of the former
which increases stability, and can sometimes provide a
distinct improvement in performance we demonstrated
in our experiments. We perform a theoretical analysis
which makes clear the precise dierence in local be-
havior of these two algorithms. Additionally, we show
how HF employs what can be viewed as a type of mo-
mentum through its use of special initializations to
conjugate gradient that are computed from the up-
date at the previous time-step. We use this property
to develop a more momentum-like version of HF which
combines some of the advantages of both methods to
further improve on the results of Martens (2010).

2. Momentum and Nesterovs

Accelerated Gradient

The momentum method (Polyak, 1964), which we refer
to as classical momentum (CM), is a technique for ac-
celerating gradient descent that accumulates a velocity
vector in directions of persistent reduction in the ob-
jective across iterations. Given an objective function
f () to be minimized, classical momentum is given by:

vt+1 = vt  "rf (t)
t+1 = t + vt+1

(1)
(2)

where "> 0 is the learning rate,  2 [0, 1] is the mo-
mentum coecient, and rf (t) is the gradient at t.
Since directions d of low-curvature have, by deni-
tion, slower local change in their rate of reduction (i.e.,
d>rf ), they will tend to persist across iterations and
be amplied by CM. Second-order methods also am-
plify steps in low-curvature directions, but instead of
accumulating changes they reweight the update along
each eigen-direction of the curvature matrix by the in-
verse of the associated curvature. And just as second-
order methods enjoy improved local convergence rates,
Polyak (1964) showed that CM can considerably accel-
erate convergence to a local minimum, requiring pR-
times fewer iterations than steepest descent to reach
the same level of accuracy, where R is the condition
number of the curvature at the minimum and  is set

to (pR  1)/(pR + 1).

Nesterovs Accelerated Gradient (abbrv. NAG; Nes-
terov, 1983) has been the subject of much recent at-
tention by the convex optimization community (e.g.,
Cotter et al., 2011; Lan, 2010). Like momentum,
NAG is a rst-order optimization method with better
convergence rate guarantee than gradient descent in

On the importance of initialization and momentum in deep learning

certain situations.
In particular, for general smooth
(non-strongly) convex functions and a deterministic
gradient, NAG achieves a global convergence rate of
O(1/T 2) (versus the O(1/T ) of gradient descent), with
constant proportional to the Lipschitz coecient of the
derivative and the squared Euclidean distance to the
solution. While NAG is not typically thought of as a
type of momentum, it indeed turns out to be closely re-
lated to classical momentum, diering only in the pre-
cise update of the velocity vector v, the signicance of
which we will discuss in the next sub-section. Speci-
cally, as shown in the appendix, the NAG update may
be rewritten as:

vt+1 = vt  "rf (t + vt)
t+1 = t + vt+1

(3)
(4)

While the classical convergence theories for both meth-
ods rely on noiseless gradient estimates (i.e., not
stochastic), with some care in practice they are both
applicable to the stochastic setting. However, the the-
ory predicts that any advantages in terms of asymp-
totic local rate of convergence will be lost (Orr, 1996;
Wiegerinck et al., 1999), a result also conrmed in ex-
periments (LeCun et al., 1998). For these reasons,
interest in momentum methods diminished after they
had received substantial attention in the 90s. And be-
cause of this apparent incompatibility with stochastic
optimization, some authors even discourage using mo-
mentum or downplay its potential advantages (LeCun
et al., 1998).

However, while local convergence is all that matters
in terms of asymptotic convergence rates (and on cer-
tain very simple/shallow neural network optimization
problems it may even dominate the total
learning
time), in practice, the transient phase of convergence
(Darken & Moody, 1993), which occurs before ne lo-
cal convergence sets in, seems to matter a lot more
for optimizing deep neural networks. In this transient
phase of learning, directions of reduction in the ob-
jective tend to persist across many successive gradient
estimates and are not completely swamped by noise.

Although the transient phase of learning is most no-
ticeable in training deep learning models, it is still no-
ticeable in convex objectives. The convergence rate
of stochastic gradient descent on smooth convex func-
tions is given by O(L/T + /pT ), where  is the
variance in the gradient estimate and L is the Lip-
shits coecient of rf .
In contrast, the convergence
rate of an accelerated gradient method of Lan (2010)
(which is related to but dierent from NAG, in that
it combines Nesterov style momentum with dual aver-
aging) is O(L/T 2 + /pT ). Thus, for convex objec-
tives, momentum-based methods will outperform SGD
in the early or transient stages of the optimization
where L/T is the dominant term. However, the two
methods will be equally eective during the nal stages

Figure 1.
terov Accelerated Gradient

(Top) Classical Momentum (Bottom) Nes-

of the optimization where /pT is the dominant term
(i.e., when the optimization problem resembles an es-
timation one).

2.1. The Relationship between CM and NAG

From Eqs. 1-4 we see that both CM and NAG compute
the new velocity by applying a gradient-based correc-
tion to the previous velocity vector (which is decayed),
and then add the velocity to t. But while CM com-
putes the gradient update from the current position
t, NAG rst performs a partial update to t, comput-
ing t + vt, which is similar to t+1, but missing the
as yet unknown correction. This benign-looking dif-
ference seems to allow NAG to change v in a quicker
and more responsive way, letting it behave more sta-
bly than CM in many situations, especially for higher
values of .

Indeed, consider the situation where the addition of
vt results in an immediate undesirable increase in
the objective f . The gradient correction to the ve-
locity vt is computed at position t + vt and if vt
is indeed a poor update, then rf (t + vt) will point
back towards t more strongly than rf (t) does, thus
providing a larger and more timely correction to vt
than CM. See g. 1 for a diagram which illustrates
this phenomenon geometrically. While each iteration
of NAG may only be slightly more eective than CM
at correcting a large and inappropriate velocity, this
dierence in eectiveness may compound as the al-
gorithms iterate. To demonstrate this compounding,
we applied both NAG and CM to a two-dimensional
oblong quadratic objective, both with the same mo-
mentum and learning rate constants (see g. 2 in the
appendix). While the optimization path taken by CM
exhibits large oscillations along the high-curvature ver-
tical direction, NAG is able to avoid these oscillations
almost entirely, conrming the intuition that it is much
more eective than CM at decelerating over the course
of multiple iterations, thus making NAG more tolerant
of large values of  compared to CM.

In order to make these intuitions more rigorous and

On the importance of initialization and momentum in deep learning

help quantify precisely the way in which CM and
NAG dier, we analyzed the behavior of each method
when applied to a positive denite quadratic objective
q(x) = x>Ax/2 + b>x. We can think of CM and NAG
as operating independently over the dierent eigendi-
rections of A. NAG operates along any one of these
directions equivalently to CM, except with an eective
value of  that is given by (1  "), where  is the
associated eigenvalue/curvature.

q(x) = q(U>y) = y>UU>DU U>y/2 + b>U>y =
p as p(y) =Pn

The rst step of this argument is to reparameterize
q(x) in terms of the coecients of x under the basis
of eigenvectors of A. Note that since A = U>DU for
a diagonal D and orthonormal U (as A is symmetric),
we can reparameterize q(x) by the matrix transform
U and optimize y = U x using the objective p(y) 
y>Dy/2 + c>y, where c = U b. We can further rewrite
i=1[p]i([y]i), where [p]i(t) = it2/2+[c]it
and i > 0 are the diagonal entries of D (and thus
the eigenvalues of A) and correspond to the curva-
ture along the associated eigenvector directions. As
shown in the appendix (Proposition 6.1), both CM
and NAG, being rst-order methods, are invariant
to these kinds of reparameterizations by orthonormal
transformations such as U . Thus when analyzing the
behavior of either algorithm applied to q(x), we can in-
stead apply them to p(y), and transform the resulting
sequence of iterates back to the default parameteriza-
tion (via multiplication by U1 = U>).

Theorem 2.1. Let p(y) = Pn

i=1[p]i([y]i) such that
[p]i(t) = it2/2 + cit. Let " be arbitrary and xed.
Denote by CM x(, p, y, v) and CM v(, p, y, v) the pa-
rameter vector and the velocity vector respectively, ob-
tained by applying one step of CM (i.e., Eq. 1 and then
Eq. 2) to the function p at point y, with velocity v,
momentum coecient , and learning rate ". Dene
N AGx and N AGv analogously. Then the following
holds for z 2{ x, v}:

CM z(, [p]1, [y]1, [v]1)

...

CM z(, [p]n, [y]n, [v]n)
CM z((1  1"), [p]1, [y]1, [v]1)

375

CM z(, p, y, v) =264
N AGz(, p, y, v) =264

...

CM z((1  n"), [p]n, [y]n, [v]n)

Proof. See the appendix.

The theorem has several implications. First, CM and
NAG become equivalent when " is small (when "  1
for every eigenvalue  of A), so NAG and CM are
distinct only when " is reasonably large. When " is
relatively large, NAG uses smaller eective momentum
for the high-curvature eigen-directions, which prevents

oscillations (or divergence) and thus allows the use of
a larger  than is possible with CM for a given ".

3. Deep Autoencoders

The aim of our experiments is three-fold. First, to
investigate the attainable performance of stochastic
momentum methods on deep autoencoders starting
from well-designed random initializations; second, to
explore the importance and eect of the schedule for
the momentum parameter  assuming an optimal xed
choice of the learning rate "; and third, to compare the
performance of NAG versus CM.

For our experiments with feed-forward nets, we fo-
cused on training the three deep autoencoder prob-
lems described in Hinton & Salakhutdinov (2006) (see
sec. A.2 for details). The task of the neural net-
work autoencoder is to reconstruct its own input sub-
ject to the constraint that one of its hidden layers is
of low-dimension. This bottleneck layer acts as a
low-dimensional code for the original input, similar to
other dimensionality reduction techniques like Princi-
ple Component Analysis (PCA). These autoencoders
are some of the deepest neural networks with pub-
lished results, ranging between 7 and 11 layers, and
have become a standard benchmarking problem (e.g.,
Martens, 2010; Glorot & Bengio, 2010; Chapelle & Er-
han, 2011; Raiko et al., 2011). See the appendix for
more details.

Because the focus of this study is on optimization, we
only report training errors in our experiments. Test
error depends strongly on the amount of overtting in
these problems, which in turn depends on the type and
amount of regularization used during training. While
regularization is an issue of vital importance when de-
signing systems of practical utility, it is outside the
scope of our discussion. And while it could be ob-
jected that the gains achieved using better optimiza-
tion methods are only due to more exact tting of the
training set in a manner that does not generalize, this
is simply not the case in these problems, where under-
trained solutions are known to perform poorly on both
the training and test sets (undertting).

The networks we trained used the standard sigmoid
nonlinearity and were initialized using the sparse ini-
tialization technique (SI) of Martens (2010) that is
described in sec. 3.1. Each trial consists of 750,000
parameter updates on minibatches of size 200. No reg-
ularization is used. The schedule for  was given by
the following formula:

t = min(1  21log2(bt/250c+1), max )

(5)

375

was

max

from
where
{0.999, 0.995, 0.99, 0.9, 0}.
This schedule was mo-
tivated by Nesterov (1983) who advocates using what
amounts to t = 1 3/(t + 5) after some manipulation

chosen

On the importance of initialization and momentum in deep learning

task

Curves
Mnist
Faces

0(SGD)
0.48
2.1
36.4

0.9N 0.99N 0.995N 0.999N
0.16
0.074
0.80
1.0
14.2
7.7

0.091
0.75
7.8

0.096
0.73
8.5

0.9M 0.99M 0.995M 0.999M SGDC
0.15
0.16
0.9
1.0
15.3
NA

0.10
0.77
8.7

0.10
0.84
8.3

0.10
0.90
9.3

HF
0.058
0.69
7.5

HF
0.11
1.40
12.0

Table 1. The table reports the squared errors on the problems for each combination of max and a momentum type
(NAG, CM). When max is 0 the choice of NAG vs CM is of no consequence so the training errors are presented in a
single column. For each choice of max , the highest-performing learning rate is used. The column SGDC lists the results of
Chapelle & Erhan (2011) who used 1.7M SGD steps and tanh networks. The column HF lists the results of HF without
L2 regularization, as described in sec. 5; and the column HF lists the results of Martens (2010).

problem before
0.096
Curves
1.20
Mnist
Faces
10.83

after
0.074
0.73
7.7

Table 2. The eect of low-momentum netuning for NAG.
The table shows the training squared errors before and
after the momentum coecient is reduced. During the pri-
mary (transient) phase of learning we used the optimal
momentum and learning rates.

(see appendix), and by Nesterov (2003) who advocates
a constant t that depends on (essentially) the con-
dition number. The constant t achieves exponential
convergence on strongly convex functions, while the
1 3/(t + 5) schedule is appropriate when the function
is not strongly convex. The schedule of Eq. 5 blends
these proposals. For each choice of max , we report
the learning rate that achieved the best training error.
Given the schedule for , the learning rate " was
chosen from {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001}
in order to achieve the lowest nal error training error
after our xed number of updates.

Table 1 summarizes the results of these experiments.
It shows that NAG achieves the lowest published
results on this set of problems,
including those of
Martens (2010).
It also shows that larger values of
max tend to achieve better performance and that
NAG usually outperforms CM, especially when max
is 0.995 and 0.999. Most surprising and importantly,
the results demonstrate that NAG can achieve results
that are comparable with some of the best HF results
for training deep autoencoders. Note that the previ-
ously published results on HF used L2 regularization,
so they cannot be directly compared. However, the
table also includes experiments we performed with an
improved version of HF (see sec. 2.1) where weight
decay was removed towards the end of training.

We found it benecial to reduce  to 0.9 (unless 
is 0, in which case it is unchanged) during the nal
1000 parameter updates of the optimization without
reducing the learning rate, as shown in Table 2.
It
appears that reducing the momentum coecient al-
lows for ner convergence to take place whereas oth-
erwise the overly aggressive nature of CM or NAG

would prevent this. This phase shift between opti-
mization that favors fast accelerated motion along the
error surface (the transient phase) followed by more
careful optimization-as-estimation phase seems consis-
tent with the picture presented by Darken & Moody
(1993). However, while asymptotically it is the second
phase which must eventually dominate computation
time, in practice it seems that for deeper networks in
particular, the rst phase dominates overall computa-
tion time as long as the second phase is cut o before
the remaining potential gains become either insigni-
cant or entirely dominated by overtting (or both).

It may be tempting then to use lower values of  from
the outset, or to reduce it immediately when progress
in reducing the error appears to slow down. However,
in our experiments we found that doing this was detri-
mental in terms of the nal errors we could achieve,
and that despite appearing to not make much progress,
or even becoming signicantly non-monotonic, the op-
timizers were doing something apparently useful over
these extended periods of time at higher values of .

A speculative explanation as to why we see this be-
havior is as follows. While a large value of  allows
the momentum methods to make useful progress along
slowly-changing directions of low-curvature, this may
not immediately result in a signicant reduction in er-
ror, due to the failure of these methods to converge in
the more turbulent high-curvature directions (which
is especially hard when  is large). Nevertheless, this
progress in low-curvature directions takes the optimiz-
ers to new regions of the parameter space that are
characterized by closer proximity to the optimum (in
the case of a convex objective), or just higher-quality
local minimia (in the case of non-convex optimiza-
tion). Thus, while it is important to adopt a more
careful scheme that allows ne convergence to take
place along the high-curvature directions, this must be
done with care. Reducing  and moving to this ne
convergence regime too early may make it dicult for
the optimization to make signicant progress along the
low-curvature directions, since without the benet of
momentum-based acceleration, rst-order methods are
notoriously bad at this (which is what motivated the
use of second-order methods like HF for deep learn-
ing).

On the importance of initialization and momentum in deep learning

SI scale multiplier

error

0.25
16

0.5
16

1

2

4

0.074

0.083

0.35

Table 3. The table reports the training squared error that
is attained by changing the scale of the initialization.

3.1. Random Initializations

The results in the previous section were obtained with
standard logistic sigmoid neural networks that were
initialized with the sparse initialization technique (SI)
described in Martens (2010). In this scheme, each ran-
dom unit is connected to 15 randomly chosen units in
the previous layer, whose weights are drawn from a
unit Gaussian, and the biases are set to zero. The in-
tuitive justication is that the total amount of input
to each unit will not depend on the size of the previ-
ous layer and hence they will not as easily saturate.
Meanwhile, because the inputs to each unit are not all
randomly weighted blends of the outputs of many 100s
or 1000s of units in the previous layer, they will tend
to be qualitatively more diverse in their response
to inputs. When using tanh units, we transform the
weights to simulate sigmoid units by setting the biases
to 0.5 and rescaling the weights by 0.25.

We investigated the performance of the optimization
as a function of the scale constant used in SI (which
defaults to 1 for sigmoid units). We found that SI
works reasonably well if it is rescaled by a factor of
2, but leads to noticeable (but not severe) slow down
when scaled by a factor of 3. When we used the factor
1/2 or 5 we did not achieve sensible results.

4. Recurrent Neural Networks

Echo-State Networks (ESNs) is a family of RNNs with
an unusually simple training method: their hidden-to-
output connections are learned from data, but their re-
current connections are xed to a random draw from
a specic distribution and are not learned. Despite
their simplicity, ESNs with many hidden units (or
with units with explicit temporal integration, like the
LSTM) have achieved high performance on tasks with
long range dependencies (?). In this section, we inves-
tigate the eectiveness of momentum-based methods
with ESN-inspired initialization at training RNNs with
conventional size and standard (i.e., non-integrating)
neurons. We nd that momentum-accelerated SGD
can successfully train such RNNs on various articial
datasets exhibiting considerable long-range temporal
dependencies. This is unexpected because RNNs were
believed to be almost impossible to successfully train
on such datasets with rst-order methods, due to var-
ious diculties such as vanishing/exploding gradients
(Bengio et al., 1994). While we found that the use
of momentum signicantly improved performance and
robustness, we obtained nontrivial results even with
standard SGD, provided that the learning rate was
set low enough.

connection type

in-to-hid (add,mul)

in-to-hid (mem)

hid-to-hid
hid-to-out
hid-bias
out-bias

sparsity
dense
dense

dense
dense
dense

15 fan-in

spectral radius of 1.1

scale

0.001N (0, 1)
0.1N (0, 1)
0.1N (0, 1)

0

average of outputs

Table 4. The RNN initialization used in the experiments.
The scale of the vis-hid connections is problem dependent.

Each task involved optimizing the parameters of a ran-
domly initialized RNN with 100 standard tanh hidden
units (the same model used by Martens & Sutskever
(2011)). The tasks were designed by Hochreiter &
Schmidhuber (1997), and are referred to as training
problems. See sec. A.3 of the appendix for details.

4.1. ESN-based Initialization

As argued by Jaeger & Haas (2004), the spectral ra-
dius of the hidden-to-hidden matrix has a profound
eect on the dynamics of the RNNs hidden state
(with a tanh nonlinearity). When it is smaller than
1, the dynamics will have a tendency to quickly for-
get whatever input signal they may have been ex-
posed to. When it is much larger than 1, the dy-
namics become oscillatory and chaotic, allowing it to
generate responses that are varied for dierent input
histories. While this allows information to be retained
over many time steps, it can also lead to severe explod-
ing gradients that make gradient-based learning much
more dicult. However, when the spectral radius is
only slightly greater than 1, the dynamics remain os-
cillatory and chaotic while the gradient are no longer
exploding (and if they do explode, then only slightly
so), so learning may be possible with a spectral ra-
dius of this order. This suggests that a spectral radius
of around 1.1 may be eective.

To achieve robust results, we also found it is essential
to carefully set the initial scale of the input-to-hidden
connections. When training RNNs to solve those tasks
that possess many random and irrelevant distractor in-
puts, we found that having the scale of these connec-
tions set too high at the start led to relevant informa-
tion in the hidden state being too easily overwritten
by the many irrelevant signals, which ultimately led
the optimizer to converge towards an extremely poor
local minimum where useful information was never re-
layed over long distances. Conversely, we found that if
this scale was set too low, it led to signicantly slower
learning. Having experimented with multiple scales we
found that a Gaussian draw with a standard deviation
of 0.001 achieved a good balance between these con-
cerns. However, unlike the value of 1.1 for the spectral
radius of the dynamics matrix, which worked well on
all tasks, we found that good choices for initial scale
of the input-to-hidden weights depended a lot on the
particular characteristics of the particular task (such

On the importance of initialization and momentum in deep learning

as its dimensionality or the input variance). Indeed,
for tasks that do not have many irrelevant inputs, a
larger scale of the input-to-hidden weights (namely,
0.1) worked better, because the aforementioned dis-
advantage of large input-to-hidden weights does not
apply. See table 4 for a summary of the initializations
used in the experiments. Finally, we found centering
(mean subtraction) of both the inputs and the outputs
to be important to reliably solve all of the training
problems. See the appendix for more details.

4.2. Experimental Results

We conducted experiments to determine the e-
cacy of our initializations, the eect of momentum,
and to compare NAG with CM. Every learning trial
used the aforementioned initialization, 50,000 param-
eter updates and on minibatches of 100 sequences,
and the following schedule for the momentum co-
ecient :  = 0.9 for the rst 1000 parameter,
after which  = 0, where 0 can take the fol-
lowing values {0, 0.9, 0.98, 0.995}. For each 0, we
use the empirically best learning rate chosen from
{103, 104, 105, 106}.
The results are presented in Table 5, which are the av-
erage loss over 4 dierent random seeds. Instead of re-
porting the loss being minimized (which is the squared
error or cross entropy), we use a more interpretable
zero-one loss, as is standard practice with these prob-
lems. For the bit memorization, we report the frac-
tion of timesteps that are computed incorrectly. And
for the addition and the multiplication problems, we
report the fraction of cases where the RNN the error
in the nal output prediction exceeded 0.04.

Our results show that despite the considerable long-
range dependencies present in training data for these
problems, RNNs can be successfully and robustly
trained to solve them, through the use of the initial-
ization discussed in sec. 4.1, momentum of the NAG
type, a large 0, and a particularly small learning rate
(as compared with feedforward networks). Our results
also suggest that with larger values of 0 achieve bet-
ter results with NAG but not with CM, possibly due to
NAGs tolerance of larger 0s (as discussed in sec. 2).

Although we were able to achieve surprisingly good
training performance on these problems using a suf-
ciently strong momentum, the results of Martens &
Sutskever (2011) appear to be moderately better and
more robust. They achieved lower error rates and their
initialization was chosen with less care, although the
initializations are in many ways similar to ours. No-
tably, Martens & Sutskever (2011) were able to solve
these problems without centering, while we had to
use centering to solve the multiplication problem (the
other problems are already centered). This suggests
that the initialization proposed here, together with the
method of Martens & Sutskever (2011), could achieve

even better performance. But the main achievement
of these results is a demonstration of the ability of
momentum methods to cope with long-range tempo-
ral dependency training tasks to a level which seems
sucient for most practical purposes. Moreover, our
approach seems to be more tolerant of smaller mini-
batches, and is considerably simpler than the partic-
ular version of HF proposed in Martens & Sutskever
(2011), which used a specialized update damping tech-
nique whose benets seemed mostly limited to training
RNNs to solve these kinds of extreme temporal depen-
dency problems.

5. Momentum and HF

Truncated Newton methods, that include the HF
method of Martens (2010) as a particular example,
work by optimizing a local quadratic model of the
objective via the linear conjugate gradient algorithm
(CG), which is a rst-order method. While HF, like
all truncated-Newton methods, takes steps computed
using partially converged calls to CG, it is naturally
accelerated along at least some directions of lower cur-
vature compared to the gradient. It can even be shown
(Martens & Sutskever, 2012) that CG will tend to fa-
vor convergence to the exact solution to the quadratic
sub-problem rst along higher curvature directions
(with a bias towards those which are more clustered
together in their curvature-scalars/eigenvalues).

While CG accumulates information as it iterates which
allows it to be optimal in a much stronger sense than
any other rst-order method (like NAG), once it is
terminated, this information is lost. Thus, standard
truncated Newton methods can be thought of as per-
sisting information which accelerates convergence (of
the current quadratic) only over the number of itera-
tions CG performs. By contrast, momentum methods
persist information that can inform new updates across
an arbitrary number of iterations.

One key dierence between standard truncated New-
ton methods and HF is the use of hot-started calls to
CG, which use as their initial solution the one found at
the previous call to CG. While this solution was com-
puted using old gradient and curvature information
from a previous point in parameter space and possi-
bly a dierent set of training data, it may be well-
converged along certain eigen-directions of the new
quadratic, despite being very poorly converged along
others (perhaps worse than the default initial solution
of ~0). However, to the extent to which the new local
quadratic model resembles the old one, and in partic-
ular in the more dicult to optimize directions of low-
curvature (which will arguably be more likely to per-
sist across nearby locations in parameter space), the
previous solution will be a preferable starting point to
0, and may even allow for gradually increasing levels
of convergence along certain directions which persist

On the importance of initialization and momentum in deep learning

problem

add T = 80
mul T = 80

mem-5 T = 200
mem-20 T = 80

biases
0.82
0.84
2.5
8.0

0

0.39
0.48
1.27
5.37

0.9N 0.98N
0.02
0.21
0.22
0.36
0.96
1.02
2.77
0.0144

0.995N
0.00025
0.0013

0.63

0.00005

0.9M 0.98M 0.995M
0.43
0.036
0.37
0.029
0.92
1.12
1.75
0.053

0.62
0.025
1.09
0.0017

Table 5. Each column reports the errors (zero-one losses; sec. 4.2) on dierent problems for each combination of 0 and
momentum type (NAG, CM), averaged over 4 dierent random seeds. The biases column lists the error attainable by
learning the output biases and ignoring the hidden state. This is the error of an RNN that failed to establish communi-
cation between its inputs and targets. For each 0, we used the xed learning rate that gave the best performance.

in the local quadratic models across many updates.

The connection between HF and momentum methods
can be made more concrete by noticing that a single
step of CG is eectively a gradient update taken from
the current point, plus the previous update reapplied,
just as with NAG, and that if CG terminated after just
1 step, HF becomes equivalent to NAG, except that it
uses a special formula based on the curvature matrix
for the learning rate instead of a xed constant. The
most eective implementations of HF even employ a
decay constant (Martens & Sutskever, 2012) which
acts analogously to the momentum constant . Thus,
in this sense, the CG initializations used by HF allow
us to view it as a hybrid of NAG and an exact second-
order method, with the number of CG iterations used
to compute each update eectively acting as a dial
between the two extremes.

Inspired by the surprising success of momentum-based
methods for deep learning problems, we experimented
with making HF behave even more like NAG than it al-
ready does. The resulting approach performed surpris-
ingly well (see Table 1). For a more detailed account
of these experiments, see sec. A.6 of the appendix.

If viewed on the basis of each CG step (instead of
each update to parameters ), HF can be thought of
as a peculiar type of rst-order method which approx-
imates the objective as a series of quadratics only so
that it can make use of the powerful rst-order CG
method. So apart from any potential benet to global
convergence from its tendency to prefer certain direc-
tions of movement in parameter space over others, per-
haps the main theoretical benet to using HF over a
rst-order method like NAG is its use of CG, which,
while itself a rst-order method, is well known to have
strongly optimal convergence properties for quadrat-
ics, and can take advantage of clustered eigenvalues
to accelerate convergence (see Martens & Sutskever
(2012) for a detailed account of this well-known phe-
nomenon). However, it is known that in the worst
case that CG, when run in batch mode, will converge
asymptotically no faster than NAG (also run in batch
mode) for certain specially designed quadratics with
very evenly distributed eigenvalues/curvatures. Thus
it is worth asking whether the quadratics which arise
during the optimization of neural networks by HF are
such that CG has a distinct advantage in optimizing

them over NAG, or if they are closer to the aforemen-
tioned worst-case examples. To examine this question
we took a quadratic generated during the middle of a
typical run of HF on the curves dataset and compared
the convergence rate of CG, initialized from zero, to
NAG (also initialized from zero). Figure 5 in the ap-
pendix presents the results of this experiment. While
this experiment indicates some potential advantages to
HF, the closeness of the performance of NAG and HF
suggests that these results might be explained by the
solutions leaving the area of trust in the quadratics be-
fore any extra speed kicks in, or more subtly, that the
faithfulness of approximation goes down just enough
as CG iterates to oset the benet of the acceleration
it provides.

6. Discussion

Martens (2010) and Martens & Sutskever (2011)
demonstrated the eectiveness of the HF method as
a tool for performing optimizations for which previ-
ous attempts to apply simpler rst-order methods had
failed. While some recent work (Chapelle & Erhan,
2011; Glorot & Bengio, 2010) suggested that rst-order
methods can actually achieve some success on these
kinds of problems when used in conjunction with good
initializations, their results still fell short of those re-
ported for HF. In this paper we have completed this
picture and demonstrated conclusively that a large
part of the remaining performance gap that is not
addressed by using a well-designed random initializa-
tion is in fact addressed by careful use of momentum-
based acceleration (possibly of the Nesterov type). We
showed that careful attention must be paid to the mo-
mentum constant , as predicted by the theory for
local and convex optimization.

Momentum-accelerated SGD, despite being a rst-
order approach, is capable of accelerating directions
of
low-curvature just like an approximate Newton
method such as HF. Our experiments support the idea
that this is important, as we observed that the use of
stronger momentum (as determined by ) had a dra-
matic eect on optimization performance, particularly
for the RNNs. Moreover, we showed that HF can be
viewed as a rst-order method, and as a generalization
of NAG in particular, and that it already derives some
of its benets through a momentum-like mechanism.

On the importance of initialization and momentum in deep learning

LeCun, Y., Bottou, L., Orr, G., and Muller, K. Ecient
backprop. Neural networks: Tricks of the trade, pp. 546
546, 1998.

Martens, J. Deep learning via Hessian-free optimization.
In Proceedings of the 27th International Conference on
Machine Learning (ICML), 2010.

Martens, J. and Sutskever, I. Learning recurrent neural
networks with hessian-free optimization. In Proceedings
of the 28th International Conference on Machine Learn-
ing (ICML), pp. 10331040, 2011.

Martens, J. and Sutskever, I. Training deep and recurrent
networks with hessian-free optimization. Neural Net-
works: Tricks of the Trade, pp. 479535, 2012.

Mikolov, Tomas, Sutskever,

Ilya, Deoras, Anoop, Le,
Hai-Son, Kombrink, Stefan, and Cernocky, J.
Sub-
word language modeling with neural networks. preprint
(http://www. t. vutbr. cz/imikolov/rnnlm/char. pdf),
2012.

Mohamed, A., Dahl, G.E., and Hinton, G. Acoustic mod-
eling using deep belief networks. Audio, Speech, and
Language Processing, IEEE Transactions on, 20(1):14
22, Jan. 2012.

Nesterov, Y. A method of solving a convex program-
ming problem with convergence rate O(1/sqr(k)). Soviet
Mathematics Doklady, 27:372376, 1983.

Nesterov, Y. Introductory lectures on convex optimization:

A basic course, volume 87. Springer, 2003.

Orr, G.B. Dynamics and algorithms for stochastic search.

1996.

Polyak, B.T. Some methods of speeding up the convergence
of iteration methods. USSR Computational Mathematics
and Mathematical Physics, 4(5):117, 1964.

Raiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep
learning made easier by linear transformations in percep-
trons. In NIPS 2011 Workshop on Deep Learning and
Unsupervised Feature Learning, Sierra Nevada, Spain,
2011.

Sutskever, I., Martens, J., and Hinton, G. Generating
text with recurrent neural networks. In Proceedings of
the 28th International Conference on Machine Learning,
ICML 11, pp. 10171024, June 2011.

Wiegerinck, W., Komoda, A., and Heskes, T. Stochas-
tic dynamics of learning with momentum in neural net-
works. Journal of Physics A: Mathematical and General,
27(13):4425, 1999.

