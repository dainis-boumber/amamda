Abstract

Hierarchical modeling is a fundamental concept in Bayesian statistics.
The basic idea is that parameters are endowed with distributions which
may themselves introduce new parameters, and this construction recurses.
In this review we discuss the role of hierarchical modeling in Bayesian non-
parametrics, focusing on models in which the innite-dimensional parame-
ters are treated hierarchically. For example, we consider a model in which
the base measure for a Dirichlet process is itself treated as a draw from
another Dirichlet process. This yields a natural recursion that we refer
to as a hierarchical Dirichlet process. We also discuss hierarchies based
on the Pitman-Yor process and on completely random processes. We
demonstrate the value of these hierarchical constructions in a wide range
of practical applications, in problems in computational biology, computer
vision and natural language processing.

Introduction

1
Hierarchical modeling is a fundamental concept in Bayesian statistics. The basic
idea is that parameters are endowed with distributions which may themselves
introduce new parameters, and this construction recurses. A common motif
in hierarchical modeling is that of the conditionally independent hierarchy, in
which a set of parameters are coupled by making their distributions depend

1

on a shared underlying parameter. These distributions are often taken to be
identical, based on an assertion of exchangeability and an appeal to de Finettis
theorem.

Hierarchies help to unify statistics, providing a Bayesian interpretation of
frequentist concepts such as shrinkage and random eects. Hierarchies also
provide ways to specify non-standard distributional forms, obtained as integrals
over underlying parameters. They play a role in computational practice in the
guise of variable augmentation. These advantages are well appreciated in the
world of parametric modeling, and few Bayesian parametric modelers fail to
make use of some aspect of hierarchical modeling in their work.

Nonparametric Bayesian models also typically include many classical nite-
dimensional parameters, including scale and location parameters, and hierar-
chical modeling concepts are often invoked in specifying distributions for these
parameters. For example, the Dirichlet process DP(, G0) involves a concentra-
tion parameter , which is generally given a prior distribution in nonparametric
(and semiparametric) models that make use of the Dirichlet process. Moreover,
the base measure, G0, is often taken to be a parametric distribution and its
parameters are endowed with prior distributions as well.

In this chapter we discuss a more thoroughgoing exploitation of hierarchi-
cal modeling ideas in Bayesian nonparametric statistics. The basic idea is that
rather than treating distributional parameters such as G0 parametrically, we
treat them nonparametrically. In particular, the base measure G0 in the Dirich-
let process can itself be viewed as a random draw from some distribution on
measuresspecically it can be viewed as a draw from the Dirichlet process.
This yields a natural recursion that we refer to as a hierarchical Dirichlet pro-
cess. Our focus in this chapter is on nonparametric hierarchies of this kind,
where the tools of Bayesian nonparametric modeling are used recursively.

The motivations for the use of hierarchical modeling ideas in the nonpara-
metric setting are at least as strong as they are in the parametric setting. In
particular, nonparametric models involve large numbers of degrees of freedom,
and hierarchical modeling ideas provide essential control over these degrees of
freedom. Moreover, hierarchical modeling makes it possible to take the build-
ing blocks provided by simple stochastic processes such as the Dirichlet process
and construct models that exhibit richer kinds of probabilistic structure. This
breathes life into the nonparametric framework.

The chapter is organized as follows. In Section 2, we discuss the hierarchical
Dirichlet process, showing how it can be used to link multiple Dirichlet processes.
We present several examples of real-world applications in which such models are
natural. Section 3 shows how the hierarchical Dirichlet process can be used to
build nonparametric hidden Markov models; these are hidden Markov models in
which the cardinality of the state space is unbounded. We also discuss extensions
to nonparametric hidden Markov trees and nonparametric probabilistic context
free grammars. In Section 4 we consider a dierent nonparametric hierarchy
based on the Pitman-Yor model, showing that it is natural in domains such
as natural language processing in which data often exhibit power-law behavior.
Section 5 discusses the beta process, an alternative to the Dirichlet process

2

which yields sparse featural representations. We show that the counterpart of
the Chinese restaurant process is a distribution on sparse binary matrices known
as the Indian buet process. We also consider hierarchical models based on the
beta process. In Section 6, we consider some semiparametric models that are
based on nonparametric hierarchies. Finally, in Section 7 we present an overview
of some of the algorithms that have been developed for posterior inference in
hierarchical Bayesian nonparametric models.

In all of these cases, we use practical applications to motivate these con-
structions and to make our presentation concrete. Our applications range from
problems in biology to computational vision to natural language processing.
Several of the models that we present provide state-of-the-art performance in
these application domains. This wide range of successful applications serves
notice as to the growing purview of Bayesian nonparametric methods.

2 Hierarchical Dirichlet Processes
The Dirichlet process (DP) is useful in models for which a component of the
model is a discrete random variable of unknown cardinality. The canonical
example of such a model is the DP mixture model, where the discrete variable
is a cluster indicator. The hierarchical Dirichlet process (HDP) is useful in
problems in which there are multiple groups of data, where the model for each
group of data incorporates a discrete variable of unknown cardinality, and where
we wish to tie these variables across groups (Teh et al., 2006). For example,
the HDP mixture model allows us to share clusters across multiple clustering
problems.
The basic building block of a hierarchical Dirichlet process is a recursion in
which the base measure G0 for a Dirichlet process G  DP(, G0) is itself a draw
from a Dirichlet process: G0  DP(, H). This recursive construction has the
eect of constraining the random measure G to place its atoms at the discrete
locations determined by G0. The major application of such a construction is to
the setting of conditionally independent hierarchical models of grouped data.
More formally, consider an indexed collection of DPs, {Gj}, one for each of a
countable set of groups and dened on a common probability space (, ). The
hierarchical Dirichlet process ties these random measures probabilistically by
letting them share their base measure and letting this base measure be random:

G0 | , H  DP(, H)
Gj | , G0  DP(, G0)

for j  J ,

(1)

where J is the index set. This conditionally independent hierarchical model
induces sharing of atoms among the random measures Gj since each inherits its
set of atoms from the same G0. To understand the precise nature of the sharing
induced by the HDP it is helpful to consider representations akin to the stick-
breaking and Chinese restaurant representations of the DP. We consider these
representations in the next three subsections before turning to a discussion of
applications of the HDP.

3

Figure 1: The HDP stick-breaking construction. The left panel depicts a draw
of , and the remaining panels depict draws of 1, 2 and 3 conditioned on
.

Note that the recursive construction of the HDP can be generalized to ar-
bitrary hierarchies in the obvious way. Each Gj is given a DP prior with base
measure Gpa(j), where pa(j) is the parent index of j in the hierarchy. As in
the two-level hierarchy in Eq. (1), the set of atoms at the top level is shared
throughout the hierarchy, while the multi-level hierarchy allows for a richer de-
pendence structure on the weights of the atoms. Section 4 presents an instance
of such a hierarchy in the setting of Pitman-Yor processes.

Other ways to couple multiple Dirichlet processes have been proposed in the
literature; in particular the dependent Dirichlet process of MacEachern et al.
(2001) provides a general formalism. Ho et al. (2006) gives a complementary
view of the HDP and its Pitman-Yor generalizations in terms of coagulation
operators. See Teh et al. (2006) and Chapter ?? for overviews.

2.1 Stick-Breaking Construction
In this section we develop a stick-breaking construction for the HDP. This rep-
resentation provides a concrete representation of draws from an HDP and it
provides insight into the sharing of atoms across multiple DPs.
We begin with the stick-breaking representation for the random base mea-
sure G0, where G0  DP(, H). Given that this base measure is distributed
according to a DP, we have (Sethuraman, 1994; Ishwaran and James, 2001, also
see Section ?? in Chapter ??):

k

k

,

for k = 1, . . . ,

(2)

(3)

G0 =

where

vk |   Beta(1, )
k1(cid:89)
(1  vl)

k = vk
k | H  H.


l=1

(cid:88)

k=1

4

010203000.20.40.60.8weights010203000.20.40.60.81010203000.20.40.60.82010203000.20.40.60.83We refer to the joint distribution on the innite sequence (1, 2, . . .) as the
GEM() distribution (Pitman, 2002) (GEM stands for Griths, Engen and
McCloskey).

The random measures Gj are also distributed (conditionally) according to
a DP. Moreover, the support of each Gj is contained within the support of G0.
Thus the stick-breaking representation for Gj is a reweighted sum of the atoms
in G0:

Gj =

jk

k

.

(4)

k=1

The problem reduces to nding a relationship between the weights  = (1, 2, . . .)
and j = (j1, j2, . . .). Let us interpret these weight vectors as probability
measures on the discrete space {1, . . . ,}. Taking partitions over integers in-
duced by partitions on , the dening property of the DP (Ferguson, 1973)
implies:

(cid:88)

Some algebra then readily yields the following explicit construction for j con-
ditioned on :

j | ,   DP(, ).
(cid:33)(cid:33)
(cid:32)
vjk | , 1, . . . , k  Beta
k1(cid:89)
(1  vjl).

(cid:32)
1  k(cid:88)

jk = vjk

k, 

l

l=1

(5)

for k = 1, . . . , (6)

l=1

Figure 1 shows a sample draw of  along with draws from 1, 2 and 3 given
.
From Eq. (3) we see that the mean of k is E[k] = k1(1 + )k which
decreases exponentially in k. The mean for j is simply its base measure ; thus
E[jk] = E[k] = k1(1+)k as well. However the law of total variance shows
that jk has higher variance than k: Var[jk] = E[ k(1k)
]+Var[k] > Var[k].
The higher variance is reected in Figure 1 by the sparser nature of j relative
to .

1+

2.2 Chinese Restaurant Franchise
The Chinese restaurant process (CRP) describes the marginal probabilities of
the DP in terms of a random partition obtained from a sequence of customers
sitting at tables in a restaurant. There is an analogous representation for the
HDP which we refer to as a Chinese restaurant franchise (CRF). In a CRF the
metaphor of a Chinese restaurant is extended to a set of restaurants, one for
each index in J . The customers in the jth restaurant sit at tables in the same
manner as the CRP, and this is done independently in the restaurants. The

5

coupling among restaurants is achieved via a franchise-wide menu. The rst
customer to sit at a table in a restaurant chooses a dish from the menu and all
subsequent customers who sit at that table inherit that dish. Dishes are chosen
with probability proportional to the number of tables (franchise-wide) which
have previously served that dish.

More formally, label the ith customer in the jth restaurant with a random
variable ji that is distributed according to Gj. Similarly, let 
jt denote a
random variable corresponding to the tth table in the jth restaurant; these
variables are drawn independently and identically distributed (iid) according to
G0. Finally, the dishes are iid variables 
k distributed according to the base
measure H. We couple these variables as follows. Each customer sits at one
table and each table serves one dish; let customer i in restaurant j sit at table
tji, and let table t serve dish kjt. Then let ji = 

= 

.

Let njtk be the number of customers in restaurant j seated around table t
and being served dish k, let mjk be the number of tables in restaurant j serving
dish k, and let K be the number of unique dishes served in the entire franchise.
We denote marginal counts with dots; e.g., njk is the number of customers in
restaurant j served dish k.

To show that the CRF captures the marginal probabilities of the HDP, we
integrate out the random measures Gj and G0 in turn from the HDP. We start
by integrating out the random measure Gj; this yields a set of conditional
distributions for the ji described by a Polya urn scheme:

jtji

kjtji

ji | j1, . . . , j,i1, , G0  mj(cid:88)

njt

 + nj

+



jt



 + nj

G0.

(7)

t=1

A draw from this mixture can be obtained by drawing from the terms on the
right-hand side with probabilities given by the corresponding mixing propor-
tions. If a term in the rst summation is chosen then the customer sits at an
already occupied table: we increment njt, set ji = 
jt and let tji = t for the
chosen t. If the second term is chosen then the customer sits at a new table:
 G0, set ji = 
we increment mj by one, set njmj = 1, draw 
and
tji = mj.

jt is drawn iid from G0 in the Polya urn scheme in Eq. (7),
and this is the only reference to G0 in that equation. Thus we can readily
integrate out G0 as well, obtaining a Polya urn scheme for the 
jt:

Notice that each 

jmj

jmj

j,t1, , H  K(cid:88)

k=1

jt | 


11, . . . , 

1m1

, . . . , 

mk

 + m

+



k



 + m

H,

(8)

where we have presumed for ease of notation that J = {1, . . . ,|J |}. As
promised, we see that the kth dish is chosen with probability proportional to
the number of tables franchise-wide that previously served that dish (mk).

The CRF is useful in understanding scaling properties of the clustering in-
duced by an HDP. In a DP the number of clusters scales logarithmically (An-
toniak, 1974). Thus mj  O( log nj
 ) where mj and nj are respectively the

6

(cid:80)

 ) = O( log( 
mj

from a DP, we have that K  O( log(cid:80)

total number of tables and customers in restaurant j. Since G0 is itself a draw
If
we assume that there are J groups and that the groups (the customers in the
dierent restaurants) have roughly the same size N, nj  O(N), we see that
K  O( log 
 ). Thus the number of
clusters scales doubly logarithmically in the size of each group, and logarithmi-
cally in the number of groups. The HDP thus expresses a prior belief that the
number of clusters grows very slowly in N. If this prior belief is inappropriate
for a given problem, there are alternatives; in particular, in Section 4.3.1 we
discuss a hierarchical model that yields power-law scaling.

 +  log J +  log log N

 ) = O( log 

 J log N

j log nj

 )).



j

2.3 Posterior Structure of the HDP
The Chinese restaurant franchise is obtained by integrating out the random
measures Gj and then integrating out G0. Integrating out the random measures
Gj yields a Chinese restaurant for each group as well as a sequence of iid draws
from the base measure G0, which are used recursively in integrating out G0.
Having obtained the CRF, it is of interest to derive conditional distributions that
condition on the CRF; this not only illuminates the combinatorial structure of
the HDP but it also prepares the ground for a discussion of inference algorithms
(see Section 7), where it can be useful to instantiate the CRF explicitly.
k }k=1,...,K, the
table tji at which the ith customer sits, and the dish kjt served at the tth table.
As functions of the state of the CRF, we also have the numbers of customers
n = {njtk}, the numbers of tables m = {mjk}, the customer labels  = {ji}
jt}. The relationship between the customer labels
and the table labels 
and the table labels is given as follows: 
Consider the distribution of G0 conditioned on the state of the CRF. G0 is
,
independent from the rest of the CRF when we condition on the iid draws 
because the restaurants interact with G0 only via the iid draws. The posterior
thus follows from the usual posterior for a DP given iid draws:

The state of the CRF consists of the dish labels 

and ji = 

jt = 
jkjt

 = {

 = {

jtji

.

G0 | , H, 

  DP

k=1 mk

k

.

(9)

(cid:32)

H +(cid:80)K

 + m,

 + m
 are determined given 

(cid:33)

Note that values for m and 
the unique values and their counts among 
constructed as follows (using the dening property of a DP):

, since they are simply
1. A draw from Eq. (9) can be

0, 1, . . . , K | , G0, 

  Dirichlet(, m1, . . . , mK)

(10)

0 | , H  DP(, H)
K(cid:88)
G(cid:48)

G0 = 0G(cid:48)

0 +

k

k

.

1Here we make the simplifying assumption that H is a continuous distribution so that

draws from H are unique. If H is not continuous then additional bookkeeping is required.

k=1

7

We see that the posterior for G0 is a mixture of atoms corresponding to the
dishes and an independent draw from DP(, H).

Conditioning on this draw of G0 as well as the state of the CRF, the posteri-
ors for the Gj are independent. In particular, the posterior for each Gj follows
from the usual posterior for a DP, given its base measure G0 and iid draws j:

(cid:32)

G0 +(cid:80)K

(cid:33)

Gj | , G0, j  DP

 + nj,

k=1 njK

k

 + nj

.

(11)

Note that nj and 
j. Making use of the decomposition of G0 into G(cid:48)
dishes 

 are simply the unique values and their counts among the
0 and atoms located at the

, a draw from Eq. (11) can thus be constructed as follows:

j0, j1, . . . , jK | , j  Dirichlet(0, 1 + nj1, . . . , K + njK)

(12)

j | , G0  DP(0, G(cid:48)
G(cid:48)
K(cid:88)
0)

Gj = j0G(cid:48)

j +

jk

.

k

k=1

We see that Gj is a mixture of atoms at 
DP, where the concentration parameter depends on 0.

k and an independent draw from a

The posterior over the entire HDP is obtained by averaging the conditional
distributions of G0 and Gj over the posterior state of the Chinese restaurant
franchise given .

This derivation shows that the posterior for the HDP can be split into a
discrete part and a continuous part. The discrete part consists of atoms at
, with dierent weights on these atoms for each DP. The
the unique values 
continuous part is a separate draw from an HDP with the same hierarchical
structure as the original HDP and global base measure H, but with altered
concentration parameters. The continuous part consists of an innite series of
atoms at locations drawn iid from H. Although we have presented this posterior
representation for a two-level hierarchy, the representation extends immediately
to general hierarchies.

2.4 Applications of the HDP
In this section we consider several applications of the HDP. These models use the
HDP at dierent depths in an overall Bayesian hierarchy. In the rst example
the random measures obtained from the HDP are used to generate data directly,
and in the second and third examples these random measures generate latent
parameters.

2.4.1 Information Retrieval
The growth of modern search engines on the World Wide Web has brought new
attention to a classical problem in the eld of information retrieval (IR)how

8

should a collection of documents be represented so that relevant documents can
be returned in response to a query? IR researchers have studied a wide variety
of representations and have found empirically that a representation known as
term frequency-inverse document frequency, or tf-idf, yields reasonably high-
quality rankings of documents (Salton and McGill, 1983). The general intuition
is that the relevance of a document to a query should be proportional to the
frequency of query terms it contains (term frequency), but that query terms
that appear in many documents should be downweighted since they are less
informative (inverse document frequency).

Cowans (2004, 2006) has shown that the HDP provides statistical justi-
cation for the intuition behind tf-idf. Let xji denote the ith word in the jth
document in some corpus of documents, where the range of xji is a discrete
vocabulary . Consider the following simple model for documents:

G0 | , H  DP(, H)
Gj | , G0  DP(, G0)

xji | Gj  Gj

(13)

for j  J
for i = 1, . . . , nj,

where H is the global probability measure over the vocabulary  and where nj is
the number of words in the jth document. (Note that nj = nj where the latter
refers to the general notation introduced in Section 2.2; here and elsewhere we
use nj as a convenient shorthand.) In this model, Gj is a discrete measure over
the vocabulary associated with document j and G0 is a discrete measure over
the vocabulary that acts to tie together word usages across the corpus. The
model is presented as a graphical model in the left panel of Figure 2.
following marginal probabilities for words    in the jth document:

Integrating out G0 and the Gj as discussed in Section 2.2, we obtain the

pj() =
p0() = m(cid:48)

n(cid:48)
j + p0()
n(cid:48)
j + 
 + H()
m(cid:48) + 

,

(14)

where n(cid:48)
j is the term frequencythe number of occurrences of  in document
jand m(cid:48)
j is the number of tables serving dish  in restaurant j in the CRF
representation.
(Note that the need for the specialized prime notation in
this case is driven by the fact that  is a discrete space in this example. In
particular, for each    there may be multiple k such that 
k = . The
term frequency n(cid:48)
= njk is the number of customers eating dish 
regardless of which menu entry they picked. Similarly, m(cid:48)

j = (cid:80)

j =(cid:80)

If we make the approximation that the number of tables serving a partic-
ular dish in a particular restaurant is at most one, then m(cid:48)
 is the document
frequencythe number of documents containing word  in the corpus. We now
rank documents by computing a relevance score R(j, Q)the log probability

k

k:

k

k:

= mjk.)

9

Figure 2: Graphical representations of HDP-based models. Left: An HDP
model for information retrieval. Center: An HDP mixture model for haplotype
phasing. Right: The HDP-LDA model for topic or admixture modeling.

10

xjiG0GjHi=1,...,njjJji1xjiG0GjHi=1,...,njjJji2jixjiG0GjHi=1,...,njjJof a query Q under each document j:

R(j, Q) =(cid:88)
=(cid:88)

Q

Q

log pj()

log

1 +

  log(n(cid:48)

j + ) + log(p0())

(15)

 .

n(cid:48)
j
 m(cid:48)
+H()

m(cid:48)+

In this score the rst term is akin to a tf-idf score, the second term is a nor-
malization penalizing large documents, and the third term can be ignored as it
does not depend on document identity j. Thus we see that a simple application
of the HDP provides a principled justication for the use of inverse document
frequency and document length normalization. Moreover, in small-scale experi-
ments, Cowans (2004, 2006) found that this score improves upon state-of-the-art
relevance scores (Robertson et al., 1992; Hiemstra and Kraaij, 1998).

2.4.2 Multi-Population Haplotype Phasing
We now consider a class of applications in which the HDP provides a distribution
on latent parameters rather than on the observed data.

Haplotype phasing is an interesting problem in statistical genetics that can
be formulated as a mixture model (Stephens et al., 2001). Consider a set of M
binary markers along a chromosome. Chromosomes come in pairs for humans,
so let i1 and i2 denote the binary-valued vectors of markers for a pair of
chromosomes for the ith individual. These vectors are referred to as haplotypes,
and the elements of these vectors are referred to as alleles. A genotype xi is
a vector which records the unordered pair of alleles for each marker; that is,
the association of alleles to chromosome is lost. The haplotype phasing problem
is to restore haplotypes (which are useful for predicting disease associations)
from genotypes (which are readily assayed experimentally whereas haplotypes
are not).

Under standard assumptions from population genetics, we can write the

probability of the ith genotype as a mixture model:

p(xi) = (cid:88)

i1,i2H

p(i1)p(i2)p(xi | i1, i2),

(16)

where H is the set of haplotypes in the population and where p(xi | i1, i2)
reects the loss of order information as well as possible measurement error.
Given that the cardinality of H is unknown, this problem is naturally formulated
as a DP mixture modeling problem where a cluster is a haplotype (Xing et al.,
2007).

Let us now consider a multi-population version of the haplotype phasing
problem in which the genotype data can be classied into (say) Asian, European
and African subsets. Here it is natural to attempt to identify the haplotypes in
each population and to share these haplotypes among populations. This can be

11

achieved with the following HDP mixture model:

G0 | , H  DP(, H)
Gj | , G0  DP(, G0)

ji1, ji2 | Gj
xji | ji1, ji2  Fji1,ji2

iid Gj

,

for each population j  J
for each individual i = 1, . . . , nj

(17)

where ji1, ji2 denote the pair of haplotypes for the ith individual in the jth
population. The model is presented as a graphical model in the center panel of
Figure 2. Xing et al. (2006) showed that this model performs eectively in multi-
population haplotype phasing, outperforming methods that lump together the
multiple populations or treat them separately.

2.4.3 Topic Modeling
A topic model or mixed membership model is a generalization of a nite mixture
model in which each data point is associated with multiple draws from a mixture
model, not a single draw (Blei et al., 2003; Erosheva, 2003). As we will see,
while the DP is the appropriate tool to extend nite mixture models to the
nonparametric setting, the appropriate tool for nonparamametric topic models
is the HDP.

To motivate the topic model formulation, consider the problem of modeling
the word occurrences in a set of newspaper articles (e.g., for the purposes of
classifying future articles). A simple clustering methodology might attempt to
place each article in a single cluster. But it would seem more useful to be able
to cross-classify articles according to topics; for example, an article might be
mainly about Italian food, but it might also refer to health, history and the
weather. Moreover, as this example suggests, it would be useful to be able to
assign numerical values to the degree to which an article treats each topic.

Topic models achieve this goal as follows. Dene a topic to be a probability
distribution across a set of words taken from some vocabulary W . A document is
modeled as a probability distribution across topics. In particular, let us assume
the following generative model for the words in a document. First choose a
probability vector  from the K-dimensional simplex, and then repeatedly (1)
select one of the K topics with probabilities given by the components of  and
(2) choose a word from the distribution dened by the selected topic. The vector
 thus encodes the expected fraction of words in a document that are allocated
to each of the K topics. In general a document will be associated with multiple
topics.

Another natural example of this kind of problem arises in statistical genet-
ics. Assume that for each individual in a population we can assay the state
of each of M markers, and recall that the collection of markers for a single
individual is referred to as a genotype. Consider a situation in which K sub-
populations which have hitherto remained separate are now thoroughly mixed
(i.e., their mating patterns are that of a single population). Individual geno-
types will now have portions that arise from the dierent subpopulations. This

12

is referred to as admixture. We can imagine generating a new admixed geno-
type by xing a distribution  across subpopulations and then repeatedly (1)
choosing a subpopulation according to  and (2) choosing the value of a marker
(an allele) from the subpopulation-specic distribution on the alleles for that
marker. This formulation is essentially isomorphic to the document modeling
formulation. (The dierence is that in the document setting the observed words
are generally assumed to be exchangeable, whereas in the genetics setting each
marker has its own distribution over alleles).

To fully specify a topic model we require a distribution for . Taking this
distribution to be symmetric Dirichlet, we obtain the latent Dirichlet allocation
(LDA) model, developed by Blei et al. (2003) and Pritchard et al. (2000) as a
model for documents and admixture, respectively. This model has been widely
used not only in the elds of information retrieval and statistical genetics, but
also in computational vision, where a topic is a distribution across visual
primitives, and an image is modeled as a distribution across topics (Fei-Fei and
Perona, 2005).

Let us now turn to the problem of developing a Bayesian nonparametric
version of LDA in which the number of topics is allowed to be open-ended.
As we have alluded to, this requires the HDP, not merely the DP. To see this,
consider the generation of a single word in a given document. According to
LDA, this is governed by a nite mixture model, in which one of K topics is
drawn and then a word is drawn from the corresponding topic distribution.
Generating all of the words in a single document requires multiple draws from
this nite mixture.
If we now consider a dierent document, we again have
a nite mixture, with the same mixture components (the topics), but with a
dierent set of mixing proportions (the document-specic vector ). Thus we
have multiple nite mixture models. In the nonparametric setting they must be
linked so that the same topics can appear in dierent documents.

We are thus led to the following model, which we refer to as HDP-LDA:

G0 | , H  DP(, H)
Gj | , G0  DP(, G0)

ji | Gj  Gj
xji | ji  Fji

,

for each document j  J
for each word i = 1, . . . , nj

(18)

where xji is the ith word in document j, H is the prior distribution over topics
and Fji
is the distribution over words. The model is presented as a graphical
model in the right panel of Figure 2. Note that the atoms present in the ran-
dom distribution G0 are shared among the random distributions Gj. Thus, as
desired, we have a collection of tied mixture models, one for each document.

Topic models can be generalized in a number of other directions. For exam-
ple, in applications to document modeling it is natural to ask that topics occur
at multiple levels of resolution. Thus, at a high level of resolution, we might
wish to obtain topics that give high probability to words that occur throughout
the documents in a corpus, while at a lower level we might wish to nd topics
that are focused on words that occur in specialized subsets of the documents.

13

A Bayesian nonparametric approach to obtaining this kind of abstraction hier-
archy has been presented by Blei et al. (2004). In the model presented by these
authors, topics are arranged into a tree, and a document is modeled as a path
down the tree. This is achieved by dening the tree procedurally in terms of a
linked set of Chinese restaurants.

3 Hidden Markov Models with Innite State Spaces
Hidden Markov models (HMMs) are widely used to model sequential data and
time series data (Rabiner, 1989). An HMM is a doubly-stochastic Markov chain
in which a state sequence, 1, 2, . . . ,  , is drawn according to a Markov chain
on a discrete state space  with transition kernel (t, t+1). A corresponding
sequence of observations, x1, x2, . . . , x , is drawn conditionally on the state se-
quence, where for all t the observation xt is conditionally independent of the
other observations given the state t. We let Ft
(xt) denote the distribution of
xt conditioned on the state t; this is referred to as the emission distribution.
In this section we show how to use Bayesian nonparametric ideas to obtain
an innite HMMan HMM with a countably innite state space (Beal et al.,
2002; Teh et al., 2006). The idea is similar in spirit to the passage from a nite
mixture model to a DP mixture model. However, as we show, the appropriate
nonparametric tool is the HDP, not the DP. The resulting model is thus referred
to as the hierarchical Dirichlet process hidden Markov model (HDP-HMM). We
present both the HDP formulation and a stick-breaking formulation in this
section; the latter is particularly helpful in understanding the relationship to
nite HMMs. It is also worth noting that a Chinese restaurant franchise (CRF)
representation of the HDP-HMM can be developed, and indeed Beal et al. (2002)
presented a precursor to the HDP-HMM that was based on an urn model akin
to the CRF.

To understand the need for the HDP rather than the DP, note rst that a
classical HMM species a set of nite mixture distributions, one for each value
of the current state t. Indeed, given t, the observation xt+1 is chosen by rst
picking a state t+1 and then choosing xt+1 conditional on that state. Thus the
transition probability (t, t+1) plays the role of a mixing proportion and the
emission distribution Ft
plays the role of the mixture component. It is natural
to consider replacing this nite mixture model by a DP mixture model. In so
doing, however, we must take into account the fact that we obtain a set of DP
mixture models, one for each value of the current state. If these DP mixture
models are not tied in some way, then the set of states accessible in a given
value of the current state will be disjoint from those accessible for some other
value of the current state. We would obtain a branching structure rather than
a chain structure. The solution to this problem is straightforwardwe use the
HDP to tie the DPs.

More formally, let us consider a collection of random transition kernels,

14

Figure 3: HDP hidden Markov model.

{G :   }, drawn from an HDP:
G0 | , H  DP(, H)
G | , G0  DP(, G0)

for   ,

(19)

where H is a base measure on the probability space (,T ). As we shall see,
the random base measure G0 allows the transitions out of each state to share
0   be a predened initial state. The
the same set of next states. Let 0 = 
conditional distributions of the sequence of latent state variables 1, . . . ,  and
observed variables x1, . . . , x are:

t | t1, Gt1

 Gt1
xt | t  Ft

.

for t = 1, . . . , 

(20)

A graphical model representation for the HDP-HMM is shown in Figure 3.

We have dened a probability model consisting of an uncountable number of
DPs, which may raise measure-theoretic concerns. These concerns can be dealt
with, however, essentially due to the fact that the sample paths of the HDP-
HMM only ever encounter a nite number of states. To see this more clearly,
and to understand the relationship of the HDP-HMM to the parametric HMM,
it is helpful to consider a stick-breaking representation of the HDP-HMM. This
representation is obtained directly from the stick-breaking representation of the

15

012x1x2xG0GHunderlying HDP:

(cid:88)
(cid:88)

k=1

k=1

G0 =

G

l

=

k

k



l

k

k

where

k | H  H

 |   GEM()
| ,   DP(, ).



k

(21)

for l = 0, 1, . . . ,,

for k = 1, . . . ,

(22)

0 and the atoms 

1 , 

k are shared across G0 and the transition distributions G

The atoms 
. Since
all states visited by the HMM are drawn from the transition distributions, the
states possibly visited by the HMM with positive probability (given G0) will
consist only of the initial state 
2 , . . .. Relating to the
parametric HMM, we see that the transition probability from state 
to state

k is given by 
k and the distribution on the observations is given by F
.
This relationship to the parametric HMM can be seen even more clearly if we
k with the integer k, for k = 0, 1, . . . ,, and if we introduce
identify the state 
integer-valued variables zt to denote the state at time t. In particular, if t = 
is the state at time t, we let zt take on value k, and write k instead of 
.
The HDP-HMM can now be expressed as:
zt | zt1, zt1
xt | zt, 

 zt1
 F

(23)

,

k

k

k

l

l

l

zt

zt

with priors on the parameters and transition probabilities given by Eq. (23).
This construction shows explicitly that the HDP-HMM can be interpreted as
an HMM with a countably innite state space.

A diculty with the HDP-HMM as discussed thus far is that it tends to
be poor at capturing state persistence; it has a tendency to create redundant
states and rapidly switch among them. This may not be problematic for ap-
plications in which the states are nuisance variables and it is overall predictive
likelihood that matters, but it can be problematic for segmentation or parsing
applications in which the states are the object of inference and when state per-
sistence is expected. This problem can be solved by giving special treatment
to self-transitions. In particular, let G denote the transition kernel associated
with state . Fox et al. (2009) proposed the following altered denition of G
(compare to Eq. (19)):

(cid:19)

G | , , G0,   DP

 + ,

G0 + 

 + 

,

(24)

(cid:18)

16

where  is a point mass at  and where  is a parameter that determines the
extra mass placed on a self-transition. To see in more detail how this aects state
persistence, consider the stick-breaking weights 
associated with one of the
countably many states 
k that can be visited by the HMM. The stick-breaking
is altered as follows (compare to Eq. (23)):
representation of G

k

(cid:18)

k

| , ,   DP



k

(cid:19)

 + ,

 + 

k

 + 

.

(25)

Fox et al. (2009) further place a vague gamma prior on  +  and a beta prior
on /( + ). The hyperparameters of these distributions allow prior control of
state persistence. See also Beal et al. (2002), who develop a related prior within
the framework of their hierarchical urn scheme.

3.1 Applications of the HDP-HMM
In the following sections we describe a number of applications and extensions of
the HDP-HMM. An application that we will not discuss but is worth mention-
ing is the application of HDP-HMMs to the problem of modeling recombination
hotspots and ancestral haplotypes for short segments of single nucleotide poly-
morphisms (Xing and Sohn, 2007).

3.1.1 Speaker Diarization
Speech recognition has been a major application area for classical parametric
HMMs (Huang et al., 2001). In a typical application, several dozen states are
used, roughly corresponding to the number of phoneme-like segments in speech.
The observations xt are spectral representations of speech over short time slices.
In many applications, however, the number of states is more fundamentally
part of the inferential problem and it does not suce to simply x an arbi-
trary value. Consider an audio recording of a meeting in which the number
of people participating in the meeting is unknown a priori. The problem of
speaker diarization is that of segmenting the audio recording into time intervals
associated with individual speakers (Wooters and Huijbregts, 2007). Here it
is natural to consider an HDP-HMM model, where a state corresponds to an
individual speaker and the observations are again short-term spectral represen-
tations. Posterior inference in the HDP-HMM yields estimates of the spectral
content of each speakers voice, an estimate of the number of speakers partici-
pating in the meeting, and a diarization of the audio stream.

Such an application of the HDP-HMM has been presented by Fox et al.
(2009), who showed that the HDP-HMM approach yielded a state-of-the-art
diarization method. A noteworthy aspect of their work is that they found that
the special treatment of self-transitions discussed in the previous section was
essential; without this special treatment the HDP-HMMs tendency to rapidly
switch among redundant states led to poor speaker diarization performance.

17

3.1.2 Word Segmentation
As another application of the HDP-HMM to speech, consider the problem of
segmenting an audio stream into a sequence of words. Speech is surprisingly
continuous with few obvious breaks between words and the problem of word seg-
mentationthat of identifying coherent segments of words and their bound-
aries in continuous speechis nontrivial. Goldwater et al. (2006b) proposed
a statistical approach to word segmentation based upon the HDP-HMM. The
latent states of the HMM correspond to words. An HDP-HMM rather than a
parametric HMM is required for this problem, since there are an unbounded
number of potential words.

In the model, an utterance is viewed as a sequence of phonemes, 1, 2, . . . ,  .
The sequence is modeled by an HDP-HMM in which words are the latent states.
A word is itself a sequence of phonemes. The model specication is as follows.
First, the number of words n is drawn from a geometric distribution. Then a
sequence of n words, 1, 2, . . . , n, is drawn from an HDP-HMM:

G0 | , H  DP(, H)
G | , G0  DP(, G0)

i | i1, Gi1

 Gi1

(26)

for   
for i = 1, . . . , n.

where 0  G is a draw from an initial state distribution. Each G is the
transition distribution over next words, given the previous word . This is
dened for every possible word , with  the set of all possible words (including
the empty word 0 which serves as an initial state for the Markov chain). The
base measure H over words is a simple independent phonemes model: the length
of the word, l  1, is rst drawn from another geometric distribution, then each
phoneme ri is drawn independently from a prior over phonemes:

l(cid:89)

H( = (r1, r2, . . . , rl)) = 0(1  0)l1

H0(rt),

(27)

t=1

where H0 is a probability measure over individual phonemes. The probability
of the observed utterance is then a sum over probabilities of sequences of words
such that their concatenation is 1, 2, . . . ,  .

Goldwater et al. (2006b) have shown that this HDP-HMM approach leads

to signicant improvements in segmentation accuracy.

3.1.3 Trees and Grammars
A number of other structured probabilistic objects are amenable to a nonpara-
metric treatment based on the HDP. In this section we briey discuss some
recent developments which go beyond the chain-structured HMM to consider
objects such as trees and grammars.

A hidden Markov tree (HMT) is a directed tree in which the nodes correspond
to states, and in which the probability of a state depends (solely) on its unique

18

parent in the tree. To each state there is optionally associated an observation,
where the probability of the observation is conditionally independent of the
other observations given the state (Chou et al., 1994).

We can generalize the HDP-HMM to a hierarchical Dirichlet process hidden
Markov tree (HDP-HMT) model in which the number of states is unbounded.
This is achieved by a generalization of the HDP-HMM model in which the tran-
sition matrix along each edge of the HMT is replaced with sets of draws from
a DP (one draw for each row of the transition matrix) and these DPs are tied
with the HDP. This model has been applied to problems in image processing
(denoising, scene recognition) in which the HDP-HMT is used to model corre-
lations among wavelet coecients in multiresolution models of images (Kivinen
et al., 2007a,b).

As a further generalization of the HDP-HMM, several groups have considered
nonparametric versions of probabilistic grammars (Johnson et al., 2007; Liang
et al., 2007; Finkel et al., 2007). These grammars consist of collections of rules, of
the form A  BC, where this transition from a symbol A to a pair of symbols
BC is modeled probabilistically. When the number of grammar symbols is
unknown a priori, it is natural to use the HDP to generate symbols and to tie
together the multiple occurrences of these symbols in a parse tree.

4 Hierarchical Pitman-Yor Processes
As discussed in Chapter ??, a variety of alternatives to the DP have been ex-
plored in the Bayesian nonparametrics literature. These alternatives can provide
a better t to prior beliefs than the DP. It is therefore natural to consider hi-
erarchical models based on these alternatives. In this section we shall describe
one such hierarchical model, the hierarchical Pitman-Yor (HPY) process, which
is based on the Pitman-Yor process (also known as the two-parameter Poisson-
Dirichlet process). We briey describe the Pitman-Yor process here; Section ??
in Chapter ?? as well as Perman et al. (1992), Pitman and Yor (1997) and Ish-
waran and James (2001) present further material on the Pitman-Yor process.
In Section 4.3.1 we describe an application of the HPY process to language
modeling. Section 4.3.2 presents a spatial extension of the HPY process and an
application to image segmentation.

4.1 Pitman-Yor Processes
The Pitman-Yor process is a two-parameter generalization of the DP, with a dis-
count parameter 0  d < 1 and a concentration parameter  > d. When d = 0
the Pitman-Yor process reduces to a DP with concentration parameter . We
write G  PY(d, , H) if G is a Pitman-Yor process with the given parameters
and base measure H. The stick-breaking construction and the Chinese restau-
rant process have natural generalizations in the Pitman-Yor process. A draw G

19

from the Pitman-Yor process has the following stick-breaking construction:

(cid:88)

G =

k

k

,

(28)

where the atoms 
follows:

k are drawn iid from H, and the weights are obtained as

k=1

vk | d,   Beta(1  d,  + kd)

for k = 1, . . . ,

(29)

k1(cid:89)
(1  vl).

l=1

k = vk

We refer to the joint distribution over 1, 2, . . . as the GEM(d, ) distribution,
this being a two-parameter generalization of the one-parameter GEM() asso-
ciated with the DP. Suppose that H is a smooth distribution and let 1, 2, . . .
be iid draws from G. Marginalizing out G, the distribution of i conditioned on
1, . . . , i1 follows a generalization of the Polya urn scheme:

i | 1, . . . , i1, d, , H  K(cid:88)

t=1

nt  d
 + i  1 

t

+  + Kd

 + i  1 H,

(30)

where 
t is the tth unique value among 1, . . . , i1, there being nt occurrences
of 
t , and K such unique values. In the Chinese restaurant analogy, each i is
t corresponds to a table, and customer i sits at table t if i = 
a customer, 
t .
There are two salient properties of this generalized Chinese restaurant process.
First, the rich-gets-richer property of the original Chinese restaurant process is
preserved, which means that there are a small number of large tables. Second,
there are a large number of small tables since the probability of occupying new
tables grows along with the number of occupied tables, and the discount d
decreases the probabilities of new customers sitting at small tables.

When 0 < d < 1 the Pitman-Yor process yields power-law behavior (Pitman,
2002; Goldwater et al., 2006a; Teh, 2006a, see also Chapter ??). It is this power-
law behavior which makes the Pitman-Yor process more suitable than the DP
for many applications involving natural phenomena. The power-law nature of
the Pitman-Yor process can be expressed in several ways. First, under Eq. (29)
we have E[k]  O(k1/d) if 0 < d < 1, which indicates that cluster sizes decay
according to a power law. Second, Zipfs Law can be derived from the Chinese
restaurant process; that is, the proportion of tables with n customers scales
as O(n1d). Finally the Chinese restaurant process also yields Heaps Law,
where the total number of tables in a restaurant with n customers scales as
O(nd). Note that the discount parameter d is the key parameter governing the
power-law behavior. These various power laws are illustrated in Figure 4.

20

Figure 4: Power-law behavior of the Pitman-Yor process. Left: E[k] vs. k.
Middle: number of tables in restaurant vs. number of customers. Right: number
of tables vs. number of customers at each table. Each plot shows the results of
10 draws (small dots) and their mean (large dots). The log-log plots are well
approximated by straight lines, indicating power laws.

4.2 Hierarchical Pitman-Yor Processes
The hierarchical Pitman-Yor (HPY) process is dened in the obvious manner:

G0 | , , H  PY(, , H)
Gj | d, , G0  PY(d, , G0)

for j  J ,

(31)

where G0 is the common base measure shared across the dierent Pitman-Yor
processes Gj, and is itself given a Pitman-Yor process prior. Similarly to the
HDP, this hierarchical construction generalizes immediately to a multiple-level
hierarchy.

Recall that one of the useful facts about the HDP is that it can be represented
using both a stick-breaking representation and a Chinese restaurant franchise
representation. It would be of interest to consider generalizations of these ob-
jects to the HPY process. As we shall see in the following, the Chinese restaurant
franchise can be readily generalized to an HPY analog. Unfortunately, however,
there is no known analytic form for the stick-breaking representation of the HPY
process.
Recall that in the Chinese restaurant franchise representation, each Gj cor-
responds to a restaurant, draws ji  Gj correspond to customers, tables t in
jt  G0, and dishes correspond to draws
restaurant j corresponds to draws 
k  H. Let njtk be the number of customers in restaurant j seated at table t

and eating dish k, mjk be the number of tables in restaurant j serving dish k,
and K be the number of dishes served throughout the franchise. The conditional
distributions given by the Chinese restaurant franchise for the HPY process are

21

as follows:

ji | j1, . . . , j,i1, , d, G0  mj(cid:88)
j,t1, , , H  K(cid:88)

, . . . , 

t=1

1m1

k=1

jt | 


11, . . . , 

njt  d
 + nj
mk  
 + m



jt



k

+  + mjd
 + nj
+  + K
 + m

G0

(32)

H,

(33)

which is a natural generalization of the CRF for the HDP (cf. Eq. (7) and
Eq. (8)).

4.3 Applications of the Hierarchical Pitman-Yor Process
In this section we describe an application of the HPY process to language mod-
eling and another application to image segmentation.

(cid:89)

4.3.1 Language Modeling
Statistical models of sentences in a natural language (e.g. English) are an in-
dispensable component of many systems for processing linguistic data, includ-
ing speech recognition, handwriting recognition and machine translation sys-
tems (Manning and Schutze, 1999). In this section we describe an application
of the hierarchical Pitman-Yor process in statistical language modeling.

Most statistical language models treat sentences as drawn from Markov mod-
els of xed order larger than one. That is, the probability of a sentence consisting
of a sequence of words (1, 2, . . . ,  ) is modeled as

p(1, . . . ,  ) =

p(t | tn+1, . . . , t1),

(34)

t=1

where for simplicity n+2, . . . , 0 are special start-of-sentence symbols, and
n  2 is one plus the order of the Markov model. Such models are known as
n-gram models. In typical applications n = 3, corresponding to a second-order
Markov model and a context consisting of just the previous two words.

In natural languages the size of the vocabulary typically consists of more
than 104 words. This means that in a 3-gram model the number of param-
eters is in excess of 1012, making maximum likelihood estimation infeasible.
In fact a nave prior treating parameters corresponding to dierent contexts
independently performs badly as wellit is important to model dependencies
across dierent contexts for a language model to be successful. In the language
modeling community such dependencies are achieved by a variety of heuristic
smoothing algorithms, which combine the counts associated with dierent con-
texts in various ways (Chen and Goodman, 1999).

It is also possible to take a hierarchical Bayesian point of view on smoothing,
and indeed such an approach was considered in a parametric setting by MacKay
and Peto (1994). However, word occurrences in natural languages tend to follow

22

power laws, and a nonparametric model such as the HPY process provides a
more natural prior for this domain (Teh, 2006a,b; Goldwater et al., 2006a).
Indeed, the most successful heuristic smoothing methods are closely related to
an HPY model.

Given a context u consisting of a sequence of words, let Gu be the distri-
bution over the next word following the context u. That is, Gu() = p(t =
 | tn+1, . . . , t1 = u) in Eq. (34). We place a Pitman-Yor prior on Gu, with
base measure Gpa(u), where pa(u) is the context with the rst word dropped
from u:

Gu | d|u|, |u|, Gpa(u)  PY(d|u|, |u|, Gpa(u)).

(35)

The parameters of the Pitman-Yor process depend on the length of the context
|u|. We recursively place a Pitman-Yor prior on Gpa(u), dropping words from
the front of the context until G, the distribution over next words given the
empty context . Finally we place a Pitman-Yor prior on G:

G | d0, 0, G0  PY(d0, 0, G0),

(36)

where G0 is the uniform distribution over the vocabulary. The structure of this
hierarchical prior reects the notion that more recent words in the context are
more informative in predicting the next word.

Teh (2006a,b) applied the HPY language model to a 14-million word corpus,
and found that it produces state-of-the-art prediction results, closely matching
results using interpolated and modied Kneser-Ney, two of the most widely-
used smoothing algorithms (Chen and Goodman, 1998). Moreover, the HPY
language model has been shown to outperform modied Kneser-Ney in the con-
text of an application to dialog transcription (Huang and Renals, 2007). These
results are unsurprising, as Teh (2006a,b) and Goldwater et al. (2006a) showed
that interpolated Kneser-Ney can be derived as an approximation to the CRF
representation of the HPY language model. In particular, interpolated Kneser-
Ney assumes that the number of tables in each restaurant serving each dish is
at most one. This is the same approximation as in Section 2.4.1.

4.3.2 Image Segmentation
Models based on the Pitman-Yor process have also had impact in the eld of
image processing, a eld that shares with the language modeling domain the fact
that power laws characterize many of the statistics within the domain. In par-
ticular, using a database of images that were manually segmented and labeled
by humans (Oliva and Torralba, 2001), Sudderth and Jordan (2009) have shown
that both the segment sizes and the label occurrences (e.g., sky, grass) fol-
low long-tailed distributions that are well captured by the Pitman-Yor process.
This suggests considering models in which the marginal distributions at each
site in an image are governed by Pitman-Yor processes. Moreover, to share
information across a collection of images it is natural to consider HPY priors.
In this section we describe a model based on such an HPY prior (Sudderth

23

and Jordan, 2009). Our focus is the problem of image segmentation, where the
observed data are a collection of images (an image is a collection of gray-scale
or color values at each point in a two-dimensional grid) and the problem is to
output a partition of each image into segments (a segment is a coherent region
of the image, as dened by human labelings).

Let us consider a generative model for image texture and color, simplifying
at rst in two ways: (1) we focus on a single image and (2) we neglect the
issue of spatial dependency within the image. Thus, for now we focus simply on
obtaining Pitman-Yor marginal statistics for segment sizes and segment labels
within a single image. Let us suppose that the image is represented as a large
collection of sites, where a site is a local region in the image (often referred to as
a pixel or a super-pixel). Let   GEM(d, ) be a draw from the two-parameter
GEM distribution. For each site i, let ti denote the segment assignment of site
i, where ti  Discrete() are independent draws from . Given a large number
of sites of equal size, the total area assigned to segment t will be roughly t,
and segment sizes will follow Pitman-Yor statistics.
We also assign a label to each segment, again using a two-parameter GEM
distribution. In particular, let   GEM(, ) be a distribution across labels.
For each segment t we label the segment by drawing kt  Discrete() indepen-
dently. We also let 
k denote an appearance model2 for label type k, where
the 
k are drawn from some prior distribution H. Putting this together, the
label assigned to site i is denoted kti
. The visual texture and color at site i are
then generated by a draw from the distribution 
kti

To obtain a spatially dependent Pitman-Yor process, Sudderth and Jordan
(2009) adapt an idea of Duan et al. (2007), who used a latent collection of
Gaussian processes to dene a spatially dependent set of draws from a Dirich-
let process. In particular, to each index t we associate a zero-mean Gaussian
process, ut. At a given site i, we thus have an innite collection of Gaussian
random variables, {uti}t=1,...,. By an appropriate choice of thresholds for this
innite sequence of Gaussian variables, it is possible to mimic a draw from the
distribution  (by basing the selection on the rst Gaussian variable in the se-
quence that is less than its threshold). Indeed, for a single site, this is simply
a change-of-variables problem from a collection of beta random variables to a
collection of Gaussian random variables. The Gaussian process framework cou-
ples the choice of segments at nearby sites via the covariance function. Figure 5
gives an example of three draws from this model, showing the underlying ran-
dom distribution  (truncated to four values), the corresponding collection of
draws from Gaussian processes (again truncated), and the resulting segmented
image.

.

This framework applies readily to multiple images by coupling the label
k across multiple images. Letting j  J
distribution  and appearance models 
index the images in the collection, we associate a segment distribution j with
each image and associate a set of Gaussian processes with each image to describe

2This parameter is generally a multinomial parameter encoding the probabilities of various

discrete-valued texture and color descriptors.

24

Figure 5: Draws from dependent Pitman-Yor processes. Top: the random pro-
portions j. Middle: draws from Gaussian processes, one for each entry in j.
Bottom: resulting segmentation.

the segmentation of that image.

The image segmentation problem can be cast as posterior inference in this
HPY-based model. Given an image represented as a collection of texture and
color descriptors, we compute the maximum a posteriori set of segments for
the sites. Sudderth and Jordan (2009) have shown that this procedure yields a
state-of-the-art unsupervised image segmentation algorithm.

5 The Beta Process and the Indian Buet Pro-

cess

The DP mixture model embodies the assumption that the data can be parti-
tioned or clustered into discrete classes. This assumption is made particularly
clear in the Chinese restaurant representation, where the table at which a data
point sits indexes the class (the mixture component) to which it is assigned. If
we represent the restaurant as a binary matrix in which the rows are the data
points and the columns are the tables, we obtain a matrix with a single one in
each row and all other elements equal to zero.

A dierent assumption that is natural in many settings is that objects can be

25

described in terms of a collection of binary features or attributes. For example,
we might describe a set of animals with features such as diurnal/nocturnal,
avian/non-avian, cold-blooded/warm-blooded, etc. Forming a binary matrix
in which the rows are the objects and the columns are the features, we obtain
a matrix in which there are multiple ones in each row. We will refer to such a
representation as a featural representation.

A featural representation can of course be converted into a set of clusters
if desired: if there are K binary features, we can place each object into one of
2K clusters. In so doing, however, we lose the ability to distinguish between
classes that have many features in common and classes that have no features
in common. Also, if K is large, it may be infeasible to consider models with
2K parameters. Using the featural representation, we might hope to construct
models that use on the order of K parameters to describe 2K classes.

In this section we discuss a Bayesian nonparametric approach to featural
representations. In essence, we replace the Dirichlet/multinomial probabilities
that underlie the Dirichlet process with a collection of beta/Bernoulli draws.
This is achieved via the beta process, a stochastic process whose realizations
provide a countably innite collection of coin-tossing probabilities. We also
discuss some other representations of the beta process that parallel those for
the DP. In particular we describe a stick-breaking construction as well as an
analog of the Chinese restaurant process known as the Indian buet process.

5.1 The Beta Process and the Bernoulli Process
The beta process is an instance of a general class of stochastic processes known
as completely random measures (Kingman, 1967, see also Chapter ??). The key
property of completely random measures is that the random variables obtained
by evaluating a random measure on disjoint subsets of the probability space
are mutually independent. Moreover, draws from a completely random measure
are discrete (up to a xed deterministic component). Thus we can represent
such a draw as a weighted collection of atoms on some probability space, as
we do for the DP. (Note, however, that the DP is not a completely random
measure because the weights are constrained to sum to one for the DP; thus,
the independence assertion does not hold for the DP. The DP can be obtained
by normalizing a completely random measure (specically the gamma process;
see ??).

Applications of the beta process in Bayesian nonparametric statistics have
mainly focused on its use as a model for random hazard functions (Hjort, 1990,
see also Chapter ??).
In this case, the probability space is the real line and
it is the cumulative integral of the sample paths that is of interest (yielding a
random, nondecreasing step function). In the application of the beta process to
featural representations, on the other hand, it is the realization itself that is of
interest and the underlying space is no longer restricted to be the real line.

Following Thibaux and Jordan (2007), let us thus consider a general proba-
bility space (, ) endowed with a nite base measure B0 (note that B0 is not
a probability measure; it does not necessarily integrate to one). Intuitively we

26

path and the red curve is the corresponding cumulative integral(cid:82) x

Figure 6: (a) A draw B  BP(1, U[0, 1]). The set of blue spikes is the sample
 B(d). (b)
100 samples from BeP(B), one sample per row. Note that a single sample is a
set of unit-weight atoms.

wish to partition  into small regions, placing atoms into these regions accord-
ing to B0 and assigning a weight to each atom, where the weight is a draw from a
beta distribution. A similar partitioning occurs in the denition of the DP, but
in that case the aggregation property of Dirichlet random variables immediately
yields a consistent set of marginals and thus an easy appeal to Kolmogorovs
theorem. Because the sum of two beta random variables is not a beta random
variable, the construction is somewhat less straightforward in the beta process
case.

The general machinery of completely random processes deals with this issue
in an elegant way. Consider rst the case in which B0 is absolutely continuous
and dene the Levy measure on the product space [0, 1]   in the following
way:

(d, d) = c1(1  )c1dB0(d),

(37)
where c > 0 is a concentration parameter. Now sample from a nonhomogeneous
Poisson process with the Levy measure  as its rate measure. This yields a
set of atoms at locations (1, 1), (2, 2) . . .. Dene a realization of the beta
process as:

(cid:88)

B =

kk

,

(38)

k=1

where k
is an atom at k with k its mass in B. We denote this stochastic
process as B  BP(c, B0). Figure 6(a) provides an example of a draw from
BP(1, U[0, 1]), where U[0, 1] is the uniform distribution on [0, 1].

We obtain a countably innite set of atoms from this construction because
the Levy measure in Eq. (37) is -nite with innite mass. Indeed, consider

27

0101201050Drawpartitioning the product space [0, 1] into stripes having equal integral under
this density. These stripes have the same nite rate under the Poisson process,
and there are an innite number of such stripes. Note also that the use of a
limiting form of the beta density implies that most of the atoms are associated
with very small weights. Campbells Theorem shows that the sum of these

weights is nite with probability one, since(cid:82) (d, d) < .

If B0 contains atoms, then these are treated separately. In particular, denote
the measure of the kth atom as qk (assumed to lie in (0, 1)). The realization B
necessarily contains that atom, with the corresponding weight k dened as an
independent draw from Beta(cqk, c(1  qk)). The overall realization B is a sum
of the weighted atoms coming from the continuous component and the discrete
component of B0.

Let us now dene a Bernoulli process BeP(B) with an atomic base measure
B as a stochastic process whose realizations are collections of atoms of unit
mass on . Atoms can only appear at the locations of atoms of B. Whether
or not an atom appears is determined by independent tosses of a coin, where
the probability of success is the corresponding weight of the atom in B. After n
draws from BeP(B) we can ll a binary matrix that has n rows and an innite
number of columns (corresponding to the atoms of B arranged in some order).
Most of the entries of the matrix are zero while a small (nite) number of the
entries are equal to one. Figure 6(b) provides an example.

The beta process and the Bernoulli process are conjugate. Consider the

specication:

B | c, B0  BP(c, B0)
Zi | B  BeP(B),

for i = 1, . . . , n,

(39)

where Z1, . . . , Zn are conditionally independent given B. The resulting posterior
distribution is itself a beta process, with updated parameters:

(cid:33)

n(cid:88)

i=1

Zi

.

(40)

(cid:32)

B | Z1, . . . , Zn, c, B0  BP

c + n,

c

c + n

B0 +

1

c + n

a  a +(cid:80)

This formula can be viewed as an analog of standard nite-dimensional beta/Bernoulli
updating. Indeed, given a prior Beta(a, b), the standard update takes the form
i zi. In Eq. (40), c plays the role of a + b and

i zi and b  b + n (cid:80)

cB0 is analogous to a.

5.2 The Indian Buet Process
Recall that the Chinese restaurant process can be obtained by integrating out
the Dirichlet process and considering the resulting distribution over partitions.
In the other direction, the Dirichlet process is the random measure that is guar-
anteed (by exchangeability and De Finettis theorem) to underlie the Chinese
restaurant process. In this section we discuss the analog of these relationships
for the beta process.

28

We begin by dening a stochastic process known as the Indian buet process
(IBP). The IBP was originally dened directly as a distribution on (equivalence
classes of) binary matrices by Griths and Ghahramani (2006) and Ghahra-
mani et al. (2007). The IBP is an innitely exchangeable distribution on these
equivalence classes, thus it is of interest to discover the random measure that
must underlie the IBP according to De Finettis Theorem. Thibaux and Jordan
(2007) showed that the underlying measure is the beta process; that is, the IBP
is obtained by integrating over the beta process B in the hierarchy in Eq. (39).
The IBP is dened as follows. Consider an Indian buet with a countably-
innite number of dishes and customers that arrive in sequence in the buet line.
Let Z denote a binary-valued matrix in which the rows are customers and the
columns are the dishes, and where Z
nk = 1 if customer n samples dish k. The
rst customer samples Poisson() dishes, where  = B0() is the total mass
c+n1, where
of B0. A subsequent customer n samples dish k with probability mk
mk is the number of customers who have previously sampled dish k; that is,
nk  Bernoulli( mk
Z
c+n1). Having sampled from the dishes previously sampled
by other customers, customer n then goes on to sample an additional number
of new dishes determined by a draw from a Poisson(
To derive the IBP from the beta process, consider rst the distribution
Eq. (40) for n = 0; in this case the base measure is simply B0. Drawing from B 
BP(B0) and then drawing Z1  BeP(B) yields atoms whose locations are dis-
tributed according to a Poisson process with rate B0; the number of such atoms
is Poisson(). Now consider the posterior distribution after Z1, . . . , Zn1 have
been observed. The updated base measure is
i=1 Zi. Treat
the discrete component and the continuous component separately. The discrete
component,
i=1 Zi, can be reorganized as a sum over the unique values
of the atoms; let mk denote the number of times the kth atom appears in one of
the previous Zi. We thus obtain draws k  Beta((c+n1)qk, (c+n1)(1qk)),
where qk = mk
c+n1 and thus (under Bernoulli
sampling) this atom appears in Zn with probability mk
c+n1. From the continu-
ous component,
c+n1 ) new atoms. Equating
atoms with dishes, and rows of Z with draws Zn, we have obtained exactly
the probabilistic specication of the IBP.

c+n1. The expected value of k is mk

c+n1 B0, we generate Poisson(

c+n1 ) distribution.

c

c+n1 B0 + 1

c+n1

(cid:80)n1

c

c

c

(cid:80)n1

1

c+n1

5.3 Stick-Breaking Constructions
The stick-breaking representation of the DP is an elegant constructive character-
ization of the DP as a discrete random measure (Chapter ??). This construction
can be viewed in terms of a metaphor of breaking o lengths of a stick, and it
can also be interpreted in terms of a size-biased ordering of the atoms.
In
this section, we consider analogous representations for the beta process. Draws
B  BP(c, B0) from the beta process are discrete with probability one, which
gives hope that such representations exist.
Indeed, we will show that there
are two stick-breaking constructions of B, one based on a size-biased ordering
of the atoms (Thibaux and Jordan, 2007), and one based on a stick-breaking
representation known as the inverse Levy measure (Wolpert and Ickstadt, 1998).

29

The size-biased ordering of Thibaux and Jordan (2007) follows straightfor-
wardly from the discussion in Section 5.2. Recall that the Indian buet process
is dened via a sequence of draws from Bernoulli processes. For each draw, a
Poisson number of new atoms are generated, and the corresponding weights in
the base measure B have a beta distribution. This yields the following truncated
representation:

N(cid:88)

Kn(cid:88)

BN =

nknk

,

n=1

k=1

where

Kn | c, B0  Poisson(

c+n1 )
nk | c  Beta(1, c + n  1)
nk | B0  B0/.

c

(41)

(42)

for n = 1, . . . ,
for k = 1, . . . , Kn

c

c

It can be shown that this size-biased construction BN converges to B with prob-
ability one. The expected total weight contributed at step N is
(c+N )(c+N1),
while the expected total weight remaining, in B  BN , is
c+N . The expected
total weight remaining decreases to zero as N  , but at a relatively slow
rate. Note also that we are not guaranteed that atoms contributed at later
stages of the construction will have small weightthe sizes of the weights need
not be in decreasing order.

The stick-breaking construction of Teh et al. (2007) can be derived from the
inverse Levy measure algorithm of Wolpert and Ickstadt (1998). This algorithm
starts from the Levy measure of the beta process, and generates a sequence of
weights of decreasing size using a nonlinear transformation of a one-dimensional
Poisson process to one with uniform rate. In general this approach does not lead
to closed forms for the weights; inverses of the incomplete Beta function need to
be computed numerically. However for the one-parameter beta process (where
c = 1) we do obtain a simple closed form:

(43)

(44)

for k = 1, . . . ,

K(cid:88)

BK =

kk

,

where

k=1

vk |   Beta(1, )
k(cid:89)
(1  vl)
k | B0  B0/.

k =

l=1

Again BK  B as K  , but in this case the expected weights decrease
exponentially to zero. Further, the weights are generated in strictly decreasing
order, so we are guaranteed to generate the larger weights rst.

30

Figure 7: Stick-breaking construction for the DP and the one-parameter BP.
The lengths i are the weights for the DP and the lengths i are the weights
for the BP.

The stick-breaking construction for the one-parameter beta process has an
intriguing connection to the stick-breaking construction for the DP. In par-
ticular, both constructions use the same beta-distributed breakpoints vk; the
dierence is that for the DP we use the lengths of the sticks just broken o
as the weights while for the beta process we use the remaining lengths of the
sticks. This is depicted graphically in Figure 7.

5.4 Hierarchical Beta Processes
Recall the construction of the hierarchical Dirichlet process: a set of Dirichlet
processes are coupled via a random base measure. A similar construction can
be carried out in the case of the beta process:
let the common base measure
for a set of beta processes be drawn from an underlying beta process (Thibaux
and Jordan, 2007). Under this hierarchical Bayesian nonparametric model, the
featural representations that are chosen for one group will be related to the
featural representations that are used for other groups.

We accordingly dene a hierarchical beta process (HBP) as follows:

B0 | , A  BP(, A)
Bj | c, B0  BP(c, B0)
Zji | Bj  BeP(Bj)

(45)

for j  J
for i = 1, . . . , nj,

where J is the set of groups and there are nj individuals in group j. The
hyperparameter c controls the degree of coupling among the groups:
larger
values of c yield realizations Bj that are closer to B0 and thus a greater degree
of overlap among the atoms chosen in the dierent groups.
As an example of the application of the HBP, Thibaux and Jordan (2007)
considered the problem of document classication, where there are |J | groups
of documents and where the goal is to classify a new document into one of
these groups. In this case, Zji is a binary vector that represents the presence or
absence in the ith document of each of the words in the vocabulary . The HBP
yields a form of regularization in which the group-specic word probabilities

31

12344321are shrunk towards each other. This can be compared to standard Laplace
smoothing, in which word probabilities are shrunk towards a xed reference
point. Such a reference point can be dicult to calibrate when there are rare
words in a corpus, and Thibaux and Jordan (2007) showed empirically that the
HBP yielded better predictive performance than Laplace smoothing.

5.5 Applications of the Beta Process
In the following sections we describe a number of applications of the beta process
to hierarchical Bayesian featural models. Note that this is a rather dierent class
of applications than the traditional class of applications of the beta process to
random hazard functions.

5.5.1 Sparse Latent Variable Models
Latent variable models play an essential role in many forms of statistical anal-
ysis. Many latent variable models take the form of a regression on a latent
vector; examples include principal component analysis, factor analysis and in-
dependent components analysis. Paralleling the interest in the regression liter-
ature in sparse regression models, one can also consider sparse latent variable
models, where each observable is a function of a relatively small number of la-
tent variables. The beta process provides a natural way of constructing such
models. Indeed, under the beta process we can work with models that dene
a countably-innite number of latent variables, with a small, nite number of
variables being active (i.e., non-zero) in any realization.

Consider a set of n observed data vectors, x1, . . . , xn. We use a beta process
to model a set of latent features, Z1, . . . , Zn, where we capture interactions
among the components of these vectors as follows:

B | c, B0  BP(c, B0)
Zi | B  BeP(B)

for i = 1, . . . , n.

(46)

As we have seen, realizations of beta and Bernoulli processes can be expressed
as weighted sums of atoms:

(cid:88)
(cid:88)

k=1

B =

Zi =

kk

Z
ikk

.

(47)

k=1

We view k as parametrizing feature k, while Zi denotes the features that are
active for item i. In particular, Z
ik = 1 if feature k is active for item i. The
data point xi is modeled as follows:

yik | H  H

xi | Zi, , yi  F{k,yik}k:Z

ik

for k = 1, . . . ,

(48)

,

=1

32

ik

=1

where yik is the value of feature k if it is active for item i, and the distribu-
tion F{k,yik}k:Z
depends only on the active features, their values, and their
parameters.

Note that this approach denes a latent variable model with an innite
number of sparse latent variables, but for each data item only a nite number
of latent variables are active. The approach would often be used in a predictive
setting in which the latent variables are integrated out, but if the sparseness
pattern is of interest per se, it is also possible to compute a posterior distribution
over the latent variables.

observation of the linear combination (cid:80)

There are several specic examples of this sparse latent variable model in the
literature. One example is an independent components analysis model with an
innite number of sparse latent components (Knowles and Ghahramani, 2007;
Teh et al., 2007), where the latent variables are real-valued and xi is a noisy
ikyikk. Another example is the
noisy-or model of Wood et al. (2006), where the latent variables are binary
and are interpreted as presence or absence of diseases, while the observations xi
are binary vectors indicating presence or absence of symptoms.

k Z

5.5.2 Relational Models
The beta process has also been applied to the modeling of relational data (also
known as dyadic data). In the relational setting, data are relations among pairs
of objects (Getoor and Taskar, 2007); examples include similarity judgments
between two objects, protein-protein interactions, user choices among a set of
options, and ratings of products by customers.

We rst consider the case in which there is a single set of objects and relations
are dened among pairs of objects in that set. Formally, dene an observation as
a relation xij between objects i and j in a collection of n objects. Each object is
modeled using a set of latent features as in Eq. (46) and Eq. (47). The observed
relation xij between objects i and j then has a conditional distribution that is
dependent only on the features active in objects i and j. For example, Navarro
and Griths (2007) modeled subjective similarity judgments between objects
jk; note that this is
a weighted sum of features active in both objects. Chu et al. (2006) modeled
high-throughput protein-protein interaction screens where the observed bind-
ing anity of proteins i and j is related to the number of overlapping features
jk, with each feature interpreted as a potential protein complex con-
sisting of proteins containing the feature. Gorur et al. (2006) proposed a non-
parametric elimination by aspects choice model where the probability of a user
choosing object i over object j is modeled as proportional to a weighted sum,
jk), across features active for object i that are not active for
object j. Note that in these examples, the parameters of the model, k, are the
atoms of the beta process.

i and j as normally distributed with mean (cid:80)
(cid:80)
(cid:80)

ik(1  Z

k=1 kZ

k=1 Z

k=1 kZ

ikZ

ikZ

Relational data involving separate collections of objects can be modeled with
the beta process as well. Meeds et al. (2007) modeled movie ratings, where the
collections of objects are movies and users, and the relational data consists of

33

ratings of movies by users. The task is to predict the ratings of movies not
yet rated by users, using these predictions to recommend new movies to users.
These tasks are called recommender systems or collaborative ltering. Meeds
et al. (2007) proposed a featural model where movies and users are modeled
using separate IBPs. Let Z be the binary matrix of movie features and Y 
the matrix of user features. The rating of movie i by user j is modeled as
jl. Note that this dyadic
model cannot be represented using two independent beta processes, since there
is a parameter kl for each combination of features in the two IBPs. The question
of what random measure underlies this model is an interesting one.

normally distributed with mean (cid:80)

(cid:80)

l=1 klZ

ikY 

k=1

6 Semiparametric Models
The nonparametric priors introduced in previous sections can be combined with
more traditional nite-dimensional priors, as well as hierarchies of such priors.
In the resulting semiparametric models, the object of inference may be the
nite-dimensional parameter, with the nonparametric component treated as a
nuisance parameter to be integrated out. In other cases, the nite-dimensional
parameters are to be integrated out and aspects of the nonparametric component
are the inferential focus. In this section we describe two such semiparametric
models based on the HDP. The rst model couples the stick-breaking represen-
tation of the HDP with a Gaussian hierarchy, while the other is based on the
Chinese restaurant franchise representation of the HDP.

6.1 Hierarchical DPs with Random Eects
An important characteristic of the HDP is that the same atoms appear in dif-
ferent DPs, allowing clusters to be shared across the dierent groups. The hier-
archical DP with random eects (HDP+RE) model of Kim and Smyth (2007)
generalizes the HDP by allowing atoms in dierent DPs to dier from each other
to better capture group-specicity of cluster parameters. This model is based
on the stick-breaking representation for HDPs. We begin with the standard
representation for the common random base measure G0  DP(, H):

 |   GEM()
k | H  H
(cid:88)


G0 =

k

k

k=1

for k = 1, . . . ,

(49)

.

34

For each group j  J , the weights and atoms for the group-specic Gj dier
from G0 in the following way:

j |   DP(, )
jk | 


k  T
(cid:88)
Gj =

k

jk

jk

,

for j  J
for k = 1, . . . ,

(50)

k=1

where T is a distribution centered at ; for example, T might be a normal
distribution with mean .

Kim and Smyth (2007) used the HDP+RE model to model bumps in func-
tional magnetic resonance imaging (fMRI) data.
fMRI analyses report areas
of high metabolic activity in the brain that are correlated with external stim-
uli in an attempt to discover the function of local brain regions. Such areas
of activities often show up as bumps in fMRI images, and each bump can be
modeled well using a normal density. An fMRI image then consists of multiple
bumps and can be modeled with a DP mixture. Each individual brain might
have slightly dierent structure and might react dierently to the same stimuli,
while each fMRI machine has dierent characteristics. The HDP+RE model
naturally captures such variations while sharing statistical strength across indi-
viduals and machines.

6.2 Analysis of Densities and Transformed DPs
In this section we describe another approach to introducing group-specic pa-
rameters within the DP framework. The common base measure G0 is still given
a DP prior as in Eq. (49), while the group-specic random measures are dened
dierently:

(cid:88)

H0 =

kT
Gj | H0  DP(, H0)

k=1

k

(51)

for j  J .

In the particular case in which H and T are normal distributions with xed
variances, this model has been termed the analysis of densities (AnDe) model
by Tomlinson and Escobar (2003), who used it for sharing statistical strength
among multiple density estimation problems.

Sudderth et al. (2008) called the model given by Eq. (49) and Eq. (51) a
transformed DP. The transformed DP is very similar to an HDP, the dierence
being that the atoms in G0 are replaced by distributions parametrized by the
atoms. If these distributions are smooth the measures Gj will not share atoms
as in the HDP. Instead each atom in Gj is drawn from T
with probability k.
Identifying an atom of Gj with 
k , the Chinese restaurant franchise representa-
tion for the HDP can be generalized to the transformed DP. We have customers
(draws from Gj) going into restaurants (Gj) and sitting around tables (draws

k

35

from H0), while tables are served dishes (atoms in G0) from a franchise-wide
menu (G0). In the HDP the actual dish served at the dierent tables that order
the same dish are identical. For the transformed DP the dishes that are served
at dierent tables ordering the same dish on the menu can take on distinct
values.

Sudderth et al. (2008) used the transformed DP as a model for visual scene
analysis that can simultaneously segment, detect and recognize objects within
the scenes. Each image is rst preprocessed into a set of low-level descriptors
of local image appearances, and Eq. (49) and Eq. (51) are completed with a
mixture model for these descriptors:

ji | Gj  Gj
xji | ji  Fji

,

for j  J and i = 1, . . . , nj

(52)

where xji is one of nj image descriptors in image j and Fji
over image descriptors parameterized by ji.

is a distribution

The Chinese restaurant franchise representation of the transformed DP trans-
lates to a hierarchical representation of visual scenes, with scenes consisting of
multiple objects and objects consisting of descriptors of local image appear-
ances. To see this, note that customers (xji) are clustered into tables (object
instances), and tables are served dishes from a global menu (each object in-
stance belongs to an object category). There could be multiple tables in the
same restaurant serving variations (dierent seasonings) on a given dish from
the global menu. This corresponds to the fact that there could be multiple in-
stances of the same object category in a single visual scene, with each instance
being in a dierent location or having dierent poses or lighting conditions (thus
yielding transformed versions of an object category template).

7

Inference for Hierarchical Bayesian Nonpara-
metric Models

In this section we discuss algorithmic aspects of inference for the hierarchical
Bayesian nonparametric models that we have discussed in earlier sections. Our
treatment will be brief and selective; in particular, we focus on relatively simple
algorithms that help to convey basic methodology and provide a sense of some
of the options that are available. An underlying theme of this section is that
the various mathematical representations available for nonparametric models
including stick-breaking representations, urn models and truncationscan be
combined in various ways to yield a wide range of possible algorithmic imple-
mentations.

While we focus on sampling-based inference methods throughout this sec-
tion, we also note that there is a growing literature on variational methods for in-
ference in hierarchical Bayesian nonparametric models; examples include Liang
et al. (2007); Sudderth and Jordan (2009) and Teh et al. (2008).

36

Inference for Hierarchical Dirichlet Processes

7.1
We begin by considering posterior inference for a simple HDP mixture model. In
this model, the random measures Gj are drawn from an HDP model according
to Eq. (1), and this HDP prior is completed as follows:

ji | Gj  Gj
xji | ji  Fji

,

for i = 1, . . . , nj

(53)

where the ith observation in the jth group is denoted xji and where this obser-
vation is drawn from a distribution Fji
indexed by ji. The latent parameter
ji is drawn from Gj and can be viewed as indexing the mixture component
associated with the data point xji. We shall assume that H is conjugate to
F for simplicity. Nonconjugate models can be treated by adapting techniques
from the DP mixture literature (cf. Neal, 2000).

Teh et al. (2006) presented sampling algorithms for the HDP mixture model
based on both the CRF representation and the stick-breaking representation.
In the following section we describe the CRF-based sampler. We then turn to
an alternative sampler that is based on the posterior representation of the HDP
described in Section 2.3.

7.1.1 Chinese Restaurant Franchise Sampler
Recall our notation for the CRF representation of the HDP. Customer i in
restaurant j is associated with an iid draw from Gj and sits at table tji. Table t
in restaurant j is associated with an iid draw from G0 and serves a dish kjt from
a franchise-wide menu. Dish k is associated with an iid draw from H. If H is an
absolutely continuous measure then each such dish is unique with probability
one. There are njtk customers in restaurant j sitting at table t and eating dish
k, and there are mjk tables in restaurant j serving dish k.

Given this setup, we describe a Gibbs sampler in which the table and dish
assignment variables are iteratively sampled conditioned on the state of all other
variables. The variables consist of {tji}jJ ,i=1,...,nj
and {kjt}jJ ,t=1,...,mj. The
parameters ji are integrated out analytically (recall our assumption of conju-
gacy). Consider the assignment of customer i in restaurant j to a table tji. To
resample tji we make use of exchangeability and imagine customer i being the
last customer to enter restaurant j. The customer can sit at an already occupied
table, can sit at a new table and be served an existing dish, or can sit at a new
table and be served a new dish. The probabilities of these events are:



with probability  nji
tji = t
jt
nji
j +
with probability  
tji = tnew, kjtnew = k
nji
j +
tji = tnew, kjtnew = knew with probability  
nji
j +

37

fkjt
mji
k
mji
 +

mji

 +

({xji})

fk({xji})
fknew({xji}),

(54)

where tnew and knew denote a new table and new dish, respectively, and where
superscript ji denotes counts in which customer i in restaurant j is removed
from the CRF (if this empties a table we also remove that table from the CRF
along with the dish served on it). The fractional terms are the conditional
priors given by the CRF in Eq. (7) and Eq. (8), and fk({xji}) is dened using
the following general notation:

(cid:90)
(cid:90)

h() (cid:89)
h() (cid:89)

j(cid:48)i(cid:48)DkD

j(cid:48)i(cid:48)Dk\D

fk({xji}jiD) =

f(xj(cid:48)i(cid:48))d

,

f(xj(cid:48)i(cid:48))d

(55)

where D is an arbitrary index set, where Dk = {j(cid:48)i(cid:48) : kj(cid:48)t
j(cid:48) i(cid:48) = k} denotes the
set of indices of data items currently associated with dish k, and where h() and
f() denote the densities of H and F respectively. In particular, fk({xji}) is
the marginal conditional probability of the singleton data point xji in cluster k,
given all of the other data points currently assigned to cluster k.

The Gibbs update for the dish kjt served at table t in restaurant j is derived

similarly. The probabilities of the relevant events in this case are:
fk({xji : tji = t})
fknew({xji : tji = t}).

with probability  mjt
k
mjt
 +

mjt

knew with probability 

kjt =

 +

k

(56)

While the computational cost of the Gibbs updates is generally dominated
by the computation of the marginal conditional probabilities fk(), the number
of possible events that can occur at one Gibbs step is one plus the total number
of tables or dishes in all restaurants that are ancestors of j, and this number
can be large in deep or wide hierarchies.

A drawback of the CRF sampler is that it couples sampling in the various
restaurants (since all DPs are integrated out). This coupling makes deriving a
CRF sampler for certain models (e.g. the HDP-HMM) dicult. An alternative
is to construct samplers that use a mixed representationsome DPs in stick-
breaking representation and some in CRP representationand thereby decouple
the restaurants (Teh et al., 2006).

The CRF-based sampler can be easily extended to arbitrary hierarchies.
It can also be extended to the hierarchical Pitman-Yor process discussed in
Section 4.

7.1.2 Posterior Representation Sampler
In Section 2.3 we showed that the posterior of the HDP consists of a discrete
part corresponding to mixture components associated with data and a contin-
uous part corresponding to components not associated with data. This repre-
sentation can be used to develop a sampler which represents only the discrete
part explicitly. In particular, referring to Eq. (10) and Eq. (12), the posterior

38

representation sampler maintains only the weights  and {j}jJ . (The atoms
{
k }k=1,...,K can be integrated out in the conjugate setting.) We also make use
of cluster index variables zji, dened so that ji = 
in the
zji
CRF representation).
The sampler iterates between two phases: the sampling of the cluster indices
{zji}, and the sampling of the weights  and {j}. The sampling of the cluster
indices is a simple variation on the Gibbs updates in the CRF sampler described
in Section 7.1.1. In particular, we dene the following Gibbs conditionals:

(i.e., zji = kjtji

(cid:40)k

zji =

with probability  jkfk({xji})

knew with probability  j0fknew({xji}).

(57)

(58)

If a new component knew is chosen, the corresponding atom is instantiated in
the sampler. Specically, the weights corresponding to this new atom can be
generated as follows:

for j  J

v0 |   Beta(, 1)
K+1) = (0v0, 0(1  v0))

, new

(new
vj | , 0, v0  Beta(0v0, 0(1  v0))
j0 , new

0

(new

j K+1) = (j0vj, j0(1  vj)).
Finally we set zji = K + 1 and increment K.
The second phase resamples the weights {j}jJ and  conditioned on the
cluster indices {zji}. The approach is to rst integrate out the random measures,
leaving a CRP representation as in Section 2.2, then the weights {j}jJ and
 can be sampled conditionally on the state of the CRF using Eq. (10) and
Eq. (12). Because we are conditioning on {zji}, and customers with dierent
values of zji cannot be assigned to the same table, each restaurant eectively
gets split into independent sub-restaurants, one for each value of k.
(See
also the related direct assignment sampler in Teh et al. (2006).) Let njk be
the number of observations in group j assigned to component k, and let mjk
be the random number of tables in a sub-restaurant with njk customers and
concentration parameter k. The {mjk} are mutually independent and thus
a draw for each of them can be simulated using the CRP. We can now sample
the  and {j} using Eq. (10) and Eq. (12).

Inference for HDP Hidden Markov Models

7.2
The posterior representation sampler of Section 7.1.2 can also be used to derive
a Gibbs sampler for the HDP-HMM. Consider the formulation of the HDP-
HMM given in Eq. (23) and Eq. (24) where we make use of a sequence of latent
indicator variables z1, . . . , z . We again assume that H is conjugate to F.
Note that the posterior of the HDP prior for the model (given z1, . . . , z ) can
be decomposed into a discrete part consisting of K atoms (corresponding to the
K states currently visited by z1, . . . , z ), as well as a continuous part consisting
of unused atoms. The weights on the K atoms (equivalently the transition

39

probabilities among the K states currently used by the HDP-HMM) can be
constructed from a CRF representation of the HDP:

(0, 1, . . . , K)  Dirichlet(, m1, . . . , mK)

(59)

for j = 1, . . . , K,

(j0, j1, . . . , jK)  Dirichlet(0, 1 + nj1, . . . , K + njK)
where njk is the number of transitions from state j to state k (equivalently the
number of customers eating dish k in restaurant j), while mk is the number of
tables serving dish k in the CRF representation of the HDP. The conditional
(cid:40)k
probabilities for the Gibbs update of zt are as follows:
with probability  zt1kkzt+1
knew with probability  zt10zt+1

fk({xt})
fknew({xt}).

zt =

(60)

The three factors on the right-hand side are the probability of transitioning into
the current state, the probability of transitioning out of the current state, and
the conditional probability of the current observation xt respectively. The zt+1
factor arises because transitions from the new state knew have not been observed
before so we need to use the conditional prior mean . The weights  and
transition probabilities j can be updated as for the posterior representation
sampler for plain HDPs.

This simple Gibbs sampler can converge very slowly due to strong depen-
dencies among the latent states (Scott, 2002). To obtain a faster algorithm we
would like to update the latent states in a block via the forward-backward al-
gorithm for HMMs; the traditional form of this algorithm cannot, however, be
applied directly to the HDP-HMM since there are an innite number of possi-
ble states. The solution is to limit the number of states to a nite number so
that the forward-backward algorithm becomes feasible. Fox et al. (2009) pro-
posed doing this via a truncation of the stick-breaking process (cf. Ishwaran and
James, 2001), while Van Gael et al. (2008) proposed a slice sampling approach
which adaptively limits the number of states to a nite number (Neal, 2003;
Walker, 2007).

Inference for Beta Processes

7.3
In this section we describe a Gibbs sampler for the beta process latent variable
model described in Section 5.5.1. This sampler is based on the stick-breaking
representation of the beta process.
Recall that the model is dened in terms of a set of feature weights {k}k=1,...,
and the atoms (feature parameters) {k}k=1,...,. Moreover, corresponding to
each data item xi, we have a set of binary feature activities {Z
ik}k=1,...,
and latent feature values {yik}k=1,...,. The observed data item xi depends on
{k, yik}k:Z
The conditional distributions dening the model are given in Eq. (44) and
ik = 1| k) = k. Gibbs sampling in this model is straight-
Eq. (48), where p(Z
forward except for a few diculties which we describe below along with their
resolution.

=1.

ik

40

The main diculty with a Gibbs sampler is that there are an innite number
of random variables that need to be sampled. To circumvent this problem,
Teh et al. (2007) propose to use slice sampling (Neal, 2003; Walker, 2007) to
adaptively truncate the representation to a nite number of features. Consider
an auxiliary variable s with conditional distribution:

(cid:20)

(cid:21)

K(cid:89)

s| Z,{k}k=1,...,  Uniform

0, min
k:i,Z

ik

=1

k

,

(61)

where the supremum in the range of s is the smallest feature weight k among
the currently active features. Conditioned on the current state of the other
variables a new value for s can easily be sampled. Conditioned on s, features
for which k < s are forced to be inactive since making them active would
make s lie outside its range. This means that we only need to update the nite
number of features for which k > s. This typically includes all the active
features, along with a small number of inactive features (needed for the sampler
to explore the use of new features).

A related issue concerns the representation of the model within the nite
memory of the computer. Using the auxiliary variable s it is clear that we need
only represent features 1, . . . , K, where K is such that K+1 < s; that is, the
model is truncated after feature K. As the values of s and the feature weights
change over the course of Gibbs sampling this value of K changes as well. If
K is decreased we simply delete the last few features, while if K is increased
we sample the variables k, k and yik corresponding to these new features
from their conditional distributions given the current state of the represented
features.

The nal issue is the problem of sampling the feature weights 1, . . . , K.
Unlike the case of DPs, it is easier in this case to work with the weights directly
instead of the stick-breaking variables vk. In particular, Teh et al. (2007) showed
that the joint probability for the weights is:

p(1, . . . , K) = I(0  K    1  1)K

K

1
k ,

(62)

where I() = 1 if the predicate is true and 0 otherwise. For k = 1, . . . , K  1 the
conditional probability of k given the other variables can be computed from
Eq. (62) and the conditional probability of Z
nk given k. For K we
also have to condition on Z
ik = 0 for all i and k > K; this probability can be
computed using the Levy-Khintchine representation for the beta process (Teh
et al., 2007).

1k, . . . , Z

k=1

Inference for Hierarchical Beta Processes

7.4
In this section we present an inference algorithm for the hierarchical beta pro-
cess given in Eq. (45). The observed data are the variables Zji; these binary
vectors denote (in the language of document classication) the presence or ab-
sence of words in document i of group j. The underlying measure space  is

41

interpreted as the vocabulary. (Each element in  is referred to as a word).
Let 1, . . . , K   denote the words that are observed among the documents.
That is, these are the    such that Zji() = 1 for some i and j.
Because both the beta and Bernoulli processes are completely random mea-
sures, the posterior over B0 and Bj for j  J decomposes into a discrete
part over the observed vocabulary {1, . . . , K} and a continuous part over
\{1, . . . , K}. The discrete part further factorizes over each observed word
k. Thus it is sucient to focus separately on inference for each observed word
and for the continuous part corresponding to unobserved words.

For a xed k, let a = A(k), 0 = B0(k), j = Bj(k) and zji = Zji(k).
The slice of the HBP corresponding to k has the following joint distribution:

0 | c0, a  Beta(c0a, c0(1  a))
j | cj, 0  Beta(cj0, cj(1  0))

zji | j  Bernoulli(j)

(63)

for j  J
for i = 1, . . . , nj.

Note that the prior over 0 is improper if A is continuous and a = 0. This beta
hierarchy is a special case of the nite Dirichlet hierarchy of Eq. (10) and Eq. (12)
and it is straightforward to use the posterior representation sampler described
in Section 7.1 to sample from the posterior given the observed zji. Thibaux
and Jordan (2007) described an alternative where the j are integrated out and
rejection sampling is used to sample from 0.

Finally, we consider the continuous part of the posterior. This component
is not simply the prior, since we have to condition on the fact that no words in
\{1, . . . , K} have been observed among the documents. Thibaux and Jordan
(2007) solved this problem by noting that the posterior factors over the levels
indexed by n in the size-biased ordering in Eq. (42). Focusing on each level
separately, they derived a posterior distribution on the number of atoms in each
level, combining this with the posterior over the level-specic weights to obtain
the overall posterior.

8 Discussion
Our goal in this chapter has been to place hierarchical modeling in the same
central role in Bayesian nonparametrics that it plays in other areas of Bayesian
statistics. Indeed, one of the principal arguments for hierarchical modeling in
parametric statistics is that it provides control over the large numbers of degrees
of freedom that arise, for example, in random eects models. Such an argument
holds a fortiori in the nonparametric setting.

Nonparametric priors generally involve hyperparameters, some of which are
nite-dimensional and some of which are innite-dimensional. Sharing the nite-
dimensional parameter among multiple draws from such a prior is a natural
modeling strategy that mimics classical hierarchical modeling concepts.
It is
our contention, however, that this form of control is far too limited, and that
the innite-dimensional parameters should generally also be shared. We have

42

made this point principally by considering examples in applied problem domains.
In domains such as computational vision, information retrieval and genetics,
nonparametric models provide natural descriptions of the complex objects under
study; in particular, it is natural to describe an image, a document or a genome
as the realization of a stochastic process. Now, in considering collections of
such objects it is natural to want to share details of the realization among the
objects in the collectionwe wish to share parts of objects, features, recurring
phrases and motifs. This can be achieved by coupling multiple draws from a
nonparametric prior via their innite-dimensional parameters.

Another advantage of hierarchical modeling in the classical setting is that
it expands the repertoire of distributional forms that can be considered. For
example, heavy-tailed distributions can be obtained by placing a prior on the
scale parameter of lighter-tailed distributions. Although this point has been
little explored to date in the nonparametric setting, we expect that it will be
a fruitful direction for further research. In particular, there are stringent com-
putational constraints that limit the nonparametric repertoire, and hierarchical
constructions oer one way forward. Indeed, as we have seen, computationally
oriented constructions such as urn models and stick-breaking representations
often carry over naturally to hierarchical nonparametric models.

Finally, it is worth noting a diculty that is raised by hierarchical mod-
eling. Although Bayesian hierarchies help to control hyperparameters, they
do not remove the need to specify distributions for hyperparameters. Indeed,
when hyperparameters are placed high in a hierarchy it can be dicult to give
operational meaning to such hyperparameters. One approach to coping with
this issue involves considering the marginal probabilities that are induced by a
nonparametric prior. For example, we argued that the marginals induced by a
Pitman-Yor prior exhibit long tails that provide a good match to the power-law
behavior found in textual data and image statistics. Further research is needed
to develop this kind of understanding for a wider range of hierarchical Bayesian
nonparametric models and problem domains.

8.1 Acknowledgements
We would like to thank David Blei, Jan Gasthaus, Sam Gershman, Tom Grif-
ths, Kurt Miller, Vinayak Rao and Erik Sudderth for their helpful comments
on the manuscript.

