Abstract

We present an automatic alignment procedure which maps the disparate
internal representations learned by several local dimensionality reduction
experts into a single, coherent global coordinate system for the original
data space. Our algorithm can be applied to any set of experts, each
of which produces a low-dimensional local representation of a high-
dimensional input. Unlike recent efforts to coordinate such models by
modifying their objective functions [1, 2], our algorithm is invoked after
training and applies an efcient eigensolver to post-process the trained
models. The post-processing has no local optima and the size of the sys-
tem it must solve scales with the number of local models rather than the
number of original data points, making it more efcient than model-free
algorithms such as Isomap [3] or LLE [4].

1 Introduction: Local vs. Global Dimensionality Reduction
Beyond density modelling, an important goal of unsupervised learning is to discover com-
pact, informative representations of high-dimensional data. If the data lie on a smooth low
dimensional manifold, then an excellent encoding is the coordinates internal to that man-
ifold. The process of determining such coordinates is dimensionality reduction. Linear
dimensionality reduction methods such as principal component analysis and factor analy-
sis are easy to train but cannot capture the structure of curved manifolds. Mixtures of these
simple unsupervised models [5, 6, 7, 8] have been used to perform local dimensionality
reduction, and can provide good density models for curved manifolds, but unfortunately
such mixtures cannot do dimensionality reduction. They do not describe a single, coher-
ent low-dimensional coordinate system for the data since there is no pressure for the local
coordinates of each component to agree.

Roweis et al [1] recently proposed a model which performs global coordination of local
coordinate systems in a mixture of factor analyzers (MFA). Their model is trained by max-
imizing the likelihood of the data, with an additional variational penalty term to encourage
the internal coordinates of the factor analyzers to agree. While their model can trade off
modelling the data and having consistent local coordinate systems, it requires a user given
trade-off parameter, training is quite inefcient (although [2] describes an improved train-
ing algorithm for a more constrained model), and it has quite serious local minima problems
(methods like LLE [4] or Isomap [3] have to be used for initialization).

In this paper we describe a novel, automatic way to align the hidden representations used by
each component of a mixture of dimensionality reducers into a single global representation
of the data throughout space. Given an already trained mixture, the alignment is achieved
by applying an eigensolver to a matrix constructed from the internal representations of the
mixture components. Our method is efcient, simple to implement, and has no local optima
in its optimization nor any learning rates or annealing schedules.


2 The Locally Linear Coordination Algorithm

Suppose we have a set of data points given by the rows of 
	


a
-dimensional space, which we assume are sampled from a

fold. We approximate the manifold coordinates using images
!
 dimensional embedding space. Suppose also that we have already trained, or have been
th reducer produces a%$ dimen-
given, a mixture of"
sional internal representation&('
$
for data point)' as well as a responsibility*	'
$,+.-
'
$
describing how reliable the#
10
is. These satisfy/

local dimensionality reducers. The#
th reducers representation of

and can be obtained, for example, using a gating network in a mixture of experts, or the
posterior probabilities in a probabilistic network. Notice that the manifold coordinates and
internal representations need not have the same number of dimensions.

from
dimensional mani-
in a




Given the data, internal representations, and responsibilities, our algorithm automatically
aligns the various hidden representations into a single global coordinate system. Two key
ideas motivate the method. First, to use a convex cost function whose unique minimum is
to

attained at the desired global coordinates. Second, to restrict the global coordinates
'
$ and responsibilities*
depend on the data

thereby leveraging the structure of the mixture model to regularize and reduce the effective
size of the optimization problem. In effect, rather than working with individual data points,
we work with large groups of points belonging to particular submodels.

' only through the local representations&

'2$ ,

'
$;:

4<5

87

'2$%9

'
$ and&

to these to obtain its guess at the global coordinates. The nal global
is obtained by averaging the guesses using the responsibilities as weights:

'
$ . Given an input
We rst parameterize the global coordinates
in terms of*
'
$ and then applies a linear projection
, each local model infers its internal coordinates&
$ and offset 465
coordinates
?A@
'
$>E
BDC
'
$
N*O'
$P&
#
HM'
N4
is the K th entry of &
where 4
Q0
= , whereJ
indices 9LK
9LK
#
R#
. For compactness, we will writeJ
the domain of9LK
to 
'
$ and theJ
Now dene the matricesI
as4
F*O'
$
(1) becomes a system of linear equations (2) with xedI

87
$>=
9LK
F8I
is the K th column of 3
$ , E
'2$
into a single new indexJ
#
:,/
0
RS%A"
R#
and3
asHT'

F7>GH
'
$
'
$ , and E
th row of3
.4
and unknown parameters3

is a bias.
This process is described in gure 1. To simplify our calculations, we have vectorized the
is an invertible mapping from

9K
R#
= .
$ . Then

(1)

(2)

responsibilities

'
$

.

alignment
parameters

lj

nju

ny

global
coordinates

9LK

rnk

znk

highdimensional
data

xn

local dimensionality
reduction models

Responsibilityweighted
local representations

local coordinates

Figure 1: Obtaining global coordinates from data via responsibility-weighted local coordinates.





'
$
*
'
'

'
3
$
'

'
$
*
3
$
&
$
7
5
*
B
4
B
$
'
G
4
G
3
J

J
=

G
B

4
G
B
$
B
$
B
5
=
=
=
$

$


J
G
E
B
G
B
	
	
	

is linear in each

'
$ and the unknown parameters4

is highly non-linear since it depends on the multipli-
cation of responsibilities and internal coordinates which are in turn non-linearly related to

The key assumption, which we have emphasized by re-expressing
mapping between the local representations and the global coordinates
'
$ ,*
of&
' and the images
original data 
the data'
We now consider determining3

' above, is that the
$ . Crucially, however, the mapping between the
= . For this we
is convex in3
- on

advocate using a convex
as well, and there is a unique optimum that can be computed efciently using a variety of
methods. This is still true if we also have feasible convex constraints
. The
case where the cost and constraints are both quadratic is particularly appealing since we
are matrices

= . Notice that since

through the inference procedure of the mixture model.

according to some given cost function

is linear in3

. In particular suppose

=
=

,

and

and
. This gives:

dening the cost and constraints, and let

can use an eigensolver to nd the optimal3
8FI







(3)




and

where

is the trace operator. The matrices

are typically obtained from the
and summarize the essential geometries among them. The solution to the
with

	
original data
constrained minimization above is given by the smallest generalized eigenvectors


Below, we investigate a cost function based on the Locally Linear Embedding (LLE) algo-
rithm of Roweis and Saul [4]. We call the resulting algorithm Locally Linear Coordina-
tion (LLC). The idea of LLE is to preserve the same locally linear relationships between
its

. In particular the columns of3

are given by these generalized eigenvectors.

. We identify for each point 
%	(

(
'

&
'

' and their counterparts
the original data points 
' and then minimize
nearest-neighbours
F7
'

subject to the constraints /

! #"#$

,

0 . The weights are unique1
' we

with respect to
and can be solved for efciently using constrained least squares (since solving for
decoupled across

is
). The weights summarize the local geometries relating the data points

!

arrange to minimize the same cost

to their neighbours, hence to preserve these relationships among the coordinates
but with respect to
we scale

*	(
is invariant to translations and rotations of
*

. In order to break these degeneracies we enforce the following constraints:

instead.

(
'

&
'



+-,



where

is a vector of 0 s. For this choice, the cost function and constraints above become:

, and scales as

(4)

(5)

(6)

(7)
(8)

(9)

with cost and constraint matrices

%	(

.
0	MI

8FI

(



(
'

M*/O
%

0


1In the unusual case where the number of neighbours is larger than the dimensionality of the data
, simple regularization of the norm of the weights once again makes them unique.

(
'





'
B
'

9


9


9

9
3

9

=





I


I


I

9

=







9

=

-




?

3

I


I
3


3

I


I
3
?

3


3


3


3
?



'
'

9
=
'


/
'








9

=
9
=



"
$
'


$
'

)

9

=


9

=
9
=

0
+
7
'

'

0
0



-
0
+
7
'

'


'

0
+



?
,
0

9

=
3

I

9

=
9
=
I
3
3


3



,
3

-


3

I

I
3

3
3
?

9

=
9
=
I
0
+
I

I
1
'
$

'
$>E

and

.

.

using (4).

As shown previously, the solution to this problem is given by the smallest generalized
eigenvectors
0(
that are orthogonal to the vector
is the smallest generalized
eigenvector, corresponding to an eigenvalue of 0. Hence the solution to the problem is

- , we need to nd eigenvectors

8

with

0	

. To satisfy 
/,

0 . Fortunately,

LLC Alignment Algorithm:

.
from (9).

and

of the generalized eigenvalue system

given by theS

Note that the edge size of the matrices

, compute local linear reconstruction weights

 smallest generalized eigenvectors instead.

, obtaining a local representation&
for each submodel# and each data point

to 9
 Using data
 Train or receive a pre-trained mixture of local dimensionality reducers.
Apply this mixture to
responsibility*
 Form the matrixI withH
'
$ and calculate
 Find the eigenvectors corresponding to the smallest
0(
 Let3 be a matrix with columns formed by theS nd to
Return theJ
th row of3
as alignment weight4
$ .
Return the global manifold coordinates as
FI
$ which scales with the number of components and dimensions of the local
is "
. As a result, solving for the
representations but not with the number of data points +
alignment weights is much more efcient than the original LLE computation (or those
. In effect, we
of Isomap) which requires solving an eigenvalue system of edge size +
have leveraged the mixture of local models to collapse large groups of points together and
worked only with those groups rather than the original data points. Notice however that
still requires determining the neighbours of the original
the computation of the weights
in the worse case.
data points, which scales as 
Coordination with LLC also yields a mixture of noiseless factor analyzers over the global

'
$ and
0 eigenvalues
0 st eigenvectors.

$ and factor loading 3
th factor analyzer having mean 4
$ .
coordinate space
, with the #
, we can infer the responsibilities*P$ and the posterior means
Given any global coordinates
$ over the latent space of each factor analyzer. If our original local dimensionality reduc-
$ , we can now infer the high dimensional mean
from*
ers also supports computing
data point which corresponds to the global coordinates

. This allows us to perform op-
erations like visualization and interpolation using the global coordinate system. This is the
method we used to infer the images in gures 4 and 5 in the next section.

whose generalized eigenvectors we seek

$ and&

3 Experimental Results using Mixtures of Factor Analyzers
The alignment computation we have described is applicable to any mixture of local dimen-
sionality reducers. In our experiments, we have used the most basic such model: a mixture
th factor analyzer in the mixture describes a proba-

of factor analyzers (MFA) [8]. The #
bilistic linear mapping from a latent variable&

The model assumes that the data manifold is locally linear and it is this local structure that
is captured by each factor analyzer. The non-linearity in the data manifold is handled by
patching multiple factor analyzers together, each handling a locally linear region.

to the data with additive Gaussian noise.

MFAs are trained in an unsupervised way by maximizing the marginal log likelihood of
the observed data, and parameter estimation is typically done using the EM algorithm2.

2In our experiments, we initialized the parameters by drawing the means from the global covari-
ance of the data and setting the factors to small random values. We also simplied the factor analyzers
to share the same spherical noise covariance
although this is not essential to the process.







,
I
3


5

I

5
'
?

:
0
=
$
'

'
'
G

*
B


:


:
B
3


:
/
$


9
+

=
5
&
$
A

B

C

D

Figure 2: LLC on the S curve (A). There are 14 factor analyzers in the mixture (B), each with 2 latent
dimensions. Each disk represents one of them with the two black lines being the factor loadings. After
alignment by LLC (C), the curve is successfully unrolled; it is also possible to retroactively align the
original data space models (D).

A

B

Figure 3: Unknotting the trefoil
curve. We generated 6000 noisy
points from the curve. Then we t
an MFA with 30 components with
1 latent dimension each (A), but
aligned them in a 2D space (B).
We used 10 neighbours to recon-
struct each data point.

$ conditioned on the data

th local representation of

as the responsibility.

th factor analyzer generated

$ , a MFA trained only

th factor analyzer generated ) as the#

to maximize likelihood cannot learn a global coordinate system for the manifold that is
consistent across every factor analyzer. Hence this is a perfect model on which to apply
(assuming
, while we use the

Since there is no constraint relating the various hidden variables&
automatic alignment. Naturally, we use the mean of&
the #
posterior probability that the#

We illustrate LLC on two synthetic toy problems to give some intuition about how it works.
The rst problem is the S curve given in gure 2(A). An MFA trained on 1200 points
sampled uniformly from the manifold with added noise (B) is able to model the linear
structure of the curve locally, however the internal coordinates of the factor analyzers are
not aligned properly. We applied LLC to the local representations and aligned them in a 2D
space (C). When solving for local weights, we used 12 neighbours to reconstruct each data
point. We see that LLC has successfully unrolled the S curve onto the 2D space. Further,
given the coordinate transforms produced by LLC, we can retroactively align the latent
spaces of the MFAs (D). This is done by determining directions in the various latent spaces
which get transformed to the same direction in the global space.

To emphasize the topological advantages of aligning representations into a space of higher
dimensionality than the local coordinates used by each submodel, we also trained a MFA
on data sampled from a trefoil curve, as shown in gure 3(A). The trefoil is a circle with a
knot in 3D. As gure 3(B) shows, LLC connects these models into a ring of local topology
faithful to the original data.

We applied LLC to MFAs trained on sets of real images believed to come from a complex
manifold with few degrees of freedom. We studied face images of a single person under
varying pose and expression changes and handwritten digits from the MNIST database.
After training the MFAs, we applied LLC to align the models. The face models were
aligned into a 2D space as shown in gure 4. The rst dimension appears to describe

Figure 4: A map of reconstructions of the faces when the global coordinates are specied. Contours
describe the likelihood of the coordinates. Note that some reconstructions around the edge of the map
are not good because the model is extrapolating from the training images to regions of low likelihood.
A MFA with 20 components and 8 latent dimensions each is used. It is trained on 1965 images. The
weights

are calculated using 36 neighbours.

changes in pose, while the second describes changes in expression. The digit models were
aligned into a 3D space. Figure 5 (top) shows maps of reconstructions of the digits. The
rst dimension appears to describe the slant of each digit, the second the fatness of each
digit, and the third the relative sizes of the upper to lower loops. Figure 5 (bottom) shows
how LLC can smoothly interpolate between any two digits.
In particular, the rst row
interpolates between left and right slanting digits, the second between fat and thin digits,
the third between thick and thin line strokes, and the fourth between having a larger bottom
loop and larger top loop.

4 Discussion and Conclusions
Previous work on nonlinear dimensionality reduction has usually emphasized either a para-
metric approach, which explicitly constructs a (sometimes probabilistic) mapping between
the high-dimensional and low-dimensional spaces, or a nonparametric approach which
merely nds low-dimensional images corresponding to high-dimensional data points but
without probabilistic models or hidden variables. Compared to the global coordination
model [1], the closest parametric approach to ours, our algorithm can be understood as post
coordination, in which the latent spaces are coordinated after they have been t to data. By
decoupling the data tting and coordination problems we gain efciency and avoid local
optima in the coordination phase. Further, since we are just maximizing likelihood when
tting the original mixture to data, we can use a whole range of known techniques to escape
local minima, and improve efciency in the rst phase as well.

On the nonparametric side, our approach can be compared to two recent algorithms, LLE


to its latent space, the nal coordinated model will also describe a (probabilistic) mapping
from the whole data space to the coordinated embedding space.

Our alignment algorithm improves upon local embedding or density models by elevating
their status to full global dimensionality reduction algorithms without requiring any modi-
cations to their training procedures or cost functions. For example, using mixtures of factor
analyzers (MFAs) as a test case, we show how our alignment method can allow a model
previously suited only for density estimation to do complex operations on high dimensional
data such as visualization and interpolation.

Brand [11] has recently proposed an approach, similar to ours, that coordinates local para-
metric models to obtain a globally valid nonlinear embedding function. Like LLC, his
charting method denes a quadratic cost function and nds the optimal coordination di-
rectly and efciently. However, charting is based on a cost function much closer in spirit to
the original global coordination model and it instantiates one local model centred on each
training point, so its scaling is the same as that of LLE and Isomap. In principle, Brands
method can be extended to work with fewer local models and our alignment procedure can
be applied using the charting cost rather than the LLE cost we have pursued here.

Automatic alignment procedures emphasizes a powerful but often overlooked interpreta-
tion of local mixture models. Rather than considering the output of such systems to be a
single quantity, such as a density estimate or a expert-weighted regression, it is possible
to view them as networks which convert high-dimensional inputs into a vector of internal
coordinates from each submodel, accompanied by responsibilities. As we have shown, this
view can lead to efcient and powerful algorithms which allow separate local models to
learn consistent global representations.

Acknowledgments

We thank Geoffrey Hinton for inspiration and interesting discussions, Brendan Frey and
Yann LeCun for sharing their data sets, and the reviewers for helpful comments.

