AbstractThis paper collects some ideas targeted at advancing
our understanding of the feature spaces associated with support
vector (SV) kernel functions. We rst discuss the geometry of
feature space. In particular, we review what is known about the
shape of the image of input space under the feature space map,
and how this inuences the capacity of SV methods. Following
this, we describe how the metric governing the intrinsic geometry
of the mapped surface can be computed in terms of the kernel,
using the example of the class of inhomogeneous polynomial
kernels, which are often used in SV pattern recognition. We then
discuss the connection between feature space and input space by
dealing with the question of how one can, given some vector in
feature space, nd a preimage (exact or approximate) in input
space. We describe algorithms to tackle this issue, and show their
utility in two applications of kernel methods. First, we use it to
reduce the computational complexity of SV decision functions;
second, we combine it with the Kernel PCA algorithm, thereby
constructing a nonlinear statistical denoising technique which is
shown to perform well on real-world data.

Index Terms Denoising, kernel methods, PCA, reduced set

method, sparse representation, support vector machines.

I. INTRODUCTION

REPRODUCING kernels are functions

forall pattern sets

which

(1)

give rise to positive matrices

[23]. Here,
is some compact set in which the data lives, typically
In the support vector
(but not necessarily) a subset of
(SV) community, reproducing kernels are often referred to as
Mercer kernels (Section II-B will show why). They provide an
elegant way of dealing with nonlinear algorithms by reducing
nonlinearly
them to linear ones in some feature space
instead of a dot product in
related to input space: Using
corresponds to mapping the data into a possibly high-
by a (usually nonlinear) map

dimensional dot product space

Manuscript received January 23, 1999; revised May 16, 1999. Part of this
work was done while P. Knirsch was with Bell Labs and B. Scholkopf and
A. J. Smola were with the Department of Engineering, Australian National
University, Canberra. This work was supported by the ARC and the DFG
under Grant Ja 379/52,71,91.

B. Scholkopf was with GMD FIRST, 12489 Berlin, Germany. He is now

with Mocrosoft Research Ltd., Cambridge CB2, U.K.

S. Mika, K.-R. Muller, G. Ratsch, and A. J. Smola are with GMD FIRST,

12489 Berlin, Germany.

C. J. C. Burges is with Bell Laboratories, Holmdel NJ, USA.
P. Knirsch is with the Max-Planck-Institut fur biologische Kybernetik,

72076 Tubingen, Germany.

Publisher Item Identier S 1045-9227(99)07268-9.

, and taking the dot product there, i.e., [4]

(2)

a feature map asso-
By virtue of this property, we shall call
ciated with Any linear algorithm which can be carried out in
terms of dot products can be made nonlinear by substituting an
a priori chosen kernel. Examples of such algorithms include
the potential function method, SV Machines, and kernel PCA
[1], [34] [29]. The price that one has to pay for this elegance,
however, is that the solutions are only obtained as expansions
in terms of input patterns mapped into feature space. For
instance, the normal vector of an SV hyperplane is expanded
in terms of SVs, just as the kernel PCA feature extractors are
expressed in terms of training examples

(3)

with some mapped test point

When evaluating an SV decision function or a kernel PCA
feature extractor, this is normally not a problem: due to (2),
transforms
multiplying
(3) into a kernel expansion which can be evaluated even
lives in an innite-dimensional space. In some cases,
if
however, there are reasons mandating a more comprehensive
understanding of what exactly is the connection between
patterns in input space and elements of feature space, given as
expansions such as (3). This eld being far from understood,
the current paper attempts to gather some ideas elucidating
the problem, and simultaneously proposes some algorithms
for situations where the above connection is important. These
are the problem of denoising by kernel PCA, and the problem
of speeding up SV decision functions.

this paper

The remainder of

is organized as follows.
Section II discusses different ways of understanding the
mapping from input space into feature space. Following
that, we briey review two feature space algorithms, SV
machines and kernel PCA (Section III). The focus of interest
of the paper,
the way back from feature space to input
space, is described in Section IV, and, in the more general
form of constructing sparse approximations of feature space
expansions, in Section V. The algorithms proposed in these
two sections are experimentally evaluated in Section VI and
discussed in Section VII.

II. FROM INPUT SPACE TO FEATURE SPACE

In this section, we show how the feature spaces in question
are dened by choice of a suitable kernel function. Insight into

10459227/99$10.00 

1999 IEEE

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

the structure of feature space can then be gained by considering
their relation to reproducing kernel Hilbert spaces, by how they
can be approximated by an empirical map, by their extrinsic
geometry, which leads to useful new capacity results, and by
their intrinsic geometry, which can be computed solely in
terms of the kernel.1

A. The Mercer Kernel Map

We start by stating the version of Mercers theorem given
to be a nite measure space.2 By

in [38]. We assume
almost all we mean except for sets of measure zero.

Theorem 1 (Mercer): Suppose

metric real-valued kernel3 such that

is a sym-
the integral operator

representation theorem, for each
function of

, call it

, such that

1001

there exists a unique

(6)

to , and

is the function on

(here,
argument of
In contrast, we use
product). In view of this property,
kernel.

obtained by xing the second
is the dot product of the RKHS.
to denote the canonical (Euclidean) dot
is called a reproducing

Note that by (6),

for all
identically zero. Hence the set of functions
spans the whole RKHS. The dot product on the RKHS thus
and can then
only needs to be dened on
be extended to the whole RKHS by linearity and continuity.
From (6), it follows that in particular

implies that

is

(4)

(7)

is

positive,

i.e.,

for

all

, we

have

Let

be the normalized eigenfunctions of

associated with the eigenvalues
order. Then

, sorted in nonincreasing

1)
2)
3)

;

and

;

Either

holds for almost all
in the latter case,
the series converges absolutely and uniformly for almost
all

or

From statement 3, it follows that
product in

i.e.,

corresponds to a dot

with

for almost all

(5)

In fact, the uniform convergence of the series implies that
such that even if the range
can be approximated within

there exists an

, between images of

given
of
accuracy

is innite-dimensional,
as a dot product in

B. The Reproducing Kernel Map

We can also think of the feature space as a reproducing
kernel Hilbert space (RKHS). To see this [2], [23], [36], [24],
on
[13], recall that a RKHS is a Hilbert space of functions
such that all evaluation functionals, i.e., the maps
some set
, are continuous. In that case, by the Riesz

1 Those readers who are chiey interested in applications and algorithms

might want to consider skipping over the present section.

2 A nite measure space is a set X with a -algebra dened on it, and
a measure dened on the latter, satisfying (X ) < 1 (i.e., up to a scaling
factor,  is a probability measure). A -algebra  on X is a family of subsets
of X ; which is closed under elementary set-theoretic operations (countable
unions, intersections, and complements), and which contains X as a member.
A measure is a function :  ! + which is -additive, i.e., the measure
of a set which is the disjoint unions of some other sets equals the sum of the
latters measures.

3 I.e., a function of two variables which gives rise to an integral operator.

,

(this implies that

for all
this means that any reproducing kernel
product in another space.

is symmetric). Note that
corresponds to a dot

Let us now consider a Mercer kernel, i.e., one which satises
the condition of Theorem 1, and construct a dot product such
that
containing the functions

becomes a reproducing kernel for the Hilbert space

By linearity, we have

(8)

(9)

is a Mercer kernel, the

can be
Since
chosen to be orthogonal with respect to the dot product in
Hence it is straightforward to construct a dot product

such that

(10)

(using the Kronecker symbol
to the reproducing kernel property (6) [using (8)].

, in which case (9) reduces

Therefore, (7) provides us with a feature map

associated

with

.

Proposition 2: For any Mercer kernel

,

there exists a

RKHS

such that for

we have

(11)

(12)

C. The Empirical Kernel Map

While giving an interesting alternative theoretical viewpoint,
does not appear all that useful at rst sight. In
the map
practice, it would seem pointless to rst map the inputs into
functions, i.e., into innite-dimensional objects. However, for
from (11) by
a given dataset, it is possible to approximate
only evaluating it on these points (cf., [15], [18], [20], [33]).

1002

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

Denition 3: For given patterns

, we call

the empirical kernel map with regard to .

(13)

kernel, and

the case where

Example: Consider rst

is a Mercer
i.e., we evaluate
on the training patterns. If we carry out a linear
algorithm in feature space, then everything will take place
in the linear span of the mapped training patterns. Therefore,
without losing
we can represent the
information. However, the dot product to use in that represen-
, since the
tation is not simply the canonical dot product in
will usually not form an orthonormal system. To turn
into a feature map associated with , we need to endow
with a dot product

of (11) as

such that

(14)

being a
To this end, we use the ansatz
positive matrix.4 Enforcing (14) on the training patterns, this
yields the self-consistency condition (cf., [28], [31])

with

(15)

Here, we have used

to denote the

kernel Gram matrix

(16)

The condition (15) can be satised for instance by the pseu-
doinverse

Equivalently, we could have incorporated this rescaling
operation, which corresponds to a kernel PCA whitening [29],
[28], [33], directly into the map, by modifying (13) to

(17)

by

This simply amounts to dividing the eigenvector basis vectors
5 It
of
parallels the rescaling of the eigenfunctions of the integral
operator belonging to the kernel, given by (10).

are the eigenvalues of

where the

For data sets where the number of examples is smaller
than their dimensionality, it can actually be computationally
explicitly rather than using kernels
attractive to carry out
in whatever subsequent algorithm (SVMs, or kernel PCA,
say) one wants to use. As an aside, note that in the case
of kernel PCA (to be described in Section III), one does
not even need to worry about the whitening step: Using the
will simply
canonical dot product in
which yields the same
lead to diagonalizing
eigenvectors with squared eigenvalues. This was pointed out
by [18] and [20].

(rather than

instead of

We end this section with two notes which illustrate why the
use of (13) need not be restricted to the special case we just
discussed.

 More general kernels. When using nonsymmetric kernels
in (13), together with the canonical dot product, one
4 Note that every dot product can be written in this form. Moreover, we do
not require deniteness of A, as the null space can be projected out, leading
to a lower-dimensional feature space.

5 It is understood that if K is singular, we use the pseudoinverse of K 1=2:

will effectively work with a matrix

, with general
Note that each positive semidenite matrix can be

written as

If we wanted to carry out the whitening step, it would
(cf., footnote 5 concerning

have to be using
potential singularities).

 Different evaluation sets. Mika [20] has performed exper-

iments to speed up kernel PCA by choosing
as a proper subset of

Now that we have described the kernel map in some detail,
including variations on it, we shall next look at its properties.
Specically, we study its effect on the capacity of kernel
methods (Section II-D) and the induced geometry in feature
space (Section II-E).

D. The Capacity of the Kernel Map

Vapnik [34], [35] gives a bound on the capacity, measured
by the VC-dimension , of optimal margin classiers. It takes
the form

(18)

is an upper bound constraining the length of the
where
is the
weight vector of the hyperplane in canonical form, and
radius of the smallest sphere containing the data in the space
where the hyperplane is constructed. The smaller this sphere
is, the smaller is also the capacity, with benecial effects on
the generalization error bounds.

If the data is distributed in a reasonably isotropic way, which
is often the case in input space, then (18) can be fairly precise.6
If, however, the distribution of the data is such that it does
not ll the sphere, then (18) is wasteful. The argument in
the remainder of the section, which is summarized from [37],
shows that using a kernel typically entails that the data in fact
lies in some box with rapidly decaying sidelengths, which can
be much smaller than the above sphere.

From statement 2 of Theorem 1, there exists some constant

depending on the kernel

such that

for all

and almost all

(19)

Therefore,
parallelepiped in with side lengths

is essentially contained in an axis parallel

[cf., (5)].

To see how this effectively restricts the class of functions we
is done in terms
are using, we rst note that everything in
of dot products. Therefore, we can compensate any invertible
by the corresponding
linear transformation of the data in
inverse adjoint transformation on the set of admissible weight
, we have
vectors in

, i.e., for any invertible operator

on

for all

(20)

Hence, we may construct a diagonal scaling operator
which inates the sides of the above parallelepiped as much
as possible, while ensuring that it still lives in a sphere of the
original radius
factor on the right hand side of (18), but it buys us something

(Fig. 1). This will not change the

in

6 In terms of entropy numbers, this is due to the tightness of the rate given

by a famous theorem of Maurey (e.g., [10]).

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1003

Fig. 1. Since everything is done in terms of dot products, scaling up the data by A can be compensated by scaling the weight vectors with A1: By
choosing A such that the data is still contained in a ball of the same radius R; we effectively reduce our function class (parameterized by the weight vector),
which leads to better generalization bounds which depend on the kernel inducing the map :

regarding the second factor: one can show that the function
class essentially behaves as if it was nite-dimensional, with
s eigenvalues.
a cut-off determined by the rate of decay of
The reasoning that leads to the improved bounds is some-
what intricate and cannot presently be explained in full detail.
In a nutshell, the idea is to compute the capacity (measured in
terms of covering numbers of the SV function class, evaluated
on an -sample) via the entropy numbers of a suitable linear
operator.7 Using
, consider rst the operator

(21)

applied to
For our purpose, the entropy numbers of
are crucial. These can be computed
a sphere of radius
Using
as the entropy numbers of the operator
factorization properties of entropy numbers, these can be upper
bounded taking into account the above scaling operator
in a rather precise way. The faster the eigenvalues of the
associated with a given kernel decay, the
integral operator
smaller the entropy numbers (and hence the capacity) of the
corresponding feature space algorithm function class, and the
stronger the generalization error bounds that one can prove.

As an example, we now consider how the entropy numbers
depend asymptotically on the eigenvalues of

Proposition

4

(Exponential-Polynomial Decay

Suppose

is a Mercer kernel with

Then

[38]):
for some

An example of such a kernel (for

is the Gaussian

This proposition allows the formulation of a priori gener-
alization error bounds depending on the eigenvalues of the
kernel. Using similar entropy number methods,
is also
possible to give rather precise data-dependent bounds in terms
of the eigenvalues of the kernel Gram matrix [27].

it

7 Consider two normed spaces E and F: For n 2 , the nth entropy number
of a set M  E is dened as n(M) := inf f > 0: there exists an -cover
for M in E containing n or fewer pointsg: (Recall that the covering number
N (; M ), being essentially its functional inverse, measures how many balls of
radius  one needs to cover M:) The entropy numbers n(T ) of an operator
T : E ! F are dened as the entropy numbers of the image of the unit ball
under T : Note that 1(T ) = jjT jj; intuitively, the higher entropy number
allow a ner characterization of the complexity of the image of T (e.g., [10],
[38]). Note that entropy numbers have some nice properties that covering
numbers are lacking. For instance, scaling a subset of a normed vector space
by some factor simply scales its entropy numbers by the same factor.

Entropy numbers are a promising tool for studying the
capacity of feature space methods. This is due to the fact that
in the linear case, which is what we are interested in for feature
space algorithms, they can be studied using powerful methods
of functional analysis (e.g., [10]).

E. The Metric of the Kernel Map

In general

Another way to gain insight into the structure of feature
space is to consider the intrinsic shape of the manifold to
which ones data is mapped. It is important here to distinguish
and the surface in that space to
between the feature space
actually map, which we will
which points in input space
dimensional submanifold
call
is
embedded in
sufciently smooth that structures such as a Riemannian metric
can be dened on it. Here we will follow the analysis of [8],
to which the reader is referred for more details, although the
application to the class of inhomogeneous polynomial kernels
is new.

For simplicity, we assume here that

will be an

We rst note that all intrinsic geometrical properties of

in

can be derived once we know the Riemannian metric induced
The Riemannian metric can be
by the embedding of
Interestingly, we
dened by a symmetric metric tensor
to construct
do not need to know the explicit mapping
it can be written solely in terms of the kernel. To see this
consider the line element

where indexes
to input space
displacement in

correspond to the vector space
Now letting
, we have

represent a small but nite

(22)

and

Thus we can read off the components of the metric tensor

(23)

For the class of kernels which are functions only of dot prod-
ucts between points in
both covariant and contravariant

1004

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

components of the metric take a simple form

(24)

(25)

where the prime denotes derivative with respect to the argu-
To illustrate, let us compute the intrinsic geomet-
ment
corresponding to the class of
rical properties of the surface
inhomogeneous polynomial Mercer kernels

(26)

is a constant. Properties relating to the intrinsic
where
curvature of a surface are completely captured by the Riemann
curvature tensor

(27)

where the Christoffel symbols of the second kind are dened
by

(28)

Thus we nd that for this class of kernels, the Riemann cur-
and polynomial

vature, for arbitrary input space dimension
order

, is given by

one can expect that the density on the surface will become
ill behaved for data whose norm is small, for homogeneous
polynomial kernels, but not for the inhomogeneous case.
Similar problems may arise in the pattern recognition case
if ones data lies near the singularity. These considerations
can be extended to compute the intrinsic volume element on
on , and
to give some simple necessary tests that must be satised by
any kernel if it is to be a Mercer kernel: the reader is referred
to [8] for details.

, which may be used to compute the density

III. FEATURE SPACE ALGORITHMS

We next describe the two algorithms used in this paper: SV
machines, and kernel PCA. The SV algorithm shall not be
described in detail, let us only briey x the notation.

SV classiers [34] construct a maximum margin hyperplane
In input space, this corresponds to a nonlinear decision

in
boundary of the form

(31)

are the training examples. Those with

are
where the
, which are
called SVs; in many applications, most of the
found by solving a quadratic program, turn out to be zero.
Excellent classication accuracies in both OCR and object
recognition have been obtained using SV machines [24], [7].
A generalization to the case of regression estimation, leading
to similar function expansion, exists [34], [26].

Kernel Principal Component Analysis [29] carries out a
The extracted features take

linear PCA in the feature space
the nonlinear form

It is interesting to compare this result with that for the

homogeneous case

[8]

where, up to a normalization, the
th eigenvector of the matrix
the

are the components of

(29)

(32)

This can be understood as follows. We wish to nd eigen-
in

of the covariance matrix

and eigenvalues
vectors
the feature space,9 where

(30)

(33)

This analysis shows that adding the constant

to the
kernel results in striking differences in the geometries. Both
,
curvatures vanish for

, as expected. However for

vanishes for all powers whereas

does not vanish for any
Furthermore, all surfaces for
homogeneous kernels with nonvanishing curvature (i.e., those
, whereas
with
none of the corresponding surfaces for inhomogeneous kernels
do, provided

have a singularity at

and

8

Thus beyond providing insight into the geometrical structure
generated by choice of kernel, the geomet-
of the surfaces in
rical analysis also gives more concrete results. For example,
8 Note that if c < 0 the kernel is not a Mercer kernel and the above analysis

does not apply.

is very high dimensional this will be
In the case when
impossible to compute directly. To be still able to solve this
problem, one uses Mercer kernels. To this end, we need to
in dot products. We
derive a formulation which only uses
This
then replace any occurrence of
way we avoid dealing with the mapped data explicitly, which
may be intractable in terms of memory and computational cost.
To nd a formulation for PCA which uses only dot prod-
ucts, we rst substitute the covariance matrix (33) into the
Note that all solutions to this
eigenequation

by

9 Here we assume that the mapped data is centered, too. In general this will
not be true, but all computations can easily be reformulated to perform an
explicit centering in F [29].

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1005

equation with
training data. Thus, we may consider the equivalent system

must lie in the span of

-images of the

and expand the solution

as

for all

(34)

(35)

Substituting (33) and (35) into (34), and dening a

matrix
we arrive at a problem which is cast

by

in terms of dot products: solve

where
Normalizing the solution

(for details on the last step see [29]).

in

translates into

(36)

To extract features, we compute the projection of the

-
onto the th eigenvector in the feature

image of a test point
space by

(37)

Usually, this will be much cheaper than taking the dot product
in the feature space explicitly.

To conclude the brief summary of kernel PCA, we state
a characterization which involves the same regularizer (the
length of the weight vector) as the one used in SV machines.
, the th kernel PCA feature extrac-
, is optimal among all feature extractors of

Proposition 5: For all

tor, scaled by
the form

Fig. 2. The preimage problem. Not each point in the span of the mapped
input data is necessarily the image of some input pattern. Therefore, not each
point  that can be written as an expansion in terms of mapped input patterns
(e.g., a kernel PCA eigenvector, or a SVM hyperplane normal vector), can
necessarily be expressed as the image of a single input pattern.

IV. FROM FEATURE SPACE TO INPUT SPACE

Unlike Section II, which described how to get from input
space into feature space, we now study the way back. There
has been a fair amount of work on aspects of this problem
in the context of developing so-called reduced set methods
(e.g., [6], [9], [25], [12], [22]). For pedagogical reasons, we
shall postpone reduced set methods to Section V, as they focus
on a problem that is already more complex than the one we
would like to start with.

A. The Preimage Problem

As stated in the introduction, feature space algorithms
express their solutions as expansions in terms of mapped input
into the feature space
points (3). However, since the map
is nonlinear, we cannot generally assert that each such

expansion will have a preimage under
such that
be easy to compute, as shown by the following result [24].

(Fig. 2). If the preimage existed, it would

, i.e., a point

Proposition 6: Consider a feature space expansion

If there exists a

such that

in the sense that it has the shortest weight vector

and if
compute

is an invertible function

of

, then we can

as

subject to the conditions that

1) it is orthogonal to the rst
extractors (in feature space)

2) applied to the training set

variance set of outputs.

kernel PCA feature

it leads to a unit

Both SV machines and kernel PCA utilize Mercer kernels to
generalize a linear algorithm to a nonlinear setting; moreover,
both use the same regularizer, albeit in different domains of
learningsupervised versus unsupervised. Nevertheless, fea-
ture extraction experiments on handwritten digit images using
kernel PCA have shown that a linear hyperplane classier
trained on the extracted features can perform as well as a
nonlinear SV machine trained directly on the inputs [29].

where

is any orthonormal basis of input space.

Proof: We expand

as

(38)

1006

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

Several remarks on this proposition should be given. First,

examples of kernels which are invertible functions of
are polynomial kernels

preimage. As we shall see in experiments, however, even
better preimages can be found, which makes some interesting
applications possible [26], [21]:

and sigmoid kernels

odd

(39)

(40)

A similar result holds for RBF kernels (using the polarization
identity)all we need is a kernel which allows the reconstruc-
from , evaluated on some input points which
tion of
we are allowed to choose (for details, cf., [24]).

The crucial assumption, clearly,

is the existence of the
preimage. Unfortunately, there are many situations where there
are no preimages. To illustrate this, we consider the feature
(11). Clearly, only points in feature space which can
map
do have a preimage under this map. To
be written as
characterize this set of points in a specic example, consider
Gaussian kernels

, map it into

Denoising: Given a noisy
higher components to obtain
preimage
data set is captured in the rst
components mainly pick up the noisein this sense,
thought of as a denoised version of

, discard
, and then compute a
Here, the hope is that the main structure in the
directions, and the remaining
can be

Compression: Given the eigenvectors

and a small num-
, compute a
This is useful

[cf., (37)] of

ber of features
preimage as an approximate reconstruction of
if

is smaller than the dimensionality of the input data.
Interpretation: Visualize a nonlinear feature extractor

, but not

by computing a preimage.

In this paper, we focus on the rst point. In the next section,
we shall develop a method for minimizing (42), which we
will later, in the experimental section, apply to the case where

(41)

B. An Algorithm for Approximate Preimages

maps each input into a Gaussian sitting on
In this case,
that point. However, it is known [19] that no Gaussian can be
written as a linear combination of Gaussians centered at other
points. Therefore, in the Gaussian case, none of the expansions
(3), excluding trivial cases with only one term, has an exact
preimage.

The problem that we had initially set out to solve has turned
out to be insolvable in the general case. Let us try to ask
for less. Rather than trying to nd exact preimages, we now
an approximate
consider approximate ones. We call
preimage for

if

(42)

is small.10

Are there vectors

for which good approximate preimages
exist? As described in Section III, kernel PCA is nothing
it provides
but PCA in
projections

Therefore, for

The present section [25] gives an analysis for the case of
the Gaussian kernel, which has proven to perform very well
in applications [30], and proposes an iteration procedure for
computing preimages of kernel expansions.

We start by considering a problem slightly more general
than the preimage problem: we are seeking to approximate
First observe that rather

by

than minimizing

we can minimize the distance between
projection of

onto span

(Fig. 3),

(45)

and the orthogonal

(43)

To this end, we maximize

(46)

(47)

with the following optimal approximation property (e.g., [24]).
are sorted according to nonin-
Here, we assume that the
being the smallest nonzero
creasing eigenvalues
eigenvalue.

with

Proposition 7:

is the -dimensional projection minimiz-

ing

(44)

Therefore,
imate preimage: trivially, already

can be expected to have a good approx-
is a good approximate

10 Just how small it needs to be in order to form a satisfactory approximation
will depend on the problem at hand. Therefore, we have refrained from giving
a formal denition.

is preferable to the one of (45) over

which can be expressed in terms of the kernel. The maximiza-
and
tion of (47) over
, since it comprises a lower-dimensional problem, and since
have different scaling behavior. Once the maximum
and
of (47) is found, it is extended to the minimum of (45) by
The function
setting [cf. (46)]
(47) can either be minimized using standard techniques (as
in [6]), or, for particular choices of kernels, using xed-point
iteration methods, as shown presently.

For kernels which satisfy

Gaussian kernels), (47) reduces to

for all

(e.g.,

(48)

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1007

the difference between two densities (modulo normalization
constants).

To see this, we dene the sets

and

and the shorthands

and

Fig. 3. Given a vector 	 2 F , we try to approximate it by a multiple of
a vector (zzz) in the image of input space ( N ) under the nonlinear map
 by nding zzz such that the projection distance of  onto span((zzz)) is
minimized.

The target

then
i.e., we are trying to nd a
reads
where the difference between the (unnormalized)
point
probabilities for the two classes is maximal, and estimate
the approximation to (54) by a Gaussian centered at
Moreover, note that we can rewrite (50) as

(48)

For the extremum, we have

, we substitute (54) to get

the sufcient condition

To evaluate the gradient

in terms of

sians, or

For

for

[e.g., Gaus-
, we obtain

where

, leading to

(52)

(53)

For the Gaussian kernel
we thus arrive at

(49)

V. REDUCED SET (RS) METHODS

A. The Problem

We now move on to a slightly more general problem, rst
studied by [6], where we are no longer only looking for single
preimages, but expansions. It will turn out that one can also
use the method developed in the last section to design an
algorithm for the more general case.

(50)

Assume we are given a vector

, expanded in images

of input patterns

and devise an iteration

(54)

,

Rather than looking for a single
with
preimage, we now try to approximate it by a reduced set
expansion [6]

(51)

and thus is nonzero in a
The denominator equals
neighborhood of the extremum of (48), unless the extremum
on
itself is zero. The latter only occurs if the projection of
is zero, in which case it is pointless
the linear span of
Numerical instabilities related to
to try to approximate
being small can thus be approached by restarting

the iteration with different starting values.

Interestingly, (51) can be interpreted in the context of
clustering (e.g., [5]). It determines the center of a single
with
Gaussian cluster, trying to capture as many of the
as possible, and simultaneously avoids those
positive
the sign of the
with negative
equals the label of the pattern
is this sign which
distinguishes (51) from plain clustering or parametric density
estimation. The occurrence of negative signs is related to the
fact that we are not trying to estimate a parametric density but

For SV classiers,
It

with
minimize [6]

,

,

To this end, one can

(55)

(56)

The crucial point is that even if
can be computed (and minimized) in terms of the kernel.

is not given explicitly, (56)

In the NIST benchmark of 60 000 handwritten digits, SV
machines are more accurate than any other single classier
[24]; however, they are inferior to neural nets in run-time
classication speed [9]. In applications where the latter is an

1008

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

issue, it is thus desirable to come up with methods to speed up
things by making the SV expansion more sparse, i.e., replacing
(54) by (55).

B. Finding the Coefcients

Evidently, the RS problem consists of two parts. One has
, and one has to compute the
to determine the RS vectors
expansion coefcients We start with the latter; partly, as it
is easier, partly, as it is common to different RS methods.

Proposition 8: The optimal coefcients
by

for approximating
linearly independent
given by

(for
in the 2-norm are

hence

the Gram matrix can either be computed only from those
examples which have a nonzero
, or from a larger set which
we want to use for the expansion.) Interestingly, it will turn
out that this problem is closely related to kernel PCA.

Let us start with the simplest case. Assume there exists an

of
eigenvector
Using (2), this reads

with eigenvalue 0, i.e.,

for all

(61)

(62)

(57)

Here,

and

Note that if the

are linearly independent, as they
should be if we want to use them for approximation, then
has full rank. Otherwise, one can use the pseudoinverse,
or select the solution which has the largest number of zero
components.

Proof: See [25]. We evaluate the derivative of the dis-

tance in

and set it to zero. Substituting
(using

hence

(58)

, we obtain

(59)

(60)

and

No RS algorithm using the 2-norm optimality criterion can
circumvent this result. For instance, suppose we are given
simultaneously
an algorithm that computes the
and comes up with a solution. Then we can always use the
proposition to recompute the optimal coefcients to get a
solution which is at least as good as the original one. Different
algorithms can, however, differ in the way they determine
in the rst place. The one dealt with in the
the vectors
, while the one in
next section simply selects subsets of the
Section V-D uses vectors different from the original

C. Reduced Set Selection

1) Selection via Kernel PCA: The idea for the rst algo-
the null space of
rithm arises from the observation that
precisely tells us
the Gram matrix
how many vectors can be removed from an expansion while
committing zero approximation error (assuming we correctly
adjust the coefcients), i.e., how sparse we can make an SV
expansion, say, without changing it the least [24], [12]. (Here,

are linearly dependent, and there-
This means that the
can
fore any of the
be expressed in terms of the others. Hence, we may use the
eigenvectors with eigenvalue zero to eliminate certain terms
from any expansion in the

which comes with a nonzero

What happens if we do not have nonzero eigenvalues,
such as in the case of Gaussian kernels [19]? Intuitively,
we would still believe that even though the above is no
longer precisely true, it should give a good approximation.
However, the crucial difference is that in order to get the best
possible approximation, we now need to take into account
if we
the coefcients of the expansion
, say, then this error will
commit an error by removing
depend on

too. How do we then select the optimal

Clearly, we would like to nd coefcients minimizing the

error we commit by replacing

with

(63)

To establish a connection to kernel PCA, we make a change of
variables. First, dene
Hence (63) equals

Normalizing

for

for

,

to obtain
problem of minimizing

, hence

, this leads to the

(64)

(note that

11
over
A straightforward calculation shows that we can recover the
, i.e., the values to
approximation coefcients for
add to the

is invariant when rescaling

for leaving out

, as

,

Rather than minimizing the nonlinear function (64), we now
devise a computationally attractive approximate solution. It is
alone is minimized
motivated by the observation that
for the eigenvector with minimal eigenvalue, consistent with
the special case discussed above [cf., (62)]. In that case,

11 The idea of approximating the support vector expansion by optimally
removing individual support vectors, and then adjusting the coefcients of
those that remain to minimize the resulting error, was arrived at independently
by Olivier Chapelle, who also derived the expression to be minimized, (64)
(private communication).

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

eigenvector of

, with eigenvalue

, then

More generally, if

is any normalized

To dispose of the modulus, we rewrite

as

1009

(68)

(65)

where
the quadratic programming problem

In terms of the new variables, we end up with

This can be minimized in
kernel PCA and scanning through the matrix
complexity can be reduced to
eigenvalues, with
smallest
, with
we can eliminate
efcient way.

operations by performing
The
by only considering the
chosen a priori. Hence,
chosen in a principled yet

Setting all computational considerations aside, the optimal
greedy solution to the above selection problem, equivalent to
(64), can also be obtained by using Proposition 8: compute
the optimal solution for all possible patterns that one could
as
leave out (i.e., use subsets of

of size

and evaluate (58) in each case.

The same applies to subsets of any size. If we have the

resources to exhaustively scan through all subsets of size

, then Proposition 8 provides the optimal
way of selecting the best expansion of a given size. Better
expansions can only be obtained if we drop the restriction that
the approximation is written in terms of the original patterns,
as done in Section V-D.

No matter how we end up choosing , we approximate

by

(69)

subject to

This problem can be solved with standard quadratic pro-
gramming tools. The solution (68) could be used directly as
expansion coefcients. For optimal precision, however, we
merely use it to select which patterns to use for the expansion
(those with nonzero coefcients), and recompute the optimal
coefcients using Proposition 8.

3) The Multiclass Case: In many applications, we face the
feature
problem of simultaneously approximating a set of
space expansions. For instance, in digit classication, a com-
binary recognizers, one for
mon approach is to train
each digit. To this end, the quadratic programming formulation
of Section V-C.2 can be modied to

(66)

(70)

The whole scheme can be iterated until the expansion of

is sufciently sparse. If one wants to avoid having to nd the
at each step anew, then approximate
smallest eigenvalue of
schemes using heuristics can be conceived.

In our experiments to be reported below, we did compute
all eigenvectors at each step, using the Gram matrix computed
from the SVs and then selected

according to (65).

2) Selection via

consider
method for enforcing sparseness which is inspired by
shrinkage penalizers (cf., [13]).

Penalization: We

next

a

Given some expansion
by minimizing

, we approximate it by

(67)

Here,

(where

is a constant determining the tradeoff
over all
between sparseness and quality of approximation. The con-
can be set to one or
is the mean of
stants
say. In the latter case, we are hoping for a sparser
all
decomposition, since more emphasis is put on shrinking terms
which are already small. This reects the intuition that it is
less promising to try to shrink very large terms. Ideally, we
would like to count the number of nonzero coefcients, rather
than sum their moduli; however, the former does not lead to
an efciently solvable optimization problem.

subject to

(71)
The indexes and are understood to range over all SVs (i.e.,
expansion vectors which have a nonzero coefcient for at least
classes, respectively.
one of the recognizers), and over the
, we use either one or, with the same rationale as above,
For

(here,

is the mean of all

over

The term

together with the constraint (71) ensures
that we only penalize the largest of the coefcients pertaining
to each individual SV. Therefore, as soon as one coefcient
is being used by one the expansions, it does not cost anything
to also use it for another one. This is precisely what we want
for speed-up purposes: if we have to compute a certain dot
product anyway, we might just as well reuse it for the other
expansions.

Using

following problem:

, with

, we arrive at the

subject to

Here,

ranges over all SVs.

(72)

(73)

1010

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

4) A Note on the Utility of Reduced Set Selection: Why
should we expect these procedures to be useful? First, for SV
for some positive value of the
machines, where
, there is reason to believe that the
regularization constant
SV expansion can be made sparser by removing the constraint
[note that (66) does not care about the constraint].12
on
Second, the number of eigenvalues which are zero (and thus
the number of patterns that can be removed without loss), or
at least small, depends on the problem at hand and the kernel
used. For instance, Gaussian kernel Gram matrices do not have
zero eigenvalues unless some of the patterns are duplicates
[19]. Nevertheless, good approximations are possible, since
the eigenvalues of Gaussian kernels decay rapidly (e.g., [38]).

D. Reduced Set Construction

So far, we have dealt with the problem of how to select a
reduced set of expansion vectors from the original one. We
now return to the originally posed problem, which includes
the construction of new vectors to reach high reduction rates.
To this end, suppose we want to approximate a vector

(74)

by an expansion of the type (55) with
we iterate the procedure of Section IV-B by
found for
, obtained by
needs to be utilized in its

iterating (51). To apply (51),
representation in terms of mapped input images

To this end,

denotes the

Here,

i.e., we need to set

,

(75)

The coefcient
However,

could be computed as

if the vectors

, then the best approximation of

are not
in their
orthogonal in
span is not obtained by computing orthogonal projections
onto each direction. Instead, we need to compute the optimal
anew in each step, using
coefcients
has not yet reached
Proposition 8 (if the discrepancy
will be invertible).
zero, then
The iteration is stopped after

steps, either specied

in advance, or by monitoring when

(i.e.,

falls below a specied threshold. The solution

vector takes the form (55).

We conclude this section by noting that in many cases, such
as multiclass SV machines, or multiple kernel PCA feature
extractors, we may actually want
to approximate several
vectors simultaneously. This leads to more complex equations,
given in [25].

12 Imagine a case where a certain pattern appears twice in the training set,
and the SV expansion has to utilize both of the copies only because the upper
bound constraint limits the coefcient of each of them to C:

Fig. 4. Kernel PCA toy example (see text):
lines of constant feature
value for the rst eight nonlinear principal components extracted with
k(xxx; yyy) = exp (jjxxx  yyyjj2=0:1): The rst two principal components (top
middle/right) separate the three clusters. Components 35 split the clusters.
Components 68 split them again, orthogonal to the above splits.

VI. EXPERIMENTS

To see how the proposed methods work in practice we ran
several toy and real-world experiments. In Section VI-A, we
give denoising results for the approach of nding approxi-
mate preimages presented in Section IV-A. In Section VI-B,
we present some experiments for the reduced set methods
described in Sections V-C and V-D.

A. Kernel PCA Denoising

1) Toy Examples: All experiments reported were carried
out with Gaussian kernels (41), minimizing (42) with the
iteration scheme given by (51). However, similar results
were obtained with polynomial kernels. Matlab code for
computing kernel PCA is available on the web from
http://svm.rst.gmd.de.

We generated an articial data set from three point sources
at ( 0.5, 0.1), (0, 0.7), (0.5, 0.1) (100 points each) with
), and performed kernel PCA on
Gaussian noise (
it (Fig. 4). Using the resulting eigenvectors, we extracted
nonlinear principal components from a set of test points
generated from the same model, and reconstructed the points
from varying numbers of principal components. Fig. 5 shows
that discarding higher order components leads to removal of
the noisethe points move toward their respective sources.

In a second experiment (Table I), we generated a data set

with zero mean and variance

from 11 Gaussians in
in each component, by selecting from each source 100 points
as a training set and 33 points for a test set (centers of the
Gaussians randomly chosen in [ 1, 1]
Then we applied
kernel PCA to the training set and computed the projections
of the points in the test set. With these, we carried out
for each
denoising, yielding an approximate preimage in
test point. This procedure was repeated for different numbers
of components in reconstruction, and for different values of
(also used in the kernel). We compared the results provided
by our algorithm to those of linear PCA via the mean squared
distance of all denoised test points to their corresponding
center. Table I shows the ratio of these values; here and below,
ratios larger than one indicate that kernel PCA performed
and
better than linear PCA. For almost every choice of

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1011

TABLE I

DENOISING GAUSSIANS IN 10 (SEE TEXT). PERFORMANCE RATIOS LARGER THAN
ONE INDICATE HOW MUCH BETTER KERNEL PCA DID, COMPARED TO LINEAR
PCA, FOR DIFFERENT CHOICES OF THE GAUSSIANS STANDARD DEVIATION ;

AND DIFFERENT NUMBERS OF COMPONENTS USED IN RECONSTRUCTION

Fig. 5. Kernel PCA denoising by reconstruction from projections onto the
eigenvectors of Fig. 4. We generated 20 new points from each Gaussian,
represented them in feature space by their rst n = 1; 2;    ; 8 nonlinear
principal components, and computed approximate preimages, shown in the
upper nine pictures (top left: original data, top middle: n = 1, top right:
n = 2, etc.). Note that by discarding higher order principal components (i.e.,
using a small n), we remove the noise inherent in the nonzero variance 
2 of
the Gaussians. The lower nine pictures show how the original points move
in the denoising. Unlike the corresponding case in linear PCA, where we
obtain lines (see Fig. 6), in kernel PCA clusters shrink to points.

Fig. 6. Reconstructions and point movements for linear PCA, based on the
rst principal component.

, kernel PCA did better. Note that using all ten components,
linear PCA is just a basis transformation and hence cannot
denoise. The extreme superiority of kernel PCA for small
is due to the fact that all test points are in this case located
close to the 11 spots in input space, and linear PCA has to
cover them with less than 10 directions. Kernel PCA moves
each point to the correct source even when using only a small
number of components.

To get some intuitive understanding in a low-dimensional
case, Fig. 7 depicts the results of denoising a half circle
and a square in the plane, using kernel PCA, a nonlinear
autoencoder, principal curves, and linear PCA. The principal
curves algorithm [14] iteratively estimates a curve captur-
ing the structure of the data. The data are projected to
the closest point on a curve which the algorithm tries to
construct such that each point
is the average of all data
the only
points projecting onto it. It can be shown that

Fig. 7. Denoising in 2-D (see text). Depicted are the data set (small points)
and its denoised version (big points, joining up to solid lines). For linear
PCA, we used one component for reconstruction, as using two components,
reconstruction is perfect and thus does not denoise. Note that all algorithms
except for our approach have problems in capturing the circular structure in
the bottom example (taken from [21]).

straight lines satisfying the latter are principal components,
so principal curves are a generalization of the latter. The
algorithm uses a smoothing parameter which is annealed
during the iteration. In the nonlinear autoencoder algorithm,
a bottleneck ve-layer network is trained to reproduce the
input values as outputs (i.e., it is used in autoassociative mode).
The hidden unit activations in the third layer form a lower-
dimensional representation of the data, closely related to PCA
(see, for instance, [11]). Training is done by conjugate gradient
descent. In all algorithms, parameter values were selected
such that the best possible denoising result was obtained.
The gure shows that on the closed square problem, kernel
PCA does (subjectively) best, followed by principal curves
and the nonlinear autoencoder; linear PCA fails completely.
However, note that all algorithms except for kernel PCA
actually provide an explicit one-dimensional parameterization
of the data, whereas kernel PCA only provides us with a
means of mapping points to their denoised versions (in this
case, we used four kernel PCA features, and hence obtain a
four-dimensional parameterization).

2) Handwritten Digit Denoising: To test our approach on
real-world data, we also applied the algorithm to the USPS
database of handwritten digits (e.g., [16], [24]) of 7291 training
16). For each of the
patterns and 2007 test patterns (size 16
ten digits, we randomly chose 300 examples from the training
set and 50 examples from the test set. We used the method
of Section IV-B, with
The width 0.5 equals twice the average of the datas variance

1012

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

Fig. 8. Visualization of eigenvectors (see text). Depicted are the 20
row: different visualizations for kernel PCA.

  

;

28th eigenvector (from left to right). First row: linear PCA, second and third

;

in each dimension. In Fig. 8, we give two possible depictions
of the eigenvectors found by kernel PCA, compared to those
found by linear PCA for the USPS set. The second row
shows the approximate preimages of the eigenvectors

,

), lower right

, found by our iterative algorithm. In the third row
is the projection
each image is computed as follows: Pixel
-image of the th canonical basis vector in input space
of the
onto the corresponding eigenvector in feature space [upper left
]. In the linear case, both
(
methods would simply yield the eigenvectors of linear PCA
depicted in the rst row; in this sense, they may be considered
as generalized eigenvectors in input space. We see that the
rst eigenvectors are almost identical (except for arbitrary
signs). However, we also see that eigenvectors in linear PCA
start to focus on high-frequency structures already at smaller
eigenvalue size. To understand this, note that in linear PCA we
only have a maximum number of 256 eigenvectors, contrary
to kernel PCA which gives us the number of training examples
(here 3000) possible eigenvectors.

This also explains some of the results we found when work-
ing with the USPS set (Figs. 9 and 10). In these experiments,
linear and kernel PCA were trained with the original data. To
the test set, we added

1) additive Gaussian noise with zero mean and standard

deviation

or

2) speckle noise, where each pixel is ipped to black or

white with probability

For the noisy test sets, we computed the projections onto
linear and nonlinear components, and carried out
the rst
reconstruction for each case. The results were compared by
taking the mean squared distance of each reconstructed digit
of the noisy test set to its original counterpart.

For the optimal number of components in linear and kernel
PCA, our approach did better by a factor of 1.6 for the
Gaussian noise, and 1.2 for the speckle noise (the optimal
number of components were 32 in linear PCA, and 512 and
256 in kernel PCA, respectively). Taking identical numbers of
components in both algorithms, kernel PCA becomes up to
eight times better than linear PCA. However, note that kernel
PCA comes with a higher computational complexity.

B. Speeding Up Support Vector Decision Rules

As in Section VI-A, we used the USPS handwritten digit
database. We approximated the SV expansions (31) of ten
binary classiers, each trained to separate one digit from the

Fig. 9. Denoising of USPS data (see text). The left half shows: top: the
rst occurrence of each digit in the test set, second row: the upper digit with
additive Gaussian noise ( = 0:5), following ve rows: the reconstruction for
linear PCA using n = 1, 4, 16, 64, 256 components, and, last ve rows: the
results of our approach using the same number of components. In the right
half we show the same but for speckle noise with probability p = 0:2:

rest. We used the Gaussian kernel

, and the approximation techniques described

in Sections V-C and V-D.

The original SV system had on average 254 SVs per clas-
sier. Tables II and III show the classication error results for
approximation using the reduced set selection techniques de-
scribed in Section V-C, while Table IV gives results for using
the reduced set construction method described in Section V-D,
for varying numbers of RS vectors. Shown in each line is the
number of misclassied digits for each single classier and the
error of the combined 10-class machine. In all RS systems, the
optimal SV threshold was recomputed on the training set.

For the method of Section V-C, RSS- means, that for
each binary classier, we removed support vectors until on
average were left; for the method of section Section V-D,
vectors
that we approximated each decision function using
steps using the described iteration procedure). For large
(i.e.,
, i.e., small reductions of the decision functions
numbers
complexity, the accuracy of the original system can be ap-

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1013

Fig. 10. Mean squared error of denoised images versus number of features used, for kernel PCA and linear PCA. Kernel PCA exploits nonlinearities
and has the potential to utilize more features to code structure rather than noise. Therefore, it outperforms linear PCA denoising if a sufciently large
number of features is used.

TABLE II

NUMBERS OF TEST ERRORS FOR EACH BINARY RECOGNIZER, AND TEST ERROR

RATES FOR 10-CLASS CLASSIFICATION USING THE RS METHOD OF SECTION

SECTION V-C.1. TOP: NUMBERS OF SVS FOR THE ORIGINAL SV RBF-SYSTEM.
BOTTOM, FIRST ROW: ORIGINAL SV SYSTEM, WITH 254 SVS ON AVERAGE;

FOLLOWING ROWS: SYSTEMS WITH VARYING AVERAGE NUMBERS OF RS

VECTORS. IN THE SYSTEM RSS-n, EQUAL FRACTIONS OF SVS WERE REMOVED
FROM EACH RECOGNIZER SUCH THAT ON AVERAGE, n RS VECTORS WERE LEFT

TABLE III

NUMBERS OF TEST ERRORS FOR EACH BINARY RECOGNIZER, AND TEST ERROR
RATES FOR TEN-CLASS CLASSIFICATION USING THE RS METHOD OF SECTION
V-C.2 (ci = =jij): FIRST ROW: ORIGINAL SV SYSTEM, WITH 254 SVS ON
AVERAGE; FOLLOWING ROWS: SYSTEMS WITH VARYING AVERAGE NUMBERS OF

RS VECTORS. IN THE SYSTEM RSS2-n,  WAS ADJUSTED SUCH THAT THE
AVERAGE NUMBER OF RS VECTORS LEFT WAS n (THE CONSTANT , GIVEN

IN PARENTHESES, WAS CHOSEN SUCH THAT THE NUMBERS n WERE

COMPARABLE TO TABLE II). THE RESULTS CAN BE FURTHER IMPROVED
USING THE METHOD OF SECTION V-C3 (ci = = maxj jj
i j): FOR

INSTANCE, USING ABOUT 570 EXPANSION VECTORS (WHICH IS THE SAME
NUMBER THAT WE GET WHEN TAKING THE UNION OF ALL SVS IN THE
RSS2  74 SYSTEM), THIS LED TO AN IMPROVED ERROR RATE OF 5.5%

proached closely with both techniques. In Tables II and III, we
see that removal of about 40% of the support vectors leaves the
error practically unchanged. Reducing the numbers of support
vectors further can lead to large performance losses.

The reduced set construction method, which is computa-
tionally and conceptually more complex, performs better in
this situation as it is able to utilize vectors different from the
original support patterns in the expansion. To get a speedup
by a factor of ten, we have to use a system with 25 RS vectors
(RSC-25). For the method in Table IV,
the classication
accuracy only drops moderately from 4.4% to 5.1%, which

1014

IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 10, NO. 5, SEPTEMBER 1999

Fig. 11. Complete display of reduced set vectors constructed by the iterative approach of Section V-D for n = 20, with coefcients (Top: recognizer of
digit 0,   , bottom: digit 9). Note that positive coefcients (roughly) correspond to positive examples in the classication problem.

TABLE IV

NUMBER OF TEST ERRORS FOR EACH BINARY RECOGNIZER, AND TEST ERROR

RATES FOR TEN-CLASS CLASSIFICATION USING THE RS CONSTRUCTION
METHOD OF SECTION V-D. FIRST ROW: ORIGINAL SV SYSTEM, WITH
254 SVS ON AVERAGE (SEE ALSO TABLES II AND III); FOLLOWING
ROWS: SYSTEMS WITH VARYING NUMBERS OF RS VECTORS (RSC-n
STANDS FOR n VECTORS CONSTRUCTED) PER BINARY RECOGNIZER,

COMPUTED BY ITERATING ONE-TERM APPROXIMATIONS, SEPARATELY FOR

EACH RECOGNIZER. LAST TWO ROWS: WITH A SUBSEQUENT GLOBAL

GRADIENT DESCENT, THE RESULTS CAN BE FURTHER IMPROVED (SEE TEST)

the rst phase by about two orders of magnitude): this led
to an error rate of 4.7%. For the considered kernel, this is
almost identical to the traditional RS method, which yielded
5.0% (for polynomial kernels, the latter method led to 4.3%
at the same speedup [6]). Note that the traditional RS method
restarts the second phase many times to make sure that the
global minimum of the cost function is actually found, which
makes it plausible that the nal results are similar.

Finally, Fig. 11 shows the RS-20 vectors of the ten binary
classiers. As an aside, note that unlike the approach of [6],
our algorithm produces images which do look meaningful (i.e.,
digit-like).

VII. DISCUSSION

Algorithms utilizing Mercer kernels construct their solutions
in terms of mapped input
as expansions
is often unknown, or too
patterns. However,
This
complex to provide any intuition about the solution
has motivated our efforts to reduce the complexity of the
expansion, summarized in this paper.

the map

As an extreme case, we have rst studied how to approx-
(i.e., how to nd an approximate
), and proposed a xed-point iteration algorithm

by a single

imate
preimage of
to perform the task.

is still competitive with convolutional neural networks on that
data base [16]. Moreover, we can further improve this result
by adding the second phase of the traditional RS algorithm,
where a global gradient descent is performed in the space
[6], [9] (computationally more expensive than
of all

In situations where no good approximate preimage exists,
by
one can still reduce the complexity of
We
expressing it as a sparser expansion
have proposed methods for computing the optimal coefcients
either by
or by iterating the above preimage

and for coming up with suitable patterns

selecting among the
algorithm.

SCH OLKOPF et al.: INPUT SPACE VERSUS FEATURE SPACE

1015

Both types of approximations are of theoretical interest for
feature space methods; however, they also lead to practical
applications. In this paper, we have considered two such
applications, namely, the problem of statistical denoising via
kernel PCA reconstruction, and the problem of speeding up
SV decision rules. We address them in turn.

components, where

Kernel PCA Denoising: In denoising experiments on real-
world data, we obtained results signicantly better than using
linear PCA. Our interpretation of this nding is as follows.
is
Linear PCA can extract at most
the dimensionality of the data. Being a basis transform, all
components together fully describe the data. If the data are
noisy, this implies that a certain fraction of the components
will be devoted to the extraction of noise. Kernel PCA, on the
other hand, allows the extraction of up to features, where
is
the number of training examples. Accordingly, kernel PCA can
provide a larger number of features carrying information about
).
the structure in the data (in our experiments, we had
In addition, if the structure to be extracted is nonlinear, then
linear PCA must necessarily fail, as we have illustrated with
toy examples.

Open questions and problems include the choice of a
suitable kernel for a given noise reduction problem, possibly
in conjunction with the regularization properties of the kernel
(e.g., [38]), the application of the approach to compression,
and the comparison (and connection) to alternative nonlinear
denoising methods (cf., [17]).

Speeding Up SV Machines: We have shown experimentally
that our approximation algorithms can be used to speed up SV
machines signicantly. Note that in the Gaussian RBF case,
the approximation can never be as good as the original, since
the kernel matrix

has full rank [19].

As in [9], good RS construction results were obtained even
though the objective function did not decrease to zero (in
our RS construction experiments, it was reduced by a factor
of two20 in the rst phase, depending on how many RS
vectors were computed; the global gradient descent yielded
another factor twothree). We conjecture that this is due to the
,
following: in classication, we are not interested in
but in

, where

is the underlying probability distribution
of the patterns (cf., [3]). This is consistent with the fact that
the performance of a RS SV classier can be improved by
recomputing an optimal threshold

The previous RS construction method [6], [9] can be used

for any SV kernel; the new one is limited to

However, it is fast, and it led to interpretable RS images
and an interesting connection between clustering and approx-
imation in feature spaces. It appears intriguing to pursue the
question whether this connection could be exploited to form
more general types of approximations of SV and kernel PCA
expansions by making use of Gaussians of variable widths.

The RS selection methods, on the other hand, are appli-
cable for any SV kernel. In our experiments, they led to
worse reduction rates than RS construction; however, they
are simpler and computationally faster. Among the rst two
RS selection methods, the one described in Section V-C.1
was slightly superior at higher reductions; however, the one

given in Section V-C.2 is computationally cheaper since unlike
the former, it does not remove the SVs one at a time and
therefore it need not be iterated. Moreover, we found that
it can be improved by simultaneously approximating several
vectors, corresponding to the ten binary recognizers in a digit
recognition task.

The proposed methods are applicable to any feature space
algorithm based on Mercer kernels. For instance, we could
also speed up SV regression machines or kernel PCA feature
extractors. Moreover, we expect further possibilities to open up
in the future, as Mercer kernel methods are being applied in an
increasing number of learning and signal processing problems.

ACKNOWLEDGMENT

The authors would like to thank A. Elisseeff for helpful

discussions.

