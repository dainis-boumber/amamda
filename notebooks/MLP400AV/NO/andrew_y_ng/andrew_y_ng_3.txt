Abstract

Semantic word spaces have been very use-
ful but cannot express the meaning of longer
phrases in a principled way. Further progress
towards understanding compositionality in
tasks such as sentiment detection requires
richer supervised training and evaluation re-
sources and more powerful models of com-
position. To remedy this, we introduce a
Sentiment Treebank. It includes ne grained
sentiment labels for 215,154 phrases in the
parse trees of 11,855 sentences and presents
new challenges for sentiment composition-
ality. To address them, we introduce the
Recursive Neural Tensor Network. When
trained on the new treebank, this model out-
performs all previous methods on several met-
rics.
It pushes the state of the art in single
sentence positive/negative classication from
80% up to 85.4%. The accuracy of predicting
ne-grained sentiment labels for all phrases
reaches 80.7%, an improvement of 9.7% over
bag of features baselines. Lastly, it is the only
model that can accurately capture the effects
of negation and its scope at various tree levels
for both positive and negative phrases.

1

Introduction

Semantic vector spaces for single words have been
widely used as features (Turney and Pantel, 2010).
Because they cannot capture the meaning of longer
phrases properly, compositionality in semantic vec-
tor spaces has recently received a lot of attention
(Mitchell and Lapata, 2010; Socher et al., 2010;
Zanzotto et al., 2010; Yessenalina and Cardie, 2011;
Socher et al., 2012; Grefenstette et al., 2013). How-
ever, progress is held back by the current lack of
large and labeled compositionality resources and

Figure 1: Example of the Recursive Neural Tensor Net-
work accurately predicting 5 sentiment classes, very neg-
ative to very positive ( , , 0, +, + +), at every node of a
parse tree and capturing the negation and its scope in this
sentence.

models to accurately capture the underlying phe-
nomena presented in such data. To address this need,
we introduce the Stanford Sentiment Treebank and
a powerful Recursive Neural Tensor Network that
can accurately predict the compositional semantic
effects present in this new corpus.

The Stanford Sentiment Treebank is the rst cor-
pus with fully labeled parse trees that allows for a
complete analysis of the compositional effects of
sentiment in language. The corpus is based on
the dataset introduced by Pang and Lee (2005) and
consists of 11,855 single sentences extracted from
movie reviews.
It was parsed with the Stanford
parser (Klein and Manning, 2003) and includes a
total of 215,154 unique phrases from those parse
trees, each annotated by 3 human judges. This new
dataset allows us to analyze the intricacies of senti-
ment and to capture complex linguistic phenomena.
Fig. 1 shows one of the many examples with clear
compositional structure. The granularity and size of

00This0lm0does0nt0+care+0about+++++cleverness0,0wit0or+00any00other+kind+0of++intelligent++humor0.this dataset will enable the community to train com-
positional models that are based on supervised and
structured machine learning techniques. While there
are several datasets with document and chunk labels
available, there is a need to better capture sentiment
from short comments, such as Twitter data, which
provide less overall signal per document.

In order to capture the compositional effects with
higher accuracy, we propose a new model called the
Recursive Neural Tensor Network (RNTN). Recur-
sive Neural Tensor Networks take as input phrases
of any length. They represent a phrase through word
vectors and a parse tree and then compute vectors for
higher nodes in the tree using the same tensor-based
composition function. We compare to several super-
vised, compositional models such as standard recur-
sive neural networks (RNN) (Socher et al., 2011b),
matrix-vector RNNs (Socher et al., 2012), and base-
lines such as neural networks that ignore word order,
Naive Bayes (NB), bi-gram NB and SVM. All mod-
els get a signicant boost when trained with the new
dataset but the RNTN obtains the highest perfor-
mance with 80.7% accuracy when predicting ne-
grained sentiment for all nodes. Lastly, we use a test
set of positive and negative sentences and their re-
spective negations to show that, unlike bag of words
models, the RNTN accurately captures the sentiment
change and scope of negation. RNTNs also learn
that sentiment of phrases following the contrastive
conjunction but dominates.

The complete training and testing code, a live
demo and the Stanford Sentiment Treebank dataset
are available at http://nlp.stanford.edu/
sentiment.

2 Related Work

This work is connected to ve different areas of NLP
research, each with their own large amount of related
work to which we cannot do full justice given space
constraints.
Semantic Vector Spaces.
The dominant ap-
proach in semantic vector spaces uses distributional
similarities of single words. Often, co-occurrence
statistics of a word and its context are used to de-
scribe each word (Turney and Pantel, 2010; Baroni
and Lenci, 2010), such as tf-idf. Variants of this idea
use more complex frequencies such as how often a

word appears in a certain syntactic context (Pado
and Lapata, 2007; Erk and Pado, 2008). However,
distributional vectors often do not properly capture
the differences in antonyms since those often have
similar contexts. One possibility to remedy this is to
use neural word vectors (Bengio et al., 2003). These
vectors can be trained in an unsupervised fashion
to capture distributional similarities (Collobert and
Weston, 2008; Huang et al., 2012) but then also be
ne-tuned and trained to specic tasks such as sen-
timent detection (Socher et al., 2011b). The models
in this paper can use purely supervised word repre-
sentations learned entirely on the new corpus.
Compositionality in Vector Spaces.
Most of
the compositionality algorithms and related datasets
capture two word compositions. Mitchell and La-
pata (2010) use e.g. two-word phrases and analyze
similarities computed by vector addition, multiplica-
tion and others. Some related models such as holo-
graphic reduced representations (Plate, 1995), quan-
tum logic (Widdows, 2008), discrete-continuous
models (Clark and Pulman, 2007) and the recent
compositional matrix space model (Rudolph and
Giesbrecht, 2010) have not been experimentally val-
idated on larger corpora. Yessenalina and Cardie
(2011) compute matrix representations for longer
phrases and dene composition as matrix multipli-
cation, and also evaluate on sentiment. Grefen-
stette and Sadrzadeh (2011) analyze subject-verb-
object triplets and nd a matrix-based categorical
model to correlate well with human judgments. We
compare to the recent line of work on supervised
compositional models.
In particular we will de-
scribe and experimentally compare our new RNTN
model to recursive neural networks (RNN) (Socher
et al., 2011b) and matrix-vector RNNs (Socher et
al., 2012) both of which have been applied to bag of
words sentiment corpora.
Logical Form.
A related eld that tackles com-
positionality from a very different angle is that of
trying to map sentences to logical form (Zettlemoyer
and Collins, 2005). While these models are highly
interesting and work well in closed domains and
on discrete sets, they could only capture sentiment
distributions using separate mechanisms beyond the
currently used logical forms.
Deep Learning. Apart from the above mentioned

work on RNNs, several compositionality ideas re-
lated to neural networks have been discussed by Bot-
tou (2011) and Hinton (1990) and rst models such
as Recursive Auto-associative memories been exper-
imented with by Pollack (1990). The idea to relate
inputs through three way interactions, parameterized
by a tensor have been proposed for relation classi-
cation (Sutskever et al., 2009; Jenatton et al., 2012),
extending Restricted Boltzmann machines (Ranzato
and Hinton, 2010) and as a special layer for speech
recognition (Yu et al., 2012).
Sentiment Analysis.
Apart from the above-
mentioned work, most approaches in sentiment anal-
ysis use bag of words representations (Pang and Lee,
2008). Snyder and Barzilay (2007) analyzed larger
reviews in more detail by analyzing the sentiment
of multiple aspects of restaurants, such as food or
atmosphere. Several works have explored sentiment
compositionality through careful engineering of fea-
tures or polarity shifting rules on syntactic structures
(Polanyi and Zaenen, 2006; Moilanen and Pulman,
2007; Rentoumi et al., 2010; Nakagawa et al., 2010).

3 Stanford Sentiment Treebank

Bag of words classiers can work well in longer
documents by relying on a few words with strong
sentiment like awesome or exhilarating. How-
ever, sentiment accuracies even for binary posi-
tive/negative classication for single sentences has
not exceeded 80% for several years. For the more
difcult multiclass case including a neutral class,
accuracy is often below 60% for short messages
on Twitter (Wang et al., 2012). From a linguistic
or cognitive standpoint, ignoring word order in the
treatment of a semantic task is not plausible, and, as
we will show, it cannot accurately classify hard ex-
amples of negation. Correctly predicting these hard
cases is necessary to further improve performance.
In this section we will introduce and provide some
analyses for the new Sentiment Treebank which in-
cludes labels for every syntactically plausible phrase
in thousands of sentences, allowing us to train and
evaluate compositional models.

We consider the corpus of movie review excerpts
from the rottentomatoes.com website orig-
inally collected and published by Pang and Lee
(2005). The original dataset includes 10,662 sen-

Figure 3: The labeling interface. Random phrases were
shown and annotators had a slider for selecting the senti-
ment and its degree.

tences, half of which were considered positive and
the other half negative. Each label is extracted from
a longer movie review and reects the writers over-
all intention for this review. The normalized, lower-
cased text is rst used to recover, from the origi-
nal website, the text with capitalization. Remaining
HTML tags and sentences that are not in English
are deleted. The Stanford Parser (Klein and Man-
ning, 2003) is used to parses all 10,662 sentences.
In approximately 1,100 cases it splits the snippet
into multiple sentences. We then used Amazon Me-
chanical Turk to label the resulting 215,154 phrases.
Fig. 3 shows the interface annotators saw. The slider
has 25 different values and is initially set to neutral.
The phrases in each hit are randomly sampled from
the set of all phrases in order to prevent labels being
inuenced by what follows. For more details on the
dataset collection, see supplementary material.

Fig. 2 shows the normalized label distributions at
each n-gram length. Starting at length 20, the ma-
jority are full sentences. One of the ndings from
labeling sentences based on readers perception is
that many of them could be considered neutral. We
also notice that stronger sentiment often builds up
in longer phrases and the majority of the shorter
phrases are neutral. Another observation is that most
annotators moved the slider to one of the ve po-
sitions: negative, somewhat negative, neutral, posi-
tive or somewhat positive. The extreme values were
rarely used and the slider was not often left in be-
tween the ticks. Hence, even a 5-class classication
into these categories captures the main variability
of the labels. We will name this ne-grained senti-
ment classication and our main experiment will be
to recover these ve labels for phrases of all lengths.

nerdy folks|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositivephenomenal fantasy best sellers|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositive  Figure 2: Normalized histogram of sentiment annotations at each n-gram length. Many shorter n-grams are neutral;
longer phrases are well distributed. Few annotators used slider positions between ticks or the extreme values. Hence
the two strongest labels and intermediate tick positions are merged into 5 classes.

4 Recursive Neural Models

The models in this section compute compositional
vector representations for phrases of variable length
and syntactic type. These representations will then
be used as features to classify each phrase. Fig. 4
displays this approach. When an n-gram is given to
the compositional models, it is parsed into a binary
tree and each leaf node, corresponding to a word,
is represented as a vector. Recursive neural mod-
els will then compute parent vectors in a bottom
up fashion using different types of compositional-
ity functions g. The parent vectors are again given
as features to a classier. For ease of exposition,
we will use the tri-gram in this gure to explain all
models.

We rst describe the operations that the below re-
cursive neural models have in common: word vector
representations and classication. This is followed
by descriptions of two previous RNN models and
our RNTN.

Each word is represented as a d-dimensional vec-
tor. We initialize all word vectors by randomly
sampling each value from a uniform distribution:
U(r, r), where r = 0.0001. All the word vec-
tors are stacked in the word embedding matrix L 
Rd|V |, where |V | is the size of the vocabulary. Ini-
tially the word vectors will be random but the L ma-
trix is seen as a parameter that is trained jointly with
the compositionality models.

We can use the word vectors immediately as
parameters to optimize and as feature inputs to
a softmax classier. For classication into ve
classes, we compute the posterior probability over

Figure 4: Approach of Recursive Neural Network mod-
els for sentiment: Compute parent vectors in a bottom up
fashion using a compositionality function g and use node
vectors as features for a classier at that node. This func-
tion varies for the different models.

labels given the word vector via:

ya = softmax(Wsa),

(1)
where Ws  R5d is the sentiment classication
matrix. For the given tri-gram, this is repeated for
vectors b and c. The main task of and difference
between the models will be to compute the hidden
vectors pi  Rd in a bottom up fashion.
4.1 RNN: Recursive Neural Network
The simplest member of this family of neural net-
work models is the standard recursive neural net-
work (Goller and Kuchler, 1996; Socher et al.,
2011a). First, it is determined which parent already
has all its children computed. In the above tree ex-
ample, p1 has its two childrens vectors since both
are words. RNNs use the following equations to
compute the parent vectors:

51015202530354045N-Gram Length0%20%40%60%80%100%% of Sentiment ValuesNeutralSomewhat PositivePositiveVery PositiveSomewhat NegativeNegativeVery Negative(a)(a)(b)(b)(c)(c)(d)(d)Distributions of sentiment values for (a) unigrams, (b) 10-grams, (c) 20-grams, and (d) full sentences.    not      very       good ...        a          b             c p1 =g(b,c)p2 = g(a,p1)00+++-(cid:18)

p1 = f

W

(cid:21)(cid:19)

(cid:20) b

c

(cid:18)

, p2 = f

W

(cid:20) a

p1

(cid:21)(cid:19)

,

where f = tanh is a standard element-wise nonlin-
earity, W  Rd2d is the main parameter to learn
and we omit the bias for simplicity. The bias can be
added as an extra column to W if an additional 1 is
added to the concatenation of the input vectors. The
parent vectors must be of the same dimensionality to
be recursively compatible and be used as input to the
next composition. Each parent vector pi, is given to
the same softmax classier of Eq. 1 to compute its
label probabilities.

This model uses the same compositionality func-
tion as the recursive autoencoder (Socher et al.,
2011b) and recursive auto-associate memories (Pol-
lack, 1990). The only difference to the former model
is that we x the tree structures and ignore the re-
construction loss. In initial experiments, we found
that with the additional amount of training data, the
reconstruction loss at each node is not necessary to
obtain high performance.

4.2 MV-RNN: Matrix-Vector RNN

The MV-RNN is linguistically motivated in that
most of the parameters are associated with words
and each composition function that computes vec-
tors for longer phrases depends on the actual words
being combined. The main idea of the MV-RNN
(Socher et al., 2012) is to represent every word and
longer phrase in a parse tree as both a vector and
a matrix. When two constituents are combined the
matrix of one is multiplied with the vector of the
other and vice versa. Hence, the compositional func-
tion is parameterized by the words that participate in
it.

Each words matrix is initialized as a dd identity
matrix, plus a small amount of Gaussian noise. Sim-
ilar to the random word vectors, the parameters of
these matrices will be trained to minimize the clas-
sication error at each node. For this model, each n-
gram is represented as a list of (vector,matrix) pairs,
together with the parse tree. For the tree with (vec-
tor,matrix) nodes:

(p2,P2)

(a,A)

(p1,P1)

(b,B)

(c,C)

the MV-RNN computes the rst parent vector and its
matrix via two equations:

(cid:18)

(cid:21)(cid:19)

(cid:20) Cb

Bc

(cid:18)

(cid:20) B

C

(cid:21)(cid:19)

,

p1 = f

W

, P1 = f

WM

where WM  Rd2d and the result is again a d  d
matrix. Similarly, the second parent node is com-
puted using the previously computed (vector,matrix)
pair (p1, P1) as well as (a, A). The vectors are used
for classifying each phrase using the same softmax
classier as in Eq. 1.

4.3 RNTN:Recursive Neural Tensor Network
One problem with the MV-RNN is that the number
of parameters becomes very large and depends on
the size of the vocabulary. It would be cognitively
more plausible if there was a single powerful com-
position function with a xed number of parameters.
The standard RNN is a good candidate for such a
function. However, in the standard RNN, the input
vectors only implicitly interact through the nonlin-
earity (squashing) function. A more direct, possibly
multiplicative, interaction would allow the model to
have greater interactions between the input vectors.
Motivated by these ideas we ask the question: Can
a single, more powerful composition function per-
form better and compose aggregate meaning from
smaller constituents more accurately than many in-
put specic ones? In order to answer this question,
we propose a new model called the Recursive Neu-
ral Tensor Network (RNTN). The main idea is to use
the same, tensor-based composition function for all
nodes.
Fig. 5 shows a single tensor layer. We dene the
output of a tensor product h  Rd via the follow-
ing vectorized notation and the equivalent but more
detailed notation for each slice V [i]  Rdd:

(cid:21)T

(cid:20) b

c

(cid:21)

(cid:20) b

c

(cid:21)T

(cid:20) b

c

(cid:20) b

c

(cid:21)

.

h =

V [1:d]

; hi =

V [i]

where V [1:d]  R2d2dd is the tensor that denes
multiple bilinear forms.

softmax classier trained on its vector representa-
tion to predict a given ground truth or target vector
t. We assume the target distribution vector at each
node has a 0-1 encoding. If there are C classes, then
it has length C and a 1 at the correct label. All other
entries are 0.

We want to maximize the probability of the cor-
rect prediction, or minimize the cross-entropy error
between the predicted distribution yi  RC1 at
node i and the target distribution ti  RC1 at that
node. This is equivalent (up to a constant) to mini-
mizing the KL-divergence between the two distribu-
tions. The error as a function of the RNTN parame-
ters  = (V, W, Ws, L) for a sentence is:
j + (cid:107)(cid:107)2

(cid:88)

(cid:88)

ti
j log yi

E() =

(2)

i

j

The derivative for the weights of the softmax clas-
sier are standard and simply sum up from each
nodes error. We dene xi to be the vector at node
i (in the example trigram, the xi  Rd1s are
(a, b, c, p1, p2)). We skip the standard derivative for
Ws. Each node backpropagates its error through to
the recursively used weights V, W . Let i,s  Rd1
be the softmax error vector at node i:

i,s =(cid:0)W T

s (yi  ti)(cid:1)  f(cid:48)(xi),

where  is the Hadamard product between the two
vectors and f(cid:48) is the element-wise derivative of f
which in the standard case of using f = tanh can
be computed using only f (xi).

The remaining derivatives can only be computed
in a top-down fashion from the top node through the
tree and into the leaf nodes. The full derivative for
V and W is the sum of the derivatives at each of
the nodes. We dene the complete incoming error
messages for a node i as i,com. The top node, in
our case p2, only received errors from the top nodes
softmax. Hence, p2,com = p2,s which we can
use to obtain the standard backprop derivative for
W (Goller and Kuchler, 1996; Socher et al., 2010).
For the derivative of each slice k = 1, . . . , d, we get:

(cid:20) a

(cid:21)(cid:20) a

p1

p1

(cid:21)T

,

Ep2
V [k]

= p2,com

k

where p2,com
is just the kth element of this vector.
Now, we can compute the error message for the two

k

Figure 5: A single layer of the Recursive Neural Ten-
sor Network. Each dashed box represents one of d-many
slices and can capture a type of inuence a child can have
on its parent.

The RNTN uses this denition for computing p1:

p1 = f

V [1:d]

+ W

,

where W is as dened in the previous models. The
next parent vector p2 in the tri-gram will be com-
puted with the same weights:

(cid:21)T

(cid:32)(cid:20) b

c

(cid:21)T

(cid:32)(cid:20) a

p1

(cid:21)

(cid:20) b

c

(cid:21)

(cid:20) a

p1

(cid:21)(cid:33)

(cid:20)b

c

(cid:21)(cid:33)

(cid:20) a

p1

.

p2 = f

V [1:d]

+ W

The main advantage over the previous RNN
model, which is a special case of the RNTN when
V is set to 0, is that the tensor can directly relate in-
put vectors. Intuitively, we can interpret each slice
of the tensor as capturing a specic type of compo-
sition.

An alternative to RNTNs would be to make the
compositional function more powerful by adding a
second neural network layer. However, initial exper-
iments showed that it is hard to optimize this model
and vector interactions are still more implicit than in
the RNTN.

4.4 Tensor Backprop through Structure
We describe in this section how to train the RNTN
model. As mentioned above, each node has a

Slices of       Standard                   Tensor Layer         Layerp = f             V[1:2]        +   WNeural Tensor LayerbcbcbcTp = f                             +          children of p2:

(cid:18)

p2,down =

W T p2,com + S

 f(cid:48)(cid:18)(cid:20) a
(cid:19)
V [k](cid:17)T(cid:19)(cid:20) a

p1

p1

(cid:21)(cid:19)

,

(cid:21)

(cid:18)

V [k] +

(cid:16)

where we dene

d(cid:88)

k=1

S =

p2,com
k

The children of p2, will then each take half of this
vector and add their own softmax error message for
the complete . In particular, we have

p1,com = p1,s + p2,down[d + 1 : 2d],

where p2,down[d + 1 : 2d] indicates that p1 is the
right child of p2 and hence takes the 2nd half of the
error, for the nal word vector derivative for a, it
will be p2,down[1 : d].

The full derivative for slice V [k] for this trigram

tree then is the sum at each node:

E
V [k]

=

Ep2
V [k]

+ p1,com

k

(cid:20) b

(cid:21)(cid:20) b

c

c

(cid:21)T

,

and similarly for W . For this nonconvex optimiza-
tion we use AdaGrad (Duchi et al., 2011) which con-
verges in less than 3 hours to a local optimum.

5 Experiments

We include two types of analyses. The rst type in-
cludes several large quantitative evaluations on the
test set. The second type focuses on two linguistic
phenomena that are important in sentiment.

For all models, we use the dev set and cross-
validate over regularization of the weights, word
vector size as well as learning rate and minibatch
size for AdaGrad. Optimal performance for all mod-
els was achieved at word vector sizes between 25
and 35 dimensions and batch sizes between 20 and
30. Performance decreased at larger or smaller vec-
tor and batch sizes. This indicates that the RNTN
does not outperform the standard RNN due to sim-
ply having more parameters. The MV-RNN has or-
ders of magnitudes more parameters than any other
model due to the word matrices. The RNTN would
usually achieve its best performance on the dev set
after training for 3 - 5 hours.
Initial experiments

Model

NB
SVM
BiNB
VecAvg
RNN
MV-RNN
RNTN

Fine-grained
Root
All
41.0
67.2
64.3
40.7
41.9
71.0
32.7
73.3
43.2
79.0
78.7
44.4
45.7
80.7

Positive/Negative
All
82.6
84.6
82.7
85.1
86.1
86.8
87.6

Root
81.8
79.4
83.1
80.1
82.4
82.9
85.4

Table 1: Accuracy for ne grained (5-class) and binary
predictions at the sentence level (root) and for all nodes.

showed that the recursive models worked signi-
cantly worse (over 5% drop in accuracy) when no
nonlinearity was used. We use f = tanh in all ex-
periments.

We compare to commonly used methods that use
bag of words features with Naive Bayes and SVMs,
as well as Naive Bayes with bag of bigram features.
We abbreviate these with NB, SVM and biNB. We
also compare to a model that averages neural word
vectors and ignores word order (VecAvg).

The sentences in the treebank were split into a
train (8544), dev (1101) and test splits (2210) and
these splits are made available with the data release.
We also analyze performance on only positive and
negative sentences, ignoring the neutral class. This
lters about 20% of the data with the three sets hav-
ing 6920/872/1821 sentences.

5.1 Fine-grained Sentiment For All Phrases
The main novel experiment and evaluation metric
analyze the accuracy of ne-grained sentiment clas-
sication for all phrases. Fig. 2 showed that a ne
grained classication into 5 classes is a reasonable
approximation to capture most of the data variation.
Fig. 6 shows the result on this new corpus. The
RNTN gets the highest performance, followed by
the MV-RNN and RNN. The recursive models work
very well on shorter phrases, where negation and
composition are important, while bag of features
baselines perform well only with longer sentences.
The RNTN accuracy upper bounds other models at
most n-gram lengths.

Table 1 (left) shows the overall accuracy numbers
for ne grained prediction at all phrase lengths and
full sentences.

Figure 6: Accuracy curves for ne grained sentiment classication at each n-gram lengths. Left: Accuracy separately
for each set of n-grams. Right: Cumulative accuracy of all  n-grams.

5.2 Full Sentence Binary Sentiment
This setup is comparable to previous work on the
original rotten tomatoes dataset which only used
full sentence labels and binary classication of pos-
itive/negative. Hence, these experiments show the
improvement even baseline methods can achieve
with the sentiment treebank. Table 1 shows results
of this binary classication for both all phrases and
for only full sentences. The previous state of the
art was below 80% (Socher et al., 2012). With the
coarse bag of words annotation for training, many of
the more complex phenomena could not be captured,
even by more powerful models. The combination of
the new sentiment treebank and the RNTN pushes
the state of the art on short phrases up to 85.4%.

5.3 Model Analysis: Contrastive Conjunction
In this section, we use a subset of the test set which
includes only sentences with an X but Y  structure:
A phrase X being followed by but which is followed
by a phrase Y . The conjunction is interpreted as
an argument for the second conjunct, with the rst
functioning concessively (Lakoff, 1971; Blakemore,
1989; Merin, 1999). Fig. 7 contains an example. We
analyze a strict setting, where X and Y are phrases
of different sentiment (including neutral). The ex-
ample is counted as correct, if the classications for
both phrases X and Y are correct. Furthermore,
the lowest node that dominates both of the word
but and the node that spans Y also have to have the
same correct sentiment. For the resulting 131 cases,
the RNTN obtains an accuracy of 41% compared to
MV-RNN (37), RNN (36) and biNB (27).

5.4 Model Analysis: High Level Negation
We investigate two types of negation. For each type,
we use a separate dataset for evaluation.

Figure 7: Example of correct prediction for contrastive
conjunction X but Y .

Set 1: Negating Positive Sentences.
The rst set
contains positive sentences and their negation.
In
this set, the negation changes the overall sentiment
of a sentence from positive to negative. Hence, we
compute accuracy in terms of correct sentiment re-
versal from positive to negative. Fig. 9 shows two
examples of positive negation the RNTN correctly
classied, even if negation is less obvious in the case
of least. Table 2 (left) gives the accuracies over 21
positive sentences and their negation for all models.
The RNTN has the highest reversal accuracy, show-
ing its ability to structurally learn negation of posi-
tive sentences. But what if the model simply makes
phrases very negative when negation is in the sen-
tence? The next experiments show that the model
captures more than such a simplistic negation rule.
Set 2: Negating Negative Sentences.
The sec-
ond set contains negative sentences and their nega-
tion. When negative sentences are negated, the sen-
timent treebank shows that overall sentiment should
become less negative, but not necessarily positive.
For instance, The movie was terrible is negative
but the The movie was not terrible says only that it
was less bad than a terrible one, not that it was good
(Horn, 1989; Israel, 2001). Hence, we evaluate ac-

510152025N-GramLength0.20.40.60.81.0Accuracy510152025N-GramLength0.60.70.80.91.0CumulativeAccuracyModelRNTNMV-RNNRNNbiNBNB++0There0are0slow0andrepetitive0parts0,0but+0it+00has00just0enough++spice+0to+0keep+0it+interesting0.Figure 9: RNTN prediction of positive and negative (bottom right) sentences and their negation.

Model

biNB
RNN
MV-RNN
RNTN

Accuracy

Negated Positive Negated Negative

19.0
33.3
52.4
71.4

27.3
45.5
54.6
81.8

Table 2: Accuracy of negation detection. Negated posi-
tive is measured as correct sentiment inversions. Negated
negative is measured as increases in positive activations.

curacy in terms of how often each model was able
to increase non-negative activation in the sentiment
of the sentence. Table 2 (right) shows the accuracy.
In over 81% of cases, the RNTN correctly increases
the positive activations. Fig. 9 (bottom right) shows
a typical case in which sentiment was made more
positive by switching the main class from negative
to neutral even though both not and dull were nega-
tive. Fig. 8 shows the changes in activation for both
sets. Negative values indicate a decrease in aver-

Figure 8: Change in activations for negations. Only the
RNTN correctly captures both types. It decreases positive
sentiment more when it is negated and learns that negat-
ing negative phrases (such as not terrible) should increase
neutral and positive activations.

age positive activation (for set 1) and positive values
mean an increase in average positive activation (set
2). The RNTN has the largest shifts in the correct di-
rections. Therefore we can conclude that the RNTN
is best able to identify the effect of negations upon
both positive and negative sentiment sentences.

++00Roger0Dodger++0is+0one+0of++0the++0most+compelling0variations00on00this0theme0.00Roger0Dodger0is0one0of0theleast+compelling0variations00on00this0theme0.+0I+++liked000every00single0minute00of00this0lm0.0I00did0nt00like000a00single0minute00of00this0lm0.0It00s0just+incrediblydull0.00It00000s+denitelynotdull0.-0.6-0.4-0.20.00.20.4biNBRRNMV-RNNRNTN-0.57-0.34-0.16-0.5NegatedPositiveSentences:ChangeinActivation-0.6-0.4-0.20.00.20.4biNBRRNMV-RNNRNTN+0.35+0.010.010.01NegatedNegativeSentences:ChangeinActivationn
1
2

3

5

8

Most positive n-grams
engaging; best; powerful; love; beautiful
excellent performances; A masterpiece; masterful
lm; wonderful movie; marvelous performances
an amazing performance; wonderful all-ages tri-
umph; a wonderful movie; most visually stunning
nicely acted and beautifully shot; gorgeous im-
agery, effective performances;
the best of the
year; a terric American sports movie; refresh-
ingly honest and ultimately touching
one of the best lms of the year; A love for lms
shines through each frame; created a masterful
piece of artistry right here; A masterful lm from
a master lmmaker,

Most negative n-grams
bad; dull; boring; fails; worst; stupid; painfully
worst movie; very bad; shapeless mess; worst
thing; instantly forgettable; complete failure
for worst movie; A lousy movie; a complete fail-
ure; most painfully marginal; very bad sign
silliest and most incoherent movie; completely
crass and forgettable movie;
just another bad
movie. A cumbersome and cliche-ridden movie;
a humorless, disjointed mess
A trashy, exploitative, thoroughly unpleasant ex-
perience ; this sloppy drama is an empty ves-
sel.; quickly drags on becoming boring and pre-
dictable.; be the worst special-effects creation of
the year

Table 3: Examples of n-grams for which the RNTN predicted the most positive and most negative responses.

6 Conclusion

We introduced Recursive Neural Tensor Networks
and the Stanford Sentiment Treebank. The combi-
nation of new model and data results in a system
for single sentence sentiment detection that pushes
state of the art by 5.4% for positive/negative sen-
tence classication. Apart from this standard set-
ting, the dataset also poses important new challenges
and allows for new evaluation metrics. For instance,
the RNTN obtains 80.7% accuracy on ne-grained
sentiment prediction across all phrases and captures
negation of different sentiments and scope more ac-
curately than previous models.

Acknowledgments

We thank Rukmani Ravisundaram and Tayyab
Tariq for the rst version of the online demo.
Richard is partly supported by a Microsoft Re-
search PhD fellowship. The authors gratefully ac-
knowledge the support of the Defense Advanced Re-
search Projects Agency (DARPA) Deep Exploration
and Filtering of Text (DEFT) Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-13-2-0040, the DARPA Deep Learning
program under contract number FA8650-10-C-7020
and NSF IIS-1159679. Any opinions, ndings, and
conclusions or recommendations expressed in this
material are those of the authors and do not neces-
sarily reect the view of DARPA, AFRL, or the US
government.

Figure 10: Average ground truth sentiment of top 10 most
positive n-grams at various n. The RNTN correctly picks
the more negative and positive examples.

5.5 Model Analysis: Most Positive and

Negative Phrases

We queried the model for its predictions on what
the most positive or negative n-grams are, measured
as the highest activation of the most negative and
most positive classes. Table 3 shows some phrases
from the dev set which the RNTN selected for their
strongest sentiment.

Due to lack of space we cannot compare top
phrases of the other models but Fig. 10 shows that
the RNTN selects more strongly positive phrases at
most n-gram lengths compared to other models.

For this and the previous experiment, please nd
additional examples and descriptions in the supple-
mentary material.

