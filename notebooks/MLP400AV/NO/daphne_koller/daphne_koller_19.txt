Abstract

contexts,

properties

independence

the conditional

independencies

of the distribution,

i.e., given a specific as

however, that there are certain inde

This allows a natural and compact

and supports effective inference

eases knowledge ac
algorithms.

that we cannot capture qualitatively
within
that

Bayesian networks provide a language for qualitatively
representing
of a distribution.
representation
quisition,
It is well-known,
pendencies
the Bayesian network structure:
hold only in certain
signment of values to certain variables..
per, we propose a formal notion of context-specific
dependence
tional probability
a technique,
for determining
given network. We then focus on a particular
quali
tative representation
for capturing CSI. We suggest ways in which this rep
resentation
algorithms.
composition
prove the performance
alternative

can be used to support effective inference
In particular,
of the resulting

de
network which can im
algorithms,
and an

of clustering
based on cutset conditioning.

in the condi
tables (CPTs) at a node. We present

to (and based on) d-separation,

(CSI), based on regularities

when such independence

we present a structural

In this pa
in

analogous

algorithm

holds in a

scheme-tree-structured  CPTs

in BNs in much the same way indepen

in current

BN represen

We formally chatacterize

and catalog

a number of the

it provides.

and inference

correlations
between

is exploited
algorithms.
representation

acyclic
a random variable

can be exploited
dence among variables
tations
this structured
advantages
A BN is a directed
resents
direct
edges between
variables
dence. More precisely,
independent
P(z [ x ) for all values
Z. A BN encodes
about each random variable:
non-descendants
in the network
[14]. For example,
independent
pendence
can be read from the network
using a graph-theoretic
criterion

given a set of variables

the following

statements

that follow

called

graph where each node  rep

of interest
the variables.

The absence

and edges represent

of

denotes

statements
we say that variables
X if P(z I :c, y) =
x, y and z of variables

of indepen
Z andY are
X, Y and

statement

of independence

a variable

is independent
given the state of its patents

of its

from these local statements

structure,

in polynomial
time,
d-separation
[14].

in the network

I, Z is
of U, V andY given X and W. Further
inde

shown in Figure

of independence,

statements

This distribution

to representing
a particular

a
In addition
(that satisfies
distribution
BN also represents
all
is specified
the independencies).
by a set
Each node X has
of conditional probability
an associated
tion of X given different
ents. Using the independencies
encoded
the network,
the joint distribution
ply multiplying
the CPTs.

for its par
of
can be computed
by sim

CPT that describes

the conditional

assignments

of values

(CPTs).

tables

in the structure

distribu

1 Introduction

distributions
relations

(BN) representations

Network
lies in the efficient

of
encoding
of in
among random variables.
These in
in the rep
acquisition

ares;_  to provide
of a distribution,

savings

ease of knowledge
and computational
savings

The power of Bayesian
probability
dependence
dependencies
resentation
and domain modeling,
ference
this power by refining
ditional
independence
gate how independence

process.1

the BN representation
to capture
relations.
In particular,
given certain

we investi
assignments

variable

The objective

of this paper is to increase

in the in

ad

using a tabular
of values

of a conditional
assuming

to the
dis
that all of

in Which each assignment
the specification

In its most naive form, a CPT is encoded
representation
parents
of X requires
over X. Thus, for example,
tribution
U, V, W and X in Figure
ify eight such distributions
of this representation
ents. Furthermore,
tain regularities
Figure 1, for example,
p1 regardless
constant
when u holds (i.e.,

is exponential

this representation

fails to capture cer
In the CPT of
in the node distribution.

1 are binary,
(or eight parameters).
in the number of par

we need to spec
The size

P(x  I u, V, W) is equal to some
of the values

taken by V and W:

when U =  t) we need not consider

refers to the computation

of a posterior

distribution,

1 Inference
conditioned

on evidence.

116 Boutilier,

Friedman,

Goldszmidt,

and Koller

u
/"-.. pi v
/"-..
pl w
/"-.. p3  p4

2 Context-Specific

Independence and Arc

Deletion

of discrete

where each variable

a finite set U =  {X 1 , ... , X n}

a finite domain. We use capital
names and lowercase

Consider
dom variables
values from
X, Y, Z, for variable
denote specific
all values of X is denoted
noted by boldface
of values to the variables
boldface lowercase
vious way).

ran
X; E U may take on
letters,
such as
x, y, z to
letters
values taken by those variables.
The set of
val( X). Sets of variables
are de
letters
X, Y, Z, and as signmen ts
in these sets will be denoted by
a:, y, z (we use val( X) in the ob

capital

letters

Definition 2.1: Let P be a joint probability
in U, and let X, Y, Z be subsets
of U.
over the variables
X and Y are conditionally
given Z, denoted
I(X; Y I Z), iffor all a:  E vai(X),
y  E vai(Y ), z  E
val(Z),

independent

relationship
holds:

the following

distribution

P(x I z, y) =  P(x I z) whenever P(y, z) > 0. (1)

called d-separation

in that they involve

[ 14]

(for all values of a:, y, z)

the assertion

path criterion

this last statement

direct dependencies

network is a directed

acyclic
to the random variables

graph B whose
X 1 , ... , X n, and
between the
of B encodes the set of inde
that each

representing
of its non-descendants
given its par
are local,
in B. Other I() statements, in
arbitrary sets of variables, follow from these local
These can be read from the structure
of B us

We summarize
by P(X I z, Y) = P(X I Z).
A Bayesian
nodes correspond
whose edges represent
variables.
The graph structure
pendence
assumptions
node X; is independent
ents Ilx1 These statements
only a node and its parents
volving
assertions.
ing a graph-theoretic
that can be tested in polynomial time.
A BN B represents
ticular  distribution
dencies
to be an /-map for the distribution
sanctioned
quired to be a minimal
of any edge in the network destroys
network with respect
B for P permits
tion: we need only specify,
tional
probability
U,,) for each possible
(See [14] for details.)
The graphical
dence relations
dencies
ables in Z. However,
dencies

independence
P. Thus, we require

encoded in B hold for P. More precisely,

in B holds in P, A BN is re

a compact representation

the 1-mapness
of the
A BN
it describes.

indepen
ofthe form I(X; Y  I Z), that is, indepen

X;, a condi
P(x; I
in {X;, Ilx, }.

that hold for any assignment

we are often interested

of the BN can only capture

table (CPT) encoding

to the distribution

value of the variables

that hold only in certain

by d-separation

for each variable

of values to the vari

that the indepen

information

a parameter

structure

contexts.

in indepen

B is said

about a par

1-map, in the sense that the deletion

of the distribu

P if every independence

Definition 2.2: Let X, Y, Z, C be pairwise disjoint
sets
of variables.
given

X and Y are contextually

independent

Figure 1: Context-Specific

Independence

at
of eight. Such reg

the values of V and W. Clearly, we need to specify
over X instead
most five distributions
ularities
occur  often  enough
BN products-Microsoft's
Tool and Knowledge
rated special
terface
CPTs.
sponding

that at least  two  well
Networks Modeling
incorpo
acquisition
the corre

DXpress-have
in
that allow the user to more easily specify

Bayesian
Industries'

in their knowledge

mechanisms

known

in the

for such reg

indepen

by using the notion of context-specific
Intuitively,

in our example, the regularities

a formal foundation

In this paper, we provide
ularities
dence.
of W and V given
CPT of X ensure that X is independent
u (U = t), but is dependent
on W, V in the con
the context
text u (U =  f). This is an assertion
in
dependence (CSI), which is more restricted than the state
ments of variable
BN structure.
statements
independence
knowledge
tational

namely, ease of
compact representation
and compu

that are encoded by the
as we show in this paper, such

can be used to extend the advantages

independence

for probabilistic

Nevertheless,

elicitation,

in inference,

of context-specific

inference,

of variable

benefits

Well-known

indepen

include

additional

extensions
to the

in order to capture

not the first to suggest

enhance inference.

[9] similarity

the use of asymmetric

and (potentially)
Heckerman's

We are certainly
BN representation
dencies
examples
the related multinets
[7]),
for decision
tations
probabilistic
variables.
trees) have been used to encode CPTs [2, 8]. The intent
this work is to formalize
resentation
pose methods for utilizing
probabilistic

making [18, 6] and Poole's [16] use of
Horn rules to encode dependencies

of
CSI, to study its rep
and to pro
to enhance

Even the representation

these representations

networks
represen

as part of a more general

algorithms.

the notion of

we emphasize

framework,

inference

between
(decision

(and

local transforma

so that CSI statements

2 by defining context-specific
indepen

a simple,

using d-separation.

and how this representation

We begin in Section
and introducing
dence formally,
tion for a BN based on arc deletion
3 dis
can be readily determined
Section
cusses in detail how trees can be used to represent
CPTs
compactly,
the algorithms
gestions
advantage
may reduce clique size for clustering
as techniques
strategy-in cutset conditioning.
cussion

CSI. Section
up probabilistic

We conclude
and future research

that use CSI-and the associated

by
4 offers sug

for determining

of CSI. We present

for speeding

inference

of related

algorithms,
as well

notions

can be exploited

by taking

arc-deletion
with a dis

directions.

network transformations that

Context-Specific

Independence

in Bayesian Networks 117

E val( C), denoted Ic(X; Y I Z, c), if
Z and the contexte
P(X I Z, c, Y) = P(X I Z, c) whenever P(Y, Z, c) > 0.

Theorem 2.5: Let B be a network
local independencies.
consistent
mapof P.

w ith Band I;, such that (B, I;) is a perfect

Then there exists

a distribution

I; be a set of
structure,
P,
CSJ

3 Structured

Representations

of CPTs

proposi
and exhaustive  generalized
set ITx. A generalized proposition is

c to C.

of X and Y

assignment

that the independence

is similar to that in Equation ( 1 ), taking CUZ
but requires

This assertion
as evidence,
hold only for the particular
It is easy to see that certain local Ic statements
-those of
the form Ic(X; Y  I c) for Y, C  c;;; IIx-can be veri
fied by direct examination
of the CPT for X. In Figure 1,
for example, we can verify Ic (X; V I u) by checking in the
CPT for X whether, for each value
w ofW, P(X I v, w, u)
it is the same for all values v of
does not depend on v (i.e.,
V). The next section explores different representations
of
statements
the CPTs that will allow us to check these local
efficiently.
to the principle
method for deciding
It turns out that this problem can be solved by a simple re
duction to a problem of validating variable
in a simpler network. The latter problem can be
statements
efficiently
solved using d-separation.

a computationally
tractable
of non-local Ic statements.

now is to establish

of d-separation:

independence

Our objective

the validity

an analogue

B

by inspecting

on the resulting

and can be determined

from Y given Z U C in B (c).

Definition 2.3: An edge from Y into X will be called vac
uous in B ,  given a context c, if Ic(X; Y I c n IIx ). Given
BN B and a context c, we define B (  c) as theBN that results
vacuous edges in B given c. We say that X
from deleting
from Y given Z in context c in B if X is
is CSI-separated
d-separated
Note that the statement Ic (X; y I  c n IIx) is a local Ic
statement
the CPT for
by transforming
X. Thus, we can decide CSI-separation
into B (  c) using these local Ic statements
to delete vacuous
network.
edges, and then usingd-separation
We now show that this notion of CSI-separation
is sound
and (in a strong sense) complete given these local indepen
Let B be a network structure
and I: be
dence statements.
a set of local Ic statements
over B. We say that ( B, z;)
im
is a CS/-map
inP,i.e.,Ic(X;Y I Z,c)holdsin
pliedby(B,I)hold
P whenever X is CS/-separated
from Y given Z in con
text c in ( B, I;). We say that (B, I;) is a peifect
if the implied independencies
are the only ones that hold in
P, i.e., if Ic(X; Y  I Z, c) ifandonlyif X is CSJ-separated
from Y given Z in context c in (B,In
Theorem 2.4: Let B be a network
local independencies,
Band I;. Then ( B,I;) is a CSI-mapof

I be a set of
consistent
and P a distribution

P if the independencies

of a distribution

structure,

CSI-map

with

P.

the soundness

of this procedure.

The theorem establishes
the procedure also complete? As for any such procedure,
there may be independencies
only local independencies
the following
provides the best results that we can hope to derive based
solely on the structural

theorem shows that, in a sense, this procedure

that we cannot detect using

of the distribution.

properties

Is

and network structure. However,

represen

and can be

combination

independence

corresponds

to regularities

we can represent

we discuss possible

of specific variable

the partitions

using a set

the fact that distinct

into distributions

in much the
condi

on tree-structured

that capture this regularity

CPTs by simply partitioning

are
one can
the space

over X. While this representation

for determining
in probabilistic

with the same distribution.  Therefore,

into regions mapping to the same distribution.

For reasons of space,
representations.

qualitatively,
captures
admit effective

over X. A compact represen
of this function
elements of val(IIx)

qualitatively
Such representations
local CSI statements
inference.

Context-specific
within CPTs. In this section,
tations
same way that a BN structure
tional independence.
algorithms
exploited
we focus primarily
In general, we can view a CPT as a function that maps
val(ITx)
tation of CPTs is simply a representation
that exploits
associated
compactly represent
val(IIx)
Most generally,
of mutually exclusive
tions over the variable
simply a truth functional
so that if Y, Z E  IIx, we may have a par
assignments,
proposition
tition characterized
by the generalized
(Y =
y) V -.(Z =  z). Each such proposition
is associated
with a
distribution
is fully gen
eral, it does not easily support either probabilistic
or inference
more convenient,
ing. For example, one could use a canonical
such as minimal CNF. Classification
the machine learning
other popular function
state space induced by the labeling
These representations
ing the fact that vacuous edges can be detected,
CPTs produced in linear time (in the size of the CPT repre
sentation).
pact CNF or tree representation
larger (exponentially
imal representation
For the purposes of this paper, we focus on CPT-trees
tree-structured
sults for CNF representations
(of  the form discussed
paper. A major advantage of tree  structures  is
uralness,
to "rule" structure
particularly  easy
man expert. As we show in subsequent
structure
gorithms.
as we discuss
also amenable to well-studied

there is a tradeoff: the most com
of a CPT might be much
larger in the worst case) than the min
in terms of generalized propositions.

with partitions
of the
of branches in the tree.
have a number of advantages,

the tree
to speed up BN inference al
trees are

about CSI. Fortunately,
representations

discussion
and graph-structured

of analogous
re
CPTs

community as decision
representation,

can also be utilized
Finally,

logical form
trees (also known in

with branch labels corresponding

(see Figure I). This intuition

by [3]) to a longer version of this

in some sense
makes it

from a hu
sections,

includ
and reduced

for this type of partition

to elicit probabilities

we can often use other,

in the conclusion,

approximation

and learning

CPTs, deferring

As expected,

inference

directly

trees) are an

their nat

118 Boutilier,

Friedman,

Goldszmidt,

and Koller

8808  A
\\II ()

B

A

B
/'...
p3  c

------

D
1\  /'-....  /'...
pl  c
pl p2 pJ  c
/'--..... /'...
/'...
()  p2' B
p4  ()
p4
/'...
1\  /'...
p5  p6  p2"  p2'" pl p6
Trtefar
X(2)

Network  Tree for X (1)

0

Figure 2: CPT-tree Representation

this using the criterion

from detecting
ever, the test above is complete in the sense that no other
edge is vacuous given the tree structure.

in the theorem. How

Theorem 3.3: LetT be a CPT-tree
let c E C be some context
that is consistent
to the leaves
parameters
c.
uous given

with c, then there exists

of
ofT such that Y -+ X is not vac

an assignment

for X, letY E Ilx and
on a path

(Y (/:. C). IfY occurs

This is similar in spirit to d-separation:

above is, in fact, the best

of the tree and not the ac

This shows that the test described
test that uses only the structure
tual probabilities.
it detects all conditional
structure
cies that are hidden in the quantification
conditional
soundness in order to exploit CSI in inference.

in belief networks,

independence

independencie

s possible

of the links. As for
we need only

of the network, but it cannot detect independen

from the

containing

to variables

the CPT conditioned

to produce a reduced CPT-tree rep
It is also straightforward
on context c. Assume c an
resenting
assignment
certain parents of X and
T is the CPT-tree of X, with root R and immediate sub
trees T1,   Tk. The reduced
T(c) is defined re
cursively
ables C, then T(c) consists
labelofRissomeY E C,thenT(c) = 7j(c),whereTj
the subtree pointed to by the  t-are
Thus, the
traversal

if the label of R is not among the vari
if the
is
labeled with value y  E c.

reduced tree T(c) can be produced with one tree
in O(ITI) time.

of R with subtrees Tj (c);

as follows:

CPT-tree

Y labels
Proposition 3.4:
and only if Y    C and Y occurs
consistent

Variable

with c.

in T(c) if
some t-node
on a path in T that is

only if Y -+ X

This implies that Y appears in T( c)  if  and
is not deemed vacuous by the test described
the reduced tree, determining
that can be deleted requires
Thus, reducing the tree gives us an efficient and sound test
for determining
of all
parents of X.

the list of arcs pointing
into X
ofT( c).
a simple tree traversal

the context-specific

independence

above. Given

4 Exploiting

CSI in Probabilistic

Inference

offer considerable

structure

inference.
The

advantages

that are exploited

in probabilistic

of distributions

independence
by well-known algorithms
is relevant

of a BN lays bare variable

how best that information

Network representations
computational
graphical
relationships
when deciding what information
query, and
a similar fashion,
trees make CSI relationships explicit.
scribe how CSI might be exploited
algorithms,
stressing
ing and cutset conditioning.
sentation;
also emphasize that  these
in which BN inference

particular
Space precludes

we provide only the basic intuitions

compact representations

to (say) a given
can be summarized. In
of CPTs such as
In this section,
we de

uses in cluster
pre
a detailed
here. We

are by no means the only ways

in various BN inference

specifically

can employ CSI.

methods [17]. In this section,
algorithms

for detecting
CSI.

we show that they admit fast

on c. This operation

CPT when we condition

X is vacuous; the second is to determine

we wish to perform
In general, there are two  operations
given a context c: the first is to determine whether
a given
arc into a variable
a reduced
carried out whenever we set evidence and should reflect the
changes to X's parents that are implied by context-specific
given c. We examine how to perform both
independencies
types of operations
To avoid confusion,
use t-node and t-are to denote nodes and arcs in the tree (as
opposed to nodes and arcs in the BN). To illustrate
these
X in Figure 2.
ideas, consider
(Left t-ares are labeled true and right t-ares
false).

the CPT-tree for the variable

on CPT-trees.

is

we

easy to tell which

it is relatively

of X given context c. As

Given this representation,
parents are rendered independent
sume that Tree l represents
clearly D remains relevant
dependent of X. Given a 1\ b, both C and D are rendered
independent
this is so because the distri
bution on X does not depend on C and D once we know
c  = a 1\ b: every path from the root to leaf which is consis
tent with c fails to mention Cor D.

the CPT for X. In context a,
while C and B are rendered in

of X. Intuitively,

Definition 3.1:  A path in the CPT-tree is the set of t-ares
lying between the root and a leaf. The labeling
of a path is
induced by the labels on the t
the assignment
to variables
Y occurs
ares of the path. A variable
t-nodes along
tent with a context c iff the labeling
with the assignment

the path tests the value ofY.

A path is consis
is consistent

on a path if one of the

of the path

of values in c.

Theorem 3.2: LetT be a CPT-tree for X and let Y be one
of its parents.
(Y (/:. C). If
Y does not lie on any path consistent
Y -+ X is vacuous

Let c E C be some context

with c, then the edge

given c.

This provides us with a sound test for context-specific
dependence (only valid independencies
However, the test is not complete,
tures that cannot be represented
minimally
stance, suppose that pl = p5 and p2 = p6 in the example
above. Given context bAc, we can tell that A is irrelevant
inspection;

but, the choice of variable

since there are CPT struc

by a tree. For in

are discovered).

ordering prevents

by
us

in

Context-Specific  Independence

in Bayesian Networks 119


x... 0  x.

'

t

A X"-' X4-
t  t
t
1
J
t
0
t  J  t
t  J  J  0
1
t
J  t
0
J
J  t
J  f  t  1
f  f J  0

 

x . 1

(a)

(b)

(c)

Figure 3: (a) A simple decomposition
of X ,  utilizing

CSI.

of the node X; (b) The CPT for the new node X; (c) A more effective

decomposition

4.1 Network Transformations

and Clustering

and then, when the value of A is revealed,
value for X is chosen.

the appropriate

"casual

noisy-or

by assuming
that

satisfying

(or generaliza

for CPTs is not a novel

contributions"

of distributions

of X make independent

compact representations

Formally,
tional

The use of
idea. For instance,
distributions
tions [19]) allow compact representation
the parents
to the value of X. These distributions
fall into the gen
causal indepen
eral category
dence [10, 11). For such distributions,
we can perform a
network,
structural
in a new network
encoded qualitatively
tially, the transformation introduces auxiliary
the network,
of deterministic
from causal independence,
structural
certain
aspects

resulting
where many of these independencies

ideas can be applied:
can be used to capture

[11). While CSI is quite distinct

are
Essen
into

of CSI directly within

network transformation

the EN-structure.

within the network structure.

them via a cascading

transformation

on our original

then connects

variables

or-nodes

sequence

similar

a

can be very useful when one uses
based on clustering  [13].  Roughly
cycles)
into cliques.

of variables

construct

over the set val(X) of the nodes X in the cluster.

process

algorithm

is determined

or clique, encodes

is carried out on the join tree, and its

a join tree, whose
in the orig
the marginal
dis

Such transformations
an inference
speaking, clustering
algorithms
nodes denote (overlapping)
clusters
inal BN. Each cluster,
tribution
The inference
complexity
clique.
that each fam
worthwhile.
ily in the BN- a node and its parents- be a subset of at
least one clique in the join tree. Therefore,
a large set
of values
clique and thereby
rithms.
values present
tional
savings

val( {X;} U ITx,) will lead to a large
to poor performance
the overall

This is where the structural

A transformation

The clustering

that reduces

largely

process

number of

by the size of the largest

transformations
requires

prove

in a family can offer considerable computa
in clustering

algorithms.

we define a random variable

X A=t, with a condi

distribution

that depends

only on B 1, ... , Bk:

a variable

XA=f. The variable X

We can  similarly define
is equal to X A=t if A = t and is equal to XA=f if A = f.
X A=t andX A=J both have the same
Note that the variables
the
set of values as X .  This perspective
allows us to replace
node X in any network
in
Figure  3(a).  The node
we
call a multiplexer
the value of
XA=t or of XA-:= f, depending
on  the value of A). Its CPT
is presented

X is a deterministic

node (since X takes either

with the subnetwork

in Figure
3(b).

illustrated

node,  which

a significant

is not particularly

(with its many tightly

decompo

CPT for X;

However,

structure

For one thing,

if X exhibits

the resulting

the total size of the two new CPTs

does not admit a more effective

node X, this decomposition

the same as the size of the original

For a generic
useful.
is exactly
for another,
coupled
sitions
amount of CSI, this type of transformation
can result
more compact representation.
For  example,
assume that X depends
and only on B3 and B4 when A is false.  Then
and XA=f will have only two parents,
these variables
two CPTs with four entries
tic multiplexer
By contrast,
CPT with 32 entries.
sulting
tree with much smaller

representation

of the re
the structure
network may well allow the construction
of a join

node with 8 (predetermined)

in a far
let k = 4, and

only on B1 and B2 when A is true,

Furthermore,

as in Figure 3(c). If

the original

are binary,

cliques.

each of XA=t

of X had a single

the new representation
each, plus a single

requires
determinis

a family with

'distributions'.

of clustering algo

we first consider

and let B1, ... ,

our transformation,

Let A be one
node X in a Bayesian  network.

In order to understand
a generic
of X 's parents,
ents. Assume, for simplicity,
valued.  Intuitively,
variable
value that X would take if A were true, and the value that
X would take if A were false.
a thought
periment

Our transformation
ply this decomposition
X is first decomposed  according
at the root of its CPT tree. Each of the conditional
nodes
(X A=t and XA=f in the binary case) has, as its CPT, one of
the subtrees
ing conditional
X as the outcome of two conditional variables:
similar
sponding
XA=f,B=J. The node XA=J,B=f can then be decomposed

that X and A are both binary
we can view the value of the random

nodes can be decomposed
In Figure 4, for example,

lly, each node
to the parent A which is

A in the CPT for X .  The result

Bk be the remaining  par

recursively,
the node corre

to XA=J can be decomposed

where these two variables

uses the structure

recursively.

into XA=J,B=t and

separately,

We can conduct

of a CPT-tree
to ap

of the t-node

are decided

fashion.

Essentia

the

in a

ex

120 Boutilier,

Friedman, Goldszmidt, and Koller



Figure 4: A  decomposition
cording to Tree (1).

of the network in Figure 2, ac

if it is observed to be true. 2 In both cases, these CSI rela
tions are captured by the deterministic relationships
used in
the transformation:
pendent if the node is set to false. In a multiplexer
node,
the value depends only on one parent once the value of the
"selectin.;"

the parents are inde

parent (the original

in an "or" node,

variable)

is known.

4.2 Cutset Conditioning

to take advantage of independencies

the join-tree

or tree representations,

The use of static precompilation
makes it diffi

al
Even using noisy-or
gorithm can only take advantage of fixed structural
inde
pendencies.
cult for the algorithm
that only occur in certain circumstances,
idence arrives.
More dynamic algorithms,
conditioning
cies more effectively.
gorithms can be modified to exploit CSI using our decision
tree representation.

[14], can exploit context-specific

such as cutset
independen

We investigate

below how cutset al

e.g., as new ev

3

since they have no parents.

into XA=J,B=J,C=t and XA=f,B=J,C=f.
The nodes XA=J,B=t and XA=J,B=J,C=t cannot be de
composed further,
decomposition
sible, this is not beneficial,
are unstructured
that this procedure
in the CPT of a node. Thus, in general,
decomposition
that this includes

W hile further
of nodes XA=t and XA=f,B=f,C=f
since the CPTs for these nodes
(a complete tree of depth 1 ). It is clear

we want to stop the
when the CPT of a node is a full tree. (Note

leaves a special case.)

is beneficial

is pos

only if there is a structure

that, once in

render the network singly connected.

algorithm works roughly as fol
i.e., a set of variables

The cutset conditioning
lows. We select a cutset,
stantiated,
is then carried out using reasoning
case is a possible assignment
C. Each such assignment
call to the polytree
ence on the resulting
are combined to give the final answer. The running time is
largely determined
by the number of calls to the poly tree al
C) I).
gorithm (i.e., Ivai(

algorithm [14], which performs infer
network. The results of these calls

by cases, where each

is instantiated

to the variables

as evidence in a

in the cutset

Inference

algorithms

transformation

but each generally

can allow clustering

For example, Figure 4 describes

for noisy-or nodes of
As in the structural
[11], our decomposition
to
form smaller cliques. After the transformation,
we have
many more nodes in the network (on the order of the size
of all CPT tree representations),
has far
fewer parents.
the transfor
mation ofthe CPT of Tree (1) of Figure 2. In this transfor
mation we have eliminated
introduced
ing on implementing
ness in practice.
auxiliary
are deterministic
be further exploited

of the
nodes, which
function of their parents. Such nodes can

a family with four parents and
We are currently
work

We also note that a large fraction

several smaller families.

these ideas, and testing

nodes we introduce

in the clustering

are multiplexer

algorithm

[12].

their effective

in clique size (and the result
depend heavily on the structure

We note that the reduction
ing computational
savings)
of the decision trees. A similar phenomenon occurs in the
transformation
depends on
the order in which we choose to cascade the different par
ents of the node.

of [11], where the effectiveness

the graphical

BN cannot capture all independencies
none of the CSI relations

structure
im

of our

value assignments-can

be read from

As in the case of noisy-or,
(transformed)
plicit in the CPTs. In particular,
induced by particular
the transformed
is our inability
are independent

to structurally

structure.

In the noisy-or

represent

case, the analogue
that a node's parents

algo

variable

a particular

For instance,

loops without the need for instantiating

CSI offers a rather obvious advantage to inference
of loop cutsets. By in
rithms based on the conditioning
stantiating
to a certain value in order to
vacuous, perhaps cut
cut a loop, CSI may render other arcs
ting additional
addi
suppose the network in Fig
tional variables.
ure 1 is to be solved using the cutset { U, V, W} (this might
be the optimal strategy
we solve the reduced singly-connected
lval(V)
ues to U, V, W. However, by recognizing
connections
u, we need not instantiate
This replaces Jval(V)
a single evaluation.
ation of V, W can no longer be ignored (the edges are not
vacuous in

V and W when we assign U =  t.
W) I network evaluations
with

I  I val(
However, when U =  f, the instanti

between X and {V, W} are vacuous in context

if I val( X) I is very large). Typically,

I times, oncefor each assignment

of val
the fact that the

llval(W)

context u).

network lval(U)I



we generalize

To capture this phenomenon,
tion of a cutset by considering
sets. These reflect the need to instantiate
some contexts,
work singly-connected.
a tree with interior

tree representations
variables
but not in others, in order to render the net

the standard no
of cut
in

nodes labeled by variables

is
and edges Ia-

Intuitively,

a conditional
cutset

certain

zThis last fact is heavily
3We believe similar ideas can be applied to other compact CPT

networks (mostly BN20 networks).

utilized by algorithms

targeted

ically at noisy-or

specif

if the node is observed to be false, but not

representations

such as noisy-or.

u

/'Z..
0

u

u
ll,f vf
v

w

v

v
Y'Z..  jtJ A
0
w
w
Y'Z..  jtJ  A
0
0
0
,.,  (<i
ll
Figure 5: Valid Conditional

0

0

0

Cutsets

values.4

value on each edge. The tree is a conditional

to the set of assignments

Each branch through the
induced by fixing

beled by (sets of) variable
tree corresponds
one variable
cutset if: (a) each branch through the tree represents
a con
text that renders the network singly-connected;
and (b) the
set of such assignments
and exhaus
cutsets for the BN in Figure 1
tive. Examples of conditional
are illustrated
in Figure 5: (a) is theobviouscompact
(b) is the tree representation
fails to exploit the structure
uation for each instantiation

is mutually exclusive

cutset;
which
one eval

cutset,

ofthe "standard"
of the CPT, requiring
of U, V, W.
cutset in hand, the extension
is fairly obvious. We con
by

cutset inference

of values to variables

Once we have a conditional
of classical
sider each assignment
branches through the tree, instantiate
assignment,
run the poly tree algorithm
work, and combine the results
plexity of this algorithm
tinct paths through
cial to find good heuristic  algorithms
conditional
sive" heuristic
of vacuous arcs maximally. This algorithm
ditional
dard heuristic
cuss computationally-motivated
shortcuts
this section.

is a function
the conditional

cutsets incrementally,

approach  that

approaches

cutsets.

exploits

constructs
in a fashion similar to stan
to the problem [20, 1]. We dis
near the end of

con

as usual.5 Clearly,

the com
of the number of dis
cutset. It is therefore
cru
for constructing
small

determined
the network with this
on the resulting net

CSI and the existence

The  standard
selects nodes for the cutset according to the heuristic

"greedy" approach to cutset construction

of X in the net

value fi?,

where the weight w(X) of variable X  is
and d(X) is the out-degree

log(lval(X)I)
work graph [20, 1]. 6 The weight measures the work needed
X in a cutset, while the degree of a vertex
to instantiate
gives an idea of its arc-cutting
outgoing edges mean a larger chance
to extend this heuristic
the extent to which arcs are cut due to CSI. The obvious
approach,
namely adding to d(X)
ally rendered vacuous by X (averaging
reasonably

the number of arcs actu
over values of X), is

straightforward,

to deal with CSI, we must estimate

but unfortunately

to cut loops. In order

is somewhat

4We explain the need for set-valued
5 As in the standard cutset  algorithm, the weights required

arc labels below.
to
combine the answers from the different cases can be obtained
from
the polytree computations [21].
6We assume that the network has been preprocessed

splitting
for details.

so that legitimate

cutsets can be selected

by node
easily. See [ 1]

potential-more incident

We focus on a "computationally inten

The expected
tiated is given by

number of arc deletions

from B if X is instan

Context-Specific

Independence in Bayesian Networks 121

we should consider the expected

choice be
cutsets pro

it ignores the potential

For example, consider

arcs into
value (other things being

using B, C and D will be very small.

for arcs to be
the family in Fig
the CPT for X. Adding A or
X to be cut, so

myopic. In particular,
cut subsequently.
ure 2, with Tree 2 reflecting
B to a cutset causes no additional
they will have the same heuristic
equal). However, clearly A is the more desirable
cause, given either value of A, the conditional
duced subsequently
Rather than using the actual num ber of arcs cut by select
ing a node for the cutset,
number of arcs that will be cut. We do this by consider
ing, for each of the children V of X, how many distinct
probability
are found in the structured
representation
of the CPT for that child for each instantia
tion X  = x; (i.e.,
CPT). The log
of this value is the expected number of parents required
for
the child V  after  X
indicating
erage this number for each of the values X may take, and
sum the expected number of cut arcs for each
of X's chil
dren. This measure then plays the role of d(X) in the cutset
let t(V) be the size of the CPT
heuristic.
structure
and let t(V, xi) be the size of the reduced CPT given con
text X = xi (we assume X is a parent of V). We define the
expected

More precisely,
(i.e.,  number  of  entries)

for V in a fixed network;

=  Xi is known, with fewer parents

entries (distributions)

the size of the reduced

number of parents

for arc-cutting.

more potential

We can then av

EP(V, x;) = L..,, E arents, ,-,.,_

"'4 p

of V given xi to be
r1t\ v logl, I( )I t(V, X = x;)
w  .... , .
I -1
IParents(V)

LveChildren(

X) Lx,Eval(X)

V, x;)
IParents(V)I-EP(

1
d (X)=

lval(X)I

algorithm
selected

Thus, ;, gives an reasonably

accurate picture of the
cutset in a network B.
proceeds recursively
by:
node X to a branch of the
for
a new network for
of X that reflects CSI; and 4)

value of adding X to a conditional
Our cutset construction
1) adding a heuristically
tree-structured
each value x; E val(X);
each of these instantiations
extending
the node that looks best in the new network corresponding
to that branch. We can very roughly sketch it as follows.
The algorithm

cutset; 2) adding t-ares to the cutset-tree

each of these new arcs recursively

begins with the original

3) constructing

by selecting

network B.

1. Remove singly-connected
If no nodes remain, return  the

nodes  from B, leaving Br.

empty cutset-tree.

2. Choose node X in Br s.t. w(X)Jd'(X)
3. For each x; E val(X),

is minimal.
construct Bx, by removing

vacuous arcs from Br and replacing
reduced CPTs using X = x;.

all CPTs by the

4. Return the tree T' where: a) X labels the root ofT';
b) one t-are for each x; emanates from the root; and c)

the context already

value of X is determined

is standard [20, 1 ]. InStep 2, it is im

Step 1 of the algorithm
portant to realize that the heuristic
with respect to the current network and
established in the existing
required to ensure that the selection
flects the fact that
nally, Step 4 emphasizes  the
selection: different variables may be selected next  given
different values of X. Steps 2-4 capture the key features of
our approach and have certain computational
to which we now turn our

branch of the cutset. Step 3 is
of the next variable re
Fi

X = Xi is part of the current context.
nature of variable

conditional

implications,

attention.

ly exponentially

can be run on

we  can add an optional

CSI to a great degree, but requires

exploits
effort greater than that for normal cutset con
:  a tree rep

stored: as variables are
to particular values for conditioning, the selec
can be made. Conceptually,
this
of the tree, with only
construction

Our algorithm
computational
struction. First, the cutset itself is structured
resentation of a standard cutset is potential
larger (a full tree). However, the algorithm
line, and the tree never completely
instantiated
variable
tion of the  next
amounts to a depth-first
one (partial or complete) branch ever being stored. In ad
step before Step 4 that de
dition,
tects structural
instantiations
effect on the arcs in B  and the representation of reduced
CPTs, then we need not
these instantiations
sub
(in cutset construction). Rather, in Step 4, we
sequently
would create
one new t-are in the cutset-tree labeled with
the set {xi, Xj} (as in Figure 5). This reduces the number of
graphs that need to be constructed
(and concomitant com
putations discussed below).  In
tings, the representation
size similar to a normal cutset,

set
of a conditional cutset would be of

equivalence in
of X to x; and xi have the same structural

the networks Bx, .  If, say, the

as in  Figure 5(b).

unstructured

distinguish

completely

in

One involves

in order to enhance this algorithm.
less ideal but more tractable methods of con

There are several other directions that we are currently
vestigating
developing
ditional cutset constructi
on. For example, we might select
a cutset by standard means, and use the considerations
scribed  above
within this cutset. Another direction
these  ideas  with
standard cutset algorithms.

the computation-saving

to order (on-line)

involves integrating

ideas of [4] for

de

the variable instantiations

5 Concluding Remarks

adding to the regularities

assignments,

representable

its representation

in BNs. Our results provide

the notion of context-specific

indepen
We  have  defined
dence as a way of capturing the independencies
induced
by specific  variable
in distributions
foundations
for CSI,
ence.  In
mined using local computation in a BN and  how specific
mechanisms (in particular, trees) allow compact representa
tion of CPTs and  enable
more,  CSI
probabilistic
gorithms.

particular,  we have shown how  CSI  can

inference in both clustering and cutset-style

efficient detection of CSI. Further

and tree-structured

CPTs can be used to speed up

and its role in infer

be deter

al

there has

the BN representa

work on extending

been con
As we mentioned in the introduction,
siderable
tion to cap
ture additional independencies. Our notion of CSI is re
lated to  what Heckerman calls subset independence in his
work on similarity networks [9]. Yet, our approach is sig
nificantly
independencies
the CPTs within a single
and multinets [9, 7] rely on a family of networks. In fact
the approach we described
in spirit to that of Poole's rule-based
works [ 1 6].

representation
network, while similarity networks

trees  is
ons of net

by providing a structured

based on decision

representati

closer

of

different in that we try to capture the additional

122 Boutilier,

Friedman,

Goldszmidt, and Koller

the t-node attached to the end ofthe x; t-are is the tree
produced by recursively
with the
network B:x,.

calling the algorithm

value d'(X)

instantiations

that gave rise to the current

Apart from the amount of information in a conditiona
l cut
to add
effort is needed to decide which variables
set, more
component d' (X) is more in
to a branch, since the heuristic
volved than vertex degree. Unfortunately, the
is not fixed (in which case it would involve a single set of
prior computations); it must be recomputed in Step 2 tore
flect the variable
network. Part of the re-evaluation
CPTs also be updated (Step 3). Fortunately,
CPTs that have
small: only the children
to have CPTs updated. This can  be  done
reduction algorithms
described above,
of
cient. These updates then affect the heuristic
"spouses" V of X need to
only their parents; i.e., only  the
have their value d'(V) recomputed. Thus, the amount of
work required is not too large, so that the reduction in  the
number of network evaluations  will
usually compensate
for
the extra  work.  We
in the process of imple
menting this algorithm to test

of d'(X) requires that
the number of
to be updated for assignment X =  x; is
of X (in the current graph) need

its performance in practice.

are currently

which are very effi

estimates

using the CPT

calculus

actions proposed

of the network trans
probabilistic

The arc-cutting technique and network transformation in
troduced in Section 2 is reminiscent
formations introduced by Pearl in  his
of action [ 1 5] .  Indeed the  semantics  of
in that paper can be viewed as an instance of CSI. This is
not a mere coincidence,
representing plans and influence diagrams  usually
a significant
cisions)
some variables, and are vacuous or trivial
stantiations
work on adding additional
by Smith et al. [ 18], Fung and Shachter [6], and the work by
Boutilier
in  the context of Markov Decision Processes.

amount of CSI. The effects of actions (or de
of
when these in
are not realized. Testimony to this fact is the

as it is easy to see that networks
contain

et al [2] on using decision

to influence diagrams

trees to represent
CPTs

structure

usually only take place for specific instantiation

the ideas presented

There are a number of future research directions
needed to elaborate
the role that CSI and compact CPT representati
probabilistic reasoning. We are currently exploring
of different CPT representations, such as decision
and the potential

ons play in
the use
graphs,
interaction between CSI and causal

here, and to expand

that are

inde-

Context-Specific

Independence

in Bayesian Networks 123

4.1

prob

(i.e.,

the local

e.g.,  it

compactly

are necessary

transformati

on algorithm

l cutset construct

of Section 4.2 (and its variants).

(as in the noisy-or

Figure 2 and suppose

no CSI) cannot be

are being conducted

a CPT exhibiting

these directions.

model). A deeper examina

ar, to determine the extent of the overhead

inference. In many cases, we may wish to  trade

involved
ion. We are currently
pursu

acquisi
little structure
represented;

in many cases,
is weaker in some circumstances

of Section
to determine the
size are
in clique
for the con
In

l experiments
es under which the reductions
Similar studies
algorithm

pendence
tion of the network
and empirica
circumstanc
significant.
ditional cutset
particul
in conditiona
ing both of
CSI can also play a significant role in approximate
abilistic
to speed up inference,  allow
a certain amount of accuracy
more compact representation
or ease knowledge
tion. For instance,
little or
may require a full tree. However,
dependence
than in oth
ers. Consider Tree 2  in
that none of
p2', p2", p2'" are very different, reflecting the fact the influ
ence of
weak in the case where
A is true and D is false. In this case, we may assume that
these three entries
are actually
ing the true CPT using a decision
Tree 1.
This saving (both in representation
techniques
racy. In ongoing
entropy)
allowing
error and the computational
fication
particularly suitable
that decision
-tree construction
learning
CPT-tree
algorithms
quired directly from the user, to simplify the CPT-tree in or
der to allow for faster inference.

and inference, using the
of this paper) comes at the expense of accu

error of a local approximation of
greedy algorithms
for practical

the CPTs, thereby
that trade off the
from the simpli

can be used to
community
from a full conditional

B and C's on X is relatively

gain derived
Tree representa

[ 1 7] can then be used on this tree, or on one ac

work, we show how to estimate the (cross

in this regard.  In particular,

thus approximat
of

tree with the structure

an appropriate

probability table;

of the network.

algorithms

from the machine

tions turn out to be

construct

the  same,

pruning

we show

representation

Structured
cial in learning Bayesian
compactness
capable
networks
complexity of the interactions

of inducing

of CPTs have also proven benefi
from data [ 5]. Due to the
networks
are

on, learning procedures
that better emulate the true
present

in the data.

of the representati

ents a starting

point for a rigorous

representations

cific independence.

of Bayesian network

This paper  repres
tension
context-spe
has
a deep and far-ranging impact on the theory and practice
of many aspects
rep
resentat
ion, inference algorithms,
ing. We consider the exploration
ideas to be a promising avenue for future

inference, including
approximation and learn
and development
of these

As we have seen,  CSI

of probabilistic

research.

ex

Acknowledgements: We would like to thank Dan Geiger,
Adam Grove, Daishi Harada, and Zohar Yakhini for useful
Some of this work was performed while Nir
discussions.
Friedman
were at Rockwell
Palo
Center, and Daphne Koller was at U.C. Berke-
Alto Science

and  Moises  Goldszmidt

to incorporate

[ 1 7] J. R. Quinlan.

C45: Programs for Machince Learning. Mor

gan Kaufmann, 1 993.

by a University

was supported

ley. This  work
President's Postdoctoral
tract F30602-95-C-0251 (Goldszmidt),
fellowship
NSERC Research

and NSF Grant IRI-95-03

Fellowship

(Koller),

of California
ARPA con

Grant OGP0121843 (Boutilier).

109 (Friedman), and

an IBM Graduate

