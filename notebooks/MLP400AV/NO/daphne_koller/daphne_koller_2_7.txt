Abstract

koller@cs.stanford.edu

Stanford, CA 94305-9010

the probability that a random individual in classC is
also in classD. The effectiveness of the algorithm re-

Knowledge representation languages invariably reect
a trade-off between expressivity and tractability. Evi-
dence suggests that the compromise chosen by descrip-
tion logics is a particularly successful one. However,
description logic (as for all variants of rst-order logic)
is severely limited in its ability to express uncertainty.
In this paper, we present P-CLASSIC, a probabilistic
version of the description logic CLASSIC.
In addi-
tion to terminological knowledge, the language utilizes
Bayesian networks to express uncertainty about the ba-
sic properties of an individual, the number of llers for
its roles, and the properties of these llers. We provide
a semantics for P-CLASSIC and an effective inference
procedure for probabilistic subsumption: computing

lies on independence assumptions and on our ability to
execute lifted inference: reasoning about similar indi-
viduals as a group rather than as separate ground terms.
We show that the complexity of the inference algorithm
is the best that can be hoped for in a language that
combines description logic with Bayesian networks.
In particular, if we restrict to Bayesian networks that
support polynomial time inference, the complexity of
our inference procedure is also polynomial time.

1 Introduction

First-order logic has been the basis for most knowledge rep-
resentation formalisms. Its basic unitsindividuals, their
properties, and the relations between themnaturally cap-
ture the way in which people encode their knowledge. Un-
fortunately, it is severely limited in its ability to represent
our uncertainty about the world: a fact can only be known to
be true, known to be false, or neither. By contrast, most of
our knowledge about the real world is not absolutely true,
but only true with some degree of certainty. This limita-
tion renders rst-order logic inapplicable to a large range of
real-world problems.

The fundamental step in addressing this problem was
taken by Bacchus [1990] and Halpern [1990]. They dened
and analyzed ways in which probabilities can be added to

rst-order logic, and claried the semantics of the resulting
formalisms. Their work focused on probabilistic extensions
of full rst-order logic. As a consequence, these logics were
shown to be highly undecidable (much more so even than
rst-order logic). Furthermore, they do not support a natural
and compact specication of independence assumptions,
which are crucial to getting nontrivial conclusions from a
probabilistic knowledge base.

There has been a recent move towards integrating prob-
abilities into less expressive subsets of rst-order logic.
By and large, knowledge representation formalisms based
on subsets of rst-order logic fall into two categories:
rule-based languages, and object-centered formalisms (e.g.,
frame-based languages, description logics). So far, most
work on probabilistic extensions has focused on augment-
ing rule-based languages [Goldman and Charniak, 1990;
Breese, 1992; Poole, 1993; Ngo et al., 1995]. In this pa-
per, we take a rst step towards integrating probabilities
with object-centered languages, by developing a probabilis-
tic description logic. Our language provides the ability to
describe classes of individuals, and to reason about the re-
lationships between classes.

(binary relations). For a given individuala, the individ-
uals related toa by some roleR are called theR-llers
ofa. Description logic allows us to describe classes of
R-llers of an individual, and the properties of the llers.

Description logics are subsets of rst-order logic with
equality that have been designed to model rich class hier-
archies. Informally, in a description logic we begin with a
set of primitive concepts (i.e., unary predicates) and roles

individuals (complex concepts) based on their properties:
the primitive concepts to which they belong, the number of

Description logics support subsumption querieswhether
one complex concept is always a subset of another, and
membership querieswhether a particular individual is an
instance of a given concept.

Several systems have been built based on description log-
ics (e.g., CLASSIC [Brachman et al., 1991], LOOM [MacGre-
gor, 1988], and BACK [Petalson, 1991]), and they have been
used in several applications (e.g., [Wright et al., 1993]). In
addition, several information integration systems (e.g., the
Information Manifold [Levy et al., 1996] and SIMS [Arens
et al., 1996]) use description logics to represent the infor-

individuals inD. By contrast, standard subsumption can
only tell us whether this number is 1 (D is subsumed byC),
0 (D is disjoint fromC), or somewhere in between.
essentially identical (e.g., differentR-llers of the same

Of course, the fact that our representation uniquely deter-
mines this number does not necessarily imply that we can
effectively compute it in practice. We show that the par-
ticular description logic and independence assumptions that
we have made also enable us to develop an effective infer-
ence algorithm for P-CLASSIC. The algorithm follows the
same general lines as the inference algorithm for standard
CLASSIC, by representing concepts as a graph. However, in
P-CLASSIC we replace the logical inference for comparing
pieces of the graph by inference with Bayesian networks for
computing the probability of parts of the graph. Further-
more, in contrast to other algorithms for reasoning in other
rst-order probabilistic formalisms, our algorithm imple-
ments a form of lifted inferencereasoning at the level of
variables rather than at the level of ground terms. This
is possible because our independence assumptions enable
the algorithm to reuse computation for individuals that are

individual). We show that, in some sense, the complex-
ity of this algorithm is the best that can be hoped for in
a language that combines the expressive power of CLAS-
SIC and Bayesian networks. In particular, if we restrict to
polynomial time Bayesian networks (e.g., polytrees [Pearl,
1988]) the complexity of our inference algorithm remains
polynomial.

Several works [Shastri, 1989; Jaeger, 1994; Heinsohn,
1991] have considered probabilistic extensions of descrip-
tion logics. There, the focus was on completing partial sta-
tistical information using default probabilistic procedures
such as entropy maximization or cross-entropy minimiza-
tion, or on deriving the minimal conclusions available from
a small, incomplete set of probabilistic statements. By con-
trast, our approach follows the more recent tradition, es-
tablished by Bayesian networks, of having the probabilistic
knowledge base completely specify a probability distribu-
tion. The full specication approach has been shown to
be both conceptually and computationally simpler. As we
have discussed, similar computational benets are obtained
in our framework, which supports an inference algorithm
which is signicantly more efcient than those for the pre-
vious works on probabilistic terminological languages.

itive concepts (unary predicates)A and roles (binary rela-
tions)R. The language uses a set of constructors to build

We rst briey review the variant of the CLASSIC descrip-
tion logic that underlies P-CLASSIC, and then describe the
probabilisticcomponent of P-CLASSIC. Finally, we describe
how these components come together to form a P-CLASSIC
knowledge base.

2.1 The Description Logic
The basic vocabulary of a description logic consists of prim-

2 The P-CLASSIC Language

descriptions, dening new classes of individuals called com-
plex concepts.

mation sources by specifying the class of individuals con-
tained in the information source. For example, an individual
might be one article in a bibliographic database. To retrieve
the complete answer to a query, the system accesses each
information source whose description overlaps with the de-
scription of the query.

One of the main limitations of description logics is that
they can express very little about the overlap between two
concepts. Given two concepts, we can infer that one sub-
sumes the other, that they are disjoint, or that they may
have a non-empty overlap. However, the degree of the
overlap cannot be described or inferred. The need for such
knowledge is clearly demonstrated in the information inte-
gration domain. Accessing every information source which
potentially overlaps with our query may be prohibitively
expensive. If we want the system to nd a large fraction of
the answers as soon as possible, it is very important to infer
the degree of overlap between classes of individuals.

In general, of course, the domain consists of many dif-
ferent types of individuals.
It is rarely the case that the
same distribution will be appropriate for each of them. For
example, the distribution over the properties of an individ-
ual is usually quite different from the distribution over the
properties of its llers. Therefore, the probabilistic com-
ponent of a P-CLASSIC knowledge base includes a number
of different p-classes (probabilistic classes), each of which
is a Bayesian network over basic properties, the number

In this paper, we describe a language, P-CLASSIC, which
is a probabilistic extension of the description logic CLAS-
SIC. P-CLASSIC allows the specication of a probability
distribution over the properties of individuals. As the ba-
sic representational tool, we use Bayesian networks [Pearl,
1988]. Bayesian networks allow a compact and natural rep-
resentation of complex probability distributions by using
independence assumptions. In this case, we use a Bayesian
network whose random variables are the basic properties of
individuals (the primitive concepts), the numbers of their
llers, and the properties of their llers.

ofR-llers (for the different rolesR), and the p-classes
C;D, what is the probability thatC holds within the set of

The semantics for P-CLASSIC is a simple extension to the
standard semantics of CLASSIC. Following [Halpern, 1990],
we interpret a p-class as a probability distribution over the
elements in the domain.
Intuitively, this corresponds to
the probability of choosing (encountering) this element
in this p-class. By assuming that the p-class distribution is
as described in the corresponding Bayesian network, and
that llers are chosen independently of each other (from
the appropriate p-class), we can show that our p-classes
uniquely determine the probability of any complex concept.
A P-CLASSIC knowledge base allows us to answer any
probabilistic subsumption query: for two complex concepts

from which the role llers are chosen. In addition to the
probabilistic component, a P-CLASSIC knowledge base also
contains a standard terminological component, describing
complex concepts in terms of primitive ones. In this paper
we do not consider knowledge bases with ground facts (i.e.,
Aboxes).

(primitive concept)
(conjunction)
(negation on primitive concepts)
(universal quantication)

The non-probabilistic component of the P-CLASSIC lan-
guage is a variant of the CLASSIC description logic. Like

to standard roles. Attributes are binary relations which are
functional: each individual has exactly one ller for that at-
tribute. In P-CLASSIC we add the restriction that the ller for
an attribute be one of a nite prespecied set of individuals.
Complex descriptions in our language are built using the

CLASSIC, we also allow a set of attributesQ in addition
following grammar, whereAA denotes a primitive con-
cept,RR a role,QQ denotes an attribute, andC and
D represent concept descriptions:
C;D!Aj
CuDj
:Aj
R:Cj
((cid:21)nR)j((cid:20)nR)j (number restrictions)
((cid:12)llsQa)
nology DT (the Tbox) and a set of ground atomic facts DA
tence of the formC :=D, whereC is a name of a dened
concept andD is a description. Each dened concept is
tion. A canonical form of a description is(cid:11)u(cid:12)R1
u:::u(cid:12)Rm ,
where(cid:11) is a conjunction of primitive concepts and their
attribute appearing more than once), and(cid:12)R is of the form
((cid:21)mRR)u((cid:20)nRR)u(R:C), whereC is also in
description is dened as follows. The depth of(cid:11) is 0. The
depth of a concept(cid:11)u(cid:12)R1
u:::u(cid:12)Rm is 1+Max(depth((cid:11)),
depth((cid:12)R1),::: , depth((cid:12)Rm)).

Readers familiar with CLASSIC will see that our language
does not contain CLASSICs same-as constructor, but does
support negation on primitive concepts that is not allowed in
CLASSIC. CLASSIC also allows the lls constructor to be ap-
plied to nonfunctionalroles. It should be noted that allowing
negation on primitive concepts does not change the expres-
sive power of CLASSIC; it therefore follows from [Borgida
and Patel-Schneider, 1994] that subsumption in the language
described above can be done in polynomial time.

(the Abox). In this paper, we do not consider Aboxes. In
CLASSIC, a terminology contains two kinds of statements:
concept introductions, describing the primitive concepts in
the terminology, and concept denitions, specifying the de-
ned concepts. In P-CLASSIC a terminology includes only
the concept denitions (as we describe shortly), while con-
cept introductions are given as part of the probabilisticcom-
ponent of a knowledge base. A concept denition is a sen-

dened by a single concept denition, and we assume that
names of dened concepts do not appear in the descriptions.1
In our analysis, we use the canonical form of a descrip-

canonical form. Any description in our language can be
converted to canonical form in linear time. The depth of a

negations and of ller specications (with no concept or

A description logic knowledge base D

includes a termi-

(ller specication)

1This is not a restriction when the terminology has no cycles
(as in CLASSIC) because the denitions in the terminology can be
unfolded. However, as usual, unfolding a terminology may cause
its size to grow exponentially.

class (p-class)P species a probability distribution over the
that an object belongs to conceptD given that it belongs to
conceptC?. We write such queries as Pr(DjC).
setP of p-classes. Intuitively, a p-class represents our prob-
Each p-classPP is represented using a Bayesian net-
work [Pearl, 1988]NP . A Bayesian network is a DAG in

properties of individuals, allowing us to dene the extent of
the overlap between them. From these numbers, and appro-
priate probabilistic independence assumptions, we are able
to deduce the answers to arbitrary probabilistic subsump-
tion queries: queries of the form What is the probability

2.2 The Probabilistic Component of P-CLASSIC
The main motivation for P-CLASSIC is to be able to express
the degree of overlap between concepts. A probabilistic

abilistic information relating to a certain class of individuals.

The probabilistic component of P-CLASSIC consists of a

which the nodes represent random variables. Each variable
takes on a value in some predened range. Each node in
the network is associated with a conditional probability ta-
ble (CPT), which denes the probability of each possible
value of the node, given each combination of values for the
nodes parents in the DAG. The network structure encodes
the independence assumption that only the parent values are
relevant when making this choice. A Bayesian network de-
nes a joint probability distribution over all combinations
of values of the variables in the network. The probability of
a particular assignment of values is the product over all the
nodes in the network of the conditional probability of the
value of that node given the values of its parents.

Animal

T
F

0.5
0.5

Mammal

A
0.3
0.7

-A
0
1

T
F

Carnivore

0
1

Vegetable

T
F

T
F

Herbivore

A,C
0
1

T
F

A
0
1

-A
1
0

A,M
0.4
0.6

0.2
0.8

*
*

A,-M -A,M -A,-M

ple, consider the truth assignment(ANIMAL;:VEGETABLE;

Figure 1 shows part of the Bayesian network for a p-
class. This network contains a node for each of the primitive
concepts ANIMAL, VEGETABLE, MAMMAL, CARNIVORE and
HERBIVORE. The value of each node is either true or false,
depending on whether an object belongs to the concept or
not. The network denes a joint probability distribution over
the set of truth assignments to these concepts. For exam-

Figure 1:
NATURAL THING p-class.

the Bayesian network for

Part of

-A,-C
0
1

A,-C
0.7
0.3

-A,C

*
*

the

computed by the product

of primitive concepts and their negations, the probability

:MAMMAL; CARNIVORE;:HERBIVORE). Its probability is
Pr(ANIMAL)(cid:1) Pr(:VEGETABLEj ANIMAL)(cid:1)
Pr(:MAMMALj ANIMAL)(cid:1) Pr(CARNIVOREj ANIMAL;:MAMMAL)(cid:1)
Pr(:HERBIVOREj ANIMAL; CARNIVORE)
= 0:5(cid:1) 1(cid:1) 0:7(cid:1) 0:2(cid:1) 1= 0:07:
In general,NP will contain a node for each primitive con-
ceptAA. For any descriptionC which is a conjunction
NP(C) is dened by the network. This allows simple sub-
sumption queries of the form Pr(DjC) to be answered
when bothC andD are conjunctions of primitive concepts
distribution over attribute values. For each attributeQ
Q,NP contains a node FILLS(Q). The CPT for a node
lists some nite set of objectsv1;:::;vk, and for eachvi,
the probability that (llsQvi) holds given each possible
each roleRR, the network species the number of
R-llers by including a NUMBER(R) node, which takes on
values between 0 and some upper boundbR. The nodes
value denotes the number ofR-llers that the object has.
which the llers are chosen. Thus, for each roleR, the
the setP of p-classes. This node is always deterministic,
of an object. One of the p-classes, denotedP(cid:3)
networkNP(cid:3) . As we traverse the Bayesian network from its

i.e., for any combination of values of its parents, exactly one
p-class is assigned probability 1, and all other p-classes are
assigned probability 0. For simplicity of presentation, we
place some restrictions on the topology of the network. We
assume that a NUMBER(R) node may only be a parent of the
corresponding PC(R) node. The PC(R) node may not be a
parent of any other node.

The probabilistic component of a P-CLASSIC knowledge
base recursively describes a distribution over the properties
, is the
root p-class, denoting the distribution over all objects. The
properties of an object are chosen according to the Bayeisan

combination of values for the nodes parents. Note that the
node for an attribute also enumerates the set of values that
the attribute may take, and therefore can be used instead of
CLASSICs one-of constructor for attributes.

To describe the properties of the llers, we simply assign a
p-class for each role, which species the distribution from

To fully describe an individual, we also need to describe
the number of its various llers and their properties. For

network contains a PC(R) node, whose value ranges over

The network for a p-class also determines the probability

and their negations.

roots down, each node tells us the probability with which its
value should be chosen. The root nodes are chosen with the
appropriate unconditional probability, while the distribution
used for other nodes is determined by prior choices for the
values of their parents. In particular, the network dictates
how the number of llers for each role is chosen given the
basic properties and attribute values. By specifying the p-
class for the llers, the network species how the properties
of the llers are chosen recursively using a similar process.
As stated earlier, CLASSIC allows us to state concept in-
troductions in its terminology. In P-CLASSIC, concept in-
troductions can be represented directly in the probabilistic

description. For simplicity of exposition, we assume that

component of the knowledge base (in fact, concept intro-
ductions are a special case of probabilistic assertions). A

concept introduction is a sentence of the formA(cid:18)D,
whereA is a name of a primitive concept andD is a concept
D does not mention any of the roles. (In Section 5.2 we
'1;:::;'n is a list of concept introductions, then the de-
scription in'i can only mention the concepts introduced
in'1;:::;'i(cid:0)1, or the concept THING (which denotes the
set of all individuals). A concept introductionA(cid:18)D is
class that the probability of:DuA is 0. Since the concept
by making the concepts appearing inD parents ofA, and
setting the appropriate entries in the CPT forA to 0.
In the NATURAL THING p-class, Pr(ANIMAL)= 0:5,
while Pr(VEGETABLEj ANIMAL)= 0, and Pr(VEGETABLEj
:ANIMAL)= 1. These assertions encode the termino-
and Pr(MAMMALj ANIMAL)= 0:3. Only animals can
:ANIMAL,MAMMAL are irrelevant since that combination

Figure 2 shows the probabilistic component of the
knowledge-base for a domain of natural objects. There are
three p-classes, each being a network containing the nodes
ANIMAL, VEGETABLE, MAMMAL, CARNIVORE, HERBIVORE,
FILLS(SIZE), NUMBER(EATS) and PC(EATS).

logical knowledge that everything is either an animal or a
vegetable, and the two concepts are disjoint.2 In the CPT
for MAMMAL, we see that only animals can be mammals,

describe how to remove this restriction.) As in CLASSIC,
we consider concept introductions that are acyclic, i.e., if

be carnivores, and mammals are more likely than other
animals to be carnivorous; the entries in the column for

encoded in the knowledge base by specifying in each p-

introductions are acyclic, we can encode this information

is impossible. The FILLS(SIZE) node indicates that the value
of the size attribute must be big, medium, or small, and
the conditional probability of each value. Since vegetables
dont eat, NUMBER(EATS) is always 0 if VEGETABLE is true,
while for non-vegetables (i.e., animals) it is a number be-
tween 1 and 6 with the given distribution. Finally, PC(EATS)
depends on CARNIVORE and HERBIVORE: for carnivores it
is CARNIVORE FOOD, for herbivores it is HERBIVORE FOOD,
while for things which are neither it is NATURAL THING.
Since nothing is both a carnivore and a herbivore, that col-
umn is irrelevant.

is

the

same

The CARNIVORE FOOD p-class

as
NATURAL THING conditioned on ANIMAL being true.
Thus VEGETABLE is false, and the other nodes only
contain columns that are consistent with these facts.
HERBIVORE FOOD is the same as NATURAL THING condi-
tioned on VEGETABLE being true. In this case the p-class
is deterministic except for the value of FILLS(SIZE), since
ANIMAL, MAMMAL, CARNIVORE and HERBIVORE are all
false, and NUMBER(EATS) is 0. PC(EATS) is irrelevant since
there are no eats-llers.

2Strictly speaking, the probabilistic version of this statement
is slightly weaker than the terminological version, because it is
possible that the set of things that are both animal and vegetable is
non-empty but of measure zero.

Natural Thing

Herbivore Food

Animal

T
F

0.5
0.5

Vegetable

Mammal

A
0.3
0.7

-A
0
1

T
F

Animal

T
F

0
1

Vegetable

Mammal

T
F

0
1

Carnivore

T
F

0
1

Herbivore

T
F

0
1

PC(eats)

Carnivore Food

Animal

T
F

1
0

Mammal

T
F

0.3
0.7

Vegetable

T
F

0
1

Herbivore

Carnivore

M
0.4
0.6

-M
0.2
0.8

T
F

Fills(size)

Figure 2: P-classes for the nature domain.

T
F

Number(eats)

1
0

1
0
0
0
0
0
0

-M,C
0.5
0.3
0.2

-M,-C
0.1
0.3
0.6

A
0
1

-A
1
0

T
F

big

0
1

*
*

T
F

T
F

-A,C

*
*

1
0
0

C,-H

-C,H

0
0
1

0
1
0

-C,-H

0.2
0.8

PC(eats)

Fills(size)

Herbivore

Carnivore

0
1
2
3
4
5
6

0
1
2
3
4
5
6

medium
small

NT
CF
HF

Number(eats)

A,C
0
1

A,M
0.4
0.6

V
1
0
0
0
0
0
0

A,-C
0.7
0.3

C,H
*
*
*

-A,-C
0
1

M,C
0.7
0.2
0.1

A,-M -A,M -A,-M

M,-C
0.3
0.3
0.4

-V
0
0.2
0.3
0.2
0.1
0.1
0.1

3 Semantics of P-CLASSIC

The semantics of P-CLASSIC is an extension of the semantics
of CLASSIC. The basic element is an interpretation. An
. It assigns

interpretationI contains a non-empty domainOI
an elementaIOI
to every individuala, a unary relation
AI
to every concept nameAA, a binary relationRI
overOI(cid:2)OI
to every roleRR, and a total function
QI
:O!O to every attributeQQ. The interpretations
as follows (]fSg denotes the cardinality of a setS):
(CuD)I=CI\DI
,(:A)I=OInAI
(R:C)I=fdOIje :(d;e)RI!eCIg,
((cid:21)nR)I=fdOIj]fej(d;e)RIg(cid:21)ng,
((cid:20)nR)I=fdOIj]fej(d;e)RIg(cid:20)ng
((cid:12)llsQa)I=fdOIjQI(d)=ag.
An interpretationI is a model of a terminology DT if
CI=DI
for every concept denitionC :=D in DT . A
conceptC is said to be subsumed by a conceptD w.r.t. a
terminology DT ifCI(cid:18)DI
for every modelI of DT .
with a distribution over the domainOI
P describes a random event: the selection of an individual

In order to extend this semantics to P-CLASSIC, we have
to provide an interpretation for the p-classes. Following
[Halpern, 1990], we interpret a p-class as an objective (sta-
tistical) probability.3 That is, each p-class will be associated
. Intuitively, a p-class

of the descriptions are dened recursively on their structure

,

3It is also possible to ascribe semantics to this language using
subjective probabilities, i.e., distribution over possible interpre-
tations. For our purposes, the statistical interpretation is quite
natural, and signicantly simpler to explain.

C
0
1

-C
0.7
0.3

M,C
0.7
0.2
0.1

M,-C
0.3
0.3
0.4

-M,C
0.5
0.3
0.2

-M,-C
0.1
0.3
0.6

big

big

small

T
F

-C,H

C,-H

1
0
0

0
0
1

0
1
0

*
*
*

medium

-C,-H

PC(eats)

Fills(size)

0
1
2
3
4
5
6

0.1
0.3
0.6

medium
small

NT
CF
HF

NT
CF
HF

Number(eats)

C,H
*
*
*

0
0.2
0.3
0.2
0.1
0.1
0.1

from the domain. The probability with which an individ-
ual is chosen depends on its properties (as determined by
the other components of the interpretation). First, a truth
assignment to the primitive concepts and an assignment of
values to attributes is chosen, according to the probability

distribution dened in the networkNP . Given this assign-
Denition 3.1: LetI be some interpretation over our vo-
cabulary. A probability distribution(cid:22) overOI
with a p-classP if the following condition holds. For every
conjunctive descriptionC such that:
(a) for every primitive classAA,C contains eitherA
or:A as a conjunct,
(b) for every attributeQQ,C contains a conjunct (lls
Qv) for somev, and
(c) for every roleRR,C contains a conjunct(=hR)
for some integerh,4
1.(cid:22)(CI)=NP(C), i.e., the probability of the set of in-
dividuals in the interpretation ofC is the same as the
probability assigned toC by the Bayesian network forP .
2. For every roleR, consider the experiment consisting of
selecting an objectx according to(cid:22), conditioning on the
4We use(=hR) as a shorthand for((cid:20)hR)u((cid:21)hR).

ment, the number of llers for each role is chosen, according
to the conditional probability tables of the NUMBER nodes.
Finally, the properties of the llers are chosen, using another
random event, as described by the p-class determined by the
PC node for the appropriate role. Our semantics require that
the probability of choosing an element be consistent with
this type of random experiment for choosing its properties.

we have that:

is consistent

. This

for a P-CLASSIC

description logic component and a set of probability dis-

other. In other words, the experiment just described gen-
erates the same distribution when we also condition on
properties of other llers, and recursively on the proper-
ties of llers of other llers, and so on. This independence
assumption applies both to other llers of the same role,
and to llers of other roles.

eventxCI
, and picking one ofxsR-llers at random.
This experiment generates a new distribution(cid:22)
new distribution(cid:22)
must be consistent withP
, whereP
is the deterministic value ofNP(PC(R)jC).
3. GivenC, the different llers are all independent of each

We can now dene an interpretationIp
knowledge base to consist of an interpretationI for the
tributions(cid:22)IP overOI
, one for every p-classP , such that
(cid:22)IP is consistent withP . For any descriptionD, we say
Ipj= Pr(D)=(cid:26) if(cid:22)IP(cid:3)(D)=(cid:26), whereP(cid:3)
distribution forR-llers must assign a non-zero probability
to some element. If that element is chosen as the rstR-
R-ller, thereby violating the independence condition.
In an innite domain, on the other hand,(cid:22)Ip is a prob-
any descriptionC, there is a unique(cid:26)[0; 1] such that
j=(Pr(C)=(cid:26)).

The conditions in Denition 3.1 are sufcient to guar-
antee that the probability of any description is uniquely
determined by a P-CLASSIC knowledge base. As we will
see, this result is a consequence of the fact that a Bayesian
network uniquely species a probability distribution, and of
our independence assumptions. The following theorem is
proved by induction on the depth of the canonical form of a
description.
Theorem 3.3: For any P-CLASSIC knowledge base D

ability measure over the domain. While each individual
element has probability zero of being selected, the measure
assigns a non-zero probability to properties of elements. In
fact, we can construct a domain of this type, by dening do-
main elements to correspond to the set of description-logic
properties that they satisfy. This construction is the basis
for the following theorem. Proofs of theorems are omitted
due to space limitations.

This result follows because the acyclicity and locality of
the CPTs prevent us from specifying an inconsistent set of
probabilistic constraints (including with respect to termino-
logical information). Note that a satisable interpretation
may assign the empty set to some concepts.

One somewhat counterintuitive consequence of the inde-
pendence assumption is that if any role has non-zero prob-
ability of having more than one ller, then any model for
the knowledge base must be innite. In a nite domain, the

Theorem 3.2: A P-CLASSIC knowledge base is always sat-
isable in some interpretation.

ller, then by the requirement that different role-llers be
distinct, it must have zero probability of being the second

is the root

p-class.

, and

4 Inference Algorithm

It is possible to devise a simple algorithm for computing the

In Bayesian
network terms, this probability is d-separated [Pearl, 1988]
from the rest of the network by NUMBER(R) and PC(R).
Thus, we can add a node to the network representing the

cost of such a procedure would be exponential in the depth
of the concept and in the number of primitive concepts and
attributes. To obtain a tractable algorithm, we make two
observations. First, probabilities can be computed bottom
up, beginning with subexpressions of depth 0. Probabilities
of deeper expressions are computed using the stored prob-
abilities of the subexpressions. The second observation is

probability of a concept of the form(cid:11)u(cid:12)R1
u:::u(cid:12)Rm , by
rst computing the probability of(cid:11), and then, recursively,
computing the probabilities of(cid:12)R1
:::;(cid:12)Rm . However, the
that the probability that(cid:12)R holds is completely determined
by the number and p-class of theR-llers.
event(cid:12)R, with the parents NUMBER(R) and PC(R). For
each pair of values(h;P) for the parents, we can compute
the conditional probability of(cid:12)R givenh andP
then assert as evidence that all the(cid:12)R hold, as well as(cid:11), and
compute the probability ofC by computing the probability
and complete. In other words, for any descriptionC and
, it returns the number(cid:26) such
j=(Pr(C)=(cid:26)).
algorithm is linear in the length ofC and polynomial in
puteProbability is linear in the length ofC and quadratic
in the number of p-classes inKB. If all the Bayesian net-
mammalu((cid:21) 1 eats)ueats.mammal using the networks
contains the depth 0 subexpressionmammal. The rst
stage of the computation calculates Prob(mammal) for

We rely on the Bayesian network inference algorithm to
compute this probability in the most efcient manner pos-
sible. In particular, if the original network is a polytree,5
thus supporting linear time inference, the new network will
continue to support linear time inference after the transfor-
mation. The complete algorithm is shown in Figure 3.
Theorem 4.1: Algorithm ComputeProbability is sound
P-CLASSIC knowledge base D
that D

the number of p-classes.
It is important to note that the
complexity of reasoning in P-CLASSIC is no worse than that
of its components, i.e., CLASSIC and Bayes nets alone.
Theorem 4.2 : The running time of algorithm Com-

works for the p-classes are polytrees, then the running time
of ComputeProbability is also polynomial in the size of the
knowledge base.

, and add
it to the conditional probability table for the new node. We

in Figure 2. The depth of this expression is 1, and it

The following theorem shows that the complexity of the

As an example, consider computing the probability of

of the evidence in the Bayesian network.

each of the p-classes NATURAL THING, CARNIVORE FOOD
these probabilities are found to
and HERBIVORE FOOD;

5A polytree is a Bayesian network whose underlying undirected
graph is acyclic, i.e., there is only one path of inuence between
any pair of nodes. Polytrees support linear time probabilistic
inference [Pearl, 1988].

D
, CPT) adds node N with parents P

// AddNode(BN, N, P
and
// conditional probability table CPT to Bayesian network BN.
// AddEvidence(BN, N = v) asserts that N has value V in BN.
// Evaluate(BN) computes the probability of the evidence in BN.

ALGORITHM ComputeProbability(C;KB)
//C is a description in canonical form.
//KB is a P-CLASSIC knowledge base.
// Returns(cid:26) such thatKBj=(Pr(C)=(cid:26)).
forx = 0 todepth(C)
for each p-classP and subexpressionC
of depthx inC do:
// whenx=depth(C), only do this for the root p-classP(cid:3)
//C=(cid:11)u(cid:12)R1
u:::u(cid:12)Rm,
//(cid:12)Rj=((cid:21)ljRj)u((cid:20)ujRj)u(Rj:Dj)
BN :=NP
forj := 1 to m // skip if m = 0
forh := 0 tobj
//bj is the bound on the
for each p-classP
// number ofRj-llers
ifh(cid:21)lj andh(cid:20)uj
CPT[h;P; true] := ProbP(Dj)h
else CPT[h;P; true] := 0
CPT[h;P; false] = 1 - CPT[h;P; true]
:=fNUMBER(Rj); PC(Rj)g
AddNode(BN,(cid:12)Rj , P
AddEvidence(BN,(cid:12)Rj= true)
AddEvidence(BN,(cid:11))
// i.e., AddEvidence for each conjunct in(cid:11)
ProbP(C) = Evaluate(BN)
return ProbP(cid:3)(C)
ProbP(cid:3)(mammalu((cid:21) 1 eats)ueats.mammal). In this
description,(cid:11) is mammal, and there is one(cid:12) compo-
nent corresponding to the eats role.P(cid:3)
NATURAL THING. A node is added toNP(cid:3) for(cid:12)eats,
shows the conditional probability that(cid:12)eats is true for
quires that((cid:21) 1 eats), the entries in the rst column are
ously computed value of ProbNT(mammal). The remain-
ing entries in the rst row are 0:152, 0:153 and so on.
Similarly, the entries for CARNIVORE FOOD are 0.3, 0:32,
ProbHF(mammal)= 0. The probability of our query is
mal and(cid:12)eats to be true, resulting in an answer of approx-

all 0. When there is exactly one ller of the p-class
NATURAL THING, the entry is 0.15, which is the previ-

then computed in the resulting network, by asserting mam-

with the parents NUMBER(EATS) and PC(EATS). Figure 4

each value of its parents. Because the description re-

etc. The entries for HERBIVORE FOOD are all 0 because

Figure 3: Algorithm ComputeProbability

be 0.15, 0.3 and 0 respectively.

Next we calculate

is the p-class

, CPT)

imately 0.007.

1

4

0

2

0

6

0

5

0

3

0

0
0
0
0

NT
CF
HF

0.000
0.001

0.023
0.09

0.000
0.002

0.001
0.008

0.004
0.027

0.15
0.3
0

given values of NUMBER(EATS) and PC(EATS).

inference algorithm. In this section, we discuss several fea-
tures and limitations of the expressive power of P-CLASSIC.
We mention several possible extensions both to the under-
lying description logic and to the probabilistic component.
We examine the extent to which these extensions can be ac-
commodated in our framework and how they would affect
the complexity of reasoning.

Figure 4: The probability of((cid:21) 1 eats)u(eats.mammal)
by expressing the probability that the number of llers isn
using a closed form functionf(n). The inference problem
nite series involvingf(n). Alternatively, arbitrarily close

5.1 The Underlying Description Logic
P-CLASSIC can be easily extended to handle disjunctive
concepts, existential quantication, negation on arbitrary
concepts (not only primitive ones) and qualied number
restrictions. Our semantics provide a well-dened proba-
bility for descriptions using these constructors. However,
the inference algorithm for computing probabilities for such
descriptions is signicantly more complicated, and is no
longer polynomial in the length of the description. This
is not surprising, since the lower bounds (NP-hardness or
worse) of subsumption in the presence of these constructs
will apply to the probabilistic extension as well.

Another restriction, as mentioned above, is that the num-
ber of llers for each role be bounded. This restriction exists
for two reasons. First, it allows us to completely specify the
distribution over the number of llers for a role. Second,
the inference algorithm considers all possible values for the
number of llers, so this restriction is required to guarantee
that the algorithm terminates. We can address the rst issue

is somewhat harder.
In certain cases, the algorithm may
be able to compute a closed form expression for an in-

approximations to the true probability can be obtained by
summing a sufciently long nite prex of this series.

The only constructor from CLASSIC that is not included
in P-CLASSIC is same-as, which enables to describe equal-
ity of attribute paths (e.g., to state that the path wife.father
reaches the same individual as the path mother). Such
equalities are harder to incorporate into our language be-
cause our semantics depends heavily on the assumption
that different llers are completely independent. Equality
of individuals reached via different ller chains obviously
contradicts this independence assumption.

5 Discussion and Extensions

The motivation in designing P-CLASSIC was to develop a
tractable rst-order probabilistic logic. To that end, we have
shown that P-CLASSIC has a sound, complete and efcient

5.2 The probabilistic component
The tractability of our language rests heavily on the in-
dependence assumptions made in the probabilistic compo-
nent: (1) the properties of different llers are independent,

P
and (2) the properties of an object and of one of its llers
are independent given the llers p-class. For example, our
assumptions prevent us from stating that a non-carnivore
always eats at least one vegetable. Such an assertion would
imply that the eats-llers are not independent, since if all
but one of the llers are not vegetables, the last one must be.
Thus, although we can (albeit at a high computational cost)
compute the probabilityof any description with disjunctions
or existentials, we cannot assert terminological properties
involving these constructs without modifying the semantics.
One type of correlation between llers can actually be
dealt with fairly easily within our framework. Assume
that our vocabulary contains the concept healthy-to-eat.
We may want to assert that the healthiness of the vari-
ous foods eaten by a person tends to be correlated. We
can accomplish this by introducing a new concept health-
conscious and two p-classes representing HEALTHY FOOD
and UNHEALTHY FOOD. The value of the node PC(EATS)
can now depend on the value of HEALTH-CONSCIOUS, in
that a value of HEALTHY FOOD is more likely when health-
conscious is true. This new concept is a hidden concept
(analogous to a hidden variable in Bayesian networks). It
plays the same role as a regular primitive concept in the def-
inition of the p-class, but it is not a primitive concept in the
language and therefore does not appear in the terminology
or in queries.

One promising alternative with greater expressive power
arises from our recent work on functional stochastic pro-
grams [Koller et al., 1997]. The basic idea is that we view
each p-class as a stochastic function, and each individual
as a call to the function for that appropriate p-class. A call
to such a stochastic function chooses (randomly) the proper-
ties of the individual for which it is called. The properties of
the llers for an individual are chosen by recursive calls to
the functions for the appropriate p-classes. Once we view a
ller as a function, we can pass the relevant properties of the
individual (e.g., on-a-diet) as a parameter to the function.
The parameters can directly inuence the distribution of the
ller properties. In our example of Section 2, the p-classes
for CARNIVORE FOOD and HERBIVORE FOOD are equivalent
to those that would have been obtained had we passed the
value of animal to the ller as a parameter (with a value of
true and false respectively).

concepteats.low-fat. The node representing cholesterol-
A(cid:18)D whereD is an arbitrary complex concept. Some-

what surprisingly, we can accomodate the functional view

In addition to taking arguments, a stochastic function can
also return values.
In our context, this feature would
allow properties of an object to depend on those of its
llers. Thus, for example, we can have the probability
over the cholesterol-level attribute depend on the actual
fat content of the foods eaten by the person. We could
represent this type of dependency by introducing a node
into the Bayesian network corresponding to the complex

level can now be a child of this new node, encoding the
desired dependency. In general, we could have llers recur-
sively pass back the truth value of deeply nested complex
concepts. This extension supports concept introductions

of llers fairly easily within our framework, and without a
signicant increase in computational cost. We defer details
to the full version of this paper.
Acknowledgements We thank Manfred Jaeger for useful
discussions. Some of this work was done while Daphne
Koller and Avi Pfeffer were visiting AT&T Research. This
work was also supported through the generosity of the Pow-
ell foundation, by ONR grant N00014-96-1-0718, and by
DARPA contract DACA76-93-C-0025, under subcontract
to Information Extraction and Transport, Inc.

