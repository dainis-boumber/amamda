Abstract

BENGIOY@IRO.UMONTREAL.CA

YVES.GRANDVALET@UTC.FR

Most machine learning researchers perform quantitative experiments to estimate generalization
error and compare the performance of different algorithms (in particular, their proposed algorithm).
In order to be able to draw statistically convincing conclusions, it is important to estimate the
uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation
estimator of generalization performance. The main theorem shows that there exists no universal
(valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The
analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix
of errors, which has only three different eigenvalues corresponding to three degrees of freedom of
the matrix and three components of the total variance. This analysis helps to better understand the
nature of the problem and how it can make naive estimators (that dont take into account the error
correlations due to the overlap between training and test sets) grossly underestimate variance. This
is conrmed by numerical experiments in which the three components of the variance are compared
when the difculty of the learning problem and the number of folds are varied.
Keywords: cross-validation, variance estimators, k-fold cross-validation, statistical comparisons
of algorithms

1. Introduction

In machine learning, the standard measure of accuracy for trained models is the prediction error
(PE), i.e.
the expected loss on future examples. Learning algorithms themselves are often com-
pared according to their average performance, which is formally dened as the expected value of
prediction error (EPE) over training sets.

When the data distribution is unknown, PE and EPE cannot be computed. If the amount of
data is large enough, PE can be estimated by the mean error over a hold-out test set. The usual
variance estimates for means of independent samples can then be computed to derive error bars
on the estimated prediction error, and to assess the statistical signicance of differences between
models.

The hold-out technique does not account for the variance with respect to the training set, and
may thus be considered inappropriate for the purpose of algorithm comparison (Dietterich, 1999).
Moreover, it makes an inefcient use of data which forbids its application to small sample sizes. In

c(cid:13)2004 Yoshua Bengio and Yves Grandvalet.

BENGIO AND GRANDVALET

this situation, one rather uses computer intensive resampling methods such as cross-validation or
bootstrap to estimate PE or EPE.

We focus here on K-fold cross-validation. While it is known that cross-validation provides an
unbiased estimate of EPE, it is also known that its variance may be very large (Breiman, 1996). This
variance should be estimated to provide faithful condence intervals on PE or EPE, and to test the
signicance of observed differences between algorithms. This paper provides theoretical arguments
showing the difculty of this estimation.

The difculties of the variance estimation have already been addressed (Dietterich, 1999; Ko-
havi, 1995; Nadeau and Bengio, 2003). Some distribution-free bounds on the deviations of cross-
validation are available, but they are specic to some locally dened decision rules, such as nearest
neighbors (Devroye et al., 1996). This paper builds upon the work of Nadeau and Bengio (2003),
which investigated in detail the theoretical and practical merits of several estimators of the variance
of cross-validation. Our analysis departs from this work in the sampling procedure dening the
cross-validation estimate. While Nadeau and Bengio (2003) consider K independent training and
test splits, we focus on the standard K-fold cross-validation procedure, where there is no overlap
between test sets: each example of the original data set is used once and only once as a test example.
This paper is organized as follows. Section 2 denes the measures of performance for algo-
rithms, their estimation by K-fold cross-validation and similar procedures such as delete-m jack-
knife. Our theoretical ndings are summarized in Sections 36. They are followed in Section 7 by
experiments illustrating the effect of experimental conditions on the total variance and its decom-
position in three components, and conrming the underestimation of variance obtained by the naive
estimator commonly used by researchers.

2. General Framework

In machine learning, the performance measure differs according to the experimenters viewpoint. In
applications, we are interested in nding the best algorithm for solving the particular task at hand,
specied by one particular training set and some information about the data generating process. In
algorithm evaluation, we want to compare several learning algorithms for different learning tasks,
and we care about the sensitivity of the learning algorithm to the choice of training examples.

2.1 Measures of Performance

Formally, we have a training set D = {z1, . . . ,zn}, with zi  Z, independently sampled from an
unknown distribution P. We also have a learning algorithm A, which maps a data set of (almost)
arbitrary size to a function A : Z  F . Throughout this paper, we consider symmetric algorithms,
i.e. A is insensitive to the ordering of examples in the training set D. The discrepancy between the
prediction and the observation z is measured by a loss functional L : F  Z  R. Typically, L is
the quadratic loss in regression (L( f , (x, y)) = ( f (x)  y)2) and the misclassication {0,1}-loss in
classication ((L( f , (x, y)) = 1 f (x)6=y).

Let f = A(D) be the function returned by algorithm A on the training set D. In application based
evaluation, the goal of learning is usually stated as the minimization of the prediction error, i.e. the
expected loss on future test examples

PE(D) = E[L( f ,z)],

(1)

1090

VARIANCE OF K-FOLD CROSS-VALIDATION

where the expectation is taken with respect to z sampled from P.1

In algorithm based evaluation, we are not really interested in performances on a specic training
set; we would like comparisons on a more general basis. In this context, the lowest level of gen-
erality can be stated as training sets of size n sampled from P, and the performance of learning
algorithm A can be measured by the expected performance of the functions returned in this situation

EPE(n) = E[L(A(D),z)],

(2)

where the expectation is taken with respect to D sampled from Pn and z independently sampled
from P.

Note that other types of performances measure can be proposed, based for example on parame-
ters, or dened by the predictability in other frameworks, such as the prequential analysis (Dawid,
1997).

When the data distribution is unknown, PE and EPE cannot be computed. They have to be

estimated, and it is often crucial to assess the uncertainty attached to this estimation:

 in application-oriented experiments, to give a condence interval on PE;

 in algorithm-oriented experiments, to take into account the stability of a given algorithm.
For comparisons between algorithms, it is essential to assess the statistical signicance of

observed differences in the estimate dEPE.

Although this point is often overlooked, estimating the variance of the estimates cPE and dEPE re-

quires caution.

2.2 Hold-Out Estimates of Performance

If the amount of data is large enough, PE can be estimated by the mean error over a hold-out test set,
and the usual variance estimate for means of independent variables can then be computed. However,
even in the ideal situation where several independent training and test sets would be available, this

estimate should not be applied to compute the variance of dEPE: even though training and test

examples are independent, the test errors are correlated, since many test errors are computed for
each training set, now considered as a random variable.

Figure 1 illustrates how crucial it is to take these correlations into account. The mean of two
variance estimators is reported vs. the empirical variance of the hold-out estimate, in an ideal situ-

on 100,000 independent experiments) is displayed for reference by the dotted line. The average of

ation where 10 independent training and test sets are available. The variance of dEPE(n) (estimated
bq 1, the variance estimator ignoring correlations, shows that this estimate is highly biased, even for
large sample sizes, whereas the variance estimator bq 2, taking into account correlations, is unbiased.

The details of this experiment are given below.

Experiment 1 Ideal hold-out estimate of EPE.

We have K = 10 independent training sets D1, . . . , DK of n independent examples zi = (xi, yi),
where xi = (xi1, . . . , xid)0 is a d-dimensional centered, unit covariance Gaussian variable (d = 30),

1. Note that we are using the same notation for random variables and their realization. The intended meaning will be

specied when not clear from the context.

1091

BENGIO AND GRANDVALET

Figure 1: Estimates of the variance of dEPE(n) vs. empirical variance of dEPE(n) (shown by bold
curve) on 100,000 experiments. The average of the variance estimators bq 1 (ignoring
correlations, dashed curve) and bq 2 (taking into account correlations, dotted curve) are

displayed for different training sample size n.

i being independent, centered, unit variance Gaussian variables.2 We

also have K independent test sets T1, . . . , TK of size n sampled from the same distribution.

The learning algorithm consists in tting a line by ordinary least squares, and the estimate of
ziTk Lki, where Lki =

(cid:229) K

k=1

1
n

K

i with e

k=1 xik +e

yi =p3/d (cid:229) d
EPE is the average quadratic loss on test examples dEPE = L = 1
The rst estimate of variance of dEPE is bq 1 =

Kn(Kn1)
biased provided there is no correlation between test errors.
K(K1)n2 (cid:229) K

L(A(Dk),zi).

k=1 (cid:229)

1

1

(cid:229) K
k=1 (cid:229)

i, j(Lki  L)(Lk j  L), which takes into account correlations between test errors.

i(Lki  L)2, which is un-

The second estimate is bq 2 =

Looking at Figure 1 suggests that asymptotically the naive estimator of variance converges to
the true variance. This can be shown formally by taking advantage of the results in this paper,
as long as the learning algorithm converges as the amount of training data goes to innity (i.e. as
n  
the function A(D) obtained does not depend on the particular training set D). In that limit, the
correlations between test errors converge to 0. The rate of convergence will depend on the stability
of the learning algorithm as well as on the nature of the data distribution (e.g., the presence of thick
tails and outliers will slow down convergence).

The hold-out technique makes an inefcient use of data which forbids its application in most
real-life applications with small samples. Then, K-fold cross-validation can provide estimates of PE
or EPE.

2.3 K-Fold Cross-Validation Estimates of Performance

Cross-validation is a computer intensive technique, using all available examples as training and test
examples. It mimics the use of training and test sets by repeatedly training the algorithm K times
with a fraction 1/K of training examples left out for testing purposes. This kind of hold-out estimate
of performance lacks computational efciency due to the repeated training, but the latter are meant
to lower the variance of the estimate (Stone, 1974).

2. Thep3/d factor provides an R2 of approximately 3/4.

1092

(cid:229)
VARIANCE OF K-FOLD CROSS-VALIDATION

In practice, the data set D is rst chunked into K disjoint subsets (or blocks) of the same size3
= n/K. Let us write Tk for the k-th such block, and Dk the training set obtained by removing the
m
elements in Tk from D. The cross-validation estimator is dened as the average of the errors on test
block Tk obtained when the training set is deprived from Tk:

CV(D) =

1
K

K(cid:229)

k=1

1
m

ziTk

L(A(Dk),zi).

(3)

Does CV estimate PE or EPE? Such a question may seem pointless considering that PE(D) is
an estimate of EPE(n), but it becomes relevant when considering the variance of CV: does it inform
us of the uncertainty about PE or EPE?

k=1.

On the one hand, only one training set, D, enters the denition of CV, which can be, up to an
approximation, an unbiased estimate of PE(D) (Hastie and Tibshirani, 1990).4 Some distribution-
free bounds on the expected deviations of |CV(D)  PE(D)| are available for leave-one-out cross-
validation applied to specic algorithms A such as nearest neighbors (Devroye et al., 1996). In
a more general context, it has also been proved that, under suitable stability assumptions on the
algorithm A, CV(D) estimates PE(D) at least as accurately as the training error (Kearns and Ron,
1996; Anthony and Holden, 1998). A more appealing result states that CV is a more accurate
estimate of PE than hold-out testing (Blum et al., 1999). However, this statement does not apply
to PE(D), but to the prediction error of a randomized algorithm picking solutions uniformly within
{A(Dk)}K

On the other hand, CV is explicitly dened from the learning algorithm A, and not from the
function f = A(D). The inner average in the denition of CV (3) is an average test loss for A(Dk)
which thus estimates unbiasedly PE(Dk). The training sets D1, . . . , DK are clearly not independent,
but they are sampled from Pnm. Hence, the outer average of (3) estimates unbiasedly EPE(n m).5
Here, following Dietterich (1999) and Nadeau and Bengio (2003), we will adopt this latter point of
view.

The variance estimate of dEPE provided by the hold-out estimate has to account for test error

dependencies due to the choice of training set, which cannot be estimated using a single training/test
experiment. Here, the situation is more complex, since there are additional dependencies due to the
overlapping training sets D1, . . . , DK. Before describing this situation in detail and summarizing
the results of our theoretical analysis in Sections 36, we detail some procedures similar to K-fold
cross-validation, for which the forthcoming analysis will also hold.

2.4 Other Estimates of the K-Fold Cross-Validation Type

One of the main use of variance estimates of dEPE is to compare learning algorithms. The analysis

presented in this paper also applies to the version of cross-validation dedicated to this purpose: if
we want to compare the performances of algorithms A1 and A2, cross-validation with matched pairs

3. To simplify the analysis below we assume that n is a multiple of K.
4. More precisely, following Hastie and Tibshirani (1990), when L is the quadratic loss, and writing f = A(D), f k =
(cid:229) K
k=1 f k(xi)  f (xi) (which is weaker than f k  f ) yields E[CV] 

A(Dk), assuming that for (xi, yi) = zi  Tk, 1
K
E[ 1
n

(cid:229) n
i=1( f (xi)  yi)2], where the expectation is taken with respect to y1, . . . , yn.

5. Note that leave-one-out cross-validation is known to fail to estimate EPE for unsmooth statistics (e.g. Breiman, 1996;
Efron and Tibshirani, 1993). This failure is due to the similarity of the training sets D1, . . . , DK which are far from
being representative samples drawn from Pnm.

1093

D
(cid:229)
BENGIO AND GRANDVALET

should be the method of choice

D CV(D) =

1
K

K(cid:229)

k=1

1
m

ziTk

L(A1(Dk),zi)  L(A2(Dk),zi).

(4)

Compared to the difference of two independent cross-validation estimates, D CV avoids the addi-
tional variability due to train/test splits.

In application oriented experiments, we would like to estimate PE(D), the expected error when
training with the given D. We have seen in Section 2.3 that under stability assumptions, CV can be
used to estimate PE. Alternatively, we may resort to the jackknife or the delete-m jackknife (see e.g.
Efron and Tibshirani (1993)) to estimate the optimism (i.e. the bias of the mean error on training
examples, when the latter is used to estimate PE(D)). Ideally, the estimate of optimism should be
an average over all subsets of size n  m, but a less computationally intensive alternative is

(K  1)

1

K(n  m)

K(cid:229)

k=1

ziDk

L(A(Dk),zi) 

L(A(D),zi)! .

1
n

n(cid:229)

i=1

(5)

The link with cross-validation is exhibited more clearly by the following expression of the (de-

biased) jackknife estimate of PE

JK = CV +

1
n

K(cid:229)

n(cid:229)

k=1

i=1

(L(A(D),zi)  L(A(Dk),zi)) .

(6)

For additional information about jackknife estimates and clues on the derivation of (5) and (6), the
reader is referred to Efron and Tibshirani (1993).

2.5 Generic Notations
This paper studies the variance of statistics such as CV, D CV or JK. In what follows, these statistics
will be denoted by , a generic notation for means of observations ei split in K groups.

ei

 =

=

1
n

1
K

n(cid:229)

i=1

K(cid:229)

1
m

ei,

iTk

k=1

where, slightly abusing notation, i  Tk means zi  Tk and

i  Tk, ei =

L(A(Dk),zi)
L(A1(Dk),zi)  L(A2(Dk),zi)
KL(A(D),zi)  (cid:229)

`6=k L(A(D`),zi)

for  = CV,
for  = D CV,
for  = JK.

Note that  is the average of identically distributed (dependent) variables. Thus, it asymptoti-
cally converges to a normally distributed variable, which is completely characterized by its expec-
tation E[] and its variance Var[] = E[2]  E[]2.

1094

(cid:229)
(cid:229)
(cid:229)
VARIANCE OF K-FOLD CROSS-VALIDATION

3. Structure of the Covariance Matrix

The variance of  is dened as follows

q =

1

n2 (cid:229)

i, j

Cov(ei, e j),

where Cov(ei, e j) = E[eie j]  E[ei]E[e j] is the covariance between variables ei and e j.

By using symmetry arguments over permutations of the examples in D, we show that many
distributions on ei and pairwise joint distributions on (ei, e j) are identical. As a result, the covariance
matrix S has a very particular block structure, with only three possible values for S
i j = Cov(ei, e j),
and the expression of q

is thus a linear combination of these three values.

Lemma 1 Using the notation introduced in section 2.5,

1. all ei are identically distributed:

there exists f such that, i, P(ei = u) = f (u).

2. all pairs (ei, e j) belonging to the same test block are jointly identically distributed:

there exists g such that, (i, j)  T 2

k : j 6= i, P(ei = u, e j = v) = g(u, v).

3. all pairs (ei, e j) belonging to different test blocks are jointly identically distributed:

there exists h such that, i  Tk,  j  T` : ` 6= k, P(ei = u, e j = v) = h(u, v).

Proof
These results are derived immediately from the permutation-invariance of P(D) and the symmetry
of A.

 invariance with respect to permutations within test blocks:

k , P(ei = u) = P(ei0 = u) = fk(u);
k ,  j  T`:

1. (i, i0)  T 2
(i, i0)  T 2
P(ei = u, e j = v) = P(ei0 = u, e j = v)
hence:

2. (i, j)  T 2
3. i  Tk,  j  T` : ` 6= k, P(ei = u, e j = v) = hk`(u, v).

k : j 6= i, P(ei = u, e j = v) = gk(u, v).

 invariance with respect to permutations between test blocks.

1. (k, k0), fk(u) = fk0(u) = f (u);
2. (k, k0), gk(u, v) = gk0(u, v) = g(u, v);
3. (k, k0), (`, `0) : ` 6= k, ` 6= k0, `0

6= k, `0

hk0`(u, v) = h(u, v).

1095

6= k0, hk`(u, v) = hk`0(u, v) = hk0`0(u, v) =

Q.E.D.

BENGIO AND GRANDVALET

Corollary 2 The covariance matrix S of cross-validation errors e = (e1, . . . , en)0 has the simple
block structure depicted in Figure 2:

1. all diagonal elements are identical

i, Cov(ei, ei) = Var[ei] = s 2;

2. all the off-diagonal entries of the K m  m diagonal blocks are identical

(i, j)  T 2

k : j 6= i, Cov(ei, e j) = w

;

3. all the remaining entries are identical

i  Tk,  j  T` : ` 6= k, Cov(ei, e j) = g .

n

}|

{

z
|{z}m

Figure 2: Structure of the covariance matrix.

Corollary 3 The variance of the cross-validation estimator is a linear combination of three mo-
ments:

q =

=

1

n2 (cid:229)
i, j
s 2 +

Cov(ei, e j)

1
n

m  1

n

w +

n  m

n

(7)

Hence, the problem of estimating q does not involve estimating n(n + 1)/2 covariances, but it
cannot be reduced to that of estimating a single variance parameter. Three components intervene,
which may be interpreted as follows when  is the K-fold cross-validation estimate of EPE:

1. The variance s 2 is the average (taken over training sets) variance of errors for true test

examples when algorithm A is fed with training sets of size m(K  1).

2. The within-block covariance w would also apply to true test examples; it arises from the

dependence of test errors stemming from the common training set.

3. The between-blocks covariance g

is due to the dependence of training sets (which share n(K 

2)/K examples) and the fact that test block Tk appears in all the training sets D` for ` 6= k.

The forthcoming section makes use of this structure to show that there is no universal unbiased
estimator of q

.

1096

g
VARIANCE OF K-FOLD CROSS-VALIDATION

4. No Unbiased Estimator of Var[] Exists
Consider a generic estimator q
(e1, e2, . . . , en)0. Let us assume that q
its Taylor expansion

that depends on the sequence of cross-validation errors e =
is an analytic function of the errors, so that we can write

q = a 0 +(cid:229)

a 1(i)ei +(cid:229)

a 2(i, j)eie j + (cid:229)

a 3(i, j, k)eie jek + . . . .

i

i, j

i, j,k

(8)

We rst show that for unbiased variance estimates (i.e. E[q ] = Var[]), all the a
vanish except for the second order coefcients a 2,i, j.

i coefcients must

Lemma 4 There is no universal unbiased estimator of Var[] that involves the ei in a non-quadratic
way.
Proof
Take the expected value of q expressed as in (8), and equate it with Var[] (7):

E[q ] =a 0 +(cid:229)



q = 1
n
For having E[q ] = q

a 1(i)E[ei] +(cid:229)
g .
w + nm
n

i

s 2 + m1
n

a 2(i, j)E[eie j] + (cid:229)

a 3(i, j, k)E[eie jek] + . . .

i, j

i, j,k

for all possible values of the moments of e, one must have a 0 = 0 because q
has no such constant term, not depending on any of the moments of e. Similarly, a 1() must be zero
because q has no term in E[ei] = . Finally, the third and higher order coefcients a
`(. . .), ` > 2
must also be zero because q has only quantities depending on the second order moments s 2, w and
g .

Since estimators that include moments other than the second moments in their expectation are

biased, we now focus on the class of estimators which are quadratic forms of the errors, i.e.

q = e0We = (cid:229)

Wi jeie j.

i, j

(9)

Q.E.D.

Lemma 5 The expectation of quadratic estimators q dened as in (9) is a linear combination of
only three terms

E[q ] = a(s 2 + 2) + b(w + 2) + c(g + 2),

where (a, b, c) are dened as follows:

a
b
c

= (cid:229) n
= (cid:229) K
= (cid:229) K

i=1Wii,
k=1 (cid:229)
k=1 (cid:229)

iTk
`6=k (cid:229)

jTk: j6=iWi j,
iTk

jT` Wi j.



A trivial representer of estimators with this expected value is

q = as1 + bs2 + cs3,

1097

(10)

(11)

D
D
(cid:229)
D
(cid:229)
BENGIO AND GRANDVALET

where (s1, s2, s3) are the only quadratic statistics of e that are invariants to the within blocks and
between blocks permutations described in Lemma 1:

s1

s2

=

=

s3

=

e2
i ,

n(cid:229)

1
n

i=1
1

n(m  1)

1

n(n  m)

K(cid:229)

k=1

K(cid:229)

iTk

jTk: j6=i

eie j,

eie j.

k=1

`6=k

iTk

jT`



(12)

in Equation (9) that have

Proof
This result is obtained exploiting Corollary 2 and grouping the terms of q
the same expected values.
K(cid:229)

Wi jE[eie j] + (cid:229)

E[q ] =

i ] + (cid:229)

iTk WiiE[e2

k=1

Wi jE[eie j]!

jTk: j6=i
Wii + (w + 2)

`6=k

jT`

K(cid:229)

k=1

iTk

jTk: j6=i

Wi j +

= (s 2 + 2)

n(cid:229)

i=1

(g + 2)

K(cid:229)

Wi j

k=1

`6=k

iTk

jT`

= a(s 2 + 2) + b(w + 2) + c(g + 2)
= aE[s1] + bE[s2] + cE[s3],

which is recognized as the expectation of the estimator dened in Equation (11).

Q.E.D.

We now use Lemma 5 to prove that there is no universally unbiased estimator of Var[], i.e.

there is no estimator q such that E[q ] = Var[] for all possible distributions of e.
Theorem 6 There exists no universally unbiased estimator of Var[].
Proof
Because of Lemma 4 and 5, it is enough to prove the result for estimators that are quadratic forms
expressed as in Equation (11). To obtain unbiasedness, the expected value of that estimator must be
equated with Var[] (7):

a(s 2 + 2) + b(w + 2) + c(g + 2) =

s 2 +

1
n

m  1

n

w +

n  m

n

g .

(13)

For this equality to be satised for all distributions of cross-validation errors, it must be satised
for all admissible values of , s 2, w
, and g . This imposes the following unsatisable constraints on
(a, b, c):

a
b
c
a + b + c = 0.

= 1
n ,
= m1
n ,
= nm
n ,



1098

(14)

Q.E.D.

D
D
(cid:229)
(cid:229)
D
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
(cid:229)
VARIANCE OF K-FOLD CROSS-VALIDATION

5. Eigenanalysis of the Covariance Matrix

One way to gain insight on the origin of the negative statement of Theorem 6 is via the eigenanalysis
of S
, the covariance of e. This decomposition can be performed analytically thanks to the very
particular block structure displayed in Figure 2.

Lemma 7 Let vk be the binary vector indicating the membership of each example to test block k.
The eigensystem of S

is as follows:

 l 1 = s 2  w with multiplicity n  K and eigenspace dened by the orthogonal of basis

{vk}K

k=1;

 l 2 = s 2 + (m  1)w  mg with multiplicity K  1 and eigenspace dened in the orthogonal of

1 by the basis {vk}K

k=1;

 l 3 = s 2 + (m  1)w + (n  m)g with eigenvector 1.

Proof
From Corollary 2, the covariance matrix S = E[ee0]  E[e]E[e]0 can be decomposed as

S = (s 2  w )S 1 + m(w  g )S 2 + ng

3,

where S 1 = I, S 2 = 1

m (v1 . . .vK) (v1 . . .vK)0 and S 3 = 1

n110.

S 1, S 2 and S 3 share the same eigenvectors, with eigenvalues being equal either to zero or one:
 the eigenvector 1 has eigenvalue 1 for S 1, S 2 and S 3;

 the eigenspace dened in the orthogonal of 1 by the basis {vk}K

with eigenvalues 1 for S 1 and S 2 and 0 for S 3;

k=1 denes K  1 eigenvectors

 all remaining eigenvectors have eigenvalues 1 for S 1 and 0 for S 2 and S 3.

Q.E.D.

Lemma 7 states that the vector e can be decomposed into three uncorrelated parts: n  K projec-
tions to the subspace orthogonal to {vk}K
k=1 in
the orthogonal of 1, and one projection on 1. A single vector example with n independent elements
can be seen as n independent examples. Similarly, these projections of e can be equivalently repre-
sented by respectively n  K, K  1 and one uncorrelated one-dimensional examples, corresponding
to the coordinates of e in these subspaces.

k=1, K 1 projections to the subspace spanned by {vk}K

n1 is precisely . Hence there is no unbiased estimate of Var[] =

In particular, for the projection on 1, with only a single one-dimensional point, the sample
variance is null, resulting in the absence of an unbiased variance estimator of l 3. The projection
l 3
of e on the eigenvector 1
n when
we have only one realization of the vector e. For the same reason, even with simple parametric
assumptions on e (such as e Gaussian), the maximum likelihood estimate of q
is not dened. Only
l 1 and l 2 can be estimated unbiasedly. Note that this problem cannot be addressed by performing
multiple K-fold splits of the data set. Such a procedure would not provide independent realizations
of e.

1099

S
BENGIO AND GRANDVALET

6. Possible Values for w and g
Theorem 6 states that no estimator is unbiased, and in its demonstration, it is shown that the bias of
any quadratic estimator is a linear combination of 2, s 2, w
and g . Regarding estimation, it is thus
interesting to see what constraints restrict the possible range of these quantities.

Lemma 8 For  = CV and  = D CV, the following inequalities hold:

 w



s 2

(cid:26)
 (cid:26)

0

 w

nm (s 2 + (m  1)w )  g  1
 1
0
 m
nm
The shape of the admissible (w
displayed in Figure 3.

 s 2
s 2  g  s 2.
,g ) region corresponding to the rst set of (tighter) inequalities is

m (s 2 + (m  1)w )

K = 2

K = 5

s 2

0

s 2

s 2

0

s 2

s 2

0

s 2

K = 10

K = 100

s 2

0

s 2

s 2

0

s 2

s 2

0

s 2

s 2

0

s 2

s 2

0

s 2

Figure 3: Possible values of (w

,g ) according to s 2 for n = 200 and K = {2,5,10,100}.

Proof
The constraints on w
Var[u]Var[v], hence

result from the Cauchy-Schwartz inequality which provides Cov(u, v)2 

s 2  w  s 2.

Moreover, the following reasoning shows that, for  = CV and  = D CV, w
is
the covariance of (differences in) test errors for training sets of size n  m and test sets of size ` = m.
` (s 2 + (`  1)w ). The
The variance of the average test error is given by the mean of covariances 1
variance s 2 and covariance w of test errors are not affected by `, and the variance of the average

is non-negative: w

1100

w
g
w
g
w
g
w
g
VARIANCE OF K-FOLD CROSS-VALIDATION

test error should be non-negative for any test set size `. Hence w
When this type of reasoning cannot be used, as for  = JK, w
s 2/(m  1).

is bound to be non-negative.
can only be proved to be greater than

The constraints on g simply rephrase that the eigenvalues l 2 and l 3 of the covariance matrix S

should be non-negative. The simpler (and looser) form is obtained by using w  s 2.

The admissible (w

Q.E.D.
,g ) region obtained in Lemma 8 is very large. Furthermore, there is no con-
straint linking  and s 2, the mean and variance of ei. Hence we cannot propose a variance estimate
with universally small bias.

and g . The admissible values provided in the preceding section suggest that w

7. Experiments
We already mentioned that the bias of any quadratic estimator is a linear combination of 2, s 2,
and g cannot be
proved to be negligible compared to s 2. This section illustrates that in practice, the contribution to
the variance of  due to w
(see Equation (7)) can be of same order than the one due s 2. It
therefore suggests that the estimators of q should indeed take into account the correlations of ei.

and g

Experiment 2 True variance of K-fold cross-validation.

We repeat the experimental setup of Experiment 1, except that now, we are in the more realistic
situation where only one sample of size n is available. Since cross-validation is known to be sensitive
to the instability of algorithms, in addition to this standard setup, we also consider another one with
outliers:

The input xi = (xi1, . . . , xid)0 is still 30-dimensional, but it is now a mixture of two centered
Gaussian variables: let ti be a binary variable, with P(ti = 1) = p = 0.95; when ti = 1, xi  N (0,I);
i  N (0,1/(p +

when ti = 0, xi  N (0,100I); yi =p3/(d(p + 100(1  p)))(cid:229) d

i  N (0,100/(p + 100(1  p))) when ti = 0.

100(1  p))) when ti = 1 and e

k=1 xik + e

i with e

We now look at the variance of K-fold cross-validation (K = 10), and decompose in the three

orthogonal components s 2, w

and g . The results are shown in Figure 4.

When there are no outliers, the contribution of g

is very important for small sample sizes. For
large sample sizes, the overall variance is considerably reduced and is mainly caused by s 2. In these
situations, the learning algorithm returns very similar answers for all training sets. When there are
outliers, w has little effect, but the contribution of g
is of same order as the one of s 2, even when
the ratio of examples to free parameters is large (here up to 20). Thus, in difcult situations, where
A(D) varies according to the realization of D, neglecting the effect of w
and g can be expected to
introduce a bias of the order of the true variance.

and g

in s 2, w

It is also interesting to see how these quantities are affected by the number of folds K. The
decomposition of q
(7) does not imply that K should be set either to n or to 2
(according to the sign of w  g ) in order to minimize the variance of . Modifying K affects s 2, w
and g
through the size and overlaps of the training sets D1, . . . , DK, as illustrated in Figure 5. For
a xed sample size, the variance of  and the contribution of s 2, w
and g effects varies smoothly
with K.6 The experiments with and without outliers illustrate that there is no general trend either in

6. Of course, the mean of  is also affected in the process.

1101

w
BENGIO AND GRANDVALET

no outliers

outliers

Figure 4: Bar plots of the contributions to total variance Var[CV] due to s 2, w

and g vs. the number

of training examples n  m for Experiment 2.

variance or decomposition of the variance in its s 2, w
can be reached for K = n or for an intermediate value of K.

and g components. The minimum variance

no outliers

outliers

Figure 5: Bar plots of contributions of s 2, w

and g

to q vs. K for n = 120 for Experiment 2.

We also report an experiment illustrating that the previous observations also apply to classi-
cation on real data. The variance of K-fold cross-validation (K = 10), decomposed in the three
orthogonal components s 2, w
Experiment 3 Classication with trees on the Letter data set.

is displayed in Figure 6.

and g

The Letter data set comprises 20,000 examples described by 16 numeric features. The original
setup considers 26 categories reprensenting the letters of the roman alphabet. Here, we used a
simplied setup with 2 classes (A to M) vs. (N to Z) in order to obtain sensible results for small
sample sizes.

Accurate estimates of s 2, w

require many independent training samples. This was
achieved by considering the set of 20,000 examples to be the population, from which independent
training samples were drawn by uniform sampling with replacement.

and g

Here again, the variance of CV is mainly due to s 2 and g . According to the number of training
examples, s 2 is only responsible for 50 to 70 % of the total variance, so that a variance estimate
based solely on s 2 has a negative bias of the order of magnitude of the variance itself.

1102

VARIANCE OF K-FOLD CROSS-VALIDATION

Figure 6: Bar plots of the contributions to total variance Var[CV] due to s 2, w

and g vs. the number

of training examples n for Experiment 3.

8. Special Cases

This section addresses how our main result can be transposed to hold-out estimates of generalization
error. We also detail how it applies to two specic instances of the general K-fold cross-validation
scheme: two-fold and leave-one-out cross validation.

8.1 Hold-Out Estimate of EPE

When having K independent training and test sets, the structure of hold-out errors resemble the one
of cross-validation errors, except that we know (from the independence of training and test sets) that

g = 0. This knowledge allows to build the unbiased variance estimate bq 2 described in 2.2. This can

be seen directly in the proof of Theorem 6: knowing that g = 0 removes the third equation in the
linear system (14). In practice, one is often restricted to K = 1 (ordinary hold-out test), which allows
to estimate the variance due to the nite test set but not due to the particular choice of training set.

8.2 Two-Fold Cross-Validation

Two-fold cross-validation has been advocated to perform hypothesis testing (Dietterich, 1999; Al-
paydin, 1999). It is a special case of K-fold cross-validation since the training blocks are mutually
independent since they do not overlap. However, this independence does not modify the structure of
e in the sense that g
is not null. The between-block correlation stems from the fact that the training
block D1 is the test block T2 and vice-versa.

8.3 Leave-One-Out Cross-Validation

Leave-one-out cross-validation is a particular case of K-fold cross-validation, where K = n. The
structure of the covariance matrix is simplied, without diagonal blocks: S = (s 2 g )S 1 +ng
3. The
estimation difculties however remain: even in this particular case, there is no unbiased estimate of
variance. From the denition of b (Lemma 5), we have b = 0, and with m = 1 the linear system (14)
reads



= 1
a
n ,
= n1
c
n ,
a + c = 0,

1103

S
BENGIO AND GRANDVALET

which still admits no solution.

9. Conclusions

It is known that K-fold cross-validation may suffer from high variability, which can be responsible
for bad choices in model selection and erratic behavior in the estimated expected prediction error.

In this paper, we show that estimating the variance of K-fold cross-validation is difcult. Esti-
mating a variance can be done from independent realizations or from dependent realizations whose
correlation is known. K-fold cross-validation produces dependent test errors. Our analysis shows
that although the correlations are structured in a very simple manner, their values cannot be esti-
mated unbiasedly. Consequently, there is no unbiased estimator of the variance of K-fold cross-
validation.

Our experimental section shows that in very simple cases, the bias incurred by ignoring the
dependencies between test errors will be of the order of the variance itself. These experiments
illustrate thus that the assessment of the signicance of observed differences in cross-validation
scores should be treated with much caution. The problem being unveiled, the next step of this study
consists in building and comparing variance estimators dedicated to the very specic structure of
the test error dependencies.

