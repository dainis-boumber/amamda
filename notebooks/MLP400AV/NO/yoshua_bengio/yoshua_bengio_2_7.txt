1 Introduction

Recurrent networks (crossreference Chapter 12) can, in principle, use their
feedback connections to store representations of recent input events in the
form of activations. The most widely used algorithms for learning what to
put in short-term memory, however, take too much time to be feasible or
do not work well at all, especially when minimal time lags between inputs
and corresponding teacher signals are long. Although theoretically fascinat-
ing, they do not provide clear practical advantages over, say, backprop in
feedforward networks with limited time windows (see crossreference Chap-
ters 11 and 12). With conventional algorithms based on the computation of
the complete gradient, such as Back-Propagation Through Time (BPTT,
e.g., [23, 28, 27]) or Real-Time Recurrent Learning (RTRL, e.g., [22]) error
signals owing backwards in time tend to either (1) blow up or (2) vanish:
the temporal evolution of the backpropagated error exponentially depends
on the size of the weights [12, 6]. Case (1) may lead to oscillating weights,
while in case (2) learning to bridge long time lags takes a prohibitive amount
of time, or does not work at all.

In what follows, we give a theoretical analysis of this problem by study-
ing the asymptotic behavior of error gradients as a function of time lags. In
Section 2, we consider the case of standard RNNs and derive the main result
using the approach rst proposed in [12]. In Section 3, we consider the more
general case of adaptive dynamical systems, which include, besides standard
RNNs, other recurrent architectures based on dierent connectivities and
choices of the activation function (e.g., RBF or second order connections).
Using the analysis reported in [6] we show that one of the following two un-
desirable situations necessarily arise: either the system is unable to robustly
store past information about its inputs, or gradients vanish exponentially.
Finally, in Section 4 we shortly review alternative optimization methods and
architectures that have been suggested to improve learning in the presence
of long-term dependencies.

2 Exponential error decay

Gradients of the error function

The results we are going to prove hold regardless of the particular kind of
cost function used (as long as its continuous in the output) and regardless of

2

the particular algorithm which is employed to compute the gradient. Here we
shortly explain how gradients are computed by the standard BPTT algorithm
(e.g., [28], see also crossreference Chapter 14 for more details) because its
analytical form is better suited to the forthcoming analyses.

The error at time t is denoted by E(t). Considering only the error at

time t, output unit ks error signal is

k(t) =

E(t)
netk(t)

and some unit js backpropagated error signal at time  < t is

(cid:2)(cid:3)

(cid:4)

j( ) = f(cid:2)

j(netj( ))

wij i( + 1)

,

where

i

wij aj(  1)

(cid:3)

j

neti( ) =

is unit is current net input,

ai( ) = fi(neti( ))

is the activation of a non-input unit i with dierentiable transfer function fi,
and wij is the weight on the connection from unit j to i. The corresponding
contribution at time  < t to wjls total weight update is  j( ) al(  1),
where  is the learning rate, and l stands for an arbitrary unit connected to
unit j.

Error path integral

Suppose we have a fully connected net whose non-input unit indices range
from 1 to n. Let us focus on local error ow from output unit k to arbitrary
unit v (later we will see that the analysis immediately extends to global error
ow). The error occurring at k at time step t is propagated back in time
for t  s time steps, to an arbitrary unit v at time s < t. This scales the
error by the following factor:
v(netv(t  1)) wkv
f(cid:2)

(cid:5)

(cid:8)

t  s = 1
t  s > 1

.

(1)

v(s)
k(t)

=

f(cid:2)
v(netv(s))

l(s+1)

k(t)

l=1

wlv

(cid:6)(cid:7)n

3

In order to solve the above recurrence, we will expand it by unrolling over
time (as done for example in deriving BPTT). In particular, for s <  < t let
l denote the index of a generic non input unit in the replica of the network
at time  . Moreover, let ls = v and lt = k. We obtain:


 f(cid:2)




f(cid:2)
l (netl ( )) wl l1

ls(netls(s))

n(cid:3)

. . .

n(cid:3)

lt1=1

ls+1=1


wltlt1


 s+1(cid:11)

 =t1

v(s)
k(t)

=

(proof by induction).

It can be immediately shown that if the local error vanishes, then the

(2)

global error vanishes too. To see this compute

(cid:3)

kO

v(s)
k(t)

where O denotes the set of output units.

(cid:14)(cid:14)(cid:14)f(cid:2)

(cid:14)(cid:14)(cid:14) > 1.0

Intuitive explanation of equation (2)

l (netl ( )) wl l1

If
for all  then the largest product increases exponentially with t s 1. That
is, the error blows up, and conicting error signals (with opposite signs)
arriving at unit v can lead to oscillating weights and unstable learning due
to overshooting (for error blow-ups or bifurcations see also [20, 2, 9]). On
the other hand, if
for all  , then the largest product decreases exponentially with t  s  1.
That is, the error vanishes, and nothing can be learned in acceptable time.
(cid:14)(cid:14)(cid:14)f(cid:2)
l is
0.25. If al1 is constant and not equal to zero, then the size of the gradient

If fl is the logistic sigmoid function, then the maximal value of f(cid:2)

(cid:14)(cid:14)(cid:14) takes on maximal values where

(cid:14)(cid:14)(cid:14) < 1.0

l (netl ( )) wl l1

l (netl )wl l1

(cid:14)(cid:14)(cid:14)f(cid:2)

(cid:16)
(cid:14)(cid:14)(cid:14)  , and it is less than 1.0
(cid:14)(cid:14)(cid:14) < 4.0 (e.g., if the absolute maximal weight value wmax is smaller

(cid:15)
(cid:14)(cid:14)(cid:14)wl l1

wl l1 =

netl

al1

coth

1
2

1

the size of the derivative goes to zero for

(cid:14)(cid:14)(cid:14)wl l1

for

,

4

than 4.0). Hence with conventional logistic sigmoid transfer functions, the
error ow tends to vanish as long as the weights have absolute values below
4.0, especially in the beginning of the training phase. In general the use of
larger initial weights does not help though  as seen above, for
the relevant derivative goes to zero faster than the absolute weight can
grow (also, some weights may have to change their signs by crossing zero).
Likewise, increasing the learning rate does not help either  it does not
change the ratio of long-range error ow and short-range error ow. BPTT
is too sensitive to recent distractions. Note that since the summation terms
in equation (2) may have dierent signs, increasing the number of units n
does not necessarily increase error ow.

(cid:14)(cid:14)(cid:14)wl l1

(cid:14)(cid:14)(cid:14)  

Weak upper bound for scaling factor

The following, slightly extended vanishing error analysis also takes n, the
number of units, into account. For t s > 1, formula (2) can be rewritten as


 s+1(cid:11)

 =t2


 Wv f(cid:2)

(cid:8)T

(cid:6)

W T
k

F (cid:2)

(t  1)

W F (cid:2)

( )

v (netv(s)) ,

k ]i := [W ]ki = wki, and F (cid:2)

where the weight matrix W is dened by [W ]ij := wij, vs outgoing weight
vector Wv is dened by [Wv]i := [W ]iv = wiv, ks incoming weight vector W T
k
is dened by [W T
(t) is the diagonal matrix of rst
order derivatives dened as: [F (cid:2)
i(neti(t))
otherwise. Here T is the transposition operator, [A]ij is the element in the
i-th column and j-th row of matrix A, and [x]i is the i-th component of
vector x.
Using a matrix norm (cid:5).(cid:5)A compatible with vector norm (cid:5).(cid:5)x, we dene

(t)]ij := 0 if i (cid:4)= j, and [F (cid:2)

(t)]ij := f(cid:2)

For maxi=1,...,n{|xi|}  (cid:5)x(cid:5)x we get

{(cid:5)F (cid:2)

 =t1,...,s

f(cid:2)
max := max

( )(cid:5)A}.
(cid:14)(cid:14)(cid:14)xT y
(cid:14)(cid:14)(cid:14)  n (cid:5)x(cid:5)x (cid:5)y(cid:5)x. Since
|f(cid:2)
(s)(cid:5)A  f(cid:2)
v(netv(s))|  (cid:5)F (cid:2)

max,

we obtain the following inequality:

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) v(s)

k(t)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)  n (f(cid:2)

max)ts (cid:5)Wv(cid:5)x (cid:5)W T

k (cid:5)x (cid:5)W(cid:5)ts2

A

 n (f(cid:2)

max (cid:5)W(cid:5)A)

ts .

5

This inequality results from

(cid:5)Wv(cid:5)x = (cid:5)W ev(cid:5)x  (cid:5)W(cid:5)A (cid:5)ev(cid:5)x  (cid:5)W(cid:5)A

and

(cid:5)W T

k (cid:5)x = (cid:5)ekW(cid:5)x  (cid:5)W(cid:5)A (cid:5)ek(cid:5)x  (cid:5)W(cid:5)A,

where ek is the unit vector whose components are 0 except for the k-th
component, which is 1. Note that this is a weak, extreme case upper bound
( )(cid:5)A take on maximal values, and if the
 it will be reached only if all (cid:5)F (cid:2)
contributions of all paths across which error ows back from unit k to unit
v have the same sign. Large (cid:5)W(cid:5)A, however, typically result in small values
of (cid:5)F (cid:2)

( )(cid:5)A, as conrmed by experiments (see, e.g., [12]).

For example, with norms

(cid:5)W(cid:5)A := maxr

(cid:3)

s

|wrs|

and

we have f(cid:2)

max = 0.25 for the logistic sigmoid. We observe that if

(cid:5)x(cid:5)x := maxr

|xr| ,

|wij|  wmax < 4.0
n

i, j,

(cid:6)

(cid:8)

then (cid:5)W(cid:5)A  n wmax < 4.0 will result in exponential decay; by setting
 :=

nwmax

4.0

< 1.0, we obtain(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)v(s)

k(t)

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)  n ts.

We refer to Hochreiters thesis [12] for more details.

3 Dilemma: Avoiding gradient decay prevents

long-term latching

In Bengio et al.s paper [6], the analysis of the problem of gradient decays
is generalized to parameterized dynamical systems (hence including second
order and other recurrent architectures). The main theorem shows that a

6

sucient condition to obtain gradient decay is also a necessary condition for
the system to robustly store discrete state information for the long-term. In
other words, when the weights and the state trajectory are such that the
network can latch on information in its hidden units (i.e., represent long-
term dependencies), the problem of gradient decay is obtained. When the
long-term gradients decay exponentially, it is very dicult to learn such long-
term dependencies because the total gradient is the sum of long-term and
short-term inuences and the short-term inuences then completely dominate
the gradient.

This result is based on a decomposition of the state-space of hidden units
in two types of regions: one where gradients decay and one where it is not
possible to robustly latch information. Let y(t) denote the n-dimensional
state vector at time t (for example, the vector [net1(t), . . . , netn(t)] when
considering a standard rst-order recurrent network) and let y(t) = M(y(t
1)) be the map from the state at time t 1 to t for the autonomous (without
inputs) dynamical system. The above decomposition is expressed in terms
of the condition |M(cid:2)| > 1 (no robust latching possible) or |M(cid:2)| < 1 (gradient
decay), where |M(cid:2)| is the norm of the Jacobian (matrix of partial derivatives)
of the map M. The analysis focuses on the basins of attraction of attractors of
M in the domain of y(t) (or manifolds within that domain). In particular, the
analysis is concerned with so-called hyperbolic attractors, which are locally
stable (but need not be xed points) and where the eigenvalues of M(cid:2)
are
less than 1 in absolute value. If the state (or a function of it) remains within
a certain region of space (versus another region) even in the presence of
perturbations (such as noise in the inputs) then it is possible to store at least
one bit of information for arbitrary durations.
In regions where |M(cid:2)| > 1 it can be shown that arbitrarily small pertur-
bations (for example due to the inputs) can eventually kick the state out of
a basin of attraction [19] (see the sample trajectory on the right of Figure
1). In regions where |M(cid:2)| <  < 1 there is a level of perturbation (depend-
ing on ) below which the state will remain in the basin of attraction (and
will gradually get closer to a certain volume around the attractor  see left
of Figure 1). For this reason we call this condition information latching,
since it allows to store discrete information for arbitrary duration in the state
variable y(t).
Unfortunately, in the regions where |M(cid:2)| < 1 (where one can latch in-
formation) one can also show that gradients decay. The argument is similar
to the one developed in the previous section. The partial derivative of y(t)

7

|M(cid:2)| > 1

|M(cid:2)| > 1

y

y(t)
|M(cid:2)| < 1

|M(cid:2)| < 1

y

y(t)

Figure 1: Robust latching. For simplicity a xed-point attractor y is shown.
The shadow region is the basin of attraction. The dark shadow region is the
subset where |M(cid:2)| < 1 and robust latch occurs. See text for details.

with respect to y(s) with s < t is simply the product of the map derivatives
between s and t:

y(t)
y(s)

=

y(t  1)
y(t  2)

y(t)
y(t  1)
y(1) is equal to the W F (cid:2)

. . .

y( )

y(s + 1)

.

y(s)

( ) in previous formulas
(Using neural networks
 compare rst equation of subsection on weak upper bounds.) When the
norm of each of the factors on the right hand side is less than 1, the left hand
side converges exponentially fast to zero as t s increases. The eect of this
decay of gradients can be made explicit as follows:
E(t)
y(t)
Hence for a term of the sum with  (cid:8) t, we have

E(t)
W =

y( )
W =

E(t)
y( )

y(t)
y( )

y( )
W

(cid:3)

t

.

t

(cid:3)
(cid:14)(cid:14)(cid:14)(cid:14)(cid:14)  0.

(cid:14)(cid:14)(cid:14)(cid:14)(cid:14) E(t)

y( )

y( )
W

This term tends to become very small in comparison to terms for which  is
close to t. This means that even though there might exist a change in W that
would allow y( ) to jump to another (better) basin of attraction, the gradient
of the cost with respect to W does not clearly reect that possibility. The
explanation is that the eect of a small change in W would be felt mostly
on the near past ( close to t).

8

4 Remedies

The above theoretical investigations indicate a basic limitation of gradient
descent as a search procedure for nding optimal weights in a RNN. Several
proposals have been made to cope with the problem of long-term dependen-
cies, some attempting to solve the optimization problem using alternative
search algorithms, other trying to devise alternative architectures.
In the
following we give a brief accounts of these proposals.

Time constants

To deal with long time lags, Mozer [18] uses time constants inuencing
changes of unit activations (deVries and Principes related approach [8] may
be viewed as a mixture of time-delay neural networks (TDNN) [15] and time
constants). For long time lags, however, the time constants need external
ne tuning [18]. Sun et al.s alternative approach [26] updates the activa-
tion of a recurrent unit by adding the old activation and the (scaled) current
net input. The net input, however, tends to perturb the stored information,
which makes long-term storage impractical. Lin et al. [17] also propose vari-
ants of time-delay networks, called NARX networks (crossreference see also
Chapter 11). Gradient ow in this architecture can be improved because em-
bedded memories eectively introduce shortcuts in the error propagation
path through time. The same idea can be applied to other architectures, by
inserting multiple delays in the connections among hidden state units rather
than output units [16]. However, these architectures cannot solve the general
problem since they can only increase by a constant multiplicative factor the
duration of the temporal dependencies that can be learned. Finally, El Hihi
& Bengio [10] looked at hierarchically organized recurrent neural networks
with dierent levels of time-constants or time-delays.

Rings approach

Ring [21] also proposes a method for bridging long time lags. Whenever a
unit in his network receives conicting error signals, that is, certain error
signals suggest to increase the units activity while others suggest otherwise,
he adds a higher order unit inuencing appropriate connections. Although
his approach can sometimes be extremely fast, to bridge a time lag involving
100 steps may require the addition of 100 units. Also, Rings net does not

9

generalize to unseen lag durations.

Searching without gradients

The diculty of learning long-term dependencies is strictly related to the
continuous optimization approach that guides the search for a weight solu-
tion. One possibility for avoiding the problem is to resort to other kinds of
search in weight space, in which the operators for generating another candi-
date weight solution are not based on continuous gradients. Bengio et al. [6]
investigate methods such as simulated annealing, multi-grid random search,
and discrete error propagation. Angeline et al.
[1] (see also crossreference
Chapter 15) propose a genetic approach that also avoids gradient computa-
tion.

The simplest kind of search without gradient, however, simply randomly
initializes all network weights until the resulting net happens to classify all
training sequences correctly. In fact, as discussed in crossreference Chapter
9 of this book, simple weight guessing solves several popular benchmarks de-
scribed in previous work faster than the recurrent net algorithms proposed
therein (compare [14]). This does not mean that weight guessing is a good
algorithm. It just means that the problems are very simple. More realis-
tic tasks require either many free parameters (e.g., input weights) or high
weight precision (e.g., for continuous-valued parameters), such that guessing
becomes completely infeasible. Currently it is unclear to which extent more
complex gradient-less methods can improve upon guessing in case of more
realistic tasks.

Probabilistic target propagation

Bengio and Frasconi [4] propose a probabilistic approach for propagating
targets. With n so-called state networks, at a given time, their system can
be in one of only n dierent discrete states. Parameters are adjusted using
the expectation-maximization algorithm. But to solve problems that require
a signicant amount of memory to store contextual information, such systems
would require an unacceptable number of states (i.e., state networks).

Adaptive sequence chunkers

Schmidhubers hierarchical chunker systems [24, 25] can in principle bridge
arbitrary time lags, but only if there is local predictability across the sub-

10

sequences causing the time lags (see also [18]). For instance, in his post-
doctoral thesis [25], Schmidhuber uses hierarchical recurrent networks with
self-organizing time scales to rapidly solve certain grammar learning tasks
involving minimal time lags in excess of 1000 steps. The performance of
chunker systems, however, deteriorates as the noise level increases and the
input sequences become less compressible.

Long Short-Term Memory

There is a novel, ecient, gradient-based method called Long Short-Term
Memory (LSTM) [13]. LSTM is designed to get rid of the vanishing error
problem. Truncating the gradient where this does not do harm, LSTM can
learn to bridge minimal time lags in excess of 1000 discrete time steps by en-
forcing constant error ow through constant error carrousels within special
units. Multiplicative gate units learn to open and close access to the constant
error ow. LSTM is local in space and time; its computational complexity
per time step and weight is O(1). So far, experiments with articial data
involved local, distributed, real-valued, and noisy pattern representations.
In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman
networks, and Neural Sequence Chunking, LSTM led to many more success-
ful runs, and learned much faster. LSTM also solved complex, articial long
time lag tasks that have never been solved by previous recurrent network
algorithms. It will be interesting to examine to which extent LSTM is appli-
cable to real world problems such as language separation from prosody (rst
results in [7]) and speech recognition.

5 Conclusions

In principle, RNNs represent the most general and powerful sequence process-
ing method. For instance, unlike Hidden Markov Models (HMMs, the most
successful technique in several applications - see [3] for a review) they are not
limited to discrete internal states but allow for continuous, distributed se-
quence representations. Hence they can solve tasks no other current method
can solve (e.g., [11]). The problem of vanishing gradients, however, makes
conventional RNNs hard to train. We suspect this is why feedforward neu-
ral networks outnumber RNNs in terms of successful real-world applications.
Some of the remedies outlined in this chapter may lead to more eective

11

learning systems. However, long lime lag research still seems to be in an
early stage  no commercial applications of any of these methods have been
reported so far.

Long time lags pose problems to any soft computing method, not just
RNNs. For instance, when dealing with long sequences (e.g., speech or bio-
logical data), HMMs mostly rely on localized representation of time by means
of highly constrained non ergodic transition diagrams (dierent states are de-
signed for dierent portions of a sequence). Belief propagation over long time
lags does not eectively occur, a phenomenon called diusion of credit [5],
which closely resembles the vanishing gradients problem in RNNs.

