Abstract

We present  a  method for  discovering  informative  patterns  from data.  With
this  method, large  databases can be reduced to  only a  few representative  data  en-
tries.  Our framework encompasses also  methods for  cleaning  databases  containing
corrupted data.  Both on-line  and off-line  algorithms are  proposed and experimen-
tally  checked on databases of  handwritten images. The generality  of  the  framework
makes it  an attractive  candidate for  new applications  in  knowledge discovery.

Keywords- knowledge discovery,  machine learning,
cleaning,  information gain.

informative  patterns,  data

1

INTRODUCTION

Databases  often  contain  redundant data.  It  would be  convenient  if  large  databases  could
be replaced  by only  a  subset  of  informative  patterns.  A difficult,  yet  important problem,
is  to  define  what informative  patterns  are.  We use the  learning  theoretic  definition  [11,
6,  8]:  given  a  model trained  on a  sequence  of  patterns,
is  informative
to  predict  by  a  model trained  on  previously  seen  data.  With that
if  it
definition,  we derive  on-line  and batch  algorithms  for  discovering  informative  patterns.
The techniques  were developed  for  classification
to
regression  and density  estimation  problems.

problems,  but  are  also  applicable

is  difficult

a  new pattern

Informative  patterns  are  often  intermixed  with  other  "bad"  outlyers  which corre-
spond to  errors  introduced  non-intentionally
in  the  database.  For databases  containing
errors,  our  algorithms  can  be  used  to  do computer-aided  data  cleaning,  with  or  without
supervision.  We review several  results  of  experiments in  handwriting recognition  [1,  5,  9]
which demonstrate  the  usefulness  of  our  data  cleaning  techniques.

KDD-94

AAAI-94  Workshop  on  Knowledge  Discovery

in  Databases

Page  145

Pattern
number

"-Pattemi ..
i .:~~.
~

Fraction
of
votes for
label  one  w  ~  level-

~  Surprise
"

(a)

(b)

Figure  1:  Small  example  database  containing  "zeros"  and  "ones".
(a)  A data  entry.
(b)  A sequence  of  data  entries  during  a  training  session  showing the  variation  of  the
surprise  level.  The patterns  which are  most surprising  are  most informative.

2 DISCOVERING

INFORMATIVE  PATTERNS

In  this  section,  we assume that  the  data  is  perfectly  clean.  First,  we give  an intuition
of  what informative  patterns  ought  to  be.  Then,  we show that  this  intuition  coincides
with  the  information  theoretic  definition.  Finally,  we derive  algorithms  to  discover
informative  patterns.

2.1 Informative Patterns are most Surprising

In figure I, we constructed a small example database containing only handwritten zeros
and.ones. Most patterns of a given category look similar. A typical zero is a circle and
a typical one is a vertical bar. However, there exist other shapes of zeros and ones. If
we wanted to keep only a few data representatives,
we would probably keep at least one
example of each basic shape.

To choose the best data representatives, we run an imaginary experiment. Imagine

that we found 100 people who did not know at all what the shape of a zero and that of
a one are. We teach those people to recognize zeros and ones by letting them examine
the patterns of our database in sequence. Every time we show them a new image, we
first hide the label and let them make a guess.

We show in figure 1 the average value of the guesses for a particular sequence of
data. Since we assumed that our subjects had never seen a zero nor a one before the
experiment, about 50% guessed "zero" and 50% guessed "one" when they were shown
the first pattern. But, for the second example of the same shape, the majority made
the correct guess. As learning goes on, familiar shapes are guessed more and more
accurately arid the percentage of wrong guesses raises only occasionally when a new
shape appears.

We represent with the size of a question mark the average amount of surprise that
was generated among our subjects when the true label was uncovered to them. People
who guessed the correct label where not surprised while people who made the wrong
guess  were surprised.  We see  on figure  1  that  a  large  average  surprise

level  coincides

Page 146

AAAI-94 Workshop  on  Knowledge Discovery

in  Databases

KDD-94

with the  apparition  of  a  new shape.  Therefore,  the  level  of  surprise  is  a  good indication
of  how informative  a  pattern  is.

More formally,  the  level  of  surprise  varies  in  the  opposite direction  as  the  probability

of guessing the correct  label  Pk(~k = Yk) = P(~k = yk[z~; (x0,  Y0), (zl,  Yl),  ...(Xk-1,  Yk-1))l.
This  probability  of  making the  correct  guess  is  precisely  what is  involved  in  Shannons
information  gain:

I(k)=--logP~(~k=yk)=--yklogPk(~k=

1)--(1--yk)log(1--Pk(~k=

1))

(1)

where Sk is  an  image, Yk E {0,  1} is  its  associated  label,  k -  1 is  the  number of  data
entries  seen  thus  far  and ~k is  the  label  predicted  for  pattern  Xk.  The log  dependency
ensures additivity  of  information quantities.
In  the  information theoretic  sense,  the  data
entries  that  are  most informative  are  those  that  are  most surprising.

2.2 Machine  Learning  Techniques

to  Estimate

the  Information

Gain

It  is  somewhat unrealistic
Let  us  now replace  people  with  machines.

to  hire  100 ignorant  people to  estimate  the  information  gain.

In  the  Machine Learning  framework,  patterns  drawn from  a  database  are  presented
to  the  learning  machine which makes predictions.  The prediction  error  is  evaluated  and
used  to  improve the  accuracy  of  further  predictions  by adjusting
the  learning  machine
parameters.

Assume first

that  we trained  100 different

its  own value  of  ~k.  This  is  referred

learning  machines (substituting  our  100
people),  each  one  predicting
to  as  a  "Bayesian"
approach  [8].  Formula 1  can  be  readily  used  to  determine  the  information  gain.  Al-
though  this
is  a  perfectly  valid  method,  we propose  here  a  more economical  one:  we
train  a  single  learning  machine to  give  an  estimate  Pk(Yk = 1)  of  the  probability
that
the  correct  label  is  "one" (figure  2).  Our prediction  ~k will  me the  most likely  category
according to  Pk(Yk  -"  1).  In  formula 1,  we substitute

l=~k(Yk  -"  1)  to  Pk(Yk "-  1).

Many Machine Learning  techniques  can  be  applied  to  discover  informative  patterns.

For example, the  learning  machine can  be  a  simple K-nearest-neighbor  classifier
[4].  All
patterns  presented to  the  classifier  are  stored.  Jbk(yk "-  1)  is  given by the  fraction  of  the
K training  patterns  that  are  nearest  to  Xk which have  label  "one".

Another  example is  a  neural  network  trained  with  a  "cross-entropy"  cost  function
for  which the  information  gain  is  the  cost  function  itself.  The mean square  error  cost
function  (Yk  --  Pk(Yk --  1)) 2  provides  an  information  criterion  which ranks  patterns  in
the  same order  as  the  cross-entropy  cost  function  and can  therefore  be  used as  well.

In  the  following,  we use  the  size  of  the  symbol "question  mark", in  the  figures,  and
to  represent  the  information  criteria  used to  measure the

in  the  text,

the  notation  I(k),
"surprise  level".

1To be perfectly correct, the probability should be also conditioned on the model class.

KDD-94

AAAI-94  Workshop  on  Knowledge  Discovery

in  Databases

Page  147

A
P(y~, 1

I(k)

the  information  gain  (or  surprise  level).
Figure  2:  A learning  machine used to  predict
In  the  classical  machine learning  framework, the  goal  is  to  train  the  learning  machine,
either
In  our  framework the  goal
is  to  discover  the  informative  patterns  of  the  database  (dashed  line).

to  provide  a  model of  the  data  or  to  make predictions.

2.3  On-line  Algorithms

and  Batch  Algorithms

In  an  on-line  algorithm,  patterns  are  presented  in  sequence  and  the  learning  machine
adjusts  its  parameters at  each presentation  of  a  new pattern.  This is  a  situation  similar
to  that  of  our  example  of  section  2.1.  In  that  case,  we will  say  that  a  pattern
is
informative  if  the  information  gain  exceeds  a  pre-defined
threshold.  The disadvantage
of  this  method is  that  informative  patterns  depend on the  sequence in  which patterns  are
presented.  However, there  may be  practical  situations  where the  data  is  only  available
on-line.

In  a  batch algorithm,  conversely,  all  data  entries  are  available  at  once and the  infor-
if  there  are  p data
in  the  database,  we need to  train  p  machines,  each  one  on all  the  data  but  one
this  is  feasible  only  if

mation gain  is  independent  on pattern  ordering.  This  implies  that,
entries
pattern,  and then  try  to  predict  that  last  pattern.  In  practice,
training  is  unexpensive,  as  for  the  K-nearest-neighbor  algorithm.

For other  batch  algorithms,  we rather  train  the  learning  machine only once on all  the
data.  We then  approximate  the  information  gain  of  each  pattern  with  an  estimate  of
how much the  cumulative  information  gain  would decrease  if  we removed that  pattern
from the  training  set.

For  batch  algorithms  all  the  patterns  of  the  database  are  uniquely  ranked  according
to  their  information  gain.  The m most informative  patterns  can  be  selected  to  represent
the  entire  database.

2.4  Minimax  Algorithms

Minimax algorithms are  batch algorithms  that  are particularly  well  suited  to  discover
informative patterns.- Most algorithms train the learning machine to  minimize the
average loss  (e.g.  the mean-square-error). Minimax algorithms minimize the maximum
loss:

(2)

Page 148

AAAI-94 Workshop on Knowledge Discovery in Databases

KDD-94

where  w represents
the  parameters  of  the  learning  machine  and  k  runs  over  all  data
entries.  Minimax algorithms  are  extreme  cases  of  some "active  learning"  methods which
emphasize  the  patterns  with  large
information  gain  [8].  The solution  of  a  minimax
algorithm  is  a  function  of  only  a  small  subset  of  the  training  patterns,  precisely  called
"informative  patterns"

[12].  These  are  the  patterns

that  have  maximum loss.

In  reference  [1],  we propose and study  a  minimax algorithm  for  classification

prob-
lems:  the  Optimum Margin  Classifier.
The  algorithm  maximize  the  minimum distance
of  the  training  patterns  to  the  decision  boundary.  It  is  shown that  the  solution  w* is  a
linear  combination of  basis  functions  of  the  informative  patterns:

k--1

ak > 0,

(3)

where yk ~ {0,1}  indicates
are  all  zeros,
except for  the  informative  patterns.  We use the  value of  the  cost  function  at  the  solution
as  cumulative information  criterion:

the  class  membership and the  ak  coefficients

k--1

from  which we derive  and  estimate  of  the  information
informative  pattern  k:

loss  incurred  by  removing the

I(k)

(5)
One important  question  is:  what is  the  rate  of  growth of  the  number of  informative
patterns  with  the  size  of  the  database.  The results  of  experiments  carried  out  on  a
database  of  handwritten  digits  using  the  Optimal Margin Classifier  suggest  a  logarithmic
growth,  in  the  regime  when the  number  of  patterns
is  small  compared  to  the  total
number of  distinct  patterns  (figure  3).

Other minimax algorithms  have been proposed for  classification

and regression  [3,  2].

3 DATA  CLEANING

In this section we tackle the problem of real world databases which may contain cor-
rupted data entries. We propose data cleaning algorithms and analyze experimental
results.

3.1 Garbage  Patterns  are  also  Surprising

The information theoretic definition of infor,,,ative pattern does not always coincide
with the common sense definition. In figure 4, we show examples of patterns drawn
from our  database  of  "zeros"  and  "ones",  which have  a  large  information  gain.  We see
two kinds  of  patterns:

 Patterns  that  are  actually
 Garbage patterns:  Meaningless  or  mislabeled  patterns.

informative:  Atypical  shapes  or  ambiguous shapes.

KDD-94

AAAI-94  Workshop  on  Knowledge  Discovery  in  Databases

Page  149

number of
informative patterns

s

s
1000 2000

I

I

I

I

4000

i

I

|

80~0

|

size of
the database

number of
informative pattams.

,,,,.

4u"
,,,,.,O.,,,,-

sO,~S

S~

I

|

1ooo2ooo4o00 8obo .izeof

I

the database

I

I

1500

I000

5OO

0

1500.

I000

500

0

Figure  3:  Variation  of  the  number of  informative  patterns  as  a  function  of  the  number
in  the  database.  Experiments  were carried  out  on a  database  of  handwrit-
of  patterns
ten  digits,  encoded with  16 real-valued  global  features  derived  from the  pen trajectory
information.  A polynomial  classifier
of  degree  2  was  trained  with  a  minimax algo-
rithm  (Optimal  Margin Classifier).  The informative  patterns  are  the  patterns  for  which
I(zk) = ak #

.
-:,..

:~,
....
I

..

INFORMATIVE


Atypical

~

(a)

-:  Ambiguous

I

(b)

 GARBAGE

Meaningless  ""  "  Mislabeled

Figure  4:  (a)  Informative  patterns  versus  (b)  garbage  patterns.
Informative  patterns
are  intermixed  with  garbage patterns  which also  generate  a  lot  of  surprise  (i.e.  have
large  information  gain).

Page  150

AAAI-94 Workshop  on  Knowledge  Discovery

in  Databases

KDD-94

Learning
machine

 UNCLEAN

DATABASE

Figure  5:  Flow diagram of  on-line  cleaning.  In  the  process  of  learning,  patterns  which
information  gain  exceeds  threshold  0 are  examined by a  human operator.  Good patterns
are  kept  in  the  database  and  sent  to  the  recognizer  for  adaptation.  Bad patterns  are
removed from  the  database.

Truly  informative  patterns  should  be  kept  in  the  database  while  garbage  patterns

should  be  eliminated.

Purely  automatic  cleaning  could  be  performed by eliminating  systematically  all  pat-
terns  with  suspiciously  large  information  gain.  However, this  is  dangerous since  valuable
informative  patterns  may also  be  eliminated.  Purely  manual cleaning,  by  examining all
patterns
in  the  database,  is  tedious  and impractical  for  very  large  databases.  We pro-
pose  a  computer-aided  cleaning  method where  a  human operator  must  check  only  those
patterns

information  gain  and are  therefore  most suspicious.

that  have  largest

3.2  On-line  Algorithms

and  Batch  Algorithms

In  figure  5  we present  our  on-line  version  of  data  cleaning.  It  combines cleaning  and
trained  with  a  few clean
training  in  one single  session.  The learning  machine is  initially
examples.  At  step  k  of  the  cleaning  process,  a  new pattern  xk  is  presented
to  the
learning  machine.  The prediction  of  the  learning  machine and  the  desired  value  yk are
used to  compute the  information criterion
I(k).  If  I(k)  is  below a  given threshold  0,  the
pattern  is  directly  sent  to  the  learning  machine for  adaptation.  Otherwise,  the  pattern
is  sent  to  the  human operator  for  checking.  Depending on the  decision  of  the  operator,
the  pattern  is  either

trashed  or  sent  to  the  learning  machine for  adaptation.

When all

the  data  has  been  processed,  both  training  and  cleaning  are  completed,
since  the  learning  machine was trained  only on clean  data.  This is  an  advantage if  further
use  is  made of  the  learning  machine.  But on-line  cleaning  has  several  disadvantages:

  One has  to  find  find  an  appropriate  threshold  0.

KDD-94

AAAI-94  Workshop  on  Knowledge Discovery

in  Databases

Page 151

Information
gain (surprise level)
MOST

~ RIES

SUSPICIOUS

I~  !  manually remove I

I~|  the bad data entriesI

,

.

~.."

?i/

ii

train the

2)

~,
with  the  ~
information gain.

UNCLEAN
DATABASE

SORTED
DATABASE

Figure  6:  Block  diagram  of  batch  cleaning.  The learning  macl~ne  is  trained  on  unclean
how  much
data.  The  information
is  sorted
information  would  be  lost
sorted  according
are  examined
the  learning  machine  needs  to  be
for  cleaning  by  the  human operator.  After  cleaning,
retrained

is  evaluated
by  estimating
that  pattern  would  be  removed.  The  database

to  obtain  a  good  model  and/or  make good  predictions.

gain.  Only  the  top  ranked  patterns

to  the  information

for  each  pattern

gain
if

 Training

may result

the

learning  machine  may be  slower
in  wasting  the  time  of  the  operator.

than  checking

the  patterns  which

 The method depends  on  the  order  of  presentation  of  the  patterns;

it

is  not  possible

to  revert  a  decision  on  whether  a  pattern

is  good  or  bad.

When possible,

batch  methods  are  preferred

for  cleaning

(figure

6).  Training

including

in  decreasing

is  computed  as  explained

performed  on  all  patterns,
patterns
sorted
operator,
"good"  patterns  exceeds  a  given  threshold.
may be  necessary

to  iterate

starting

order  of  information

garbage  patterns.

The  information

in  sections  2.3  and  2.4.  The  data  entries

gain.  The  patterns

gain  of  the
are  then
are  examined  by  the
the  number of  consecutive
it

errors,

If  the  database  contains  correlated

from  the  top  (most  suspicious),

and  until

this  procedure  several

The  combination  of  batch  cleaning

In  that  case,  only  informative  patterns

tive.
be  examined.  In  figure  7  we  show  the  first
minimax classifier
discriminate  between  the  digit  "two"  and  all
ak  (our  estimate  of  the  information  gain  from  equation  5)  is  a  garbage  pattern.

(the  Optimum Margin  Classifier

and  minimax  algorithms
(with  non-zero
few  informative

times  to  remove  all  "bad"  patterns.
is  particularly
attrac-
information  gain)  need
patterns
[1]).  The  classifier

was  trained

obtained  with  a

the  other  digits.  The patterns  with  largest

3.3  Point

of  Optimal  Cleaning

We may wonder  how reliable
terns
for  cleaning,

that  should  be  candidates

did  we remove  too  many or  too  few  patterns?

these  cleaning

techniques  are:  Did  we examine  all

for  cleaning?  Among the  patterns

the  pat-
that  were  candidates

Page  152

AAAI-94  Workshop  on  Knowledge  Discovery

in  Databases

KDD-94

(a)

(b)

obtained  by  a  minimax algorithm

(the  Optimum

Figure  7:  The  informative  patterns
Margin Algorithm),  for  the  separation  of  handwritten  digit  "two"  against  all  other
[1].  Patterns  are  represented  by  a  16x16 grey-level  pixel-map.  The
digit  categories
informative  patterns  are  shown in  order  of  decreasing  information  gain.(a)
Informative
patterns
for  all
other  classes:  several  ambiguous or  atypical  shapes  are  among the  patterns  with  largest
information  gain.

for  class  2:  a  garbage  pattern  comes first.

(b)  Informative  patterns

Training error (%)

Validation error (%)

11.24
mm

8.1

5.2

-CORRUPl"EO

NO CLEANING

(a)

9.7

8.6
"-1 8.1l

I

OPTIMUM

~CLEANING

v
CLEANINQ

I"
I

NO CLEANING

(b)

Figure  8:  Data cleaning  results  showing a  point  of  optimal cleaning  [9].  A neural  network
was trained  with  a  mean-square-error  cost  function  to  recognize  handwritten  lowercase
letters.  The representation
is  630 local  features  of  the  pen trajectory.  A training  data
set  of  9513 characters  and a  validation  data  set  of  2000 characters  from disjoint  set  of
writers  was used for  cleA~ning.  Starting  with  the  "uncleaned"  database,  several  levels
of  cleaning  were applied  by  the  human operator;  Each stage  was more strict,
i.e.  he
lowered the  tolerance  for  marginal-quality  characters.  To test  the  power of  the  cleaning
technique,  we also  corrupted  the  initial,  uncleaned database,  by artificially  mislabeling
5% of  the  characters  in  the  training  set  (left  most column).

KDD-94

AAAI-94  Workshop  on  Knowledge  Discovery

in  Databases

Page  153

We can  again  use  our  learning  machine to  provide  us  with  an  answer.  In  the  exper-
iments of  figure  8,  we varied  the  amount of  cleaning.  The data  was split
into  a  training
set  and a  validation  set.  The learning  machine was trained  on data  from the  training  set
with  various  amounts of  cleaning.  It  was tested  with  the  unclean  data  of  the  validation
set.  We observe  that,  as  more patterns  are  removed through  the  cleaning  process,
the
error  rate  on the  training  set  decreases.  This  is  understandable  since  the  learning  task
to  the  learning  machine.  The validation  error  however goes
becomes easier  and easier
through  a  minimum. This  is  because  we remove first  only  really  bad  patterns,
then  we
start
removing ~aluable  informative  patterns  that  we mistook for  garbage  patterns.  The
minimum of  the  validation  error  is  the  point  of  optimal cleaning.  If  our  validation  error
does  not  go through  a  minimum, more patterns  should  be  examined and  considered  for
cleaning.

It  is  not  always  possible  nor  desirable

the  database  into  a  training  and a
validation  set.  Another  way of  obtaining  the  point  of  optimum cleaning  is  to  use  the
predictions  of  the  Vapnik-Chervonenkis (VC)theory  [121.  According  to  the  VC-theory,
the  point  of  optimum cleaning  is  the  minimum of  the  so-called  "guaranteed  risk",  which
is  a  bound on the  validation  set  frequency of  errors:

to  split

Bd(In2  --~ ~  + 1)  + m(ln~  + 1)

G

Et,,i,,(m) +

(6)
where/~  is  a  constant  which  was  experimentally  determined  with  the  method de-
is  the  frequency training  errors  when m

scribed  in  reference  [7]  (/~  -  0.5),  Et,.,,,,(m)
suspicious  patterns  have  been removed from the  training  set,  p is  the  number of  train-
ing  patterns  before  cleaning  (p--9513)  and  d is  the  VC-dimension.

p--m

These predictions  can  be  made only  if  the  capacity  (or  VC-dimension) of  the  learning
machine is  known and  if  the  training  algorithm  does  not  learn
the  training  set  with
zero  error.
In  our  experiments  on  data  cleaning  wi~h neural  networks  (figure  9)  [9]
we "obtained  good agreement  between the  prediction  of  the  VC-theory  and  that  of  the
validation  set,  even with  a  very  rough estimate  of  the  VC-dimension. In  fact,  we checked
the  robustness  of  the  VC-prediction  of  optimum cleaning  with  respect
to  a  change  in
the  estimate  of  the  VC-dimension. We found  no significant
change in  the  position  of
the  optimum in  the  range  300 < d  < 5000.

4  CONCLUSIONS

We presented  a  computer-aided  method for  detecting
informative  patterns  and  cleaning
data.  We used  this  method on various  databases  of  handwritten  characters.  The results
the  number of  informative
are  summarized in  table  1.  It  is  important  to  stress
patterns  varies  sub-linearly  with  the  number of  data  entries  and  that
the
cleaning  time  varies  sub-linearly  with  the  num__ber of  data  entries.  The point  of  optimum
cleaning  Can be  predictedby
between a  training  set  and a  validation  set.  Our cleaning  method clearly
quality  of  the  data,  as  measured by the  improvement in  classification
a  test  set  independent  from  the  training
  to  other  problems than  classification  problems (regression  or  density  estimation)  which
makes it  attractive

the  data
improves the
performance  on
set.  Our framework is  general  and  applies

the  VC-theory which does  not  require  splitting

in  knowledge discovery.

for  new applications

therefore

that

Page 154

AAAI-94  Workshop  on  Knowledge  Discovery

in  Databases

KDD-94

2O

~

T

:f (F.~=., p, m, d)

idation

0

5

i0  15  20  25  30  35  40

% removed pattems

(m/p)

A  neural  network  was  trained

Figure  9:  Detection  of  the  point  of  optimal  cleaning  with
prediction.
Training
neural  network  was  estimated
free.  parameters).  G has  been  rescaled

to  d=2000  (approximately
to  fit  on  the  figure.

and  validation

to  minimize

sets  are

the  same  as  in  figure  8.  The  VC-dimension  of  the
of  the  number  of

one  third

the  Vapnik-Chervonenkis
error.

the  maximum squared

Database

