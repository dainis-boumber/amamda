Abstract. DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine
whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array
devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether
cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.
In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene
expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal
patients, we build a classier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address
this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support
Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the
genes selected by our techniques yield better classication performance and are biologically relevant to cancer.

In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better
and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-
one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error).
In the colon cancer database, using only 4 genes our method is 98% accurate, while the baseline method is only
86% accurate.

Keywords: diagnosis, diagnostic tests, drug discovery, RNA expression, genomics, gene selection, DNA micro-
array, proteomics, cancer classication, feature selection, support vector machines, recursive feature elimination

1.

Introduction

The advent of DNA micro-array technology has brought to data analysts broad patterns
of gene expression simultaneously recorded in a single experiment (Fodor, 1997). In the
past few months, several data sets have become publicly available on the Internet. These
data sets present multiple challenges, including a large number of gene expression values
per experiment (several thousands to tens of thousands), and a relatively small number of
experiments (a few dozen).

The data can be analyzed from many different viewpoints. The literature already abounds
in studies of gene clusters discovered by unsupervised learning techniques (see e.g. Eisen,

390

I. GUYON ET AL.

1998; Perou, 1999; Alon, 1999; Alizadeh, 2000). Clustering is often done along the other
dimension of the data. For example, each experiment may correspond to one patient carrying
or not carrying a specic disease (see e.g. Golub, 1999). In this case, clustering usually
groups patients with similar clinical records. Recently, supervised learning has also been
applied, to the classication of proteins (Brown, 2000) and to cancer classication (Golub,
1999).

This last paper on leukemia classication presents a feasibility study of diagnosis based
solely on gene expression monitoring. In the present paper, we go further in this direction
and demonstrate that, by applying state-of-the-art classication algorithms (Support Vector
Machines (Boser, 1992; Vapnik, 1998), a small subset of highly discriminant genes can
be extracted to build very reliable cancer classiers. We make connections with related
approaches that were developed independently, which either combine (Furey, 2000; Pavlidis,
2000) or integrate (Mukherjee, 1999; Chapelle, 2000; Weston, 2000) feature selection with
SVMs.

The identication of discriminant genes is of fundamental and practical interest. Research
in Biology and Medicine may benet from the examination of the top ranking genes to
conrm recent discoveries in cancer research or suggest new avenues to be explored. Medical
diagnostic tests that measure the abundance of a given protein in serum may be derived
from a small subset of discriminant genes.

This application also illustrates new aspects of the applicability of Support Vector Ma-
chines (SVMs) in knowledge discovery and data mining. SVMs were already known as
a tool that discovers informative patterns (Guyon, 1996). The present application demon-
strates that SVMs are also very effective for discovering informative features or attributes
(such as critically important genes). In a comparison with several other gene selection
methods on Colon cancer data (Alon, 1999) we demonstrate that SVMs have both quantita-
tive and qualitative advantages. Our techniques outperform other methods in classication
performance for small gene subsets while selecting genes that have plausible relevance to
cancer diagnosis.

After formally stating the problem and reviewing prior work (Section 2), we present in
Section 3 a new method of gene selection using SVMs. Before turning to the experimental
section (Section 5), we describe the data sets under study and provide the basis of our
experimental method (Section 4). Particular care is given to evaluate the statistical signi-
cance of the results for small sample sizes. In the discussion section (Section 6), we review
computational complexity issues, contrast qualitatively our feature selection method with
others, and propose possible extensions of the algorithm.

2. Problem description and prior work

2.1. Classication problems

In this paper we address classication problems where the input is a vector that we call a
pattern of n components which we call features. We call F the n-dimensional feature
space. In the case of the problem at hand, the features are gene expression coefcients and
patterns correspond to patients. We limit ourselves to two-class classication problems. We

GENE SELECTION FOR CANCER CLASSIFICATION

391

identify the two classes with the symbols (+) and (). A training set of a number of patterns
{x1, x2, . . . xk , . . . x(cid:4)} with known class labels {y1, y2, . . . yk , . . . y(cid:4)}, yk  {1,+1}, is
given. The training patterns are used to build a decision function (or discriminant function)
D(x), that is a scalar function of an input pattern x. New patterns are classied according
to the sign of the decision function:
D(x) > 0  x  class (+)
D(x) < 0  x  class ()
D(x) = 0, decision boundary.

Decision functions that are simple weighted sums of the training patterns plus a bias are
called linear discriminant functions (see e.g. Duda, 1973). In our notations:

D(x) = w  x+ b,

(1)

where w is the weight vector and b is a bias value.

A data set is said to be linearly separable if a linear discriminant function can separate

it without error.

2.2.

Space dimensionality reduction and feature selection

A known problem in classication specically, and machine learning in general, is to
nd ways to reduce the dimensionality n of the feature space F to overcome the risk of
overtting. Data overtting arises when the number n of features is large (in our case
thousands of genes) and the number (cid:4) of training patterns is comparatively small (in our
case a few dozen patients). In such a situation, one can easily nd a decision function that
separates the training data (even a linear decision function) but will perform poorly on test
data. Training techniques that use regularization (see e.g. Vapnik, 1998) avoid overtting of
the data to some extent without requiring space dimensionality reduction. Such is the case,
for instance, of Support Vector Machines (SVMs) (Boser, 1992; Vapnik, 1998; Cristianini,
1999). Yet, as we shall see from experimental results (Section 5), even SVMs benet from
space dimensionality reduction.

Projecting on the rst few principal directions of the data is a method commonly used
to reduce feature space dimensionality (see, e.g. Duda, 73). With such a method, new
features are obtained that are linear combinations of the original features. One disadvantage
of projection methods is that none of the original input features can be discarded. In this
paper we investigate pruning techniques that eliminate some of the original input features
and retain a minimum subset of features that yield best classication performance. Pruning
techniques lend themselves to the applications that we are interested in. To build diagnostic
tests, it is of practical importance to be able to select a small subset of genes. The reasons
include cost effectiveness and ease of verication of the relevance of selected genes.

The problem of feature selection is well known in machine learning. For a review of
feature selection, see e.g. (Kohavi, 1997). Given a particular classication technique, it is
conceivable to select the best subset of features satisfying a given model selection criterion

392

I. GUYON ET AL.

by exhaustive enumeration of all subsets of features. For a review of model selection, see
e.g. (Kearns, 1997). Exhaustive enumeration is impractical for large numbers of features
(in our case thousands of genes) because of the combinatorial explosion of the number of
subsets. In the discussion section (Section 6), we shall go back to this method that can
be used in combination with another method that rst reduces the number of features to a
manageable size.

Performing feature selection in large dimensional input spaces therefore involves greedy
algorithms. Among various possible methods feature-ranking techniques are particularly
attractive. A xed number of top ranked features may be selected for further analysis or
to design a classier. Alternatively, a threshold can be set on the ranking criterion. Only
the features whose criterion exceeds the threshold are retained. In the spirit of Structural
Risk Minimization (see e.g. Vapnik, 1998; Guyon, 1992) it is possible to use the ranking
to dene nested subsets of features F1  F2    F, and select an optimum subset
of features with a model selection criterion by varying a single parameter: the number of
features. In the following, we compare several feature-ranking algorithms.

2.3. Feature ranking with correlation coefcients

In the test problems under study, it is not possible to achieve an errorless separation with a
single gene. Better results are obtained when increasing the number of genes. Classical gene
selection methods select the genes that individually classify best the training data. These
methods include correlation methods and expression ratio methods. They eliminate genes
that are useless for discrimination (noise), but they do not yield compact gene sets because
genes are redundant. Moreover, complementary genes that individually do not separate well
the data are missed.

Evaluating how well an individual feature contributes to the separation (e.g. cancer vs.
normal) can produce a simple feature (gene) ranking. Various correlation coefcients are
used as ranking criteria. The coefcient used in Golub (1999) is dened as:

wi = (i (+)  i ())/(i (+) + i ())

(2)

where i and i are the mean and standard deviation of the gene expression values of
gene i for all the patients of class (+) or class (), i = 1, . . . n. Large positive wi values
indicate strong correlation with class (+) whereas large negative wi values indicate strong
correlation with class (). The original method of Golub (1999) is to select an equal number
of genes with positive and with negative correlation coefcient. Others (Furey, 2000) have
been using the absolute value of wi as ranking criterion. Recently, in Pavlidis (2000), the
authors have been using a related coefcient (i (+) i ())2/(i (+)2 + i ()2), which
is similar to Fishers discriminant criterion (Duda, 1973).

What characterizes feature ranking with correlation methods is the implicit orthogonality
assumptions that are made. Each coefcient wi is computed with information about a single
feature (gene) and does not take into account mutual information between features. In the
next section, we explain in more details what such orthogonality assumptions mean.

GENE SELECTION FOR CANCER CLASSIFICATION

393

2.4. Ranking criterion and classication

One possible use of feature ranking is the design of a class predictor (or classier) based on
a pre-selected subset of features. Each feature that is correlated (or anti-correlated) with the
separation of interest is by itself such a class predictor, albeit an imperfect one. This suggests
a simple method of classication based on weighted voting: the features vote proportionally
to their correlation coefcient. Such is the method being used in Golub (1999). The weighted
voting scheme yields a particular linear discriminant classier:

D(x) = w  (x  )

(3)

where w is dened in Eq. (2) and  = ((+) + ())/2.

It is interesting to relate this classier to Fishers linear discriminant. Such a classier is

also of the form of Eq. (3), with
1((+)  ()),

w = S

where S is the (n, n) within class scatter matrix dened as

(x  (+))(x  (+))T +

(x  ())(x  ())T

S =

(cid:2)
xX (+)

(cid:2)
xX ()

And where  is the mean vector over all training patterns. We denote by X (+) and X () the
training sets of class (+) and (). This particular form of Fishers linear discriminant implies
that S is invertible. This is not the case if the number of features n is larger than the number of
examples (cid:4) since then the rank of S is at most (cid:4). The classier of Golub (1999) and Fishers
classier are particularly similar in this formulation if the scatter matrix is approximated by
its diagonal elements. This approximation is exact when the vectors formed by the values
of one feature across all training patterns are orthogonal, after subtracting the class mean.
It retains some validity if the features are uncorrelated, that is if the expected value of the
product of two different feature is zero, after removing the class mean. Approximating S by
its diagonal elements is one way of regularizing it (making it invertible). But, in practice,
features are usually correlated and therefore the diagonal approximation is not valid.

We have just established that the feature ranking coefcients can be used as classier
weights. Reciprocally, the weights multiplying the inputs of a given classier can be used
as feature ranking coefcients. The inputs that are weighted by the largest value inuence
most the classication decision. Therefore, if the classier performs well, those inputs with
the largest weights correspond to the most informative features. This scheme generalizes
the previous one. In particular, there exist many algorithms to train linear discriminant
functions that may provide a better feature ranking than correlation coefcients. These
algorithms include Fishers linear discriminant, just mentioned, and SVMs that are the
subject of this paper. Both methods are known in statistics as multivariate classiers, which
means that they are optimized during training to handle multiple variables (or features)
simultaneously. The method of Golub (1999), in contrast, is a combination of multiple
univariate classiers.

394

I. GUYON ET AL.

2.5. Feature ranking by sensitivity analysis

In this section, we show that ranking features with the magnitude of the weights of a linear
discriminant classier is a principled method. Several authors have suggested to use the
change in objective function when one feature is removed as a ranking criterion (Kohavi,
1997). For classication problems, the ideal objective function is the expected value of the
error, that is the error rate computed on an innite number of examples. For the purpose of
training, this ideal objective is replaced by a cost function J computed on training examples
only. Such a cost function is usually a bound or an approximation of the ideal objective,
chosen for convenience and efciency reasons. Hence the idea to compute the change in
cost function D J (i ) caused by removing a given feature or, equivalently, by bringing its
weight to zero. The OBD algorithm (LeCun, 1990) approximates D J (i ) by expanding J
in Taylor series to second order. At the optimum of J , the rst order term can be neglected,
yielding:

D J (i ) = (1/2)

 2 J
w2
i

(Dwi )2

(4)

The change in weight Dwi = wi corresponds to removing feature i. The authors of the OBD
algorithm advocate using D J (i ) instead of the magnitude of the weights as a weight pruning
criterion. For linear discriminant functions whose cost function J is a quadratic function of
classier (Duda, 1973) with cost function J = (cid:3)
wi these two criteria are equivalent. This is the case for example of the mean-squared-error
(cid:12)w x y(cid:12)2 and linear SVMs (Boser,
1992; Vapnik, 1998; Cristianini, 1999), which minimize J = (1/2)||w||2, under constrains.
This justies the use of (wi )2 as a feature ranking criterion.

xX

2.6. Recursive Feature Elimination

A good feature ranking criterion is not necessarily a good feature subset ranking criterion.
The criteria D J (i ) or (wi )2 estimate the effect of removing one feature at a time on the
objective function. They become very sub-optimal when it comes to removing several
features at a time, which is necessary to obtain a small feature subset. This problem can
be overcome by using the following iterative procedure that we call Recursive Feature
Elimination:

1. Train the classier (optimize the weights wi with respect to J ).
2. Compute the ranking criterion for all features (D J (i ) or (wi )2).
3. Remove the feature with smallest ranking criterion.

This iterative procedure is an instance of backward feature elimination (Kohavi, 2000 and
