Abstract

We present a novel clustering method using the approach of support vector machines. Data
points are mapped by means of a Gaussian kernel to a high dimensional feature space, where
we search for the minimal enclosing sphere. This sphere, when mapped back to data space,
can separate into several components, each enclosing a separate cluster of points. We
present a simple algorithm for identifying these clusters. The width of the Gaussian kernel
controls the scale at which the data is probed while the soft margin constant helps coping
with outliers and overlapping clusters. The structure of a dataset is explored by varying
the two parameters, maintaining a minimal number of support vectors to assure smooth
cluster boundaries. We demonstrate the performance of our algorithm on several datasets.
Keywords: Clustering, Support Vectors Machines, Gaussian Kernel

1. Introduction

Clustering algorithms group data points according to various criteria , as discussed by
Jain and Dubes (1988), Fukunaga (1990), Duda et al. (2001). Clustering may proceed
according to some parametric model, as in the k-means algorithm of MacQueen (1965),
or by grouping points according to some distance or similarity measure as in hierarchical
clustering algorithms. Other approaches include graph theoretic methods, such as Shamir
and Sharan (2000), physically motivated algorithms, as in Blatt et al. (1997), and algorithms
based on density estimation as in Roberts (1997) and Fukunaga (1990). In this paper we
propose a non-parametric clustering algorithm based on the support vector approach of

c(cid:1)2001 Ben-Hur, Horn, Siegelmann and Vapnik.

Ben-Hur, Horn, Siegelmann and Vapnik

Vapnik (1995).
In Scholkopf et al. (2000, 2001), Tax and Duin (1999) a support vector
algorithm was used to characterize the support of a high dimensional distribution. As a by-
product of the algorithm one can compute a set of contours which enclose the data points.
These contours were interpreted by us as cluster boundaries in Ben-Hur et al. (2000). Here
we discuss in detail a method which allows for a systematic search for clustering solutions
without making assumptions on their number or shape, rst introduced in Ben-Hur et al.
(2001).

In our Support Vector Clustering (SVC) algorithm data points are mapped from data
space to a high dimensional feature space using a Gaussian kernel.
In feature space we
look for the smallest sphere that encloses the image of the data. This sphere is mapped
back to data space, where it forms a set of contours which enclose the data points. These
contours are interpreted as cluster boundaries. Points enclosed by each separate contour
are associated with the same cluster. As the width parameter of the Gaussian kernel
is decreased, the number of disconnected contours in data space increases, leading to an
increasing number of clusters. Since the contours can be interpreted as delineating the
support of the underlying probability distribution, our algorithm can be viewed as one
identifying valleys in this probability distribution.

SVC can deal with outliers by employing a soft margin constant that allows the sphere
in feature space not to enclose all points. For large values of this parameter, we can also deal
with overlapping clusters. In this range our algorithm is similar to the scale space clustering
method of Roberts (1997) that is based on a Parzen window estimate of the probability
density with a Gaussian kernel function.

In the next Section we dene the SVC algorithm. In Section 3 it is applied to problems
with and without outliers. We rst describe a problem without outliers to illustrate the type
of clustering boundaries and clustering solutions that are obtained by varying the scale of the
Gaussian kernel. Then we proceed to discuss problems that necessitate invoking outliers
in order to obtain smooth clustering boundaries. These problems include two standard
benchmark examples.

2. The SVC Algorithm

2.1 Cluster Boundaries

Following Scholkopf et al. (2000) and Tax and Duin (1999) we formulate a support vector
description of a data set, that is used as the basis of our clustering algorithm. Let {xi}  
be a data set of N points, with   IRd, the data space. Using a nonlinear transformation
 from  to some high dimensional feature-space, we look for the smallest enclosing sphere
of radius R. This is described by the constraints:

||(xj)  a||2  R2 j ,

where ||  || is the Euclidean norm and a is the center of the sphere. Soft constraints are
incorporated by adding slack variables j:

||(xj)  a||2  R2 + j

126

(1)

Support Vector Clustering

(cid:1)
(cid:1)

j

j = 1

a =

j(xj)
j = C  j .

j

(2)

(3)

(4)

(5)

(6)
(7)

with j  0. To solve this problem we introduce the Lagrangian

L = R2 

(R2 + j  ||(xj)  a||2)j 

jj + C

(cid:1)

j

(cid:1)

(cid:1)

j ,

(cid:2)

where j  0 and j  0 are Lagrange multipliers, C is a constant, and C
j is a penalty
term. Setting to zero the derivative of L with respect to R, a and j, respectively, leads to

The KKT complementarity conditions of Fletcher (1987) result in

jj = 0,
(R2 + j  ||(xj)  a||2)j = 0.

It follows from Eq. (7) that the image of a point xi with i > 0 and i > 0 lies outside
the feature-space sphere. Eq. (6) states that such a point has i = 0, hence we conclude
from Eq. (5) that i = C. This will be called a bounded support vector or BSV. A point
xi with i = 0 is mapped to the inside or to the surface of the feature space sphere. If its
0 < i < C then Eq. (7) implies that its image (xi) lies on the surface of the feature
space sphere. Such a point will be referred to as a support vector or SV. SVs lie on cluster
boundaries, BSVs lie outside the boundaries, and all other points lie inside them. Note that
when C  1 no BSVs exist because of the constraint (3).

Using these relations we may eliminate the variables R, a and j, turning the Lagrangian

into the Wolfe dual form that is a function of the variables j:

(cid:1)

(cid:1)

W =

(xj)2j 

ij(xi)  (xj).

(8)

j

i,j

Since the variables j dont appear in the Lagrangian they may be replaced with the con-
straints:

(9)
We follow the SV method and represent the dot products (xi)  (xj) by an appropriate
Mercer kernel K(xi, xj). Throughout this paper we use the Gaussian kernel

0  j  C, j = 1, . . . , N.

K(xi, xj) = eq||xixj||2 ,

(10)

with width parameter q. As noted in Tax and Duin (1999), polynomial kernels do not yield
tight contours representations of a cluster. The Lagrangian W is now written as:

ijK(xi, xj).

(11)

(cid:1)

W =

K(xj, xj)j 

(cid:1)

j

i,j

127

Ben-Hur, Horn, Siegelmann and Vapnik

At each point x we dene the distance of its image in feature space from the center of

the sphere:

R2(x) = ||(x)  a||2 .
In view of (4) and the denition of the kernel we have:
(cid:1)

(cid:1)

R2(x) = K(x, x)  2

jK(xj, x) +

ijK(xi, xj) .

j

i,j

The radius of the sphere is:

R = {R(xi) | xi is a support vector } .

The contours that enclose the points in data space are dened by the set

{x | R(x) = R} .

(12)

(13)

(14)

(15)

They are interpreted by us as forming cluster boundaries (see Figures 1 and 3). In view
of equation (14), SVs lie on cluster boundaries, BSVs are outside, and all other points lie
inside the clusters.

2.2 Cluster Assignment

The cluster description algorithm does not dierentiate between points that belong to dier-
ent clusters. To do so, we use a geometric approach involving R(x), based on the following
observation: given a pair of data points that belong to dierent components (clusters), any
path that connects them must exit from the sphere in feature space. Therefore, such a path
contains a segment of points y such that R(y) > R. This leads to the denition of the
adjacency matrix Aij between pairs of points xi and xj whose images lie in or on the sphere
in feature space:

1 if, for all y on the line segment connecting xi and xj, R(y)  R
0 otherwise.

(16)

(cid:3)

Aij =

Clusters are now dened as the connected components of the graph induced by A. Checking
the line segment is implemented by sampling a number of points (20 points were used in
our numerical experiments).

BSVs are unclassied by this procedure since their feature space images lie outside the
enclosing sphere. One may decide either to leave them unclassied, or to assign them to
the cluster that they are closest to, as we will do in the examples studied below.

3. Examples

The shape of the enclosing contours in data space is governed by two parameters: q, the
scale parameter of the Gaussian kernel, and C, the soft margin constant. In the examples
studied in this section we will demonstrate the eects of these two parameters.

128

Support Vector Clustering

1

0.5

0

0.5

1
1

1

0.5

0

0.5

1
1

1

0.5

0

0.5

1
1

1

0.5

0

0.5

1
1

0.5

0

0.5

(a)

0.5

0

0.5

(c)

0.5

0

0.5

(b)

0.5

0

0.5

(d)

Figure 1: Clustering of a data set containing 183 points using SVC with C = 1. Support
vectors are designated by small circles, and cluster assignments are represented
by dierent grey scales of the data points. (a): q = 1 (b): q = 20 (c): q = 24 (d):
q = 48.

3.1 Example without BSVs

We begin with a data set in which the separation into clusters can be achieved without
invoking outliers, i.e. C = 1. Figure 1 demonstrates that as the scale parameter of the
Gaussian kernel, q, is increased, the shape of the boundary in data-space varies: with
increasing q the boundary ts more tightly the data, and at several q values the enclosing
contour splits, forming an increasing number of components (clusters). Figure 1a has the
smoothest cluster boundary, dened by six SVs. With increasing q, the number of support
vectors nsv increases. This is demonstrated in Figure 2 where we plot nsv as a function of
q for the data considered in Figure 1.

3.2 Example with BSVs

In real data, clusters are usually not as well separated as in Figure 1. Thus, in order to
observe splitting of contours, we must allow for BSVs. The number of outliers is controlled

129

Ben-Hur, Horn, Siegelmann and Vapnik

80

70

60

50

40

30

20

10

s
r
o

t
c
e
v

t
r
o
p
p
u
s

0

0

10

20

30

40

50
q

60

70

80

90

100

Figure 2: Number of SVs as a function of q for the data of Figure 1. Contour splitting

points are denoted by vertical lines.

by the parameter C. From the constraints (3,9) it follows that

nbsv < 1/C ,

(17)

where nbsv is the number of BSVs. Thus 1/(N C) is an upper bound on the fraction of
BSVs, and it is more natural to work with the parameter

p =

1
N C

.

(18)

Asymptotically (for large N), the fraction of outliers tends to p , as noted in Scholkopf et al.
(2000).

When distinct clusters are present, but some outliers (e.g. due to noise) prevent contour
separation, it is very useful to employ BSVs. This is demonstrated in Figure 3a: without
BSVs contour separation does not occur for the two outer rings for any value of q. When
some BSVs are present, the clusters are separated easily (Figure 3b). The dierence between
data that are contour-separable without BSVs and data that require use of BSVs is illus-
trated schematically in Figure 4. A small overlap between the two probability distributions
that generate the data is enough to prevent separation if there are no BSVs.

In the spirit of the examples displayed in Figures 1 and 3 we propose to use SVC
iteratively: Starting with a low value of q where there is a single cluster, and increasing
it, to observe the formation of an increasing number of clusters, as the Gaussian kernel
describes the data with increasing precision. If, however, the number of SVs is excessive,

130

Support Vector Clustering

(a)

(b)

4

3

2

1

0

1

2

3

4

4

2

0

2

4

4

3

2

1

0

1

2

3

4

4

2

0

2

4

Figure 3: Clustering with and without BSVs. The inner cluster is composed of 50 points
generated from a Gaussian distribution. The two concentric rings contain 150/300
points, generated from a uniform angular distribution and radial Gaussian dis-
tribution. (a) The rings cannot be distinguished when C = 1. Shown here is
q = 3.5, the lowest q value that leads to separation of the inner cluster.
(b)
Outliers allow easy clustering. The parameters are p = 0.3 and q = 1.0.

0.2

0.15

0.1

0.05

0

0.2

0.15

0.1

0.05

BSVs

BSVs

BSVs

5

0
(a)

5

0

5

0
(b)

5

Figure 4: Clusters with overlapping density functions require the introduction of BSVs.

i.e. a large fraction of the data turns into SVs (Figure 3a), or a number of singleton clusters
form, one should increase p to allow these points to turn into outliers, thus facilitating
contour separation (Figure 3b). As p is increased not only does the number of BSVs
increase, but their inuence on the shape of the cluster contour decreases, as shown in
Ben-Hur et al. (2000). The number of support vectors depends on both q and p. For xed

131

Ben-Hur, Horn, Siegelmann and Vapnik

q, as p is increased, the number of SVs decreases since some of them turn into BSVs and
the contours become smoother (see Figure 3).

4. Strongly Overlapping Clusters

Our algorithm may also be useful in cases where clusters strongly overlap, however a dierent
interpretation of the results is required. We propose to use in such a case a high BSV regime,
and reinterpret the sphere in feature space as representing cluster cores, rather than the
envelope of all data.

Note that equation (15) for the reection of the sphere in data space can be expressed

as

{x |

iK(xi, x) = } ,

(19)

where  is determined by the value of this sum on the support vectors. The set of points
enclosed by the contour is:

{x |

iK(xi, x) > } .

In the extreme case when almost all data points are BSVs (p  1), the sum in this expres-
sion,

(cid:1)

(cid:1)

i

i

(20)

(21)

(22)

is approximately equal to

Pw =

1
N

(cid:1)

Psvc =

i

iK(xi, x)
(cid:1)

K(xi, x) .

i

This last expression is recognized as a Parzen window estimate of the density function (up
to a normalization factor, if the kernel is not appropriately normalized), see Duda et al.
(2001). In this high BSV regime, we expect the contour in data space to enclose a small
number of points which lie near the maximum of the Parzen-estimated density. In other
words, the contour species the core of the probability distribution. This is schematically
represented in Figure 5.

In this regime our algorithm is closely related to the scale-space algorithm proposed
by Roberts (1997). He denes cluster centers as maxima of the Parzen window estimator
Pw(x). The Gaussian kernel plays an important role in his analysis: it is the only kernel
for which the number of maxima (hence the number of clusters) is a monotonically non-
decreasing function of q. This is the counterpart of contour splitting in SVC. As an example
we study the crab data set of Ripley (1996) in Figure 6. We plot the topographic maps of Pw
and Psvc in the high BSV regime. The two maps are very similar. In Figure 6a we present
the SVC clustering assignment. Figure 6b shows the original classication superimposed on
the topographic map of Pw. In the scale space clustering approach it is dicult to identify
the bottom right cluster, since there is only a small region that attracts points to this
local maximum. We propose to rst identify the contours that form cluster cores, the dark
contours in Figure 6a, and then associate points (including BSVs) to clusters according to
their distances from cluster cores.

132

Support Vector Clustering

core

core

BSVs

BSVs

BSVs

4

2

0

2

4

6

0.25

0.2

0.15

0.1

0.05

0
6

Figure 5: In the case of signicant overlap between clusters the algorithm identies clusters
according to dense cores, or maxima of the underlying probability distribution.

(a)

(b)

3
C
P

2

1

0

1

2

2

1

1

2

3

0
PC2

3
C
P

2

1

0

1

2

2

1

1

2

3

0
PC2

Figure 6: Ripleys crab data displayed on a plot of their 2nd and 3rd principal compo-
nents: (a) Topographic map of Psvc(x) and SVC cluster assignments. Cluster
core boundaries are denoted by bold contours; parameters were q = 4.8, p = 0.7.
(b) The Parzen window topographic map Pw(x) for the same q value, and the
data represented by the original classication given by Ripley (1996).

The computational advantage of SVC over Roberts method is that, instead of solving
a problem with many local maxima, we identify core boundaries by an SV method with a
global optimal solution. The conceptual advantage of our method is that we dene a region,
rather than just a peak, as the core of the cluster.

133

Ben-Hur, Horn, Siegelmann and Vapnik

1.5

1

0.5

0

0.5

1

1.5
4

3

2

1

0

1

2

3

4

Figure 7: Cluster boundaries of the iris data set analyzed in a two-dimensional space
spanned by the rst two principal components. Parameters used are q = 6.0 p =
0.6.

4.1 The Iris Data

We ran SVC on the iris data set of Fisher (1936), which is a standard benchmark in the
pattern recognition literature, and can be obtained from Blake and Merz (1998). The data
set contains 150 instances each composed of four measurements of an iris ower. There
are three types of owers, represented by 50 instances each. Clustering of this data in
the space of its rst two principal components is depicted in Figure 7 (data was centered
prior to extraction of principal components). One of the clusters is linearly separable
from the other two by a clear gap in the probability distribution. The remaining two
clusters have signicant overlap, and were separated at q = 6 p = 0.6. However, at these
values of the parameters, the third cluster split into two (see Figure 7). When these two
clusters are considered together, the result is 2 misclassications. Adding the third principal
component we obtained the three clusters at q = 7.0 p = 0.70, with four misclassications.
With the fourth principal component the number of misclassications increased to 14 (using
q = 9.0 p = 0.75). In addition, the number of support vectors increased with increasing
dimensionality (18 in 2 dimensions, 23 in 3 dimensions and 34 in 4 dimensions). The
improved performance in 2 or 3 dimensions can be attributed to the noise reduction eect
of PCA. Our results compare favorably with other non-parametric clustering algorithms:
the information theoretic approach of Tishby and Slonim (2001) leads to 5 misclassications
and the SPC algorithm of Blatt et al. (1997), when applied to the dataset in the original
data-space, has 15 misclassications. For high dimensional datasets, e.g. the Isolet dataset
which has 617 dimensions, the problem was obtaining a support vector description: the
number of support vectors jumped from very few (one cluster) to all data points being
support vectors (every point in a separate cluster). Using PCA to reduce the dimensionality
produced data that clustered well.

134

Support Vector Clustering

4.2 Varying q and p

We propose to use SVC as a divisive clustering algorithm, see Jain and Dubes (1988):
starting from a small value of q and increasing it. The initial value of q may be chosen as

q =

1

maxi,j ||xi  xj||2

.

(23)

At this scale all pairs of points produce a sizeable kernel value, resulting in a single cluster.
At this value no outliers are needed, hence we choose C = 1.

As q is increased we expect to nd bifurcations of clusters. Although this may look
as hierarchical clustering, we have found counterexamples when using BSVs. Thus strict
hierarchy is not guaranteed, unless the algorithm is applied separately to each cluster rather
than to the whole dataset. We do not pursue this choice here, in order to show how the
cluster structure is unraveled as q is increased. Starting out with p = 1/N, or C = 1,
we do not allow for any outliers.
If, as q is being increased, clusters of single or few
points break o, or cluster boundaries become very rough (as in Figure 3a), p should be
increased in order to investigate what happens when BSVs are allowed. In general, a good
criterion seems to be the number of SVs: a low number guarantees smooth boundaries. As
q increases this number increases, as in Figure 2. If the number of SVs is excessive, p should
be increased, whereby many SVs may be turned into BSVs, and smooth cluster (or core)
boundaries emerge, as in Figure 3b. In other words, we propose to systematically increase
q and p along a direction that guarantees a minimal number of SVs. A second criterion for
good clustering solutions is the stability of cluster assignments over some range of the two
parameters.

An important issue in the divisive approach is the decision when to stop dividing the
clusters. Many approaches to this problem exist, such as Milligan and Cooper (1985), Ben-
