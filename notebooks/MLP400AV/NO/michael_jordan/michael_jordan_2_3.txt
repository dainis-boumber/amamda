Abstract

We build up the mathematical connection between the \Expectation-Maximization" (EM) algorithm and

gradient-based approaches for maximum likelihood learning of (cid:12)nite Gaussian mixtures. We show that the
EM step in parameter space is obtained from the gradient via a projection matrix P , and we provide an
explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties
of P and provide new results analyzing the e(cid:11)ect that P has on the likelihood surface. Based on these
mathematical results, we present a comparative discussion of the advantages and disadvantages of EM

and other algorithms for the learning of Gaussian mixture models.

Copyright c(cid:13) Massachusetts Institute of Technology, 		

This report describes research done at the Center for Biological and Computational Learning and the Arti(cid:12)cial Intelligence
Laboratory of the Massachusetts Institute of Technology. Support for the Center is provided in part by a grant from the
National Science Foundation under contract ASC{	. Support for the laboratorys arti(cid:12)cial
intelligence research is
provided in part by the Advanced Research Projects Agency of the Department of Defense under O(cid:14)ce of Naval Research
contract N--A-. The authors were also supported by the HK RGC Earmarked Grant CUHK/	E, by a grant
from the McDonnell-Pew Foundation, by a grant from ATR Human Information Processing Research Laboratories, by a grant
from Siemens Corporation, and by grant N-	-- from the O(cid:14)ce of Naval Research. Michael I. Jordan is an NSF
Presidential Young Investigator.

the component populations in a mixture are poorly sep-

arated, the EM algorithm can be expected to produce

in a very small number of iterations parameter values

such that the mixture density determined by them re-

(cid:13)ects the sample data very well." In the context of the

current literature on learning, in which the predictive

aspect of data modeling is emphasized at the expense of

the traditional Fisherian statisticians concern over the

\true" values of parameters, such rapid convergence in

likelihood is a major desideratum of a learning algorithm

and undercuts the critique of EM as a \slow" algorithm.

In the current paper, we provide a comparative anal-

ysis of EM and other optimization methods. We empha-

size the comparison between EM and other (cid:12)rst-order

methods (gradient ascent, conjugate gradient methods),

because these have tended to be the methods of choice

in the neural network literature. However, we also com-

pare EM to superlinear and second-order methods. We

argue that EM has a number of advantages, including its

naturalness at handling the probabilistic constraints of

mixture problems and its guarantees of convergence. We

also provide new results suggesting that under appropri-

ate conditions EM may in fact approximate a superlin-

ear method; this would explain some of the promising

empirical results that have been obtained (Jordan & Ja-

cobs, 		), and would further temper the critique of EM

o(cid:11)ered by Redner and Walker. The analysis in the cur-

rent paper focuses on unsupervised learning; for related

results in the supervised learning domain see Jordan and

Xu (in press).

The remainder of the paper is organized as follows.

We (cid:12)rst brie(cid:13)y review the EM algorithm for Gaussian

mixtures. The second section establishes a connection

between EM and the gradient of the log likelihood. We

then present a comparative discussion of the advantages

and disadvantages of various optimization algorithms in

the Gaussian mixture setting. We then present empir-

ical results suggesting that EM regularizes the condi-

tion number of the e(cid:11)ective Hessian. The fourth section

presents a theoretical analysis of this empirical (cid:12)nding.

The (cid:12)nal section presents our conclusions.

 The EM algorithm for Gaussian

mixtures

We study the following probabilistic model:

P (xj(cid:2)) =

(cid:11)jP (xjmj; (cid:6)j );

()

K

Xj=





Introduction

The \Expectation-Maximization" (EM) algorithm is a

general technique for maximum likelihood (ML) or max-

imum a posteriori (MAP) estimation. The recent em-

phasis in the neural network literature on probabilistic

models has led to increased interest in EM as a possible

alternative to gradient-based methods for optimization.

EM has been used for variations on the traditional theme

of Gaussian mixture modeling (Ghahramani & Jordan,

		; Nowlan, 		; Xu & Jordan, 		a, b; Tresp, Ah-

mad & Neuneier, 		; Xu, Jordan & Hinton, 		) and

has also been used for novel chain-structured and tree-

structured architectures (Bengio & Frasconi, 		; Jor-

dan & Jacobs, 		). The empirical results reported in

these papers suggest that EM has considerable promise

as an optimization method for such architectures. More-

over, new theoretical results have been obtained that

link EM to other topics in learning theory (Amari, 		;

Jordan & Xu, 		; Neal & Hinton, 		; Xu & Jordan,

		c; Yuille, Stolorz & Utans, 		).

Despite these developments, there are grounds for

caution about the promise of the EM algorithm. One

reason for caution comes from consideration of theoret-

ical convergence rates, which show that EM is a (cid:12)rst

order algorithm.



More precisely, there are two key re-

sults available in the statistical literature on the con-

vergence of EM. First, it has been established that un-

der mild conditions EM is guaranteed to converge to
a local maximum of the log likelihood l (Boyles, 	;
Dempster, Laird & Rubin, 	; Redner & Walker,

(k+)

	; Wu, 	). (Indeed the convergence is monotonic:
l((cid:2)
is the value of the pa-
rameter vector (cid:2) at iteration k.) Second, considering

) (cid:21) l((cid:2)

), where (cid:2)

(k)

(k)

EM as a mapping (cid:2)

(cid:3)

(cid:2)

= M ((cid:2)

(cid:3)

), we have (cid:2)

(k+)

(k)

= M ((cid:2)
(k+)(cid:0) (cid:2)
(cid:3) (cid:25)

) with (cid:12)xed point
@M ((cid:2)(cid:3))

(cid:3)

((cid:2)

(k)(cid:0) (cid:2)

)

@(cid:2)(cid:3)

when (cid:2)

(k+)

is near (cid:2)

(cid:3)

, and thus

k(cid:2)

(k+) (cid:0) (cid:2)

(cid:3)k (cid:20) k

@M ((cid:2)

(cid:3)

)

(cid:3)

@(cid:2)

k (cid:1) k(cid:2)

(k) (cid:0) (cid:2)

(cid:3)k;

with

@M ((cid:2)

(cid:3)

)

(cid:3)

@(cid:2)

k

k = 

almost surely. That is, EM is a (cid:12)rst order algorithm.

The (cid:12)rst-order convergence of EM has been cited in

the statistical literature as a major drawback. Red-

ner and Walker (	), in a widely-cited article, argued

that superlinear (quasi-Newton, method of scoring) and

second-order (Newton) methods should generally be pre-

ferred to EM. They reported empirical results demon-

strating the slow convergence of EM on a Gaussian mix-

ture model problem for which the mixture components

and

were not well separated. These results did not include

tests of competing algorithms, however. Moreover, even

though the convergence toward the \optimal" parameter

values was slow in these experiments, the convergence in

likelihood was rapid.

Indeed, Redner and Walker ac-

knowledge that their results show that \... even when

An iterative algorithm is said to have a local convergence
rate of order q (cid:21)  if k(cid:2)(k+) (cid:0) (cid:2)(cid:3)k=k(cid:2)(k) (cid:0) (cid:2)(cid:3)kq (cid:20) r +
o(k(cid:2)(k) (cid:0) (cid:2)(cid:3)k) for k su(cid:14)ciently large.

 (x(cid:0)mj )T (cid:6)(cid:0)

j

(x(cid:0)mj )

e(cid:0) 

((cid:25))

P (xjmj; (cid:6)j ) =
where (cid:11)j (cid:21)  and PK

d=j(cid:6)jj=
j= (cid:11)j =  and d is the dimension-
ality of the vector x. The parameter vector (cid:2) consists
of the mixing proportions (cid:11)j , the mean vectors mj , and
the covariance matrices (cid:6)j .

Given K and given N independent, identically dis-
 , we obtain the following log

tributed samples fx(t)gN



likelihood:



l((cid:2)) = log

N

Yt=

P (x(t)j(cid:2)) =

N

Xt=

log P (x(t)j(cid:2));

()

which can be optimized via the following iterative algo-

rithm (see, e.g, Dempster, Laird & Rubin, 	):

of the matrix B, and \(cid:10)" denotes the Kronecker prod-
uct. Moreover, given the constraints PK
j =  and
j (cid:21) , P (k)
(cid:11)(k)
A is a positive de(cid:12)nite matrix and the ma-
trices P (k)
(cid:6)j are positive de(cid:12)nite with probability
one for N su(cid:14)ciently large.

mj and P (k)

j= (cid:11)(k)

()

Proof. () We begin by considering the EM update
for the mixing proportions (cid:11)i. From Eqs. () and (),
we have

t= h(k)

j (t)
j (t)[x(t) (cid:0) m(k+)

j

t= h(k)

][x(t) (cid:0) m(k+)

j

@l

@AjA=A(k) =

N

Xt=

T

]

[P (x(t); (cid:18)(k)

 ); (cid:1) (cid:1) (cid:1) ; P (x(t); (cid:18)(k)
i= (cid:11)(k)

i P (x(t); (cid:18)(k)

)

i

K )]

T

:

Premultiplying by P (k)

A , we obtain

PK

t= h(k)

j (t)

N

t= h(k)

j (t)x(t)

(cid:11)(k+)

j

m(k+)

j

(k+)
j

(cid:6)

= PN
= PN
PN
= PN

t= h(k)

j

(t)x(t)

PN

where the posterior probabilities h(k)
lows:

j

are de(cid:12)ned as fol-

(cid:11)(k)
j P (x(t)jm(k)
i= (cid:11)(k)

i P (x(t)jm(k)

j

i

; (cid:6)

(k)
j )

; (cid:6)

(k)
i

)

:

h(k)
j (t) =

PK

 Connection between EM and

gradient ascent

In the following theorem we establish a relationship be-

tween the gradient of the log likelihood and the step in

parameter space taken by the EM algorithm. In par-

ticular we show that the EM step can be obtained by

premultiplying the gradient by a positive de(cid:12)nite ma-

trix. We provide an explicit expression for the matrix.

Theorem  At each iteration of the EM algorithm Eq.
(), we have

A(k+) (cid:0) A(k)
m(k+)
(cid:0) m(k)

j

j

= P (k)

A

= P (k)
mj

(k+)
vec[(cid:6)
j

] (cid:0) vec[(cid:6)

(k)
j

] = P (k)
(cid:6)j

@l

@AjA=A(k)
@l
@mj jmj =m(k)

j

@l

@vec[(cid:6)j ]j(cid:6)j=(cid:6)(k)

j

()

()

()

where

P (k)

A

=

P (k)

mj =

P (k)

(cid:6)j =



N fdiag[(cid:11)(k)

 ; (cid:1) (cid:1) (cid:1) ; (cid:11)(k)

K ] (cid:0) A(k)

(k)
j

(cid:6)

t= h(k)

j (t)



t= h(k)

j (t)

PN
PN

(cid:6)

(k)

j (cid:10) (cid:6)

(k)
j

(A(k)

)

Tg ()

()

(	)

where A denotes the vector of mixing proportions
T , j indexes the mixture components (j =
[(cid:11); (cid:1) (cid:1) (cid:1) ; (cid:11)K ]
; (cid:1) (cid:1) (cid:1) ; K), k denotes the iteration number, \vec[B]" de-
notes the vector obtained by stacking the column vectors

Although we focus on maximum likelihood (ML) estima-
tion in this paper, it is straightforward to apply our results
to maximum a posteriori (MAP) estimation by multiplying
the likelihood by a prior.



P (k)

A

=

=

@l

N


N

@AjA=A(k)
Xt=
Xt=


N

N

 P (x(t); (cid:18)(k)

f[(cid:11)(k)

i= (cid:11)(k)

i P (x(t); (cid:18)(k)

i

)g

i P (x(t); (cid:18)(k)

i= (cid:11)(k)

T (cid:0) A(k)PK
 ); (cid:1) (cid:1) (cid:1)]
PK
T (cid:0) A(k):

i

)

[h(k)
 (t); (cid:1) (cid:1) (cid:1) ; h(k)

K (t)]

The update formula for A in Eq. () can be rewritten as
T (cid:0) A(k):
A(k+)

[h(k)
 (t); (cid:1) (cid:1) (cid:1) ; h(k)

= A(k)

K (t)]


N

+

N

Xt=

Combining the last two equations establishes the update

diag[(cid:11)(k)
. By Jensens inequality we have

rule for A (Eq. ). Furthermore, for an arbitrary vec-
tor u, we have N uT P (k)
K ]u (cid:0)
(uTA(k)
uT

 ; (cid:1) (cid:1) (cid:1) ; (cid:11)(k)

A u = uT

diag[(cid:11)(k)

K

)



(cid:11)(k)
j u

j

K ]u =

 ; (cid:1) (cid:1) (cid:1) ; (cid:11)(k)

Xj=
Xj=

K

> (

(cid:11)(k)

j uj )



= (uTA(k)

:

)

Thus, uT P (k)

A u >  and P (k)

A is positive de(cid:12)nite given

() We now consider the EM update for the means

j= (cid:11)(k)

j =  and (cid:11)(k)

j (cid:21)  for all j.

the constraints PK

mi. It follows from Eqs. () and () that

@l
@mj jmj =m(k)

j

=

h(k)
j

(t)((cid:6)

(cid:0)

(k)
j

)

[x(t) (cid:0) m(k)

j

]:

N

Xt=

Premultiplying by P (k)

mj yields

t= h(k)

j



=

P (k)
mj

@l
@mj jmj =m(k)

j

PN
From Eq. (), we have PN

j

= m(k+)

N

(t)

Xt=
(cid:0) m(k)

:

j

is positive de(cid:12)nite with probability one assuming that N

t= h(k)

j

(k)
(t) > ; moreover, (cid:6)
j

h(k)
j (t)x(t) (cid:0) m(k)

j

is large enough such that the matrix is of full rank. Thus,
it follows from Eq. () that P (k)
probability one.

mj is positive de(cid:12)nite with

() Finally, we prove the third part of the theorem.

It follows from Eqs. () and () that

Using the notation
(cid:2) = [mT

 ; (cid:1) (cid:1) (cid:1) ; mT

K ; vec[(cid:6)]

T ; (cid:1) (cid:1) (cid:1) ; vec[(cid:6)K ]

T ; AT

T ;

]

and P ((cid:2)) = diag[Pm ; (cid:1) (cid:1) (cid:1) ; PmK ; P(cid:6) , (cid:1) (cid:1) (cid:1) ; P(cid:6)K ; PA], we

can combine the three updates in Theorem  into a single

N

equation:

(k)
h(k)
j (t)((cid:6)
j

(cid:0)

)

(k+)

(cid:2)

= (cid:2)

(k)

+ P ((cid:2)

(k)

)

@l
@(cid:2)j(cid:2)=(cid:2)(k) ;

()

@l
@(cid:6)j j(cid:6)j =(cid:6)(k)

j





= (cid:0)

Xt=
j (cid:0) [x(t) (cid:0) m(k)

(k)

j

f(cid:6)

][x(t) (cid:0) m(k)

j

)]

Tg((cid:6)

(k)
j

(cid:0):

)

With this in mind, we rewrite the EM update formula

for (cid:6)

(k)
j

as

Under the conditions of Theorem , P ((cid:2)
de(cid:12)nite matrix with probability one. Recalling that for
a positive de(cid:12)nite matrix B, we have
@(cid:2) > , we
have the following corollary:

) is a positive

B @l

@l
@(cid:2)

T

(k)

(k+)
j

(cid:6)

= (cid:6)

(k)
j +



PN
[x(t) (cid:0) m(k)

t= h(k)
(t)
T (cid:0) (cid:6)

j

j

]

(k)
j

N

Xt=

h(k)
j (t)[x(t) (cid:0) m(k)

j

]

Corollary  For each iteration of the EM algorithm
(k) has
given by Eq.(), the search direction (cid:2)
a positive projection on the gradient of l.

(k+) (cid:0) (cid:2)

That is, the EM algorithm can be viewed as a variable

vec[U ]

T

((cid:6)

= tr((cid:6)

(k)
j )vec[U ]

(k)

j U T

)

(k)

j (cid:10) (cid:6)
(k)
j U (cid:6)
(k)
T
j U )
(k)
T
j U ]

(k)
j U ))
(k)
j U ]

= vec[(cid:6)

vec[(cid:6)

= tr(((cid:6)

((cid:6)

(cid:21) ;

where equality holds only when (cid:6)

(k)
j U =  for all U .

Equality is impossible, however, since (cid:6)
is positive
de(cid:12)nite with probability one N is su(cid:14)ciently large. Thus
j (t) >  that P (k)

t= h(k)

it follows from Eq. (	) and PN

is positive de(cid:12)nite with probability one.

(cid:6)j

(k)
j





= (cid:6)

(k)
j +

V(cid:6)j (cid:6)

(k)
j

;

(t)

(k)
(cid:6)
j

t= h(k)

j

PN

where

V(cid:6)j = (cid:0)

(cid:0)

N





)

(k)
j

h(k)
j (t)((cid:6)

Xt=
j (cid:0) [x(t) (cid:0) m(k)

(k)

j

f(cid:6)
@l
@(cid:6)j j(cid:6)j=(cid:6)(k)

j

=

That is, we have

(k+)
j

(cid:6)

= (cid:6)

(k)
j +

:

(cid:6)

(k)
j

t= h(k)

j

PN

][x(t) (cid:0) m(k)

j

]

Tg((cid:6)

(k)
j

)

(cid:0)

@l
@(cid:6)j j(cid:6)j=(cid:6)(k)

j

(k)
j

:

(cid:6)

(t)

Utilizing the identity vec[ABC ] = (C T (cid:10) A)vec[B], we

obtain

vec[(cid:6)

(k+)
j

] = vec[(cid:6)

(k)
j

]

+



t= h(k)

j (t)

((cid:6)

(k)

j (cid:10) (cid:6)

(k)
j

@l
@(cid:6)j j(cid:6)j =(cid:6)(k)

j

:

)

Thus P (k)

(cid:6)j =


h(k)

j

((cid:6)

(t)

(k)

j (cid:10) (cid:6)

(k)
j ). Moreover, for an

arbitrary matrix U , we have

PN
PN

t=

metric gradient ascent algorithm for which the projection
matrix P ((cid:2)

) changes at each iteration as a function

(k)

of the current parameter value (cid:2)

(k)

.

Our results extend earlier results due to Baum and

Sell (	). Baum and Sell studied recursive equations

of the following form:

x(k+)
T (x(k)

= T (x(k)
)
) = [T (x(k)
x(k)
i @J=@x(k)
i= x(k)

i

i @J=@x(k)

i

); (cid:1) (cid:1) (cid:1) ; T (x(k)

)K ]

T (x(k)

))i =

PK

i

i= x(k)

having positive coe(cid:14)cients. They showed

i = , where J is a polyno-

i (cid:21) ; PK
) (cid:0) x(k)

where x(k)
mial in x(k)
that the search direction of this recursive formula, i.e.,
T (x(k)
of of J with respect to the x(k)
biner & Sondhi, 	). It can be shown that Baum and

, has a positive projection on the gradient

(see also Levinson, Ra-

Sells recursive formula implies the EM update formula

for A in a Gaussian mixture. Thus, the (cid:12)rst statement

in Theorem  is a special case of Baum and Sells earlier

work. However, Baum and Sells theorem is an existence

theorem and does not provide an explicit expression for
the matrix PA that transforms the gradient direction
into the EM direction. Our theorem provides such an
explicit form for PA. Moreover, we generalize Baum and
Sells results to handle the updates for mj and (cid:6)j , and
we provide explicit expressions for the positive de(cid:12)nite
transformation matrices Pmj and P(cid:6)j as well.

It is also worthwhile to compare the EM algorithm to
other gradient-based optimization methods. Newtons
method is obtained by premultiplying the gradient by
the inverse of the Hessian of the log likelihood:

(k+)

(cid:2)

= (cid:2)

(k)

+ H ((cid:2)

(k)

)

(cid:0) @l

@(cid:2)(k)

:

()

Newtons method is the method of choice when it can

be applied, but the algorithm is often di(cid:14)cult to use

in practice.

In particular, the algorithm can diverge

when the Hessian becomes nearly singular; moreover,

the computational costs of computing the inverse Hes-

sian at each step can be considerable. An alternative

is to approximate the inverse by a recursively updated
matrix B(k+)
. Such a modi(cid:12)cation
is called a quasi-Newton method. Conventional quasi-
Newton methods are unconstrained optimization meth-

+ (cid:17)(cid:1)B(k)

= B(k)

on unconstrained convergence rates problematic. More-

over, it is not easy to meet the constraints on the covari-

ance matrices in the mixture using such techniques.

A second appealing property of P ((cid:2)

(k)

) is that each

ods, however, and must be modi(cid:12)ed in order to be used

in the mixture setting (where there are probabilistic con-

straints on the parameters). In addition, quasi-Newton

iteration of EM is guaranteed to increase the likelihood
(i.e., l((cid:2)
of the likelihood is achieved without step-size parameters

)). This monotonic convergence

) (cid:21) l((cid:2)

(k+)

(k)

methods generally require that a one-dimensional search

or line searches. Other gradient-based optimization tech-

be performed at each iteration in order to guarantee con-

niques, including gradient descent, quasi-Newton, and

vergence. The EM algorithm can be viewed as a special

Newtons method, do not provide such a simple theo-

form of quasi-Newton method in which the projection
matrix P ((cid:2)
we discuss in the remainder of the paper, this partic-

) in Eq. () plays the role of B(k)

. As

(k)

ular matrix has a number of favorable properties that

make EM particularly attractive for optimization in the

mixture setting.

 Constrained optimization and general

convergence

An important property of the matrix P is that the EM
step in parameter space automatically satis(cid:12)es the prob-

abilistic constraints of the mixture model in Eq.

().

The domain of (cid:2) contains two regions that embody the

probabilistic constraints: D = f(cid:2) :PK
j = g and
D = f(cid:2) : (cid:11)(k)
j (cid:21) , (cid:6)j positive de(cid:12)niteg. For the EM
algorithm the update for the mixing proportions (cid:11)j can
be rewritten as follows:

j= (cid:11)(k)

(cid:11)(k+)

j

=


N

N

Xt=

j P (x(t)jm(k)
(cid:11)(k)
i= (cid:11)(k)

i P (x(t)jm(k)

j

i

; (cid:6)

(k)
j

)

(k)
; (cid:6)
i

)

:

PK

It is obvious that the iteration stays within D. Simi-

larly, the update for (cid:6)j can be rewritten as:

(k+)
j

(cid:6)

=

N



t= h(k)
PN
j (t)
[x(t) (cid:0) m(k)

j

Xt=
PK
][x(t) (cid:0) m(k)

j

T

]

(cid:11)(k)
j P (x(t)jm(k)
i= (cid:11)(k)

i P (x(t)jm(k)

j

i

(k)
; (cid:6)
j

)

; (cid:6)

(k)
i

)

which stays within D for N su(cid:14)ciently large.

Whereas EM automatically satis(cid:12)es the probabilistic

constraints of a mixture model, other optimization tech-

niques generally require modi(cid:12)cation to satisfy the con-

straints. One approach is to modify each iterative step

to keep the parameters within the constrained domain.

A number of such techniques have been developed, in-

cluding feasible direction methods, active sets, gradient

projection, reduced-gradient, and linearly-constrained

quasi-Newton. These constrained methods all incur ex-

tra computational costs to check and maintain the con-

straints and, moreover, the theoretical convergence rates

for such constrained algorithms need not be the same as

retical guarantee, even assuming that the constrained

(k(cid:0))

(k+) (cid:0) (cid:2)

problem has been transformed into an unconstrained
one. For gradient ascent, the step size (cid:17) must be chosen
to ensure that k(cid:2)
)k (cid:20)
))k < . This requires a one-dimensional
kI + (cid:17)H ((cid:2)
line search or an optimization of (cid:17) at each iteration,
which requires extra computation which can slow down
the convergence. An alternative is to (cid:12)x (cid:17) to a very
small value which generally makes kI + (cid:17)H ((cid:2)
))k

(k(cid:0))k=k((cid:2)

close to one and results in slow convergence. For New-

(k) (cid:0) (cid:2)

(k(cid:0))

(k(cid:0))

tons method, the iterative process is usually required

to be near a solution, otherwise the Hessian may be in-

de(cid:12)nite and the iteration may not converge. Levenberg-

Marquardt methods handle the inde(cid:12)nite Hessian ma-

trix problem; however, a one-dimensional optimization

or other form of search is required for a suitable scalar

to be added to the diagonal elements of Hessian. Fisher

scoring methods can also handle the inde(cid:12)nite Hessian

matrix problem, but for non-quadratic nonlinear opti-
mization Fisher scoring requires a stepsize (cid:17) that obeys
))k < , where B is the Fisher infor-
kI + (cid:17)BH ((cid:2)

mation matrix. Thus, problems similar to those of gra-

(k(cid:0))

dient ascent arise here as well. Finally, for the quasi-

Newton methods or conjugate gradient methods, a one-

dimensional line search is required at each iteration. In

summary, all of these gradient-based methods incur ex-

tra computational costs at each iteration, rendering sim-

ple comparisons based on local convergence rates unre-

liable.

For large scale problems, algorithms that change the

parameters immediately after each data point (\on-line

algorithms") are often signi(cid:12)cantly faster in practice

than batch algorithms. The popularity of gradient de-

scent algorithms for neural networks is in part to the

ease of obtaining on-line variants of gradient descent.

It is worth noting that on-line variants of the EM algo-

rithm can be derived (Neal & Hinton, 		, Titterington,

	), and this is a further factor that weighs in favor

of EM as compared to conjugate gradient and Newton

methods.

 Convergence rate comparisons

In this section, we provide a comparative theoretical dis-

cussion of the convergence rates of constrained gradient

that for the corresponding unconstrained algorithms. A

ascent and EM.

second approach is to transform the constrained opti-

mization problem into an unconstrained problem before

using the unconstrained method. This can be accom-

plished via penalty and barrier functions, Lagrangian

terms, or re-parameterization. Once again, the extra al-

gorithmic machinery renders simple comparisons based



For gradient ascent a local convergence result can by

obtained by Taylor expanding the log likelihood around

the maximum likelihood estimate (cid:2)
large k we have:
(k+) (cid:0) (cid:2)

(cid:3)k (cid:20) kI + (cid:17)H ((cid:2)

k(cid:2)

))kk((cid:2)

(cid:3)

(cid:3)

. For su(cid:14)ciently

(k) (cid:0) (cid:2)

(cid:3)

)k

()

and

Since H ((cid:2)

(cid:3)

) is negative de(cid:12)nite, we obtain

(cid:3)

kI + (cid:17)H ((cid:2)

)k (cid:20) (cid:21)M [I + (cid:17)H ((cid:2)

()
where H is the Hessian of l, (cid:17) is the step size, and
)]j;
r = maxfj (cid:0) (cid:17)(cid:21)M [(cid:0)H ((cid:2)
)]jg,
where (cid:21)M [A] and (cid:21)m [A] denote the largest and small-
est eigenvalues of A, respectively.

j (cid:0) (cid:17)(cid:21)m [(cid:0)H ((cid:2)

)] = r;

(cid:3)

(cid:3)

(cid:3)

Smaller values of r correspond to faster convergence
rates. To guarantee convergence, we require r <  or
 < (cid:17) < =(cid:21)M [(cid:0)H ((cid:2)
)]. The minimum possible value
of r is obtained when (cid:17) = =(cid:21)M [H ((cid:2)

)] with

(cid:3)

(cid:3)

rmin =  (cid:0) (cid:21)m[H ((cid:2)
[H ((cid:2)

(cid:17)  (cid:0) (cid:20)(cid:0)

(cid:3)

)];

(cid:3)

)]=(cid:21)M [H ((cid:2)

(cid:3)

)]

where (cid:20)[H ] = (cid:21)M [H ]=(cid:21)m[H ] is the condition number of
H . Larger values of the condition number correspond to
slower convergence. When (cid:20)[H ] =  we have rmin = ,
which corresponds to a superlinear rate of convergence.

Indeed, Newtons method can be viewed as a method

for obtaining a more desirable condition number|the
inverse Hessian H (cid:0)
balances the Hessian H such that
the resulting condition number is one. E(cid:11)ectively, New-

rc (cid:20)q + (cid:17)(cid:21)

M [(cid:0)Hc] (cid:0) (cid:17)(cid:21)m [(cid:0)Hc]:

()

We see from this derivation that the convergence

In this equation Hc = ET H ((cid:2))E is the Hessian of l
restricted to D.
speed depends on (cid:20)[Hc] = (cid:21)M [(cid:0)Hc]=(cid:21)m[(cid:0)Hc]. When
(cid:20)[Hc] = , we have p + (cid:17)(cid:21)
M ((cid:0)Hc) (cid:0) (cid:17)(cid:21)m[(cid:0)Hc] =
 (cid:0) (cid:17)(cid:21)[(cid:0)Hc], which in principle can be made to equal
zero if (cid:17) is selected appropriately. In this case, a super-
linear rate is obtained. Generally, however, (cid:20)[Hc] = ,
with smaller values of (cid:20)[Hc] corresponding to faster con-
vergence.

We now turn to an analysis of the EM algorithm. As

we have seen EM keeps the parameter vector within D

automatically. Thus, in the new basis the connection

between EM and gradient ascent (cf. Eq. ()) becomes

(k+)
c

(cid:2)

= (cid:2)

(k)
c + ET P ((cid:2)

(k)

)

@l
@(cid:2)

ton can be regarded as gradient ascent on a new func-

and we have

tion with an e(cid:11)ective Hessian that is the identity matrix:
Hef f = H (cid:0)H = I . In practice, however, (cid:20)[H ] is usually
quite large. The larger (cid:20)[H ] is, the more di(cid:14)cult it is to
compute H (cid:0)
accurately. Hence it is di(cid:14)cult to balance
the Hessian as desired.

In addition, as we mentioned

in the previous section, the Hessian varies from point

to point in the parameter space, and at each iteration

we need recompute the inverse Hessian. Quasi-Newton
methods approximate H ((cid:2)
B(k)

that is easy to compute.

by a positive matrix

(k)

(cid:0)

)

The discussion thus far has treated unconstrained op-

timization.

In order to compare gradient ascent with

the EM algorithm on the constrained mixture estima-

tion problem, we consider a gradient projection method:

(k+)

(cid:2)

(k)

= (cid:2)

+ (cid:17)(cid:5)k

@l

@(cid:2)(k)

()

@l

@(cid:2)(k)

where (cid:5)k is the projection matrix that projects the gra-
dient

into D. This gradient projection iteration
will remain in D as long as the initial parameter vector
is in D. To keep the iteration within D, we choose an
()  D and keep (cid:17) su(cid:14)ciently small at each
Suppose that E = [e; (cid:1) (cid:1) (cid:1) ; em ] are a set of indepen-
dent unit basis vectors that span the space D. In this

iteration.

initial (cid:2)

@l

basis, (cid:2)

(k)

and (cid:5)k

@(cid:2)(k) become (cid:2)

(k)
c = ET
(k)

(k)

(cid:2)

and

@l

@(cid:2)(k) c

k(cid:2)

(k) (cid:0) (cid:2)

= ET @l

@(cid:2)(k) , respectively, with k(cid:2)

ck =
(cid:3)k. In this representation the projective gradi-

c (cid:0) (cid:2)

(cid:3)

ent algorithm Eq. () becomes simple gradient ascent:

a result, the convergence rate is bounded by

(I + (cid:17)H ((cid:2)

(cid:3)

))kk((cid:2)

(k) (cid:0) (cid:2)

(cid:3)

)k. As

(I + (cid:17)H ((cid:2)

(cid:3)

))k
(I + (cid:17)H ((cid:2)

(cid:3)

))(I + (cid:17)H ((cid:2)

(cid:3)

))

T E]

(I + (cid:17)H ((cid:2)

(cid:3)

) + (cid:17)H ((cid:2)

(cid:3)

))E]:

(k)
c + (cid:17) @l
@(cid:2)(k)
(cid:3)k (cid:20) kET

c

(k+)
c

(cid:2)

= (cid:2)

k(cid:2)

(k+) (cid:0) (cid:2)
rc = kET

(cid:20) q(cid:21)M [ET
= q(cid:21)M [ET

k(cid:2)

(k+) (cid:0) (cid:2)

(cid:3)k (cid:20) kET

(I + P H ((cid:2)

(cid:3)

))kk((cid:2)

(cid:3)

(k) (cid:0) (cid:2)

)k

with

(I + P H ((cid:2)

(cid:3)

))k
(I + P H ((cid:2)

(cid:3)

))(I + P H ((cid:2)

(cid:3)

T E]:

))

rc = kET

(cid:20) q(cid:21)M [ET
rc (cid:20)q + (cid:21)

The latter equation can be further manipulated to yield:

M [ET P HE] (cid:0) (cid:21)m[(cid:0)ET P HE]:

()

Thus we see that the convergence speed of EM de-
pends on (cid:20)[ET P HE] = (cid:21)M [ET P HE]=(cid:21)m[ET P HE].
When (cid:20)[ET P HE] = , (cid:21)M [ET P HE] = , we
M [ET P HE] (cid:0) (cid:21)m [(cid:0)ET P HE] = ( (cid:0)

have p + (cid:21)
(cid:21)M [(cid:0)ET P HE]) = .

In this case, a superlinear rate

is obtained. We discuss the possibility of obtaining su-

perlinear convergence with EM in more detail below.

These results show that the convergence of gradient

ascent and EM both depend on the shape of the log likeli-
hood as measured by the condition number. When (cid:20)[H ]
is near one, the con(cid:12)guration is quite regular, and the

update direction points directly to the solution yielding
fast convergence. When (cid:20)[H ] is very large, the l sur-
face has an elongated shape, and the search along the

update direction is a zigzag path, making convergence

very slow. The key idea of Newton and quasi-Newton

methods is to reshape the surface. The nearer it is to a

ball shape (Newtons method achieves this shape in the

methods aim to achieve an e(cid:11)ective Hessian whose con-

dition number is as close as possible to one.

Interest-

ingly, the results that we now present suggest that the
projection matrix P for the EM algorithm also serves
to e(cid:11)ectively reshape the likelihood yielding an e(cid:11)ective

condition number that tends to one. We (cid:12)rst present

empirical results that support this suggestion and then

present a theoretical analysis.



. Moreover, Eq. () becomes

ideal case), the better the convergence. Quasi-Newton

1000

0



n
g
s


i

h

t
i

w


r
e
b
m
u
n
n
o



i
t
i

d
n
o
c


e
h

t

-1000

-2000

-3000

-4000

solid -  the original Hessian
dash-dot -. the constrained Hessian
dashed -- the EM-equivalent Hessian

-5000
0

10

20

30

50

40
60
the learning steps

70

80

90

100

103


r
e
b
m
u
n
n
o



i
t
i

d
n
o
c

e
h

t

102

101

100
20

solid -  the original Hessian

dash-dot -. the constrained Hessian

dashed -- the EM-equivalent Hessian

1000

800

600

400

200

0

-200

-400

-600

-800



i

n
g
s

h

t
i

w


r
e
b
m
u
n
n
o



i
t
i

d
n
o
c

e
h

t

the original Hessian

the constrained Hessian

the EM-equivalent Hessian

30

40

50

60

the learning steps

70

80

90

100

(a)

-1000
0

50

100

150

250

200
300
the learning steps

350

400

450

500

103


r
e
b
m
u
n



n
o

i
t
i

d
n
o
c


e
h

t

102

101
0

solid -  the original Hessian

the constrained Hessian

the EM-equivalent Hessian

50

100

150

250

200
300
the learning steps

350

400

450

500

(b)

(a)

(b)

Figure : Experimental results for the estimation of the

parameters of a two-component Gaussian mixture. (a)

The condition numbers as a function of the iteration

number. (b) A zoomed version of (a) after discarding

the (cid:12)rst  iterations. The terminology original, con-

strained, and EM-equivalent Hessians refers to the ma-
trices H; ET HE, and ET P HE respectively.

We sampled  points from a simple (cid:12)nite mixture

model given by

p(x) = (cid:11)p(x) + (cid:11)p(x)

where

pi(x) =



p(cid:25)(cid:27)

i

expf(cid:0)





(x (cid:0) mi)

(cid:27)
i



g:

The parameter values were as

(cid:11) =
 =
. We ran both the EM algorithm and gradient ascent

:; (cid:11) = :; m = (cid:0); m = ; (cid:27)

 = ; (cid:27)

follows:

(k)

on the data. At each step of the simulation, we calcu-
lated the condition number of the Hessian ((cid:20)[H ((cid:2)
the condition number determining the rate of conver-
gence of the gradient algorithm ((cid:20)[ET H ((cid:2)
)E]), and
the condition number determining the rate of conver-
gence of EM ((cid:20)[ET P ((cid:2)
)E]). We also calcu-
lated the largest eigenvalues of the matrices H ((cid:2)
),
ET H ((cid:2)
)E. The results
are shown in Fig. . As can be seen in Fig. (a), the con-

)E, and ET P ((cid:2)

)H ((cid:2)

)H ((cid:2)

)]),

(k)

(k)

(k)

(k)

(k)

(k)

(k)

dition numbers change rapidly in the vicinity of the th

iteration and the corresponding Hessian matrices be-

come inde(cid:12)nite. Afterward, the Hessians quickly become

de(cid:12)nite and the condition numbers converge.



As shown

(k)

(k)

)] = :, (cid:20)[ET H ((cid:2)

in Fig. (b), the condition numbers converge toward the
values (cid:20)[H ((cid:2)
)E] = :, and
(cid:20)[ET P ((cid:2)
)E] = :. That is, the matrix P
has greatly reduced the condition number, by factors of
and . This signi(cid:12)cantly improves the shape of l and
speeds up the convergence.

)H ((cid:2)

(k)

(k)

We ran a second experiment in which the means of the

component Gaussians were m = (cid:0) and m = . The

results are similar to those shown in Fig. . Since the

distance between two distributions is reduced into half,

Interestingly, the EM algorithm converges soon afterward
as well, showing that for this problem EM spends little time
in the region of parameter space in which a local analysis is
valid.



Figure : Experimental results for the estimation of the

parameters of a two-component Gaussian mixture (cf.

Fig. ). The separation of the Gaussians is half the

separation in Fig. .

104

103

102

101

100

l

i



e
u
a
v
n
e
g
e
m
u
m
x
a
m
e
h



i

t

the original Hessian

the constrained Hessian

l

i



e
u
a
v
n
e
g
e
m
u
m
x
a
m
e
h



i

the EM-equivalent Hessian

t

10-1
0

10

20

30

50

40
60
the learning steps

70

80

90

100

104

103

102

101

100

10-1
0

the original Hessian

the constrained Hessian

the EM-equivalent Hessian

50

100

150

250

200
300
the learning steps

350

400

450

500

(a)

(b)

Figure : The largest eigenvalues of the matrices
H; ET HE, and ET P HE plotted as a function of the
number of iterations. The plot in (a) is for the experi-

ment in Fig. ; (b) is for the experiment reported in Fig.

.

the shape of l becomes more irregular. The condition
number (cid:20)[H ((cid:2)
)E] in-
creases to , and (cid:20)[ET P ((cid:2)
)E] increases to
. We see once again a signi(cid:12)cant improvement in the

)] increases to , (cid:20)[ET H ((cid:2)

)H ((cid:2)

(k)

(k)

(k)

(k)

case of EM, by factors of  and .

Fig.  shows that the matrix P has also reduced the
largest eigenvalues of the Hessian from between  to

 to around . This demonstrates clearly the sta-

ble convergence that is obtained via EM, without a line

search or the need for external selection of a learning

stepsize.

In the remainder of the paper we provide some theo-

retical analyses that attempt to shed some light on these

empirical results. To illustrate the issues involved, con-

sider a degenerate mixture problem in which the mixture
has a single component. (In this case (cid:11) = .) Let us fur-
thermore assume that the covariance matrix is (cid:12)xed (i.e.,
only the mean vector m is to be estimated). The Hes-
sian with respect to the mean m is H = (cid:0)N (cid:6)
and the
EM projection matrix P is (cid:6)=N . For gradient ascent, we
have (cid:20)[ET HE] = (cid:20)[(cid:6)
], which is larger than one when-
ever (cid:6) = cI . EM, on the other hand, achieves a condi-
tion number of one exactly ((cid:20)[ET P HE] = (cid:20)[P H ] =
(cid:20)[I ] =  and (cid:21)M [ET P HE] = ). Thus, EM and New-

(cid:0)

(cid:0)

tons method are the same for this simple quadratic

that the theorem can be extended to apply more widely,

problem. For general non-quadratic optimization prob-

in particular to the case of the full EM update in which

lems, Newton retains the quadratic assumption, yield-

the mixing proportions and covariances are estimated,

ing fast convergence but possible divergence. EM is

and also, within limits, to cases in which the means are

a more conservative algorithm that retains the conver-

not well separated. To obtain an initial indication as to

gence guarantee but also maintains quasi-Newton be-

possible conditions that can be usefully imposed on the

havior. We now analyze this behavior in more detail.

separation of the mixture components, we have stud-

We consider the special case of estimating the means in

a Gaussian mixture when the Gaussians are well sepa-

rated.

Theorem  Consider the EM algorithm in Eq.
(),
where the parameters (cid:11)j and (cid:6)j are assumed to be
known. Assume that the K Gaussian distributions are
well separated, such that for su(cid:14)ciently large k the pos-
terior probabilities h(k)
(t) are nearly zero or one. For
such k, the condition number associated with EM is al-
ways smaller than the condition number associated with
gradient ascent. That is:

j

(cid:20)[ET P ((cid:2)

(k)

)H ((cid:2)

(k)

)E] < (cid:20)[ET H ((cid:2)

(k)

)E]:

Furthermore, (cid:21)M [ET P ((cid:2)
as k goes to in(cid:12)nity.

(k)

)H ((cid:2)

(k)

)E] approaches one

where

(	) is
ied the case in which the second term in Eq.
neglected only for Hii and is retained for the Hij com-
ponents, where j = i. Consider, for example, the case

of a univariate mixture having two mixture components.

For (cid:12)xed mixing proportions and (cid:12)xed covariances, the

Hessian matrix (Eq. ) becomes:

H =(cid:20) h h
h h(cid:21) ;

and the projection matrix (Eq. 	) becomes:

P =(cid:20) (cid:0)h(cid:0)







 (cid:21) ;
(cid:0)h(cid:0)

hii = (cid:0)



(cid:27)(k)
i

N

Xt=

h(k)
i

(t); i = ; 


(cid:27)(k)
(cid:27)(k)
j
i

N

Xt=

((cid:0)h(k)

i

(t))h(k)

j (t)(x(t)(cid:0)mj )

T

(x(t)(cid:0)mi);

and

hij =

()

for i = j = ; . If H is negative de(cid:12)nite, (i.e., hh (cid:0)
hh < ), then we can show that the conclusions of
Theorem  remain true, even for Gaussians that are not

necessarily well-separated. The proof is achieved via the

()

following lemma:

Lemma  Consider the positive de(cid:12)nite matrix

Proof. The Hessian is

H H
H H

.
.
.

.
.
.

HK HK

H =


(cid:1) (cid:1) (cid:1) HK
(cid:1) (cid:1) (cid:1) HK

.
.
.

(cid:1) (cid:1) (cid:1) HKK




@l

@mi@mT
j

N

where

Hij (cid:17)

= (cid:0)((cid:6)

(k)
j

(cid:0)

)

N

)

(cid:0)

(cid:14)ij h(k)

(k)
j (t) + ((cid:6)
j

Xt=
)(x(t) (cid:0) mj )(x(t) (cid:0) mi)

T

]((cid:6)

(cid:0)

(k)
i

)

[

(cid:13)ij (x(t)

Xt=
) = ((cid:14)ij (cid:0) h(k)

i

with (cid:13)ij (x(t)
matrix P is

(t))h(k)

j

(t). The projection

where

P (k)

= diag[P (k)

 ; (cid:1) (cid:1) (cid:1) ; P (k)
KK ];

h(k)
j (t):

(k)
j

(cid:6)

P (k)

jj =

PN
(t)( (cid:0) h(k)

t=

j

Given that h(k)
j (t)) is negligible for su(cid:14)-
ciently large k, the second term in Eq.
(	) can be
(cid:0)PN
(t) and
neglected, yielding Hii = (cid:0)((cid:6)
H = diag[H; (cid:1) (cid:1) (cid:1) ; HKK ]. This implies that P H = (cid:0)I ,
and thus (cid:20)[P H ] = , whereas (cid:20)[H ] = .

t= h(k)

(k)
j



)

j

This theorem, although restrictive in its assumptions,

gives some indication as to why the projection matrix

in the EM algorithm appears to condition the Hessian,

yielding improved convergence.

In fact, we conjecture



(cid:6) =(cid:20) (cid:27) (cid:27)
(cid:27) (cid:27)(cid:21)

For the diagonal matrix B = diag[(cid:27)(cid:0)
(cid:20)[B(cid:6)] < (cid:20)[(cid:6)].

 ; (cid:27)(cid:0)

 ], we have

Proof. The eigenvalues of (cid:6) are the roots of ((cid:27) (cid:0)

(cid:21))((cid:27) (cid:0) (cid:21)) (cid:0) (cid:27)(cid:27) = , which gives

(cid:21)M =

(cid:21)m =

(cid:27) + (cid:27) + (cid:13)



(cid:27) + (cid:27) (cid:0) (cid:13)



(cid:13) = p((cid:27) + (cid:27)) (cid:0) ((cid:27)(cid:27) (cid:0) (cid:27)(cid:27))

and

(cid:20)[(cid:6)] =

(cid:27) + (cid:27) + (cid:13)
(cid:27) + (cid:27) (cid:0) (cid:13)

The condition number (cid:20)[(cid:6)] can be written as (cid:20)[(cid:6)] =
( + s)=( (cid:0) s) (cid:17) f (s), where s is de(cid:12)ned as follows:

s =s (cid:0)

((cid:27)(cid:27) (cid:0) (cid:27)(cid:27))

((cid:27) + (cid:27))

:

Furthermore, the eigenvalues of B(cid:6) are the roots
of ( (cid:0) (cid:21))( (cid:0) (cid:21)) (cid:0) ((cid:27)(cid:27))=((cid:27)(cid:27)) = , which
gives (cid:21)M =  + p((cid:27)(cid:27))=((cid:27)(cid:27)) and (cid:21)m =
 (cid:0) p((cid:27)(cid:27))=((cid:27)(cid:27)).
p((cid:27)(cid:27))=((cid:27)(cid:27)), we have (cid:20)[B(cid:6)] = ( + r)=( (cid:0) r) =

Thus, de(cid:12)ning r =

We now examine the quotient s=r:

f (r).

s

r

=



rs (cid:0)

( (cid:0) r)

((cid:27) + (cid:27))=((cid:27)(cid:27))

Given that ((cid:27) + (cid:27))


=((cid:27)(cid:27)) (cid:21) , we have

s
r >

rp (cid:0) ( (cid:0) r) = . That is, s > r. Since f (x) =
( + x)=((cid:0) x) is a monotonically increasing function for
x > , we have f (s) > f (r). Therefore, (cid:20)[B(cid:6)] < (cid:20)[(cid:6)].


Hessian of the EM algorithm tends toward one, showing

that EM can approximate a superlinear method. Finally,

in cases of a poorly conditioned Hessian, superlinear con-

vergence is not necessarily a virtue. In such cases many

optimization schemes, including EM, essentially revert

to gradient ascent.

We feel that EM will continue to play an important

role in the development of learning systems that empha-

size the predictive aspect of data modeling. EM has in-

deed played a critical role in the development of hidden

Markov models (HMMs), an important example of pre-

dictive data modeling.



EM generally converges rapidly

in this setting. Similarly, in the case of hierarchical mix-

tures of experts the empirical results on convergence in

likelihood have been quite promising (Jordan & Jacobs,

		; Waterhouse & Robinson, 		). Finally, EM can

play an important conceptual role as an organizing prin-

ciple in the design of learning algorithms. Its role in this

We think that it should be possible to generalize

case is to focus attention on the \missing variables" in

this lemma beyond the univariate, two-component case,

the problem. This clari(cid:12)es the structure of the algorithm

thereby weakening the conditions on separability in The-

and invites comparisons with statistical physics, where

orem  in a more general setting.

missing variables often provide a powerful analytic tool.

 Conclusions

