Abstract

We develop the relational topic model (RTM), a
model of documents and the links between them.
For each pair of documents, the RTM models
their link as a binary random variable that is
conditioned on their contents. The model can
be used to summarize a network of documents,
predict links between them, and predict words
within them. We derive efcient inference and
learning algorithms based on variational meth-
ods and evaluate the predictive performance of
the RTM for large networks of scientic abstracts
and web documents.

1

INTRODUCTION

Network data, such as citation networks of documents, hy-
perlinked networks of web pages, and social networks of
friends, are becoming pervasive in modern machine learn-
ing applications. Analyzing network data provides useful
predictive models, pointing social network members to-
wards new friends, scientic papers towards relevant cita-
tions, and web pages towards other related pages.
Recent research in this eld has focused on latent variable
models of link structure, models which decompose a net-
work according to hidden patterns of connections between
its nodes (Kemp et al. 2004; Hoff et al. 2002; Hofman and
Wiggins 2007; Airoldi et al. 2008). Though powerful, these
models account only for the structure of the network, ig-
noring observed attributes of the nodes. For example, a
network model can nd patterns which account for the ci-
tation connections between scientic articles, but it cannot
also account for the texts.
This type of information about the nodes, along with the

Appearing in Proceedings of the 12th International Confe-rence
on Articial Intelligence and Statistics (AISTATS) 2009, Clearwa-
ter Beach, Florida, USA. Volume 5 of JMLR: W&CP 5. Copyright
2009 by the authors.

links between them, should be used for uncovering, under-
standing and exploiting the latent structure in the data. To
this end, we develop a new model of network data that ac-
counts for both links such as citations and attributes such
as text.
Accounting for patterns in both sources of data leads to a
more powerful model than those that only consider links.
Given a new node and some of its links, traditional models
of network structure can provide a predictive distribution
of other nodes with which it it might be connected. Our
model need not observe any links of a new node; it can pre-
dict links using only its attributes. Thus, we can suggest
citations of newly written papers, predict the likely hyper-
links of a web page in development, or suggest friendships
in a social network based only on a new users prole of
interests. Moreover, given a new node and its links, our
model provides a predictive distribution of node attributes.
This complementary predictive mechanism can be used to
predict keywords from citations or a users interests from
his or her social connections. These types of predictions
are out of reach for traditional network models.
Our model is the relational topic model (RTM), a hierar-
chical model of links and node attributes. Focusing on net-
works of text data, the RTM explicitly ties the content of the
documents with the connections between them. First, we
describe the statistical assumptions behind the RTM. Then,
we derive efcient algorithms for approximate posterior in-
ference, parameter estimation, and prediction. Finally, we
study its performance on scientic citation networks and
hyperlinked web pages. The RTM provides signicantly
better word prediction and link prediction than natural al-
ternatives and the current state of the art.

2 RELATIONAL TOPIC MODELS

The relational topic model (RTM) is a model of data com-
posed of documents, which are collections of words, and
links between them (see Figure 1). It embeds this data in a
latent space that explains both the words of the documents
and how they are connected.

81Relational Topic Models for Document Networks

Figure 1: Example data appropriate for the relational topic model. Each document is represented as a bag of words and
linked to other documents via citation. The RTM denes a joint distribution over the words in each document and the
citation links between them.

The RTM is based on latent Dirichlet allocation (LDA)
(Blei et al. 2003). LDA is a generative probabilistic model
that uses a set of topics, distributions over a xed vocab-
ulary, to describe a corpus of documents.
In its genera-
tive process, each document is endowed with a Dirichlet-
distributed vector of topic proportions, and each word of
the document is assumed drawn by rst drawing a topic
assignment from those proportions and then drawing the
word from the corresponding topic distribution.
In the RTM, each document is rst generated from topics as
in LDA. The links between documents are then modeled as
binary variables, one for each pair of documents. These are
distributed according to a distribution that depends on the
topics used to generate each of the constituent documents.
In this way, the content of the documents are statistically
connected to the link structure between them.
The parameters to the RTM are K distributions over terms
1:K, a K-dimensional Dirichlet parameter , and a func-
tion  that provides binary probabilities. (This function is
explained in detail below.) The RTM assumes that a set
of observed documents w1:D,1:N and binary links between
them y1:D,1:D are generated by the following process.

1. For each document d:

(a) Draw topic proportions d|  Dir().
(b) For each word wd,n:

i. Draw assignment zd,n|d  Mult(d).
ii. Draw word wd,n|zd,n, 1:K  Mult(zd,n).

2. For each pair of documents d, d0:
(a) Draw binary link indicator

y|zd, zd0  (|zd, zd0).

Figure 2 illustrates the graphical model for this process for
a single pair of documents. The full model, which is dif-
cult to illustrate, contains the observed words from all D
documents, and D2 link variables for each possible con-
nection between them.
The function  is the link probability function that denes
a distribution over the link between two documents. This
function is dependent on the topic assignments that gener-
ated their words, zd and zd0. We explore two possibilities.
First, we consider

n zd,n,

(y = 1) = (T(zd  zd0) + ),

(1)
the  notation denotes the
where zd = 1
Nd
Hadamard (element-wise) product, and the function  is
the sigmoid. This link function models each per-pair bi-
nary variable as a logistic regression with hidden covari-
ates. It is parameterized by coefcients  and intercept .
The covariates are constructed by the Hadamard product of
zd and zd0, which captures similarity between the hidden
topic representations of the two documents.
Second, we consider

P

e(y = 1) = exp(T(zd  zd0) + ).

(2)
Here, e uses the same covariates as , but has an ex-
ponential mean function instead. Rather than tapering off
when zd  zd0 are close, the probabilities returned by this
function continue to increases exponentially. With some
algebraic manipulation, the function e can be viewed as
an approximate variant of the modeling methodology pre-
sented in Blei and Jordan (2003).
In both of the  functions we consider, the response is a
function of the latent feature expectations, zd and zd0. This

8252478430248775288112321222299135418541855896359224381364791096401196861201959153914717217796591121921489885178378286208156923431270218129022322723616172541176256634264196321951377303426209131316425348013353445851244229126171627229012753751027396167824472583106169212079601238201216442042381418179212846515241165219715682593169854768321371637255720336321020436442449474649263623005395416031047722660806112111388318371335902964966981167311401481143212531590106099299410011010165115781039104013441345134813551420108914831188167416802272128515921234130413171426169514651743194422592213We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts...Irrelevant features and the subset selection problemIn many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible...Learning with many irrelevant featuresIn this introduction, we define the term bias as it is used in machine learning systems. We motivate the importance of automated methods for evaluating...Evaluation and selection of biases in machine learningThe inductive learning problem consists of learning a concept given examples and nonexamples of the concept. To perform this learning task, inductive learning algorithms bias their learning method...Utilizing prior concepts for learningThe problem of learning decision rules for sequential tasks is addressed, focusing on the problem of learning tactical plans from a simple flight simulator where a plane must avoid a missile...Improving tactical plans with genetic algorithmsEvolutionary learning methods have been found to be useful in several areas in the development of intelligent robots. In the approach described here, evolutionary...An evolutionary approach to learning in robotsNavigation through obstacles such as mine fields is an important capability for autonomous underwater vehicles. One way to produce robust behavior...Using a genetic algorithm to learn strategies for collision avoidance and local navigation..............................Chang, Blei

Figure 2: A two-document segment of the RTM. The variable y indicates whether the two documents are linked. The
complete model contains this variable for each pair of documents. The plates indicate replication. This model captures
both the words and the link structure of the data shown in Figure 1.

formulation, inspired by the supervised LDA model (Blei
and McAuliffe 2007), ensures that the same latent topic as-
signments used to generate the content of the documents
also generates their link structure. Models which do not
enforce this coupling, such as Nallapati et al. (2008), might
divide the topics into two independent subsetsone for
links and the other for words. Such a decomposition pre-
vents these models from making meaningful predictions
about links given words and words given links.
In Sec-
tion 4 we demonstrate empirically that the RTM outper-
forms such models on these tasks.

3

INFERENCE, ESTIMATION, AND
PREDICTION

With the model dened, we turn to approximate poste-
rior inference, parameter estimation, and prediction. We
develop a variational inference procedure for approximat-
ing the posterior. We use this procedure in a variational
expectation-maximization (EM) algorithm for parameter
estimation. Finally, we show how a model whose parame-
ters have been estimated can be used as a predictive model
of words and links.

Inference
In posterior inference, we seek to compute
the posterior distribution of the latent variables condi-
tioned on the observations. Exact posterior inference is in-
tractable (Blei et al. 2003; Blei and McAuliffe 2007). We
appeal to variational methods.
In variational methods, we posit a family of distributions
over the latent variables indexed by free variational pa-
rameters. Those parameters are t to be close to the true
posterior, where closeness is measured by relative entropy.
See Jordan et al. (1999) for a review. We use the fully-
factorized family,

q(, Z|, ) =Q

d [q(d|d)Q

n qz(zd,n|d,n)] ,

(3)

1Sums over document pairs (d1, d2) are understood to range

over pairs for which a link has been observed.

where  is a set of Dirichlet parameters, one for each doc-
ument, and  is a set of multinomial parameters, one for
each word in each document. Note that Eq [zd,n] = d,n.
Minimizing the relative entropy is equivalent to maximiz-
ing the Jensens lower bound on the marginal probability of
the observations, i.e., the evidence lower bound (ELBO),

L =P
P
P
P

d

d

d

(d1,d2)

P
P

Eq [log p(yd1,d2|zd1, zd2, , )] +
Eq [log p(wd,n|1:K, zd,n)] +
Eq [log p(zd,n|d)] +
Eq [log p(d|)] + H(q),

n

n

(4)

where (d1, d2) denotes all document pairs. The rst term
of the ELBO differentiates the RTM from LDA (Blei et al.
2003). The connections between documents affect the ob-
jective in approximate posterior inference (and, below, in
parameter estimation).
We develop the inference procedure under the assumption
that only observed links will be modeled (i.e., yd1,d2 is ei-
ther 1 or unobserved).1 We do this for two reasons.
First, while one can x yd1,d2 = 1 whenever a link is ob-
served between d1 and d2 and set yd1,d2 = 0 otherwise, this
approach is inappropriate in corpora where the absence of
a link cannot be construed as evidence for yd1,d2 = 0. In
these cases, treating these links as unobserved variables is
more faithful to the underlying semantics of the data. For
example, in large social networks such as Facebook the ab-
sence of a link between two people does not necessarily
mean that they are not friends; they may be real friends
who are unaware of each others existence in the network.
Treating this link as unobserved better respects our lack of
knowledge about the status of their relationship.
Second, treating non-links links as hidden decreases the

83Nddwd,nzd,nKkyd,d'Nd'd'wd',nzd',nRelational Topic Models for Document Networks

computational cost of inference; since the link variables are
leaves in the graphical model they can be removed when-
ever they are unobserved. Thus the complexity of compu-
tation scales with the number of observed links rather than
the number of document pairs. This provides a signicant
computational advantage.
Our aim now is to compute each term of the objective
function given in Equation 4. The rst term depends on
our choice of link probability function. This term is not
tractable to compute when the logistic function of Equa-
tion 1 is chosen. We use a rst-order approximation (Braun
and McAuliffe 2007),

T d1,d2 +  + log (cid:0)T d1,d2  (cid:1) ,

Ld1,d2  Eq [log p(yd1,d2 = 1|zd1, zd2, , )] 
(5)
P
where d1,d2 = d1  d2 and d = Eq [zd] =
n d,n. When e is the response function, this term
1
Nd
can be computed explicitly as
Eq [log p(yd1,d2 = 1|zd1, zd2, , )] = T d1,d2 + .

(6)

We use coordinate ascent to optimize the ELBO with re-
spect to the variational parameters , ,

d,j  exp{P

d06=d(d,d0 Ld,d0) d0
Eq [log d|d] + log ,wd,j},

+

Nd

where Ld,d0 is computed according to either Equation 5 or
log ,wd,j can be com-
6 depending on the choice of .
puted by taking the element-wise logarithm of the wd,jth

column of . Eq [log d|d] is (d)  (P d,i), where
for LDA (Blei et al. 2003), d   +P

 is the digamma function. (A digamma of a vector is the
vector of digammas.)
The update for  is identical to that in variational inference

n d,n.

Parameter estimation We t the model by nding max-
imum likelihood estimates for each of the parameters:
multinomial topic vectors 1:K and link function param-
eters , . Once again, this is intractable so we turn to
an approximation. We employ variational expectation-
maximization, where we iterate between optimizing the
ELBO of Equation 4 with respect to the variational distri-
bution and with respect to the model parameters.
Optimizing with respect to the variational distribution is de-
scribed in Section 3. Optimizing with respect to the model
parameters is equivalent to maximum likelihood estimation
with expected sufcient statistics, where the expectation is
taken with respect to the variational distribution.
Since the terms in Equation 4 that involve  are identical
to those in LDA, estimating the topic vectors can be done
via the same update:

k,w X

X

d

n

1(wd,n = w)k

d,n.

In practice, we smooth our estimates of k,w using a sym-
metric Dirichlet prior on the topics.
It is not possible to directly optimize the parameters of
the link probability function without negative observations
(i.e., yd1,d2 = 0). We address this by applying a regular-
ization penalty parameterized by a scalar, . The effect of
this regularization is to posit some number of latent neg-
ative observations in the network and to incorporate them
into the parameter estimates. The frequency of the nega-
tive observations is controlled by . (For space we omit the
derivation of this regularization term.)
When using the logistic function of Equation 1, we use
gradient-based optimization to estimate the parameters 
and . Using the approximation used in Equation 5, the
relevant gradients of the ELBO are

(cid:2)1  (cid:0)Td1,d2 + (cid:1)(cid:3) d1,d2
(cid:2)1  (cid:0)Td1,d2 + (cid:1)(cid:3)




(d1,d2)

(d1,d2)

L P
L P

(cid:0)/K 2 + (cid:1) /K 2,
(cid:0)1T/K 2 + (cid:1) .
(cid:18)
  log(cid:0)1  1T (cid:1)  log
(cid:16)  + 
  log(cid:0) (cid:1)  log
where  =P

(d1,d2) d1,d2.

K 2 1

(cid:19)

+ 1  1T 



K  1
K

(cid:17)  1,

When using the exponential function of Equation 2, we can
estimate the parameters  and  analytically,

Prediction With a tted model, our ultimate goal is to
make predictions about new data. We describe two kinds of
prediction: link prediction from words and word prediction
from links.
In link prediction, we are given a new document (i.e. a
document which is not in the training set) and its words.
We are asked to predict its links to the other documents.
This requires computing
p(yd,d0|wd, wd0) =

P
zd,zd0 p(yd,d0|zd, zd0)p(zd, zd0|wd, wd0),

an expectation with respect to a posterior that we cannot
compute. Using the inference algorithm from Section 3, we
nd variational parameters which optimize the ELBO for
the given evidence, i.e., the words and links for the training
documents and the words in the test document. Replacing
the posterior with this approximation q(, Z), the predic-
tive probability is approximated with

p(yd,d0|wd, wd0)  Eq [p(yd,d0|zd, zd0)] .

(7)

In a variant of link prediction, we are given a new set of
documents (documents not in the training set) along with

84Chang, Blei

their words and asked to select the links most likely to exist.
The predictive probability for this task is proportional to
Equation 7.
The second predictive task is word prediction, where we
predict the words of a new document based only on its
links. As with link prediction, p(wd,i|yd) cannot be com-
puted. Using the same technique, a variational distribution
can approximate this posterior. This yields the predictive
probability

p(wd,i|yd)  Eq [p(wd,i|zd,i)] .

Note that models which treat the endpoints of links as lex-
ical tokens cannot participate in the two tasks presented
here because they cannot make meaningful predictions for
documents that do not appear in the training set (Nallap-
ati and Cohen 2008; Cohn and Hofmann 2001; Sinkkonen
et al. 2008). By modeling both documents and links gen-
eratively, our model is able to give predictive distributions
for words given links, links given words, or any mixture
thereof.

4 EMPIRICAL RESULTS

We examined the RTM on three data sets. Words were
stemmed; stop words and infrequently occurring words
were removed. Directed links were converted to undirected
links2 and documents with no links were removed. The
Cora data (McCallum et al. 2000) contains abstracts from
the Cora research paper search engine, with links between
documents that cite each other. The WebKB data (Craven
et al. 1998) contains web pages from the computer science
departments of different universities, with links determined
from the hyperlinks on each page. The PNAS data con-
tains recent abstracts from the Proceedings of the National
Academy of Sciences. The links between documents are
intra-PNAS citations.3

Evaluating the predictive distribution As with any
probabilistic model, the RTM denes a probability distri-
bution over unseen data. After inferring the latent variables
from data (as described in Section 3), we ask how well the
model predicts the links and words of unseen nodes. Mod-
els that give higher probability to the unseen documents
better capture the joint structure of words and links.
We study the two variants of the RTM discussed above: lo-
gistic RTM uses the logistic link of Equation 1; exponential

2The RTM can be extended to accommodate directed connec-

tions. Here we modeled undirected links.

3After processing, the Cora data contained 2708 documents,
49216 words, 5278 links, and a lexicon of 1433 terms. The We-
bKB data contained 877 documents, 79365 words, 1388 links, and
a lexicon of 1703 terms. The PNAS data contained 2128 doc-
uments, 119162 words, 1577 links, and had a lexicon of 2239
terms.

RTM uses the exponential link of Equation 2. We compare
these models against three alternative approaches. The rst
(Baseline) models words and links independently. The
words are modeled with a multinomial; the links are mod-
eled with a Bernoulli. The second (Mixed-Membership)
is the model proposed by Nallapati et al. (2008), which
is an extension of the mixed membership stochastic block
model (Airoldi et al. 2008) to model network structure and
node attributes. The third (LDA + Regression) rst ts an
LDA model to the documents and then ts a logistic regres-
sion model to the observed links, with input given by the
Hadamard product of the latent class distributions of each
pair of documents. Rather than performing dimensional-
ity reduction and regression simultaneously, this method
performs unsupervised dimensionality reduction rst, and
then regresses to understand the relationship between the
latent space and underlying link structure. All models were
trained such that the total mass of the Dirichlet hyperpa-
rameter  was 5.0. (While we omit a full sensitivity study
here, we observed that the performance of the models was
similar for  within a factor of 2 above and below the value
we chose.)
We measured the performance of these models on link pre-
diction and word prediction (see Section 3). We divided
each data set into ve folds. For each fold and for each
model, we ask two predictive queries: given the words of
a new document, what is the likelihood of its links; and
given the links of a new document, what is the likelihood
of its words? Again, the predictive queries are for com-
pletely new test documents that are not observed in train-
ing. During training the test documents are removed along
with their attendant links. We show the results for both
tasks in Figure 3.
In predicting links, the two variants of the RTM perform
better than all of the alternative models for all of the data
sets (see Figure 3, top row). Cora is paradigmatic, showing
a nearly 6% improvement in log likelihood for exponential
RTM over baseline and 5% improvement over LDA + Re-
gression. Logistic RTM performs nearly as well on Cora
with an approximately 5% improvement over baseline and
4% improvement over LDA + Regression. We emphasize
that the links are predicted to documents seen in the train-
ing set from documents which were held out. By incor-
porating link and node information in a joint fashion, the
model is able to generalize to new documents for which no
link information was previously known.
The performance of the Mixed-Membership model rarely
deviates from the baseline. Despite its increased dimen-
sionality (and commensurate increase in computational dif-
culty), only on PNAS and only when the number of top-
ics is large is the Mixed-Membership model competitive
with any of the proposed models. We hypothesize that the
Mixed-Membership model exhibits this behavior because it
uses some topics to explain the words observed in the train-

85Relational Topic Models for Document Networks

Figure 3: Average held-out predictive link log likelihood (top) and word log likelihood (bottom) as a function of the number
of topics. For all three corpora, RTMs outperform baseline unigram, LDA, and Mixed-Membership, which is the model
of Nallapati et al. (2008).

ing set, and other topics to explain the links observed in the
training set. Therefore, it cannot use word observations to
predict links.
In predicting words, the two variants of the RTM again out-
perform all of the alternative models (see Figure 3, bottom
row). This is because the RTM uses link information to in-
uence the predictive distribution of words. In contrast, the
predictions of LDA + Regression are similar to the Base-
line. The predictions of the Mixed-Membership model are
rarely higher than Baseline, and often lower.

Automatic link suggestion A natural real-world applica-
tion of link prediction is to suggest links to a user based on
the text of a document. One might suggest citations for an
abstract or friends for a user in a social network.
Table 1 illustrates suggested citations using RTM (e) and
LDA + Regression as predictive models. These suggestions
were computed from a model trained on one of the folds of
the Cora data. The top results illustrate suggested links
for Markov chain Monte Carlo convergence diagnostics:

A comparative review, which occurs in this folds training
set. The bottom results illustrate suggested links for Com-
petitive environments evolve better solutions for complex
tasks, which is in the test set.
RTM outperforms LDA + Regression in being able to iden-
tify more true connections. For the rst document, RTM
nds 3 of the connected documents versus 1 for LDA +
Regression. For the second document, RTM nds 3 while
LDA + Regression does not nd any. This qualitative be-
havior is borne out quantitatively over the entire corpus.
Considering the precision of the rst 20 documents re-
trieved by the models, RTM improves precision over LDA
+ Regression by 80%. (Twenty is a reasonable number of
documents for a user to examine.)
While both models found several connections which were
not observed in the data, those found by the RTM are qual-
itatively different. In the rst document, both sets of sug-
gested links are about Markov chain Monte Carlo. How-
ever, the RTM nds more documents relating specically to
convergence and stationary behavior of Monte Carlo meth-

8651015202514.013.813.613.4CoraLink Log Likelihoodlllll5101520253600355035003450Number of topicsWord Log Likelihoodlllll51015202511.811.711.611.511.4WebKBlllll5101520251145114011351130Number of topicsllllllRTM,,  yyssRTM,,  yyeLDA + Regression       MixedMembershipUnigram/Bernoulli51015202513.613.513.413.313.2PNASlllll5101520252970296029502940Number of topicslllllChang, Blei

Markov chain Monte Carlo convergence diagnostics: A comparative review

Minorization conditions and convergence rates for Markov chain Monte Carlo

Rates of convergence of the Hastings and Metropolis algorithms
Possible biases induced by MCMC convergence diagnostics

Bounding convergence time of the Gibbs sampler in Bayesian image restoration

Self regenerative Markov chain Monte Carlo

Auxiliary variable methods for Markov chain Monte Carlo with applications
Rate of Convergence of the Gibbs Sampler by Gaussian Approximation

Diagnosing convergence of Markov chain Monte Carlo algorithms

Exact Bound for the Convergence of Metropolis Chains

Self regenerative Markov chain Monte Carlo

Minorization conditions and convergence rates for Markov chain Monte Carlo

Gibbs-markov models

Auxiliary variable methods for Markov chain Monte Carlo with applications

Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Models

Mediating instrumental variables

A qualitative framework for probabilistic inference

Adaptation for Self Regenerative MCMC

R
T
M

(


e
)

L
D
A
+
R
e
g
r
e
s
s
i
o
n

Competitive environments evolve better solutions for complex tasks

Coevolving High Level Representations

A Survey of Evolutionary Strategies

Genetic Algorithms in Search, Optimization and Machine Learning

Strongly typed genetic programming in evolving cooperation strategies

Solving combinatorial problems using evolutionary algorithms

A promising genetic algorithm approach to job-shop scheduling, rescheduling, and open-shop scheduling problems

An Empirical Investigation of Multi-Parent Recombination Operators in Evolution Strategies

Evolutionary Module Acquisition

A promising genetic algorithm approach to job-shop scheduling, rescheduling, and open-shop scheduling problems

A New Algorithm for DNA Sequence Assembly

Identication of protein coding regions in genomic DNA

Solving combinatorial problems using evolutionary algorithms

A genetic algorithm for passive management

The Performance of a Genetic Algorithm on a Chaotic Objective Function

Adaptive global optimization with local search

Mutation rates as adaptations

R
T
M

(


e
)

L
D
A
+
R
e
g
r
e
s
s
i
o
n

Table 1: Top eight link predictions made by RTM (e) and LDA + Regression for two documents (italicized) from Cora.
The models were trained with 10 topics. Boldfaced titles indicate actual documents cited by or citing each document. Over
the whole corpus, RTM improves precision over LDA + Regression by 80% when evaluated on the rst 20 documents
retrieved.

ods. LDA + Regression nds connections to documents in
the milieu of MCMC, but many are only indirectly related
to the input document. The RTM is able to capture that the
notion of convergence is an important predictor for ci-
tations, and has adjusted the topic distribution and predic-
tors correspondingly. For the second document, the docu-
ments found by the RTM are also of a different nature than
those found by LDA + Regression. All of the documents
suggested by RTM relate to genetic algorithms. LDA +
Regression, however, suggests some documents which are
about genomics. By relying only on words, LDA + Re-
gression conates two genetic topics which are similar
in vocabulary but different in citation structure. In contrast,
the RTM partitions the latent space differently, recognizing
that papers about DNA sequencing are unlikely to cite pa-
pers about genetic algorithms, and vice versa. It is better
able to capture the joint distribution of words and links.

5 RELATED WORK AND DISCUSSION

The RTM builds on previous research in statistics and ma-
chine learning. Many models have been developed to
explain network link structure (Wasserman and Pattison
1996; Newman 2002) and extensions which incorporate
node attributes have been proposed (Getoor et al. 2001;
Taskar et al. 2004). However, these models are not la-
tent space approaches and therefore cannot provide the
benets of dimensionality reduction and produce the inter-
pretable clusters of nodes useful for understanding commu-
nity structure.
The RTM, in contrast, is a latent space approach which
can provide meaningful clusterings of both nodes and at-
tributes. Several latent space models for modeling net-
work structure have been proposed (Kemp et al. 2004; Hoff
et al. 2002; Hofman and Wiggins 2007; Airoldi et al. 2008);
though powerful, these models only account for links in the
data and cannot model node attributes as well.

87Relational Topic Models for Document Networks

Because the RTM jointly models node attributes and link
structure, it can make predictions about one given the other.
Previous work tends to explore one or the other of these two
prediction problems. Some previous work uses link struc-
ture to make attribute predictions (Chakrabarti et al. 1998;
Kleinberg 1999), including several topic models (Dietz
et al. 2007; McCallum et al. 2005; Wang et al. 2005). How-
ever, none of these methods can make predictions about
links given words.
In addition to being able to make predictions about links
given words and words given links, the RTM is able to
do so for new documentsdocuments outside of training
data. Approaches which generate document links through
topic models (Nallapati and Cohen 2008; Cohn and Hof-
mann 2001; Sinkkonen et al. 2008; Gruber et al. 2008) treat
links as discrete terms from a separate vocabulary. This
encodes the observed training data into the model, which
cannot be generalized to observations outside of it. Link
and word predictions for new documents, of the kind we
evaluate in Section 4, are ill-dened in these models.
Closest to the RTM is recent work by Nallapati et al.
(2008) and Mei et al. (2008), which attempts to address
these issues by extending the mixed-membership stochastic
block model (Airoldi et al. 2008) to include word attributes.
Because of their underlying exchangeability assumptions,
these models allow for the links to be explained by some
topics and the words to be explained by others. This hin-
ders their predictions when using information about words
to predict link structure and vice versa.
In contrast, the
RTM enforces the constraint that topics be used to explain
both words and links. We showed in Section 4 that the
RTM outperforms such models on these tasks.
The RTM is a new probabilistic generative model of doc-
uments and links between them. The RTM is used to ana-
lyze linked corpora such as citation networks, linked web
pages, and social networks with user proles. We have
demonstrated qualitatively and quantitatively that the RTM
provides an effective and useful mechanism for analyzing
and using such data. It signicantly improves on previous
models, integrating both node-specic information and link
structure to give better predictions.

Acknowledgements

David M. Blei is supported by ONR 175-6343, NSF CA-
REER 0745520, and grants from Google and Microsoft.

