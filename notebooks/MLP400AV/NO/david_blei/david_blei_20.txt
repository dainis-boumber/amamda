Abstract

We propose the hierarchical Dirichlet process (HDP), a nonparametric
Bayesian model for clustering problems involving multiple groups of
data. Each group of data is modeled with a mixture, with the number of
components being open-ended and inferred automatically by the model.
Further, components can be shared across groups, allowing dependencies
across groups to be modeled effectively as well as conferring generaliza-
tion to new groups. Such grouped clustering problems occur often in
practice, e.g. in the problem of topic discovery in document corpora. We
report experimental results on three text corpora showing the effective
and superior performance of the HDP over previous models.

1 Introduction

One of the most signicant conceptual and practical tools in the Bayesian paradigm is
the notion of a hierarchical model. Building on the notion that a parameter is a random
variable, hierarchical models have applications to a variety of forms of grouped or relational
data and to general problems involving multi-task learning or learning to learn. A
simple and classical example is the Gaussian means problem, in which a grand mean 0
is drawn from some distribution, a set of K means are then drawn independently from a
Gaussian with mean 0, and data are subsequently drawn independently from K Gaussian
distributions with these means. The posterior distribution based on these data couples the
means, such that posterior estimates of the means are shrunk towards each other. The
estimates share statistical strength, a notion that can be made precise within both the
Bayesian and the frequentist paradigms.

Here we consider the application of hierarchical Bayesian ideas to a problem in multi-task
learning in which the tasks are clustering problems, and our goal is to share clusters
among multiple, related clustering problems. We are motivated by the task of discovering
topics in document corpora [1]. A topic (i.e., a cluster) is a distribution across words while
documents are viewed as distributions across topics. We want to discover topics that are
common across multiple documents in the same corpus, as well as across multiple corpora.

Our work is based on a tool from nonparametric Bayesian analysis known as the Dirichlet
process (DP) mixture model [2, 3]. Skirting technical denitions for now, nonparametric

can be understood simply as implying that the number of clusters is open-ended. Indeed,
at each step of generating data points, a DP mixture model can either assign the data point
to a previously-generated cluster or can start a new cluster. The number of clusters is a
random variable whose mean grows at rate logarithmic in the number of data points.

Extending the DP mixture model framework to the setting of multiple related clustering
problems, we will be able to make the (realistic) assumption that we do not know the
number of clusters a priori in any of the problems, nor do we know how clusters should be
shared among the problems.

When generating a new cluster, a DP mixture model selects the parameters for the cluster
(e.g., in the case of Gaussian mixtures, the mean and covariance matrix) from a distribution
G0the base distribution. So as to allow any possible parameter value, the distribution
G0 is often assumed to be a smooth distribution (i.e., non-atomic). Unfortunately, if we
now wish to extend DP mixtures to groups of clustering problems, the assumption that G0
is smooth conicts with the goal of sharing clusters among groups. That is, even if each
group shares the same underlying base distribution G0, the smoothness of G0 implies that
they will generate distinct cluster parameters (with probability one). We will show that this
problem can be resolved by taking a hierarchical Bayesian approach. We present a notion
of a hierarchical Dirichlet process (HDP) in which the base distribution G0 for a set of DPs
is itself a draw from a DP. This turns out to provide an elegant and simple solution to the
problem of sharing clusters among multiple clustering problems.

The paper is organized as follows. In Section 2, we provide the basic technical denition
of DPs and discuss related representations involving stick-breaking processes and Chinese
restaurant processes. Section 3 then introduces the HDP, motivated by the requirement
of a more powerful formalism for the grouped data setting. As for the DP, we present
analogous stick-breaking and Chinese restaurant representations for the HDP. We present
empirical results on a number of text corpora in Section 5, demonstrating various aspects of
the HDP including its nonparametric nature, hierarchical nature, and the ease with which
the framework can be applied to other realms such as hidden Markov models.

2 Dirichlet Processes

The Dirichlet process (DP) and the DP mixture model are mainstays of nonparametric
Bayesian statistics (see, e.g., [3]). They have also begun to be seen in applications in ma-
chine learning (e.g., [7, 8, 9]). In this section we give a brief overview with an eye towards
generalization to HDPs. We begin with the denition of DPs [4]. Let (, B) be a measur-
able space, with G0 a probability measure on the space, and let 0 be a positive real number.
A Dirichlet process is the distribution of a random probability measure G over (, B) such
that, for any nite partition (A1, . . . , Ar) of , the random vector (G(A1), . . . , G(Ar)) is
distributed as a nite-dimensional Dirichlet distribution:

(G(A1), . . . , G(Ar))  Dir(cid:0)0G0(A1), . . . , 0G0(Ar)(cid:1) .

(1)
We write G  DP(0, G0) if G is a random probability measure distributed according to
a DP. We call G0 the base measure of G, and 0 the concentration parameter.
The DP can be used in the mixture model setting in the following way. Consider a set
of data, x = (x1, . . . , xn), assumed exchangeable. Given a draw G  DP(0, G0),
independently draw n latent factors from G: i  G. Then, for each i = 1, . . . , n,
draw xi  F (i), for a distribution F . This setup is referred to as a DP mixture model.
If the factors i were all distinct, then this setup would yield an (uninteresting) mixture
model with n components. In fact, the DP exhibits an important clustering property, such
that the draws i are generally not distinct. Rather, the number of distinct values grows as
O(log n), and it is this that denes the random number of mixture components.

There are several perspectives on the DP that help to understand this clustering property.
In this paper we will refer to two: the Chinese restaurant process (CRP), and the stick-
breaking process. The CRP is a distribution on partitions that directly captures the cluster-
ing of draws from a DP via a metaphor in which customers share tables in a Chinese restau-
rant [5]. As we will see in Section 4, the CRP refers to properties of the joint distribution
of the factors {i}. The stick-breaking process, on the other hand, refers to properties of
G, and directly reveals its discrete nature [6]. For k = 1, 2 . . ., let:

k  G0

 0
k  Beta(1, 0)

kQk1
Then with probability one the random measure dened by G = P

(2)
l=1 (1   0
k=1 kk is a sample
from DP(0, G0). The construction for 1, 2, . . . in (2) can be understood as taking a
stick of unit length, and repeatedly breaking off segments of length k. The stick-breaking
construction shows that DP mixture models can be viewed as mixture models with a count-
ably innite number of components. To see this, identify each k as the parameter of the
kth mixture component, with mixing proportion given by k.

k =  0

k).

3 Hierarchical Dirichlet Processes

We will introduce the hierarchical Dirichlet process (HDP) in this section. First we de-
scribe the general setting in which the HDP is most usefulthat of grouped data. We
assume that we have J groups of data, each consisting of nj data points (xj1, . . . , xjnj ).
We assume that the data points in each group are exchangeable, and are to be modeled with
a mixture model. While each mixture model has mixing proportions specic to the group,
we require that the different groups share the same set of mixture components. The idea is
that while different groups have different characteristics given by a different combination
of mixing proportions, using the same set of mixture components allows statistical strength
to be shared across groups, and allows generalization to new groups.

The HDP is a nonparametric prior which allows the mixture models to share components.
It is a distribution over a set of random probability measures over (, B): one probability
measure Gj for each group j, and a global probability measure G0. The global measure G0
is distributed as DP(, H), with H the base measure and  the concentration parameter,
while each Gj is conditionally independent given G0, with distribution Gj  DP(0, G0).
To complete the description of the HDP mixture model, we associate each xji with a factor
ji, with distributions given by F (ji) and Gj respectively. The overall model is given in
Figure 1 left, with conditional distributions:

G0 | , H  DP(, H)

Gj | , G0  DP(0, G0)

ji | Gj  Gj

xji | ji  F (ji) .

(3)
(4)

sum of point masses: G0 = P

The stick-breaking construction (2) shows that a draw of G0 can be expressed as a weighted
k=1 kk. This fact that G0 is atomic plays an important
role in ensuring that mixture components are shared across different groups. Since G0 is
the base distribution for the individual Gjs, (2) again shows that the atoms of the individual
Gj are samples from G0. In particular, since G0 places non-zero mass only on the atoms
 = (k)

k=1, the atoms of Gj must also come from , hence we may write:

k=1 kk

G0 =P

(5)
Identifying k as the parameters of the kth mixture component, we see that each submodel
corresponding to distinct groups share the same set of mixture components, but have dif-
fering mixing proportions, j = (jk)
Finally, it is useful to explicitly describe the relationships between the mixing proportions
 and (j)J
j=1. Details are provided in [10]. Note that the weights j are conditionally in-
dependent given  since each Gj is independent given G0. Applying (1) to nite partitions

Gj =P

k=1 jkk .

k=1.



0

H

G0

G2

2i

x2i

n2

0

G1


1i

x
1i

n
1

0

G3


3i

x
3i

global

2


23



32




11

13



12





22


1
31
3


24

21



16




15

12


12


22

21


21



25


23


22



26




24

23



36




35

32


31



31



34



33


32



18



11

11




13

14



17



13


24


28



27
group j=2

n3

group j=1

group j=3

Figure 1: Left: graphical model of an example HDP mixture model with 3 groups. Corresponding
to each DP node we also plot a sample draw from the DP using the stick-breaking construction.
Right: an instantiation of the CRF representation for the 3 group HDP. Each of the 3 restaurants has
customers sitting around tables, and each table is served a dish (which corresponds to customers in
the Chinese restaurant for the global DP).

of , we get j  DP(0, ), where we interpret  and j as probability measures over
the positive integers. Hence  is simply the putative mixing proportion over the groups.
We may in fact obtain an explicit stick-breaking construction for the js as well. Applying
(1) to partitions ({1, . . . , k  1}, {k}, {k + 1, . . .}) of positive integers, we have:

0

jk  Beta(cid:16)0k, 0(cid:16)1 Pk

l=1 l(cid:17)(cid:17)

jk = 0

l=1 (1  0

jl) .

jkQk1

(6)

4 The Chinese Restaurant Franchise

We describe an alternative view of the HDP based directly upon the distribution a HDP in-
duces on the samples ji, where we marginalize out G0 and Gjs. This view directly leads
to an efcient Gibbs sampler for HDP mixture models, which is detailed in the appendix.
Consider, for one group j, the distribution of j1, . . . , jnj as we marginalize out Gj. Re-
call that since Gj  DP(0, G0) we can describe this distribution by describing how to
generate j1, . . . , jnj using the CRP. Imagine nj customers (each corresponds to a ji)
at a Chinese restaurant with an unbounded number of tables. The rst customer sits at the
rst table. A subsequent customer sits at an occupied table with probability proportional
to the number of customers already there, or at the next unoccupied table with probability
proportional to 0. Suppose customer i sat at table tji. The conditional distributions are:

tji | tj1, . . . , tji1, 0  Xt

njt

Pt0 njt0 +0

t +

0

Pt0 njt0 +0

tnew ,

(7)

where njt is the number of customers currently at table t. Once all customers have sat down
the seating plan corresponds to a partition of j1, . . . , jnj . This is an exchangeable pro-
cess in that the probability of a partition does not depend on the order in which customers
sit down. Now we associate with table t a draw jt from G0, and assign ji = jtji .
Performing this process independently for each group j, we have now integrated out all the
Gjs, and have an assignment of each ji to a sample jtji from G0, with the partition
structures given by CRPs. Notice now that all jts are simply i.i.d. draws from G0, which
is again distributed according to DP(, H), so we may apply the same CRP partitioning
process to the jts. Let the customer associated with jt sit at table kjt. We have:

kjt | k11, . . . , k1n1 , k21, . . . , kjt1,   Xk

mk

Pk0 mjk0 + k +



Pk0 mk0 +0

knew .

(8)

Perplexity on test abstacts of LDA and HDP mixture

Posterior over number of topics in HDP mixture

1050

1000

950

900

850

800

y
t
i
x
e
l
p
r
e
P

750

10

20

30

40

LDA
HDP Mixture

50

60

Number of LDA topics

70

80

90 100 110 120

s
e
l
p
m
a
s

f
o

r
e
b
m
u
N

15

10

5

0

61 62 63 64 65 66 67 68 69 70 71 72 73

Number of topics

Figure 2: Left: comparison of LDA and HDP mixture. Results are averaged over 10 runs, with error
bars being 1 standard error. Right: histogram of the number of topics the HDP mixture used over 100
posterior samples.

Finally we associate with table k a draw k from H and assign jt = kjt . This completes
the generative process for the jis, where we marginalize out G0 and Gjs. We call this
generative process the Chinese restaurant franchise (CRF). The metaphor is as follows: we
have J restaurants, each with nj customers (jis), who sit at tables (jts). Now each table
is served a dish (ks) from a menu common to all restaurants. The customers are sociable,
prefering large tables with many customers present, and also prefer popular dishes.

5 Experiments

We describe 3 experiments in this section to highlight the various aspects of the HDP: its
nonparametric nature; its hierarchical nature; and the ease with which we can apply the
framework to other models, specically the HMM.
Nematode biology abstracts. To demonstrate the strength of the nonparametric approach
as exemplied by the HDP mixture, we compared it against latent Dirichlet allocation
(LDA), which is a parametric model similar in structure to the HDP [1]. In particular,
we applied both models to a corpus of nematode biology abstracts1, evaluating the per-
plexity of both models on held out abstracts. Here abstracts correspond to groups, words
correspond to observations, and topics correspond to mixture components, and exchange-
ability correspond to the typical bag-of-words assumption. In order to study specically the
nonparametric nature of the HDP, we used the same experimental setup for both models2,
except that in LDA we had to vary the number of topics used between 10 and 120, while
the HDP obtained posterior samples over this automatically.

The results are shown in Figure 2. LDA performs best using between 50 and 80 topics,
while the HDP performed just as well as these. Further, the posterior over the number of
topics used by HDP is consistent with this range. Notice however that the HDP infers the
number of topics automatically, while LDA requires some method of model selection.
NIPS sections. We applied HDP mixture models to a dataset of NIPS 1-12 papers orga-
nized into sections3. To highlight the transfer of learning achievable with the HDP, we

1Available at http://elegans.swmed.edu/wli/cgcbib. There are 5838 abstracts in total. After removing
standard stop words and words appearing less than 10 times, we are left with 476441 words in total
and a vocabulary size of 5699.

2In both models, we used a symmetric Dirichlet distribution with weights of 0.5 for the prior H
over topic distributions, while the concentration parameters are integrated out using a vague gamma
prior. Gibbs sampling using the CRF is used, while the concentration parameters are sampled using
a method described in [10]. This also applies to the NIPS sections experiment on next page.

3To ensure we are dealing with informative words in the documents, we culled stop words as well

Average perplexity over NIPS sections of 3 models

Generalization from LT, AA, AP to VS

6000

5500

5000

4500

4000

3500

3000

y
t
i
x
e
l
p
r
e
P

2500
0

10

M1: additional sction ignored
M2: flat, additional section
M3: hierarchical, additional section

60
20
Number of VS training documents

30

40

50

5000

4500

4000

3500

3000

y
t
i
x
e
l
p
r
e
P

70

80

2500
0

10

LT
AA
AP

70

80

60
20
Number of VS training documents

30

40

50

Figure 3: Left: perplexity of test VS documents given training documents from VS and another
section for 3 different models. Curves shown are averaged over the other sections and 5 runs. Right:
perplexity of test VS documents given LT, AA and AP documents respectively, using M3, averaged
over 5 runs. In both, the error bars are 1 standard error.

show improvements to the modeling of a section when the model is also given documents
from another section. Our test section is always the VS (vision sciences) section, while
the additional section is varied across the other eight. The training set always consist of
80 documents from the other section (so that larger sections like AA (algorithms and ar-
chitecures) do not get an unfair advantage), plus between 0 and 80 documents from VS.
There are 47 test documents, which are held xed as we vary over the other section and the
number N of training VS documents. We compared 3 different models for this task. The
rst model (M1) simply ignores documents from the additional section, and uses a HDP to
model the VS documents. It serves as a baseline. The second model (M2) uses a HDP mix-
ture model, with one group per document, but lumping together training documents from
both sections. The third model (M3) takes a hierarchical approach and models each section
separately using a HDP mixture model, and places another DP prior over the common base
distributions for both submodels4.
As we see in Figure 3 left, the more hierarchical approach of M3 performs best, with per-
plexity decreasing drastically with modest values of N , while M1 does worst for small N .
However with increasing N , M1 improves until it is competitive with M3 but M2 does
worst. This is because M2 lumps all the documents together, so is not able to differentiate
between the sections, as a result the inuence of documents from the other section is un-
duly strong. This result conrms that the hierarchical approach to the transfer-of-learning
problem is a useful one, as it allows useful information to be transfered to a new task (here
the modeling of a new section), without the data from the previous tasks overwhelming
those in the new task.

We also looked at the performance of the M3 model on VS documents given specic other
sections. This is shown in Figure 3 right. As expected, the performance is worst given LT
(learning theory), and improves as we move to AA and AP (applications). In Table 1 we
show the topics pertinent to VS discovered by the M3 model. First we trained the model
on all documents from the other section. Then, keeping the assignments of words to topics
xed in the other section, we introduced VS documents and the model decides to reuse
some topics from the other section, as well as create new ones. The topics reused by VS
documents conrm to our expectations of the overlap between VS and other sections.

as words occurring more than 4000 or less than 50 times in the documents. As sections differ over
the years, we assigned by hand the various sections to one of 9 prototypical sections: CS, NS, LT,
AA, IM, SP, VS, AP and CN.

4Though we have only described the 2 layer HDP the 3 layer extension is straightforward. In
fact on our website http://www.cs.berkeley.edu/ywteh/research/npbayes we have an implementation of the
general case where DPs are coupled hierarchically in a tree-structured model.

CS

AA

CN

IM
processing
pattern
approach
architecture
single shows
simple based
large control

NS
cells cell
activity
response
neuron visual
patterns
pattern single
g

LT
signal layer
gaussian cells
g nonlinearity
nonlinear rate
eq cell

SP

visual images
video language
image pixel
acoustic delta
lowpass ow

AP
approach
based trained
test layer
features table
classication
rate paper

algorithms test
approach
methods based
point problems
form large
paper

task
representation
pattern
processing
trained
representations
three process
unit patterns
examples
concept
similarity
bayesian
hypotheses
generalization
numbers
positive classes
hypothesis
Table 1: Topics shared between VS and the other sections. Shown are the two topics with most
numbers of VS words, but also with signicant numbers of words from the other section.

distance
tangent image
images
transformation
transformations
pattern vectors
convolution
simard

visual cells
cortical
orientation
receptive
contrast spatial
cortex stimulus
tuning

ii tree pomdp
observable
strategy class
stochastic
history
strategies
density

motion visual
velocity ow
target chip eye
smooth
direction optical

signals
separation
signal sources
source matrix
blind mixing
gradient eq

image images
face similarity
pixel visual
database
matching facial
examples

policy optimal
reinforcement
control action
states actions
step problems
goal

large examples
form point see
parameter
consider
random small
optimal

Alice in Wonderland. The innite hidden Markov model (iHMM) is a nonparametric
model for sequential data where the number of hidden states is open-ended and inferred
from data [11]. In [10] we show that the HDP framework can be applied to obtain a cleaner
formulation of the iHMM, providing effective new inference algorithms and potentially hi-
erarchical extensions. In fact the original iHMM paper [11] served as inspiration for this
work and rst coined the term hierarchical Dirichlet processesthough their model is
not hierarchical in the Bayesian sense, involving priors upon priors, but is rather a set of
coupled urn models similar to the CRF. Here we report experimental comparisons of the
iHMM against other approaches on sentences taken from Lewis Carrolls Alices Adven-
tures in Wonderland.

y
t
i
x
e
l
p
r
e
P

50

40

30

20

10

0
0

Perplexity on test sentences of Alice

ML
MAP
VB

5

20
10
Number of hidden states

15

25

30

Figure 4: Comparing iHMM (horizontal line)
versus ML, MAP and VB trained HMMs. Er-
ror bars are 1 standard error (those for iHMM too
small to see).

6 Discussion

ML, MAP, and variational Bayesian (VB)
[12] models with numbers of states rang-
ing from 1 to 30 were trained multiple
times on 20 sentences of average length
51 symbols (27 distinct symbols, consist-
ing of 26 letters and  ), and tested on
40 sequences of average length 100. Fig-
ure 4 shows the perplexity of test sen-
tences. For VB, the predictive probability
is intractable to compute, so the modal set-
ting of parameters was used. Both MAP
and VB models were given optimal set-
tings of the hyperparameters found in the
iHMM. We see that the iHMM has a lower
perlexity than every model size for ML,
MAP, and VB, and obtains this with one
countably innite model.

We have described the hierarchical Dirichlet process, a hierarchical, nonparametric model
for clustering problems involving multiple groups of data. HDP mixture models are able
to automatically determine the appropriate number of mixture components needed, and
exhibit sharing of statistical strength across groups by having components shared across
groups. We have described the HDP as a distribution over distributions, using both the
stick-breaking construction and the Chinese restaurant franchise. In [10] we also describe
a fourth perspective based on the innite limit of nite mixture models, and give detail for

how the HDP can be applied to the iHMM. Direct extensions of the model include use of
nonparametric priors other than the DP, building higher level hierarchies as in our NIPS
experiment, as well as hierarchical extensions to the iHMM.
Appendix: Gibbs Sampling in the CRF
The CRF is dened by the variables t = (tji), k = (kjt), and  = (k). We describe an
inference procedure for the HDP mixture model based on Gibbs sampling t, k and  given
data items x. For the full derivation see [10]. Let f (|) and h be the density functions for
F () and H respectively, ni
be
the number of kj 0t0s equal to k except kjt. The conditional probability for tji given the
other variables is proportional to the product of a prior and likelihood term. The prior term
is given by (7) where, by exchangeability, we can take tji to be the last one assigned. The
likelihood is given by f (xji|kjt ) where for t = tnew we may sample kjtnew using (8), and
knew  H. The distribution is then:

jt be the number of tji0 s equal to t except tji, and mjt

k

p(tji = t | t\tji, k, , x) (cid:26)0f (xji|kjt )

jt f (xji|kjt )

ni

if t = tnew
if t currently used.

Similarly the conditional distributions for kjt and k are:

p(kjt = k | t, k\kjt, , x) (Qi:tji =t f (xji|k)
k Qi:tji=t f (xji|k)

p(k | t, k, \k, x)  h(k) Yji:kjtji =k

f (xji|k)

mt

if k = knew
if k currently used.

(9)

(10)

(11)

where knew  H. If H is conjugate to F () we have the option of integrating out .

