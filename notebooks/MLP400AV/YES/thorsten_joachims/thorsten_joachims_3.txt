ABSTRACT
Machine learning is commonly used to improve ranked re-
trieval systems. Due to computational diculties, few learn-
ing techniques have been developed to directly optimize for
mean average precision (MAP), despite its widespread use
in evaluating such systems. Existing approaches optimiz-
ing MAP either do not nd a globally optimal solution,
or are computationally expensive. In contrast, we present
a general SVM learning algorithm that eciently nds a
globally optimal solution to a straightforward relaxation of
MAP. We evaluate our approach using the TREC 9 and
TREC 10 Web Track corpora (WT10g), comparing against
SVMs optimized for accuracy and ROCArea. In most cases
we show our method to produce statistically signicant im-
provements in MAP scores.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models

General Terms
Algorithm, Theory, Experimentation

Keywords
Machine Learning for Information Retrieval, Support Vector
Machines, Ranking

1.

INTRODUCTION

State of the art information retrieval systems commonly
use machine learning techniques to learn ranking functions.
However, most current approaches do not optimize for the
evaluation measure most often used, namely Mean Average
Precision (MAP).

Instead, current algorithms tend to take one of two gen-
eral approaches. The rst approach is to learn a model that
estimates the probability of a document being relevant given

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
SIGIR07, July 2327, 2007, Amsterdam, The Netherlands.
Copyright 2007 ACM 978-1-59593-597-7/07/0007 ...$5.00.

a query (e.g., [18, 14]). If solved eectively, the ranking with
best MAP performance can easily be derived from the prob-
abilities of relevance. However, achieving high MAP only
requires nding a good ordering of the documents. As a re-
sult, nding good probabilities requires solving a more di-
cult problem than necessary, likely requiring more training
data to achieve the same MAP performance.

The second common approach is to learn a function that
maximizes a surrogate measure. Performance measures op-
timized include accuracy [17, 15], ROCArea [1, 5, 10, 11,
13, 21] or modications of ROCArea [4], and NDCG [2, 3].
Learning a model to optimize for such measures might result
in suboptimal MAP performance.
In fact, although some
previous systems have obtained good MAP performance, it
is known that neither achieving optimal accuracy nor ROC-
Area can guarantee optimal MAP performance[7].

In this paper, we present a general approach for learning
ranking functions that maximize MAP performance. Specif-
ically, we present an SVM algorithm that globally optimizes
a hinge-loss relaxation of MAP. This approach simplies
the process of obtaining ranking functions with high MAP
performance by avoiding additional intermediate steps and
heuristics. The new algorithm also makes it conceptually
just as easy to optimize SVMs for MAP as was previously
possible only for accuracy and ROCArea.

In contrast to recent work directly optimizing for MAP
performance by Metzler & Croft [16] and Caruana et al.
[6], our technique is computationally ecient while nding
a globally optimal solution. Like [6, 16], our method learns
a linear model, but is much more ecient in practice and,
unlike [16], can handle many thousands of features.

We now describe the algorithm in detail and provide proof
of correctness. Following this, we provide an analysis of run-
ning time. We nish with empirical results from experiments
on the TREC 9 and TREC 10 Web Track corpus. We have
also developed a software package implementing our algo-
rithm that is available for public use1.

2. THE LEARNING PROBLEM
Following the standard machine learning setup, our goal
is to learn a function h : X  Y between an input space
X (all possible queries) and output space Y (rankings over
a corpus). In order to quantify the quality of a prediction,
y = h(x), we will consider a loss function  : Y  Y  <.
(y, y) quanties the penalty for making prediction y if the
correct output is y. The loss function allows us to incorpo-
rate specic performance measures, which we will exploit
1http://svmrank.yisongyue.com

for optimizing MAP. We restrict ourselves to the supervised
learning scenario, where input/output pairs (x, y) are avail-
able for training and are assumed to come from some xed
distribution P (x, y). The goal is to nd a function h such
that the risk (i.e., expected loss),

R

P (h) = ZXY

(y, h(x))dP (x, y),

is minimized. Of course, P (x, y) is unknown. But given
a nite set of training pairs, S = {(xi, yi)  X  Y : i =
1, . . . , n}, the performance of h on S can be measured by
the empirical risk,

R

S (h) =

1
n

n

Xi=1

(yi, h(xi)).

In the case of learning a ranked retrieval function, X de-
notes a space of queries, and Y the space of (possibly weak)
rankings over some corpus of documents C = {d 1, . . . ,d|C|}.

We can dene average precision loss as

map(y, y) = 1  MAP(rank(y), rank(y)),

where rank(y) is a vector of the rank values of each doc-
ument in C. For example, for a corpus of two documents,
{d1, d2}, with d1 having higher rank than d2, rank(y) =
(1, 0). We assume true rankings have two rank values, where
relevant documents have rank value 1 and non-relevant doc-
uments rank value 0. We further assume that all predicted
rankings are complete rankings (no ties).

Let p = rank(y) and p = rank(y). The average precision

score is dened as

MAP(p, p) =

1

rel Xj:pj =1

P rec@j,

where rel = |{i : pi = 1}| is the number of relevant docu-
ments, and P rec@j is the percentage of relevant documents
in the top j documents in predicted ranking y. MAP is the
mean of the average precision scores of a group of queries.
2.1 MAP vs ROCArea

Most learning algorithms optimize for accuracy or ROC-
Area. While optimizing for these measures might achieve
good MAP performance, we use two simple examples to
show it can also be suboptimal in terms of MAP.

ROCArea assigns equal penalty to each misordering of a
relevant/non-relevant pair. In contrast, MAP assigns greater
penalties to misorderings higher up in the predicted ranking.
Using our notation, ROCArea can be dened as

ROC(p, p) =

1

rel  (|C|  rel) Xi:pi=1 Xj:pj =0

1[ pi> pj ],

where p is the true (weak) ranking, p is the predicted rank-
ing, and 1[b] is the indicator function conditioned on b.

Doc ID

p

rank(h1(x))
rank(h2(x))

1
1
8
1

2
0
7
2

3
0
6
3

4
0
5
4

5
0
4
5

6
1
3
6

7
1
2
7

8
0
1
8

Table 1: Toy Example and Models

Suppose we have a hypothesis space with only two hy-
pothesis functions, h1 and h2, as shown in Table 1. These

two hypotheses predict a ranking for query x over a corpus
of eight documents.

Hypothesis MAP ROCArea

h1(x)
h2(x)

0.59
0.51

0.47
0.53

Table 2: Performance of Toy Models

Table 2 shows the MAP and ROCArea scores of h1 and
h2. Here, a learning method which optimizes for ROC-
Area would choose h2 since that results in a higher ROC-
Area score, but this yields a suboptimal MAP score.
2.2 MAP vs Accuracy

Using a very similar example, we now demonstrate how
optimizing for accuracy might result in suboptimal MAP.
Models which optimize for accuracy are not directly con-
cerned with the ranking.
Instead, they learn a threshold
such that documents scoring higher than the threshold can
be classied as relevant and documents scoring lower as non-
relevant.

Doc ID

p

rank(h1(x))
rank(h2(x))

1
1
11
1

2
0
10
2

3
0
9
3

4
0
8
4

5
0
7
5

6
1
6
6

7
1
5
7

8
1
4
8

9
1
3
9

10
0
2
10

11
0
1
11

Table 3: Toy Example and Models

We consider again a hypothesis space with two hypothe-
ses. Table 3 shows the predictions of the two hypotheses on
a single query x.

Hypothesis MAP Best Acc.

h1(q)
h2(q)

0.56
0.51

0.64
0.73

Table 4: Performance of Toy Models

Table 4 shows the MAP and best accuracy scores of h1(q)
and h2(q). The best accuracy refers to the highest achiev-
able accuracy on that ranking when considering all possi-
ble thresholds. For instance, with h1(q), a threshold be-
tween documents 1 and 2 gives 4 errors (documents 6-9 in-
correctly classied as non-relevant), yielding an accuracy of
0.64. Similarly, with h2(q), a threshold between documents
5 and 6 gives 3 errors (documents 10-11 incorrectly classi-
ed as relevant, and document 1 as non-relevant), yielding
an accuracy of 0.73. A learning method which optimizes
for accuracy would choose h2 since that results in a higher
accuracy score, but this yields a suboptimal MAP score.

3. OPTIMIZING AVERAGE PRECISION

We build upon the approach used by [13] for optimiz-
ing ROCArea. Unlike ROCArea, however, MAP does not
decompose linearly in the examples and requires a substan-
tially extended algorithm, which we describe in this section.
Recall that the true ranking is a weak ranking with two
rank values (relevant and non-relevant). Let Cx and C x de-
note the set of relevant and non-relevant documents of C for
query x, respectively.

S (w)  R

We focus on functions which are parametrized by a weight
vector w, and thus wish to nd w to minimize the empirical
S (h(; w)). Our approach is to learn a
risk, R
discriminant function F : X  Y  < over input-output
pairs. Given query x, we can derive a prediction by nding
the ranking y that maximizes the discriminant function:

h(x; w) = argmax

yY

F (x, y; w).

(1)

We assume F to be linear in some combined feature repre-
sentation of inputs and outputs (x, y)  RN , i.e.,

F (x, y; w) = wT (x, y).

(2)

The combined feature function we use is

(x, y) =

1

|C x|  |C x| Xi:diCx Xj:djC x

[yij ((x, di)  (x, dj))] ,

where  : X  C  <N is a feature mapping function from
a query/document pair to a point in N dimensional space2.
We represent rankings as a matrix of pairwise orderings,
Y  {1, 0, +1}|C||C|. For any y  Y, yij = +1 if di is
ranked ahead of dj, and yij = 1 if dj is ranked ahead of di,
and yij = 0 if di and dj have equal rank. We consider only
matrices which correspond to valid rankings (i.e, obeying
antisymmetry and transitivity). Intuitively,  is a summa-
tion over the vector dierences of all relevant/non-relevant
document pairings. Since we assume predicted rankings to
be complete rankings, yij is either +1 or 1 (never 0).

Given a learned weight vector w, predicting a ranking (i.e.
solving equation (1)) given query x reduces to picking each
yij to maximize wT (x, y). As is also discussed in [13],
this is attained by sorting the documents by wT (x, d) in
descending order. We will discuss later the choices of  we
used for our experiments.
3.1 Structural SVMs

The above formulation is very similar to learning a straight-
forward linear model while training on the pairwise dif-
ference of relevant/non-relevant document pairings. Many
SVM-based approaches optimize over these pairwise dier-
ences (e.g., [5, 10, 13, 4]), although these methods do not
optimize for MAP during training. Previously, it was not
clear how to incorporate non-linear multivariate loss func-
tions such as MAP loss directly into global optimization
problems such as SVM training. We now present a method
based on structural SVMs [19] to address this problem.
timization Problem 1, to learn a w  RN .

We use the structural SVM formulation, presented in Op-

Optimization Problem 1. (Structural SVM)

min
w,0

kwk2 +
s.t. i,y  Y \ yi :

1
2

C
n

n

Xi=1

i

(3)

(4)

wT (xi, yi)  wT (xi, y) + (yi, y)  i

The objective function to be minimized (3) is a tradeo
between model complexity, kwk2, and a hinge loss relaxation
of MAP loss, P i. As is usual in SVM training, C is a

2For example, one dimension might be the number of times
the query words appear in the document.

for i = 1, . . . , n do

Algorithm 1 Cutting plane algorithm for solving OP 1
within tolerance .
1: Input: (x1, y1), . . . , (xn, yn), C, 
2: Wi   for all i = 1, . . . , n
3: repeat
4:
5:
6:
7:
8:
Wi  Wi  {y}
9:
w  optimize (3) over W = Si Wi
10:
11:
12:
13: until no Wi has changed during iteration

H(y; w)  (yi, y) + wT (xi, y)  wT (xi, yi)
compute y = argmaxyY H(y; w)
compute i = max{0, maxyWi H(y; w)}
if H(y; w) > i +  then

end if
end for

parameter that controls this tradeo and can be tuned to
achieve good performance in dierent training tasks.

For each (xi, yi) in the training set, a set of constraints
of the form in equation (4) is added to the optimization
problem. Note that wT (x, y) is exactly our discriminant
function F (x, y; w) (see equation (2)). During prediction,
our model chooses the ranking which maximizes the discrim-
inant (1). If the discriminant value for an incorrect ranking
y is greater than for the true ranking yi (e.g., F (xi, y; w) >
F (xi, yi; w)), then corresponding slack variable, i, must be
at least (yi, y) for that constraint to be satised. There-

This is stated formally in Proposition 1.

fore, the sum of slacks, P i, upper bounds the MAP loss.
Proposition 1. Let (w) be the optimal solution of the
slack variables for OP 1 for a given weight vector w. Then
n Pn
1
S (w).

i=1 i is an upper bound on the empirical risk R

(see [19] for proof )

Proposition 1 shows that OP 1 learns a ranking function
that optimizes an upper bound on MAP error on the train-
ing set. Unfortunately there is a problem: a constraint is
required for every possible wrong output y, and the num-
ber of possible wrong outputs is exponential in the size of
C. Fortunately, we may employ Algorithm 1 to solve OP 1.
Algorithm 1 is a cutting plane algorithm, iteratively intro-
ducing constraints until we have solved the original problem
within a desired tolerance  [19]. The algorithm starts with
no constraints, and iteratively nds for each example (xi, yi)
the output y associated with the most violated constraint.
If the corresponding constraint is violated by more than  we
introduce y into the working set Wi of active constraints for
example i, and re-solve (3) using the updated W. It can be
shown that Algorithm 1s outer loop is guaranteed to halt
within a polynomial number of iterations for any desired
precision .

Theorem 1. Let R = maxi maxy k(xi, yi)  (xi, y)k,
 = maxi maxy (yi, y), and for any  > 0, Algorithm 1
terminates after adding at most

max 2n 



,

8C  R2
2 

constraints to the working set W. (see [19] for proof )

However, within the inner loop of this algorithm we have

to compute argmaxyY H(y; w), where

H(y; w) = (yi, y) + wT (xi, y)  wT (xi, yi),

or equivalently,

argmax

yY

(yi, y) + wT (xi, y),

since wT (xi, yi) is constant with respect to y. Though
closely related to the classication procedure, this has the
substantial complication that we must contend with the ad-
ditional (yi, y) term. Without the ability to eciently nd
the most violated constraint (i.e., solve argmaxyY H(y, w)),
the constraint generation procedure is not tractable.
3.2 Finding the Most Violated Constraint

Using OP 1 and optimizing to ROCArea loss (roc), the
problem of nding the most violated constraint, or solving
argmaxyY H(y, w) (henceforth argmax H), is addressed in
[13]. Solving argmax H for map is more dicult. This is
primarily because ROCArea decomposes nicely into a sum
of scores computed independently on each relative order-
ing of a relevant/non-relevant document pair. MAP, on the
other hand, does not decompose in the same way as ROC-
Area. The main algorithmic contribution of this paper is an
ecient method for solving argmax H for map.

One useful property of map is that it is invariant to swap-
ping two documents with equal relevance. For example, if
documents da and db are both relevant, then swapping the
positions of da and db in any ranking does not aect map.
By extension, map is invariant to any arbitrary permuta-
tion of the relevant documents amongst themselves and of
the non-relevant documents amongst themselves. However,
this reshuing will aect the discriminant score, wT (x, y).
This leads us to Observation 1.

Observation 1. Consider rankings which are constrained
by xing the relevance at each position in the ranking (e.g.,
the 3rd document in the ranking must be relevant). Every
ranking which satises the same set of constraints will have
the same map.
If the relevant documents are sorted by
wT (x, d) in descending order, and the non-relevant docu-
ments are likewise sorted by wT (x, d), then the interleav-
ing of the two sorted lists which satises the constraints will
maximize H for that constrained set of rankings.

Observation 1 implies that in the ranking which maxi-
mizes H, the relevant documents will be sorted by wT (x, d),
and the non-relevant documents will also be sorted likewise.
By rst sorting the relevant and non-relevant documents,
the problem is simplied to nding the optimal interleaving
of two sorted lists. For the rest of our discussion, we assume
that the relevant documents and non-relevant documents
are both sorted by descending wT (x, d). For convenience,
we also refer to relevant documents as {dx
1 , . . . dx|Cx|} = Cx,
and non-relevant documents as {dx

1 , . . . dx|C x|} = C x.

We dene j(i1, i2), with i1 < i2, as the change in H from
when the highest ranked relevant document ranked after dx
j
is dx

i1 . For i2 = i1 + 1, we have

i2 to when it is dx
|Cx|  j

1

j(i, i + 1) =

j + i

j + i  1  2  (sx
 j  1

i  sx
j )
|Cx|  |C x|

, (5)

where si = wT (x, di). The rst term in (5) is the change
in map when the ith relevant document has j non-relevant

documents ranked before it, as opposed to j1. The second
term is the change in the discriminant score, wT (x, y),
when yij changes from +1 to 1.

. . . , dx
. . . , dx

i , dx
j , dx

j , dx
i , dx

i+1, . . .

i+1, . . .

Figure 1: Example for j(i, i + 1)

Figure 1 gives a conceptual example for j(i, i + 1). The
bottom ranking diers from the top only where dx
j slides up
one rank. The dierence in the value of H for these two
rankings is exactly j(i, i + 1).

For any i1 < i2, we can then dene j(i1, i2) as

j(i1, i2) =

i21
Xk=i1

or equivalently,
i21
Xk=i1

j(i1, i2) =

 1
|Cx|  j

j + k

j(k, k + 1),

(6)

j + k  1  2  (sx
 j  1

|Cx|  |C x|  .

k  sx
j )

Let o1, . . . , o|C x| encode the positions of the non-relevant
documents, where dx
oj is the highest ranked relevant docu-
ment ranked after the jth non-relevant document. Due to
Observation 1, this encoding uniquely identies a complete
ranking. We can recover the ranking as

0
sign(si  sj)
sign(oj0  i0  0.5)
sign(j0  oi0 + 0.5)

if i = j
if di, dj equal relevance
if di = dx
if di = dx

i0 , dj = dx
j0
i0 , dj = dx
j0

. (7)

yij =

8>>><
>>>:

We can now reformulate H into a new objective function,

0

(o1, . . . , o|C x||w) = H(y|w) +

H

|C x|

Xk=1

k(ok,|Cx| + 1),

where y is the true (weak) ranking. Conceptually H0 starts
with a perfect ranking y, and adds the change in H when
each successive non-relevant document slides up the ranking.

We can then reformulate the argmax H problem as

argmax H

0

= argmax
o1,...,o|C x|

|C x|

Xk=1

k(ok,|Cx| + 1)

s.t.

o1  . . .  o|C x|.

(8)

(9)

Algorithm 2 describes the algorithm used to solve equa-
tion (8). Conceptually, Algorithm 2 starts with a perfect
ranking. Then for each successive non-relevant document,
the algorithm modies the solution by sliding that docu-
ment up the ranking to locally maximize H0 while keeping
the positions of the other non-relevant documents constant.
3.2.1 Proof of Correctness
Algorithm 2 is greedy in the sense that it nds the best
position of each non-relevant document independently from
the other non-relevant documents. In other words, the al-
gorithm maximizes H0 for each non-relevant document, dx
j ,

Algorithm 2 Finding the Most Violated Constraint
(argmax H) for Algorithm 1 with map
1: Input: w, Cx, C x
2: sort Cx and C x in descending order of wT (x, d)
i ), i = 1, . . . ,|Cx|
i  wT (x, dx
3: sx
i ), i = 1, . . . ,|C x|
i  wT (x, dx
4: sx
5: for j = 1, . . . ,|C x| do
optj  argmaxk j(k,|Cx| + 1)
6:
7: end for
8: encode y according to (7)
9: return y

without considering the positions of the other non-relevant
documents, and thus ignores the constraints of (9).
In order for the solution to be feasible, then jth non-
relevant document must be ranked after the rst j  1 non-
relevant documents, thus satisfying

opt1  opt2  . . .  opt|C x|.

(10)

If the solution is feasible, the it clearly solves (8). Therefore,
it suces to prove that Algorithm 2 satises (10). We rst
prove that j(,) is monotonically decreasing in j.

Lemma 1. For any 1  i1 < i2  |Cx| + 1 and 1  j <

|C x|, it must be the case that

j+1(i1, i2)  j(i1, i2).

Proof. Recall from (6) that both j(i1, i2) and j+1(i1, i2)
are summations of i2  i1 terms. We will show that each
term in the summation of j+1(i1, i2) is no greater than the
corresponding term in j(i1, i2), or

j+1(k, k + 1)  j(k, k + 1)

for k = i1, . . . , i2  1.

Each term in j(k, k + 1) and j+1(k, k + 1) can be further
decomposed into two parts (see (5)). We will show that each
part of j+1(k, k + 1) is no greater than the corresponding
part in j(k, k + 1). In other words, we will show that both

j + 1

j + k + 1

 j

j + k

 j

j + k

 j  1
j + k  1

(11)

(12)

and

 2  (sx

k  sx
|Cx|  |C x|   2  (sx

k  sx
j )
|Cx|  |C x|

j+1)

It is easy to see that (11) is true by observing that for any

are true for the aforementioned values of j and k.
two positive integers 1  a < b,
 a
b

 a  1
b  1

a + 1
b + 1

 a
b

,

and choosing a = j and b = j + k.

The second inequality (12) holds because Algorithm 2 rst

sorts dx in descending order of sx, implying sx

j+1  sx
j .

Thus we see that each term in j+1 is no greater than the

corresponding term in j, which completes the proof.

The result of Lemma 1 leads directly to our main correct-

ness result:

Theorem 2. In Algorithm 2, the computed values of optj
satisfy (10), implying that the solution returned by Algorithm
2 is feasible and thus optimal.

Proof. We will prove that

optj  optj+1

holds for any 1  j < |C x|, thus implying (10).

Since Algorithm 2 computes optj as

optj = argmax

j(k,|Cx| + 1),

(13)

then by denition of j (6), for any 1  i < optj,

k

j(i, optj) = j(i,|Cx| + 1)  j(optj,|Cx| + 1) < 0.

Using Lemma 1, we know that

j+1(i, optj)  j(i, optj) < 0,

which implies that for any 1  i < optj,

j+1(i,|Cx| + 1)  j+1(optj,|Cx| + 1) < 0.
Suppose for contradiction that optj+1 < optj. Then
j+1(optj+1,|Cx| + 1) < j+1(optj,|Cx| + 1),

which contradicts (13). Therefore, it must be the case that
optj  optj+1, which completes the proof.
3.2.2 Running Time
The running time of Algorithm 2 can be split into two
parts. The rst part is the sort by wT (x, d), which re-
quires O(n log n) time, where n = |Cx| + |C x|. The second
part computes each optj, which requires O(|Cx|  |C x|) time.
Though in the worst case this is O(n2), the number of rel-
evant documents, |Cx|, is often very small (e.g., constant
with respect to n), in which case the running time for the
second part is simply O(n). For most real-world datasets,
Algorithm 2 is dominated by the sort and has complexity
O(n log n).

Algorithm 1 is guaranteed to halt in a polynomial num-
ber of iterations [19], and each iteration runs Algorithm 2.
Virtually all well-performing models were trained in a rea-
sonable amount of time (usually less than one hour). Once
training is complete, making predictions on query x us-
ing the resulting hypothesis h(x|w) requires only sorting
by wT (x, d).

We developed our software using a Python interface3 to
SVMstruct, since the Python language greatly simplied the
coding process. To improve performance, it is advisable to
use the standard C implementation4 of SVMstruct.

4. EXPERIMENT SETUP

The main goal of our experiments is to evaluate whether
directly optimizing MAP leads to improved MAP perfor-
mance compared to conventional SVM methods that opti-
mize a substitute loss such as accuracy or ROCArea. We
empirically evaluate our method using two sets of TREC
Web Track queries, one each from TREC 9 and TREC 10
(topics 451-500 and 501-550), both of which used the WT10g
corpus. For each query, TREC provides the relevance judg-
ments of the documents. We generated our features using
the scores of existing retrieval functions on these queries.
While our method is agnostic to the meaning of the fea-
tures, we chose to use existing retrieval functions as a simple
yet eective way of acquiring useful features. As such, our
3http://www.cs.cornell.edu/~tomf/svmpython/
4http://svmlight.joachims.org/svm_struct.html

Base Funcs Features

TREC 9

TREC 10

Dataset
TREC 9 Indri
TREC 10 Indri
TREC 9 Submissions
TREC 10 Submissions

15
15
53
18

750
750
2650
900

Table 5: Dataset Statistics

experiments essentially test our methods ability to re-rank
the highly ranked documents (e.g., re-combine the scores of
the retrieval functions) to improve MAP.

We compare our method against the best retrieval func-
tions trained on (henceforth base functions), as well as against
previously proposed SVM methods. Comparing with the
best base functions tests our methods ability to learn a use-
ful combination. Comparing with previous SVM methods
allows us to test whether optimizing directly for MAP (as
opposed to accuracy or ROCArea) achieves a higher MAP
score in practice. The rest of this section describes the base
functions and the feature generation method in detail.
4.1 Choosing Retrieval Functions

We chose two sets of base functions for our experiments.
For the rst set, we generated three indices over the WT10g
corpus using Indri5. The rst index was generated using
default settings, the second used Porter-stemming, and the
last used Porter-stemming and Indris default stopwords.

For both TREC 9 and TREC 10, we used the descrip-
tion portion of each query and scored the documents using
ve of Indris built-in retrieval methods, which are Cosine
Similarity, TFIDF, Okapi, Language Model with Dirichlet
Prior, and Language Model with Jelinek-Mercer Prior. All
parameters were kept as their defaults.

We computed the scores of these ve retrieval methods
over the three indices, giving 15 base functions in total. For
each query, we considered the scores of documents found in
the union of the top 1000 documents of each base function.
For our second set of base functions, we used scores from
the TREC 9 [8] and TREC 10 [9] Web Track submissions.
We used only the non-manual, non-short submissions from
both years. For TREC 9 and TREC 10, there were 53 and
18 such submissions, respectively. A typical submission con-
tained scores of its top 1000 documents.

)
d
,

x
(

T
w

f (d|x)

Figure 2: Example Feature Binning

4.2 Generating Features

In order to generate input examples for our method, a
concrete instantiation of  must be provided. For each doc-
5http://www.lemurproject.org

map

Model
SVM
Best Func.
2nd Best
3rd Best



MAP W/L
0.242
0.204
0.199
0.188

39/11 **
38/12 **
34/16 **



MAP W/L
0.236
0.181
0.174
0.174

37/13 **
43/7 **
38/12 **

Table 6: Comparison with Indri Functions

ument d scored by a set of retrieval functions F on query x,
we generate the features as a vector

(x, d) = h1[f (d|x)>k] : f  F,k  Kfi,

where f (d|x) denotes the score that retrieval function f as-
signs to document d for query x, and each Kf is a set of
real values. From a high level, we are expressing the score
of each retrieval function using |Kf| + 1 bins.

Since we are using linear kernels, one can think of the
learning problem as nding a good piecewise-constant com-
bination of the scores of the retrieval functions. Figure 2
shows an example of our feature mapping method. In this
example we have a single feature F = {f}. Here, Kf =
{a, b, c}, and the weight vector is w = hwa, wb, wci. For any
document d and query x, we have

wT (x, d) =

0
wa
wa + wb
wa + wb + wc

if f (d|x) < a
if a  f (d|x) < b
if b  f (d|x) < c
if c  f (d|x)

.

8>><
>>:

This is expressed qualitatively in Figure 2, where wa and wb
are positive, and wc is negative.
We ran our main experiments using four choices of F : the
set of aforementioned Indri retrieval functions for TREC 9
and TREC 10, and the Web Track submissions for TREC
9 and TREC 10. For each F and each function f  F ,
we chose 50 values for Kf which are reasonably spaced and
capture the sensitive region of f .
Using the four choices of F , we generated four datasets
for our main experiments. Table 5 contains statistics of
the generated datasets. There are many ways to generate
features, and we are not advocating our method over others.
This was simply an ecient means to normalize the outputs
of dierent functions and allow for a more expressive model.

5. EXPERIMENTS

For each dataset in Table 5, we performed 50 trials. For
each trial, we train on 10 randomly selected queries, and se-
lect another 5 queries at random for a validation set. Mod-
els were trained using a wide range of C values. The model
which performed best on the validation set was selected and
tested on the remaining 35 queries.

map), an SVM optimizing for ROCArea (SVM

All queries were selected to be in the training, validation
and test sets the same number of times. Using this setup,
we performed the same experiments while using our method
(SVM
roc) [13],
and a conventional classication SVM (SVMacc) [20]. All
SVM methods used a linear kernel. We reported the average
performance of all models over the 50 trials.
5.1 Comparison with Base Functions

In analyzing our results, the rst question to answer is,
map learn a model which outperforms the best base

can SVM

bcaTREC 9

TREC 10

TREC 9

TREC 10

map

Model
SVM
Best Func.
2nd Best
3rd Best



MAP W/L
0.290
0.280
0.269
0.266

28/22
30/20
30/20



MAP W/L
0.287
0.283
0.251
0.233

36/14 **
36/14 **

29/21

Table 7: Comparison with TREC Submissions

TREC 9

TREC 10

map

Model
SVM
Best Func.
2nd Best
3rd Best



MAP W/L
0.284
0.280
0.269
0.266

27/23
30/20
30/20



MAP W/L
0.288
0.283
0.251
0.233

36/14 **
35/15 **

31/19

Table 8: Comparison with TREC Subm. (w/o best)

functions? Table 6 presents the comparison of SVM
map with
the best Indri base functions. Each column group contains
the macro-averaged MAP performance of SVM
map or a base
function. The W/L columns show the number of queries
where SVM
map achieved a higher MAP score. Signicance
tests were performed using the two-tailed Wilcoxon signed
rank test. Two stars indicate a signicance level of 0.95.
All tables displaying our experimental results are structured
identically. Here, we nd that SVM
map signicantly outper-
forms the best base functions.

Table 7 shows the comparison when trained on TREC sub-
missions. While achieving a higher MAP score than the best
base functions, the performance dierence between SVM
map
the base functions is not signicant. Given that many of
these submissions use scoring functions which are carefully
crafted to achieve high MAP, it is possible that the best
performing submissions use techniques which subsume the
techniques of the other submissions. As a result, SVM
map
would not be able to learn a hypothesis which can signi-
cantly out-perform the best submission.

Hence, we ran the same experiments using a modied
dataset where the features computed using the best submis-
sion were removed. Table 8 shows the results (note that we
are still comparing against the best submission though we
are not using it for training). Notice that while the perfor-
mance of SVM
map degraded slightly, the performance was
still comparable with that of the best submission.
5.2 Comparison w/ Previous SVM Methods

The next question to answer is, does SVM

map produce
higher MAP scores than previous SVM methods? Tables 9
and 10 present the results of SVM
roc, and SVMacc
when trained on the Indri retrieval functions and TREC sub-
missions, respectively. Table 11 contains the corresponding
results when trained on the TREC submissions without the
best submission.

map, SVM

map and SVM

To start with, our results indicate that SVMacc was not
competitive with SVM
roc, and at times un-
derperformed dramatically. As such, we tried several ap-
proaches to improve the performance of SVMacc.
5.2.1 Alternate SVMacc Methods
One issue which may cause SVMacc to underperform is
the severe imbalance between relevant and non-relevant doc-

map

Model
SVM
SVM
roc
SVMacc
SVMacc2
SVMacc3
SVMacc4



MAP W/L
0.242
0.237
0.147
0.219
0.113
0.155

29/21
47/3 **
39/11 **
49/1 **
48/2 **



MAP W/L
0.236
0.234
0.155
0.207
0.153
0.155

24/26
47/3 **
43/7 **
45/5 **
48/2 **

Table 9: Trained on Indri Functions

TREC 9

TREC 10

map

Model
SVM
SVM
roc
SVMacc
SVMacc2
SVMacc3
SVMacc4



MAP W/L
0.290
0.282
0.213
0.270
0.133
0.233

29/21
49/1 **
34/16 **
50/0 **
47/3 **



MAP W/L
0.287
0.278
0.222
0.261
0.182
0.238

35/15 **
49/1 **
42/8 **
46/4 **
46/4 **

Table 10: Trained on TREC Submissions

uments. The vast majority of the documents are not rele-
vant. SVMacc2 addresses this problem by assigning more
penalty to false negative errors. For each dataset, the ratio
of the false negative to false positive penalties is equal to the
ratio of the number non-relevant and relevant documents in
that dataset. Tables 9, 10 and 11 indicate that SVMacc2 still
performs signicantly worse than SVM

map.

Another possible issue is that SVMacc attempts to nd
just one discriminating threshold b that is query-invariant.
It may be that dierent queries require dierent values of
b. Having the learning method trying to nd a good b value
(when one does not exist) may be detrimental.

We took two approaches to address this issue. The rst
method, SVMacc3, converts the retrieval function scores into
percentiles. For example, for document d, query q and re-
trieval function f , if the score f (d|q) is in the top 90% of
the scores f (|q) for query q, then the converted score is
f0(d|q) = 0.9. Each Kf contains 50 evenly spaced values
between 0 and 1. Tables 9, 10 and 11 show that the perfor-
mance of SVMacc3 was also not competitive with SVM
map.
The second method, SVMacc4, normalizes the scores given
by f for each query. For example, assume for query q that
f outputs scores in the range 0.2 to 0.7. Then for document
d, if f (d|q) = 0.6, the converted score would be f0(d|q) =
(0.6  0.2)/(0.7  0.2) = 0.8. Each Kf contains 50 evenly
spaced values between 0 and 1. Again, Tables 9, 10 and 11
show that SVMacc4 was not competitive with SVM

map

5.2.2 MAP vs ROCArea
SVM

roc performed much better than SVMacc in our ex-
periments. When trained on Indri retrieval functions (see
Table 9), the performance of SVM
roc was slight, though
not signicantly, worse than the performances of SVM
map.
However, Table 10 shows that SVM
map did signicantly out-
perform SVM

roc when trained on the TREC submissions.

Table 11 shows the performance of the models when trained
on the TREC submissions with the best submission removed.
The performance of most models degraded by a small amount,
with SVM

map still having the best performance.

TREC 9

TREC 10

map

Model
SVM
SVM
roc
SVMacc
SVMacc2
SVMacc3
SVMacc4



MAP W/L
0.284
0.274
0.215
0.267
0.133
0.228

31/19 **
49/1 **
35/15 **
50/0 **
46/4 **



MAP W/L
0.288
0.272
0.211
0.258
0.174
0.234

38/12 **
50/0 **
44/6 **
46/4 **
45/5 **

Table 11: Trained on TREC Subm. (w/o Best)

6. CONCLUSIONS AND FUTURE WORK
We have presented an SVM method that directly opti-
mizes MAP. It provides a principled approach and avoids
dicult to control heuristics. We formulated the optimiza-
tion problem and presented an algorithm which provably
nds the solution in polynomial time. We have shown em-
pirically that our method is generally superior to or com-
petitive with conventional SVMs methods.

Our new method makes it conceptually just as easy to
optimize SVMs for MAP as was previously possible only
for Accuracy and ROCArea. The computational cost for
training is very reasonable in practice. Since other methods
typically require tuning multiple heuristics, we also expect
to train fewer models before nding one which achieves good
performance.

The learning framework used by our method is fairly gen-
eral. A natural extension of this framework would be to
develop methods to optimize for other important IR mea-
sures, such as Normalized Discounted Cumulative Gain [2,
3, 4, 12] and Mean Reciprocal Rank.

7. ACKNOWLEDGMENTS

This work was funded under NSF Award IIS-0412894,
NSF CAREER Award 0237381, and a gift from Yahoo! Re-
search. The third author was also partly supported by a
Microsoft Research Fellowship.

