ABSTRACT
We now have incrementally-grown databases of text docu-
ments ranging back for over a decade in areas ranging from
personal email, to news-articles and conference proceedings.
While accessing individual documents is easy, methods for
overviewing and understanding these collections as a whole
are lacking in number and in scope. In this paper, we ad-
dress one such global analysis task, namely the problem of
automatically uncovering how ideas spread through the col-
lection over time. We refer to this problem as Information
Genealogy.
In contrast to bibliometric methods that are
limited to collections with explicit citation structure, we in-
vestigate content-based methods requiring only the text and
timestamps of the documents. In particular, we propose a
language-modeling approach and a likelihood ratio test to
detect inuence between documents in a statistically well-
founded way. Furthermore, we show how this method can
be used to infer citation graphs and to identify the most
inuential documents in the collection. Experiments on the
NIPS conference proceedings and the Physics ArXiv show
that our method is more eective than methods based on
document similarity.
Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous
General Terms
Algorithms, Measurement, Performance
Keywords
Information Genealogy, Flow of Ideas, Language Models,
Citation Inference, Text Mining, Temporal Data

1.

INTRODUCTION

In many domains, complete electronic records of docu-
ments now reach back for over a decade, including computer
science research papers, US news articles, and most peo-
ples personal email. These databases incrementally grow

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
KDD07, August 1215, 2007, San Jose, California, USA.
Copyright 2007 ACM 978-1-59593-609-7/07/0008 ...$5.00.

through an evolutionary process, where new documents
are inuenced by the content of already existing documents.
For example, scientic documents extend existing ideas, newssto-
ries rene and comment on other articles, and emails aggre-
gate or respond to other emails.

While keyword-based retrieval systems allow ecient ac-
cess to individual documents in such corpora, we yet lack
methods to understand the corpus as a whole. To rem-
edy this shortcoming, this paper investigates whether it is
possible to uncover the temporal dependency structure of a
corpus. Which documents inuenced each other? How did
ideas spread through the corpus over time? Which docu-
ments (or authors) were most inuential? While many of
these question have been addressed for hyperlinked data
with explicit citation structure, explicit citations are not
available in most domains. We therefore aim to address
these questions based on the (textual) content of the docu-
ments alone.

The premise for this research is that ideas manifest them-
selves in statistical properties of a document (e.g. the dis-
tribution of words), and that these properties can act as
a signature for an idea which can be traced through the
database. Following this premise, we present a probabilistic
model of inuence between documents and design a content-
based signicance test to detect whether one document was
inuenced by an idea rst presented in another document.
The test takes the form of a Likelihood Ratio Test (LRT)
and leads to a convex programming problem that can be
solved eciently. Our goal is to use this test for inferring
an inuence graph derived from the text of the documents
alone. Analogous to detecting inheritance from genes, we
refer to this text-mining problem as Information Genealogy.
Using corpora of scientic literature, we show that it is in-
deed possible to infer meaningful inuence graphs from the
text of the documents. Evaluating against the explicit cita-
tion graphs for these corpora, we nd that the automatically-
computed inuence graphs are similar to the citation graphs.
The ablility to automatically generate an inuence graph for
a collection enables a range of applications, from browsing,
to visualizing and mining the structure of the network. As
a simple example, we demonstrate that the in-degree of the
inuence graph provides an interesting measure of document
impact, similar to the in-degree of the citation graph.

2. MEASURING INFLUENCE

In this paper, we investigate and operationalize the no-
tion of inuence between documents. Inuence is an inter-
esting relationship between documents in historically grown

databases, since such corpora have grown through a self-
referential process: documents are inuenced by the con-
tent of prior documents, but also contribute new ideas which
in turn inuence later documents. Our goal is to uncover
and mine how ideas introduced in some document spread
through the corpus over time.

At rst glance, one might think that similarity, as cap-
tured by information retrieval metrics like TFIDF cosine
similarity (see e.g.
[32]), provides the full picture of inu-
ence. However, this is not the case.

On the one hand, similarity can occur without inuence.
First, if a document d(1) introduces an idea that is picked up
in documents d(2) and d(3), then d(2) and d(3) will likely be
similar but do not necessarily inuence each other. Second,
two documents might concurrently propose the same idea.
Again, neither document inuences the other although the
documents likely are similar.

On the other hand, inuence can occur with very little
similarity. In the scientic literature, for example, a large
textbook might devote a section to an idea introduced in an
earlier research paper. Clearly, the paper had inuence on
the textbook. However, the overall similarity between the
book and the paper is small, since the book covers many
other ideas as well.

As we will briey review in the following, most prior work
on analyzing temporal corpora has focused on identifying
relatedness between documents, not inuence. We will then
develop a probabilistic model and a statistical test for de-
tecting inuence, and show that it captures inuence better
than similarity and provides a more complete understanding
and model of inuence.
2.1 Topic Detection and Tracking

Topic Detection and Tracking (TDT) [5, 6] has the goal
of grouping documents by topic. Unlike inuence, which
is a directed relationship, TDT aims to group documents
into equivalence classes. While TDT approaches have relied
heavily on nding similarity measures that capture closeness
in topic, this approach is not necessarily detecting inuence,
as we have argued above. Methods that model inuence
not only can detect and track topics and ideas, but also
can provide reference points for why a document collection
developed as it did. Another minor dierence is that the
TDT studies were performed in an online setting, while we
assume access to the full corpus at any time.

Similar work on detecting and visualizing topic develop-
ment includes visualization methods such as Temporal Clus-
ter Histograms [34] and ThemeRiver [15], EM-based cor-
pus evolution detection [29], temporal clustering methods [7,
37], continuous time clustering models [37], Thread Decom-
position [14], Independent Component Analysis [22], topic-
intensity tracking [23], and Topical Precedence [27].
2.2 Real-World Inuence on Documents

Research on Burst Detection [21] and TimeMines [36] aims
to identify hidden causes based on changes in the word dis-
tribution over time. However, their notion of inuence is
dierent from ours. These approaches determine inuence
from real-world events on topics (e.g., events inuencing US
State of the Union Addresses). Instead, we model the inu-
ence of documents on each other.

2.3 Citation and Hyperlink Analysis

In bibliometrics, a documents inuence is measured through

properties of the citation graph [30, 31, 20, 12]. Our work
diers from citation analysis because our method is based
on document content, not on citations. We assume that in-
uence is inherently reected in the statistical properties of
documents. In particular, we conjecture that when one doc-
ument inuences another, the inuenced document shows
traces of the word distribution of the original document1.
Besides bibliometrics consideration of citation analysis on
research papers, other methods work on general hyperlink
structure. One of the most well-known such methods is
PageRank [31], which uses hyperlink structure to nd in-
uential Web pages.
2.4 Automatic Hypertext

There is related work on automatically adding hyperlinks
in information retrieval and related elds. Most promi-
nently, Link Detection was a key task in the TDT evalu-
ations [5]. Several proposals and methods exist for intro-
ducing hyperlinks between similar documents or passages of
documents [11, 10, 33, 26, 2, 4, 3, 24, 25]. Good surveys
are given in [38] and the 1997 special issue of Information
Processing and Management [1]. The work we propose is
dierent in several respects. First, our goal is to detect
inuence between documents, not just their relatedness.
This will allow a causal interpretation of the resulting cita-
tion graph. Second, we take a statistical testing approach
to the problem of identifying inuence links, which can be
seen as synonymous to citations. This will give a formal
semantic to the predictions of the methods, give theoretical
guidance on how to apply the methods, and expose under-
lying assumptions.
2.5 Language and Topic Models

We take a probabilistic language modeling approach in
the development of our methods. While we rely on a rather
basic language model for the sake of simplicity, more de-
tailed language models exist and can possibly be employed
as well. Previous work by Steyvers et al. [35] looks at how
document text can be generated by a two-step model of gen-
erating topics probabilistically from authors, and then words
probabilistically from topics. There has also been language
modeling work done in the natural language processing and
machine learning [28, 16, 8], speech recognition [19], and
information retrieval communities [39, 24, 25].

3. METHODS

In constructing an inuence graph for a database of doc-
uments, the core problem is to determine when and where
ideas ow from one document to another document. In the
following, we propose a probabilistic model of inuence in
a language-modeling framework, and develop a Likelihood
Ratio Test (LRT) [9] for detecting whether one document
has signicantly inuenced another document.
3.1 Probabilistic Model and Motivation

To make the method widely applicable, we have only two
basic requirements for our corpus of documents  rst, the
documents contain text and, second, the documents have

1Note that our goal is not plagiarism detection, where au-
thors would try to disguise their choice of words.

timestamps. Formally, the corpus D is a collection of n doc-
uments {D(1)  D(n)}, where each document D(i)  D has
an associated timestamp time(D(i)). There are m dier-
ent terms (i.e. words) across the entire corpus, which are
denoted by {t1  tm}.
We assume that the document is a vector-valued random
variable D = (W1  W|D|), which describes a document as
a sequence of random variables Wi, one for each word in
the document. A particular observed document is denoted
as d = (w1  w|d|). In the following, we assume that each
document D(i)  D was generated by a unigram language
model P (D(i) = d(i) | (i)) with parameters (i) specic to
that document.

Model 1. (Document Language Model)
A document D(i)  D is assumed to be generated by
independently drawing |D(i)| words from a document spe-
cic distribution with individual word probabilities (i) =
((i)
t1 , ..., (i)
P (D(i) = d(i) | (i)) = P (D(i) = (w(i)

tm ), i.e.

1  w(i)

|D(i)|) | (i))

|D(i)|Y
|D(i)|Y

j=1

=

=

P (W (i) = w(i)

j

| (i))

(i)
wj

j=1

Note that we do not explicitly model document length.
We chose this basic language model for mathematical and
computational convenience. However, our approach can be
extended to more complex language models as well (e.g. n-
gram models).

Since we wish to detect the ow of ideas and inuence
between documents, we also need a model of inter-document
relationship. We formalize this as a question of how the
language model (new) of a new document D(new) depends
on the language models {(1) } of the documents that
precede (new) in time. In particular, we assume that the
language model of a new document can be (approximately)
expressed as a mixture distribution over the language models
of previous documents.

Model 2. (Inter-Document Influence Model)

A new document D(new) is generated by a mixture distri-
bution of the already existing documents D(i) with i  P for
P = {i : time(D(i)) < t0}, in particular

P(D(new)= d(new)|) =

with mixing weights  satisfying 0  i andP

pP(W (p)= wj|(p))

pP

j=1

i = 1.

(1)

i

In this dependency model, a new document is composed
of parts generated by the word distributions of old docu-
ments, where the mixing coecient p indicates the frac-
tion of D(new) that is generated from D(p). Clearly, there is
direct inuence of a document D(p) on D(new), if the respec-
tive mixing coecient is non-zero. Note that the resulting
language model for D(new) is again a unigram model, so that
P (D(new)= d(new) | ) = P (D(new)= d(new) | (new)) with

|D(new)|Y

X

X

pP

In actual document collections, documents typically con-
tain some original part that does not come from previous
documents. To account for the original portion of a docu-
ment in our model, we include a distribution (o) with weight
o in the mixture. It models the distribution of words that
is original to the document and that cannot be explained by
previous documents. (In practice, we will assume that o is
xed, but that we have no knowledge of (o).

Model 3. (Inter-Document Influence Model with

Original Content)
A new document D(new) is generated by a mixture distri-
bution of the already existing documents D(i) with i  P for
P = {i : time(D(i)) < t0}, and a document specic mixture
component (o) with weight o, in particular

|D(new)|Y

X

P(D(new)= d(new)|) =

with mixing weights  s.t. 0  i, o and o +P

pP{o}

pP(W (p)= wj|(p))

j=1

i = 1.

(3)

i

In the case when the documents have no original content,
setting o = 0 in the Inter-Document Inuence Model with
Original Content results in Model 2. Vice versa, Model 2
also subsumes Model 3 by simply introducing an articial
single-word document for each term in the corpus and con-
straining their mixture weights to sum to o. We will there-
fore focus our further derivations on Model 2 for the sake of
simplicity.

We will now show how this probabilistic setup can be
used in a signicance test for detecting whether a particular
mixing weight p is non-zero in a given document collection.
3.2 A Statistical Test for Detecting Inuence
How can one decide whether a candidate inuential doc-
ument d(can) had a signicant inuence on d(new) given
the other documents in the collection? First, d(can) can
only have had an inuence on d(new) if it had been pub-
lished before d(new) (i.e. time(d(can)) < time(d(new))). Note
that this is already encoded in the Inter-Document Inu-
ence Models dened above. Second, inuence should be
attributed to the rst publication that introduced an idea
through an original section or portion, not to other docu-
ments that later copied an idea. To illustrate this in the
context of research papers, this means that inuence should
be credited to the original article, not a tutorial that repro-
duced the original idea.

Under these conditions, the decision of whether document
d(new) shows signicant inuence from d(can) can be phrased
as a Likelihood Ratio Test [9]. In general, a Likelihood Ratio
Test decides between two families of densities described by
sets of parameters  and 0 that are nested, i.e. 0 
. Applied to our case,  will be all mixture models of
D(new) as in Eq. (1) with parameters i for all documents
P published prior to t0 = time(d(can)) (and therefore prior
to d(new)), as well as a parameter can for d(can).

8<: :

 =

X

iP{can}

i = 1  i  0

9=;

(new) =

p(p).

(2)

The subset 0 of the mixture models in  will be the models
where d(can) has zero mixture weight (i.e. can = 0).

8<: :

0 =

X

iP{can}

i = 1  i  0  can = 0

9=;

Note that the set of prior documents P = {i : time(d(i)) <
time(d(can))} serves as a background model of what was
already known when d(can) was published. Against this
background, we can then measure how much the new ideas
in document d(can) inuenced d(new).

The null hypothesis of the Likelihood Ratio test is that
the data comes from a model in 0 (i.e. document d(new)
was not inuenced by d(can) given the documents published
before d(can)). To reject this null hypothesis, a likelihood
ratio test considers the following test statistic

sup0{P (D(new) = d(new)|)}
sup0{P (D(new) = d(new)|0)}

d(can) (d(new)) =
Note that P (D(new) = d(new)|) is convex over  and 0,
so that the suprema can be computed eciently. We will
elaborate on the computational aspects below. Intuitively,
the value of d(can) (d(new)) measures whether using d(can) in
the mixture model better explains the content of d(new) than
just using previously published documents. More formally,
d(can) (d(new)) compares the likelihood sup0{P (D(new) =
d(new)|0)} of the best mixture model containing d(can) with
the likelihood sup0{P (D(new) = d(new)|)} of the best
mixture model that does not use d(can) (i.e. can = 0). The
test then decides whether there is signicant evidence that
a non-empty part of d(new) was generated from d(can), in
comparison to using a mixture only over the other language
models.
If the null hypothesis is true, then the distribution of the
LRT statistic 2 log(d(can) (d(new))) is asymptotically (in
the document length under the unigram model) 2 with one
degree of freedom.

2 log(d(can) (d(new)))  2
The null hypothesis H0 should be rejected, if
2 log(d(can) (d(new))) > c

1

for some c selected dependent on the desired signicance
level. For a signicance level of 95%, c should be 3.84. This
captures the intuition that we can reject the null hypothe-
sis and conclude that d(can) had a signicant inuence on
d(new), if the best model that does not use d(can) has a
much worse likelihood than the best model that considers
d(can). Specically, if 2 log(d(can) (d(new))) is large, then
d(can) signicantly inuenced d(new) given all other docu-
ments published at that time.

To estimate the language models (i) of the documents en-
tering into the mixture model of d(new), we use the maximum-
likelihood estimate. We denote with tf (i) the term frequency
(TF) vector of document d(i), where each entry tf (i)
is the
number of times that term tj appears in the document d(i).
The estimator is

j

(i)
wj =

tf (i)
wj
|d(i)| ,

which is simply the fraction of times the particular word oc-
curs in the observed document d(i). Using a more advanced

estimator instead is straightforward, but we will not discuss
this for the sake of simplicity.
3.3 Relating the LRT to Detecting Inuence

What does it mean for the LRT to signicantly reject
the null hypothesis? A good intuition is to think of this
method in the context of trying to explain the ideas and
content found in d(new). There are two choices. First, ex-
plain d(new) using only other documents preceding d(can) as
well as some original component. Second, explain d(new)
with these plus an additional d(can). If the rst case already
provides a wonderful model for d(new), then adding d(can)
will not explain d(new) any more accurately. Thus, d(can)
really does not contribute to d(new). On the other hand,
if d(can) introduced some new ideas and terminology that
then owed to d(new), using d(can) will provide a better ex-
planation than only using P. Consequently, the likelihood
of d(new) using d(can) will be signicantly higher than with-
out it, and we can reject the null hypothesis. To summarize,
rejecting the null hypothesis means that d(can) signicantly
exerted inuence on d(new).
3.4 Computing the LRT

Computing the value of d(can) (d(new)) requires solving

two optimization problems.

L0 = sup
0
L = sup


{P (D(new) = d(new)|)} and
{P (D(new) = d(new)|)}.

(4)

(5)

Given our model, these problems can be solved eciently.
Note that we can write the log-likelihood L( | d(new),S) of
the document d(new) w.r.t. a xed  as

log L( | d(new)) = log P (d(new) | )

mX

j=1

X

iS

=

tf (new)

j

log(

i(i)

j ).

With S we denote the set of documents considered in the
model. This gives S = P{can} for  and S = P for 0. In
this notation, each of the optimization problems in Eq. (4)
and (5) takes the form

max
<|S|

subject to

log L( | d(new))

X

i = 1

iS
i  S : i  0.

For Model 3 an additional linear constraint is introduced to
limit the amount of original content o to not be more than
a user-specied parameter . This constraint is necessary,
since otherwise the (o) mixture component could always
perfectly explain d(new).

It is easy to see that these optimization problems are con-
vex, which means that they have no local optima and that
there are ecient methods for computing the solution. We
currently use the separable convex implementation for the
general-purpose solver Mosek [18] to solve the optimization
problems. However, more specialized code is likely to be
substantially more ecient.

While solving each optimization problem is ecient, an-
alyzing a collection requires a quadratic number of LRTs,
each with on the order of n documents in the background

model. In particular, for each document d(new), we need to
test all prior documents

d(i) : time(d(i)) < time(d(new))

(6)

in the collection, since all of these are candidates for having
inuenced d(new). For each document d(can) in the candidate
candidate set C of d(new), we then have a background model

n

C =

n

Pd(can) =

o

o

d(i) : time(d(i)) < time(d(can))

.

(7)

Computing all tests exhaustively for a large corpus can be
expensive. We therefore use the following approximations.
Both approximations are based on the insight that some
similarity is necessary for inuence. The potentially inuen-
tial document d(can) must have some similarity with d(new).
Therefore, we rst approximate the candidate set to con-
tain the kC nearest neighbors of d(new) from C. We use
cosine distance between TF and TFIDF vectors for docu-
ment similarity. Second, an analogous argument applies to
the background models Pd(can) . We therefore approximate
the background model, using only the kP most similar docu-
ments from P. Since selecting P combines document vectors
by addition, we use cosine distance between document TF
vectors to select P. In the experiments we set kC = kP and
refer to this parameter as k. We will empirically evaluate
the eect of these approximations depending on k.

4. EXPERIMENTS

We wish to measure how well these models assumptions
match real data. First, how does an inuence graph inferred
by the LRT method compare against a citation graph? Sec-
ond, can the inuence graph identify top inuential papers?
4.1 Experiment Setup and Corpora

The concept of inuence and idea ow between documents
corresponds well with the notion of a citation. Consequently,
we focus on research papers to provide a quantitative eval-
uation of the LRT method by comparing with citations.

The rst corpus is the full-text proceedings of the Neu-
ral Information Processing Systems (NIPS) conference [17]
from 1987-2000, with a timestamp of the publication year.
NIPS has 1955 documents, with 74731 terms (features). We
manually constructed the graph of 1512 intra-corpus cita-
tions, but only compare to citations of previous documents
in time. We ignore citations of rst-year documents since
the LRT requires a background model.

The second corpus is the theoretical high-energy physics
(HEPTH) section of the Physics ArXiv [13] from Aug. 1991
to Apr. 2006. We aggregate the full-text papers by year.
HEPTH has 39008 documents, 229194 terms, and 557582
citations. SLAC-SPIRES compiled these citations.
4.2

Inferring Inuence Graphs

This set of experiments analyzes how well the LRT re-
covers the inuence graph. After an illustrative example,
we explore the LRTs sensitivity on synthetic data under
controlled experiment conditions, and then evaluate on two
real-world datasets.
4.2.1 Qualitative Evaluation
We rst discuss a simple example to illustrate the LRT
methods behavior and how it compares to citations. Fig-
ure 2 shows those documents that NIPS document 1541

Figure 1: ROC-Area comparing the LRT method
against a cosine similarity baseline. The x-axis is
can. At a can level, the ROC-Area measures the
quality of inuence prediction in documents with the
specied can as compared against documents with
can = 0.

(Schoelkopf et al. on Shrinking the Tube: a New Sup-
port Vector Regression Algorithm) most signicantly in-
uenced according to the LRT statistic. Three of the top
ve papers actually cite document 1541 (or a document with
equivalent content from another venue). Furthermore, the
top document could arguably have cited 1541 as well, since
it relies on the -parameterization of SVMs that document
1541 introduced to NIPS. In fact, all papers (except Fast
Training of Support Vector Classiers) consider this new
parameterization. Note that the paper -arc: Ensemble
Learning in the Presence of Outliers is not about SVMs,
but uses the -parametrization in the context of boosting.

The LRT appears to accurately focus on the papers origi-
nal contribution, the -parameterization. General SVM pa-
pers do not score highly, since they are already modeled by
earlier papers, e.g. paper 1217 Support Vector Method for
Function Approximation, Regression Estimation, and Sig-
nal Processing of V. Vapnik et al., which was one of the
rst SVM papers in NIPS. When considering inuencers
of A Support Vector Method for Clustering by A. Ben-
Hur et al. (using the conventional parameterization), the
method correctly recognizes that paper 1541s inuence is
low (2 log(d(1541) (d(new))) = 67.0) even though the docu-
ments are similar. Paper 1217 already explains the SVM
content (2 log(d(1217) (d(new))) = 535.0).
4.2.2 Quantitative Evaluation on Synthetic Data
Beyond this qualitative example, how accurately can the
LRT discover inuence? How much must d(new) copy from
d(can) before the LRT can detect it?

To explore these questions, we constructed articial docu-
ments d(new) from the NIPS corpus as follows. A candidate
document d(can) and a set P of k = 100 previous documents
are chosen at random form the NIPS corpus so that the
documents in P preceed d(can) in time. Then, 101 articial
new documents are generated according to Eq. 1, where each
new document has been inuenced by d(can) at the fractional
levels of can  {0.00, 0.01, 0.02, , 1.00}. The remaining
mixing weights i are selected by generating random num-

0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 0 0.2 0.4 0.6 0.8 1ROC-AreaCandidate document mixing weightLRT ROC-AreaSIM ROC-Area2 log(d(1541)(d(new))) Cite? Title and Author(s) of d0

321.2455

no

221.8297

219.8769

184.5493

168.8972

yes

yes

no

yes

Support Vector Method for Novelty Detection, B. Schoelkopf, Robert C. Williamson,
Alex Smola, John Shawe-Taylor, John C. Platt.
An Improved Decomposition Algorithm for Regression Support Vector Machines, Pavel
Laskov.
-arc: Ensemble Learning in the Presence of Outliers, Gunnar Raetsch, B. Scholkopf,
Alex Smola, Kenneth D. Miller, Takashi Onoda, Steve Mims.
Fast Training of Support Vector Classiers, Fernando Perez-Cruz, Pedro Alarcon-
Diana, Angel Navia-Vazquez, Antonio Artes-Rodriguez.
Uniqueness of the SVM Solution, Christopher J. C. Burges, David J. Crisp.

Figure 2: Papers that are inuenced by NIPS paper 1541, Shrinking the Tube: a New Support Vector
Regression Algorithm written by B. Schoelkopf, P. Bartlett, A. Smola, and R. Williamson. The leftmost
column shows the LRT statistic value. (Larger LRT statistic values represent greater inuence.)

bers uniformly on the interval [0, 1], and then normalizing
them so that they sum to 1  can. The LRTs are run on
each new document. Additionally, TF document vector co-
sine similarity is measured between d(can) and each d(new).
The entire process is repeated for 1000 random selections of
P and d(can).
We computed ROC-Area in the following manner. First,
we select a particular can  {0.01 1.00}. The generated
documents at the can level are marked as positive exam-
ples. The negative examples are documents with can = 0.
Finally, a ranking, either LRT statistic scores or cosine dis-
tance similarity, is used to compute ROC-Area.

Figure 1 shows that even if only a small portion (i.e. a
few percent) of d(new) is drawn from d(can), the LRT accu-
rately detects the inuence. The similarity baseline needs a
much larger signal. This example illustrates that similarity
and inuence are in fact dierent, and that the well-founded
statistical approach can be more accurate and sensitive than
an ad-hoc heuristic.

4.2.3 Quantitative Evaluation on Real Data
Moving to real data, we use the LRTs to discover the in-
uence graph for NIPS and HEPTH. For each document
d(new), we rst compute a set of candidate documents C
based on similarity. The elements of C are then ranked ac-
cording to the LRT statistic (i.e. whether d(can) was signif-
icant in explaining d(new)). The higher d(can) is ranked, the
more likely that it inuenced d(new), and we can derive the
inuence graph by thresholding (discussed below).

We evaluate the inuence graph by a graph-based mean-
average-precision (G-MAP) metric. For a document d, aver-
age the precision of the ranked predicted list of inuencers
at the positions corresponding to documents that d actu-
ally cites. Citations not in the list are averaged as 0, i.e.
ranked at innity. (As an information retrieval analogy, the
inuence list is the search result page, with citations being
relevant results.) G-MAP is the mean of the per-document
average precision scores. We exclude documents from the
rst two years due to edge eects (the LRT cannot predict
citations for the rst years since C or P are empty).

We compare G-MAP for the LRT method against G-MAP
of a similarity-based heuristic, which serves as a baseline.
This baseline method ranks the elements of C not by LRT
score, but by similarity. We explored several similarity mea-
sures. The best similarity measures in our experiments are
TF cosine and TFIDF cosine. We report their performance.
Note that citations are not necessarily a perfect gold stan-

TF

TFIDF

G-MAP
NIPS

LRT
0.4489
HEPTH 0.2432

SIM

0.3948
0.2216

LRT
0.4531
0.2543

SIM

0.4412
0.2167

Table 1: G-MAP scores comparing the LRT against
the similarity baseline. The similarity measure to
select P is the TF cosine and to select/rank C is
either the TF cosine or the TFIDF cosine. Results
are reported for k = 100 and  = 0.05.

TFIDF C for LRT with k = 100 and S = .05

Figure 3: Precision vs. Recall on NIPS. The three
lines are (from top to bottom) the LRT methods
precision at a recall level with TFIDF cosine used
to select C, the TFIDF distance C similarity baseline,
and the TF distance C similarity baseline.

dard for inuence, since they reect idiosycracies of how sci-
entic communities cite prior work. For example, in Figure 2
authors sometimes cited a journal paper or book instead of
the NIPS paper. Therefore, a G-MAP of 1 is not achievable.

LRTs are more accurate than similarities.

Table 1 shows that the LRT achieves higher G-MAP scores
than the similarity baselines on both NIPS and HEPTH.
Among the two heuristic baselines, TFIDF cosine performs
better then TF cosine. TFIDF cosine also appears to select
better sets C for the LRT. The HEPTH results are reported
for a random sample of 1600 documents.

0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1PrecisionRecallNIPS k=100 Prec vs RecTF Similarity BaselineTFIDF Similarity BaselineTF

TFIDF

G-MAP
 = .001
 = .01
 = .05
 = .1
 = .2

LRT
0.4575
0.4620
0.4489
0.4475
0.4373

SIM

0.3948

SIM

0.4412

LRT
0.4597
0.4649
0.4531
0.4535
0.4447

Table 2: G-MAP scores comparing the LRT for a
range of d(can) inuence mixing weights  against
the similarity baseline. The similarity measure to
select C is either TF or TFIDF cosine. Results are
reported on NIPS for k = 100.

G-MAP
NIPS (k = 100)
NIPS (k = 10)
HEPTH (k = 100)
HEPTH (k = 20)

TF

TFIDF

LRT
0.4489
0.4067
0.2432
0.2227

SIM

0.3948
0.3754
0.2216
0.2037

LRT
0.4531
0.4580
0.2543
0.2264

SIM

0.4412
0.4226
0.2167
0.1943

Table 3: G-MAP scores comparing the LRT against
the similarity baseline for two k-NN approximation
levels. The similarity measure for selecting C is ei-
ther TF or TFIDF cosine. Results are reported on
NIPS and HEPTH for  = .05.

LRT scores are more comparable than similarities.

Table 1 showed that the LRT can nd the most inuential
papers for one particular document. Figure 3 measures how
well it can nd the strongest edges in the whole inuence
graph. This precision-recall graph uses the ranking of all
LRT statistic scores of all documents, with actual citations
marked as positive examples. Figure 3 also shows the scores
for using lists of TF and TFIDF cosine similarities. The
LRT graph dominates the similarity baselines over the whole
range and the dierence in performance is larger than in
the per-document evaluation. We conclude from this that
LRT scores are more comparable between documents than
similarity scores. This is to be expected because the LRT
values have a clear probabilistic semantic. However, the
similarity scores have no such guarantees.

Effects of the  parameter.

Table 2 shows that the LRT is robust over a large range 
values. The LRTs G-MAP dominates the similarity base-
lines. However,  = 0.01 seems to perform better than our
initial guess of 0.05 used above.

Effect of k parameter in LRT approximations.

Table 3 shows G-MAP scores at diering levels of the k-
NN approximation. Recall from Table 1 that G-MAP scores
for HEPTH are substantially lower than for NIPS. We con-
jecture that this is due to the size of the corpus in relation
to k. With a large corpus, k = 100 is likely to exclude too
many relevant documents from consideration. We further
analyze the role of k, in its two roles in controlling the sizes
of C and P.

First, k controls the size of C.

If k is too small, truly
inuential documents will not be tested by the LRT. E.g., in
HEPTH, each document has 14 citations on average. With
k = 10, it would be simply impossible to recover the entire
citation graph. Therefore we conclude that k must be large
enough to include all documents that make contributions to
d(new). On HEPTH, k = 100 is better than k = 20 for TF

Dataset (C)
NIPS (TFIDF)
NIPS (TF)
HEPTH (TFIDF)
HEPTH (TF)

GMAP GMAP (perfect C)
0.4531
0.4489
0.2543
0.2432

0.4556
0.4590
0.3803
0.3906

Table 4: How close is the approximation to the op-
timal? G-MAP scores are reported for S = .05.

and TFIDF cosine, and for LRT and similarity baseline. We
believe this is because k = 20 is too restrictive. NIPS with
TF cosine shows the same behavior.
Optimal C.
To better understand how much loss in performance is due
to the k-NN approximation of C, the following experiment
explores the G-MAP scores of the LRT for a perfect C. In
particular, we construct C so that it includes all documents
that d(new) actually cites, and then ll the remaining places
in C with the most similar documents. Table 4 shows that
for k = 100 the loss in performance due to an approximate
C is fairly small on NIPS. For HEPTH, on the other hand,
k = 100 shows a much greater loss, with G-MAP scores
only about 60-65% of the optimal. We believe this loss oc-
curs because C is too small to accomodate all the inuential
documents.
4.3

Identifying Inuential Documents

What are the inuential documents that have the most
eect on the document collections development? Which
documents should one read to best grasp this development?
We have already shown that LRTs can be used to infer an
inuence graph that is similar to a citation graph. We now
investigate whether this inuence graph can be used to iden-
tify the documents with the overall largest inuence on the
collection. In analogy to citation counts (i.e. the in-degree
in the citation graph), we propose the in-degree in the inu-
ence graph as a measure of impact. If not noted otherwise,
we form the inuence graph by connecting each document
d(new) with the l other nodes that receive the highest LRT
value. We typically use l = 10, although we also explore this
parameters eect on performance.

4.3.1 Qualitative evaluation
For each year in NIPS, Table 5 lists the paper with the
highest in-degree in the inuence graph computed by the
LRT method with k = 100 and l = 10. We expect these
to have high citation counts, which we test by showing the
papers citation counts both from within the NIPS corpus
(as of 2000) and from Google Scholar (as of 2007). For
most documents, the citation count is indeed high when
compared to the average NIPS document citation count of
0.7734 other NIPS papers. An interesting example is Sup-
port Vector Method for Function Approximation, Regres-
sion Estimation, and Signal Processing from 1996. While
this is one of the papers that introduced SVMs to NIPS,
it has only 3 citations within NIPS and only 44 citations
in Google Scholar. Nevertheless, SVMs had a huge impact
on NIPS. In this sense our LRT method is correct and is
not inuenced by citation habits. In this example, most au-
thors cite Vapniks later book (with 5144 citations) instead
of this paper. The LRT method is unaected and correctly
identies the SVM idea as highly inuential on NIPS.

Year Document Title and Author(s)
1988
1989

Document

Citation Counts

NIPS Google Scholar

An Optimality Principle for Unsupervised Learning by Terence D. Sanger
Training Stochastic Model Recognition Algorithms as Networks Can Lead to Maximum Mutual
Information Estimation of Parameters by John S. Bridle
Learning Theory and Experiments with Competitive Networks by Gni L. Bilbro, David E.
van den Bout
The Eective Number of Parameters: An Analysis of Generalization and Regularization in
Nonlinear Learning Systems by John Moody
Reinforcement Learning Applied to Linear Quadratic Regulation by Steven J. Bradtke
Supervised Learning from Incomplete Data via an EM approach by Zoubin Ghahramani,
Michael I. Jordan
Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems by
Tommi Jakkola, Sizarad Singhal, Michael I. Jordan
EM Optimization of Latent-Variable Density Models by Chris M. Bishop, M. Svensen, Chisto-
pher K.I. Williams
Support Vector Method for Function Approximation, Regression Estimation, and Signal Pro-
cessing by V. Vapnik, Steven E. Golowich, Alex Smola
EM Algorithms for PCA and SPCA by Sam Roweis

1990

1991

1992
1993

1994

1995

1996

1997

4
11

0

12

6
12

10

1

3

1

61
113

0

234

56
163

142

27

44 (5144)

177

Table 5: The most inuential paper per year in NIPS, as measured by inuence graph in-degree, with k = 100,
 = .05, and TFIDF cosine for C. We exclude years with edge eects and the last 3 years, since they do not have
statistically signicant counts. Comparison is against the within-NIPS citation counts, and Google-scholar
citation counts (on Feb. 28, 2007).

Corpus



NIPS
0.4216
HEPTH 0.3887

LRT

RMap@3

0.2771
0.2558

TF

SIM

@12

0.3126
0.2376



RMap@3

0.3379
0.3497

0.1475
0.1421

@12

0.2561
0.1594



0.4163
0.3549

LRT

RMap@3

0.2751
0.1456

TFIDF

SIM

@12

0.3022
0.1582



RMap@3

0.3686
0.3190

0.1959
0.1139

@12

0.2585
0.1138

Table 6: Rank metrics comparing the LRT against similarity on NIPS (k = 100) and HEPTH (k = 20), using
 = .05 and TF or TFIDF cosine for C. We ignore the rst two and last two years because of edge eects.

4.3.2 Quantitative Evaluation
We compare the ranking of documents by in-degree in the
inuence graph to the ranking by citation count. As simi-
larity measures, we use Kendalls  and a ranking version of
MAP, which we term R-MAP.

Kendalls .

Kendalls  measures how many pairs two rankings rank
in the same order. It ranges between -1 and 1, with higher
numbers indicating greater similarity. Formally,
2  number of concordant pairs

total number of pairs  number of tied pairs

 =

R-MAP@k.

R-MAP@k measures the average precision of a ranking.
With the k top-ranked documents as positive examples, av-
erage the rankings precision at the positions of these docu-
ments. We calculate R-MAP@3 and R-MAP@12.

There is one caveat with rank-based metrics. Edge eects
(e.g., older papers have more citations, papers from the last
year have no citations) make it dicult to present one uni-
ed ranking of all documents. Therefore, we calculate each
metric per-year and average the year-by-year values to get
a single score for the entire corpus. Additionally, because
of edge eects, the rst two and the last two years are not
used, since they do not contain meaningful results.

The TF and TFIDF baselines use the most similar docu-

ments instead of the LRT predictions.

LRTs are better than similarity.

Table 6 shows that the LRT gives substantially better
rankings than the similarity baseline for all metrics on both
NIPS and HEPTH with both TF and TFIDF cosine C.

Effect of the parameter l.

The left plot of Figure 4 explores whether selecting in-
uencers is sensitive to the parameter l. For the inuence
graph, we considered each documents l predicted inuencers
with highest LRT scores. Figure 4 shows how varying l af-
fects  for both LRT and the similarity baseline. Since NIPS
documents do not have many citations, we explore l = 1 to
15. The upper line is LRT performance with 95% con-
dence interval error bars. (The condence interval is com-
puted using the multiple  values per data point, because
each graphed  is the average of multiple (here, 10) years
of  metric scores.) The lower line depicts  on the similar-
ity baseline. For the TFIDF cosine C, when l is small, the
method computes a count over only the few top inuential
documents selected by the LRTs for d(new).
It turns out
that small l seem to perform better than our initial guess
of l = 10. As l increases, more non-inuential documents
are counted and  correspondingly falls. When l approaches
100 (not shown), the LRT and the baseline are identical as
expected by construction.

Thresholding on the LRT score.

The right plot of Figure 4 depicts how  varies if we do
not select a xed number of l neighbors per document, but
instead use a threshold on the LRT statistic. The LRT is
set up to reject the null hypothesis and declare that d(can)
inuences d(new) if the LRT statistic is suciently large.
Varying this threshold controls the level of condence in the
LRT, so we use the threshold level as the x-axis and examine
how it aects  . Thresholding the LRT values actually gives
better performance than using the l parameter, since we are
not forcing a certain number of inuence links for each doc-
ument. There are four dierent regions in this graph. First,

Figure 4: Using  to compare the LRT against the similarity baseline, both with the l parameter (left) and
by thresholding the LRT statistic values (right). Results are for NIPS with TFIDF cosine C and k = 100. The
TF plot looks similar, except that the baseline is smoother.

if the threshold is too low, performance suers because the
null hypothesis is being accepted erroneously. Second, per-
formance increases as the threshold approaches reasonable
condence levels. Third, a large range of threshold values
(approximately 100-2000) give good and similar  scores,
showing that the LRT method is robust. Fourth, when the
threshold is too high, many inuential documents are no
longer detected, and performance subsequently falls.

Note that a condence level of 95% per test (i.e. a thresh-
old of 3.84) performs quite poorly. This level means that 5%
of the inuence links are erroneous. NIPS, with 2000 papers,
would have an expected 100,050 false links (and only 1512
real citations). Therefore, we need a much higher condence
level to account for the multiple-testing bias. Using Bonfer-
roni adjustment, each tests level is the overall level divided
by the number of tests.

5. DISCUSSION AND FUTURE WORK

One obvious limitation of the current model is the sim-
plicity of the language model. The assumption that each
document is a sequence of independent words is, in reality,
likely violated. This observation motivates more expressive
language models such as n-gram language models.

There is also the question of whether these methods can
generalize to other domains. LRTs do not use citation data,
so many domains should be applicable. However, we have
only conducted experiments on research publications.

Finally, there is scalability and eciency. Much of the
computing time is spent solving convex optimization prob-
lems. While C and P prune this space, there may be other
criteria to provably eliminate certain LRTs without aecting
the results. Furthermore, the optimization problems have a
special structure, which can probably be exploited by spe-
cialized methods to solve the optimization problems.

6. CONCLUSIONS

We presented a probabilistic model of inuence between
documents for corpora that have grown over time. In this
model, we derived a Likelihood Ratio Test to detect inu-
ence based on the content of documents and showed how the
test can be computed eciently. We found that the inuence

graphs derived from the content resemble the structure of ex-
plicit citation graphs for corpora of scientic literature. Fur-
thermore, we showed that in-degree in the inuence graph
is an eective indicator of a documents impact. The ability
to create inuence graphs based on document content alone
has the potential to open databases without explicit citation
structure to the large repertoire of graph mining algorithms.

7. ACKNOWLEDGMENTS

We thank Johannes Gehrke and Rich Caruana for the dis-
cussions that lead to this work. This work was funded in
part by NSF Career Award IIS-0237381, NSF Award OISE-
0611783, and the KD-D grant.

