

Abstract. We propose general procedures for posterior sampling from
additive and generalized additive models. The procedure is a stochastic
generalization of the well-known backtting algorithm for tting addi-
tive models. One chooses a linear operator (smoother) for each predic-
tor, and the algorithm requires only the application of the operator and
its square root. The procedure is general and modular, and we describe
its application to nonparametric, semiparametric and mixed models.

Key words and phrases: Additive models, backtting, Bayes, Gibbs sam-
pling, random effects, MetropolisHastings procedure.

1. INTRODUCTION

In this paper we propose general procedures for
posterior sampling from additive and generalized
additive models. The main idea evolves from the
close relationship between the backtting algorithm
for tting additive models, and the Gibbs sampler
for drawing realizations from a posterior distribu-
tion.

As an example, Figure 1 shows the results of an
additive model t to four air pollution variables,
in a dataset with 330 observations. The response
variable is log ozone concentration. The t is repre-
sented by the solid curves in each of the panels and
was obtained using cubic smoothing splines within
the popular backtting algorithm. Also shown are
posterior realizations from a Bayesian version of the
additive model. The posterior realizations were pro-
duced from a stochastic version of backtting, which
we call Bayesian backtting. That is the central
topic of this paper.

An additive model is a popular tool for modelling
regression data. It expresses the response variable
as a sum of (typically nonlinear) functions of the
predictor variables. The backtting procedure is a
modular way of tting an additive model. It cycles
through the predictors, replacing each current func-

Trevor Hastie is Professor, Department of Statis-
tics and Division of Biostatistics, Stanford Uni-
versity, Stanford, California 94305 (e-mail: trevor
@stat.stanford.edu). Robert Tibshirani is Professor,
Departments of Health Research and Policy and
Statistics, Stanford University, Stanford, California
94305 (e-mail: tibs@stat.stanford.edu).

tion estimate by a curve derived from smoothing
a partial residual on each predictor. The Bayesian
backtting procedure, introduced here, smooths the
same partial residual and then adds appropriate
noise to obtain a new realization of the current func-
tion. This is equivalent to Gibbs sampling for an
appropriately dened Bayesian model.

In the important special case of an additive cubic
smoothing spline model with n observations, we
obtain an O(cid:1)n(cid:2) algorithm for sampling from the
posterior. This is not the rst such procedure: Wong
and Kohn (1985) derive an O(cid:1)n(cid:2) algorithm using
the state-space representation of splines; see also
Carter and Kohn (1994). Denison, Mallick and
Smith (1998) employ polynomial splines and back-
tting in a Bayesian additive model. Our proposal
has the advantage of being conceptually simple,
modular and general; it can be used with a wide
range of operators representing nonparametric
smoothers, as well as linear xed and random
effects models.

We begin with an exposition of posterior sampling
for cubic smoothing splines in Section 2 and then
discuss our general proposal for additive models
(Section 3) and give an example involving growth
curves. In Section 4 we discuss approaches for
estimation of the variance components (including
Bayes, empirical Bayes, REML and ML), and how
to choose appropriate priors. The relationship with
bootstrap sampling is briey explored in Section 5.
Generalized additive models and the Metropolis
Hastings procedure are discussed in Section 6, and
we end with a discussion, including a description of
a new public domain S-plus function for Bayesian
backtting.

196

BAYESIAN BACKFITTING

197

Fig. 1. Fifty posterior realizations (cid:1)grey curves(cid:2) for an additive model t to four air-pollution variables(cid:3) The additive model tted
functions are shown with thick(cid:4) dark curves(cid:3) The points are partial residuals from the posterior means and give an idea of the spread of
the data available for each posterior sample(cid:3)

2. POSTERIOR SAMPLING FOR A

CUBIC SMOOTHING SPLINE

Consider a scatterplot smoothing problem with
data (cid:1)x1(cid:4) y1(cid:2)(cid:4)(cid:1)x2(cid:4) y2(cid:2)(cid:4) (cid:3) (cid:3) (cid:3), (cid:1)xn(cid:4) yn(cid:2). Here yi are the
response values and xi are the inputs (predictors).
We postulate a model

(1)

yi = f(cid:1)xi(cid:2) + i(cid:5)

i  N(cid:1)0(cid:4)  2(cid:2)(cid:3)

The smoothing spline is a popular model for rep-
resenting f(cid:1)x(cid:2), and is usually derived as the mini-
mizer of the penalized sum of squares criterion
(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx

J(cid:1)f(cid:2) =(cid:1)

(cid:1)yi  f(cid:1)xi(cid:2)(cid:2)2 + 

(cid:8)f

(cid:2)

(2)

i

over all functions f(cid:1)x(cid:2) such that the integral exists.
The constant   0 is a tuning parameter, with
larger values resulting in smoother curves. The
solution function f is a natural cubic spline, with
knots at each of the unique values of xi. This

implies a representation

f(cid:1)x(cid:2) = M(cid:1)

bj(cid:1)x(cid:2)j(cid:4)

j=1

f = S(cid:1)(cid:2)y(cid:3)

(3)
where the M  n basis functions bj represent the
linear space of such functions.
Letting y = (cid:1)y1(cid:4) y2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) yn(cid:2)T, f = (cid:1)f(cid:1)x1(cid:2)(cid:4) f(cid:1)x2(cid:2)(cid:4)
(cid:3) (cid:3) (cid:3) (cid:4) f(cid:1)xn(cid:2)(cid:2)T, the tted values at the n input values
xi can be written as
(4)
Here S(cid:1)(cid:2) is a symmetric n  n operator matrix,
called the smoother matrix. It depends on the val-
ues xi and the tuning parameter , but not on y.
We will sometimes write it simply as S. It is also
possible to estimate  in an adaptive (nonlinear)
manner, depending on the response y, but we do
not consider that here. Most smoothers, and in par-
ticular smoothing splines, give a prediction at any
values of x, not just the ones in the dataset, since
they produce a function f.

198

T. HASTIE AND R. TIBSHIRANI

There is a Bayesian characterization of

It is often convenient to parametrize the smooth-
ing spline in terms of this tted vector f rather than
 in (3). This is an equivalent representation (Green
and Silverman, 1994), since f = B, where B is the
full-rank n M basis matrix evaluated at the n val-
ues of xi. An advantage is that one obtains expres-
sions that immediately suggest generalizations to
other smoothing methods.
f. By
choosing a particular partially improper Gaussian
prior for f,
(5)
the resulting posterior distribution of f has the form
(6)
with  =  2/2. Hence the smoothing spline is the
mean of the posterior distribution. Often it is con-
venient to parametrize S(cid:1)(cid:2) using df(cid:1)(cid:2) = tr S(cid:1)(cid:2),
the effective degrees of freedom, which is monotone
in . We have assumed that  2, 2 and hence , are
xed.

f(cid:15)y  N(cid:1)S(cid:1)(cid:2)y(cid:4) S(cid:1)(cid:2) 2(cid:2)

f  N(cid:1)0(cid:4) K2(cid:2)(cid:4)

(cid:3)(cid:8)f

Here and elsewhere, the notation K refers to a
generalized inverse of a matrix K, with the under-
standing that an eigenvalue of zero for K gives an
eigenvalue of + for K. In the case of smooth-
ing splines and the parametrization f, K computes
(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx = f TKf, and the
the penalty in (2):
zero eigenvectors correspond to linear functions
of x. The prior therefore gives innite variance
to linear functions (is vague), and hence they are
unrestricted. More details on K for splines are
given in Appendix A, as well as Hastie and Tibshi-
rani (1990). More generally, for symmetric smoother
operators S(cid:1)(cid:2) we can identify a prior covariance
(7)
where S indicates a generalized matrix inverse.
See Buja, Hastie and Tibshirani (1989) for more
details.

K = (cid:8)S(cid:1)(cid:2)  I(cid:10)(cid:4)

In this paper our interest is not just the mean
but the entire posterior distribution of f given in
(6). Throughout the paper we use the notation z =
(cid:1)z1(cid:4) z2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) zn(cid:2)T to represent a vector of indepen-
dent N(cid:1)0(cid:4) 1(cid:2) variates. Notice that (6) can be written
as

(8)

f = Sy + S1/2z(cid:4)

where we have dropped the dependence on . There-
fore we can generate a posterior realization of f
by adding the noise S1/2z to the tted smoothing
spline. The quantity S1/2z can be generated ef-
ciently with the same order of computations as
Sy, typically O(cid:1)n(cid:2). In Appendix A, we give two
algorithms for this, one exclusively for smoothing

Fig. 2. Los Angeles air pollution data: Upland Maximum Ozone
vs Daggot Pressure Gradient. Shown 100 realizations from the
posterior distribution f(cid:15)y(cid:4) with the smoothing parameter xed
at df = 5(cid:3) The smoothing-spline (cid:1)posterior mean(cid:2) is shown with
a thick(cid:4) dark curve(cid:3) Included are the pointwise 95% posterior
intervals, computed exactly(cid:3)

splines and the other for general smoothing oper-
ators. Although (8) is derived for cubic smoothing
splines, by analogy we can use it for any smooth-
ing operator, even nonlinear smoothers. Once
again, expression (8) can be used to generate pos-
terior realizations at any arbitrary input values
t1(cid:4) t2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) tm, including ones not in the dataset.

Figure 2 shows an example. The response vari-
able represents ozone measurements on 330 days
in the Los Angeles basin, and the predictor vari-
able is the pressure gradient measured at the Dag-
got Airport. The gure shows 100 realizations of
the posterior distribution, using a cubic smoothing
spline with a xed number (df = 5) degrees of free-
dom, and  xed at the unbiased estimate  2 =

cid:1)yi  yi(cid:2)2/(cid:1)n df(cid:2). We used a burn-in period of

500 iterations. Convergence issues for Markov chain
Monte Carlo methods are important, but there is
insufcient space to address here. See, for example,
Gelman, Stern and Rubin (1995) for a general dis-
cussion and references. The gure suggests that the
variance of log ozone is not constant as a function
of Daggot pressure gradient. An appropriate trans-
formation of the response might help alleviate this,
but we do not pursue that here.

The gure includes pointwise 95% posterior
bands, which can in fact be computed exactly from
the diagonal of S 2 [also in O(cid:1)n(cid:2) operations]. While
they show the shadow of the posterior distribution,
they do not show the individual realizations. In
Section 3.1 we make use of the individual real-
izations, and display the posterior distributions of

interesting functionals of them. Here the smoothing
parameter  is xed at df(cid:1)(cid:2) = 5; in Section 4 we
show how to incorporate priors for the smoothing
parameters and  2.

Notice that adding noise Sz in (8) would give
posterior covariance S2, which is not the same as
S 2 since S is not idempotent. In fact, S2 2 is the
frequentist covariance of Sy, and S 2  S2 2: the
posterior covariance exceeds the frequentist covari-
ance because it incorporates prior uncertainty.

There is a version of result (8) for simple linear

and multiple regression. Suppose

(9)

yi = xi + i(cid:5)
  N(cid:1)0(cid:4) 2(cid:2)(cid:3)

i  N(cid:1)0(cid:4)  2(cid:2)(cid:4)

Letting x = (cid:1)x1(cid:4) x2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) xn(cid:2)T, the posterior distri-

bution of xT is

xT(cid:15)y  N(cid:1)H(cid:1)2(cid:2)y(cid:4) H(cid:1)2(cid:2) 2(cid:2)(cid:4)

(10)

where

H(cid:1)2(cid:2) = x(cid:1)xTx + I/2(cid:2)1xT

(11)
with I being the nn identity matrix. As for general
smoothers we can write the posterior realizations as
(12)

xT = H(cid:1)2(cid:2)y + H(cid:1)2(cid:2)1/2z(cid:3)

Taking    to represent prior ignorance, then
H(cid:1)2(cid:2)  H(cid:1)(cid:2) = x(cid:1)xTx(cid:2)1xT = H, the hat matrix.
The operator H is an idempotent projection matrix
and H1/2 = H, so that the posterior realizations can
be written in the simpler form,
(13)

xT = Hy + Hz(cid:3)

3. ADDITIVE MODELS AND
BAYESIAN BACKFITTING

We now consider the main topic of this paper,
Bayesian posterior sampling for additive models.
Our data consists of n observations of an out-
come variable and p inputs: we write this as
(cid:1)x1(cid:4) y1(cid:2)(cid:4)(cid:1)x2(cid:4) y2(cid:2)(cid:4) (cid:3) (cid:3) (cid:3) (cid:4)(cid:1)xn(cid:4) yn(cid:2) with xi = (cid:1)xi1(cid:4) xi2(cid:4)
(cid:3) (cid:3) (cid:3) (cid:4) xip(cid:2). Our model is
(14)

i  N(cid:1)0(cid:4)  2(cid:2)(cid:3)

yi =  + p(cid:1)
fj(cid:1)xij(cid:2) + i(cid:5)
(cid:1)

j=1

For identiability between  and the fj(cid:4) j > 0 we
assume
(15)

fj(cid:1)xij(cid:2) = 0

j(cid:3)

i

Suppose we dene a cubic smoothing spline opera-
tor Sj(cid:1)j(cid:2) for each input variable j. Then the back-
tting procedure for estimating the fjs uses itera-
tions of the form
(16)

y  y1 (cid:1)

(cid:5)

(cid:6)

fj  Sj

fk

k(cid:21)=j

BAYESIAN BACKFITTING

199
for j = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:4) 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) . At each stage, the most
current values of the functions fk are used on the
right-hand side, forming a partial residual that is
smoothed as a function of xj.

Rather than obtain estimates of the fj, which in
Bayesian language means to compute the MAP esti-
mates (here, the posterior means) we want to gen-
erate from their joint distribution. To achieve this
we simply add the appropriate noise to the estimate
at each backtting step. For ease of notation dene
f0 = 1 and the associated operator S0 = 11T/n.
Recall that the variance  2 is considered to be xed.
We dene the Bayesian backtting algorithm as fol-
lows.

Algorithm 3.1. Bayesian backtting.
 Take initial values for f 0
 Do for t = 1(cid:4) 2(cid:4) 3(cid:4) 4(cid:4) (cid:3) (cid:3) (cid:3) :

j (cid:4) j  0.

 Do for j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p:

* Dene the partial residual
k>j f t1

rt
* Generate zt
j  Sjrt
f t

k<j f t
j  N(cid:1)0(cid:4) 1(cid:2) and update
j + S1/2
j zt

.

k

j

 Until the joint distribution of (cid:1)f t

0(cid:4) f t

1(cid:4) f t

p(cid:2)
2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) f t

doesnt change.

The most appropriate starting values in the rst
step are the tted curves from a standard additive
model t to the data. At the end of the procedure,
the phrase doesnt change means that the proce-
dure has converged to an appropriate stationary dis-
tribution. Convergence may be checked in practice
in a number of ways; see, for example, the discus-
sion in Gelman et al. (1995).

Bayesian backtting is the Gibbs sampling pro-
cedure applied to additive models. Gibbs sampling
(Geman and Geman, 1984; Gelfand and Smith,
1990) for general random variables A1(cid:4) A2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) Ap
operates by successive sampling of each Aj condi-
tional on the other Ak. At steady state a complete
cycle delivers a sample from their joint distribu-
tion. The connection between Bayesian backtting
and Gibbs sampling is established by examining
the conditional distribution of each fj. For cubic
smoothing splines this connection is clear from the
above development, but we give a more general
result for a larger class of operators Sj.
Let Sj be any symmetric matrices with eigen-
values in (cid:8)0(cid:4) 1(cid:10). Dene priors on the fj by fj 
N(cid:1)0(cid:4)(cid:1)S
j  I(cid:2) are sym-
metric with eigenvalues in (cid:8)0(cid:4)+(cid:10) and hence are
nonnegative denite. Consider  2 to be xed (as well

j  I(cid:2) 2(cid:2)(cid:3) The matrices (cid:1)S

200

T. HASTIE AND R. TIBSHIRANI

(cid:8)

(cid:7)
(cid:7)

fj(cid:15)y (cid:1)
(cid:7)
y (cid:1)

Sj

as the smoothing parameter implicit in Sj). Then
fk(cid:4)(cid:23)fk(cid:4) k (cid:21)= j(cid:24)
(cid:1) (cid:1)fj(cid:15)y(cid:4) fk(cid:4) k (cid:21)= j(cid:2)= (cid:1)
(cid:8)
(cid:8)
(17)

k(cid:21)=j

= N

fk

(cid:4) Sj 2

(cid:3)

k(cid:21)=j

Hence Bayesian backtting corresponds to sampling
from the conditional distribution of each fj. By the
results in Tierney (1994), the joint distribution of
the iterates (cid:1)f0(cid:4) f t
p(cid:2) convergences to that of
the true distribution of (cid:1)f0(cid:4) f1(cid:4) f2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) fp(cid:2)(cid:15)y. Further-
more, sample averages of functions of these quanti-
ties converge to their true values. This holds since
the conditional densities are everywhere positive
and hence the Markov chain is ergodic.

2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) f t

1(cid:4) f t

Figure 1 shows 50 realizations for the addi-
tive model t to four air pollution variables. We
xed the degrees of freedom of the smoothers at
4(cid:3)6(cid:4) 4(cid:3)8(cid:4) 2(cid:3)8(cid:4) 6(cid:3)0, which are the values obtained
from generalized cross-validation using an adap-
tive backtting procedure (Hastie and Tibshirani,
1990, Chapter 9: the BRUTO procedure). From this
information we can form posterior bands for the
functions or carry out Bayesian inference for any
other quantity of interest.

Recall the additive model constraints (15). These
are necessary to ensure that the posterior distri-
bution of  and the fj is not singular. Practically
speaking, it means that in the Bayesian backtting
algorithm, we have to center the ts after smoothing
and generation. We discuss this and more sophisti-
cated decorrelation procedures in Appendix A.

The standard backtting algorithm is a general,
modular method for tting a wide variety of addi-
tive models. One chooses the smoother operator
Sj for each input, and then backts to estimate
the joint model. The operator Sj can t a exi-
ble smooth, a linear regression (including dummy
variable ts), an adaptive regression (e.g., wavelet
smoother), and, more generally, any regression
operator. Convergence has only been proved for a
certain class of xed, nonadaptive operators (Buja,
Hastie and Tibshirani, 1989), such as smoothing
splines, but the algorithm seems well behaved in
general.

In the same way, we can choose an operator Sj for
each input, and then paste them together as condi-
tional sampling steps of the form
fj  Sjrj + S1/2

(18)
in the Bayesian backtting algorithm (see Appen-
dix A for a general procedure for computing S1/2z.)
Given a single input xj = (cid:1)x1j(cid:4) x2j(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) xnj(cid:2)T, we
summarize some of the possibilities for choice of Sj:

j zj

1. Smoothing Splines. Sj

computes a cubic
smoothing spline. The conditional sampling
step corresponds to the Gaussian process prior
fj  N(cid:1)0(cid:4) 2K
can be com-
puted in O(cid:1)n(cid:2) operations, the latter discussed
in Appendix A.

j(cid:2). Both Sj and S1/2

j

j

3. Fixed linear effects. Sj = Xj(cid:1)XT

we simply obtain   N(cid:1)ave(cid:8)y p

2. General nonparametric smoother. Sj denes
the smoothing operation, with implicit prior
j  I(cid:2) 2(cid:2). The operator S1/2
fj  N(cid:1)0(cid:4)(cid:1)S
is
applied using Algorithm A.1 in the Appendix.
j Xj(cid:2)1XT
j ,
where Xj is a matrix consisting of one or
more predictors. This results from the model
fj = Xjj with j  N(cid:1)0(cid:4) 2D(cid:2) and D diago-
j = Sj and is easily
nal, and   . Then S1/2
applied. For the intercept term, for example,
1 fj(cid:10)(cid:4)  2/n(cid:2).
j Xj +
 21(cid:2)1XT
j j
with j  N(cid:1)0(cid:4) (cid:2). Algorithms for implement-
ing these random effects smoothers are very
similar to those used in smoothing splines,
which we discuss in Appendix A. We look more
closely at a special case in the mixed effects
example below.

4. Random linear effects. Sj = Xj(cid:1)XT

j . This results from fj = XT

3.1 Example: Growth Curves

The data in the top left panel of Figure 3 are
measurements of spinal bone mineral density for a
sample of 153 girls, as a function of age. There are
between two and four measurements per girl, 471 in
all. The consecutive data for each girl are connected
in the plot. We see a great deal of between-girl vari-
ation, and a clear indication of the growth spurt
around age 12. There is also a strong ethnic effect
that is hidden in the variation of the growth frag-
ments. A goal is to characterize the growth behavior
and establish whether ethnic differences exist.

yij = f(cid:1)tij(cid:2) + xT

We consider the mixed effects model:
i E + Vi + ij(cid:4)

(19)
where:
 yij is the bone mineral density for girl i mea-
sured on occasion j, for i = 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) 153, and j =
1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) ni with ni  (cid:23)2(cid:4) 3(cid:4) 4(cid:24).
 f(cid:1)tij(cid:2) is the population growth curve as a func-
tion of the age measurement tij made on girl i on
occasion j.
 E is an effect due to ethnic class; the data
consist of white, black, Asian and Hispanic North
American girls. xi is any appropriate coding of con-
trasts to represent the 4-level factor.
 Vi is a random girl effect that allows a separate
vertical shift in f.

BAYESIAN BACKFITTING

201

Fig. 3. The top left panel contains 471 measurements of bone mineral density against age for 153 girls of different ethnic origin(cid:3)
Repeated measurements are connected(cid:3) The remaining three panels show 100 posterior realizations from model (cid:1)19(cid:2)(cid:3) In the nal panel(cid:4)
each random effect distribution is plotted against the mean age for that girl(cid:3)

 ij is measurement and other variation, which
we assume to be i.i.d.

A standard frequentist approach for tting such
data would be to treat f and the parameters in
E as parametric xed effects, and Vi as a ran-
dom effect. One could model f by polynomials or
more exibly by splines with selected knots in
age. Typically one assumes the Vi  N(cid:1)0(cid:4)  2
V(cid:2)
independently across girls, and ij  N(cid:1)0(cid:4)  2(cid:2) inde-
pendently across all measurements. Estimation of
these mixed effects models is typically done by
maximum likelihood (Laird and Ware, 1982), and
focuses on:

standard errors,

 The parameters of the xed effects and their
 The variance components  2
 The posterior mean or BLUP estimates of the

V and  2,

Vi.

Here we take a Bayesian approach, and treat
everything as random. We treat f as random, and

use the smoothing spline process prior. We also
treat the coefcients E as random, but with a at
prior. For the moment we assume the variance com-
ponents are xed (df  9, V  0(cid:3)11 and   0(cid:3)03),
and focus on generating realizations from the joint
posterior. In Section 4 we describe a variety of
methods for estimating the variance components
as well, including the REML method and the fully
Bayesian procedure that was used here.

rij

(cid:4)

 2

(cid:8)

(cid:4)

(20)

The set-up is tailor-made for the Bayesian backt-
ting procedure. The random effects have conditional
posterior distributions
Vi(cid:15)y(cid:4) f(cid:4) E  N

(cid:7) ni(cid:1)

ni + V

ni + V
i E and V =  2/ 2
V.

j=1
where rij = yij  f(cid:1)tij(cid:2)  xT
The remaining three panels in Figure 3 show 100
realizations from the model. The 153 posterior real-
izations for the random effects are shown vertically
in the last panel, centered at the average age for
that girl. Figure 4 focuses on the posterior realiza-

202

T. HASTIE AND R. TIBSHIRANI

GCV (Wahba, 1990), or related methods aimed at
minimizing prediction error on future observations.

We give more details on the rst two of these (in
reverse order).

4.1 REML, ML and Empirical Bayes

Model (19) can be regarded as an hierarchical
mixed effects model. The function f is random at
level 0 (a single coefcient vector), while the Vi
are random at level 1 (a coefcient per cluster).
Mixed effects models are typically t by maximum
likelihood or REML (Laird and Ware, 1982), and the
popular packages such as SAS and Splus have rou-
tines for tting them. Maximum likelihood provides
estimates of the variance components, 2,  2
V and  2
in this case, the parameters of the xed effects, and
the BLUP or posterior mean estimates E(cid:1)f(cid:15)(cid:23)yij(cid:24)(cid:2)
and E(cid:1)Vi(cid:15)(cid:23)yij(cid:24)(cid:2) of the random effects. Restricted
maximum likelihood (REML) is a slight modica-
tion which takes into account the degrees of freedom
used in estimating the xed effects, when estimat-
ing the variance components (like the n  1 versus
n correction in the sample variance.)

The empirical Bayes approach is to form the
marginal likelihood by integrating out everything
random and then estimating the remaining hyper-
parameters by maximum likelihood. It turns out,
that if the xed effects are given a at prior, then
empirical Bayes is equivalent to REML (Laird and
Ware, 1982)

Treating smoothing splines as random effects and
estimating 2 by REML is not a new idea (Speed,
1991), also known as GML in the spline literature
(Wahba, 1990). Lin and Zhang (1997) in fact use
REML in this way to estimate the smoothing param-
eters for additive spline models. Their approach is
to represent the functions as fj = Pjj with j 
N(cid:1)0(cid:4) 2
jI(cid:2), and treat the Pj as a block of regression
variables with random coefcients j. The number
of columns in Pj is mj  2, where mj is the number
of unique elements of xj. In general their algorithm
j mj(cid:4) n(cid:2)3(cid:2) computations, and so defeats
our purposes here of efciency. A promising alterna-
tive is to approximate Pjj by P
j has
a xed number (10 or 15) columns, for the purpose
of estimating the variance components efciently.
Approximations of this kind, based on the leading
eigenvectors of K, are developed in Hastie (1995)
(but are put to different uses there).

is O(cid:1)min(cid:1)

j

j, where P

4.2 Priors for the Variance Components

A more mainstream Bayesian approach would be
to provide priors for the variance components and
integrate. Wong and Kohn (1996) suggest the fol-

Fig. 4. One hundred posterior realizations of the derivative of f(cid:3)
Included in the plot are the distributions of two functionals(cid:27) the
location of the maximum and the location of the point at which
growth is less than 0(cid:3)5% per year(cid:3)

tions for f. Since each realization is a natural cubic
spline, we are easily able to produce the derivatives
for each curve (in which the natural boundary con-
ditions are evident). These are displayed, along with
the posterior distributions of two functionals:

 The location of the maximum, which is the
age at which the growth velocity is fastest.
This distribution is fairly tightly concentrated
at 12.5 years old.
 The location of the point at which bone growth
increase levels off. We have used a threshold of
0.005, which corresponds to 0.5% per annum.
This distribution is rather spread out; indeed, the
derivative posterior is rather wiggly in this region.

4. ESTIMATING THE VARIANCE COMPONENTS

In the preceding development, the smoothing
j, j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p,
parameters or variances  2 and 2
were considered xed. In practice they have to be
determined as well. There are several approaches:
 The full Bayesian approach, where we put pri-
ors on the variance components and estimate their
posterior along with the functions;
 The empirical Bayes approach, that treats the
variance components as parameters, which are esti-
mated by maximum (marginal) likelihood.
 The frequentist approach, that treats every-
thing as a smoother or regression tting method,
including the random effects operators. All the
parameters are then estimated by cross-validation,

BAYESIAN BACKFITTING

203

(21)

p(cid:1)2

lowing priors:
j(cid:2)  1
2
j
p(cid:1) 2(cid:2)  1
 2

(22)

exp(cid:1)j/2

j(cid:2) with j = 1010(cid:4)

(cid:3)

These priors are almost indistinguishable. The prior
for  2 makes the prior for log(cid:1) 2(cid:2) at. The prior for
2
j is almost at and still improper, and we give some
insight into the additional term involving j later in
this section. Both these priors are conjugate for the
Gaussian distribution and lead to inverse gamma
posterior distributions. More generally, one can use
proper inverse gamma priors

(23)

p(cid:1) 2(cid:2) 

exp(cid:1)/ 2(cid:2)(cid:4)

r > 0

(cid:7)

(cid:8)r+1

1
 2

for which both the above are degenerate special
cases.

Since 2

j enters the model only through fj, the

p(cid:1)2

corresponding conditional distributions are
j(cid:15)y(cid:4)  2(cid:4)(cid:23)fj(cid:4) j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:24)(cid:2)
(cid:8)
= p(cid:1)2
 exp

(cid:7)
2 +r+1(cid:2)
j(cid:15)fj(cid:2)  (cid:1)2
j Kjfj + j
 1
2f T

j(cid:2)(cid:1) n

(24)

(cid:3)

2
j

2(cid:15)(cid:15)e(cid:15)(cid:15)2 + (cid:2) where e = y 

This is an inverse gamma distribution IG(cid:1)n/2 +
j Kjfj + j(cid:2).
r(cid:4) 1
2f T
Similarly the conditional distribution of  2 is
IG(cid:1)n/2 + r(cid:4) 1
j fj. To
generate from the full posterior distribution, we
include conditional sampling steps for the 2
j and
 2 in the Bayesian backtting algorithm. Theoret-
ical convergence of the procedure to stationarity
is unaffected. Note that calculation in (24) of
j Kfj  f T
j S
f T
j fj requires
no new computation besides inner products, since
j fj = rT
j z +  2zTz, and Sjrj
j S
f T
and S1/2

j  I(cid:2)fj = f T
j S1/2

j (cid:1)S
j Sjrj + 2rT

j z are already available.

j fj  f T

The posterior realizations of the air-pollution
functions do not look any different from those in
Figure 1, so we do not repeat them here. The real-
izations produced for the bone data in Figures 3
and 4 were obtained in the manner just described.
Figure 5 shows the posterior distributions of the
degrees of freedom dfj from Bayesian backtting,
both for the air pollution data and the bone-growth
data. The degrees of freedom are a one-to-one func-
tion of j =  2/2
j: df(cid:1)(cid:2) = tr S(cid:1)(cid:2). Estimated
optimal values from generalized cross-validation
(GCV) are indicated by horizontal broken lines for
the air pollution data. Compared to GCV, the fully

Bayesian procedure applies slightly less smooth-
ing (more degrees of freedom) to inversion base
temperature, but the t does not change much.

For the lower two panels, the horizontal lines indi-
cate the df chosen by REML, which appear to match
these posterior realizations more closely. Since the
between-girl variation,  2
V, is very large compared
to the within-girl  2, not much shrinking is done
from the maximum of 153 df for the V effect.

For the remainder of this section, we investigate
the priors (21) and (22) in terms of the prior degrees
of freedom and develop a more general framework
for any smoother. One might ask what the implicit
prior is for df, given particular priors on  2 and 2.
Holmes and Mallick (1997) and Hodges and Sargent
(1998) similarly investigate priors based on degrees
of freedom. Figure 6 shows the implied prior dis-
tributions for df for two commonly used priors on
the variance components, obtained by simulation.
The X-values are taken to be 50 uniformly spaced
observations on (cid:8)0(cid:4) 1(cid:10). For any given pair  2 and 2,
we compute  =  2/2, and df(cid:1)(cid:2) = tr S(cid:1)(cid:2), where
S(cid:1)(cid:2) is the smoothing spline operator applied to the
50 values of X. Notice that if K = UDUT is the
eigen-decomposition of K in S(cid:1)(cid:2) = (cid:1)I+K(cid:2)1, then
j=1 1/(cid:1)1 + dj(cid:2) and can be computed ef-

df(cid:1)(cid:2) =

ciently for different values of .

 Using the prior (21) for both p(cid:1) 2

 In the left panel, we have used at improper pri-
ors (22) for both log  2  1, log 2  1. The prior for
df puts mass 1
2 on 2 and 50, the linear and interpo-
lating ts! This is easily proved (see appendix), and
has some negative consequences on the Gibbs sam-
pler. It implies that these two states are absorbing,
and hence the real posterior would end up in one of
these states as well.
j(cid:2)  (cid:1)1/ 2
j(cid:2)
exp(cid:1)j/ 2
j(cid:2) and likewise for 2, one sees exactly
the same behavior. This prior is still improper, but
the presence of  = 1010 appears to prevent the
absorptions at the two extreme states.
 In the right panel, we use IG(cid:1)0(cid:3)01(cid:4) 0(cid:3)01(cid:2), con-
sidered to be reasonably at proper priors in the
MCMC literature (Spiegelhalter, Best, Gilks and
Inskip, 1996). The histogram was obtained by sim-
ulating 10(cid:4)000 values from these priors. It exhibits
very similar behavior to the rst, although it
appears there is support everywhere. A Gibbs sam-
pler, starting at some value of df in this at interior
immediately concentrates the posterior away from
the boundaries and appears not to run into trouble.

In all cases the strong U-shape is troublesome and
does not seem very sensible as a prior for df. After
some experimentation, we found that priors  2 

T. HASTIE AND R. TIBSHIRANI

Fig. 5. Top four panels(cid:27) 5000 posterior realizations of the df for each predictor for the air pollution data(cid:3) Estimated optimal values
from generalized cross-validation are indicated by horizontal broken lines(cid:3) Lower two panels(cid:27) 3700 posterior realizations of df for the
age curve and the random effect V for the bone growth data(cid:3)
IG(cid:1)2(cid:4) 0(cid:3)01(cid:2) and 2  IG(cid:1)0(cid:3)5(cid:4) 0(cid:3)01(cid:2) gave a reasonable
prior for df, without the right spike (Figure 7).

range of df. Since df is monotone with , a mea-
sure of noise-to-signal ratio, it is quite reasonable
to generate these independently of each other.

Here is an alternative strategy that one might use
for prior selection. One could use the usual prior
for  2, but then pose a prior p(cid:1)df(cid:2) for df itself,
rather than indirectly through 2, and avoid the
rather strange right tail behavior. This prior might
put more mass on smoother models than rough (as
in Figure 7), or might itself be at over the entire

The posterior (24) is expressed in terms of K,
the penalty matrix for a smoothing spline or sim-
ilar smoother, which is based on a prior covariance
2K for f. Since K(cid:1)(cid:2) = (cid:1)S(cid:1)(cid:2)  I(cid:2) = K, and
parametrizing the smoother through df rather than
, we get an equivalent representation for the prior

BAYESIAN BACKFITTING

205

Fig. 6. Left panel(cid:27) The prior distribution of df based on a at improper prior for log 2 and log 2(cid:3) Right panel(cid:27) the prior for df based
on fairly noninformative proper priors IG(cid:1)0(cid:3)01(cid:4) 0(cid:3)01(cid:2) prior for 2 and 2(cid:3)

The joint posterior distribution is now

p(cid:1)f(cid:4) i(cid:4) Vi(cid:4) E(cid:15)y(cid:2)

(cid:9)

ni(cid:1)

 K(cid:1)

i=1

(27)

(cid:1)yij  f(cid:1)tij  i(cid:2)  xT

j=1

 2

+ 2
i
 2


i E  Vi(cid:2)2
(cid:10)

+ V2
i
V

+ J(cid:1)f(cid:2)
 2
f

Fig. 7. The implied prior for df based on 2  IG(cid:1)2(cid:4) 0(cid:3)01(cid:2) and
2  IG(cid:1)0(cid:3)5(cid:4) 0(cid:3)01(cid:2)(cid:3)

covariance: 2K =  2K(cid:1)df(cid:2) =  2(cid:1)S(cid:1)(cid:2)  I(cid:2).
The posterior distribution for dfj is then

(cid:7)
p(cid:1)dfj(cid:15)y(cid:4)  2(cid:4)(cid:23)fj(cid:4) j = 0(cid:4) 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:24)(cid:2)
j Kj(cid:1)df(cid:2)fj
 1
2f T

 (cid:15)Kj(cid:1)dfj(cid:2)(cid:15)1/2

p(cid:1)df(cid:2) exp

(25)

(cid:8)

(cid:3)

 2

 2
j

4.3 Example: Growth Curves Continued

(19)

The bone growth model

in Section 3.1
assumes that each girl has her growth spurt at the
same age. There is some evidence of the deciency
of this model in the lower right panel of Figure 3,
where the random effects distributions seem to
have larger variance  2
V around age 13. Here we
consider a richer model, that attempts to correct for
this deciency:

(26)

yij = f(cid:1)tij  i(cid:2) + xT

i E + Vi + ij(cid:3)

The parametrization is the same as before, except
we introduced an additional random effect, the age
shift i, which we assume has distribution N(cid:1)0(cid:4)  2
(cid:2).

up to a constant and the components of variance
(cid:4) V(cid:4) f and . The prior of E is at. We rst
produce the MAP estimates for all the random and
xed effects. This is a large penalized nonlinear
least squares problem.

 We have introduced an additional variance com-
ponent . In practice this needs to be estimated as
well, either via empirical or full Bayes methods. For
expediency, we selected  = 1(cid:3)5 based on a crude
grid search and the BIC statistic and base our sub-
sequent analysis on this value.

 We alternate between:
1. Fitting all the parameters holding the i xed.
This includes the variance components, as well
as the MAP estimates of Vi and f and .

2. Fixing all the parameters in step (i), and com-

puting the MAP estimates of the i.

The rst step (i) requires exactly the same technol-
ogy as in Section 3.1.
 The alternating procedure requires initial val-
ues for the i. Ignoring the Vi and xed effects, we
can produce an approximate collapsed version of the
model,

yi = f(cid:1)i(cid:2) + i(cid:4)
ti = i + i(cid:4)

where yi represents an average of all the y val-
ues for subject i, and so on. This has the form of
a nonlinear errors-in-variables model, and can be

T. HASTIE AND R. TIBSHIRANI

Fig. 8. The MAP estimates of the nonlinear random effects model(cid:3) The top left panel shows a principal curve t to the reduced data(cid:4)
from which initial estimates of i were obtained(cid:3) The top right panel shows the estimate of f(cid:4) along with the overall residuals ij(cid:3) The
lower left panel shows the estimated random effects Vi and the lower right the estimated i(cid:3) The latter are far more variable around the
growth spurt(cid:3)
estimated by the principal curves algorithm (Hastie
and Stuetzle, 1989).

rior

p(cid:1)i(cid:15)rest(cid:2)  ni(cid:1)

j=1

(28)

Figures 8 and 9 show the MAP estimates and
illustrate the effect of the inclusion of the ran-
dom age shift effects i on the tted random BMD
effects Vi.

This sample modication to the model has made
it quite nonlinear, and in particular the joint pos-
terior of (cid:1)i(cid:4) Vi(cid:2) will depend on where the obser-
vations lie. We do not expect to learn much about
i if the growth spurt is over and expect the pos-
terior distributions to look much like the prior. The
area where we can learn something is at the ear-
lier ages, where large deviations are attributable to
both horizontal and vertical shifts.

The Gibbs simpler for xed values of i proceeds
exactly as before. The only difcult part is sampling
from the posterior for i given the rest. The poste-

(cid:1)rij  f(cid:1)tij  i(cid:2)(cid:2)2

 2

+ 2
i
 2


(cid:4)

where rij = yij  xTE  Vi. This is a univariate
simulation problem, and we resort to a simple Tay-
lor approximation to f in (28),

f(cid:1)tij  i(cid:2)  f(cid:1)tij  i(cid:2)  f(cid:9)(cid:1)tij  i(cid:2)(cid:1)i  i(cid:2)(cid:4)

(29)
where i is the previous realization of i. We let
aij = f(cid:1)tij  i(cid:2), bij = f(cid:9)(cid:1)tij  i(cid:2) and uij = bij
i +
aij  rij, and after some simple calculations we nd
that
(cid:8)

p(cid:1)i(cid:15)rest(cid:2)
 N

(cid:4)

(30)

j=1 bijuij
ij +  2/ 2



j=1 b2

 2
ij +  2/ 2



j=1 b2

(cid:3)

BAYESIAN BACKFITTING

207

Fig. 9. The top left panel shows a single average age curve(cid:4) along with the various shifted versions obtained by adjusting for individual
values of i(cid:3) The top right panel is similar(cid:4) except the adjustments are now vertical shifts caused by the estimated random effects Vi(cid:3)
The lower left panel shows the movement of the Vi when the i are included in the model(cid:3)

Figure 10 (left gure) shows the joint distribution
of (cid:1)i(cid:4) Vi(cid:2) for nine particular values of i. The right
gure show the original data, with the four MAP
curves for each ethnic class and with the data for
the nine values of i indicated.

5. RELATIONSHIP TO BOOTSTRAP SAMPLING

There is a close relation between Bayesian back-
tting for additive models and the bootstrap applied
to standard backtting procedure.

Assume for simplicity that  2 is known. In the
standard backtting algorithm with smoothers Sj,
the tting values y and functions fj satisfy y =
Ay, fj = Ajy where the matrices A and Aj are
functions of Sj, j = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p.
It can be shown that the Bayes posterior functions
have marginal distributions,

(31)

fj  N(cid:1)Ajy(cid:4)(cid:1)I  Aj(cid:2)Sj(cid:1)I  Sj(cid:2) 2(cid:2)(cid:3)

On the other hand, suppose we carry out para-
metric bootstrap sampling by adding residuals r 
N(cid:1)0(cid:4) I 2(cid:2) to the t y = Ay giving responses y =
Ay + r. We then apply standard backtting to the
data y, giving
(32)

j 2(cid:2)(cid:3)
In the simple case of only one function (p = 1), we
have Aj = A = Sj, and the Bayesian and bootstrap
distributions are N(cid:1)Sjy(cid:4) Sj 2(cid:2) and N(cid:1)S2
j 2(cid:2).
Now S2
j < Sj for cubic spline smoothers and many
other smoothers, but typically the two are not very
different.

j = Ajy = Aj(cid:1)Ay+ r(cid:2)  N(cid:1)AjAy(cid:4) A2
f

jy(cid:4) S2

In the general case with p functions, the boot-
strap mean AjAy is what we obtain if we apply
backtting twice: once to y to obtain Ay and then
again to the response Ay. Hence it will tend to be
smoother than (but similar to) the Bayesian mean
Ay. The Bayesian covariance matrice reduces to
Sj 2 in the orthogonal case, that is, the inputs are



T. HASTIE AND R. TIBSHIRANI

Table 1

Average standard deviation for Bayes posterior

f1 and bootstrap realization f

1

(cid:2)
0.0
0.5
0.9

Bayes
0.41
0.43
0.51

Bootstrap

0.45
0.47
0.64

model, reducing the effect of collinearity in the pos-
terior. This interesting issue deserves further study.

6. GENERALIZED ADDITIVE MODELS

Hastie and Tibshirani (1986) introduced the gen-
eralized additive model for modeling non-Gaussian
data. This includes members of the exponential fam-
ily of distributions and other models such as the
proportional hazards model for survival data. For
a Bayesian analysis of this model, the conditional
distributions do not have a simple form in general,
as they do in the Gaussian case. Hence Gibbs sam-
pling is no longer convenient. However the basic
Gibbs step fj = Sjrj + S1/2
j z can instead be used
as a proposal distribution in a MetropolisHastings
algorithm, as we outline below.

We rst give some background on generalized
additive models. In the exponential
family, the
mean i of the response variable Yi is assumed to
be related to the inputs via

i  g(cid:1)i(cid:2) =(cid:1)

fj(cid:1)xij(cid:2)(cid:4)

j

(33)

(cid:1) =

where g(cid:1)(cid:2) is a specied function, known as the link
function. The functions fj(cid:1)(cid:2) are estimated by max-
imizing a penalized log-likelihood analogous to the
penalized least squares criterion used for Gaussian
additive models. Using vector notation, this crite-
rion has the form

J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) = log L(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) (cid:1)

(34)

jf TKjf (cid:3)

j

The function L(cid:1)(cid:1)(cid:4) (cid:2)(cid:2) is the likelihood of the data,
j fj, (cid:2) = (cid:1)1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) p(cid:4)  2(cid:2), the tuning parame-
ters and Kj is the penalty matrix, as dened previ-
ously. The local scoring algorithm for maximization
of J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2), proposed in Hastie and Tibshirani (1986),
is equivalent to a NewtonRaphson procedure. It
works by approximating the log likelihood by a
quadratic, resulting in a working response variate
vi. A weighted backtting algorithm is applied to
the response vi, and then vi is recomputed and the
process is repeated, until convergence. The actual

Fig. 10. Upper gure(cid:27) the joint posterior scatterplot for 100 real-
izations of (cid:1)i(cid:4) Vi(cid:2) for eight different values of i(cid:4) ordered from
left to right (cid:1)and bottom to top(cid:2) in age(cid:3) For large values of age(cid:4)
where f is at(cid:4) the posterior distribution of i has spread simi-
lar to the prior (cid:1)1(cid:3)5 units(cid:2)(cid:3) For values of i corresponding to age at
the growth spurt(cid:4) the posterior has smaller spread and is corre-
lated with Vi(cid:3) Lower gure(cid:27) the numbered data fragments show
the data and values of i corresponding to the eight panels in the
left gure(cid:3)

arranged on a lattice. In general however, it is not
clear how the Bayesian covariance compares to the
bootstrap covariance.
We did a small simulation with n = 20 obser-
vations and two bivariate standard Gaussian pre-
dictors having correlation , for  = 0(cid:4) 0(cid:3)5(cid:4) 0(cid:3)9. We
computed the matrices S1 and A1, and the resulting
square root of the average diagonal of the covariance
matrices. Table 1 shows the results. We see that
the standard deviations are roughly equal, except
when the correlation between the inputs is very
high. In that case, the bootstrap standard devia-
tion is nearly twice the Bayes posterior standard
deviation, and hence would lead to condence bands
that are nearly twice as wide. This may be due to
the assumption of proper independence in the Bayes

BAYESIAN BACKFITTING

209

forms for vi and wi are

(35)

i =
w1

vi = i + (cid:1)yi  i(cid:2)

(cid:7)

(cid:8)

i
i

vi(cid:4)

(cid:7)

(cid:8)

(cid:4)

i
i

(cid:14)

where vi is the variance of yi at i.
How can we simulate from the posterior density
exp(cid:1)J(cid:1)(cid:1)(cid:4) (cid:2)(cid:2)(cid:2)? The MetropolisHastings procedure
(Hastings, 1970) is a convenient approach here. In
general this method works as follows. Given a pos-
terior density (cid:1)u(cid:2) from which we wish to generate
realizations u, we dene a proposal distribution
q(cid:1)u(cid:4) v(cid:2) that species the probability of moving from
state u to v. If we are currently at state u, we gen-
erate a random state v according to q(cid:1)u(cid:4) v(cid:2), and
then move to v with probability
(cid:1)u(cid:4) v(cid:2)
(36)
=

if (cid:1)u(cid:2)q(cid:1)u(cid:4)v(cid:2) > 0,
if (cid:1)u(cid:2)q(cid:1)u(cid:4)v(cid:2) = 0.
In our application, we consider a move for a sin-
gle function fj  f(cid:9)
j, with all other parameters
j and let
the corresponding working responses and diagonal
weight matrices be v(cid:4) v(cid:9) and W(cid:4) W(cid:9). We choose as
the proposal distribution the normal approxima-
tion q(cid:1)fj(cid:4) f(cid:9)
j(cid:2) = N(cid:1)Sjv(cid:4) SjW1 2(cid:2) which results
from expanding the log-likelihood in a second-order
Taylor series. The move from f to f(cid:9) has the form

held xed. Let (cid:1) =

(cid:15)
(cid:1)v(cid:2)q(cid:1)v(cid:4) u(cid:2)
(cid:1)u(cid:2)q(cid:1)u(cid:4) v(cid:2) (cid:4)1

fj, (cid:1)(cid:9) =

min

k(cid:21)=j fk + f(cid:9)

1(cid:4)

(cid:4)

(37)

f(cid:9) = Sjv + S1/2

j W1/2z(cid:4)

which is just a weighted version of the basic oper-
ation in the Bayesian backtting procedure given
in Algorithm 3.1. The acceptance probability works
out to be

(cid:1)f(cid:9)(cid:2)q(cid:1)f(cid:9)(cid:4) f(cid:2)
(cid:1)f(cid:2)q(cid:1)f (cid:4) f(cid:9)(cid:2) =
(cid:15)S(cid:9)
jW(cid:9)1(cid:15)
eJ(cid:1)(cid:1)(cid:4)(cid:2)(cid:2)
(cid:15)SjW1(cid:15)
eJ(cid:1)(cid:1)(cid:9)(cid:4)(cid:2)(cid:2)

(38)

e(cid:1)1/2(cid:2)(cid:1)f(cid:9)
e(cid:1)1/2(cid:2)(cid:1)fjS(cid:9)

jSjv(cid:2)T(cid:8)SjW1(cid:10)1(cid:1)f(cid:9)

jSjv(cid:2)
jW(cid:9)1(cid:10)1(cid:1)fjS(cid:9)

jv(cid:9)(cid:2)T(cid:8)S(cid:9)

jv(cid:9)(cid:2) (cid:3)
For smoothing splines the operator that uses
observation weights W has the form Sj = (cid:1)W +
jKj(cid:2)1W, and the acceptance probability is easily
computed in O(cid:1)n(cid:2) operations.
j =  2/j and  2 can
The tuning parameters 2
be sampled in a similar way. In expression (24) for
the conditional distribution of 2
j, we again need to
j  I(cid:2)f is replaced
j Kjf. f t(cid:1)S
compute the penalty f T
j f = rTSWr +
by f tW(cid:1)S
2rTS1/2W1/2z + zTz, all quantities that are already
available.

j  I(cid:2)f and we have f tWS

The determinant ratio is not as easy to compute,
but is typically very close to 1 (all that changes are
the weights), and we can ignore it in the calcula-
tions. Details of this procedure, including an SPlus
software implementation and a comparison to the
related approach of (Zeger and Karim, 1991), will
appear elsewhere.

7. DISCUSSION

The additive model used here is a special case of
the Gaussian process model for exible regression.
In this class of models, a Gaussian process prior is
assumed for the regression function, and inference
is carried out from the posterior. Different choices
for the prior covariance function lead to particular
models: the additive smoothing spline model results
from the prior discussed in this paper. The general
Gaussian process model was proposed by OHagan
(1978). More recently Neal (1996) and Williams and
Rasmussen (1996) have explored the computational
aspects in depth. They use MCMC for the tuning
parameters, and an O(cid:1)n3(cid:2) procedure to obtain the
the mean and covariance of the Gaussian posterior.
This O(cid:1)n3(cid:2) operation can make the analysis infea-
sible for large n. Because of the banded nature of
the matrices arising in the cubic smoothing spline
model, we are able to reduce this computation to
O(cid:1)nM(cid:2) where M is the number of Gibbs sampling
steps.
As mentioned in the introduction, Wong and Kohn
(1996) provide an O(cid:1)n(cid:2) algorithm for the additive
spline model using the state-space representation
of splines, introduced in Ansley and Kohn (1996).
This framework is formally equivalent to that of
Wahba (1980). We make no claims that our proce-
dure is more efcient that theirs in the additive
spline model; rather we believe that our proposal
has the advantages of conceptual simplicity and
generality.

As suggested by a referee, extensions to scale mix-
ture and auto-correlated errors are possible using
the methods proposed in Smith, Wong and Kohn
(1998). We have also restricted ourselves to the use
of proper priors. In general, choosing the prior to
ensure that the posterior is proper can be a tricky
exercise, especially in random effects models. See
Hobert and Casella (1996) for detailed discussion of
this issue.

We have written several functions in the S-plus
language for implementing the ideas in this paper.
In particular, a function gibbs.gam() takes as input
a tted gam object (Chambers and Hastie, 1991) and
samples from the posterior distribution. The follow-

T. HASTIE AND R. TIBSHIRANI

ing lines produced the essential ingredients for the
gures in Section 3.1:

bonefit <- gam(spnbmd  s(age,9) + ethnic

+ random(factor(idnum)), data=Bonef)

bone.samples <-gibbs.gam(bonefit, nwarm=3600,

nkeep=100, var.comp=T)

Even though bonefit requested 9df in s(age,9),
this acts simply as a starting value in the call
to gibbs.gam(). The gam() object can specify any
number of smooth terms and random effects, and
they all get accommodated automatically. Although
random() is a rather simple random intercept
smoother, it is not difcult for users to provide
their own random effects methods.

The gibbs.gam collection will be made available
from the public archive at Carnegie-Mellon Univer-
sity: http://www.lib.stat.cmu.edu.
From a mixed effects or empirical Bayes point
of view, we have provided an O(cid:1)n(cid:2) algorithm for
sampling from the posterior distributions, given the
variance components, even when the random effects
(smoothing splines) have dimension n. The usual
backtting procedure delivers the posterior means
or BLUPs in O(cid:1)n(cid:2) computations. We are currently
exploring approximations that allow the estimation
of the variance components as well in O(cid:1)n(cid:2) com-
putations. We gave one such approximation in Sec-
tion 4.1.

APPENDIX A

ALGORITHMIC DETAILS

Algorithms for Generating S1/2z

We present two algorithms for generating an n-
vector S1/2z, where as before z is a vector of N(cid:1)0(cid:4) 1(cid:2)
variates. The rst algorithm is iterative, and uses
repeated applications of the smoothing operator S.
It has the same order of complexity as the smoother
and hence is O(cid:1)n(cid:2) if the smoother can be applied
with only O(cid:1)n(cid:2) calculations. This is the case for
many popular smoothers including cubic smooth-
ing splines, kernels and wavelet smoothers. The
second procedure is specically designed for cubic
smoothing splines, and uses the banded nature of
the covariance kernel to generate S1/2z. It is more
efcient than the rst algorithm but applicable only
to cubic smoothing splines.

General algorithm. Consider

the

Taylor

series

(39)

S1/2 = I  1
+ 3

2(cid:1)S  I(cid:2)
8(cid:1)S  I(cid:2)2  5

16(cid:1)S  I(cid:2)3 

Table 2

Number of iterations until convergence in 1000 experiments(cid:4)

for general S1/2z algorithm

3
25

4
162

5
252

6
237

7
157

8
104

9
41

10
14

11
5

12
3

and premultiply by S, giving

S1/2 = S  S1/2

(40)

= S  1

2 S(cid:1)S  I(cid:2) + 3

8 S(cid:1)S  I(cid:2)2
16 S(cid:1)S  I(cid:2)3  (cid:3)

 5

Hence we can apply S1/2 by repeated applications
of S representing the right-hand side of (40). This
leads to the following algorithm.

Algorithm A.1. General procedure for generat-

ing S1/2z.

1. Take z  N(cid:1)0(cid:4) I(cid:2). Set z(cid:9) = Sz(cid:4) z(cid:9)(cid:9) = z.
2. Do for b = 2(cid:4) 3(cid:4) (cid:3) (cid:3) (cid:3) (cid:4)

 z(cid:9)(cid:9)  3/2b
 z(cid:9)  z(cid:9) + Sz(cid:9)(cid:9)

(cid:1)b1(cid:2)  (cid:1)Sz(cid:9)(cid:9)  z(cid:9)(cid:9)(cid:2)(cid:5)

(cid:3)

3. Until (cid:15)(cid:15)Sz(cid:9)(cid:9)(cid:15)(cid:15) is small.
In step 2, the strange-looking multiplier (cid:1) 3

2 
b(cid:2)/(cid:1)b  1(cid:2) generates the coefcients in the Tay-
lor series (40). It is easy to show that z(cid:9)  S1/2z,
as long as S(cid:1)S  I(cid:2)b  0. This is true for any
symmetric smoother having eigenvalues in (cid:8)0(cid:4) 1(cid:10):
this includes cubic smoothing splines and some
symmetrized kernel smoothers. Note that for pro-
jections, S(cid:1)S  I(cid:2) = 0 and so convergence is
immediate (no iterations of step 2).

Table 2 shows the results of a simulation experi-
ment to examine the convergence of this procedure.
With a sample size n = 100, we generated a ran-
dom normal vector z and applied the above algo-
rithm with a cubic smoothing spline operator with
degrees of freedom randomly chosen between 2 and
20. The convergence criterion was max(cid:15)Sz(cid:9)(cid:9)(cid:15) < 0(cid:3)01.
The number of iterations until convergence for 1000
simulations is shown in Table 2. The convergence is
quite fast, never requiring more than 12 iterations
and usually no more than six or seven.

Algorithm for smoothing splines. When the
smoother S represents a smoothing spline, we
can implement a more precise and efcient algo-
rithm for generating S1/2z. Our implementation of
smoothing splines follows de Boor (1978), where we
represent the tted functions in a basis of cubic

BAYESIAN BACKFITTING

211

B-splines,

(41)

f(cid:1)x(cid:2) = M(cid:1)

j=1

bj(cid:1)x(cid:2)j(cid:3)

The number of basis functions M depends on the
number of unique values of x among the n input val-
ues xi, as well as the particular representation used.
In our case nu unique values of x dene nu  2 inte-
rior knots and a corresponding basis of M = nu + 2
cubic B-splines. If all the n values of x are unique,
then M = n + 2. The smoothing spline solution is
given by

(42)

f = Sy
= B(cid:1)BTB + (cid:2)1BTy
= B(cid:2)(cid:4)

where the n rows of the nM basis matrix B consist
of the vector of M basis functions b(cid:1)x(cid:2) evaluated at
the n sample values xi. The M  M penalty matrix
has elements

(cid:2)

ij =

i(cid:1)t(cid:2)b(cid:9)(cid:9)
b(cid:9)(cid:9)

j(cid:1)t(cid:2) dt(cid:3)

Likewise, the tted function at an arbitrary input
value x is given by

(43)

f(cid:1)x(cid:2)= bT(cid:1)x(cid:2)(cid:1)BTB + (cid:2)1BTy

= bT(cid:1)x(cid:2)(cid:2)(cid:3)

The coefcient estimates (cid:2) are the posterior mean

for (cid:2) based on a model y = bT(cid:1)x(cid:2)(cid:2) + , where:

   N(cid:1)0(cid:4)  2(cid:2).
 (cid:2) has a (degenerate) prior normal distribution
N(cid:1)0(cid:4) 2(cid:2) with  =  2/2. Here  has a two-
dimensional null space corresponding to parameters
leading to linear functions of x. It also gives effec-
tively innite penalty to nonzero second derivatives
at the boundary knots, and hence enforces the nat-
ural boundary conditions.
 The prior covariance matrix K for f in (5) is
 The above expressions generalize easily to the
case where each observation has a weight. This hap-
pens naturally in nonlinear likelihood settings as
in the next section, and also when the x values
are tied. In the latter case the observations are
collapsed onto the unique values of xi, the yi are
replaced by the average at the tied values of xi, and
the observations receive weights proportional to the
counts at each unique x.

BBT evaluated at the data.

Thus the posterior distribution for (cid:2) is (cid:2)(cid:15)y 
the posterior

N(cid:1)(cid:2)(cid:4)  2(cid:1)BTB + (cid:2)1(cid:2). Likewise,

distribution of f is

f(cid:15)y  N(cid:1)B(cid:2)(cid:4)  2B(cid:1)BTB + (cid:2)1BT(cid:2)

(44)

= N(cid:1)Sy(cid:4)  2S(cid:2)(cid:3)

Hence to simulate from this posterior, it is sufcient
to simulate a parameter (cid:3)  N(cid:1)0(cid:4)  2(cid:1)BTB+(cid:2)1(cid:2),
and hence we can produce a posterior realization of
the entire function.

It turns out that there is no additional computa-
tional burden over and above the usual smoothing
spline O(cid:1)n(cid:2) computations. The matrix B has four
nonzero bands, and both BTB and  are 4-banded.
This means that BTB +  = LTL has a 4-banded
cholesky factorization L (Silverman, 1984). This L
is computed as part of the smoothing-spline calcu-
lations, and is available as part of the t. Hence
(cid:3) = L1z  N(cid:1)0(cid:4)(cid:1)BTB + (cid:2)1(cid:2) if z  N(cid:1)0(cid:4) I(cid:2). We
obtain  by solving L(cid:3) = z, which takes O(cid:1)n(cid:2) com-
putations, because of the banded nature of L.

Modied Backtting and Efciency

In Section 3 we mentioned that the output of the
smoothers have to be centered, to avoid identiabil-
ity problems. Here we describe a more general cen-
tering that speeds up convergence of the Bayesian
backtting algorithm.

In the standard backtting algorithm, strong
correlations among the inputs can cause slow con-
vergence, because the procedure slowly seesaws
towards the solution. In Buja, Hastie and Tibshi-
rani (1989) a modied backtting algorithm was
proposed, in which all of the (linear) projections
for all of the inputs were t together, while the
iterative one-at-a-time smoothing was applied just
to the nonlinear parts of each function. This can
noticeably speed up the convergence of backtting,
because it immediately captures the linear corre-
lations. We let X denote the linear part of the
model (including intercept) with projection opera-
tor H, and Hj the operator that projects onto the
two-dimensional linear subspace of eigenvalue 1 of
Sj. Then the modied backtting algorithm uses
the smoothers H and Sj = Sj  Hj (for symmetric
smoothers, such as smoothing splines). Sj produces
the nonlinear part of the t for variable xj.
An analogous strategy can be used to speed up
the Bayesian backtting procedure (Liu, Wong and
Kong, 1994). We can separate each function f =
Xjj + f, where Xjj includes the constant and
linear part of fj. The prior and posterior distribu-
tions factor accordingly, Xjj(cid:15)y  N(cid:1)Hjy(cid:4)  2Hj(cid:2)
and f(cid:15)y  N(cid:1) Sjy(cid:4)  2 Sj(cid:2), and they are independent.
Notice as well that Sj
j Sj. Then we alter-
nately generate realizations of the linear component

1/2 = S1/2

212

T. HASTIE AND R. TIBSHIRANI

X all grouped together, separately from the non-
linear functions fj. The latter is achieved by rst
generating the usual realization fj, and then remov-
ing the linear trend fj = fj  Hjfj.

APPENDIX B

EXACT PRIOR FOR df BASED ON FLAT
PRIORS FOR VARIANCE COMPONENTS

Theorem. Consider

the Bayesian smoothing
spline model, on N unique values of x(cid:3) Let the prior
for 2 and  2 both be improper and at on the
log-scale(cid:5) p(cid:1)2(cid:2)  1/2 and p(cid:1) 2(cid:2)  1/ 2(cid:3) Then the
implicit prior on df is discrete, and puts mass 1
2 on
2 and N(cid:3)

Lemma. Consider the random variable VD =
1/(cid:1)1 + /ZD(cid:2)(cid:4) where log(cid:1)ZD(cid:2)  U(cid:8)D(cid:4) D(cid:10)(cid:3) Then
V = limD VD is 0 or 1 with probability 1
2 (cid:3)
(cid:8)

Proof of Lemma.

(cid:7)

(cid:16)

P(cid:1)VD > v0(cid:2) = P(cid:1)log(cid:1)ZD(cid:2) < log
vo/1  v0
(cid:16)

1 + log

= 1
2

(45)

D

(cid:17)

(cid:19)
(cid:18)
vo
1  v0

for v0 

1

1 + /eD

(cid:4)

1

1 + /eD

(cid:19)

(cid:3)

Now for any v0  (cid:1)0(cid:4) 1(cid:2),

lim

D P(cid:1)VD > v0(cid:2)= P(cid:1)V > v0(cid:2)

= 1
2 (cid:3)

Proof of Theorem. For a smoothing spline,

df = N(cid:1)

j=1

1

1 + dj

(cid:3)

(46)

(47)

Here  =  2/2, the di are the eigenvalues of the
N N penalty matrix K, and d1 = d2 = 0 and dj >
0 for j = 3(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) N. So for any xed value of  2, each
of the contributions for j > 2 is the same and either
0 or 1, and the rst two contributions are always
1. Thus for xed  2 df is 2 or N with probability
1
2 . Since this does not depend on  2, this is also
unconditionally true. Finally, since p(cid:1)log(cid:1)2(cid:2)(cid:2)  1,
we see that limD (cid:1) (cid:1)log ZD(cid:2) = (cid:1) (cid:1)log 2(cid:2).

We have not proved this for the nearly-at prior
p(cid:1)2(cid:2)  (cid:1)1/2(cid:2)  exp(cid:1)/2(cid:2) (which is in fact not
integrable and so also improper). Empirical evi-
dence suggests that this has the same distribution,
obtained by simulating from an IG(cid:1)(cid:4) (cid:2) density,

and studying the behavior of the quantiles of V as
 gets small.

One puzzling aspect of this prior distribution is
that it has no support except on these two extreme
points. This implies the posterior should be the
same. One explanation for this somewhat contra-
dictory behavior is that the Gibbs samplers are
never run long enough! df = N is an absorb-
ing state, since this implies that !e! = 0 and
hence the posterior for  2, IG(cid:1)N/2(cid:4)!e!2(cid:2) will pro-
duce a 0 with probability 1, leading to  = 0 and
another exact t. Likewise df = 2 is an absorb-
ing state if 1/2 is used for the prior. This is not
the case for (cid:1)1/2(cid:2) exp(cid:1)/2(cid:2), whose posterior is
2f TKf + (cid:2). Even though the penalty may
IG(cid:1)N/2(cid:4) 1
be zero (for exact linear ts), the presence of  > 0
protects!

ACKNOWLEDGMENTS

We thank Radford Neal for suggesting the use
of the MetropolisHastings procedure in Section 6,
Bernard Silverman for help with the smoothing
spline representation, Larry Wassermen, the Edi-
tor and two referees for helpful comments. Xihong
Lin was especially helpful in providing (personal
communication) an up-to-date survey of the mixed
effects eld and making her preprints available.
Trevor Hastie was supported in part by NSF
Grant DMS-95-04495 NIH Grant ROI-CA-72028-
01. Robert Tibshirani was supported by the Natural
Sciences and Engineering Research Council of
Canada.

REFERENCES

Ansley, C. and Kohn, R.

(1985). Estimation, ltering and
smoothing in state space models with diffuse initial condi-
tions. Ann. Statist. 13 12861316.

Buja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers
and additive models (with discussion). Ann. Statist. 17 453
555.

Carter, C. and Kohn, R. (1994). On Gibbs sampling for state

space models. Biometrika 81 541553.

Chambers, J. and Hastie, T. (1991). Statistical Models in S.

Wadsworth/Brooks Cole, Pacic Grove, CA.

de Boor, C. (1978). A Practical Guide to Splines. Springer, New

York.

Denison, D., Mallick, B. and Smith, A.

(1998). Automatic
Bayesian curve tting. J. Roy. Statist. Soc. Ser. B 60 333
350.

Gelfand, A. E. and Smith, A. F. M. (1990). Sampling based
approaches to calculating marginal densities. J. Amer.
Statist. Assoc. 85 398409.

Gelman, A., Carlin, J., Stern, H. and Rubin, D.

(1995).

Bayesian Data Analysis. CRC Press, Boca Raton, FL.

Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs
distributions and the Bayesian restoration of images. IEEE
Trans. Pattern Anal. Machine Intelligence 6 721741.

213

Green, P. and Silverman, B. (1994). Nonparametric Regres-
sion and Generalized Linear Models: A Roughness Penalty
Approach. Chapman and Hall, London.

Hastie, T. (1995). Pseudosplines. J. Roy. Statist. Soc. Ser. B

OHagan, A.

(1978). Curve tting and optimal design for
regression (with discussion). J. Roy. Statist. Soc. Ser. B
40 142.

Silverman, B. (1984). Spline smoothing: the equivalent kernel

58 379396.

Hastie, T. and Stuetzle, W. (1989). Principle curves. J. Amer.

Statist. Assoc. 84 502516.

Hastie, T. and Tibshirani, R. (1986). Generalized additive mod-

els. Statist. Sci. 1 295318.

method. Ann. Statist. 12 8989164.

Smith, M., Wong, C. and Kohn, R. (1998). Additive nonpara-
metric regression with autocorrelated errors. J. Roy. Statist.
Soc. Ser. B 60 311332.

Speed, T. (1991). Comment on That BLUP is a good thing: the

Hastie, T. and Tibshirani, R. (1990). Generalized Additive Mod-

estimation of random effects. Statist. Sci. 6 4244.

els. Chapman and Hall, London.

Hastings, W. K. (1970). Monte Carlo sampling methods using
Markov chains and their applications. Biometrika 57 97
109.

Hobert, J. and Casella, G. (1996). The effect of improper pri-
ors on Gibbs sampling in hierarchical linear mixed models.
J. Amer. Statist. Assoc. 91 14611473.

Hodges, J. and Sargent, D. (1998). Counting degrees of free-
dom in hierarchical and other richly parametrized models.
Technical report, Div., Biostatistics, Univ. Minnesota.

Holmes, C. and Mallick, B. (1997). Bayesian wavelet networks
for nonparametric regression. IEEE. Trans. Neural Net-
works. To appear.

Laird, N. M. and Ware, J. H. (1982). Random-effects models for

longitudinal data. Biometrics 38 963974.

Lin, X. and Zhang, D. (1997). Inference in generalized additive
mixed models. Technical report, Biostatistics, Dept., Univ.
Michigan.

Liu, J. S., Wong, W. H. and Kong, A. (1994). Covariance struc-
ture of the Gibbs sampler with applications to the compar-
isons of estimators and augmentation schemes. Biometrika
81 2740.

Neal, R. M. (1996). Bayesian Learning for Neural Networks.

Springer, New York.

Spiegelhalter, D., Best, N., Gilks, W. and Inskip, H.
(1996). Hepatitis B: a case study in mcmc methods.
In Markov Chain Monte Carlo in Practice (W. Gilks,
S. Richardson and D. Spegelhalter, eds.) Chapman and Hall,
London.

Tierney, L. (1994). Markov chains for exploring posterior distri-

butions (with discussion). Ann. Statist. 22 17011762.

Wahba, G. (1980). Spline bases, regularization, and generalized
cross-validation for solving approximation problems with
large quantities of noisy data. In Proceedings of the Inter-
national Conference on Approximation Theory in Honour of
George Lorenz. Academic Press, Austin, TX.

Wahba, G. (1990). Spline Models for Observational Data. SIAM,

Philadelphia.

Williams, C. and Rasmussen, C. (1996). Gaussian processes for
regression. In Neural Information Processing Systems 8 (D.
S. Touretzky, M. C. Mozer and M. E. Hasselmo, eds.) MIT
Press.

Wong, C. and Kohn, R. (1996). A Bayesian approach to estimat-
ing and forecasting additive nonparametric autoregressive
models. J. Time Ser. Anal. 17 203220.

Zeger, S. and Karim, M. (1991). Generalized linear models with
random effects: a Gibbs sampling approach. J. Amer. Statist.
Assoc. 86 7986.

Comment
R. Dennis Cook and Iain Pardoe

1. INTRODUCTION

Hastie and Tibshirani propose an intriguing idea,
neatly linking Bayesian modeling of the functions
in a generalized additive model with Gibbs sam-
pling to obtain posterior realizations of these func-
tions. Since their procedure utilizes only smoother
matrices for individual predictors, Sj, partial resid-
uals, rj, and normal random vectors, zj, the method
would appear to be applicable to any models with
additive components that can be expressed in the
form Sjy.

R. Dennis Cook is Professor and Iain Pardoe is
Graduate Student at the School of Statistics, Uni-
versity of Minnesota, St. Paul, Minnesota 55108
(e-mail: dennis@stat.umn.edu).

A natural question to ask of any proposed method-
ology is to what use can it be put? Hastie and
Tibshiranis examples, while interesting in them-
selves, left us questioning what information could
be gleaned from plots such as Figures 1 and 2
for the ozone data and Figure 3 for the growth
curves data. For example, do the individual realiza-
tions in Figure 2 add anything to the information
already provided by the pointwise posterior inter-
vals? Figure 4 goes some way to addressing these
thoughts with a graphical display of two function-
als of the posterior realizations. We decided to pur-
sue these thoughts in a different direction, that of
model checking, and we outline our ndings in Sec-
tion 2. We discuss other potential applications in
Section 3 and make some more general comments in
Section 4.

214

Fig. 1. Marginal model plot for the tted values for the additive
model t to four air pollution variables(cid:3)

Fig. 4. Gibbs marginal model plot for inversion base height for
the additive model t to (cid:1)w1(cid:4) w2(cid:4) w12(cid:2)(cid:3)

2. MARGINAL MODEL PLOTS

The goal of a regression analysis can be expressed
as inference about the dependence of an unknown
cdf F of the conditional random variable y (cid:15) x on
the value of x. Consider a generic regression model
for y (cid:15) x represented by the cdf M; estimating this

model gives rise to an estimated cdf (cid:20)M. We now
(cid:20)M. We use the fact that F(cid:1)y (cid:15) x(cid:2) = M(cid:1)y (cid:15) x(cid:2) for

consider graphics for comparing selected character-
istics of F to the corresponding characteristics of

all values of x in its sample space if and only if
F(cid:1)y (cid:15) h(cid:2) = M(cid:1)y (cid:15) h(cid:2) for all functions h = h(cid:1)x(cid:2). This
is a more general version of the approach proposed
by Cook and Weisberg (1997) which sets h = aTx,
where a  (cid:4)p. In particular, we focus on comparing
a nonparametric estimate of the mean of y (cid:15) h to the

For some xed h, plot y versus h. Add a nonpara-
metric mean estimate, say a cubic smoothing spline
with xed degrees of freedom, to the plot; denote

corresponding mean computed from (cid:20)M, for various
this (cid:21)EF(cid:1)y (cid:15) h(cid:2), where EF denotes expectation under
a mean estimate under (cid:20)M, (cid:21)E(cid:21)M
(cid:1)y (cid:15) h(cid:2), where E(cid:21)M
denotes expectation under (cid:20)M. Since E(cid:21)M
(cid:1)y (cid:15) x(cid:2) (cid:15) h(cid:10), we can obtain (cid:21)E(cid:21)M
E(cid:8)E(cid:21)M
the tted values under M, E(cid:21)M

F. We wish to compare this mean estimate with
(cid:1)y (cid:15) h(cid:2) =
(cid:1)y (cid:15) h(cid:2) from a
nonparametric mean estimate for the regression of
(cid:1)y (cid:15) x(cid:2), on h. We can
then add this to the plot to obtain a marginal model
plot (MMP) for h; this can be thought of as a plot
for checking the model in the (marginal) direction
h. Using the same method (and smoothing param-
eter) to obtain this estimate as that used to obtain
the mean estimate under F allows point-wise com-
parison of the two estimates, since any estimation
bias should cancel. See Bowman and Young (1996)
for further discussion of this point. If the model is

functions h.

Fig. 2. Gibbs marginal model plot for the tted values for the
additive model t to four air pollution variables(cid:3)

Fig. 3. Gibbs marginal model plot for the tted values for the
additive model t to (cid:1)w1(cid:4) w2(cid:4) w12(cid:2)(cid:3)

a close representation of F, we can expect that for
any quantity h the marginal mean estimates should

agree, (cid:21)E(cid:21)M

(cid:1)y (cid:15) h(cid:2)  (cid:21)EF (cid:1)y (cid:15) h(cid:2).

Ideas for selecting which MMPs (i.e., which func-
tions h) to consider in practice are given in Cook
and Weisberg (1997), with additional discussion pro-
vided in Cook (1998) and Cook and Weisberg (1999).
Some examples of useful MMPs include those for
tted values, individual predictors and linear com-
binations of the predictors. Any indication that the
estimated marginal means do not agree for one par-
ticular MMP suggests that the model could perhaps
be improved; if they agree for a variety of plots, we
have support for the model. The ideas above can be
extended to variance estimates to provide further
ways for checking models.

Consider, for example, a MMP for the tted val-
ues for Hastie and Tibshiranis ozone data example
with four predictor variables. The plot in Figure 1
shows a systematic discrepancy between the (black)
mean estimate under F and the (gray) mean esti-

mate under (cid:20)M; the mean estimate under (cid:20)M is too

low on the left, too high in the middle and too low
again on the right. Both mean estimates were calcu-
lated using the S-plus function smooth.spline with
(the default) four degrees of freedom.

the data, the mean estimate under (cid:20)M does not

On the other hand, relative to the variation in

appear to be too far from the mean estimate under
F. So, are the discrepancies enough to indicate any
potential for model improvement? Porzio and Weis-
berg (1999) provide some frequentist methodology
to address this issue: pointwise reference bands to
aid visualization and statistics to calibrate discrep-
ancies. Hastie and Tibshiranis procedure also pro-
vides methodology to address this issue. They make
the well-taken points that we can make use of the
individual realizations of the posterior distributions
of the functions in an additive model, and display
the posterior distributions of interesting function-
als of them. They also note that we can carry out
Bayesian inference for any quantity of interest. This
would appear to offer a Bayesian way to aid visu-
alization in a MMP, with potential possibilities for
calibrating discrepancies.

For any particular MMP, it would be useful to dis-
play mean estimates for the individual realizations
from the posterior distribution of the tted values,
j=0 f t
j,
where the tted-value realizations are just
t = 1(cid:4) 2(cid:4) 3(cid:4) (cid:3) (cid:3) (cid:3) and f t
j. So, instead

of adding the mean estimate under (cid:20)M to the plot

j + S1/2
j zt

j = Sjrt



of the mean estimate under F, we can instead add
a mean estimate for each Gibbs sample, Gt, and
obtain what we call a Gibbs marginal model plot
(GMMP). If enough samples are taken, say 50 or

imate mean estimate band under (cid:20)M. This plot may

100, the Gibbs mean estimates will form an approx-

215

provide a visual way of determining whether there
is any evidence to contradict the possibility that
F(cid:1)y (cid:15) h(cid:2) = M(cid:1)y (cid:15) h(cid:2). Intuitively, if, for a particular
h, the mean estimate under F lies substantially out-

side the mean estimate band under (cid:20)M (formed from
inside the mean estimate band under (cid:20)M, then per-

the mean estimates under Gt), then perhaps the
model can be improved. If, no matter what the func-
tion h is, the mean estimate under F lies broadly

haps the model provides a reasonable description of
the conditional distribution of y (cid:15) x. It would appear
to be possible to supplement this purely graphical
methodology with more formal Bayesian inference.
Consider a GMMP for the tted values for the
ozone data. Hastie and Tibshirani kindly provided
us with the S plus functions for implementing the
ideas in their paper, as well as with help in using
their code. This enabled us to construct the GMMP
in Figure 2. The Gibbs sampling was carried out
using the fully Bayesian procedure described in
Hastie and Tibshiranis Section 4, with a warm-up
period of 300 iterations. The plot shows the (black)
mean estimate under F lying mostly outside the

mean estimate band under (cid:20)M [formed from 100

(gray) mean estimates under Gt]. This appears to
offer clear evidence that the tted model can be
improved.

As curious applied statisticians, we couldnt resist
trying to see if we could come up with a bet-
ter model for these data. One particular technique
we applied was sliced average variance estimation
(SAVE), introduced by Cook and Weisberg (1991)
and developed by Cook and Lee (1999). SAVE is a
model-free method for estimating the smallest sub-
space (cid:5) of (cid:4)p so that y and x are independent
given the projection of x onto (cid:5) , P(cid:5) x. In words,
all the information about y that is available from
x is contained in P(cid:5) x. Following Li (1991), (cid:5) is a
dimension reduction subspace for the regression of y
on x. The smallest such (cid:5) is called the central sub-
space, (cid:5)y(cid:15)x (Cook, 1994; Cook, 1998); SAVE yields a
subspace estimate, (cid:5)SAVE  (cid:5)y(cid:15)x. This estimate can
then be used to postulate a model, as described by
example below.

Since the additive model t above appears unable
to account for the curvature in the MMP for the
tted values, we felt that SAVE might be able to
provide us with a better model. We used the SAVE
methodology to infer the dimension of (cid:5)y(cid:15)x to be two,
and obtained two linear combinations of predictors,
w1 and w2, as an estimate of a basis for (cid:5)y(cid:15)x. A
three-dimensional plot of y versus w1 and w2 indi-
cated that an interaction term, w12, might also be

216

important. So, we decided to t an additive model:
E(cid:1)y (cid:15) x(cid:2) =  + f1(cid:1)w1(cid:2) + f2(cid:1)w2(cid:2) + f12(cid:1)w12(cid:2). Smooth-
ing splines with (the S plus default) four degrees of
freedom were used to estimate the f functions. A
GMMP for the tted values for this model is shown
in Figure 3. The plot shows the (black) mean esti-
mate under F lying inside the mean estimate band

under (cid:20)M (formed from the (gray) mean estimates

under Gt). There is little evidence in this plot to
suggest that the tted model can be improved.

However, there is evidence from a MMP for one of
the original predictors, inversion base height, that
this model too could be improved. Again, the dis-
crepancy between the marginal mean estimates in
this plot (not shown) is difcult to assess relative
to the variability in the data. The corresponding
GMMP in Figure 4 allows this discrepancy to be
evaluated visually, and the plot reinforces the sup-
position that the model could possibly be improved
(at least for low values of inversion base height).

Having applied Hastie and Tibshiranis method-
ology to these data, GMMPs appear to offer a quick
and easy way to graphically check models. The
Gibbs sampling only needs to be done once for each
model; with Hastie and Tibshiranis S plus code
this is straightforward. The analyst can then cycle
through a variety of GMMPs to get some guidance
on whether (and how) an alternative model might
provide an improvement. For example, in the above
analysis, a next step might be to develop a model
that deals with low values of inversion base height
more satisfactorily, say by increasing the degrees of
freedom for the smoothers in the additive model, or
by trying different smoothers such as loess.

Does a GMMP suffer the same shortcoming as
Hastie and Tibshiranis Figure 2; namely, would we
be able to obtain equivalent information by plotting
pointwise posterior intervals instead of individual
posterior realizations? The answer to this question
would surely be yes, were it not for the fact that
it is not clear how such intervals might be dened
in practice. For example, posterior intervals could
be calculated for the tted values in an additive
model by summing the posterior intervals for the
individual functions in the model. It would then be
straightforward to plot the pointwise intervals on a
MMP for the tted values. But, for MMPs for any
other function h, it is unclear what pointwise pos-
terior intervals should be dened to be. One possi-
bility would be to smooth the pointwise upper and
lower limits for the tted values using the same
method as used to obtain the mean estimates under

F and (cid:20)M, but it is not clear that this will give us
pointwise posterior intervals for E(cid:21)M

(cid:1)y (cid:15) h(cid:2).

3. OTHER POTENTIAL APPLICATIONS

Returning to Hastie and Tibshiranis Figure 1,
can these plots (of partial residuals versus individ-
ual predictors) be used for model checking? The
answer to this question would appear to be no. The
black curves are smooths of the partial residuals,
fj = Sjrj, while the gray curves are the Gibbs pos-
terior realizations, f t
j. These plots
would appear to offer visualization only of the vari-
ability in the tted functions. Appropriate plots for
model checking in this context are GMMPs for the
individual predictors, as shown for example in Fig-
ure 4.

j + S1/2
j zt

j = Sjrt

There are other plots used in model checking and
regression diagnostics that can be difcult to assess
relative to the variation in the data. Some exam-
ples include: residual plots; CERES plots, which are
a generalization of partial residual plots and were
introduced by Cook (1993); net effect plots, which
aid in assessing the contribution of a selected pre-
dictor to a regression and were introduced by Cook
(1995). The ideas discussed above would appear to
have a role to play in the analysis of such plots.
Work is in progress on these issues, as well as
on developing supplementary Bayesian inference
methodology.

4. MISCELLANEA

Hastie and Tibshiranis procedure appears to
live up to its claim of modularity and generality.
Although the procedure derives from the backtting
algorithm for tting additive models, it could proba-
bly be applied fairly easily to other families of mod-
els such as generalized linear models. Whether the
procedure could also be described as conceptually
simple is perhaps more open to debate. For example,
choosing priors for the variance components is far
from trivial, and MCMC convergence should always
be checked in practice. That said, there is clearly
a wealth of potential applications for the posterior
samples generated with this technique.

SAVE techniques can be applied using Arc (Cook
and Weisberg, 1999), a comprehensive regression
program. Information about the program is avail-
able at the Internet site www.stat.umn.edu/arc.

ACKNOWLEDGMENT

Research Supported in part by National Science

Foundation.

Comment
Alan E. Gelfand

This generous manuscript offers much food for
thought. I admire its generality, the ability to han-
dle both Gaussian and non-Gaussian likelihoods,
to accommodate both nonparametric and semipara-
metric forms, to handle a broad range of smoothers.
I applaud its pragmatic stance. Much energy is
invested on details of the model tting, worrying
about efciency of algorithms and introducing use-
ful approximations. Finally, I appreciate its effort to
be comparative, frequently linking Bayesian, empir-
ical Bayes and classical perspectives, attempting a
bridging of ideologies within a rich regression set-
ting.

But then, what is there that is worth comment-
ing upon? I will focus on two main issues. First,
I believe the authors are a bit too casual in their
Bayesian formulation. There is confusion through-
out with regard to singular versus improper priors,
with regard to proper versus improper posteriors. If
the contribution is viewed as primarily algorithmic,
so be it. But if the claim is that legitimate Bayesian
inference is being implemented, that credible poste-
rior analysis is being provided, then I have reserva-
tions.

Second, at least in the Gaussian case, within the
authors general objectives, there seems to be no
need to introduce iterative simulation. A direct sim-
ulation formulation is straightforward, avoiding all
concerns with MCMC model tting.

To my rst point, at the outset (Section 2), we
begin with a prior on (cid:2) which induces a prior on
f = B(cid:2). If the dimension of (cid:2) is less than that of f,
a proper Gaussian prior on (cid:2) induces a singular but
proper distribution on f. An improper distribution
on (cid:2) necessarily yields an improper distribution on
f. For smoothers, it is apparently more typical that
the dimension of (cid:2) is greater than that of f and
that an improper (not degenerate) prior is implicit
for (cid:2), hence an induced improper prior for f. Thus,
these priors are not normal. Expression (5) should
be written as

f (cid:15) 2  1(cid:17)

(cid:18)a exp

2

(cid:17)f TKf /22(cid:18)

(cid:4)

(1)

Alan E. Gelfand is Professor, Department of Statis-
tics, University of Connecticut, Storrs, Connecticut
06269-3120 (e-mail: alan@stat.uconn.edu).

217

where the power a is arbitrary since the distribu-
tion is improper. Similarly,

(cid:2) (cid:15) 2  1(cid:17)

2

f (cid:15) 2  1(cid:17)

(cid:18)a exp

2

(cid:3)

(cid:17)(cid:2)T(cid:2)/22(cid:18)

(cid:18)a exp
(cid:17)f TBTBf /22(cid:18)

(cid:4)

(2)

(3)

If we start with (2), adopting a specic general-
ized inverse B and operate formally [since (2) is
improper] we obtain

thus determining K.

Introduction of K, (cid:4) and S(cid:1)(cid:2) [e.g., expres-
sion (7)] serves to cloud matters. Ultimately, in (6),
if f (cid:15) y is proper, I + K is full rank and S(cid:1)(cid:2) =
(cid:1)I + K(cid:2)1 (which doesnt appear until halfway
through Section 4.2) is all we need. If f (cid:15) y is not
proper, how can we speak of its posterior? In this
case, when the dim of (cid:2) exceeds the dimension of f,
an improper posterior distribution for (cid:2) may induce
a unique proper posterior for f. See, for example,
Gelfand and Sahu (1999) in this regard.

When we move to Section 3, the situation becomes
even more disturbing. Now, p functions are intro-
duced additively in the mean structure, along with
an intercept. A centering constraint is introduced
on each function for identiability. In fact, from
a Bayesian point of view, with a proper posterior,
there is no identiability issue (as in, e.g., Lind-
ley, 1971). With improper priors, such constraints
are customarily introduced to achieve proper pos-
teriors as well as to provide well-behaved posteri-
ors, yielding well-behaved MCMC algorithms. The
recommended centering on-the-y, that is, after
each iteration, has become standard in these situa-
tions. See, for example, Besag, Green, Higdon and
Mengersen (1995).

In any event, while the Bayesian backtting
method is presented as an algorithm, it does not
appear to correspond to a Gibbs sampler for a well-
dened Bayes model. The existence of proper full
conditionals for all model unknowns says nothing
about the propriety of the joint posterior (see, e.g.,
Casella and George, 1992). I cannot see how a
proper posterior can be associated with the model
in (14), using the various improper priors proposed
for the fj(cid:3) Furthermore, what is the prior on ? Why
isnt it updated in the sampler? What is the distri-
butional justication for inserting y into (16)? In


this spirit, the updating using the approximation in
(29) can be implemented, perhaps as a Metropolis
step, but it is not a draw from the full conditional
distribution for i.

As to my second point, the authors concede that
xing  2 in (1), or, more generally in (14),
is
clearly inappropriate, as is xing the various 2
j(cid:3)
The improper prior for  2 in (22) along with the
improper choices discussed for the 2s will surely
produce an improper posterior, following Hobert and
Casella (1996). Such priors, along with the improper
priors for fj, appear to run counter to the authors
claim in Section 7 that, We have restricted our-
selves to the use of proper priors, citing Hobert
and Casella in this regard! The essentially improper
prior in (21) in place of (22) cannot be expected to
yield a convergent MCMC algorithm if the posterior
is improper using (22).

Moreover, even if priors are introduced for  2
and the 2
j such that a proper posterior is ensured,
the Gibbs sampler in this setting will be very slow.
Apart from the usual convergence concerns, at each
iteration each Sj must be updated since j changes
with each iteration.

An alternative specication enables direct simula-
tion of the entire posterior and permits Sj to be con-
stant for each simulation. Convergence and updat-
ing problems vanish. Suppose we retain the same

Comment
Peter J. Green

(cid:6)

(cid:5)

(cid:24)

f T
j

/2 2

(cid:22)
rst-stage specication as in (14) but take the prior
on (cid:23)fj(cid:24) to be of the form
(cid:23)fj(cid:24) (cid:15)  2  exp

(4)
adding a proper N(cid:1)0(cid:4)  2/0(cid:2) prior for . Here

(cid:23)Kj = jKj with j specied but not  2. With
the j xed, (cid:23)Kj need only be computed once at the

(cid:23)Kjfj

outset. We may view j as a relative precision.
Again, if (4) is improper, there is no unique choice
for the power of  2 in the proportionality constant,
as in (1)(3).

(cid:4)

However, once this power is provided, if  2 fol-
(cid:17)
lows an inverse gamma prior, we may factor the
joint posterior density for , (cid:23)fj(cid:24) and  2 as
(5)

(cid:17)
(cid:4)(cid:23)fj(cid:24)(cid:4)  2 (cid:15) y

(cid:4)(cid:23)fj(cid:24) (cid:15)  2(cid:4) y

(cid:18) = p

(cid:18)  p

 2 (cid:15) y

(cid:17)

(cid:18)

p

(cid:3)

If (5) is proper, both densities on the right-hand
side are. In fact, the rst is an updated normal, the
second an updated inverse gamma. Sampling  2
from p(cid:1) 2 (cid:15) y(cid:2) and then, (cid:4)(cid:23)f
j(cid:24) from p(cid:1)(cid:4)(cid:23)fj(cid:24) (cid:15)
 2(cid:4) y(cid:2) directly provides a posterior realization. This
approach appears to t well with the authors goals
of pragmatism and efciency.

In summary, while the ideas in this paper are
attractive, the Bayesian modeling and resultant
simulation-based inference, which are at its heart,
are somewhat uncomfortable.

I warmly congratulate the authors on this paper. I
am sure they will succeed in broadening acceptance
of the Bayesian paradigm in inference in regres-
sion by providing this well-written and accessible
treatment of the use of Markov chain Monte Carlo
(MCMC) in tting the important class of (general-
ized) additive models.

The paper promotes several

ideas. It can be
interesting and revealing to examine Bayesian
analogues of familiar frequentist models and pro-
cedures; MCMC is important in Bayesian infer-
ence; Gibbs sampling is a convenient general recipe
for MCMC; if that isnt available, try Metropolis
Hastings; Gibbs sampling is a close analogue of

Peter J. Green is Professor, Department of Mathemat-
ics, University of Bristol, Bristol BS8 1TW, United
Kingdom (e-mail: P.J.Green@bristol.ac.uk).

backtting. None of these points are individually
very original, of course! But it is very appealing
to see their combination applied to additive mod-
els, with a number of practical details worked out
to produce an efcient methodology, especially since
Splus software to implement the resulting method-
ology is provided.

My comments focus on some of these practical
details, and some other relations and connections
to the proposed methods.

BAYESIAN FUNCTION ESTIMATION

The reader who comes to this work from a back-
ground of backtting in (generalized) additive mod-
els rather than experience of inference about func-
tions and surfaces in the Bayesian paradigm might
get an impression that this paper is close to the
state-of-the-art in Bayesian function estimation. In
fact, the authors do not claim this; researchers have

been investigating and using Bayesian models and
MCMC calculations for more complicated situations
than this, almost from the earliest days of MCMC
in statistics. One could even claim that a driving
force in the broader acceptance of practical Bayesian
methodology for inference in complex data struc-
tures has been that using MCMC methods there
was a comparatively trivial computational penalty
to be paid in moving on from simple models to more
complicated ones (in the case of inference about
functions, for example, replacing Gaussian priors on
functions by non-Gaussian ones).

MCMC in statistics is generally accepted to have
begun in statistical image analysis, and of course
nonlinear smoothers, which do not destroy bound-
aries between objects in the scene, are routinely
needed there. In discrete spatial settings, such as
arise in pixellized images and region-based geo-
graphical and ecological problems, particular use
has been made of pairwise-difference priors that are
not Gaussian, but have heavier tails; for example

(cid:9)

(cid:1)

ij



(cid:1)fi  fj(cid:2)

p(cid:1)f(cid:2)  exp

(cid:10)

(cid:4)

where fi
is the true image intensity in pixel
i, and i  j means the summation is over
pairs of neighboring pixels
(see Geman and
McClure, 1985; Green, 1990; Besag, Green, Hig-
don and Mengersen, 1995, among many others).
Instead of using (cid:1)u(cid:2) = u2, taking it to be (cid:15)u(cid:15), or
something more complicated such as log cosh u or
1/(cid:1)1 + u2(cid:2) has proved successful in many image
restoration contexts. (The apparent connection here
with M-estimation and robustness is not entirely
supercial.)

These methods are all for discrete spatial prob-
lems, whether on lattices or irregular graphs. Non-
parametric Bayesian surface tting methods for
continuous space include that of Heikkinen and
Arjas (1998) for inference on a Poisson intensity,
using ideas of model averaging over appropriately
dened step functions to yield smooth posterior
mean surfaces, and the variogram-based methods of
Diggle, Tawn and Moyeed (1998). Turning to regres-
sion on more general, nonspatial, covariates, apart
from the work of Denison, Mallick and Smith (1998)
and Holmes and Mallick (1997) that is mentioned in
this paper, and other methods investigated by these
researchers and colleagues, there is the interesting
approach of M uller, Erkanli and West (1996), based
on Dirichlet process priors.

219

BAYES FROM CONVICTION

OR CONVENIENCE?

The conceptual connection between smoothing
methods, especially those based on penalized likeli-
hood, and Bayesian formulations of inference about
functions is often made, but there always seems
to be an implicit or explicit warning: Dont take
this too literally. Formal use of such connections
is rarely made, a notable exception being Wahbas
important 1983 paper, exploiting the Bayesian con-
nection to construct condence intervals about
spline estimates. However, the frequentist proper-
ties of such intervals are well known to be problem-
atical.

This is all well understood by the authors, but
I do think that if they seek to use the Bayesian
paradigmand presenting credible intervals for
functions is certainly doing thatthey should try
to take it a bit more seriously! There are several
issues here.

(cid:3)(cid:8)f(cid:9)(cid:9)(cid:1)x(cid:2)(cid:10)2 dx have

The rst concerns the use of the roughness
penalty as a negative-log-prior on the regression
function. Does the usual penalty 
particular merit in this context, over other (say)
quadratic forms in f that are zero for constant or
rst-order f, or is the choice driven, as in straight-
forward smoothing, by the computational advan-
tages? Specically, do autocorrelations decline with
lag in a reasonable way, and what about homogene-
ity of variance? (Answers to these questions are
complicated by the partially improper nature of the
prior in this case.) Are Gaussian prior assumptions
reasonable, or should we consider heavy-tailed mod-
ications? Choice of functional form of the penalty
has far greater consequences for the Bayesian pro-
cedure than for the simple smoother, as we are going
to use it to generate much more subtle inferences,
invested, presumably, with real probabilistic inter-
pretations. To be fair, the authors are in good com-
pany in not raising these questions; they are almost
never considered by anybody else either!

The second issue is the treatment of smoothing
(tuning) parameters (or, equivalently, variance com-
ponents). This is explicitly discussed, in Section 4 of
the paper. I must say that I nd the full Bayesian
version much more compelling than either of the
alternatives.

A third concern is about sensitivity to prior
assumptions, always problematic in hierarchical
models. It would be good to see at least an empiri-
cal study of the effect on posterior inference of vari-
ations in the authors assumptions. I would antici-
pate that everything about the inference except the
posterior means is actually rather sensitive.

220

THE ROOT-S APPROXIMATION

SOME MCMC DETAILS

I cannot be the only reader to worry about
the quality of the approximation to S1/2(cid:6) pro-
posed in Algorithm A.1. The approach looks sim-
plistic: why should the size of the rst neglected
term of the series be a reliable guide to the
size of
the sum of all neglected terms? and
indeed a small numerical experiment bears out this
concern.

Although an alternative, superior, approach using
Cholesky decomposition is available for the cubic
spline smoother, this case provides a convenient
choice for a numerical check. Taking xi = i, i =
1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n = 100 and drawing a single vector z 
N(cid:1)0(cid:4) I(cid:2), I compared !S1/2z  S1/2
HTz! with the toler-
ance on !Sz(cid:9)(cid:9)! used in the authors algorithm, where
S1/2
HT represents their approximation. Table 3 shows
that, especially if higher precision is sought, the
method becomes both expensive and much less suc-
cessful.

Are better approximations available for an
amount of work comparable, say, to Algorithm A.1
with a 102 tolerance? If not, I wonder if there is
merit in turning the approach around, and tak-
ing S1/2 to be some convenient (symmetric) linear
smoother, and dening S to be its square, that is,
the smoother obtained by applying S1/2 twice? This
obviously changes the prior covariance structure so
we would have to revisit that question. However,
there is a clear potential for saving computational
effort.
To a rough degree of approximation, there can be
surprisingly little change; for example, with xi = i,
i = 1(cid:4) 2(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n = 100 again, S2
10 is close to S8(cid:3)07
where Sdf is the cubic spline smoother with df
degrees of freedom; the eigenvalues differ by a max-
imum of about 0.05, and their outputs are visually
nearly indistinguishable.

Table 3

Relationship between tolerance on rst neglected term(cid:4) number
of terms included and overall precision for Algorithm A.1 (cid:1)!  ! is

the sup norm(cid:2)

df = 6

(cid:7) = 2493(cid:3)2

df = 12
(cid:7) = 105(cid:3)55

!Sz(cid:9)(cid:9)!
101
102
103
104
105
106

nal b
2
5
25
95
620
2875

!S1/2z
S1/2
HTz!
0.100
0.073
0.032
0.024
0.015
0.010

!Sz(cid:9)(cid:9)!
101
102
103
104
105
106

nal b
2
6
25
175
847
2702

!S1/2z
S
HTz!
1/2
0.139
0.105
0.069
0.042
0.042
0.043

There are several ideas for improving MCMC per-
formance in the literature that could be benecial in
the present context. For example, the modications
to backtting introduced at the end of Appendix A
appear to be related to the ideas of hierarchical cen-
tering in normal linear mixed models, of Gelfand,
Sahu and Carlin (1995).

Organizing variables in a Gibbs sampler into
blocks to be simultaneously updated is a common
strategy, and often pays off if the blocked variables
are highly correlated and the multivariable update
is not expensive to implement. The authors back-
tting strategy is precisely an example of this idea.
The variables (cid:1)fj(cid:1)xij(cid:2)(cid:2) in the jth block will indeed
be strongly correlated. Many questions of MCMC
strategy about blocking, updating schedules and
reparameterization, precisely for the present case
of multivariate Gaussian models, are discussed by
Roberts and Sahu (1997), which is strongly recom-
mended reading.

A posteriori, there are of course also strong corre-
lations between some variables in different blocks,
especially if the predictors are highly correlated
themselves, and the anticipated impact of this on
MCMC convergence provides another explanation
for poor performance in this case.

In other contexts, the correlation between vari-
ance parameters and their associated sums-of-
squares has proved damaging for MCMC perfor-
mance; a commonly successful work-around has
been to integrate out
the variance parameter
(assuming we are in the usual conjugate setting
with normal random effects and inverse gamma
hyperpriors) and then update the random effects by
MetropolisHastings targetted at a t density. This
approach may be useful if the sampler based on
equation (24), for example, should mix slowly in a
particular case.

Regarding the approach to generalized additive
models in Section 6, I have also been a great enthu-
siast (in other contexts) for MetropolisHastings
based on a Gaussian approximation to the full con-
ditional. People tell me this is not a free lunch, how-
ever. Whether the resulting chain is even geomet-
rically ergodic is, I think, not fully understood. In
similar problems, this depends on the relative size
of the tails of the proposal and target densities (see,
e.g., Roberts and Tweedie, 1996), and so geometric
ergodicity may be problematical in skew cases such
as GAMs. Can the authors reassure me? Is the sam-
pler provably good?

Finally, brief mention should be made of the
required length of MCMC runs. The authors are

rather silent on this; a passing reference in the Dis-
cussion seems to suggest that only 100 sweeps were
made, after burn-in, in one example. This seems
very few for such a large-dimensional problem, espe-

cially if full posterior distributions are being esti-
mated, not just their means. In all their emphasis
on order-n computation, the authors do not allow for
any increase in MCMC convergence times with n.

221

Rejoinder
Trevor Hastie and Robert Tibshirani

We knew we were taking a chance writing a
paper with Bayesian in the title, since neither of
us work in this area, and we have not paid our
dues by attending a Valencia meeting. Visions of
the BayesianFrequentist battles that have raged
over the years in the Royal Statistical Society jour-
nals left many a sleepless night. We breathed a sigh
of relief when Professor Greens discussion arrived;
we appear to have got off lightly with a few small
raps on the knuckles from a well-respected applied
Bayesian. The discussion of Professor Cook and Mr
Pardoe did no damage either. Just as we began
to relax, the computer screen started to quiver,
and after a quick degauss, there it was! The dis-
cussion of Professor Gelfand had arrived in all
its fury.

We thank all the discussants for their contribu-
tions. All three were complimentary in their open-
ing paragraphs about our pragmatic approach to
the problem, and we thus feel that our main mis-
sion in writing this paper was accomplished. We will
address their comments and concerns separately.

Professor Green gives a very useful history of
MCMC and its use in Bayesian function estimation
and image smoothing.

He is quite right; we have not spent much time
investigating other priors, and realize, especially in
spatial statistics and signal processing, that there
are many other considerations besides smoothness.
In (1) below we express the prior in a slightly differ-
ent format, which allows perhaps for relatively easy
tailoring for function approximation.

The prior assumptions on the individual effects
in model (27) clearly have an important impact on
the model. Without priors to pin them down, i
and Vi are strongly aliased. Consider an individual
with bone measurements above the curve near the
growth spurt (e.g., girl 124 in Figure 10). She could
either have a positive value for i (and Vi = 0) or
a negative value for Vi (and i = 0) or values in
between for both. Priors can save the day, since we
have some idea from many different sources of the
distribution of the onset of puberty in girls.

It seems our root-S approximation is not too
precise; truth be told, our Splus implementation,
gibbs.gam(), handles smoothing splines only, where
this approximation is not needed.

We are reassured by Professor Greens endorse-
ment of our blocking and efciency strategy out-
lined in the Appendix, and grateful for his providing
more details and references. Unfortunately, we can-
not vouch at this time for the geometric ergodicity
of our MetropolisHastings sampler for GAMs, but
expect such results to be forthcoming.

Professor Cook and Mr. Pardoe provide some
interesting graphical techniques for model assess-
ment. Almost surely an additive model is an approx-
imation to the truth, so we are not surprised by
the small discrepancy in their Figure 1 between
the MMP and the additive t. One has to decide
whether the gains obtained by tting a more com-
plex model
is worth the
sacrice in simplicity. One small concern: since
typically smoothers are not projection operators
(!SSy!  !Sy!), we wonder whether the bands
in the GMMP are articially narrow due to double
smoothing?

(based on projections)

Professor Gelfand reproaches us for our vague
description of the prior distribution in Section 2,
motivated by smoothing splines. Our goal in the sec-
tion was to avoid details and inspire generalities;
in fact, we purposely postponed details for smooth-
ing splines till the Appendix. Professor Gelfand then
attempts a more precise statement of the prior dis-
tribution (apparently to correct our treatment for
smoothing splines in the Appendix), but does not
get it quite right.

For simplicity we assume the n values xi are dis-
tinct. When a smoothing spline is represented in an
M-dimensional B-spline basis, with M = n + 2, the
coefcients (cid:2) have an M-dimensional prior distribu-
tion which is both:

 Improper

subspace,
corresponding to constant and linear functions
of x;

in a two-dimensional

222

 Degenerate or singular in a two-dimensional
subspace, corresponding to the two natural bound-
ary conditions.

So our statement in the the second bulleted item
below (43) in the Appendix does have errors, but not
the errors claimed by Professor Gelfand. (cid:2) has both
a degenerate and improper prior distribution.

It appears that Professor Gelfands (1) is simply
a more precise way of stating our (5). That does not
make our (5) incorrect; we admit it is sloppy, but
it appears to be the style used by many Bayesian
authors.

There is a better way of expressing the prior dis-
tribution for smoothing splines or similar methods.
Let K = UDUT be an eigen-decomposition of the
nn penalty matrix K [see (5) and below in our arti-
cle; also Green and Silverman (1994) for a detailed
description of K and an algorithm for computing
it]. The null-space of K is two-dimensional, and
spans the column space of (cid:1)1(cid:4) x(cid:2) (linear functions
of x). Suppose we partition U = (cid:1)U1 (cid:27) U2(cid:2) such that
U1 spans this null space, and the diagonal matrix
D = diag(cid:1)D1(cid:4) D2(cid:2) with the 2  2 matrix D1 = 0. U2
is n  (cid:1)n  2(cid:2) and represents nonlinear functions in
x. U2 can be represented as a linear transformation
of the n  M B-spline matrix B, which imposes (a)
the natural boundary conditions, (b) orthogonality
to U1 and (c) orthogonal columns. The same trans-
formation applied to the M B-spline basis functions
bj(cid:1)x(cid:2) yields the n  2 DemmlerReinch basis func-
tions hE(cid:1)x(cid:2), E = 1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n [and U2 is the matrix of
sample realizations of these hE(cid:1)x(cid:2)].
This leads to a representation for the smoothing
spline model:

f(cid:1)x(cid:2) = 0 + 1x + n2(cid:1)

hE(cid:1)x(cid:2)E(cid:3)

E=1

(1)
The parameters are divided into (cid:3) = (cid:1)0(cid:4) 1(cid:2) and
(cid:4) = (cid:1)1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) n2(cid:2). The prior on (cid:3) is noninfor-
mative, and (cid:4)  N(cid:1)0(cid:4) 2D2(cid:2) is proper. This is
commonly referred to as a mixed effects model,
with (cid:3) regarded as a xed effect. This repre-
sentation makes explicit the proper and improper
parts of the prior for smoothing splines and simi-
lar models and avoids the degeneracy due to over-
parametrization. The roughness (as computed by
the second-derivative penalty) of the hE(cid:1)x(cid:2) increases
with E, and the prior variances on the diagonal of
2D2 decrease toward zero accordingly.

We are not quite sure what is bothering Profes-
sor Gelfand in our Section 3. An additive model
f = 1 + f1 +  + fp in which each fj includes
the constant term has a (p-fold) degeneracy which
j 1 = 0. This
can be removed by assuming each f T

is all we are doing. The priors are easily modied
to accommodate this centering [see (2) below]. Pro-
fessor Gelfand is concerned about the existence of
a proper posterior for the additive model (14). This
is most easily demonstrated by extending (1) to the
additive case (Lin and Zhang, 1999):

1 x + p(cid:1)

n2(cid:1)

j=1

E=1

(2)

f(cid:1)(cid:2) = 0 + (cid:3)T

hEj(cid:1)xj(cid:2)Ej(cid:4)

(cid:17)

where the hEj represent the j different series of
DemmlerReinch basis functions dened separately
for each predictor. There is an improper prior on the
p + 1 (xed effects) (cid:3), and proper (and indepen-
dent) normal priors on all the Ej,
0(cid:4) diag(cid:1)2
(cid:4) = (cid:1)(cid:4)1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) (cid:4)p(cid:2)  N
This has the same structure as the one-dimensional
smoothing spline and has a proper Gaussian poste-
rior for the same reasons. The composed functions
are simple linear combinations of the parameters
and have proper posteriors as well.

pDp(cid:2)(cid:18)

1D1(cid:4) (cid:3) (cid:3) (cid:3) (cid:4) 2

(cid:3)

We do sample the constant in Algorithm 3.1; the
index j runs from 0, and step 0 samples the con-
stant from a N(cid:1)0(cid:4)  2/n(cid:2). Possibly Professor Gelfand
was misled by (16), which is simply an iterative
algorithm (backtting) for computing the posterior
mean.

Although the additive model can be sampled with-
out using Gibbs sampling, there is an overhead
of O(cid:1)2(cid:1)p + 2(cid:2)n3(cid:2) computations up front to com-
pute the relevant posterior covariances and diag-
onalize them. Each realization from the Gibbs sam-
pler takes O(cid:1)n(cid:2) computations and can represent a
dramatic savings for large problems.

ACKNOWLEDGMENTS

The authors thank Jun Liu for reassuring dis-
cussions while preparing this rejoinder. They also
thank the editors for arranging this forum.


