Abstract Intelligent tasks, such as visual perception, auditory
perception, and language understanding require the construction
of good internal representations of the world (or features),
which must be invariant to irrelevant variations of the input
while, preserving relevant information. A major question for
Machine Learning is how to learn such good features auto-
matically. Convolutional Networks (ConvNets) are a biologically-
inspired trainable architecture that can learn invariant features.
Each stage in a ConvNets is composed of a lter bank, some
non-linearities, and feature pooling layers. With multiple stages,
a ConvNet can learn multi-level hierarchies of features. While
ConvNets have been successfully deployed in many commercial
applications from OCR to video surveillance, they require large
amounts of labeled training samples. We describe new unsu-
pervised learning algorithms, and new non-linear stages that
allow ConvNets to be trained with very few labeled samples.
Applications to visual object recognition and vision navigation
for off-road mobile robots are described.

I. LEARNING INTERNAL REPRESENTATIONS

independently of pose, scale,

One of the key questions of Vision Science (natural and
articial) is how to produce good internal representations of
the visual world. What sort of internal representation would
allow an articial vision system to detect and classify objects
into categories,
illumination,
conformation, and clutter? More interestingly, how could an
articial vision system learn appropriate internal representa-
tions automatically, the way animals and human seem to learn
by simply looking at the world? In the time-honored approach
to computer vision (and to pattern recognition in general),
the question is avoided: internal representations are produced
by a hand-crafted feature extractor, whose output is fed to a
trainable classier. While the issue of learning features has
been a topic of interest for many years, considerable progress
has been achieved in the last few years with the development
of so-called deep learning methods.

Good internal representations are hierarchical. In vision,
pixels are assembled into edglets, edglets into motifs, motifs
into parts, parts into objects, and objects into scenes. This
suggests that recognition architectures for vision (and for
other modalities such as audio and natural language) should
have multiple trainable stages stacked on top of each other,
one for each level in the feature hierarchy. This raises two
new questions: what to put in each stage? and how to train
such deep, multi-stage architectures? Convolutional Networks
(ConvNets) are an answer to the rst question. Until recently,
the answer to the second question was to use gradient-based
supervised learning, but recent research in deep learning has
produced a number of unsupervised methods which greatly
reduce the need for labeled samples.

Convolutional Networks

Convolutional Networks [1], [2] are trainable multistage
architectures composed of multiple stages. The input and
output of each stage are sets of arrays called feature maps. For
example, if the input is a color image, each feature map would
be a 2D array containing a color channel of the input image
(for an audio input each feature map would be a 1D array,
and for a video or volumetric image, it would be a 3D array).
At the output, each feature map represents a particular feature

Fig. 1. A typical ConvNet architecture with two feature stages

extracted at all locations on the input. Each stage is composed
of three layers: a lter bank layer, a non-linearity layer, and a
feature pooling layer. A typical ConvNet is composed of one,
two or three such 3-layer stages, followed by a classication
module. Each layer type is now described for the case of image
recognition.
Filter Bank Layer - F : the input is a 3D array with n1 2D
feature maps of size n2 n3. Each component is denoted xijk,
and each feature map is denoted xi. The output is also a 3D
array, y composed of m1 feature maps of size m2  m3. A
trainable lter (kernel) kij in the lter bank has size l1  l2
and connects input feature map xi to output feature map yj.
The module computes yj = bj + Pi kij  xi where  is
the 2D discrete convolution operator and bj is a trainable
bias parameter. Each lter detects a particular feature at every
location on the input. Hence spatially translating the input of
a feature detection layer will translate the output but leave it
otherwise unchanged.
Non-Linearity Layer: In traditional ConvNets this simply
consists in a pointwise tanh() sigmoid function applied to
each site (ijk). However, recent implementations have used
more sophisticated non-linearities. A useful one for natural im-
age recognition is the rectied sigmoid Rabs: abs(gi.tanh())
where gi is a trainable gain parameter. The rectied sigmoid is
sometimes followed by a subtractive and divisive local normal-
ization N , which enforces local competition between adjacent
features in a feature map, and between features at the same
spatial location. The subtractive normalization operation for a
given site xijk computes: vijk = xijk  Pipq wpq.xi,j+p,k+q,
where wpq is a normalized truncated Gaussian weighting
window (typically of size 9x9). The divisive normalization
computes yijk = vijk/max(mean(jk), jk) where jk =
i,j+p,k+q)1/2. The local contrast normalization
(Pipq wpq.v2
layer is inspired by visual neuroscience models [3], [4].
Feature Pooling Layer: This layer treats each feature map
separately. In its simplest instance, called PA, it computes
the average values over a neighborhood in each feature map.
The neighborhoods are stepped by a stride larger than 1
(but smaller than or equal the pooling neighborhood). This
results in a reduced-resolution output feature map which is
robust to small variations in the location of features in the
previous layer. The average operation is sometimes replaced
by a max PM . Traditional ConvNets use a pointwise tanh()
after the pooling layer, but more recent models do not. Some
ConvNets dispense with the separate pooling layer entirely, but
use strides larger than one in the lter bank layer to reduce

978-1-4244-5309-2/10/$26.00 2010 IEEE

253

Fig. 2. An example of feature extraction stage of the type F RabsN PA.
An input image (or a feature map) is passed through a lter bank, followed
by abs(gi. tanh()), local subtractive and divisive contrast normalization, and
spatial pooling/sub-sampling.

the resolution [5], [6]. In some recent versions of ConvNets,
the pooling also pools similar feature at the same location, in
addition to the same feature at nearby locations [7].

Supervised training is performed using a form of stochastic
gradient descent to minimize the discrepancy between the
desired output and the actual output of the network. All
the coefcient of all the lters in all the layers are updated
simultaneously by the learning procedure. The gradients are
computed with the back-propagation method. Details of the
procedure are given in [2], and methods for efcient training
are detailed in [8].

History and Applications

ConvNets can be seen as a representatives of a wide
class of models that we will call Multi-Stage Hubel-Wiesel
Architectures. The idea is rooted in Hubel and Wiesels classic
1962 work on the cats primary visual cortex. It identied
orientation-selective simple cells with local receptive elds,
whose role is similar to the ConvNets lter bank layers, and
complex cells, whose role is similar to the pooling layers.
The rst such model to be simulated on a computer was
Fukushimas Neocognitron [9], which used a layer-wise, un-
supervised competitive learning algorithm for the lter banks,
and a separately-trained supervised linear classier for the
output layer. The innovation in [5], [1] was to simplify the
architecture and to use the back-propagation algorithm to
train the entire system in a supervised fashion. The approach
was very successful for such tasks as OCR and handwrit-
ing recognition. An operational bank check reading system
built around ConvNets was developed at AT&T in the early
1990s [2]. It was rst deployed commercially in 1993, running
on a DSP board in check-reading ATM machines in Europe
and the US, and was deployed in large bank check reading
machines in 1996. By the late 90s it was reading over
10% of all the checks in the US. This motivated Microsoft
to deploy ConvNets in a number of OCR and handwriting
recognition systems [6], [10], [11] including for Arabic [12]
and Chinese characters [13]. Supervised ConvNets have also
been used for object detection in images,
including faces
with record accuracy and real-time performance [14], [15],
[16], [17], Google recently deployed a ConvNet to detect
faces and license plate in StreetView images so as to protect
privacy [18]. NEC has deployed ConvNet-based system in
Japan for tracking customers in supermarket and recognizing
their gender and age. Vidient Technologies has developed a
ConvNet-based video surveillance system deployed in several
airports in the US. France Telecom has deployed ConvNet-
based face detection systems for video-conference and other
systems [15]. Other experimental detection applications in-
clude hands/gesture [19], logos and text [20]. A big advantage
of ConvNets for detection is their computational efciency:
even though the system is trained on small windows, it sufces
to extend the convolutions to the size of the input image
and replicate the output layer to compute detections at every
location. Supervised ConvNets have also been used for vision-
based obstacle avoidance for off-road mobile robots [21]. Two

participants in the recent DARPA-sponsored LAGR program
on vision-based navigation for off-road robots used ConvNets
for long-range obstacle detection [22], [23]. In [22], the system
is pre-trained off-line using a combination of unsupervised
learning (as described in section II) and supervised learning.
It is then adapted on-line, as the robot runs, using labels
provided by a short-range stereovision system (see videos at
http://www.cs.nyu.edu/ yann/research/lagr). Inter-
esting new applications include image restoration [24] and
image segmentation, particularly for biological images [25].
The big advantage over MRFs is the ability to take a large
context window into account. Stunning results were obtained
at MIT for reconstructing neuronal circuits from an stack of
brain slice images a few nanometer thick. [26].

Over the years, other instances of the Multi-Stage Hubel-
Wiesel Architecture have appeared that are in the tradition
of the Neocognitron: unlike supervised ConvNets, they use
a combination of hand-crafting, and simple unsupervised
methods to design the lter banks. Notable examples include
Mozers visual models [27], and the so-called HMAX family
of models from T. Poggios lab at MIT [28], [29], which
uses hard-wired Gabor lters in the rst stage, and a simple
unsupervised random template selection algorithm for the
second stage. All stages use point-wise non-linearities and
max pooling. From the same institute, Pinto et al. [4] have
identied the most appropriate non-linearities and normaliza-
tions by running systematic experiments with a a single-stage
architecture using GPU-based parallel hardware.

II. UNSUPERVISED LEARNING OF CONVNETS

Training deep, multi-stage architectures using supervised
gradient back propagation requires many labeled samples.
However in many problems labeled data is scarce whereas un-
labeled data is abundant. Recent research in deep learning [30],
[31], [32] has shown that unsupervised learning can be used
to train each stage one after the other using only unlabeled
data, reducing the requirement for labeled samples signi-
cantly. In [33], using abs and normalization non-linearities,
unsupervised pre-training, and supervised global renement
has been shown to yield excellent performance on the Caltech-
101 dataset with only 30 training samples per category (more
on this below). In [34], good accuracy was obtained on the
same set using a very different unsupervised method based on
sparse Restricted Boltzmann Machines. Several works at NEC
have also shown that using auxiliary tasks [35], [36] helps
regularizing the system and produces excellent performance.

Unsupervised Training with Predictive Sparse Decomposition
The unsupervised method we propose, to learn the lter
coefcients in the lter bank layers, is called Predictive Sparse
Decomposition (PSD) [37]. Similar to the well-known sparse
coding algorithms [38], inputs are approximated as a sparse
linear combination of dictionary elements. In conventional
sparse coding for any given input X, one needs to run
an expensive optimization algorithm to nd Z  (the basis
pursuit problem). PSD trains a feed-forward regressor (or
encoder) C(X, K) to quickly approximate the sparse solution
Z . During training, the feature vector Z  is obtained by
minimizing:
E(Z, W, K) = kX  W Zk2

2 + kZk1 + kZ  C(X, K)k2

2

where W is the matrix whose columns are the dictionnary
elements and K are the lters. For each training sample X,
one rst nds Z  that minimizes E, then W and K are

254

TABLE I

AVERAGE RECOGNITION RATES ON CALTECH-101.

Rabs  N  PA Rabs  PA N  PM

U+
R+
U
R

65.5%
64.7%
63.7%
62.9%

60.5%
59.5%
46.7%
33.7%

61.0%
60.0%
56.0%
37.6%

PA

32.0%
29.7%
9.1%
8.8%

to lower E. Once
adjusted by stochastic gradient descent
training is complete, the feature vector for a given input is
simply obtained with Z  = C(X, K), hence the process is
extremely fast (feed-forward).
Results on Object Recognition

In this section, various architectures and training procedures
are compared to determine which non-linearities are prefer-
able, and which training protocol makes a difference.

Generic Object Recognition using Caltech 101 Dataset: We
use a two-stage system where, the rst stage is composed of
an F layer with 64 lters of size 9  9, followed by different
combinations of non-linearities and pooling. The second-stage
feature extractor is fed with the output of the rst stage and
extracts 256 output features maps, each of which combines
a random subset of 16 feature maps from the previous stage
using 9  9 kernels. Hence the total number of convolution
kernels is 256  16 = 4096.

Table I summarizes the results for the experiments, where
U and R denotes unsupervised pre-training and random
initialization respectively, and + denotes supervised ne-
tuning of the whole system.
1. Excellent accuracy of 65.5% is obtained using unsupervised
pre-training and supervised renement with abs
and
normalization non-linearities. The result is on par with the
popular model based on SIFT and pyramid match kernel
SVM [39]. It is clear that abs and normalization are cruciala
for achieving good performance. This
is an extremely
important fact for users of convolutional networks, which
traditionally only use tanh().
2. Astonishingly, random lters without any lter learning
whatsoever achieve decent performance (62.9% for R), as
long as abs and normalization are present (Rabs  N  PA).
A more detailed study on this particular case can be found
in [33].
3. Comparing experiments from rows R vs R+, U vs U +,
we see that supervised ne tuning consistently improves the
performance, particularly with weak non-linearities.
4. It seems that unsupervised pre-training (U , U +) is crucial
when newly proposed non-linearities are not in place.

Handwritten Digit Classication using MNIST Dataset:

Using the evidence gathered in previous experiments, we used
a two-stage system with a two-layer fully-connected classier.
The two convolutional stages were pre-trained unsupervised,
and rened supervised. An error rate of 0.53% was achieved
ont he test set. To our knowledge, this is the lowest error
rate ever reported on the original MNIST dataset, without
distortions or preprocessing. The best previously reported
error rate was 0.60% [32].
Connection with Other Approaches in Object Recognition

Many recent successful object recognition systems can also
be seen as single or multi-layer feature extraction systems fol-
lowed by a classier. Most common feature extraction systems
like SIFT [40], HoG [41] are composed of lterbanks (oriented
edge detectors at multiple scales) followed by non-linearities
(winner take all) and pooling (histogramming). A Pyramid

Match Kernel (PMK) SVM [39] classifer can also be seen as
another layer of feature extraction since it performs a K-means
based feature extraction followed by local histogramming.

III. HARDWARE AND SOFTWARE IMPLEMENTATIONS
Implementing ConvNets in software is best achieved
using the modular, object-oriented approach suggested
in [2]. Each basic module (convolution, pooling, etc)
is implemented as a class with three member
functions
module.fprop(input,output), which computes the
output from the input, module.bprop(input,output),
which back-propagates gradients from the outputs back to
the inputs and the internal
trainable parameters, and op-
tionally module.bbprop(input,output), which may
back-propagate second diagonal derivatives for the implemen-
tation of second-order optimization algorithms [8].

Several software implementations of ConvNets are built
around this concept, and have four basic capabilities: 1. a ex-
ible multi-dimensional array library with basic operations such
as dot products, and convolutions, 2. a class hierarchy of basic
learning machine building blocs (e.g. multiple convolutions
non-linear transforms, cost functions, . . . ), 3. a set of classes
for energy-based inference [42], gradient-based optimization,
and performance measurement.

Three available ConvNet implementations use this concept.
The rst one is part of the Lush system, a Lisp dialect with
an interpreter and compiler with an easy interface to C [43]
. The second one is EBlearn, a C++ machine learning library
with class hierarchy to the Lush implementation [44]. Third
is Torch-5 [45] a C library with an interpreter front end based
on Lua. All three systems come with facilities to manipulate
large datasets, images, and videos.

The rst hardware implementations of ConvNets date back
to the early 90s with Bell Labs ANNA chip, a mixed analog-
digital processor that could compute 64 simultaneous 8  8
convolutions at a peak rate of 4.109 multiply-accumulate
operations per second [46], [47], with 4 bit resolution on the
states and 6 bits on the weights. More recently, a group from
the Canon corporation developed a prototype ConvNet chip for
low-power intelligent cameras [48]. Some current approaches
rely on Addressed-Event Representation (AER) convolvers,
which present
the advantage of not requiring multipliers
to compute the convolutions. CAVIAR is the leading such
project, with a performance of 12G connections/sec [49].

FPGA implementations of ConvNets appeared in the
mid-90s with [50], which used low-accuracy arithmetic to
avoid implementing full-edged multipliers. Fortunately, re-
cent DSP-oriented FPGAs include large numbers of hard-
wired MAC units, which allow extremely fast and low power
implementations of ConvNets. The CNP developed in our
group [51] achieves 10GOPS for 7x7 kernels, with an archi-
tecture that implements entire ConvNets, including pre/post-
processing, and is entirely programmable. An actual face de-
tection application was demonstrated on this system, achieving
10fps on VGA images [52].

IV. CONCLUSION

The Convolutional Network architecture is a remarkably
versatile, yet conceptually simple paradigm that can be applied
to a wide spectrum of perceptual
tasks. While traditional
ConvNet trained with supervised learning are very effective,
training them require a large number of labeled training
samples. We have shown that using simple architectural tricks
such as rectication and contrast normalization, and using

255

unsupervised pre-training of each lter bank, the need for
labeled samples is considerably reduced. Because of their
applicability to a wide range of tasks, and because of their rel-
atively uniform architecture, ConvNets are perfect candidates
for hardware implementations, and embedded applications, as
demonstrated by the increasing amount of work in this area.
We expect to see many new embedded vision systems based
on ConvNets in the next few years.

Despite the recent progress in deep learning, one of the
major challenges of computer vision, machine learning, and
AI in general in the next decade will be to devise methods
that can automatically learn good features hierarchies from
unlabeled and labeled data in an integrated fashion. Current
and future research will focus on performing unsupervised
learning on multiple stages simultaneously, on the integration
of unsupervised and unsupervised learning, and on using the
feed-back path implemented by the decoders to perform visual
inference, such as pattern completion and disambiguation.

