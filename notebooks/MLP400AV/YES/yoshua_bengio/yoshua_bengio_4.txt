Abstract

Recently, many applications for Restricted
Boltzmann Machines (RBMs) have been de-
veloped for a large variety of learning prob-
lems. However, RBMs are usually used as
feature extractors for another learning al-
gorithm or to provide a good initialization
for deep feed-forward neural network clas-
siers, and are not considered as a stand-
alone solution to classication problems. In
this paper, we argue that RBMs provide a
self-contained framework for deriving com-
petitive non-linear classiers. We present an
evaluation of dierent learning algorithms for
RBMs which aim at introducing a discrimi-
native component to RBM training and im-
prove their performance as classiers. This
approach is simple in that RBMs are used
directly to build a classier, rather than as a
stepping stone. Finally, we demonstrate how
discriminative RBMs can also be successfully
employed in a semi-supervised setting.

1. Introduction

Restricted Boltzmann Machines (RBMs) (Smolensky,
1986) are generative models based on latent (usually
binary) variables to model an input distribution, and
have seen their applicability grow to a large variety
of problems and settings in the past few years. From
binary inputs, they have been extended to model var-
ious types of input distributions (Welling et al., 2005;
Hinton et al., 2006). Conditional versions of RBMs
have also been developed for collaborative ltering
(Salakhutdinov et al., 2007) and to model motion cap-
ture data (Taylor et al., 2006) and video sequences
(Sutskever & Hinton, 2007).
RBMs have been particularly successful in classica-
tion problems either as feature extractors for text and

Appearing in Proceedings of the 25 th International Confer-
ence on Machine Learning, Helsinki, Finland, 2008. Copy-
right 2008 by the author(s)/owner(s).

image data (Gehler et al., 2006) or as a good initial
training phase for deep neural network classiers (Hin-
ton, 2007). However, in both cases, the RBMs are
merely the rst step of another learning algorithm, ei-
ther providing a preprocessing of the data or an initial-
ization for the parameters of a neural network. When
trained in an unsupervised fashion, RBMs provide no
guarantees that the features implemented by their hid-
den layer will ultimately be useful for the supervised
task that needs to be solved. More practically, model
selection can also become problematic, as we need to
explore jointly the space of hyper-parameters of both
the RBM (size of the hidden layer, learning rate, num-
ber of training iterations) and the supervised learning
algorithm that is fed the learned features. In partic-
ular, having two separate learning phases (feature ex-
traction, followed by classier training) can be prob-
lematic in an online learning setting.
In this paper, we argue that RBMs can be used suc-
cessfully as stand-alone non-linear classiers along-
side other standard classiers like neural networks
and Support Vector Machines, and not only as fea-
ture extractors. We investigate training objectives for
RBMs that are more appropriate for training clas-
siers than the common generative objective. We
describe Discriminative Restricted Boltzmann Ma-
chines (DRBMs), i.e. RBMs that are trained more
specically to be good classication models, and Hy-
brid Discriminative Restricted Boltzmann Machines
(HDRBMs) which explore the space between discrim-
inative and generative learning and can combine their
advantages. We also demonstrate that RBMs can be
successfully adapted to the common semi-supervised
learning setting (Chapelle et al., 2006) for classica-
tion problems. Finally, the algorithms investigated in
this paper are well suited for online learning on large
datasets.

2. Restricted Boltzmann Machines

Restricted Boltzmann Machines are undirected gener-
ative models that use a layer of hidden variables to
model a distribution over visible variables. Though
they are most often trained to only model the inputs

Classication using Discriminative Restricted Boltzmann Machines

of a classication task, they can also model the joint
distribution of the inputs and associated target classes
(e.g. in the last layer of a Deep Belief Network in Hin-
ton et al. (2006)).
In this section, we will focus on
such joint models.
We assume given a training set Dtrain = {(xi, yi)},
comprising for the i-th example an input vector xi and
a target class yi  {1, . . . , C}. To train a generative
model on such data we consider minimization of the
negative log-likelihood

Lgen(Dtrain) = 

log p(yi, xi).

(1)

i=1

An RBM with n hidden units is a parametric model
of the joint distribution between a layer of hidden
variables (referred to as neurons or features) h =
(h1, . . . , hn) and the observed variables made of x =
(x1, . . . , xd) and y, that takes the form
p(y, x, h)  eE(y,x,h)

where
E(y, x, h) = hT Wx  bT x  cT h  dT ~y  hT U~y

|Dtrain|X

with parameters  = (W, b, c, d, U) and ~y =
(1y=i)C
i=1 for C classes. This model is illustrated in
Figure 2. For now, we consider for simplicity binary
input variables, but the model can be easily gener-
alized to non-binary categories,
integer-valued, and
continuous-valued inputs (Welling et al., 2005; Hinton
et al., 2006). It is straightforward to show that

p(xi|h)

p(x|h) = Y
p(xi = 1|h) = sigm(bi +X
edy+P
P
y edy +P

p(y|h) =

j

i

Wjihj)

j Ujyhj

j Ujy hj

(2)

(3)

where sigm is the logistic sigmoid. Equations 2 and 3
illustrate that the hidden units are meant to capture
predictive information about the input vector as well
as the target class. p(h|y, x) also has a similar form:

p(h|y, x) = Y

p(hj = 1|y, x) = sigm(cj + Ujy +X

p(hj|y, x)

j

Wjixi).

i

When the number of hidden variables is xed, an RBM
can be considered a parametric model, but when it
is allowed to vary with the data, it becomes a non-
parametric model.
In particular, Freund and Haus-
sler (1994); Le Roux and Bengio (2008) showed that

Figure 1. Restricted Boltzmann Machine modeling the
joint distribution of inputs and target classes

an RBM with enough hidden units can represent any
distribution over binary vectors, and that adding hid-
den units guarantees that a better likelihood can be
achieved, unless the generated distribution already
equals the training distribution.
In order to minimize the negative log-likelihood (eq. 1),
we would like an estimator of its gradient with respect
to the model parameters. The exact gradient, for any
parameter    can be written as follows:

 log p(yi, xi)



= EEh|yi,xi

E(yi, xi, h)

+EEy,x,h

E(y, x, h)

.

(cid:20) 
(cid:20) 





(cid:21)

(cid:21)

Though the rst expectation is tractable, the second
one is not. Fortunately, there exists a good stochastic
approximation of this gradient, called the contrastive
divergence gradient (Hinton, 2002). This approxima-
tion replaces the expectation by a sample generated
after a limited number of Gibbs sampling iterations,
with the samplers initial state for the visible variables
initialized at the training sample (yi, xi). Even when
using only one Gibbs sampling iteration, contrastive
divergence has been shown to produce only a small
bias for a large speed-up in training time (Carreira-
Perpinan & Hinton, 2005).
Online training of an RBM thus consists in cy-
cling through the training examples and updating the
RBMs parameters according to Algorithm 1, where
the learning rate is controlled by .
Computing p(y, x) is intractable, but it is possible
to compute p(y|x), sample from it, or choose the
most probable class under this model. As shown in
Salakhutdinov et al. (2007), for reasonable numbers of
classes C (over which we must sum), this conditional
distribution can be computed exactly and eciently,
by writing it as follows:
p(y|x) =

i Wjixi(cid:1)
(cid:0)1 + ecj +Ujy+P
(cid:0)1 + ecj +Ujy +P
i Wjixi(cid:1) .

edyQn
y edyQn
P

j=1

j=1

0  0  0  1yxhUWyClassication using Discriminative Restricted Boltzmann Machines

Algorithm 1 Training update for RBM over (y, x)
using Contrastive Divergence

approach has been used previously for ne-tuning the
top RBM of a Deep Belief Network (Hinton, 2007).

Input: training pair (yi, xi) and learning rate 
% Notation: a  b means a is set to value b
a  p means a is sampled from p
%

% Positive phase

y0  yi, x0  xi, bh0  sigm(c + W x0 + U ~y0)
bh1  sigm(c + W x1 + U ~y1)

% Negative phase
h0  p(h|y0, x0), y1  p(y|h0), x1  p(x|h0)

end for

% Update
for    do
    

 
 E(y0, x0,bh0)  
Precomputing the terms cj +P
them when computing Qn


 E(y1, x1,bh1)
(cid:0)1 + ecj +Ujy +P

i Wjixi and reusing
for all classes y permits to compute this conditional
distribution in time O(nd + nC).

i Wjixi(cid:1)

j=1

3. Discriminative Restricted Boltzmann

Machines

In a classication setting, one is ultimately only inter-
ested in correct classication, not necessarily to have
a good p(x). Because our models p(x) can be in-
appropriate, it can then be advantageous to optimize
directly p(y|x) instead of p(y, x):

|Dtrain|X

i=1

log p(yi|xi).

Ldisc(Dtrain) = 

(4)
We refer to RBMs trained according to Ldisc as Dis-
criminative RBMs (DRBMs).
Since RBMs (with
enough hidden units) are universal approximators for
binary inputs, it follows also that DRBMs are uni-
versal approximators of conditional distributions with
binary inputs.
A DRBM can be trained by contrastive divergence,
as has been done in conditional RBMs (Taylor et al.,
2006), but since p(y|x) can be computed exactly, we
can compute the exact gradient:
 log p(yi|xi)

sigm(oyj(xi)) oyj(xi)





= X
 X
where oyj(x) = cj +P

j,y

j

k Wjkxk + Ujy. This gradient
can be computed eciently and then used in a stochas-
tic gradient descent optimization. This discriminative

sigm(oyj(xi))p(y|xi) oyj(xi)



4. Hybrid Discriminative Restricted

Boltzmann Machines

The advantage brought by discriminative training usu-
ally depends on the amount of available training data.
Smaller training sets tend to favor generative learn-
ing and bigger ones favor discriminative learning (Ng
& Jordan, 2001). However,
instead of solely rely-
ing on one or the other perspective, one can adopt a
hybrid discriminative/generative approach simply by
combining the respective training criteria. Though
this method cannot be interpreted as a maximum like-
lihood approach for a particular generative model as
in Lasserre et al.
(2006), it proved useful here and
elsewhere (Bouchard & Triggs, 2004). In this paper,
we used the following criterion:
Lhybrid(Dtrain) = Ldisc(Dtrain) + Lgen(Dtrain)

(5)

where the weight  of the generative criterion can be
optimized (e.g., based on the validation set classica-
tion error). Here, the generative criterion can also be
seen as a data-dependent regularizer for a DRBM. We
will refer to RBMs trained using the criterion of equa-
tion 5 as Hybrid DRBMs (HDRBMs).
To train an HDRBM, we can use stochastic gradient
descent and add for each example the gradient contri-
bution due to Ldisc with  times the stochastic gradi-
ent estimator associated with Lgen for that example.

5. Semi-supervised Learning

A frequent classication setting is where there are few
labeled training data but many unlabeled examples of
inputs. Semi-supervised learning algorithms (Chapelle
et al., 2006) address this situation by using the un-
labeled data to introduce constraints on the trained
model. For example, for purely discriminative models,
these constraints are often imposed on the decision sur-
face of the model. In the RBM framework, a natural
constraint is to ask that the model be a good gener-
ative model of the unlabeled data, which corresponds
to the following objective:

Lunsup(Dunlab) = 

log p(xi)

(6)

i=1

where Dunlab = {(xi)}|Dunlab|
contains unlabeled ex-
amples of inputs. To train on this objective, we can
once again use a contrastive divergence approximation

i=1

|Dunlab|X

Classication using Discriminative Restricted Boltzmann Machines

of the log-likelihood gradient:

 log p(xi)



= EEy,h|xi

E(yi, xi, h)

+EEy,x,h

E(y, x, h)

(cid:20) 
(cid:20) 





(cid:21)

(cid:21)

The contrastive divergence approximation is slightly
dierent here. The rst term can be computed in time
O(Cn + nd), by noticing that it is equal to

(cid:20)

(cid:20) 



EEy|xi

EEh|y,xi

E(yi, xi, h)

(cid:21)(cid:21)

.
One could either average the usual RBM gradient
 E(yi, xi, h) for each class y (weighted by p(y|xi)), or

sample a y from p(y|xi) and only collect the gradient
for that value of y. In the sampling version, the online
training update for this objective can be described by
replacing the statement y0  yi with y0  p(y|xi) in
Algorithm 1. We used this version in our experiments.
In order to perform semi-supervised learning, we can
weight and combine the objective of equation 6 with
those of equations 1, 4 or 5
Lsemisup(Dtrain,Dunlab) = LTYPE(Dtrain) (7)
+Lunsup(Dunlab)
where TYPE  {gen, disc, hybrid}. Online training
according to this objective simply consists in apply-
ing the appropriate update for each training example,
based on whether it is labeled or not.

6. Related Work

As mentioned earlier, RBMs (sometimes also referred
to as harmoniums (Welling et al., 2005)) have already
been used successfully in the past to extract useful fea-
tures for another supervised learning algorithm. One
of the main contributions of this paper lies in the
demonstration that RBMs can be used on their own
without relying on another learning algorithm, and
provide a self-contained framework for deriving com-
petitive classiers.
In addition to ensuring that the
features learned by the RBMs hidden layer are dis-
criminative, this approach facilitates model selection
since the discriminative power of the hidden layer units
(or features) can be tracked during learning by observ-
ing the progression of classication error on a valida-
tion set. It also makes it easier to tackle online learning
problems relatively to approaches where learning fea-
tures (hidden representation) and learning to classify
are done in two separate phases (Hinton et al., 2006;
Bengio et al., 2007).

Gehler et al. (2006); Xing et al. (2005) have shown
that the features learned by an RBM trained by ig-
noring the labeled targets can be useful for retriev-
ing documents or classifying images of objects. How-
ever, in both these cases, the extracted features were
linear in the input, were not trained discriminatively
and had to be fed to another learning algorithm which
ultimately performed classication. McCallum et al.
(2006) presented Multi-Conditional Learning (MCL)1
for harmoniums in order to introduce a discriminative
component to harmoniums training, but the learned
features still had to be fed to another learning algo-
rithm.
RBMs can also provide a good initialization for the pa-
rameters of neural network classiers (Hinton, 2007),
however model selection issues arise, for instance when
considering the appropriate number of learning up-
dates and the magnitude of learning rates of each
training phase. It has also been argued that the gen-
erative learning aspect of RBM training was a key ele-
ment to their success as good starting points for neural
network training (Bengio et al., 2007), but the extent
to which the nal solution for the parameters of the
neural network is inuenced by generative learning is
not well controlled. HDRBMs can be seen as a way of
addressing this issue.
Finally, though semi-supervised learning was never
reported for RBMs before, Druck et al.
(2007) in-
troduced semi-supervised learning in hybrid genera-
tive/discriminative models using a similar approach to
the one presented in section 5. However, they worked
with log-linear models, whereas the RBMs used here
can perform non-linear classication. Log-linear mod-
els depend much more on the discriminative quality of
the features that are fed as input, whereas an RBM
can learn useful features using their hidden variables,
at the price of non-convex optimization.

7. Experiments

We present experiments on two classication problems:
character recognition and text classication. In all ex-
periments, we performed model selection on a valida-
tion set before testing. For the dierent RBM models,
model selection2 consisted in nding good values for

1We experimented with a version of MCL for the RBMs
considered in this paper, however the results did not im-
prove on those of HDRBMs.

2Model selection was done with a grid-like search over
 (between 0.0005 and 0.1, on a log scale), n (50 to 6000),
 for HDRBMs (0 to 0.5, on a log scale) and  for semi-
supervised learning (0, 0.01 or 0.1). In general, bigger val-
ues of n were found to be more appropriate with more
generative learning. If no local minima was apparent, the

Classication using Discriminative Restricted Boltzmann Machines

the learning rate , the size of the hidden layer n and
good weights for the dierent types of learning (gener-
ative and semi-supervised weights). Also, the number
of iterations over the training set was determined using
early stopping according to the validation set classi-
cation error, with a look ahead of 15 iterations.

7.1. Character Recognition

We evaluated the dierent RBM models on the prob-
lem of classifying images of digits. The images were
taken from the MNIST dataset, where we separated
the original training set into training and validation
sets of 50000 and 10000 examples and used the stan-
dard test set of 10000 examples. The results are
given in Table 1. The ordinary RBM model is trained
generatively (to model (x, y)), whereas RBM+NNet
is an unsupervised RBM used to initialize a one-
hidden layer supervised neural net (as in (Bengio et al.,
2007)). We give as a comparison the results of a Gaus-
sian kernel SVM and of a regular neural network (ran-
dom initialization, one hidden layer, hyperbolic tan-
gent hidden activation functions).
First, we observe that a DRBM outperforms a genera-
tive RBM. However, an HDRBM appears able to make
the best out of discriminative and generative learning
and outperforms the other models.
We also experimented with a sparse version of the
HDRBM model, since sparsity is known to be a good
characteristic for features of images. Sparse RBMs
were developed by Lee et al.
(2008) in the context
of deep neural networks. To introduce sparsity in the
hidden layer of an RBM in Lee et al. (2008), after each
iteration through the whole training set, the biases c
in the hidden layer are set to a value that maintains
the average of the conditional expected value of these
neurons to an arbitrarily small value. This procedure
tends to make the biases negative and large. We fol-
low a dierent approach by simply subtracting a small
constant  value, considered as an hyper-parameter3,
from the biases after each update, which is more ap-
propriate in an online setting or for large datasets.
This sparse version of HDRBMs outperforms all the
other RBM models, and yields signicantly lower clas-

grid was extended. The biases b, c and d were initialized
to 0 and the initial values for the elements of the weight
matrices U and W were each taken from uniform samples

inm0.5, m0.5, where m is the maximum between the

number of rows and columns of the matrix.

3To chose , given the selected values for  and  for
the non sparse HDRBM, we performed a second grid-
search over  (105 and 0.1, on a log scale) and the hidden
layer size, testing bigger hidden layer sizes then previously
selected.

Figure 2. Filters learned by the HDRBM on the MNIST
dataset. The top row shows lters that act as spatially lo-
calized stroke detectors, and the bottom shows lters more
specic to a particular shape of digit.

Table 1. Comparison of the classication performances on
the MNIST dataset.
SVM results for MNIST were
taken from http://yann.lecun.com/exdb/mnist/. On this
dataset, dierences of 0.2% in classication error is statis-
tically signicant.

Model

RBM ( = 0.005, n = 6000)
DRBM ( = 0.05, n = 500)
RBM+NNet
HDRBM ( = 0.01,  = 0.05, n = 1500 )
Sparse HDRBM (idem + n = 3000,  = 104)

SVM
NNet

Error

3.39%
1.81%
1.41%
1.28%
1.16%

1.40%
1.93%

sication error then the SVM and the standard neural
network classiers. The performance achieved by the
sparse HDRBM is particularly impressive when com-
pared to reported performances for Deep Belief Net-
works (1.25% in Hinton et al. (2006)) or of a deep
neural network initialized using RBMs (around 1.2%
in Bengio et al.
(2007) and Hinton (2007)) for the
MNIST dataset with 50000 training examples.
The discriminative power of the HDRBM can be better
understood by looking a the rows of the weight matrix
W, which act as lter features. Figure 2 displays some
of these learned lters. Some of them are spatially
localized stroke detectors which can possibly be active
for a wide variety of digit images, and others are much
more specic to a particular shape of digit.

7.2. Document Classication

We also evaluated the RBM models on the problem of
classifying documents into their corresponding news-
group topic. We used a version of the 20-newsgroup
dataset4 for which the training and test sets contain
documents collected at dierent times, a setting that
is more reective of a practical application. The orig-
inal training set was divided into a smaller training

4This dataset is available in Matlab format here:
http://people.csail.mit.edu/jrennie/20Newsgroups/20news-
bydate-matlab.tgz

Classication using Discriminative Restricted Boltzmann Machines

set and a validation set, with 9578 and 1691 examples
respectively. The test set contains 7505 examples. We
used the 5000 most frequent words for the binary input
features. The results are given in Figure 3(a). We also
provide the results of a Gaussian kernel SVM5 and of
a regular neural network for comparison.
Once again, HDRBM outperforms the other RBM
models. However, here the generatively trained RBM
performs better then the DRBMs. The HDRBM also
outperforms the SVM and neural network classiers.
In order to get a better understanding of how the
HDRBM solves this classication problem, we rst
looked at the weights connecting each of the classes to
the hidden neurons. This corresponds to the columns
Uy of the weight matrix U. Figure 3(b) shows a sim-
ilarity matrix M(U) for the weights of the dierent
newsgroups, where M(U)y1y2 = sigm(UTy1Uy2). We
see that the HDRBM does not use dierent neurons for
dierent newsgroups, but shares some of those neurons
for newsgroups that are semantically related. Another
interesting visualization of this characteristic is given
in Figure 3(c), where the columns of U were projected
on their two principal components. In both cases, we
see that the HDRBM tends to share neurons for simi-
lar topics, such as computer (comp.*), science (sci.*)
and politics (talk.politics.*), or secondary topics
such as sports (rec.sports.*) and other recreational
activities (rec.autos and rec.motorcycles).
Table 2 also gives the set of words used by the HDRBM
to recognize some of the newsgroups. To obtain this
table we proceeded as follows: for each newsgroup y,
we looked at the 20 neurons with the largest weight
among Uy, aggregated (by summing) the associated
input-to-hidden weight vectors, sorted the words in de-
creasing order of their associated aggregated weights
and picked the rst words according to that order.
This procedure attempts to approximate the positive
contribution of the words to the conditional probabil-
ity of each newsgroup.

7.3. Semi-supervised Learning

We evaluated our semi-supervised learning algorithm
for the HDRBM on both the digit recognition and doc-
ument classication problems. We also experimented
with a version (noted MNIST-BI) of the MNIST
dataset proposed by Larochelle et al.
(2007) where
background images have been added to MNIST digit
images. This version corresponds to a much harder
problem, but it will help to illustrate the advantage
brought by semi-supervised learning in HDRBMs. The

5We used libSVM v2.85 to train the SVM model

HDRBM trained on this data used truncated exponen-
tial input units (see (Bengio et al., 2007)).
In this semi-supervised setting, we reduced the size
of the labeled training set to 800 examples, and used
some of the remaining data to form an unlabeled
dataset Dunlab. The validation set was also reduced
to 200 labeled examples. Model selection6 covered all
the parameters of the HDRBM as well as the unsuper-
vised objective weight  of equation 7. For compar-
ison purposes, we also provide the performance of a
standard non-parametric semi-supervised learning al-
gorithm based on function induction (Bengio et al.,
2006b), which includes as a particular case or is very
similar to other non-parametric semi-supervised learn-
ing algorithms such as Zhu et al. (2003). We provide
results for the use of a Gaussian kernel (NP-Gauss)
and a data-dependent truncated Gaussian kernel (NP-
Trunc-Gauss) used in Bengio et al. (2006b), which es-
sentially outputs zero for pairs of inputs that are not
near neighbors. The experiments on the MNIST and
MNIST-BI (with background images) datasets used
5000 unlabeled examples and the experiment on 20-
newsgroup used 8778. The results are given in Table 3,
where we observe that semi-supervised learning consis-
tently improves the performance of the HDRBM.
The usefulness of non-parametric semi-supervised
learning algorithms has been demonstrated many
times in the past, but usually so on problems where the
dimensionality of the inputs is low or the data lies on
a much lower dimensional manifold. This is reected
in the result on MNIST for the non-parametric meth-
ods. However, for high dimensional data with many
factors of variation, these methods can quickly suer
from the curse of dimensionality, as argued by Bengio
et al. (2006a). This is also reected in the results for
the MNIST-BI dataset which contains many factors of
variation, and for the 20-newsgroup dataset where the
input is very high dimensional.
Finally, it is important to notice that semi-supervised
learning in HDRBMs proceeds in an online fashion and
hence could scale to very large datasets, unlike more
standard non-parametric methods.

7.4. Relationship with Feed-forward Neural

Networks

There are several similarities between discriminative
RBMs and neural networks. In particular, the com-
putation of p(y|x) could be implemented by a single
layer neural network with softplus and softmax acti-

6 = 0.1 for MNIST and 20-newsgroup and  = 0.01

for MNIST-BI was found to perform best.

Classication using Discriminative Restricted Boltzmann Machines

Model

RBM ( = 0.0005, n = 1000)
DRBM ( = 0.0005, n = 50)
RBM+NNet
HDRBM ( = 0.005,  = 0.1, n = 1000 )

SVM
NNet

Error

24.9%
27.6%
26.8%
23.8%

32.8%
28.2%

(a) Classication performances

(b) Similarity matrix

(c) PCA embedding

Figure 3. Experiment on 20-newsgroup dataset. (Top left) Classication performance for the dierent models. The error
dierences between HDRBM and other models is statistically signicant. (Bottom left) Similarity matrix of the newsgroup
weights vectors Uy. (Right) Two dimensional PCA embedding of the newsgroup weights.

Table 2. Most inuential words in the HDRBM for predicting some of the document classes

Words

Class
alt.atheism bible, atheists, benedikt, atheism, religion
misc.forsale
sci.crypt

sell, condition, oppy, week, am, obo
sternlight, bontchev, nsa, escrow, hamburg

Table 3. Comparison of the classication errors in semi-
supervised learning setting. The errors in bold are statis-
tically signicantly better.

Model

MNIST MNIST-BI

20-news

HDRBM
9.73%
Semi-sup HDRBM 8.04%

42.4%
37.5%

40.5%
31.8%

66.5%
61.3%

10.60%
7.49%

85.0%
NP-Gauss
NP-Trunc-Gauss
82.6%
vation functions in its hidden and output layers re-
spectively, with a special structure in the output and
hidden weights where the value of the output weights is
xed and many of the hidden layer weights are shared.
The advantage of working in the framework of RBMs
is that it provides a natural way to introduce gener-
ative learning, which we used here to derive a semi-
supervised learning algorithm. As mentioned earlier, a
form of generative learning can be introduced in stan-

Class
comp.graphics
rec.autos
talk.politics.guns

Words
ti, ftp, window, gif, images, pixel
cars, ford, autos, sho, toyota, roads
rearms, handgun, rearm, gun, rkba

dard neural networks simply by using RBMs to ini-
tialize the hidden layer weights. However the extent
to which the nal solution for the parameters of the
neural network is inuenced by generative learning is
not well controlled. This might explain the superior
performance obtained by a HDRBM compared to a
single hidden layer neural network initialized with an
RBM (RBM+NNet in the tables).

8. Conclusion and Future Work

We argued that RBMs can and should be used as
stand-alone non-linear classiers alongside other stan-
dard and more popular classiers, instead of merely
being considered as simple feature extractors. We eval-
uated dierent training objectives that are more ap-
propriate to train an RBM in a classication setting.
These discriminative versions of RBMs integrate the
process of discovering features of inputs with their use
in classication, without relying on a separate classi-

Classication using Discriminative Restricted Boltzmann Machines

er. This insures that the learned features are dis-
criminative and facilitates model selection. We also
presented a novel but straightforward semi-supervised
learning algorithm for RBMs and demonstrated its
usefulness for complex or high dimensional data.
For future work, we would like to investigate the use
of discriminative versions of RBMs in more challeng-
ing settings such as in multi-task or structured out-
put problems. The analysis of the target weights
for the 20-newsgroup dataset seem to indicate that
RBMs would be good at capturing the conditional sta-
tistical relationship between multiple tasks or in the
components in a complex target space. Exact com-
putation of the conditional distribution for the tar-
get is not tractable anymore, but there exists promis-
ing techniques such as mean-eld approximations that
could estimate that distribution. Moreover, in the 20-
newsgroup experiment, we only used 5000 words in
input because generative training using Algorithm 1
does not exploit the sparsity of the input, unlike an
SVM or a DRBM (since in that case the sparsity of the
input makes the discriminative gradient sparse too).
Motivated by this observation, we intend to explore
ways to introduce generative learning in RBMs and
HDRBMs which would be less computationally expen-
sive when the input vectors are large but sparse.

Acknowledgments

We thank Dumitru Erhan for discussions about sparse
RBMs and anonymous reviewers for helpful comments.

