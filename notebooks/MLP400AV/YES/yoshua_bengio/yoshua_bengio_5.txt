Abstract

This  paper  studies  the  convergence  properties  of the  well  known
K-Means clustering algorithm.  The K-Means algorithm can be de(cid:173)
scribed either as a gradient descent algorithm or by slightly extend(cid:173)
ing  the  mathematics of the  EM  algorithm  to  this  hard  threshold
case.  We show that the K-Means algorithm actually minimizes the
quantization error  using  the very  fast  Newton  algorithm.

INTRODUCTION

1
K-Means is a  popular clustering algorithm used  in many applications, including the
initialization  of  more  computationally  expensive  algorithms  (Gaussian  mixtures,
Radial  Basis  Functions,  Learning  Vector  Quantization  and  some  Hidden  Markov
Models).  The  practice  of this  initialization  procedure  often  gives  the  frustrating
feeling  that  K-Means  performs  most of the  task  in  a  small fraction  of the  overall
time.  This motivated us  to better  understand  this convergence  speed.
A  second  reason  lies  in  the  traditional  debate  between  hard  threshold  (e.g.  K(cid:173)
Means,  Viterbi Training)  and soft threshold  (e.g.  Gaussian Mixtures, Baum Welch)
algorithms  (Nowlan,  1991) .  Soft  threshold  algorithms are  often  preferred  because
they have an elegant  probabilistic framework and a general optimization algorithm
named EM  (expectation-maximization) (Dempster,  Laird and Rubin,  1977).  In the
case of a gaussian  mixture,  the EM  algorithm has  recently  been  shown  to  approxi(cid:173)
mate the Newton  optimization algorithm (Xu  and Jordan,  1994).  We  prove  in this

"also,  AT&T Bell  Labs,  Holmdel,  NJ 07733

586

Leon  Bottou,  Yoshua  Bengio

paper  that  the  corresponding  hard  threshold  algorithm)  K-Means)  minimizes  the
quantization error  using  exactly the Newton algorithm.
In  the  next  section)  we  derive  the  K-Means  algorithm as  a  gradient  descent  pro(cid:173)
cedure.  Section  3  extends  the  mathematics  of the  EM  algorithm  to  the  case  of
K-Means.  This second  derivation  of K-Means  provides  us  with  proper  values  for
the  learning  rates.  In  section  4  we  show  that  this  choice  of learning  rates  opti(cid:173)
mally rescales  the  parameter space  using  Newton)s  method.  Finally)  in  section  5
we  present  and discuss a few  experimental results comparing various versions of the
K-Means  algorithm.  The  5 clustering  algorithms  presented  here  were  chosen  for  a
good coverage  of the  algorithms related  to K-Means)  but  this  paper does  not have
the ambition of presenting  a literature survey  on  the subject.

2  K-MEANS  AS  A  GRADIENT DESCENT
Given  a  set  of P  examples  (Xi)
the  K-Means  algorithm  computes  k  prototypes
W  =  (Wk)  which minimize the  quantization  error, i.e.)  the average distance  between
each  pattern  and the closest  prototype:

E  W  =  L....J  L  Xi) W  =  L....J  - mln Xi  - Wk)

)  def  ""'  (

.  (

(

2

.
t

)  def  ""' 1
.  2
t

(1)

Writing Si (w)  for  the subscript  of the closest  prototype  to example Xi,  we  have

(2)

2.1  GRADIENT DESCENT  ALGORITHM

We  can  then  derive  a  gradient  descent  algorithm  for  the  quantization  error:
~W = -(t 8~;:).  This leads to the following  batch  update equation  (updating pro(cid:173)
totypes  after presenting all the examples) :

(3)

We  can also derive a corresponding  online algorithm which updates the prototypes
after  the  presentation of each  pattern  Xi:

_

A
u.W - -(t

OL(Xi) w)

OW

)

l.e.)

if k =  Si(W)
otherwise.

(4)

The  proper  value of the  learning rate  (t  remain to  be  specified  in  both batch  and
online  algorithms.  Convergence  proofs  for  both  algorithms  (Bottou)  1991)  exist
for  decreasing  values of the learning rates  satisfying the conditions E (t  =  00  and
E (;  <  00.  Following  (Kohonen)  1989),  we  could  choose  (t  =  (o/t .  We  prove
however  in this  paper  that there exist  a  much better choice  of learning rates.

Convergence  Properties  of the  K-Means  Algorithms

587

3  K-MEANS  AS  AN EM  STYLE  ALGORITHM
3.1  EM  STYLE  ALGORITHM
The  following  derivation  of K-Means  is  similar  to  the  derivation  of  (MacQueen,
1967).  We  insist  however  on  the  identity  between  this  derivation  and  the  mathe(cid:173)
matics of EM  (Liporace,  1976)  (Dempster,  Laird and Rubin,  1977).
Although  [{-Means  does  not  fit  in  a  probabilistic  framework,  this  similarity  holds
for  a  very  deep  reason:  The  semi-ring  of probabilies  (!R+, +, x)  and  the  idempo(cid:173)
tent  semi-ring  of hard-threshold  scores  (!R,  Min, +)  share  the  most  significant  al(cid:173)
gebraic  properties  (Bacceli,  Cohen  and  Olsder,  1992).  This  assertion  completely
describes  the  similarities  and  the  potential  differences  between  soft-threshold  and
hard-threshold algorithms.  A complete discussion however stands outside the scope
of this  paper.
The principle of EM  is to introduce additional "hidden"  variables whose  knowledge
would  make the  optimization problem easier.  Since  these  hidden  variables  are  un(cid:173)
known,  we  maximize  an  auxiliary function  which  averages  over  the  values  of the
hidden  variables  given  the  values  of the  parameters  at  the  previous  iteration.  In
our case,  the hidden variables are . the assignments  Si(W)  of the  patterns to the pro(cid:173)
totypes.  Instead  of considering  the  expected  value  over  the  distribution  on  these
hidden  variables,  we just consider  the values  of the hidden variables  that  minimize
the cost,  given  the  previous  values of the  parameters:

Q(

W ,W  =  L...J '2  Xi  - W &.(w)

)  def  '"' 1 (

I

I

)2

The  next  step  consists  then  in  finding  a  new  set  of  prototypes  Wi  which  mllll(cid:173)
mizes  Q( Wi, w)  where  w  is  the  previous  set  of prototypes.  We  can  analytically
compute  the  explicit  solution of this  minimization problem.  Solving  the  equation
8Q(w' , W)/8W'k  = 0 yields:

i

I

wk  = 111
1
k i:k=&i(w)

'"'
L...J

Xi

(5)

where  Nk  is  the  number  of examples  assigned  to  prototype  Wk .  The  algorithm
consists in repeatedly replacing w  by Wi  using update equation (6)  until convergence.
Since  si(w' ) is  by  definition  the  best  assignment  of patterns  Xi  to  the  prototypes
w~, we  have  the following inequality:

E(w' ) - Q(w' , w)  =  ~ L(Xi - Wi &.(w,))2  - (Xi  - Wi Si(W))2  ::;  0

i

Using  this  result,  the  identity  E(w)  =  Q(w, w)  and  the  definition  of Wi,  we  can
derive  the following inequality:

E(w' ) - E(w)

E(w' ) - Q(w' , w) + Q(w' , w)  - Q(w, w)

<  Q(w' , w)  - Q(w, w)

::;  0

Each  iteration of the  algorithm thus  decreases  the  otherwise  positive  quantization
error  E  (equation  1)  until the error  reaches  a fixed  point where condition W*'  =  w*
is  verified  (unicity  of the  minimum of Q(., w*)).  Since  the  assignment  functions
Si (w)  are  discrete,  there  is  an  open  neighborhood  of w*  on  which  the  assignments
are  constant.  According  to their  definition,  functions  E(.)  and Q(., w*)  are  equal
on this neighborhood.  Being the minimum of function Q(.,w*), the fixed  point w*
of this  algorithm is  also a  local  minimum of the quantization error  E.

0

588

Leon  Bottou,  Yoshua  Bengio

3.2  BATCH  K-MEANS
The  above  algorithm  (5)  can  be  rewritten  in  a  form  similar  to  that  of the  batch
gradient descent  algorithm (3).

if k  = S(Xi' w)
otherwise.

(6)

This  algorithm is  thus equivalent  to  a  batch  gradient  descent  with  a  specific,  pro(cid:173)
totype dependent,  learning  rate Jk 

3.3  ONLINE K-MEANS
The online version of our EM  style update equation (5)  is  based on the computation
of the  mean  flt  of the examples  Xl, ... ,Xt  with  the following  recursive formula:

flt+l  = t~l (t  flt + Xt+l)  = flt  + t~l (Xt+l  -

fld

Let  us  introduce  new  variables  nk  which  count  the  number  of  examples  so  far
assigned  to  prototype  Wk.  We  can  then  rewrite  (5)  as  an  online  update  applied
after  the  presentation of each  pattern  Xi:

D..nk

D..wk

if k = S(Xi' w)

0  otherwise.

{ 1
{ n\ (Xi  - Wk)

0

if k  =  S(Xi'W)
otherwise.

(7)

This algorithm is  equivalent to an online gradient descent  (4)  with a specific,  proto(cid:173)
type dependent,  learning rate  nlk.  Unlike in the batch case,  the pattern assignments
S(Xi , w)  are thus changing after each  pattern  presentation.  Before  applying this  al(cid:173)
gorithm,  we  must  of course  set  nk  to  zero  and  Wk  to  some  initial  value.  Various
methods have  been  proposed  including initializing Wk  with the first  k patterns.

3.4  CONVERGENCE
General convergence proofs for  the batch and online gradient descent  (Bottou, 1991;
Driancourt,  1994)  directly  apply for  all four  algorithms.  Although  the  derivatives
are  undefined  on  a  few  points,  these  theorems  prove  that  the  algorithms  almost
surely converge to a local minimum because  the local variations of the loss function
are conveniently bounded (semi-differentiability).  Unlike previous results,  the above
convergence  proofs  allow  for  non-linearity,  non-differentiability  (on  a  few  points)
(Bottou,  1991),  and  replacing  learning  rates  by  a  positive  definite  matrix  (Drian(cid:173)
court,  1994).

4  K-MEANS  AS  A  NEWTON OPTIMIZATION
We  prove in  this section that Batch  K-Means  (6)  applies the Newton  algorithm.

4.1  THE  HESSIAN  OF  K-MEANS
Let  us  compute  the  Hessian  H  of  the  K-Means  cost  function  (2).  This  matrix
contains  the  second  derivatives  of the  cost  E(w)  with  respect  to  each  pair  of pa(cid:173)
rameters.  Since  E(w)  is  a  sum of terms  L(Xi'W), we  can  decompose  H  as  the sum

Convergence  Properties  of the  K-Means  Algorithms

589

of matrices  Hi  for  each  term of the cost function:

L(Xi'W) = ~in~(xi - Wk)2.

Furthermore,  the  Hi  can  be  decomposed  in  blocks  corresponding  to  each  pair  of
prototypes.  Since  L( Xi, w)  depends  only on the closest  prototype to pattern  Xi,  all
these blocks are zero except block  (Si (w), Si (w))  which is the identity matrix. Sum(cid:173)
ming the  partial Hessian  matrices  Hi  thus gives  a  diagonal matrix whose  diagonal
elements  are  the counts of examples Nk  assigned  to each  prototype.

H=(T  o

We can thus  write the  Newton update of the parameters as follows:

~w =  _H- 1 oE(w)

ow

which can be exactly rewritten as the batch EM style algorithm (6) presented earlier:

(8)

~w =" { ~k (Xi  - Wk)

k

if k = ~(Xi' w)

otherwlse.

L..J

i

0

4.2  CONVERGENCE  SPEED
When optimizing a  quadratic function,  Newton's algorithm requires  only one step.
In  the  case  of a  non  quadratic  function,  Newton's  algorithm  is  superlinear  if we
can  bound the variations of the second  derivatives.  Standard theorems that  bound
this  variation  using  the  third  derivative  are  not  useful  for  K-Means  because  the
gradient of the cost  function  is  discontinuous.  We  could notice  that  the  variations
of the second  derivatives  are however  nicely  bounded  and derive  similar proofs for
K-Means.
For  the sake of brevity however,  we  are just giving here  an intuitive argument:  we
can  make  the  cost  function  indefinitely  differentiable  by  rounding  up  the  angles
around the non differentiable points.  We  can even  restrict  this cost function change
to  an  arbitrary  small  region  of the  space.  The  iterations  of K-Means  will  avoid
this  region  with  a  probability arbitrarily close  to  1.  In  practice,  we  obtain thus  a
superlinear convergence.
Batch  K-Means  thus  searches  for  the  optimal prototypes  at  Newton  speed.  Once
it  comes  close  enough  to  the  optimal  prototypes  (i.e.  the  pattern  assignment  is
optimal and the cost function  becomes quadratic), K-Means jumps to the optimum
and terminates.
Online  K-Means  benefits  of these  optimal learning  rates  because  they  remove  the
usual  conditioning  problems  of  the  optimization.  However ,  the  stochastic  noise
induced by the online procedure limits the final convergence of the algorithm.  Final
convergence  speed  is  thus  essentially  determined  by  the  schedule  of the  learning
rates.
Online K-Means also benefits from the redundancies of the training set.  It converges
significantly  faster  than  batch  K-Means  during  the  first  training  epochs  (Darken

Leon  Bottou,  Yoshua  Bengio

BII  Co.t

590

IaI  Co.t

2500
2'00
2300
2200
2100
2000
1900
1800
1700
1600
1500
1600
1300
1200
1100
1000
900
800
700
600
500
'00
300
200
100

1  2

3

,

5  6

7

8

9  10  11  12  13  16  15  16  17  18  19  20

Figure  1:  Et  - Eoo  versus  t.  black  circles:  online  K-Means;  black  squares:  batch
K-Means;  empty circles:  online gradient; empty squares:  batch gradient;  no  mark:
EM +Gaussian mixture

and  Moody,  1991).  After  going  through  the  first  few  patterns  (depending  of the
amount of redundancy),  online  K-Means  indeed  improves  the  prototypes  as  much
as  a  complete  batch  K-Means  epoch.  Other  researchers  have  compared batch  and
online algorithms for  neural  networks,  with similar conclusions  (Bengio,  1991).

5  EXPERIMENTS
Experiments have been carried out with Fisher's iris data set,  which is  composed of
150  points in  a four  dimensional space  representing  physical measurements on var(cid:173)
ious species  of iris flowers.  Codebooks of six prototypes  have  been  computed using
both  batch  and online  K-Means  with  the  proper  learning  rates  (6)  and  (7).  These
results  are  compared  with  those  obtained  using  both gradient  descent  algorithms
(3)  and  (4)  using learning rate  ft = 0.03/t that we  have found optimal.  Results  are
also  compared  with  likelihood maximization with  the  EM  algorithm,  applied  to  a
mixture of six  Gaussians,  with fixed  and  uniform  mixture weights,  and fixed  unit
variance.  Inputs  were  scaled down  empirically so  that  the  average  cluster  variance
was  around unity.  Thus only the cluster positions were learned,  as for  the K-Means
algorithms.
Each run of an algorithm consists in (a) selecting a random initial set  of prototypes,
(b)  running the algorithm during 20 epochs and recording the error measure E t  after
each  epoch,  (c)  running  the  batch  K-Means  algorithm!  during  40  more  epochs  in
order  to  locate  the  local  minimum Eoo  corresponding  to  the  current  initialization
of the  algorithm.  For  the  four  K-Means  algorithms,  E t  is  the  quantization  error
(equation 1).  For the Gaussian mixture trained with EM, the cost Et  is  the negative

1 except for the case of the mixture of Gaussians, in which the EM algorithm was applied

Convergence  Properties  of the  K-Means  Algorithms

591

1

2

3



5

6

7

8

9  10  11  12  13  U  15  16  17  18  19

Figure  2:  E~-;~~~oo  versus  t .  black  circles:  online  K-Means;  black  squares:  batch
K-Means; empty circles:  online gradient;  empty squares:  batch gradient; no mark:
EM+Gaussian mixture

logarithm of the likelihood of the  data given  the model.
Twenty  trials  were  run  for  each  algorithm.  Using  more  than  twenty  runs  did  not
improve the standard deviation of the averaged measures because  various initializa(cid:173)
tions  lead  to  very  different  local  minima.  The  value  Eoo  of the  quantization error
on  the  local  minima ranges  between  3300  and  5800.  This  variability is  caused  by
the different  initializations and not by the different  algorithms. The average values
of Eoo  for  each  algorithm indeed fall in a  very  small range  (4050  to 4080).
Figure  1  shows  the  average  value  of the  residual  error  Et  - Eoo  during  the  first
20  epochs.  Online K-Means  (black circles)  outperforms all other algorithms during
the first  five  epochs  and stabilizes on  a  level  related  to the stochastic  noise  of the
online  procedure.  Batch  K-Means  (black  squares)  initially converges  more  slowly
but outperforms all other methods after 5 epochs.  All  20  runs converged  before the
15th  epoch.  Both gradients  algorithms display  poor  convergence  because  they do
not  benefit  of the  Newton  effect.  Again,  the  online  version  (white  circles)  starts
faster  then  the  batch  version  (white  square)  but  is  outperformed  in  the  long  run.
The  negative  logarithm  of the  Gaussian  mixture  is  shown  on  the  curve  with  no
point marks, and the scale is  displayed on  the  right  of Figure  1.
Figure 2 show the final convergence  properties of all five  algorithms. The evolutions
of the  ratio  (Et+l  - Eoo)/(Et - Eoo)  characterize  the  relative improvement of the
residual error  after each iteration.  All algorithms exhibit the same behavior after a
few  epochs except batch K-Means (black squares).  The fast convergence ofthis ratio
to  zero  demonstrates  the final  convergence  of batch  K-Means.  The  EM  algorithm
displays  a  better  behavior  than  all  the  other  algorithms  except  batch  K-Means.
Clearly, however , its relative improvement ratio doesn't display the fast convergence
behavior of batch  K-Means.

592

Uon  Bottou,  Yoshua  Bengio

The  online  K-Means  curve  crosses  the  batch  K-Means  curve  during  the  second
epoch,  suggesting that it is  better to run the online algorithm (7)  during one epoch
and then switch  to  the  batch algorithm (6).

6  CONCLUSION
We  have  shown  with  theoretical  arguments  and  simple  experiments  that  a  well
implemented K-Means  algorithm minimizes  the quantization error  using  Newton's
algorithm.  The EM  style derivation of K-Means shows that the mathematics of EM
are valid well outside the framework of probabilistic models.  Moreover the provable
convergence  properties  of the  hard  threshold  K-Means  algorithm  are  superior  to
those  of the  EM  algorithm for  an  equivalent  soft  threshold  mixture of Gaussians.
Extending  these  results  to  other  hard  threshold  algorithms  (e.g.  Viterbi  Training)
is  an interesting open question.

