Abstract

We report on the successful application of
feature selection methods to a classi(cid:12)ca-
tion problem in molecular biology involving
only 72 data points in a 7130 dimensional
space. Our approach is a hybrid of (cid:12)lter
and wrapper approaches to feature selec-
tion. We make use of a sequence of simple
(cid:12)lters, culminating in Koller and Sahamis
(1996) Markov Blanket (cid:12)lter, to decide on
particular feature subsets for each subset
cardinality. We compare between the re-
sulting subset cardinalities using cross val-
idation. The paper also investigates regu-
larization methods as an alternative to fea-
ture selection, showing that feature selec-
tion methods are preferable in this prob-
lem.

1. Introduction
Structural and functional data from analysis of the
human genome have increased many fold in recent
years, presenting enormous opportunities and chal-
lenges for machine learning. In particular, gene ex-
pression microarrays are a rapidly maturing technol-
ogy that provide the opportunity to assay the ex-
pression levels of thousands or tens of thousands of
genes in a single experiment (Shalon et al., 1996).
These assays provide the input to a wide variety
of statistical modeling e(cid:11)orts, including classi(cid:12)ca-
tion, clustering, and density estimation. For exam-
ple, by measuring expression levels associated with
two kinds of tissue, tumor or non-tumor, one obtains
labeled data sets that can be used to build diagnostic
classi(cid:12)ers. The number of replicates in these exper-
iments are often severely limited, however; indeed,
in the data that we analyze here (cf. Golub, et al.,
1999), there are only 72 observations of the expres-
sion levels of each of 7130 genes. In this extreme of
very few observations on very many features, it is
natural|and perhaps essential|to investigate fea-

ture selection and regularization methods.
Feature selection methods have received much atten-
tion in the classi(cid:12)cation literature (Kohavi & John,
1997; Langley, 1994), where two kinds of meth-
ods have generally been studied|(cid:12)lter methods and
wrapper methods. The essential di(cid:11)erence between
these approaches is that a wrapper method makes
use of the algorithm that will be used to build the
(cid:12)nal classi(cid:12)er, while a (cid:12)lter method does not. Thus,
given a classi(cid:12)er C, and given a set of features F ,
a wrapper method searches in the space of subsets
of F , using cross validation to compare the perfor-
mance of the trained classi(cid:12)er C on each tested sub-
set. A (cid:12)lter method, on the other hand, does not
make use of C, but rather attempts to (cid:12)nd predic-
tive subsets of the features by making use of simple
statistics computed from the empirical distribution.
An example is an algorithm that ranks features in
terms of the mutual information between the fea-
tures and the class label. Wrapper algorithms can
perform better than (cid:12)lter algorithms, but they can
require orders of magnitude more computation time.
An additional problem with wrapper methods is that
the repeated use of cross validation on a single data
set can lead to uncontrolled growth in the proba-
bility of (cid:12)nding a feature subset that performs well
on the validation data by chance alone. In essence,
in hypothesis spaces that are extremely large, cross
validation can over(cid:12)t.
While theoretical attempts to calculate complexity
measures in the feature selection setting generally
lead to the pessimistic conclusion that exponentially
many data points are needed to provide guarantees
of choosing good feature subsets, Ng has recently
described a generic feature selection methodology,
referred to as FS-ORDERED, that leads to more
optimistic conclusions (Ng, 1998). In Ngs approach,
cross validation is used only to compare between fea-
ture subsets of di(cid:11)erent cardinality. Ng proves that
this approach yields a generalization error that is
upper-bounded by the logarithm of the number of

irrelevant features.
In a problem with over 7000 features, (cid:12)ltering meth-
ods have the key advantage of signi(cid:12)cantly smaller
computational complexity than wrapper methods,
and for this reason these methods are the main fo-
cus of this paper. Earlier papers that have ana-
lyzed microarray data have also used (cid:12)ltering meth-
ods (Golub et al., 1999; Chow et al.,
in press;
Dudoit et al., 2000). We show, however, that it
is also possible to exploit prediction-error-oriented
wrapper methods in the context of a large feature
space.
In particular, we adopt the spirit of Ngs
FS-ORDERED approach and present a speci(cid:12)c al-
gorithmic instantiation of his general approach in
which (cid:12)ltering methods are used to choose best sub-
sets for a given cardinality. Thus we use simple (cid:12)l-
tering methods to carry out the major pruning of the
hypothesis space, and use cross validation for (cid:12)nal
comparisons.
While feature selection methods search in the com-
binatorial space of feature subsets, regularization
or shrinkage methods trim the hypothesis space by
constraining the magnitudes of parameters (Bishop,
1995). Consider, for example, a linear regression
problem in which the parameters (cid:18)i are (cid:12)t by least
squares. Regularization adds a penalty term to
the least squares cost function, typically either the
squared L2 norm or the L1 norm. These terms are
multiplied by a parameter (cid:21), the regularization pa-
rameter. Choosing (cid:21) by cross validation, one ob-
tains a (cid:12)t in which the parameters (cid:18)i are shrunk
toward zero. This approach e(cid:11)ectively restricts the
hypothesis space, providing much of the protection
against over(cid:12)tting that feature selection methods
aim to provide.
If the goal is to obtain small feature sets for com-
putational or interpretational reasons, then feature
selection is an obligatory step. If the goal is to ob-
tain the best predictive classi(cid:12)er, however, then reg-
ularization methods may perform better than fea-
ture selection methods. Few papers in the machine
learning literature have compared these approaches
directly; we propose to do so in the current paper.

2. Feature Selection
In this section we describe the feature selection
To summarize
methodology that we adopted.
briey, our approach proceeds in three phases.
In
the (cid:12)rst phase we use unconditional univariate mix-
ture modeling to provide an initial assessment of the
viability of a (cid:12)ltering approach, and to provide a
discretization for the second phase.
In the second
phase, we rank features according to an information
gain measure, substantially reducing the number of
features that are input to the third phase. Finally, in

the third phase we use the more computationally in-
tense procedure of Markov blanket (cid:12)ltering to choose
candidate feature subsets that are then passed to a
classi(cid:12)cation algorithm.

2.1 Unconditional Mixture Modeling
A useful empirical assumption about the activity
of genes, and hence their expression, is that they
generally assume two distinct biological states (ei-
ther \on" or \o(cid:11)"). The combination of such binary
patterns from multiple genes determines the sample
phenotype. Given this assumption, we expect that
the marginal probability of a given expression level
can be modeled as a univariate mixture with two
components (which includes the degenerate case of
a single component). Representative samples of em-
pirical marginals are shown in Figure 1.

feature 109

feature 1902

10

8

6

4

2

0
1

0.5

0

0.5

1

15

10

5

0
1

0.5

0

0.5

1

Figure 1. Two representative histograms of gene expres-
sion measurements. The x-axes represents the normal-
ized expression level.

If the underlying binary state of the gene does
not vary between the two classes, then the gene is
not discriminative for the classi(cid:12)cation problem and
should be discarded. This suggests a heuristic pro-
cedure in which we measure the separability of the
mixture components as an assay of the discriminabil-
ity of the feature.
j (cid:18)i) denote a two-component Gaussian
Let P (fi
mixture model for feature fi, where (cid:18)i denotes the
means, standard deviations and mixing proportions
of the mixture model. We (cid:12)t these parameters using
the EM algorithm. Note that each feature fi is (cid:12)t
independently.
Suppose that we denote the underlying state of gene
i as a latent variable zi 2 f0; 1g. Suppose moreover
that we de(cid:12)ne a decision d(fi) on feature fi to be 0 if
the posterior probability of fzi = 0g is greater than
0:5 under the mixture model, and let d(fi) equal 1
otherwise. We now de(cid:12)ne a mixture overlap proba-
bility:

(cid:15) = P (zi = 0)P (d(fi) = 1jzi = 0)
+ P (zi = 1)P (d(fi) = 0jzi = 1):

(1)

If the mixture model were a true representation of
the probability of gene expression, then the mixture

overlap probability would represent the Bayes error
of classi(cid:12)cation under this model. We use this proba-
bility as a heuristic surrogate for the discriminating
potential of the gene, as assessed via its uncondi-
tional marginal.
Note that a mixture model can also be used as
a quantizer, allowing us to discretize the measure-
ments for a given feature. We simply replace the
continuous measurement fi with the associated bi-
nary value d(fi). This is in fact the main use that
we make of the mixture models in the remainder of
the paper. In particular, in the following section we
use the quantized features to de(cid:12)ne an information
gain measure.

2.2 Information Gain Ranking
We now turn to methods that make use of the class
labels. The goal of these methods is to (cid:12)nd a good
approximation of the conditional distribution, P (C j
F), where F is the overall feature vector and C is the
class label.
The information gain is commonly used as a surro-
gate for approximating a conditional distribution in
the classi(cid:12)cation setting (Cover & Thomas, 1991).
Let the class labels induce a reference partition
S1; : : : ; SC. Let the probability of this partition be
the empirical proportions: P (T ) = jTj=jSj for any
subset T . Now suppose a test on feature Fi induces
a partition of the training set into E1; : : : ; EK. Let
P (ScjEk) = P (Sc\Ek)=P (Ek). We de(cid:12)ne the infor-
mation gain due to this feature with respect to the
reference partition as:

Igain = H(P (S1); : : : ; P (SC))

 KX

P (Ek)H(P (S1jEk); : : : ; P (SCjEk));(2)

k=1

where H is the entropy function. The information
gain provides a simple initial (cid:12)lter with which to
screen features. For example, one can rank all genes
in the order of increasing information gain and select
features conservatively via a statistical signi(cid:12)cance
test (Ben-Dor et al., 2000).
To calculate the information gain, we need to quan-
tize the values of the features. This is achieved in
our approach via the unconditional mixture model
quantization discussed in the previous section.

X

f

G. Markov blanket (cid:12)ltering aims to minimize the
discrepancy between the conditional distributions
P (CjF = f) and P (CjG = fG), as measured by a
conditional entropy:

(cid:1)G =

P (f)D(P (CjF = f) k P (CjG = fG))

P

x

where D(PkQ) =
P (x) log(P (x)=Q(x)) is the
Kullback-Leibler divergence. The goal is to (cid:12)nd a
small feature set G for which (cid:1)G is small.
Intuitively, if a feature Fi is conditionally indepen-
dent of the class label given some small subset of
the other features, then we should be able to omit
Fi without compromising the accuracy of class pre-
diction. Koller and Sahami formalize this idea using
the notion of a Markov blanket.

De(cid:12)nition 1 (Markov Blanket) For a feature
set G and class label C, the set Mi (cid:18) G (Fi =2 Mi)
is a Markov Blanket of Fi (Fi 2 G) if

Fi ? G  Mi  fFig; C j Mi

The following proposition due to Koller and Sahami
establishes the relevance of the Markov blanket con-
cept to the measure (cid:1)G.

Proposition 2 For a complete feature set F, let G
be a subset of F, and G0 = G  Fi. If 9Mi (cid:18) G
(where Mi is a Markov blanket of Fi), then (cid:1)G0 =
(cid:1)G.

The proposition implies that once we (cid:12)nd a Markov
blanket of feature Fi in a feature set G, we can safely
remove Fi from G without increasing the divergence
to the desired distribution. Koller and Sahami fur-
ther prove that in a sequential (cid:12)ltering process in
which unnecessary features are removed one by one,
a feature tagged as unnecessary based on the ex-
istence of a Markov blanket Mi remains unneces-
sary in later stages when more features have been
removed.
In most cases, however, few if any features will have
a Markov blanket of limited size, and we must in-
stead look for features that have an \approximate
Markov blanket." For this purpose we de(cid:12)ne
(cid:1)(FijM) =

P (M = fM; Fi = fi)

X

2.3 Markov Blanket Filtering
Features that pass the information gain (cid:12)lter are in-
put to a more computationally intensive subset se-
lection procedure known as Markov blanket (cid:12)ltering,
a technique due to Koller and Sahami (1996).
Let G be a subset of the overall feature set F. Let
fG denote the projection of f onto the variables in

fM;fi

D(P (CjM = fM ; Fi = fi) kP (CjM = fM )):(3)
If M is a Markov blanket for Fi then (cid:1)(FijM) = 0.
Since an exact zero is unlikely to occur, we relax the
condition and seek a set M such that (cid:1)(FijM) is
small. Note that if M is really a Markov blanket
of Fi, then we have P (CjM; Fi) = P (CjM). This

suggests an easy heuristic way to to search for a
feature with an approximate Markov blanket.
Since the goal is to (cid:12)nd a small non-redundant fea-
ture subset, and those features that form an approx-
imate Markov blanket of feature Di are most likely
to be more strongly correlated to Fi, we construct a
candidate Markov blanket for Fi by collecting the
k features that have the highest correlations (de-
(cid:12)ned by the Pearson correlations between the origi-
nal non-quantized feature vectors) with Fi, where k
is a small integer. We have the following algorithm
as proposed in (Koller & Sahami, 1996):

Initialize
- G = F
Iterate
- For each feature Fi 2 G, let Mi be the set of
k features Fj 2 G  fFig for which the cor-
relations between Fi and Fj are the highest.
- Compute (cid:1)(FijMi) for each i
- Choose the i that minimizes (cid:1)(FijMi), and
de(cid:12)ne G = G  fFig

This heuristic sequential method is far more e(cid:14)-
cient than methods that conduct an extensive com-
binatorial search over subsets of the feature set.
The heuristic method only requires computation of
quantities of the form P (CjM = fM ; Fi = fi) and
P (CjM = fM ), which can be easily computed using
the discretization discussed in Section 2.1.

3. Classi(cid:12)cation Algorithms
We used a Gaussian classi(cid:12)er, a logistic regression
classi(cid:12)er and a nearest neighbor classi(cid:12)er in our
study. In this section we provide a brief description
of these classi(cid:12)ers.

3.1 Gaussian Classi(cid:12)er
A Gaussian classi(cid:12)er is a generative classi(cid:12)cation
model. The model consists of a prior probability
(cid:25)c for each class c, as well as a Gaussian class-
conditional density N ((cid:22)c; (cid:6)c) for class c.1 Maximum
likelihood estimates of the parameters are readily
obtained.
Restricting ourselves to binary classi(cid:12)cation, the
posterior probability associated with a Gaussian
classi(cid:12)er is the logistic function of a quadratic func-
tion of the feature vector, which we denote here by
x:

P (y = 1jx; (cid:18)) =

1

1 + expf 1

2 xT (cid:3)x  (cid:12)T x  g

1Note that when the covariance matrix (cid:6)c is diagonal,
then the features are independent given the class and we
obtain a continuous-valued analog of the popular naive
Bayes classi(cid:12)er.

where (cid:3), (cid:12) and  are functions of the underlying
covariances, means and class priors.
If the classes
have equal covariance then (cid:3) is equal to zero and
the quadratic function reduces to a linear function.

3.2 Logistic Regression
Logistic regression is the discriminative counterpart
of the Gaussian classi(cid:12)er. Here we assume that the
posterior probability is the logistic of a linear func-
tion of the feature vector:
P (y = 1jx; (cid:18)) =

1

;

1 + e(cid:18)T x

where (cid:18) is the parameter to be estimated. Geomet-
rically, this classi(cid:12)er corresponds to a smooth ramp-
like function increasing from zero to one around a
decision hyperplane in the feature space.
Maximum likelihood estimates of the parameter vec-
tor (cid:18) can be found via iterative optimization algo-
rithms. Given our high-dimensional setting, and
given the small number of data points, we found
that stochastic gradient ascent provided an e(cid:11)ective
optimization procedure. The stochastic gradient al-
gorithm takes the following simple form:

(cid:18)(t+1) = (cid:18)(t) + (cid:26)(yn  (cid:22)(t)

n )xn;

where (cid:18)(t) is the parameter vector at the tth itera-
(t)T xn), and where (cid:26) is
tion, where (cid:22)(t)
a step size (chosen empirically in our experiments).

n (cid:17) 1=(1 + e(cid:18)

3.3 K Nearest Neighbor Classi(cid:12)cation
We also used a simple K nearest neighbor classi-
(cid:12)cation algorithm, setting K equal to three. The
distance metric that we used was the Pearson corre-
lation coe(cid:14)cient.

4. Regularization Methods
Regularization methods provide a popular strategy
to cope with over(cid:12)tting problems (Bishop, 1995).
Let l((cid:18) j D) represent the log likelihood associated
with a probabilistic model for a data set D. Rather
than simply maximizing the log likelihood, we con-
sider a \penalized likelihood," and de(cid:12)ne the follow-
ing \regularized estimate" of the parameters:

^(cid:18) = arg max

(cid:18)

fl((cid:18) j D)  (cid:21)k(cid:18)kg ;

where k(cid:18)k is an appropriate norm, typically the L1
or the L2 norm, and where (cid:21) is a free parameter
known as the \regularization parameter." The basic
idea is that the penalty term often leads to a sig-
ni(cid:12)cant decrease in the variance of the estimate, at
the expense of a slight bias, yielding an overall de-
crease in risk. One can also take a Bayesian point of

Predictive power of the genes

information gain for each genes with respect to the given partition

1

0.8

0.6

0.4

0.2

r
o
r
r
e

s
e
y
a
b

0

0

1000

2000

3000

4000
gene

(a)

5000

6000

7000

8000

0.7

0.6

0.5

0.4

0.3

0.2

0.1

i

n
a
g

n
o
i
t
a
m
r
o
f
n

i

0

0

KL of each removal gene w.r.t. to its MB

0.2

0.15

0.1

0.05

e
c
n
e
g
r
e
v
D

L

K

i

1000

2000

3000

4000
gene

(b)

5000

6000

7000

8000

0

0

50

100

150

200
gene

250

300

350

400

(c)

Figure 2. Feature selection using using a 3-stage procedure. (a) Genes ranked by (cid:15) (Eq. 1); (b) Genes ranked by Igain
(Eq. 2); (c) Genes ranked by (cid:1)(FijM) (Eq. 3).

view and interpret the penalty term as a log prior,
in which case regularization can be viewed as a max-
imum a posteriori estimation method.
The regularization parameter (cid:21) is generally set via
some form of cross validation.
An L2 penalty is a rotation-invariant penalty, and
shrinks the parameters along a ray toward the ori-
gin. Using an L1 penalty, on the other hand, shrinks
the parameters toward the L1 ball, which is not rota-
tion invariant. Some of the parameters shrink more
quickly than others, and indeed parameters can be
set to zero in the L1 case (Tibshirani, 1995). Thus
an L1 penalty has something of the avor of a fea-
ture selection method.
Parameter estimation is straightforward in the regu-
larization setting, with the penalty term simply con-
tributing an additive term to the gradient. For ex-
ample, in the case of logistic regression with an L2
penalty, we obtain the following stochastic gradient:

(cid:16)
(yn  (cid:22)(t)

n )xn  (cid:21)(cid:18)(t)

(cid:18)(t+1) = (cid:18)(t) + (cid:26)

(cid:17)

;

where the shrinkage toward the origin is apparent.
In the case of Gaussian classi(cid:12)er, the regularized
ML estimate of (cid:18) can be easily solved in closed form.

5. Experiments and Results
In this section, we report the results of analysis
of the data from a microarray classi(cid:12)cation prob-
lem. Our data is a collection of 72 samples from
leukemia patients, with each sample giving the ex-
pression levels of 7130 genes (Golub et al., 1999).
According to pathological/histological criteria, these
samples include 47 type I Leukemias (called ALL)
and 25 type II Leukemias (called AML). The sam-
ples are split into two sets by the provider, with 38
(ALL/AML=27/11) serving as a training set and the
remaining 34 (20/14) as a test set. The goal is to
learn a binary classi(cid:12)er (for the two cancer subtypes)
based on the gene expression patterns.

5.1 Filtering Results
Figure 2(a) shows the mixture overlap probability (cid:15)
(de(cid:12)ned by Eq. 1) for each single gene in ascending
order. It can be seen that only a small percentage of

the genes have an overlap probability signi(cid:12)cantly
smaller than (cid:15) (cid:28) 0:5, where 0:5 would constitute
random guessing under a Gaussian model if the un-
derlying mixture components were construed as class
labels.
In Figure 2(b) we present the information gain that
can be provided by each individual gene with re-
spect to the reference partition (the Leukemia class
labels), compared to the partition obtained from the
mixture models. Only a very small fraction of the
genes induce a signi(cid:12)cant information gain. We take
the top 360 genes from this list and proceed with
(approximate) Markov blanket (cid:12)ltering.
Figure 2(c) displays the values of (cid:1)(FijMi) (cf.
Eq. 3) for each Fi, an assessment of the extent
to which the approximate Markov blanket Mi sub-
sumes information carried by Fi and thus renders
Fi redundant. Genes are ordered in their removal
sequence from right to left. Note that the re-
dundancy measure (cid:1)(FijMi) increases until there
are fewer than 40 genes remaining. At this point
(cid:1)(FijMi) decreases, presumably because of a com-
positional change in the approximate Markov blan-
kets of these genes compared to the original contents
before many genes were removed. The increasing
trend of (cid:1)(FijMi) then resumes.
The fact that in a real biological regulatory network
the fan-in and fan-out will generally be small pro-
vides some justi(cid:12)cation for enforcing small Markov
blankets. In any case, we have to keep the Markov
blankets small to avoid fragmenting our small data
set.

5.2 Classi(cid:12)cation Results
Figure 3 shows training set and test set errors for
each of the three di(cid:11)erent classi(cid:12)ers. For each
feature subset cardinality (the abscissa in these
graphs), we chose a feature subset using Markov
blanket (cid:12)ltering. This is a classi(cid:12)er-independent
method, thus the feature subsets are the same in
all three (cid:12)gures.
The (cid:12)gures show that for all classi(cid:12)ers, after an
initial coevolving trend of the training and testing
curves for low-dimensional feature spaces (the di-
mensionality di(cid:11)ers for the di(cid:11)erent classi(cid:12)ers), the

kNN (k=3)

Gaussian generative model

logistic regression

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

training error
testing error

20

40

60

80

100

number of features

(a)

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

training error
testing error

20

40

60

80

100

number of features

(b)

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

training error
testing error

20

40

60

80

100

number of features

(c)

Figure 3. Classi(cid:12)cation in a sequence of di(cid:11)erent feature spaces with increasing dimensionality due to inclusion of
gradually less quali(cid:12)ed features. (a) Classi(cid:12)cation using kNN classi(cid:12)er; (b) Classi(cid:12)cation using a quadratic Bayesian
classi(cid:12)er given by a Gaussian generative model; (c) A linear classi(cid:12)er obtained from logistic regression. All three
classi(cid:12)ers use the same 2-100 genes selected by the three stages of feature selection.

classi(cid:12)ers quickly over(cid:12)t the training data. For the
logistic linear classi(cid:12)er and kNN, the test error tops
out at approximately 20 percent when the entire fea-
ture set of 7130 genes is used. The generative Gaus-
sian quadratic classi(cid:12)er over(cid:12)ts less severely in the
full feature space. For all three classi(cid:12)ers, the best
performance is achieved only in a signi(cid:12)cantly lower
dimensional feature space. Of the three classi(cid:12)ers,
kNN requires the most features to achieve its best
performance.
Figure 3 shows that by an optimal choice of the num-
ber of features it is possible to achieve error rates of
2.9%, 0%, and 0% for the Gaussian classi(cid:12)er, the
logistic regression classi(cid:12)er, and kNN, respectively.
Of course, in actual diagnostic practice we do not
have the test set available, so these numbers are op-
timistic. To choose the number of features in an
automatic way, we make use of leave-one-out cross
validation on the training data. That is, for each
cardinality of feature subset, given the feature sub-
set chosen by our (cid:12)ltering method, we choose among
cardinalities by cross validation. Thus we have in
essence a hybrid of a (cid:12)lter method and a wrapper
method|the (cid:12)lter method is used to choose feature
subsets, and the wrapper method is used to compare
between best subsets for di(cid:11)erent cardinalities.
The results of leave-one-out cross validation are
shown in Figure 4. Note that we have several minima
for each of the cross-validation curves. Breaking ties
by choosing the minima having the smallest cardi-
nality, and running the resulting classi(cid:12)er on the test
set, we obtain error rates of 8.8%, 0%, and 5.9% for
the Gaussian classi(cid:12)er, the logistic regression classi-
(cid:12)er, and kNN, respectively.
We also compared prediction performance when us-
ing the unconditional mixture modeling (MM) (cid:12)lter
alone and the information gain (IG) (cid:12)lter alone (in
the latter case, using the discretization provided by

Table 1. Performance of classi(cid:12)cation based on randomly
selected features (200 trials)

training error (%)

test error (%)

classi(cid:12)er Max Min Average Max Min Average

kNN 50.0 7.9
28.9 5.3
31.6 2.6

Gaussian
Logistic

27.1
14.2
17.4

35.6
50.0 8.8
64.7 14.7 35.6
50.0 20.6 35.6

the (cid:12)rst phase of mixture modeling). The results for
the logistic regression classi(cid:12)er are shown in Figure
5. As can be seen, the number of features deter-
mined by cross-validation using the MM (cid:12)lter is 20
(compared to 8 using the full Markov blanket (cid:12)l-
tering) and the resulting classi(cid:12)er also has a higher
test set error (5.9% versus 0%). For the IG (cid:12)lter,
the selected number of features is 59, and the test
set error rate is signi(cid:12)cantly higher (13.5%). The
latter result in particular suggests that it is not suf-
(cid:12)cient to simply performance a \relevance check"
to select features, but rather that a redundancy re-
duction method such as the Markov blanket (cid:12)lter
appears to be required. Note also that using the
MM (cid:12)lter alone results in better performance than
using the IG (cid:12)lter alone. While neither approach
performs as well as Markov Blanket (cid:12)ltering, the
MM (cid:12)lter has the advantage that it does not require
class labels. This opens up the possibility of doing
feature selection on this data set in the context of
unsupervised clustering (see Xing & Karp, 2001).
In some high-dimensional problems, it may be pos-
sible to bypass feature selection algorithms and ob-
tain reasonable classi(cid:12)cation performance by choos-
ing random subsets of features. That this is not the
case in the Leukemia data set is shown by the re-
sults (Table 1). In the experiments reported in this
table, we chose ten randomly selected features for
each classi(cid:12)er. The performance is poorer than in
the case of explicit feature selection.

kNN

crossvalidation error

20

40

60

80

100

number of features

(a)

Gaussian generative model

logistic regression

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

crossvalidation error

20

40

60

80

100

number of features

(b)

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

crossvalidation error

20

40

60

80

100

number of features

(c)

Figure 4. Plots of leave-one-out cross validation error for the three classi(cid:12)ers.

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

LR (with only MM filter)

LR (with only IG filter)

crossvalidation error

20

40

60

80

100

number of features

(a)

0.5

0.4

0.3

0.2

0.1

e

t

a
r

r
o
r
r
e

0

0

crossvalidation error

20

40

60

80

100

number of features

(b)

Figure 5. Plots of leave-one-out cross validation error for
the logistic regression classi(cid:12)er with (a) only the uncon-
ditional mixture modeling (cid:12)lter and (b) only the infor-
mation gain (cid:12)lter.

5.3 Regularization Versus Feature Selection

0.25

0.2

e
t
a
r

r
o
r
r
e

0.15

0.1

0.05

0
106

L1 for Gaussian generative model

training error
testing error

104

105
lambda

(a)

t

e
a
r

r
o
r
r
e

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

L2 for Gaussian generative model

training error
testing error

1010

108

lambda

(b)

Figure 6. Training set and test set error as a function of
the regularization parameter (cid:21). The results are for the
Gaussian classi(cid:12)er using the (a) L1 and (b) L2 penalties.
Figure 6 shows the results for the regularized Gaus-
sian classi(cid:12)er using the L1 and L2 penalties. Similar
results were found for the logistic regression classi-
(cid:12)er.
By choosing an optimal value of (cid:21) based (optimisti-
cally) on the test set, we obtain test set errors of
8.8% and 8.8% for the Gaussian classi(cid:12)er for the L1
and L2 norm respectively. For the logistic regres-
sion classi(cid:12)er, we obtain test set errors of 17.6% and
20.1%.

These errors are higher than those obtained with ex-
plicit feature selection. Indeed, a comparison of Fig-
ures 3 and 6, which show the range of test set perfor-
mance achievable from the feature selection and the
regularization approaches, respectively, show that
the feature selection curves are generally associated
with smaller error. Given that the regularization ap-
proach can, in the worst case, leave us with all 7130
features, we feel that feature selection provides the
better alternative for our problem.

6. Discussion and Conclusion
We have shown that feature selection methods can
be applied successfully to a classi(cid:12)cation problem in-
volving only 38 training data points in a 7130 dimen-
sional space. This problem exempli(cid:12)es a situation
that will be increasingly common in applications of
machine learning to molecular biology. Microarray
technology makes it possible to put the probes for
the genes of an entire genome onto a chip, such that
each data point provided by an experimenter lies in
the high-dimensional space de(cid:12)ned by the size of the
genome under investigation.
In high-dimensional problems such as these, feature
selection methods are essential if the investigator is
to make sense of his or her data, particularly if the
goal of the study is to identify genes whose expres-
sion patterns have meaningful biological relation-
ships to the classi(cid:12)cation problem. Computational
reasons can also impose important constraints. Fi-
nally, as demonstrated in smaller problems in the ex-
tant literature on feature selection (Kohavi & John,
1997; Langley, 1994), and as we have seen in the
high-dimensional problem studied here, feature se-
lection can lead to improved classi(cid:12)cation. All of the
classi(cid:12)ers that we studied|a generative Gaussian
classi(cid:12)er, a discriminative logistic regression classi-
(cid:12)er, and a k-NN classi(cid:12)er, performed signi(cid:12)cantly
better in the reduced feature space than in the full
feature space.
We have not attempted to discuss the biological sig-

cancer: Class discovery and class prediction by
gene expression monitoring. Science, 286, 531{
537.

Kohavi, R., & John, G. (1997). Wrapper for feature
subset selection. Arti(cid:12)cial Intelligence, 97, 273{
324.

Koller, D., & Sahami, M. (1996). Toward optimal
feature selection. Proceedings of the Thirteenth
International Conference on Machine Learning.

Langley, P. (1994). Selection of relevant features in
machine learning. Proceedings of the AAAI Fall
Symposium on Relevance. AAAI Press.

Ng, A. (1998). On feature selection: Learning with
exponentially many irrelevant features as training
examples. Proceedings of the Fifteenth Interna-
tional Conference on Machine Learning.

Shalon, D., Smith, S. J., & Brown, P. O. (1996).
A DNA microarray system for analyzing complex
DNA samples using two-color uorescent probe
hybridization. Genome Research, 6(7), 639{45.

Tibshirani, R. (1995). Regression selection and
shrinkage via the lasso. Journal of the Royal Sta-
tistical Society B, 1, 267{288.

Xing, E. P., & Karp, R. M. (2001). Cli(cid:11): Clustering
of high-dimensional microarray data via iterative
feature (cid:12)ltering using normalized cuts. Proceed-
ings of the Nineteenth International Conference
on Intelligent Systems for Molecular Biology.

ni(cid:12)cance of the speci(cid:12)c features that our algorithm
identi(cid:12)ed, but it is worth noting that seven out of
the (cid:12)fteen best features identi(cid:12)ed by our algorithm
are included in the set of 50 informative features
used in (Golub et al., 1999), and moreover there
is a similar degree of overlap with another recent
study on this data set (Chow et al., in press). The
fact that the overlap is less than perfect is likely
due to the redundancy of the features in this data
set. Note in particular that our algorithm works ex-
plicitly to eliminate redundant features, whereas the
Golub and Chow methods do not.
We have compared feature selection to regulariza-
tion methods, which leave the feature set intact, but
shrink the numerical values of the parameters to-
ward zero. Our results show that explicit feature
selection yields classi(cid:12)ers that perform better than
regularization methods. Given the other advantages
associated with feature selection, including compu-
tational and interpretational, we feel that feature
selection provides the preferred alternative on these
data.
It is worth noting, however, that these ap-
proaches are not mutually exclusive and it may be
worthwhile to consider combinations.

Acknowledgements
We thank Andrew Ng for helpful comments. This
work is partially supported by ONR MURI N00014-
00-1-0637 and NSF grant IIS-9988642.

