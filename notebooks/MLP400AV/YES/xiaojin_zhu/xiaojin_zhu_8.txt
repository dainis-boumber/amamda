Abstract

Active and semi-supervised learning are impor-
tant techniques when labeled data are scarce.
We combine the two under a Gaussian random
eld model. Labeled and unlabeled data are rep-
resented as vertices in a weighted graph, with
edge weights encoding the similarity between in-
stances. The semi-supervised learning problem
is then formulated in terms of a Gaussian random
eld on this graph, the mean of which is char-
acterized in terms of harmonic functions. Ac-
tive learning is performed on top of the semi-
supervised learning scheme by greedily select-
ing queries from the unlabeled data to minimize
the estimated expected classication error (risk);
in the case of Gaussian elds the risk is ef-
ciently computed using matrix methods. We
present experimental results on synthetic data,
handwritten digit recognition, and text classica-
tion tasks. The active learning scheme requires a
much smaller number of queries to achieve high
accuracy compared with random query selection.

1. Introduction

Semi-supervised learning targets the common situation
where labeled data are scarce but unlabeled data are abun-
dant. Under suitable assumptions, it uses unlabeled data
to help supervised learning tasks. Various semi-supervised
learning methods have been proposed and show promising
results; Seeger (2001) gives a survey. These methods typi-
cally assume that the labeled data set is given and xed.

In practice, it may make sense to utilize active learning
in conjunction with semi-supervised learning. That is, we
might allow the learning algorithm to pick a set of unla-
beled instances to be labeled by a domain expert, which

will then be used as (or to augment) the labeled data set. In
other words, if we have to label a few instances for semi-
supervised learning, it may be attractive to let the learning
algorithm tell us which instances to label, rather than se-
lecting them randomly. We will limit the range of query se-
lection to the unlabeled data set, a practice known as pool-
based active learning or selective sampling.

There has been a great deal of research in active learning.
For example, Tong and Koller (2000) select queries to min-
imize the version space size for support vector machines;
Cohn et al. (1996) minimize the variance component of the
estimated generalization error; Freund et al. (1997) employ
a committee of classiers, and query a point whenever the
committee members disagree. Most of the active learning
methods do not take further advantage of the large amount
of unlabeled data once the queries are selected. Exceptions
include McCallum and Nigam (1998) who use EM with un-
labeled data integrated into the active learning, and Muslea
et al. (2002) who use a semi-supervised learning method
during training. In addition to this body of work from the
machine learning community, there is a large literature on
the closely related topic of experimental design in statistics;
Chaloner and Verdinelli (1995) give a survey of experimen-
tal design from a Bayesian perspective.

Recently Zhu et al. (2003) introduced a semi-supervised
learning framework which is based on Gaussian random
elds and harmonic functions.
In this paper we demon-
strate how this framework allows a combination of active
learning and semi-supervised learning. In brief, the frame-
work allows one to efciently estimate the expected gener-
alization error after querying a point, which leads to a bet-
ter query selection criterion than naively selecting the point
with maximum label ambiguity. Then, once the queries are
selected and added to the labeled data set, the classier can
be trained using both the labeled and remaining unlabeled
data. We present results on synthetic data, text classica-

Proceedings of the ICML-2003 Workshop on The Continuum from Labeled to Unlabeled Data, Washington DC, 2003.


tion and image classication tasks that indicate the com-
bination of these techniques can result in highly effective
learning schemes.

2. Gaussian random elds and harmonic

energy minimizing functions

(1)

can be the

radial basis function (RBF):

are labeled with the corresponding

We begin by briey describing the semi-supervised learn-
la-

to de-
the
total number of points. We assume the labels are binary:

The semi-supervised algorithm in this paper is based on
a relaxation of the requirement that labels are binary, re-
sulting in a simple and tractable family of algorithms. We
.
The labels on labeled data are still constrained to be 0 or 1:

so that nearby points in Euclidean space are assigned large
edge weights. Other weightings are possible, of course,
is discrete or sym-
fully species the
data manifold structure. We note that a method for learn-
is proposed in (Zhu et al., 2003).

ing framework of Zhu et al. (2003). There are
beled points	

	 , and unlabeled points
 ; usually
. We will use
,!
note the labeled and unlabeled set, and"$#%'&(
*),+.-0/1243 . We assume a connected graph56#687'	9:
is given with nodes7
corresponding to the" data points,
of which the nodes in
are represented by an"<;=" weight matrix
 s. The edges9
,>
> which is given. For example if?+A@CB
NPO
TVU
RW
DFEHG

#JIKML
QR
and may be more appropriate when
bolic. For our purposes the matrix>
ing the scale parameterQ
allow continuous labels on unlabeled nodesMXZY[7]\^@
Uhi
_`C#a4)bc_d+A-e/243 for_P#(24f . We also denote the
constraint byg
EmG
9jc1C#
_`
El
over the graph. Dene the diagonal matrixq6#sr*_`t*u[8r
DFEmG are the weighted degrees of
whose entriesr
#wv
. The combinatorial Laplacian is the"x;y"
the nodes in5
matrixz{#aq
matrix form as9|c1#(}z:
OF
P
c1C#
I
KML<
where
is an inverse temperature parameter, and
h
is the partition function
9jM	r4
#s

. Since we want unlabeled points that
are nearby in the graph to have similar labels, we dene the
energy to be

9jc1f
Uh
IKpL<

. We can rewrite the energy function in
. We consider the Gaussian

so that low energy corresponds to a slowly varying function

onp

random eld

OF

(2)

.

.

z|
0
Uh

is also the mean of the eld.

corresponding to unlabeled data.

into blocks for labeled and unlabeled nodes,

is connected and labeled nodes from both classes

puted with matrix methods. We partition the Laplacian ma-

is the mode
of the Gaussian random eld, but since the joint distribu-

The Gaussian random eld differs from the standard dis-
crete Markov random eld in that the eld congurations
are over a continuous state space. Moreover, for a Gaus-
sian eld the joint probability distribution over unlabeled
is

. The harmonic property means that the
value at each unlabeled node is the average of neighboring
nodes:

which is consistent with our prior notion of smoothness
with respect to the graph. Because of the maximum prin-
is

nodes is Gaussian with covariance matrix
z0
arg minh
the submatrix ofz
The minimum energy function#
9|c1 of
the Gaussian eld is harmonic; that is, it satiseszA#/
on unlabeled data points!
, and is equal to
) on the la-
beled data points
DEmG
bonpC#
b_`
 forn#Z&a24f&?
EG
ciple of harmonic functions (Doyle & Snell, 1984),
unique. Furthermore,
satises/(bonp2
forn+!
when!
are present at the boundary (the usual case; otherwise
takes on the extremum 0 or 1). By denition
tion is Gaussian,
The harmonic energy minimizing function can be com-
trixz
zm
z
z#
e
0
0
, and denotes the
 where#
and let,#

e
0
labeled data, is a multivariate normal:*yafz
.
0
that the harmonic energy minimizing function
is to label node_ as class 1 in casebc_dJ/1H , and to label
node_ class 0 otherwise.
The harmonic function has many nice interpretations, of
Dene the transition matrix#q
random walk on graph5
EHG
unlabeled node_ , it moves to a noden with probability
a labeled node. Then
starting from node_ , reaches a labeled node with label 1.

which the random walk view is particularly relevant here.
. Consider the
by a particle. Starting from an

To carry out classication with a Gaussian eld, we note
is the
mean of the eld. Therefore the Bayes classication rule

after one step. The walk continues until the particle reaches
is the probability that the particle,

mean values on the unlabeled data points. The solution is
given by

It is not hard to show that the Gaussian eld, conditioned on

(3)

2
B
S

E
T
O

G
T

)
2
k
S
G
D
O
R
E

E
G
O
>
~
2

i
i


i
i
2
r
G
S
z
z

)


#
O
z


z






>
E
The labeled data are the absorbing boundary for the random
walk. More on this semi-supervised learning framework
can be found in (Zhu et al., 2003).

3. Active learning



to be

is the unknown true

We dene the true risk

We propose to perform active learning with the Gaussian
random eld model by greedily selecting queries from the
unlabeled data to minimize the risk of the harmonic energy
minimization function. The risk is the estimated general-
ization error of the Bayes classier, and can be efciently
computed with matrix methods.

mean of the Gaussian eld model:

tion we can compute the estimated risk

is the Bayes decision rule, where (with a

is not computable. In order to proceed, it is
of this
necessary to make assumptions. We begin by assuming that

is the probability of reaching 1 in
a random walk on the graph, our assumption is that we
can approximate the distribution using a biased coin at each

 of the Bayes classier based
on the harmonic function
E

 sgn

#,/ otherwise. Here~
where sgn8
#
^/
 and
slight abuse of notation) sgn
if

sgn
label distribution at node_ , given the labeled data. Because
we can estimate the unknown distribution~
8
d with the
c
2pg
c
d
Z
Intuitively, recalling
E . With this assump-
node, whose probability of heads is
8 as

&
2
 sgn
8
 sgn8
#(2

2

, we will receive an answer

harmonic function by




8
Since we do not know what answer
we again assume the answer is approximated with

. The expected estimated risk after querying
$


 
8
2


If we perform active learning and query an unlabeled node
(0 or 1). Adding this point
to the training set and retraining, the Gaussian eld and its
mean function will of course change. We denote the new
 . The estimated risk will

2
!#

!4"

we will receive,

also change:

is therefore

node 

2
g

8




(4)




While more labeled data required:

Find best query  using (5)

, weight matrix>
Input: =V!
Compute harmonic using (3)
Query point
, receive answer
Addc

to
, remove
from!
and classier
Output:

Figure1. The active learning algorithm

end

.

:

%

(5)

arg min



The active learning criterion we use in this paper is the
that min-
greedy procedure of choosing the next query 
imizes the expected estimated risk:

standard result (a derivation is given in Appendix A) gives

for node  ?
This is the same as nding the conditional distribution of
. Noting that

, a multivariate normal distribution, a

To carry out this procedure, we need to compute the har-
to the cur-
rent labeled training set. This is the retraining problem and
is computationally intensive in general. However for Gaus-
sian elds and harmonic functions, there is an efcient way
to retrain. Recall that the harmonic function solution is

 
8
monic function
 after addingc&1
e
0
What is the solution if we x the value
all unlabeled nodes, given the value of'
*,$fz
0
the conditional once we x'
8z

&Zc
8z
where8z
0
on unlabeled data, andz
0
we compute the harmonic function
. As a nal word on computational efciency, we
"
note that after adding query
and its answer to
 ,
the next iteration we will need to compute	8z
.-
0
row/column for
 ; a derivation is given in Appendix B.
z
eM

is the  -th column of the inverse Laplacian
is the  -th diagonal ele-
ment of the same matrix. Both are already computed when
. This is a linear com-

the inverse of the Laplacian on unlabeled data, with the
removed. Instead of naively taking the
inverse, there are efcient algorithms to compute it from

To summarize, the active learning algorithm is shown in
Figure 1. The time complexity to nd the best query is

putation and therefore can be carried out efciently.

Figure 2 shows (top left) a synthetic dataset with two la-
beled data (marked 1, 0), an unlabeled point a in the

4. Experiments

)(
$*

0
0

, in

+



#

S
E
U

S
h

U

l

E
#

~

E
g


E

E
2
E
E


E
g




E
g
~

E
#
E
E



#

S
E
U

E
#
/

O

E
E


E
#

S
E
U

E
O

E


l
h



l
h



#

S
E
U


l
h


E
O


l
h


E


~

#





#
O


l



&


l








#



%


l
h



#
O
z


z







l
h



#



O











(




,
R






2.5

2

1.5

1

0.5

0

0.5

1
1.5

100

80

60

40

20

0

k
s
R

i

a

1

0

B

1

0.5

0

0.5

1

1.5

Active Learning
Random Query
Most Uncertain Query

5

10

Labeled set size

15

20

35

30

25

20

15

10

5

0

0

y
c
a
r
u
c
c
A

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

25

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

35

30

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1
11

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

5

10

15

20

Active Learning
Random Query
Most Uncertain Query

5

10

Labeled set size

15

20

Figure2.Synthetic data experiments. Top left: synthetic data 1; top right: synthetic data 2. Bottom left: risk on synthetic data 2; bottom
right: classication accuracy on synthetic data 2. Standard errors are shown as dotted lines.



, and

. Points in

center above and a cluster of 9 unlabeled points B be-
low. B is slighted shifted to the right. The graph is fully
is the

EmG
connected with weightD=EmG
EmG
, wherer
#I
KML
n . In this conguration, we
Euclidean distance between_V
/
have the most uncertainty in a:btM#
B have around 0.32, so we are more certain about the
2 . This shows that the
w24
+

8tM|#

labels of B than a. However, the risk minimization cri-
terion picks the upper center point (marked with a star) in
B to query, instead of a. In fact the estimated risk is

active learning algorithm is not simply picking the most
uncertain point for query, but rather it thinks globally.

Figure 2 also shows (top right) another synthetic dataset,
where the true labels for the 400 points form a chess-board
pattern. We expect active learning to discover the pattern
and query a small number of representatives from each
cluster. On the other hand, we expect a much larger num-
ber of queries if queries are randomly selected. We use

We perform 20 random trials. At the beginning of each
trial we randomly select a positive example and a negative
example as the initial training set. We then run active learn-
ing and compare it to two baselines: (1) Random Query:
; (2) Most Un-
,

a fully connected graph with weightDEmG
EmG	

.
#{IKpL
randomly selecting the next query from!
certain Query: selecting the most uncertain instance in!
i.e. the one with closest to 0.5. In each case we run for 20

iterations (queries). At each iteration we plot the estimated

(lower right). The error bars are


2

risk (4) of the selected query (lower left), and the classi-

cation accuracy on!

standard deviation, averaged over the random trials. As ex-
pected, with active learning we reduce risk more quickly
than random queries or the most uncertain queries. In fact,
active learning with about 15 queries (plus 2 initial random
points) learns the correct concept, which is nearly optimal
given that there are 16 clusters. Looking at the queries, we
nd that active learning mostly selects the central points of
the clusters.

Next we report the results of document categorization ex-
periments using the 20 newsgroups dataset1. We evaluate
the following binary classication tasks: rec.sport.baseball
(994 documents) vs. rec.sport.hockey (999); comp.sys.-
ibm.pc.hardware (982) vs. comp.sys.mac.hardware (961);
talk.religion.misc (628) vs. alt.atheism (799). They repre-
sent easy, moderate and hard problems respectively. Each
document is minimally processed into a tf.idf vector,
without applying header removal, frequency cutoff, stem-
are connected
is

ming, or a stopword list. Two documentsP
by an edge if
 . We use the following weight
among s 10 nearest neighbors, as measured by their co-
/*
/1

e
2
J#I
KML<

sine similarity
function on the edges:
As before, we perform 20 trials, and randomly pick two

1http://www.ai.mit.edu/people/jrennie/20Newsgroups/

s 10 nearest neighbors or if

is among



H





O
r
R


k




O
r
R


#

}




D
O
O

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

Active Learning
Random Query

10

20
Labeled set size

30

40

50

Active Learning
Random Query
SVM
40

50

10

20
Labeled set size

30

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

Active Learning
Random Query

10

20
Labeled set size

30

40

50

Active Learning
Random Query
SVM
40

50

10

20
Labeled set size

30

10

20
Labeled set size

30

Active Learning
Random Query

40

50

Active Learning
Random Query
SVM
40

50

10

20
Labeled set size

30

Figure3.Risk (top) and classication accuracy (bottom) on 20 newsgroups, compared to random queries; Baseball vs. Hockey (left),
PC vs. MAC (center), and Religion vs. Atheism (right).

initial training examples to start each trial. The rest of the
documents are treated as unlabeled data. In each trial we
answer 50 queries. Again we compare active learning with
two baselines:

1. Figure 3 compares it with random queries. Active
learning reduces risk faster, and achieves high clas-
sication accuracy more rapidly than random queries.
For easy datasets only a handful of queries are neces-
sary for active learning to achieve 90% accuracy. We
observe that active learning tends to query the same
small set of documents in different trials. We also
trained a SVM classier on the random queries, with
the cosine similarity kernel and
(which was the
best setting among several different kernels and a wide
range of
values). Note that the SVM does not utilize
unlabeled data; on these datasets the semi-supervised
method outperforms the SVM.

#,

2. Figure 4 compares it with the most uncertain queries.

Most Uncertain Query selects the instance whose

value is closest to 0.5 as the next query. SVM Most
Uncertain selects the instance whose margin is clos-
est to 0 (closest to the decision boundary). As before
the SVM does not utilize unlabeled data. The har-
monic function classication is worse with the most
uncertain queries than with random queries, while the
SVM improves with the most uncertain queries.

In all cases our proposed active learning scheme clearly
outperforms both baselines for the 20 newsgroups datasets.

We then evaluate active learning on a handwritten digits
dataset, originally from the Cedar Buffalo binary digits
database (Hull, 1994). The digits were preprocessed to re-
grid by
down-sampling and Gaussian smoothing, with pixel val-
ues ranging from 0 to 255 (Le Cun et al., 1990). They are
further scaled down to
Each image is thus represented by a 64-dimensional vector.
We consider the binary problem of classifying digits 1
vs. 2, with 1100 images in each class. We create a graph

duce the size of each image down to a2
;,2
by averagingk
k pixel bins.
;
on the 2200 images, with an edge between images_Vn
iff_
is inn s 10 nearest neighbors (in Euclidean distance) or vice
R	
versa. The weights on edges areD
EmG
EmG
4/
#{IKpL
EmG
wherer
ages_Vn . In each of 20 trials we randomly pick one exam-

ple from each class to form the initial training set. We then
compare active learning with random queries and the most
uncertain queries for 50 iterations. Figure 5 (left) shows
the risk. With active learning the risk decreases faster than
with the baselines. Figure 5 (center) shows the classica-
tion accuracy where active learning is seen to outperform
the baselines. Again only a handful of examples are needed
for active learning to reach high accuracy. The SVM uses

is the pixel-wise Euclidean distance between im-

among polynomial kernels with order 1 to 5, and a wide
range of
values. We also observe that there are certain

kernel2

&a

R and

2 , which is one of the best



;
O
r
R

}
E

G


#

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

600

500

400

k
s
R

i

300

200

100

0

1

0.9

0.8

0.7

0.6

0.5

0.4

y
c
a
r
u
c
c
A

Active Learning
Most Uncertain Query

10

20
Labeled set size

30

40

50

Active Learning
Most Uncertain Query
SVM Most Uncertain

10

20
Labeled set size

30

40

50

Active Learning
Most Uncertain Query

10

20
Labeled set size

30

40

50

Active Learning
Most Uncertain Query
SVM Most Uncertain

10

20
Labeled set size

30

40

50

Active Learning
Most Uncertain Query

10

20
Labeled set size

30

40

50

Active Learning
Most Uncertain Query
SVM Most Uncertain

10

20
Labeled set size

30

40

50

Figure4.Risk (top) and classication accuracy (bottom) on 20 newsgroups, compared to the most uncertain queries; Baseball vs. Hockey
(left), PC vs. MAC (center), and Religion vs. Atheism (right).

images that active learning frequently queries in different
trials. Figure 5 (right) shows some of the most frequently
queried images; we believe these images are representative
of the variations in the dataset.

We also evaluate on the difcult binary problem of classify-
ing odd digits vs. even digits. That is, we group 1,3,5,7,9
and 2,4,6,8,0 into two classes. There are 400 images per
digit (2000 per class). It is a difcult dataset because the
target concept is rather articial; on the other hand this
dataset resembles synthetic data 2 (Figure 2) where each
class has several internal clusters. The experimental setup
is the same as above except that we run for 100 iterations.
Figure 6 shows the results. Again active learning is supe-
rior than the baselines. We also see that odd vs. even is a
harder concept which takes active learning about 50 queries
to approximately learn. The digits shown are the 25 most
frequently queried instances in the rst 30 iterations across
all trials. With 4000 instances this dataset is also the largest
and slowest. With a naive Matlab implementation, the cal-
culations take roughly 50 seconds per iteration on a 1GHz
Linux machine. In contrast, the 1 vs.
2 dataset re-
quires ve seconds per iteration, while all of the 20 news-
groups datasets take less than two seconds per iteration.

We nally note that if we train an SVM on the active
queries chosen from the harmonic function risk minimiza-
tion procedure, the accuracy is always worse than our pro-
posed active learning method, and often even worse than

the SVM on random queries.

5. Summary

We have proposed an approach to active learning which is
tightly coupled with semi-supervised learning using Gaus-
sian elds and harmonic functions. The algorithm selects
queries to minimize an approximation to the expected gen-
eralization error. Experiments on text categorization and
handwritten digit recognition indicate that the active learn-
ing algorithm can be highly effective.

