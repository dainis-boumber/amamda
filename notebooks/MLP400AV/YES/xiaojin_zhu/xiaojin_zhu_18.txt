Abstract

We pose transductive classication as a matrix completion problem. By assuming
the underlying matrix has a low rank, our formulation is able to handle three prob-
lems simultaneously: i) multi-label learning, where each item has more than one
label, ii) transduction, where most of these labels are unspecied, and iii) miss-
ing data, where a large number of features are missing. We obtained satisfactory
results on several real-world tasks, suggesting that the low rank assumption may
not be as restrictive as it seems. Our method allows for different loss functions to
apply on the feature and label entries of the matrix. The resulting nuclear norm
minimization problem is solved with a modied xed-point continuation method
that is guaranteed to nd the global optimum.

1

Introduction

Semi-supervised learning methods make assumptions about how unlabeled data can help in the
learning process, such as the manifold assumption (data lies on a low-dimensional manifold) and
the cluster assumption (classes are separated by low density regions) [4, 16].
In this work, we
present two transductive learning methods under the novel assumption that the feature-by-item and
label-by-item matrices are jointly low rank. This assumption effectively couples different label pre-
diction tasks, allowing us to implicitly use observed labels in one task to recover unobserved labels
in others. The same is true for imputing missing features. In fact, our methods learn in the dif-
cult regime of multi-label transductive learning with missing data that one sometimes encounters in
practice. That is, each item is associated with many class labels, many of the items labels may be
unobserved (some items may be completely unlabeled across all labels), and many features may also
be unobserved. Our methods build upon recent advances in matrix completion, with efcient algo-
rithms to handle matrices with mixed real-valued features and discrete labels. We obtain promising
experimental results on a range of synthetic and real-world data.

2 Problem Formulation
Let x1 . . . xn  Rd be feature vectors associated with n items. Let X = [x1 . . . xn] be the d  n
feature matrix whose columns are the items. Let there be t binary classication tasks, y1 . . . yn 
{1, 1}t be the label vectors, and Y = [y1 . . . yn] be the t  n label matrix. Entries in X or Y can
be missing at random. Let X be the index set of observed features in X, such that (i, j)  X if
and only if xij is observed. Similarly, let Y be the index set of observed labels in Y. Our main goal
is to predict the missing labels yij for (i, j) / Y. Of course, this reduces to standard transductive
learning when t = 1, |X| = nd (no missing features), and 1 < |Y| < n (some missing labels).
In our more general setting, as a side product we are also interested in imputing the missing features,
and de-noising the observed features, in X.

1

2.1 Model Assumptions

The above problem is in general ill-posed. We now describe our assumptions to make it a well-
dened problem. In a nutshell, we assume that X and Y are jointly produced by an underlying
low rank matrix. We then take advantage of the sparsity to ll in the missing labels and features
using a modied method of matrix completion. Specically, we assume the following generative
story. It starts from a d  n low rank pre-feature matrix X0, with rank(X0) (cid:28) min(d, n). The
actual feature matrix X is obtained by adding iid Gaussian noise to the entries of X0: X = X0 + ,
where ij  N(0, 2
j  Rt of item j are
j + b, where W is a t  d weight matrix, and b  Rt is a bias vector. Let
produced by y0
1 . . . y0
n

 ). Meanwhile, the t soft labels(cid:0)y0
(cid:3) be the soft label matrix. Note the combined (t + d)  n matrix(cid:2)Y0; X0(cid:3) is low
Y0 =(cid:2)y0
rank too: rank((cid:2)Y0; X0(cid:3))  rank(X0) + 1. The actual label yij  {1, 1} is generated randomly
ij) = 1/(cid:0)1 + exp(yijy0
ij)(cid:1). Finally, two random masks X, Y

via a sigmoid function: P (yij|y0
are applied to expose only some of the entries in X and Y, and we use  to denote the percentage of
observed entries. This generative story may seem restrictive, but our approaches based on it perform
well on synthetic and real datasets, outperforming several baselines with linear classiers.

(cid:1)>  y0

j = Wx0

1j . . . y0
tj

2.2 Matrix Completion for Heterogeneous Matrix Entries

With the above data generation model, our task can be dened as follows. Given the partially
observed features and labels as specied by X, Y, X, Y, we would like to recover the interme-

diate low rank matrix(cid:2)Y0; X0(cid:3). Then, X0 will contain the denoised and completed features, and
The key assumption is that the (t + d)  n stacked matrix(cid:2)Y0; X0(cid:3) is of low rank. We will start

sign(Y0) will contain the completed and correct labels.

from a hard formulation that is illustrative but impractical, then relax it.

argmin
ZR(t+d)n
s.t.

rank(Z)
sign(zij) = yij, (i, j)  Y;

Here, Z is meant to recover(cid:2)Y0; X0(cid:3) by directly minimizing the rank while obeying the observed

features and labels. Note the indices (i, j)  X are with respect to X, such that i  {1, . . . , d}. To
index the corresponding element in the larger stacked matrix Z, we need to shift the row index by t
to skip the part for Y0, and hence the constraints z(i+t)j = xij. The above formulation assumes that
there is no noise in the generation processes X0  X and Y0  Y. Of course, there are several
issues with formulation (1), and we handle them as follows:

z(i+t)j = xij, (i, j)  X

(1)

k=1

Pmin(t+d,n)

 rank() is a non-convex function and difcult to optimize. Following recent work in
matrix completion [3, 2], we relax rank() with the convex nuclear norm kZk =
k(Z), where ks are the singular values of Z. The relationship between
rank(Z) and kZk is analogous to that of 0-norm and 1-norm for vectors.
 There is feature noise from X0 to X. Instead of the equality constraints in (1), we minimize
2(u  v)2 in this
 Similarly, there is label noise from Y0 to Y. The observed labels are of a different type
than the observed features. We therefore introduce another loss function cy(zij, yij) to
In this work, we use the logistic loss cy(u, v) =
account for the heterogeneous data.
log(1 + exp(uv)).

a loss function cx(z(i+t)j, xij). We choose the squared loss cx(u, v) = 1
work, but other convex loss functions are possible too.

In addition to these changes, we will model the bias b either explicitly or implicitly, leading to two
alternative matrix completion formulations below.
Formulation 1 (MC-b). In this formulation, we explicitly optimize the bias b  Rt in addition to

Z  R(t+d)n, hence the name. Here, Z corresponds to the stacked matrix(cid:2)WX0; X0(cid:3) instead of
(cid:2)Y0; X0(cid:3), making it potentially lower rank. The optimization problem is
X

X

cy(zij + bi, yij) +

cx(z(i+t)j, xij),

(2)

argmin

Z,b

kZk + 
|Y|

(i,j)Y

1
|X|

(i,j)X

2

where ,  are positive trade-off weights. Notice the bias b is not regularized. This is a convex
problem, whose optimization procedure will be discussed in section 3. Once the optimal Z, b are
found, we recover the task-i label of item j by sign(zij + bi), and feature k of item j by z(k+t)j.
Formulation 2 (MC-1). In this formulation, the bias is modeled implicitly within Z. Similar to how
bias is commonly handled in linear classiers, we append an additional feature with constant value

one to each item. The corresponding pre-feature matrix is augmented into(cid:2)X0; 1>(cid:3), where 1 is the
Y0 are linear combinations of rows in(cid:2)X0; 1>(cid:3), i.e., rank((cid:2)Y0; X0; 1>(cid:3)) = rank((cid:2)X0; 1>(cid:3)). We
then let Z correspond to the (t + d + 1)  n stacked matrix(cid:2)Y0; X0; 1>(cid:3), by forcing its last row to

all-1 vector. Under the same label assumption y0

j + b, the rows of the soft label matrix

j = Wx0

be 1> (hence the name):

argmin

ZR(t+d+1)n

s.t.

kZk + 
|Y|
z(t+d+1) = 1>.

X

(i,j)Y

cy(zij, yij) +

1
|X|

cx(z(i+t)j, xij)

(3)

X

(i,j)X

This is a constrained convex optimization problem. Once the optimal Z is found, we recover the
task-i label of item j by sign(zij), and feature k of item j by z(k+t)j.
MC-b and MC-1 differ mainly in what is in Z, which leads to different behaviors of the nuclear norm.
Despite the generative story, we do not explicitly recover the weight matrix W in these formulations.

Other formulations are certainly possible. One way is to let Z correspond to(cid:2)Y0; X0(cid:3) directly,

without introducing bias b or the all-1 row, and hope nuclear norm minimization will prevail. This
is inferior in our preliminary experiments, and we do not explore it further in this paper.

3 Optimization Techniques

We solve MC-b and MC-1 using modications of the Fixed Point Continuation (FPC) method of Ma,
Goldfarb, and Chen [10].1 While nuclear norm minimization can be converted into a semidenite
programming (SDP) problem [2], current SDP solvers are severely limited in the size of problems
they can solve. Instead, the basic xed point approach is a computationally efcient alternative,
which provably converges to the globally optimal solution and has been shown to outperform SDP
solvers in terms of matrix recoverability.

3.1 Fixed Point Continuation for MC-b

We rst describe our modied FPC method for MC-b. It differs from [10] in the extra bias variables
and multiple loss functions. Our xed point iterative algorithm to solve the unconstrained problem
of (2) consists of two alternating steps for each iteration k:

1. (gradient step) bk+1 = bk  bg(bk), Ak = Zk  Zg(Zk)
2. (shrinkage step) Zk+1 = SZ(Ak).

In the gradient step, b and Z are step sizes whose choice will be discussed next. Overloading
notation a bit, g(bk) is the vector gradient, and g(Zk) is the matrix gradient, respectively, of the two
loss terms in (2) (i.e., excluding the nuclear norm term):
yij

X

g(bi) =

g(zij) =


|Y|

 |Y|

j:(i,j)Y

1 + exp(yij(zij + bi))
yij

1+exp(yij (zij +bi)) ,

1|X|(zij  x(it)j),
0,

i  t and (i, j)  Y
i > t and (i  t, j)  X
otherwise

(4)

(5)

Note for g(zij), i > t, we need to shift down (un-stack) the row index by t in order to map the
element in Z back to the item x(it)j.

1While the primary method of [10] is Fixed Point Continuation with Approximate Singular Value Decom-
position (FPCA), where the approximate SVD is used to speed up the algorithm, we opt to use an exact SVD
for simplicity and will refer to the method simply as FPC.

3

Input: Initial matrix Z0, bias b0,
parameters , , Step sizes b, Z
Determine 1 > 2 >  > L =  > 0.
Set Z = Z0, b = b0.
foreach  = 1, 2, . . . , L do

while Not converged do

Compute b = b  bg(b), A = Z  Zg(Z)
Compute SVD of A = UV>
Compute Z = U max(  Z, 0)V>

end

end
Output: Recovered matrix Z, bias b

Algorithm 1: FPC algorithm for MC-b.

parameters , , Step sizes Z

Input: Initial matrix Z0,
Determine 1 > 2 >  > L =  > 0.
Set Z = Z0.
foreach  = 1, 2, . . . , L do

while Not converged do

Compute A = Z  Zg(Z)
Compute SVD of A = UV>
Compute Z = U max(  Z, 0)V>
Project Z to feasible region z(t+d+1) = 1>

end

end
Output: Recovered matrix Z

Algorithm 2: FPC algorithm for MC-1.

In the shrinkage step, SZ() is a matrix shrinkage operator. Let Ak = UV> be the SVD of
Ak. Then SZ(Ak) = U max(  Z, 0)V>, where max is elementwise. That is, the shrinkage
operator shifts the singular values down, and truncates any negative values to zero. This step reduces
the nuclear norm.

Even though the problem is convex, convergence can be slow. We follow [10] and use a con-
tinuation or homotopy method to improve the speed. This involves beginning with a large value
1 >  and solving a sequence of subproblems, each with a decreasing value and using the pre-
vious solution as its initial point. The sequence of values is determined by a decay parameter :
k = 1, . . . , L  1, where  is the nal value to use, and L is the number
k+1 = max{k, },
of rounds of continuation. The complete FPC algorithm for MC-b is listed in Algorithm 1.

A minor modication of the argument in [10] reveals that as long as we choose non-negative step
sizes satisfying b < 4|Y|/(n) and Z < min{4|Y|/,|X|}, the algorithms MC-b will be
guaranteed to converge to a global optimum. Indeed, to guarantee convergence, we only need that
the gradient step is non-expansive in the sense that
kb1bg(b1)b2 +bg(b2)k2 +kZ1Zg(Z1)Z2 +Zg(Z2)k2
F  kb1b2k2 +kZ1Z2k2
F
for all b1, b2, Z1, and Z2. Our choice of b and Z guarantee such non-expansiveness. Once this
non-expansiveness is satised, the remainder of the convergence analysis is the same as in [10].

3.2 Fixed Point Continuation for MC-1

Our modied FPC method for MC-1 is similar except for two differences. First, there is no bias
variable b. Second, the shrinkage step will in general not satisfy the all-1-row constraints in (3).
Thus, we add a third projection step at the end of each iteration to project Zk+1 back to the feasible
region, by simply setting its last row to all 1s. The complete algorithm for MC-1 is given in Algo-
rithm 2. We were unable to prove convergence for this gradient + shrinkage + projection algorithm.
Nonetheless, in our empirical experiments, Algorithm 2 always converges and tends to outperform
MC-b. The two algorithms have about the same convergence speed.

4 Experiments

We now empirically study the ability of matrix completion to perform multi-class transductive clas-
sication when there is missing data. We rst present a family of 24 experiments on a synthetic
task by systematically varying different aspects of the task, including the rank of the problem, noise
level, number of items, and observed label and feature percentage. We then present experiments on
two real-world datasets: music emotions and yeast microarray. In each experiments, we compare
MC-b and MC-1 against four other baseline algorithms. Our results show that MC-1 consistently
outperforms other methods, and MC-b follows closely.
Parameter Tuning and Other Settings for MC-b and MC-1: To tune the parameters  and ,
we use 5-fold cross validation (CV) separately for each experiment. Specically, we randomly

4



,|X|), b = 3.8|Y|

5 of the observed entries, measure its performance on the remaining 1

divide X and Y into ve disjoint subsets each. We then run our matrix completion algorithms
using 4
5 , and average over
the ve folds. Since our main goal is to predict unobserved labels, we use label error as the CV
performance criterion to select parameters. Note that tuning  is quite efcient since all values
under consideration can be evaluated in one run of the continuation method. We set  = 0.25 and,
as in [10], consider  values starting at 1, where 1 is the largest singular value of the matrix
of observed entries in [Y; X] (with the unobserved entries set to 0), and decrease  until 105.
The range of  values considered was {103, 102, 101, 1}. We initialized b0 to be all zero and
Z0 to be the rank-1 approximation of the matrix of observed entries in [Y; X] (with unobserved
entries set to 0) obtained by performing an SVD and reconstructing the matrix using only the largest
singular value and corresponding left and right singular vectors. The step sizes were set as follows:
Z = min( 3.8|Y|
n . Convergence was dened as relative change in objective
functions (2)(3) smaller than 105.
Baselines: We compare to the following baselines, each consisting of some missing feature impu-
tation step on X rst, then using a standard SVM to predict the labels: [FPC+SVM] Matrix com-
pletion on X alone using FPC [10]. [EM(k)+SVM] Expectation Maximization algorithm to impute
missing X entries using a mixture of k Gaussian components. As in [9], missing features, mixing
component parameters, and the assignments of items to components are treated as hidden variables,
which are estimated in an iterative manner to maximize the likelihood of the data. [Mean+SVM]
Impute each missing feature by the mean of the observed entries for that feature. [Zero+SVM]
Impute missing features by lling in zeros.
After imputation, an SVM is trained using the available (noisy) labels in Y for that task, and
predictions are made for the rest of the labels. All SVMs are linear, trained using SVMlin2, and the
regularization parameter is tuned using 5-fold cross validation separately for each task. The range
of parameter values considered was {108, 107, . . . , 107, 108}.
Evaluation Method: To evaluate performance, we consider two measures: transductive label error,
i.e., the percentage of unobserved labels predicted incorrectly; and relative feature imputation error
ij, where x is the predicted feature value. In the tables below,
x2
for each parameter setting, we report the mean performance (and standard deviation in parenthesis)
of different algorithms over 10 random trials. The best algorithm within each parameter setting,
as well as any statistically indistinguishable algorithms via a two-tailed paired t-test at signicance
level  = 0.05, are marked in bold.

(xij  xij)2(cid:17)

/P

(cid:16)P

ij /X

ij /X

4.1 Synthetic Data Experiments

Synthetic Data Generation: We generate a family of synthetic datasets to systematically explore
the performance of the algorithms. We rst create a rank-r matrix X0 = LR>, where L  Rdr
and R  Rnr with entries drawn iid from N (0, 1). We then normalize X0 such that its entries
have variance 1. Next, we create a weight matrix W  Rtd and bias vector b  Rt, with all entries
drawn iid from N (0, 10). We then produce X, Y0, Y according to section 2.1. Finally, we produce
the random X, Y masks with  percent observed entries.
Using the above procedure, we vary  = 10%, 20%, 40%, n = 100, 400, r = 2, 4, and 2
 =
0.01, 0.1, while xing t = 10, d = 20, to produce 24 different parameter settings. For each setting,
we generate 10 trials, where the randomness is in the data and mask.
Synthetic experiment results: Table 1 shows the transductive label errors, and Table 2 shows the
relative feature imputation errors, on the synthetic datasets. We make several observations.

Observation 1: MC-b and MC-1 are the best for feature imputation, as Table 2 shows. However,
the imputations are not perfect, because in these particular parameter settings the ratio between the
number of observed entries over the degrees of freedom needed to describe the feature matrix (i.e.,
r(d + n  r)) is below the necessary condition for perfect matrix completion [2], and because there
is some feature noise. Furthermore, our CV tuning procedure selects parameters ,  to optimize
label error, which often leads to suboptimal imputation performance. In a separate experiment (not
reported here) when we made the ratio sufciently large and without noise, and specically tuned for

2http://vikas.sindhwani.org/svmlin.html

5

Table 1: Transductive label error of six algorithms on the 24 synthetic datasets. The varying pa-
 , rank(X0) = r, number of items n, and observed label and feature
rameters are feature noise 2
percentage . Each row is for a unique parameter combination. Each cell shows the mean(standard
deviation) of transductive label error (in percentage) over 10 random trials. The meta-average row
is the simple average over all parameter settings and all trials.

2

0.01

r
2

n
100

400

4

100

400

0.1

2

100

400

4

100

400



MC-b
10% 37.8(4.0)
20% 23.5(2.9)
40% 15.1(3.1)
10% 26.5(2.0)
20% 15.9(2.5)
40% 11.7(2.0)
10% 42.5(4.0)
20% 33.2(2.3)
40% 19.6(3.1)
10% 35.3(3.1)
20% 24.4(2.3)
40% 14.6(1.8)
10% 39.6(5.5)
20% 25.2(2.6)
40% 15.7(3.1)
10% 27.6(2.1)
20% 18.0(2.2)
40% 12.0(2.1)
10% 42.5(4.3)
20% 33.3(1.9)
40% 21.4(2.7)
10% 36.3(2.7)
20% 25.5(2.0)
40% 16.0(1.8)

MC-1
31.8(4.3)
17.0(2.2)
10.8(1.8)
19.9(1.7)
11.7(1.9)
8.0(1.6)
40.8(4.4)
26.2(2.8)
14.3(2.7)
32.1(1.6)
19.1(1.3)
9.5(0.5)
34.6(3.5)
20.1(1.7)
12.6(1.4)
22.6(1.9)
15.2(1.7)
10.1(1.3)
41.5(2.5)
29.0(2.2)
18.4(3.1)
34.0(1.7)
21.8(1.0)
12.8(0.8)
21.4

FPC+SVM EM1+SVM Mean+SVM
34.8(7.0)
40.5(5.7)
17.6(2.1)
28.7(4.1)
9.6(1.5)
16.5(2.5)
32.4(2.9)
23.7(1.7)
20.0(1.9)
12.6(2.2)
7.2(1.8)
12.2(1.8)
41.5(2.6)
43.5(2.9)
26.7(1.7)
35.5(1.4)
13.6(2.6)
22.5(2.0)
33.4(1.6)
37.7(1.2)
26.9(1.5)
20.5(1.4)
16.4(1.2)
9.2(0.9)
37.3(6.4)
41.5(6.0)
21.6(2.6)
31.8(4.7)
13.2(2.0)
18.5(2.7)
34.5(3.3)
27.6(2.4)
22.6(2.4)
16.8(2.3)
10.4(2.1)
14.1(2.0)
42.3(2.0)
44.6(2.9)
30.9(3.1)
36.2(2.3)
18.7(2.4)
23.9(2.0)
35.1(1.2)
38.7(1.3)
23.8(1.5)
28.4(1.7)
18.3(1.2)
13.9(1.2)
22.6
28.6

34.6(3.9)
19.7(2.4)
10.4(1.0)
24.2(1.9)
12.0(1.9)
7.3(1.4)
43.2(2.2)
30.8(2.7)
14.1(2.4)
34.2(1.8)
19.8(1.1)
8.6(1.1)
40.2(5.3)
26.8(3.7)
15.1(2.4)
28.8(2.6)
18.4(2.5)
11.1(1.9)
45.6(1.9)
34.9(3.0)
21.6(2.4)
36.3(1.4)
25.1(1.4)
14.7(1.3)
24.1

Zero+SVM
40.5(5.1)
27.4(4.4)
15.4(2.3)
31.5(2.7)
19.7(1.7)
12.1(2.0)
42.9(2.9)
33.9(1.5)
21.7(2.3)
38.2(1.4)
26.9(1.3)
16.5(1.3)
41.0(5.7)
29.9(4.0)
17.2(2.4)
33.6(2.8)
21.8(2.5)
14.0(2.4)
43.6(2.3)
35.4(1.6)
23.3(2.5)
39.1(1.2)
28.4(1.8)
18.2(1.2)
28.0

meta-average

25.6

imputation error, both MC-b and MC-1 did achieve perfect feature imputation. Also, FPC+SVM is
slightly worse in feature imputation. This may seem curious as FPC focuses exclusively on imputing
X. We believe the fact that MC-b and MC-1 can use information in Y to enhance feature imputation
in X made them better than FPC+SVM.
Observation 2: MC-1 is the best for multi-label transductive classication, as suggested by Table 1.
Surprisingly, the feature imputation advantage of MC-b did not translate into classication, and
FPC+SVM took second place.

Observation 3: The same factors that affect standard matrix completion also affect classication
performance of MC-b and MC-1. As the tables show, everything else being equal, less feature noise
(smaller 2
 ), lower rank r, more items, or more observed features and labels, reduce label error.
Benecial combination of these factors (the 6th row) produces the lowest label errors.
Matrix completion benets from more tasks. We performed one additional synthetic data exper-
iment examining the effect of t (the number of tasks) on MC-b and MC-1, with the remaining data
parameters xed at  = 10%, n = 400, r = 2, d = 20, and 2
 = 0.01. Table 3 reveals that both
MC methods achieve statistically signicantly better label prediction and imputation performance
with t = 10 than with only t = 2 (as determined by two-sample t-tests at signicance level 0.05).

4.2 Music Emotions Data Experiments

In this task introduced by Trohidis et al. [14], the goal is to predict which of several types of emotion
are present in a piece of music. The data3 consists of n = 593 songs of a variety of musical genres,
each labeled with one or more of t = 6 emotions (i.e., amazed-surprised, happy-pleased, relaxing-
calm, quiet-still, sad-lonely, and angry-fearful). Each song is represented by d = 72 features (8
rhythmic, 64 timbre-based) automatically extracted from a 30-second sound clip.

3Available at http://mulan.sourceforge.net/datasets.html

6

Table 2: Relative feature imputation error on the synthetic datasets. The algorithm Zero+SVM is
not shown because it by denition has relative feature imputation error 1.

2

0.01

r
2

n
100

400

4

100

400

0.1

2

100

400

4

100

400



MC-b
10% 0.84(0.04)
20% 0.54(0.08)
40% 0.29(0.06)
10% 0.73(0.03)
20% 0.43(0.04)
40% 0.30(0.10)
10% 0.99(0.04)
20% 0.77(0.05)
40% 0.42(0.07)
10% 0.87(0.04)
20% 0.69(0.07)
40% 0.34(0.05)
10% 0.92(0.05)
20% 0.69(0.07)
40% 0.51(0.05)
10% 0.79(0.03)
20% 0.64(0.06)
40% 0.48(0.04)
10% 1.01(0.04)
20% 0.84(0.03)
40% 0.59(0.03)
10% 0.90(0.02)
20% 0.75(0.04)
40% 0.56(0.03)

MC-1
0.87(0.06)
0.57(0.06)
0.27(0.06)
0.72(0.04)
0.46(0.05)
0.22(0.04)
0.96(0.03)
0.78(0.05)
0.40(0.03)
0.88(0.03)
0.67(0.04)
0.34(0.03)
0.93(0.04)
0.72(0.06)
0.52(0.05)
0.80(0.03)
0.64(0.06)
0.45(0.05)
0.97(0.03)
0.85(0.03)
0.61(0.04)
0.92(0.02)
0.77(0.02)
0.55(0.04)
0.66

FPC+SVM
0.88(0.06)
0.57(0.07)
0.27(0.06)
0.76(0.03)
0.50(0.04)
0.24(0.05)
0.96(0.03)
0.77(0.04)
0.42(0.04)
0.89(0.01)
0.69(0.03)
0.38(0.03)
0.93(0.05)
0.74(0.06)
0.53(0.05)
0.84(0.03)
0.67(0.04)
0.49(0.05)
0.97(0.03)
0.85(0.03)
0.63(0.04)
0.92(0.01)
0.79(0.03)
0.59(0.04)
0.68

EM1+SVM
1.01(0.12)
0.67(0.13)
0.34(0.03)
0.79(0.07)
0.45(0.04)
0.21(0.04)
1.22(0.11)
0.92(0.07)
0.49(0.04)
1.00(0.08)
0.66(0.03)
0.29(0.02)
1.18(0.10)
0.94(0.07)
0.67(0.08)
0.96(0.07)
0.73(0.07)
0.57(0.07)
1.25(0.05)
1.07(0.06)
0.80(0.09)
1.08(0.07)
0.86(0.05)
0.66(0.06)
0.78

Mean+SVM
1.06(0.02)
1.03(0.02)
1.01(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.01)
1.02(0.01)
1.01(0.01)
1.01(0.00)
1.01(0.00)
1.00(0.00)
1.06(0.02)
1.03(0.02)
1.02(0.01)
1.02(0.01)
1.01(0.00)
1.00(0.00)
1.05(0.02)
1.02(0.01)
1.01(0.01)
1.01(0.01)
1.01(0.00)
1.00(0.00)
1.02

meta-average

0.66

Table 3: More tasks help matrix completion ( = 10%, n = 400, r = 2, d = 20, 2
FPC+SVM
0.76(0.03)
0.76(0.03)

MC-1
0.78(0.04)
0.72(0.04)

MC-b
0.78(0.07)
0.73(0.03)

MC-1
22.9(2.2)
19.9(1.7)

FPC+SVM
20.5(2.5)
23.7(1.7)

t
2
10

MC-b
30.1(2.8)
26.5(2.0)

 = 0.01).

transductive label error

relative feature imputation error

Table 4: Performance on the music emotions data.

 =40%
28.0(1.2)
27.4(0.8)
26.9(0.7)
26.0(1.1)
26.2(0.9)
26.3(0.8)
30.3(0.6)

60%
25.2(1.0)
23.7(1.6)
25.2(1.6)
23.6(1.1)
23.1(1.2)
24.2(1.0)
28.9(1.1)

80%
22.2(1.6)
19.8(2.4)
24.4(2.0)
21.2(2.3)
21.6(1.6)
22.6(1.3)
25.7(1.4)

Algorithm

MC-b
MC-1

FPC+SVM
EM1+SVM
EM4+SVM
Mean+SVM
Zero+SVM

 =40%
0.69(0.05)
0.60(0.05)
0.64(0.01)
0.46(0.09)
0.49(0.10)
0.18(0.00)
1.00(0.00)

60%
0.54(0.10)
0.46(0.12)
0.46(0.02)
0.23(0.04)
0.27(0.04)
0.19(0.00)
1.00(0.00)

80%
0.41(0.02)
0.25(0.03)
0.31(0.03)
0.13(0.01)
0.15(0.02)
0.20(0.01)
1.00(0.00)

transductive label error

relative feature imputation error

We vary the percentage of observed entries  = 40%, 60%, 80%. For each , we run 10 random
trials with different masks X, Y. For this dataset, we tuned only  with CV, and set  = 1.
The results are in Table 4. Most importantly, these results show that MC-1 is useful for this real-
world multi-label classication problem, leading to the best (or statistically indistinguishable from
the best) transductive error performance with 60% and 80% of the data available, and close to the
best with only 40%.

We also compared these algorithms against an oracle baseline (not shown in the table). In this
baseline, we give 100% features (i.e., no indices are missing from X) and the training labels
in Y to a standard SVM, and let it predict the unspecied labels. On the same random tri-
als, for observed percentage  = 40%, 60%, 80%, the oracle baseline achieved label error rate
22.1(0.8), 21.3(0.8), 20.5(1.8) respectively. Interestingly, MC-1 with  = 80% (19.8) is statisti-
cally indistinguishable from the oracle baseline.

7

4.3 Yeast Microarray Data Experiments

This dataset comes from a biological domain and involves the problem of Yeast gene functional
classication. We use the data studied by Elisseeff and Weston [5], which contains n = 2417
examples (Yeast genes) with d = 103 input features (results from microarray experiments).4 We
follow the approach of [5] and predict each genes membership in t = 14 functional classes. For
this larger dataset, we omitted the computationally expensive EM4+SVM methods, and tuned only
 for matrix completion while xing  = 1.
Table 5 reveals that MC-b leads to statistically signicantly lower transductive label error for this bi-
ological dataset. Although not highlighted in the table, MC-1 is also statistically better than the SVM
methods in label error. In terms of feature imputation performance, the MC methods are weaker than
FPC+SVM. However, it seems simultaneously predicting the missing labels and features appears to
provide a large advantage to the MC methods. It should be pointed out that all algorithms except
Zero+SVM in fact have small but non-zero standard deviation on imputation error, despite what the
xed-point formatting in the table suggests. For instance, with  = 40%, the standard deviation is
0.0009 for MC-1, 0.0011 for FPC+SVM, and 0.0001 for Mean+SVM.
Again, we compared these algorithms to an oracle SVM baseline with 100% observed entries in X.
The oracle SVM approach achieves label error of 20.9(0.1), 20.4(0.2), and 20.1(0.3) for  =40%,
60%, and 80% observed labels, respectively. Both MC-b and MC-1 signicantly outperform this
oracle under paired t-tests at signicance level 0.05. We attribute this advantage to a combination
of multi-label learning and transduction that is intrinsic to our matrix completion methods.

 =40%
16.1(0.3)
16.7(0.3)
21.5(0.3)
22.0(0.2)
21.7(0.2)
21.6(0.2)

60%
12.2(0.3)
13.0(0.2)
20.8(0.3)
21.2(0.2)
21.1(0.2)
21.1(0.2)

80%
8.7(0.4)
8.5(0.4)
20.3(0.3)
20.4(0.2)
20.5(0.4)
20.5(0.4)

Algorithm

MC-b
MC-1

FPC+SVM
EM1+SVM
Mean+SVM
Zero+SVM

Table 5: Performance on the yeast data.

 =40%
0.83(0.02)
0.86(0.00)
0.81(0.00)
1.15(0.02)
1.00(0.00)
1.00(0.00)

60%
0.76(0.00)
0.92(0.00)
0.76(0.00)
1.04(0.02)
1.00(0.00)
1.00(0.00)

80%
0.73(0.02)
0.74(0.00)
0.72(0.00)
0.77(0.01)
1.00(0.00)
1.00(0.00)

transductive label error

relative feature imputation error

5 Discussions and Future Work

We have introduced two matrix completion methods for multi-label transductive learning with miss-
ing features, which outperformed several baselines. In terms of problem formulation, our methods
differ considerably from sparse multi-task learning [11, 1, 13] in that we regularize the feature and
label matrix directly, without ever learning explicit weight vectors. Our methods also differ from
multi-label prediction via reduction to binary classication or ranking [15], and via compressed
sensing [7], which assumes sparsity in that each item has a small number of positive labels, rather
than the low-rank nature of feature matrices. These methods do not naturally allow for missing fea-
tures. Yet other multi-label methods identify a subspace of highly predictive features across tasks in
a rst stage, and learn in this subspace in a second stage [8, 12]. Our methods do not require separate
stages. Learning in the presence of missing data typically involves imputation followed by learning
with completed data [9]. Our methods perform imputation plus learning in one step, similar to EM
on missing labels and features [6], but the underlying model assumption is quite different.

A drawback of our methods is their restriction to linear classiers only. One future extension is to
explicitly map the partial feature matrix to a partially observed polynomial (or other) kernel Gram
matrix, and apply our methods there. Though such mapping proliferates the missing entries, we
hope that the low-rank structure in the kernel matrix will allow us to recover labels that are nonlinear
functions of the original features.

Acknowledgements: This work is supported in part by NSF IIS-0916038, NSF IIS-0953219, AFOSR FA9550-
09-1-0313, and AFOSR A9550-09-1-0423. We also wish to thank Brian Eriksson for useful discussions and
source code implementing EM-based imputation.

4Available at http://mulan.sourceforge.net/datasets.html

8

