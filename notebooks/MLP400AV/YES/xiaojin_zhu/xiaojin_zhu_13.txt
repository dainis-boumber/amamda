Abstract

In traditional machine learning approaches to classication, one uses only a labeled
set to train the classier. Labeled instances however are often difcult, expensive,
or time consuming to obtain, as they require the efforts of experienced human
annotators. Meanwhile unlabeled data may be relatively easy to collect, but there
has been few ways to use them. Semi-supervised learning addresses this problem
by using large amount of unlabeled data, together with the labeled data, to build
better classiers. Because semi-supervised learning requires less human effort and
gives higher accuracy, it is of great interest both in theory and in practice.

We present a series of novel semi-supervised learning approaches arising from
a graph representation, where labeled and unlabeled instances are represented as
vertices, and edges encode the similarity between instances. They address the fol-
lowing questions: How to use unlabeled data? (label propagation); What is the
probabilistic interpretation? (Gaussian elds and harmonic functions); What if
we can choose labeled data? (active learning); How to construct good graphs?
(hyperparameter learning); How to work with kernel machines like SVM? (graph
kernels); How to handle complex data like sequences? (kernel conditional ran-
dom elds); How to handle scalability and induction? (harmonic mixtures). An
extensive literature review is included at the end.

iii

iv

Acknowledgments

First I would like to thank my thesis committee members. Roni Rosenfeld brought
me into the wonderful world of research. He not only gave me valuable advices
in academics, but also helped my transition into a different culture. John Lafferty
guided me further into machine learning. I am always impressed by his mathe-
matical vigor and sharp thinking. Zoubin Ghahramani has been a great mentor
and collaborator, energetic and full of ideas. I wish he could stay in Pittsburgh
more! Tommi Jaakkola helped me by asking insightful questions, and giving me
thoughtful comments on the thesis. I enjoyed working with them, and beneted
enormously from the interactions with them.

I spent nearly seven years in Carnegie Mellon University.

I thank the fol-
lowing collaborators, faculties, staffs, fellow students and friends, who made my
graduate life a very memorable experience: Maria Florina Balcan, Paul Bennett,
Adam Berger, Michael Bett, Alan Black, Avrim Blum, Dan Bohus, Sharon Burks,
Can Cai, Jamie Callan, Rich Caruana, Arthur Chan, Peng Chang, Shuchi Chawla,
Lifei Cheng, Stanley Chen, Tao Chen, Pak Yan Choi, Ananlada Chotimongicol,
Tianjiao Chu, Debbie Clement, William Cohen, Catherine Copetas, Derek Dreyer,
Dannie Durand, Maxine Eskenazi, Christos Faloutsos, Li Fan, Zhaohui Fan, Marc
Fasnacht, Stephen Fienberg, Robert Frederking, Rayid Ghani, Anna Goldenberg,
Evandro Gouvea, Alexander Gray, Ralph Gross, Benjamin Han, Thomas Harris,
Alexander Hauptmann, Rose Hoberman, Fei Huang, Pu Huang, Xiaoqiu Huang,
Yi-Fen Huang, Jianing Hu, Changhao Jiang, Qin Jin, Rong Jin, Rosie Jones, Szu-
Chen Jou, Jaz Kandola, Chris Koch, John Kominek, Leonid Kontorovich, Chad
Langley, Guy Lebanon, Lillian Lee, Kevin Lenzo, Hongliang Liu, Yan Liu, Xi-
ang Li, Ariadna Font Llitjos, Si Luo, Yong Lu, Matt Mason, Iain Matthews, An-
drew McCallum, Uwe Meier, Tom Minka, Tom Mitchell, Andrew W Moore, Jack
Mostow, Ravishankar Mosur, Jon Nedel, Kamal Nigam, Eric Nyberg, Alice Oh,
Chris Paciorek, Brian Pantano, Yue Pan, Vasco Calais Pedro, Francisco Pereira,
Yanjun Qi, Bhiksha Raj, Radha Rao, Pradeep Ravikumar, Nadine Reaves, Max
Ritter, Chuck Rosenberg, Steven Rudich, Alex Rudnicky, Mugizi Robert Rweban-
gira, Kenji Sagae, Barbara Sandling, Henry Schneiderman, Tanja Schultz, Teddy

v

vi

Seidenfeld, Michael Seltzer, Kristie Seymore, Minglong Shao, Chen Shimin, Rita
Singh, Jim Skees, Richard Stern, Diane Stidle, Yong Sun, Sebastian Thrun, Ste-
fanie Tomko, Laura Mayeld Tomokiyo, Arthur Toth, Yanghai Tsin, Alex Waibel,
Lisha Wang, Mengzhi Wang, Larry Wasserman, Jeannette Wing, Weng-Keen Wong,
Sharon Woodside, Hao Xu, Mingxin Xu, Wei Xu, Jie Yang, Jun Yang, Ke Yang,
Wei Yang, Yiming Yang, Rong Yan, Rong Yan, Stacey Young, Hua Yu, Klaus
Zechner, Jian Zhang, Jieyuan Zhang, Li Zhang, Rong Zhang, Ying Zhang, Yi
Zhang, Bing Zhao, Pei Zheng, Jie Zhu. I spent some serious effort nding ev-
eryone from archival emails. My apologies if I left your name out. In particular, I
thank you if you are reading this thesis.

Finally I thank my family. My parents Yu and Jingquan endowed me with the
curiosity about the natural world. My dear wife Jing brings to life so much love
and happiness, making thesis writing an enjoyable endeavor. Last but not least, my
ten-month-old daughter Amanda helped me ty

pe the ,manuscr ihpt .

Contents

1 Introduction

1.1 What is Semi-Supervised Learning? . . . . .
. . . . . . . . . . . . .
1.2 A Short History . . .
1.3 Structure of the Thesis
. . . . . . . . . . . .

. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .

2 Label Propagation

2.1 Problem Setup . . . .
2.2 The Algorithm . . .
2.3 Convergence
. . . .
2.4

. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
Illustrative Examples . . . . . . . . . . . . .

. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .

3 What is a Good Graph?

3.1 Example One: Handwritten Digits . . . . . .
3.2 Example Two: Document Categorization . . .
3.3 Example Three: The FreeFoodCam . . . . .
3.4 Common Ways to Create Graphs . . . . . . .

. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .

4 Gaussian Random Fields

4.1 Gaussian Random Fields . . . . . . . . . . .
. . . . . . . . . . .
4.2 The Graph Laplacian . . . . . . . . . . . . .
. . . . . . . . . . .
4.3 Harmonic Functions .
. . . . . . . . . . . . .
. . . . . . . . . . .
Interpretation and Connections . . . . . . . .
4.4
. . . . . . . . . . .
4.4.1 Random Walks . . . . . . . . . . . .
. . . . . . . . . . .
4.4.2 Electric Networks . . . . . . . . . . .
. . . . . . . . . . .
4.4.3 Graph Mincut . . . . . . . . . . . . .
. . . . . . . . . . .
Incorporating Class Proportion Knowledge
.
. . . . . . . . . . .
Incorporating Vertex Potentials on Unlabeled Instances . . . . . .
. . . . . . . . . . .

4.5
4.6
4.7 Experimental Results . . . . . . . . . . . . .

1
1
2
4

5
5
6
6
7

9
9
12
12
16

21
21
22
22
23
23
24
24
25
26
26

vii

viii

5 Active Learning

CONTENTS

5.1 Combining Semi-Supervised and Active Learning . . . . .
. . . . . . . . . . . . .
5.2 Why not Entropy Minimization . .
5.3 Experiments . . . . . . . . . . . .
. . . . . . . . . . . . .

. . . .
. . . .
. . . .

6 Connection to Gaussian Processes

Incorporating a Noise Model

6.1 A Finite Set Gaussian Process Model . . . . . . . . . . . .
. . .
6.2
. . . . . . . . . . . . .
. . . . . . . . . . . . .
6.3 Experiments . . . . . . . . . . . .
6.4 Extending to Unseen Data
. . . .
. . . . . . . . . . . . .

. . . .
. . . .
. . . .
. . . .

7 Graph Hyperparameter Learning

7.1 Evidence Maximization . . . . . .
7.2 Entropy Minimization . . . . . . .
7.3 Minimum Spanning Tree . . . . .
7.4 Discussion . . . . . . . . . . . . .

. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .

. . . .
. . . .
. . . .
. . . .

8 Kernels from the Spectrum of Laplacians

. . . . . . . . . . . . .
. . .
8.1 The Spectrum of Laplacians
8.2 From Laplacians to Kernels . . . .
. . . . . . . . . . . . .
8.3 Convex Optimization using QCQP . . . . . . . . . . . . .
8.4 Semi-Supervised Kernels with Order Constraints
. . . . .
. . . . . . . . . . . . .
8.5 Experiments . . . . . . . . . . . .

. . . .
. . . .
. . . .
. . . .
. . . .

9 Sequences and Beyond

9.1 Cliques and Two Graphs
. . . . .
. . . . . . . . . . . . .
9.2 Representer Theorem for KCRFs .
. . . . . . . . . . . . .
9.3 Sparse Training: Clique Selection . . . . . . . . . . . . .
9.4 Synthetic Data Experiments
. . . . . . . . . . . . .

. . .

. . . .
. . . .
. . . .
. . . .

10 Harmonic Mixtures

10.1 Review of Mixture Models and the EM Algorithm . . . . .
10.2 Label Smoothness on the Graph .
. . . . . . . . . . . . .
10.3 Combining Mixture Model and Graph . . . . . . . . . . .
10.3.1 The Special Case with  = 0 . . . . . . . . . . . .
10.3.2 The General Case with  > 0 . . . . . . . . . . .
10.4 Experiments . . . . . . . . . . . .
. . . . . . . . . . . . .
10.4.1 Synthetic Data . . . . . .
. . . . . . . . . . . . .
10.4.2 Image Recognition: Handwritten Digits . . . . . .
10.4.3 Text Categorization: PC vs. Mac . . . . . . . . . .

. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .
. . . .

35
35
38
39

45
45
47
47
50

51
51
53
56
56

57
57
58
60
61
64

69
70
71
73
74

79
80
82
83
83
86
89
89
91
92

CONTENTS

10.5 Related Work . . . .
10.6 Discussion . . . . . .

. . . . . . . . . . . . .
. . . . . . . . . . . . .

. . . . . . . . . . .
. . . . . . . . . . .

ix

92
94

11 Literature Review

97
97
. . . . . . . . . . .
11.1 Q&A . . . . . . . .
. . . . . . . . . . . . .
99
. . . . . . . . . . .
11.2 Generative Mixture Models and EM . . . . .
. . . . . . . . . . .
99
11.2.1 Identiability . . . . . . . . . . . . .
. . . . . . . . . . . 100
11.2.2 Model Correctness . . . . . . . . . .
. . . . . . . . . . . 101
11.2.3 EM Local Maxima . . . . . . . . . .
. . . . . . . . . . . 101
11.2.4 Cluster and Label . . . . . . . . . . .
. . . . . . . . . . . 101
11.3 Self-Training . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . 102
11.4 Co-Training . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . 103
11.5 Maximizing Separation . . . . . . . . . . . .
. . . . . . . . . . . 103
11.5.1 Transductive SVM . . . . . . . . . .
. . . . . . . . . . . 104
11.5.2 Gaussian Processes . . . . . . . . . .
. . . . . . . . . . . 104
11.5.3 Information Regularization . . . . . .
. . . . . . . . . . . 105
11.5.4 Entropy Minimization . . . . . . . .
. . . . . . . . . . . 105
. . . . . . . . . . . .
. . . . . . . . . . . 105
11.6.1 Regularization by Graph . . . . . . .
. . . . . . . . . . . 109
11.6.2 Graph Construction . . . . . . . . . .
. . . . . . . . . . . 109
. . . . . . . . . . . . .
11.6.3 Induction . .
. . . . . . . . . . . 110
11.6.4 Consistency .
. . . . . . . . . . . . .
. . . . . . . . . . . 110
. . . . . . . . . . . . .
11.6.5 Ranking . . .
. . . . . . . . . . . 110
11.6.6 Directed Graphs
. . . . . . . . . . .
. . . . . . . . . . . 111
11.6.7 Fast Computation . . . . . . . . . . .
. . . . . . . . . . . 111
11.7 Metric-Based Model Selection . . . . . . . .
. . . . . . . . . . . 112
. . . . . . . . . . . . .
11.8 Related Areas . . . .
. . . . . . . . . . . 112
11.8.1 Spectral Clustering . . . . . . . . . .
11.8.2 Clustering with Side Information . .
. . . . . . . . . . . 112
11.8.3 Nonlinear Dimensionality Reduction . . . . . . . . . . . 113
11.8.4 Learning a Distance Metric . . . . . .
. . . . . . . . . . . 113
11.8.5 Inferring Label Sampling Mechanisms . . . . . . . . . . . 115

11.6 Graph-Based Methods

12 Discussions

A Update Harmonic Function

B Matrix Inverse

C Laplace Approximation for Gaussian Processes

117

121

123

125

x

D Evidence Maximization

E Mean Field Approximation

F Comparing Iterative Algorithms

. . . . . . . . . . . . .
F.1 Label Propagation . . . . . . . . .
F.2 Conjugate Gradient
. . . . . . . . . . . . .
. . . . . . . .
F.3 Loopy belief propagation on Gaussian elds . . . . . . . .
. . . . . . . . . . . . .
F.4 Empirical Results . . . . . . . . .

Notation

CONTENTS

129

135

139
. . . . 140
. . . . 140
. . . . 140
. . . . 144

161

Chapter 1

Introduction

1.1 What is Semi-Supervised Learning?

The eld of machine learning has traditionally been divided into three sub-elds:
 unsupervised learning. The learning system observes an unlabeled set of
items, represented by their features {x1, . . . , xn}. The goal is to organize
the items. Typical unsupervised learning tasks include clustering that groups
items into clusters; outlier detection which determines if a new item x is sig-
nicantly different from items seen so far; dimensionality reduction which
maps x into a low dimensional space, while preserving certain properties of
the dataset.

 supervised learning. The learning system observes a labeled training set
consisting of (feature, label) pairs, denoted by {(x1, y1), . . . , (xn, yn)}. The
goal is to predict the label y for any new input with feature x. A supervised
learning task is called regression when y  R, and classication when y
takes a set of discrete values.

 reinforcement learning. The learning system repeatedly observes the envi-
ronment x, performs an action a, and receives a reward r. The goal is to
choose the actions that maximize the future rewards.

This thesis focuses on classication, which is traditionally a supervised learn-

ing task. To train a classier one needs the labeled training set {(x1, y1), . . . , (xn, yn)}.
However the labels y are often hard, expensive, and slow to obtain, because it may
require experienced human annotators. For instance,

 Speech recognition. Accurate transcription of speech utterance at phonetic
level is extremely time consuming (as slow as 400RT, i.e. 400 times longer

1

2

CHAPTER1. INTRODUCTION

than the utterance duration), and requires linguistic expertise. Transcription
at word level is still time consuming (about 10RT), especially for conver-
sational or spontaneous speech. This problem is more prominent for foreign
languages or dialects with less speakers, when linguistic experts of that lan-
guage are hard to nd.

 Text categorization. Filtering out spam emails, categorizing user messages,
recommending Internet articles  many such tasks need the user to label
text document as interesting or not. Having to read and label thousands of
documents is daunting for average users.

 Parsing. To train a good parser one needs sentence / parse tree pairs, known
as treebanks. Treebanks are very time consuming to construct by linguists.
It took the experts several years to create parse trees for only a few thousand
sentences.

 Video surveillance. Manually labeling people in large amount of surveil-

lance camera images can be time consuming.

 Protein structure prediction. It may take months of expensive lab work by

expert crystallographers to identify the 3D structure of a single protein.

On the other hand, unlabeled data x, without labels, is usually available in large
quantity and costs little to collect. Utterances can be recorded from radio broad-
cast; Text documents can be crawled from the Internet; Sentences are everywhere;
Surveillance cameras run 24 hours a day; DNA sequences of proteins are readily
available from gene databases. The problem with traditional classication methods
is: they cannot use unlabeled data to train classiers.

The question semi-supervised learning addresses is: given a relatively small
labeled dataset {(x, y)} and a large unlabeled dataset {x}, can one devise ways
to learn from both for classication? The name semi-supervised learning comes
from the fact that the data used is between supervised and unsupervised learning.
Semi-supervised learning promises higher accuracies with less annotating effort.
It is therefore of great theoretic and practical interest. A broader denition of
semi-supervised learning includes regression and clustering as well, but we will
not pursued that direction here.

1.2 A Short History of Semi-Supervised Learning

There has been a whole spectrum of interesting ideas on how to learn from both
labeled and unlabeled data. We give a highly simplied history of semi-supervised

1.2. ASHORTHISTORY

3

learning in this section. Interested readers can skip to Chapter 11 for an extended
literature review. It should be pointed out that semi-supervised learning is a rapidly
evolving eld, and the review is necessarily incomplete.

Early work in semi-supervised learning assumes there are two classes, and each
class has a Gaussian distribution. This amounts to assuming the complete data
comes from a mixture model . With large amount of unlabeled data, the mixture
components can be identied with the expectation-maximization (EM) algorithm.
One needs only a single labeled example per component to fully determine the
mixture model. This model has been successfully applied to text categorization.

A variant is self-training : A classier is rst trained with the labeled data. It
is then used to classify the unlabeled data. The most condent unlabeled points,
together with their predicted labels, are added to the training set. The classier is
re-trained and the procedure repeated. Note the classier uses its own predictions
to teach itself. This is a hard version of the mixture model and EM algorithm.
The procedure is also called self-teaching , or bootstrapping1 in some research
communities. One can imagine that a classication mistake can reinforce itself.

Both methods have been used since long time ago. They remain popular be-

cause of their conceptual and algorithmic simplicity.

Co-training reduces the mistake-reinforcing danger of self-training. This recent
method assumes that the features of an item can be split into two subsets. Each sub-
feature set is sufcient to train a good classier; and the two sets are conditionally
independent given the class. Initially two classiers are trained with the labeled
data, one on each sub-feature set. Each classier then iteratively classies the
unlabeled data, and teaches the other classier with its predictions.

With the rising popularity of support vector machines (SVMs), transductive
SVMs emerge as an extension to standard SVMs for semi-supervised learning.
Transductive SVMs nd a labeling for all the unlabeled data, and a separating
hyperplane, such that maximum margin is achieved on both the labeled data and
the (now labeled) unlabeled data. Intuitively unlabeled data guides the decision
boundary away from dense regions.

Recently graph-based semi-supervised learning methods have attracted great
attention. Graph-based methods start with a graph where the nodes are the labeled
and unlabeled data points, and (weighted) edges reect the similarity of nodes.
The assumption is that nodes connected by a large-weight edge tend to have the
same label, and labels can propagation throughout the graph. Graph-based meth-
ods enjoy nice properties from spectral graph theory. This thesis mainly discusses
graph-based semi-supervised methods.

We summarize a few representative semi-supervised methods in Table 1.1.

1Not to be confused with the resample procedure with the same name in statistics.

4

CHAPTER1. INTRODUCTION

Method

Assumptions

mixture model, EM generative mixture model
transductive SVM low density region between classes

co-training

graph methods

conditionally independent and redundant features splits
labels smooth on graph

Table 1.1: Some representative semi-supervised learning methods

1.3 Structure of the Thesis

The rest of the thesis is organized as follows:

Chapter 2 starts with the simple label propagation algorithm, which propagates
class labels on a graph. This is the rst semi-supervised learning algorithm we will
encounter. It is also the basis for many variations later.

Chapter 3 discusses how one constructs a graph. The emphasis is on the intu-
ition  what graphs make sense for semi-supervised learning? We will give several
examples on various datasets.

Chapter 4 formalizes label propagation in a probabilistic framework with Gaus-
sian random elds. Concepts like graph Laplacian and harmonic function are intro-
duced. We will explore interesting connections to electric networks, random walk,
and spectral clustering. Issues like the balance between classes, and inclusion of
external classiers are also discussed here.

Chapter 5 assumes that one can choose a data point and ask an oracle for the
label. This is the standard active learning scheme. We show that active learning
and semi-supervised learning can be naturally combined.

Chapter 6 establishes the link to Gaussian processes. The kernel matrices are

shown to be the smoothed inverse graph Laplacian.

Chapter 7 no longer assumes the graph is given and xed.

Instead, we pa-
rameterize the graph weights, and learn the optimal hyperparameters. We will
discuss several methods: evidence maximization, entropy minimization, and mini-
mum spanning tree.

Chapter 8 turns semi-supervised learning problem into kernel learning. We
show a natural family of kernels derived from the graph Laplacian, and nd the
best kernel via convex optimization.

Chapter 9 discusses kernel conditional random elds, and its potential applica-

tion in semi-supervised learning, for sequences and other complex structures.

Chapter 10 explores scalability and induction for semi-supervised learning.
Chapter 11 reviews the literatures on semi-supervised learning.

Chapter 2

Label Propagation

In this chapter we introduce our rst semi-supervised learning algorithm: Label
Propagation. We formulate the problem as a form of propagation on a graph, where
a nodes label propagates to neighboring nodes according to their proximity. In this
process we x the labels on the labeled data. Thus labeled data act like sources that
push out labels through unlabeled data.

2.1 Problem Setup

Let {(x1, y1) . . . (xl, yl)} be the labeled data, y  {1 . . . C}, and {xl+1 . . . xl+u}
the unlabeled data, usually l (cid:28) u. Let n = l + u. We will often use L and U to
denote labeled and unlabeled data respectively. We assume the number of classes
C is known, and all classes are present in the labeled data. In most of the thesis we
study the transductive problem of nding the labels for U. The inductive problem
of nding labels for points outside of L  U will be discussed in Chapter 10.
Intuitively we want data points that are similar to have the same label. We
create a graph where the nodes are all the data points, both labeled and unlabeled.
The edge between nodes i, j represents their similarity. For the time being let us
assume the graph is fully connected with the following weights:

wij = exp(cid:18)kxi  xjk2

2

(cid:19)

(2.1)

where  is a bandwidth hyperparameter. The construction of graphs will be dis-
cussed in later Chapters.

5

6

CHAPTER2. LABELPROPAGATION

2.2 The Algorithm

We propagate the labels through the edges. Larger edge weights allow labels to
travel through more easily. Dene a n  n probabilistic transition matrix P

(2.2)

Pij = P (i  j) =

k=1 wik

wijPn

where Pij is the probability of transit from node i to j. Also dene a l  C label
matrix YL, whose ith row is an indicator vector for yi, i  L: Yic = (yi, c). We
will compute soft labels f for the nodes. f is a n  C matrix, the rows can be
interpreted as the probability distributions over labels. The initialization of f is not
important. We are now ready to present the algorithm.

The label propagation algorithm is as follows:

1. Propagate f  P f
2. Clamp the labeled data fL = YL.

3. Repeat from step 1 until f converges.

In step 1, all nodes propagate their labels to their neighbors for one step. Step 2
is critical: we want persistent label sources from labeled data. So instead of letting
the initially labels fade away, we clamp them at YL. With this constant push from
labeled nodes, the class boundaries will be pushed through high density regions
and settle in low density gaps. If this structure of data ts the classication goal,
then the algorithm can use unlabeled data to help learning.

2.3 Convergence

We now show the algorithm converges to a simple solution. Let f = (cid:18) fL
fU (cid:19).

Since fL is clamped to YL, we are solely interested in fU . We split P into labeled
and unlabeled sub-matrices

P =(cid:20) PLL PLU
PU L PU U (cid:21)

It can be shown that our algorithm is

fU  PU U fU + PU LYL

(2.3)

(2.4)

2.4. ILLUSTRATIVEEXAMPLES

which leads to

fU = lim
n

(PU U )nf 0

U +  nXi=1

7

(2.5)

(PU U )(i1)! PU LYL

U is the initial value for fU . We need to show (PU U )nf 0

where f 0
row normalized, and PU U is a sub-matrix of P , it follows

U  0. Since P is

Therefore

Xj

 < 1,

(PU U )ij  ,i = 1 . . . u

uXj=1
ij = Xj Xk
= Xk
 Xk
 n

(PU U )n

(PU U )(n1)

ik(PU U )kj

(PU U )(n1)

(PU U )(n1)

(PU U )kj

ikXj

ik

(2.6)

(2.7)

(2.8)

(2.9)

(2.10)

(2.11)

Therefore the row sums of (PU U )n converges to zero, which means (PU U )nf 0
0. Thus the initial value f 0

U 

U is inconsequential. Obviously
fU = (I  PU U )1PU LYL

is a xed point. Therefore it is the unique xed point and the solution to our
iterative algorithm. This gives us a way to solve the label propagation problem
directly without iterative propagation.

Note the solution is valid only when I  PU U is invertible. The condition is
satised, intuitively, when every connected component in the graph has at least one
labeled point in it.

2.4 Illustrative Examples

We demonstrate the properties of the Label Propagation algorithm on two synthetic
datasets. Figure 2.1(a) shows a synthetic dataset with three classes, each being a
narrow horizontal band. Data points are uniformly drawn from the bands. There
are 3 labeled points and 178 unlabeled points. 1-nearest-neighbor algorithm, one of
the standard supervised learning methods, ignores the unlabeled data and thus the

8

3.5

3

2.5

2

1.5

1

0.5

0

0

CHAPTER2. LABELPROPAGATION

3.5

3

2.5

2

1.5

1

0.5

3.5

3

2.5

2

1.5

1

0.5

0.5

1

1.5

2

2.5

3

3.5

0

0

0.5

1

1.5

2

2.5

3

3.5

0

0

0.5

1

1.5

2

2.5

3

3.5

(a) The data

(b) 1NN

(c) Label Propagation

Figure 2.1: The Three Bands dataset. Labeled data are marked with color symbols,
and unlabeled data are black dots in (a). 1NN ignores unlabeled data structure (b),
while Label Propagation takes advantage of it (c).

band structure (b). On the other hand, the Label Propagation algorithm takes into
account the unlabeled data (c). It propagates labels along the bands. In this exam-
ple, we used  = 0.22 from the minimum spanning tree heuristic (see Chapter 7).

Figure 2.2 shows a synthetic dataset with two classes as intertwined three-
dimensional spirals. There are 2 labeled points and 184 unlabeled points. Again,
1NN fails to notice the structure of unlabeled data, while Label Propagation nds
the spirals. We used  = 0.43.

3.5

3

2.5

2

1.5

1

0.5

0
2

3.5

3

2.5

2

1.5

1

0.5

0
2

3.5

3

2.5

2

1.5

1

0.5

0
2

1

0

1

0

1

2

2

(a) The data

2

1

1

0

1

0

1

2

2

(b) 1NN

2

1

1

0

1

0

1

2

2

2

1

(c) Label Propagation

Figure 2.2: The Springs dataset. Again 1NN ignores unlabeled data structure,
while Label Propagation takes advantage of it.

Chapter 3

What is a Good Graph?

In Label Propagation we need a graph , represented by the weight matrix W . How
does one construct a graph? What is a good graph? In this chapter we give several
examples on different datasets. The goal is not to rigorously dene good graphs,
but to illustrate the assumptions behind graph based semi-supervised learning.

A good graph should reect our prior knowledge about the domain. At the
present time, its design is more of an art than science. It is the practitioners respon-
sibility to feed a good graph to graph-based semi-supervised learning algorithms,
in order to expect useful output. The algorithms in this thesis do not deal directly
with the design of graphs (with the exception of Chapter 7).

3.1 Example One: Handwritten Digits

Our rst example is optical character recognition (OCR) for handwritten digits.
The handwritten digits dataset originates from the Cedar Buffalo binary digits
database (Hull, 1994). The digits were initially preprocessed to reduce the size
of each image down to a 16 16 grid by down-sampling and Gaussian smoothing,
with pixel values in 0 to 255 (Le Cun et al., 1990). Figure 3.1 shows a random sam-
ple of the digits. In some of the experiments below they are further scaled down to
8  8 by averaging 2  2 pixel bins.
We show why graphs based on pixel-wise Euclidean distance make sense for
digits semi-supervised learning. Euclidean distance by itself is a bad similarity
measure. For example the two images in Figure 3.2(a) have a large Euclidean
distance although they are in the same class. However Euclidean distance is a
good local similarity measure. If it is small, we can expect the two images to
be in the same class. Consider a k-nearest-neighbor graph based on Euclidean
distance. Neighboring images have small Euclidean distance. With large amount

9

10

CHAPTER3. WHATISAGOODGRAPH?

Figure 3.1: some random samples of the handwritten digits dataset

(a) two images of 2 with large Euclidean distance

(b) a path in an Euclidean distance kNN graph between them

Figure 3.2: Locally similar images propagate labels to globally dissimilar ones.

of unlabeled images of 2s, there will be many paths connecting the two images in
(a). One such path is shown in Figure 3.2(b). Note adjacent pairs are similar to
each other. Although the two images in (a) are not directly connected (not similar
in Euclidean distance), Label Propagation can propagate along the paths, marking
them with the same label.

Figure 3.3 shows a symmetrized 1 2NN graph based on Euclidean distance.
The small dataset has only a few 1s and 2s for clarity. The actual graphs used in
the OCR experiments are too large to show.

It should be mentioned that our focus is on semi-supervised learning methods,
not OCR handwriting recognizers. We could have normalized the image intensity,
or used edge detection or other invariant features instead of Euclidean distance.
These should be used for any real applications, as the graph should represent do-
main knowledge. The same is true for all other tasks described below.

1Symmetrization means we connect nodes i, j if i is in js kNN or vice versa, and therefore a

node can have more than k edges.

3.1. EXAMPLEONE:HANDWRITTENDIGITS

11

Figure 3.3: A symmetrized Euclidean 2NN graph on some 1s and 2s. Label Prop-
agation on this graph works well.

12

CHAPTER3. WHATISAGOODGRAPH?

3.2 Example Two: Document Categorization

Our second example is document categorization on 20 newsgroups dataset 2 . Each
document has no header except From and Subject lines. Each document is
minimally processed into a tf.idf vector, without frequency cutoff, stemming, or
a stopword list. The From and Subject lines are included. We measure the
similarity between two documents u, v with the cosine similarity cs(u, v) = u>v
|u||v|.
Like Euclidean distance, cosine similarity is not a good global measure:
two
documents from the same class can have few common words. However it is a good
local measure.

A graph based on cosine similarity in this domain makes good sense. Docu-
ments from the same thread (class) tend to quote one another, giving them high
cosine similarities. Many paths in the graph are quotations. Even though the rst
and last documents in a thread share few common words, them can be classied in
the same class via the graph.

The full graphs are again too large to visualize. We show the few nearest neigh-
bors of document 60532 in comp.sys.ibm.pc.hardware vs. comp.sys.mac.hardware
sub-dataset in Figure 3.4. The example is typical in the whole graph. Nevertheless
we note that not all edges are due to quotation.

3.3 Example Three: The FreeFoodCam

The Carnegie Mellon University School of Computer Science has a lounge, where
leftover pizza from various meetings converge, to the delight of students. In fact
a webcam (the FreeFoodCam 3) was set up in the lounge, so that people can see
whether food is available. The FreeFoodCam provides interesting research oppor-
tunities. We collect webcam images of 10 people over a period of several months.
The data is used for 10-way people recognition, i.e. identify the name of person in
FreeFoodCam images. The dataset consists of 5254 images with one and only one
person in it. Figure 3.5 shows some random images in the dataset. The task is not
trivial:

1. The images of each person were captured on multiple days during a four
month period. People changed clothes, had hair cut, one person even grew a
beard. We simulate a video surveillance scenario where a person is manually
labeled at rst, and needs to be recognized on later days. Therefore we
choose labeled data within the rst day of a persons appearance, and test on

2http://www.ai.mit.edu/people/jrennie/20Newsgroups/, 18828 version
3http://www-2.cs.cmu.edu/coke/, Carnegie Mellon internal access.

3.3. EXAMPLETHREE:THEFREEFOODCAM

13

From: rash@access.digex.com (Wayne Rash)
Subject: Re: 17" Monitors
mikey@sgi.com (Mike Yang) writes:
>In article <1qslfs$bm1@access.digex.net> rash@access.digex.com (Wayne Rash) writes:
>>I also reviewed a new Nanao, the F550iW, which has just
>>been released.
>Whats the difference between the F550i and the new F550iW? Im
>about to buy a Gateway system and was going to take the F550i
>upgrade. Should I get the F550iW instead?
>-----------------------------------------------------------------------
>
>
The F550iW is optimized for Windows. It powers down when the screen
blanker appears, it powers down with you turn your computer off, and it
meets all of the Swedish standards. Its also protected against EMI from
adjacent monitors.
Personally, I think the F550i is more bang for the buck right now.

Silicon Graphics, Inc.

mikey@sgi.com

415/390-1786

Mike Yang

(a) document 60532. Its nearest neighbors are shown below.

From: mikey@eukanuba.wpd.sgi.com (Mike Yang)
Subject: Re: 17" Monitors
In article <1qulqa$hp2@access.digex.net>, rash@access.digex.com (Wayne Rash) writes:
|> The F550iW is optimized for Windows. It powers down when the screen
|> blanker appears, it powers down with you turn your computer off, and it
|> meets all of the Swedish standards. Its also protected against EMI from
|> adjacent monitors.
Thanks for the info.
|> Personally, I think the F550i is more bang for the buck right now.
How much more does the F550iW cost?
-----------------------------------------------------------------------

Mike Yang

mikey@sgi.com

Silicon Graphics, Inc.

415/390-1786

(b) The nearest neighbor 60538. It quotes a large portion of 60532.

From: rash@access.digex.com (Wayne Rash)
Subject: Re: 17" Monitors
mikey@eukanuba.wpd.sgi.com (Mike Yang) writes:
>In article <1qulqa$hp2@access.digex.net>, rash@access.digex.com (Wayne Rash) writes:
>|> The F550iW is optimized for Windows. It powers down when the screen
>|> blanker appears, it powers down with you turn your computer off, and it
>|> meets all of the Swedish standards. Its also protected against EMI from
>|> adjacent monitors.
>Thanks for the info.
>|> Personally, I think the F550i is more bang for the buck right now.
>How much more does the F550iW cost?
>-----------------------------------------------------------------------
>
>
I think the difference is about 400 dollars, but I could be wrong. These
things change between press time and publication.

Silicon Graphics, Inc.

mikey@sgi.com

415/390-1786

Mike Yang

(c) The 2nd nearest neighbor 60574. It also quotes 60532.

Figure 3.4: (continued on next page)

14

CHAPTER3. WHATISAGOODGRAPH?

From: mikey@sgi.com (Mike Yang)
Subject: Re: 17" Monitors
In article <1qslfs$bm1@access.digex.net> rash@access.digex.com (Wayne Rash) writes:
>I also reviewed a new Nanao, the F550iW, which has just
>been released.
Whats the difference between the F550i and the new F550iW? Im
about to buy a Gateway system and was going to take the F550i
upgrade. Should I get the F550iW instead?
-----------------------------------------------------------------------

Mike Yang

mikey@sgi.com

Silicon Graphics, Inc.

415/390-1786

(d) The 3rd nearest neighbor 60445, quoted by 60532.

From: goyal@utdallas.edu (MOHIT K GOYAL)
Subject: Re: 17" Monitors
>the Mitsubishi. I also reviewed a new Nanao, the F550iW, which has just
>been released. Last year for the May 92 issue of Windows, I reviewed
Do you have the specs for this monitor? What have they changed from the
F550i?
Do you know if their is going to be a new T560i soon? (a T560iW?)
Thanks.

(e) The 4th nearest neighbor 60463. It and 60532 quote the same source.

From: mikey@eukanuba.wpd.sgi.com (Mike Yang)
Subject: Gateway 4DX2-66V update
I just ordered my 4DX2-66V system from Gateway. Thanks for all the net
discussions which helped me decide among all the vendors and options.
Right now, the 4DX2-66V system includes 16MB of RAM. The 8MB upgrade
used to cost an additional $340.
-----------------------------------------------------------------------

Mike Yang

mikey@sgi.com

Silicon Graphics, Inc.

415/390-1786

(f) The 5th nearest neighbor 61165. It has a different subject than 60532, but the

same author signature appears in both.

Figure 3.4: The nearest neighbors of document 60532 in the 20newsgroups dataset,
as measured by cosine similarity. Notice many neighbors either quote or are quoted
by the document. Many also share the same subject line.

3.3. EXAMPLETHREE:THEFREEFOODCAM

15

Figure 3.5: A few FreeFoodCam image examples

the remaining images of the day and all other days. It is harder than testing
only on the same day, or allowing labeled data to come from all days.

2. The FreeFoodCam is a low quality webcam. Each frame is 640  480 so
faces of far away people are small; The frame rate is a little over 0.5 frame
per second; Lighting in the lounge is complex and changing.

3. The person could turn the back to the camera. About one third of the images

have no face.

Since only a few images are labeled, and we have all the test images, it is a
natural task to apply semi-supervised learning techniques. As computer vision is
not the focus of the paper, we use only primitive image processing methods to
extract the following features:

Time. Each image has a time stamp.

Foreground color histogram. A simple background subtraction algorithm is ap-
plied to each image to nd the foreground area. The foreground area is
assumed to be the person (head and body). We compute the color histogram
(hue, saturation and brightness) of the foreground pixels. The histogram is a
100 dimensional vector.

16

CHAPTER3. WHATISAGOODGRAPH?

Face image. We apply a face detector (Schneiderman, 2004b) (Schneiderman,
2004a) to each image. Note it is not a face recognizer (we do not use a
face recognizer for this task). It simply detects the presence of frontal or
prole faces. The output is the estimated center and radius of the detected
face. We take a square area around the center as the face image. If no face is
detected, the face image is empty.

One theme throughout the thesis is that the graph should reect domain knowl-
edge of similarity. The FreeFoodCam is a good example. The nodes in the graph
are all the images. An edge is put between two images by the following criteria:

1. Time edges People normally move around in the lounge in moderate speed,
thus adjacent frames are likely to contain the same person. We represent
this belief in the graph by putting an edge between images i, j whose time
difference is less than a threshold t1 (usually a few seconds).

2. Color edges The color histogram is largely determined by a persons clothes.
We assume people change clothes on different days, so color histogram is
unusable across multiple days. However it is an informative feature during a
shorter time period (t2) like half a day. In the graph for every image i, we nd
the set of images having a time difference between (t1, t2) to i, and connect
i with its kc-nearest-neighbors (in terms of cosine similarity on histograms)
in the set. kc is a small number, e.g. 3.

3. Face edges We resort to face similarity over longer time spans. For every
image i with a face, we nd the set of images more than t2 apart from i,
and connect i with its kf -nearest-neighbor in the set. We use pixel-wise
Euclidean distance between face images (the pair of face images are scaled
to the same size).

The nal graph is the union of the three kinds of edges. The edges are unweighted
in the experiments (one could also learn different weights for different kinds of
edges. For example it might be advantageous to give time edges higher weights).
We used t1 = 2 second, t2 = 12 hours, kc = 3 and kf = 1 below. Incidentally
these parameters give a connected graph. It is impossible to visualize the whole
graph. Instead we show the neighbors of a random node in Figure 3.6.

3.4 Common Ways to Create Graphs

Sometimes one faces a dataset with limited domain knowledge. This section dis-
cusses some common ways to create a graph as a starting point.

3.4. COMMONWAYSTOCREATEGRAPHS

17

image 4005

neighbor 1: time edge

neighbor 2: color edge

neighbor 3: color edge

neighbor 4: color edge

neighbor 5: face edge

Figure 3.6: A random image and its neighbors in the graph

18

CHAPTER3. WHATISAGOODGRAPH?

Fully connected graphs One can create a fully connected graph with an edge be-
tween all pairs of nodes. The graph needs to be weighted so that similar
nodes have large edge weight between them. The advantage of a fully con-
nected graph is in weight learning  with a differentiable weight function,
one can easily take the derivatives of the graph w.r.t. weight hyperparam-
eters. The disadvantage is in computational cost as the graph is dense (al-
though sometimes one can apply fast approximate algorithms like N-body
problems). Furthermore we have observed that empirically fully connect
graphs performs worse than sparse graphs.

Sparse graphs One can create kNN or NN graphs as shown below, where each
node connects to only a few nodes. Such sparse graphs are computationally
fast. They also tend to enjoy good empirical performance. We surmise it
is because spurious connections between dissimilar nodes (which tend to be
in different classes) are removed. With sparse graphs, the edges can be un-
weighted or weighted. One disadvantage is weight learning  a change in
weight hyperparameters will likely change the neighborhood, making opti-
mization awkward.

kNN graphs Nodes i, j are connected by an edge if i is in js k-nearest-neighborhood

or vice versa. k is a hyperparameter that controls the density of the graph.
kNN has the nice property of adaptive scales, because the neighborhood
radius is different in low and high data density regions. Small k may re-
sult in disconnected graphs. For Label Propagation this is not a problem if
each connected component has some labeled points. For other algorithms
introduced later in the thesis, one can smooth the Laplacian.

NN graphs Nodes i, j are connected by an edge, if the distance d(i, j)  . The
hyperparameter  controls neighborhood radius. Although  is continuous,
the search for the optimal value is discrete, with at most O(n2) values (the
edge lengths in the graph).

tanh-weighted graphs wij = (tanh(1(d(i, j)  2)) + 1)/2. The hyperbolic
tangent function is a soft step function that simulates NN in that when
d(i, j) (cid:29) 2, wij  0; d(i, j) (cid:28) 2, wij  1. The hyperparameters 1, 2
controls the slope and cutoff value respectively. The intuition is to create a
soft cutoff around distance 2, so that close examples (presumably from the
same class) are connected and examples from different classes (presumably
with large distance) are nearly disconnected. Unlike NN, tanh-weighted
graph is continuous with respect to 1, 2 and is amenable to learning with
gradient methods.

3.4. COMMONWAYSTOCREATEGRAPHS

19

exp-weighted graphs wij = exp(d(i, j)2/2). Again this is a continuous weight-
ing scheme, but the cutoff is not as clear as tanh(). Hyperparameter 
controls the decay rate. If d is e.g. Euclidean distance, one can have one
hyperparameter per feature dimension.

These weight functions are all potentially useful when we do not have enough do-
main knowledge. However we observed that weighted kNN graphs with a small k
tend to perform well empirically. All the graph construction methods have hyper-
parameters. We will discuss graph hyperparameter learning in Chapter 7.

A graph is represented by the n  n weight matrix W , wij = 0 if there is
no edge between node i, j. We point out that W does not have to be positive
semi-denite. Nor need it satisfy metric conditions. As long as W s entries are
non-negative and symmetric, the graph Laplacian, an important quantity dened in
the next chapter, will be well dened and positive semi-denite.

20

CHAPTER3. WHATISAGOODGRAPH?

Chapter 4

Gaussian Random Fields and
Harmonic Functions

In this chapter we formalize label propagation with a probabilistic framework.
Without loss of generality we assume binary classication y  {0, 1}. We as-
sume the n  n weight matrix W is given, which denes the graph. W has to be
symmetric with non-negative entries, but otherwise need not to be positive semi-
denite. Intuitively W species the local similarity between points. Our task is
to assign labels to unlabeled nodes.

4.1 Gaussian Random Fields

Our strategy is to dene a continuous random eld on the graph. First we dene
a real function over the nodes f : L  U  R. Notice f can be negative or
larger than 1. Intuitively, we want unlabeled points that are similar (as determined
by edge weights) to have similar labels. This motivates the choice of the quadratic
energy function

E(f ) =

1

2Xi,j

wij (f (i)  f (j))2

(4.1)

Obviously E is minimized by constant functions. But since we have observed some
labeled data, we constrain f to take values f (i) = yi, i  L on the labeled data.
We assign a probability distribution to functions f by a Gaussian random eld

p(f ) =

1
Z

eE(f )

21

(4.2)

22

CHAPTER4. GAUSSIANRANDOMFIELDS

where  is an inverse temperature parameter, and Z is the partition function

Z =ZfL=YL

exp (E(f )) df

(4.3)

terested in the inference problem p(fi|YL), i  U, or the meanR 

which normalizes over functions constrained to YL on the labeled data. We are in-
 fip(fi|YL) dfi.
The distribution p(f ) is very similar to a standard Markov Random eld with
discrete states (the Ising model, or Boltzmann machines (Zhu & Ghahramani,
2002b)). In fact the only difference is the relaxation to real-valued states. However
this relaxation greatly simplify the inference problem. Because of the quadratic
energy, p(f ) and p(fU|YL) are both multivariate Gaussian distributions. This is
why p is called a Gaussian random eld. The marginals p(fi|YL) are univariate
Gaussian too, and have closed form solutions.

4.2 The Graph Laplacian

  D  W

(4.4)

We now introduce an important quantity: the combinatorial Laplacian . Let D

be the diagonal degree matrix, where Dii =Pj Wij is the degree of node i. The

Laplacian is dened as

For the time being the Laplacian is useful shorthand for the energy function: One
can verify that

E(f ) =

1

2Xi,j

wij (f (i)  f (j))2 = f >f

The Gaussian random eld can be written as

p(f ) =

ef >f

1
Z

(4.5)

(4.6)

where the quadratic form becomes obvious.  plays the role of the precision (in-
verse covariance) matrix in a multivariate Gaussian distribution. It is always pos-
itive semi-denite if W is symmetric and non-negative. The Laplacian will be
further explored in later chapters.

4.3 Harmonic Functions

It is not difcult to show that the minimum energy function f = arg minfL=YL
E(f )
is harmonic; namely, it satises f = 0 on unlabeled data points U, and is equal
to YL on the labeled data points L. We use h to represent this harmonic function.

The harmonic solution h = 0 subject to hL = YL is given by

W =(cid:20) WLL WLU
WU L WU U (cid:21)

hU = (DU U  WU U )1WU LYL

= (U U )1U LYL
= (I  PU U )1PU LYL

(4.8)

(4.9)
(4.10)
(4.11)

4.4. INTERPRETATIONANDCONNECTIONS

23

The harmonic property means that the value of h(i) at each unlabeled data

point i is the average of its neighbors in the graph:

h(i) =

1

DiiXji

wijh(j), for i  U

(4.7)

which is consistent with our prior notion of smoothness with respect to the graph.
Because of the maximum principle of harmonic functions (Doyle & Snell, 1984),
h is unique and satises 0  h(i)  1 for i  U (remember h(i) = 0 or 1 for
i  L).
To compute the harmonic solution, we partition the weight matrix W (and
similarly D, , etc.) into 4 blocks for L and U:

The last representation is the same as equation (2.11), where P = D1W is the
transition matrix on the graph. The Label Propagation algorithm in Chapter 2 in
fact computes the harmonic function.

The harmonic function minimizes the energy and is thus the mode of (4.2).
Since (4.2) denes a Gaussian distribution which is symmetric and unimodal, the
mode is also the mean.

4.4 Interpretation and Connections

The harmonic function can be viewed in several fundamentally different ways, and
these different viewpoints provide a rich and complementary set of techniques for
reasoning about this approach to the semi-supervised learning problem.

4.4.1 Random Walks

Imagine a random walk on the graph. Starting from an unlabeled node i, we move
to a node j with probability Pij after one step. The walk stops when we hit a
labeled node. Then h(i) is the probability that the random walk, starting from
node i, hits a labeled node with label 1. Here the labeled data is viewed as an
absorbing boundary for the random walk. The random walk interpretation is
shown in Figure 4.1.

24

CHAPTER4. GAUSSIANRANDOMFIELDS

1

0

i

Figure 4.1: Harmonic function as random walk on the graph

R  =ij

1
wij

1

0

+1 volt

Figure 4.2: Harmonic function as electric network graph

4.4.2 Electric Networks

We can also view the framework as electrical networks. Imagine the edges of the
graph to be resistors with conductance W . Equivalently the resistance between
nodes i, j is 1/wij. We connect positive labeled nodes to a +1 volt source, and
negative labeled nodes to the ground. Then hU is the voltage in the resulting elec-
tric network on each of the unlabeled nodes (Figure 4.2). Furthermore hU min-
imizes the energy dissipation, in the form of heat, of the electric network. The
energy dissipation is exactly E(h) as in (4.1). The harmonic property here follows
from Kirchoffs and Ohms laws, and the maximum principle then shows that this
is precisely the same solution obtained in (4.11).

4.4.3 Graph Mincut

The harmonic function can be viewed as a soft version of the graph mincut ap-
proach by Blum and Chawla (2001). In graph mincut the problem is cast as one

4.5. INCORPORATINGCLASSPROPORTIONKNOWLEDGE

25

of nding a minimum st-cut. The minimum st-cuts minimize the same energy
function (4.1) but with discrete labels 0,1. Therefore they are the modes of a stan-
dard Boltzmann machine. It is difcult to compute the mean. One often has to use
Monte Carlo Markov Chain or use approximation methods. Furthermore, the min-
imum st-cut is not necessarily unique. For example, consider a linear chain graph
with n nodes. Let wi,i+1 = 1 and other edges zero. Let node 1 be labeled positive,
node n negative. Then a cut on any one edge is a minimum st-cut. In contrast, the
harmonic solution has a closed form, unique solution for the mean, which is also
the mode.

The Gaussian random elds and harmonic functions also have connection to
graph spectral clustering, and kernel regularization. These will be discussed later.

4.5 Incorporating Class Proportion Knowledge

To go from f to class labels, the obvious decision rule is to assign label 1 to node
i if h(i) > 0.5, and label 0 otherwise. We call this rule 0.5-threshold. In terms
of the random walk interpretation if h(i) > 0.5, then starting at i, the random
walk is more likely to reach a positively labeled point before a negatively labeled
point. This decision rule works well when the classes are well separated. However
in practice, 0.5-threshold tends to produce unbalanced classication (most points
in one of the classes). The problem stems from the fact that W , which species
the data manifold, is often poorly estimated in practice and does not reect the
classication goal. In other words, we should not fully trust the graph structure.
Often we have the knowledge of class proportions, i.e. how many unlabeled
data are from class 0 and 1 respectively. This can either be estimated from the
labeled set, or given by domain experts. This is a valuable piece of complementary
information.

We propose a heuristic method called class mass normalization (CMN) to in-
corporate the information as follows. Lets assume the desirable proportions for
classes 1 and 0 are q and 1  q respectively. Dene the mass of class 1 to be
Pi hU (i), and the mass of class 0 to bePi(1  hU (i)). Class mass normalization
scales these masses to match q and 1  q. In particular an unlabeled point i is
classied as class 1 iff

q

> (1  q)

(4.12)

hU (i)

Pi hU (i)

1  hU (i)

Pi(1  hU (i))

CMN extends naturally to the general multi-label case. It is interesting to note
CMNs potential connection to the procedures in (Belkin et al., 2004a). Further
research is needed to study whether the heuristic (or its variation) can be justied
in theory.

26

CHAPTER4. GAUSSIANRANDOMFIELDS

4.6 Incorporating Vertex Potentials on Unlabeled Instances

We can incorporate the knowledge on individual class label of unlabeled instances
too. This is similar to using a assignment cost for each unlabeled instance. For
example, the external knowledge may come from an external classier which is
constructed on labeled data alone (It could come from domain expert too). The
external classier produces labels gU on the unlabeled data; g can be 0/1 or soft
labels in [0, 1]. We combine g with the harmonic function h by a simple modi-
cation of the graph. For each unlabeled node i in the original graph, we attach a
dongle node which is a labeled node with value gi. Let the transition probabil-
ity from i to its dongle be , and discount other transitions from i by 1  . We
then compute the harmonic function on this augmented graph. Thus, the external
classier introduces assignment costs to the energy function, which play the role
of vertex potentials in the random eld. It is not difcult to show that the harmonic
solution on the augmented graph is, in the random walk view,

hU = (I  (1  )PU U )1 ((1  )PU LYL + gU )

(4.13)

We note that up to now we have assumed the labeled data to be noise free, and
so clamping their values makes sense. If there is reason to doubt this assumption,
it would be reasonable to attach dongles to labeled nodes as well, and to move the
labels to these dongles. An alternative is to use Gaussian process classiers with a
noise model, which will be discussed in Chapter 6.

4.7 Experimental Results

We evaluate harmonic functions on the following tasks. For each task, we gradually
increase the labeled set size systematically. For each labeled set size, we perform
30 random trials. In each trial we randomly sample a labeled set with the specic
size (except for the Freefoodcam task where we sample labeled set from the rst
day only). However if a class is missing from the sampled labeled set, we redo the
random sampling. We use the remaining data as the unlabeled set and report the
classication accuracy with harmonic functions on them.

To compare the harmonic function solution against a standard supervised learn-
ing method, we use a Matlab implementation of SVM (Gunn, 1997) as the baseline.
Notice the SVMs are not semi-supervised: the unlabeled data are merely used as
test data. For c-class multiclass problems, we use a one-against-all scheme which
creates c binary subproblems, one for each class against the rest classes, and select
the class with the largest margin. We use 3 standard kernels for each task: linear
K(i, j) = hxi, xji, quadratic K(i, j) = (hxi, xji + 1)2, and radial basis function

4.7. EXPERIMENTALRESULTS

27

(RBF) K(i, j) = exp(cid:0)kxi  xjk2/22(cid:1). The slack variable upper bound (usu-

ally denoted by C) for each kernel, as well as the bandwidth  for RBF, are tuned
by 5 fold cross validation for each task.

1. 1 vs. 2. Binary classication for OCR handwritten digits 1 vs. 2. This
is a subset of the handwritten digits dataset. There are 2200 images, half are
1s and the other half are 2s.

The graph (or equivalently the weight matrix W ) is the single most important
input to the harmonic algorithm. To demonstrate its importance, we show the
results of not one but six related graphs:

(a) 16  16 full. Each digit image is 16  16 gray scale with pixel values
between 0 and 255. The graph is fully connected, and the weights
decrease exponentially with Euclidean distance:

wij = exp 

256Xd=1

(xi,d  xj,d)2

3802

!

(4.14)

The parameter 380 is chosen by evidence maximization (see Section
7.1). This was the graph used in (Zhu et al., 2003a).

(b) 16 16 10NN weighted. Same as 16 16 full, but i, j are connected
only if i is in js 10-nearest-neighbor or vice versa. Other edges are re-
moved. The weights on the surviving edges are unchanged. Therefore
this is a much sparser graph. The number 10 is chosen arbitrarily and
not tuned for semi-supervised learning.

(c) 16  16 10NN unweighted. Same as 16  16 10NN weighted except
that the weights on the surviving edges are all set to 1. This represents
a further simplication of prior knowledge.

(d) 8  8 full. All images are down sampled to 8  8 by averaging 2  2
pixel bins. Lowering resolution helps to make Euclidean distance less
sensitive to small spatial variations. The graph is fully connected with
weights

wij = exp 

j,d)2

(x0

i,d  x0
1402

64Xd=1

!

(4.15)

(e) 8  8 10NN weighted. Similar to 16  16 10NN weighted.
(f) 8  8 10NN unweighted. Ditto.

28

CHAPTER4. GAUSSIANRANDOMFIELDS

The classication accuracy with these graphs are shown in Figure 4.3(a).
Different graphs give very different accuracies. This should be a reminder
that the quality of the graph determines the performance of harmonic func-
tion (as well as semi-supervised learning methods based on graphs in gen-
eral). 8  8 seems to be better than 16  16. Sparser graphs are better than
fully connected graphs. The better graphs outperform SVM baselines when
labeled set size is not too small.

2. ten digits. 10-class classication for 4000 OCR handwritten digit images.
The class proportions are intentionally chosen to be skewed, with 213, 129,
100, 754, 970, 275, 585, 166, 353, and 455 images for digits 1,2,3,4,5,6,7,8,9,0
respectively. We use 6 graphs constructed similarly as in 1 vs. 2. Figure
4.3(b) shows the result, which is similar to 1 vs. 2 except the overall accu-
racy is lower.

3. odd vs. even. Binary classication for OCR handwritten digits 1,3,5,7,9
vs. 0,2,4,6,8. Each digit has 400 images, i.e. 2000 per class and 4000 total.
We show only the 8  8 graphs in Figure 4.3(c), which do not outperform
the baseline.

4. baseball vs. hockey Binary document classication for rec.sport.baseball
vs. rec.sport.hockey in the 20newsgroups dataset (18828 version). The pro-
cessing of documents into tf.idf vectors has been described in section 3.2.
The classes have 994 and 999 documents respectively. We report the results
of three graphs in Figure 4.3(d):

(a) full. A fully connected graph with weights

wij = exp(cid:18)

1

0.03(cid:18)1  hdi, dji
|di||dj|(cid:19)(cid:19)

(4.16)

so that the weights decreases with the cosine similarity between docu-
ment di, dj.

(b) 10NN weighted. Only symmetrized 10-nearest-neighbor edges are kept
in the graph, with the same weights above. This was the graph in (Zhu
et al., 2003a).

(c) 10NN unweighted. Same as above except all weights are set to 1.

5. PC vs. MAC Binary classication on comp.sys.ibm.pc.hardware (number
of documents 982) vs. comp.sys.mac.hardware (961) in the 20 newsgroups
dataset. The three graphs are constructed in the same way as baseball vs.
hockey. See Figure 4.3(e).

4.7. EXPERIMENTALRESULTS

29

6. religion vs. atheism Binary classication on talk.religion.misc (628) vs.
alt.atheism (799). See Figure 4.3(f). The three 20newsgroups tasks have
increasing difculty.

7. isolet This is the ISOLET dataset from the UCI data repository (Blake &
Merz, 1998). It is a 26-class classication problem for isolated spoken En-
glish letter recognition. There are 7797 instances. We use the Euclidean
distance on raw features, and create a 100NN unweighted graph. The result
is in Figure 4.3(g).

8. freefoodcam The details of the dataset and graph construction are discussed
in section 3.3. The experiments need special treatment compared to other
datasets. Since we want to recognize people across multiple days, we only
sample the labeled set from the rst days of a persons appearance. This is
harder and more realistic than sampling labeled set from the whole dataset.
We show two graphs in Figure 4.3(h), one with t1 = 2 seconds, t2 = 12
hours, kc = 3, kf = 1, the other the same except kc = 1.
The kernel for SVM baseline is optimized differently as well. We use an
interpolated linear kernel K(i, j) = wtKt(i, j) + wcKc(i, j) + wf Kf (i, j),
where Kt, Kc, Kf are linear kernels (inner products) on time stamp, color
histogram, and face sub-image (normalized to 50  50 pixels) respectively.
If an image i contains no face, we dene Kf (i,) = 0. The interpolation
weights wt, wc, wf are optimized with cross validation.

The experiments demonstrate that the performance of harmonic function varies
considerably depending on the graphs. With certain graphs, the semi-supervised
learning method outperforms SVM, a standard supervised learning method. In par-
ticular sparse nearest-neighbor graphs, even unweighted, tend to outperform fully
connected graphs. We believe the reason is that in fully connected graphs the edges
between different classes, even with relatively small weights, create unwarrantedly
strong connections across the classes. This highlights the sensitivity to the graph
in graph-based semi-supervised learning methods.

It is also apparent from the results that the benet of semi-supervised learn-
ing deminishes as the labeled set size grows. This suggests that semi-supervised
learning is most helpful when the cost of getting labels is prohibitive.

CMN: Incorporating Class Proportion Knowledge

The harmonic function accuracy can be signicantly improved, if we incorporate
class proportion knowledge with the simple CMN heuristic. The class proportion is
estimated from labeled data with Laplace (add one) smoothing. All the graphs and

30

CHAPTER4. GAUSSIANRANDOMFIELDS

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1 vs. 2, harmonic function

ten digits, harmonic function

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

8x8 10NN weighted
8x8 10NN unweighted
16x16 10NN unweighted
8x8 full
16x16 10NN weighted
16x16 full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(a) 1 vs. 2

ten digits, harmonic function

8x8 10NN weighted
8x8 10NN unweighted
8x8 full
SVM RBF
SVM linear
SVM quadratic

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

10

20

30

40

50

60

70

80

90

100

labeled set size

(c) odd vs. even

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

8x8 10NN weighted
8x8 10NN unweighted
16x16 10NN unweighted
8x8 full
16x16 10NN weighted
16x16 full
SVM RBF
SVM linear
SVM quadratic

10

20

30

40

50

60

70

80

90

100

labeled set size

(b) ten digits

baseball vs. hockey, harmonic function

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(d) baseball vs. hockey

Figure 4.3: harmonic function accuracy

4.7. EXPERIMENTALRESULTS

31

PC vs. MAC, harmonic function

religion vs. atheism, harmonic function

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

y
c
a
r
u
c
c
a


t

l

e
s

d
e
e
b
a
n
u

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

5

10

15

20

25

30

35

40

45

50

labeled set size

(e) PC vs. MAC

isolet, harmonic function

100NN unweighted
SVM RBF
SVM linear
SVM quadratic

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(f) religion vs. atheism

freefoodcam, harmonic function

t1=2sec,t2=12hr,kc=3,kf=1
t1=2sec,t2=12hr,kc=1,kf=1
SVM linear

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

60

70

80

90

100

110
labeled set size

120

130

140

150

0
20

40

60

80

100
120
labeled set size

140

160

180

200

(g) isolet

(h) freefoodcam

Figure 4.3: harmonic function accuracy (continued)

32

CHAPTER4. GAUSSIANRANDOMFIELDS

other settings are the same as in section 4.7. The CMN results are shown in Figure
4.4. Compared to Figure 4.3 we see that in most cases CMN helps to improve
accuracy.

For several tasks, CMN gives a huge improvement for the smallest labeled set
size. The improvement is so large that the curves become V shaped at the left
hand side. This is an artifact: we often use the number of classes as the smallest
labeled set size. Because of our sampling method, there will be one instance from
each class in the labeled set. The CMN class proportion estimation is thus uniform.
Incidentally, many datasets have close to uniform class proportions. Therefore the
CMN class proportion estimation is close to the truth for the smallest labeled set
size, and produces large improvement. On the other hand, intermediate labeled set
size tends to give the worst class proportion estimates and hence little improve-
ment.

In conclusion, it is important to incorporate class proportion knowledge to as-
sist semi-supervised learning. However for clarity, CMN is not used in the remain-
ing experiments.

Dongles: Incorporating External Classier

We use the odd vs. even task, where the RBF SVM baseline is sometimes better
than the harmonic function with a 10NN unweighted graph. We augment the graph
with a dongle on each unlabeled node. We use the hard (0/1) labels from the RBF
SVM (Figure 4.3) on the dongles. The dongle transition probability  is set to
0.1 by cross validation. As before, we experiment on different labeled set sizes,
and 30 random trials per size. In Figure 4.5, we compare the average accuracy of
incorporating the external classier (dongle) to the external classier (SVM) or the
harmonic function (harmonic) alone. The combination results in higher accuracy
than either method alone, suggesting there is complementary information used by
each.

4.7. EXPERIMENTALRESULTS

33

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1 vs. 2, harmonic function + CMN

ten digits, harmonic function + CMN

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

8x8 10NN weighted
8x8 10NN unweighted
16x16 10NN unweighted
8x8 full
16x16 10NN weighted
16x16 full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(a) 1 vs. 2

ten digits, harmonic function + CMN

8x8 10NN weighted
8x8 10NN unweighted
8x8 full
SVM RBF
SVM linear
SVM quadratic

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

10

20

30

40

50

60

70

80

90

100

labeled set size

(c) odd vs. even

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

8x8 10NN weighted
8x8 10NN unweighted
16x16 10NN unweighted
8x8 full
16x16 10NN weighted
16x16 full
SVM RBF
SVM linear
SVM quadratic

10

20

30

40

50

60

70

80

90

100

labeled set size

(b) ten digits

baseball vs. hockey, harmonic function + CMN

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(d) baseball vs. hockey

Figure 4.4: CMN accuracy

34

CHAPTER4. GAUSSIANRANDOMFIELDS

PC vs. MAC, harmonic function + CMN

religion vs. atheism, harmonic function + CMN

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

y
c
a
r
u
c
c
a


t

l

e
s

d
e
e
b
a
n
u

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(e) PC vs. MAC

isolet, harmonic function + CMN

100NN unweighted
SVM RBF
SVM linear
SVM quadratic

10NN weighted
10NN unweighted
full
SVM RBF
SVM linear
SVM quadratic

5

10

15

20

25

30

35

40

45

50

labeled set size

(f) religion vs. atheism

freefoodcam, harmonic function + CMN

t1=2sec,t2=12hr,kc=3,kf=1
t1=2sec,t2=12hr,kc=1,kf=1
SVM linear

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

60

70

80

90

100

110
labeled set size

120

130

140

150

0
20

40

60

80

100
120
labeled set size

140

160

180

200

(g) isolet

(h) freefoodcam

Figure 4.4: CMN accuracy (continued)

y
c
a
r
u
c
c
a

t

l

e
s

d
e
e
b
a
n
u

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

dongle
SVM
harmonic

10

20

30

40

50

60

70

80

90

100

labeled set size

Figure 4.5: Incorporating external classier with dongles

Chapter 5

Active Learning

In this chapter, we take a brief detour to look at the active learning problem. We
combine semi-supervised learning and active learning naturally and efciently.

5.1 Combining Semi-Supervised and Active Learning

So far, we assumed the labeled data set is given and xed. In practice, it may make
sense to utilize active learning in conjunction with semi-supervised learning. That
is, we might allow the learning algorithm to pick unlabeled instances to be labeled
by a domain expert. The expert returns the label, which will then be used as (or to
augment) the labeled data set. In other words, if we have to label a few instances
for semi-supervised learning, it may be attractive to let the learning algorithm tell
us which instances to label, rather than selecting them randomly. We will limit the
range of query selection to the unlabeled data set, a practice known as pool-based
active learning or selective sampling.

There has been a great deal of research in active learning. For example, Tong
and Koller (2000) select queries to minimize the version space size for support
vector machines; Cohn et al. (1996) minimize the variance component of the esti-
mated generalization error; Freund et al. (1997) employ a committee of classiers,
and query a point whenever the committee members disagree. Most of the active
learning methods do not take further advantage of the large amount of unlabeled
data once the queries are selected. The work by McCallum and Nigam (1998b)
is an exception, where EM with unlabeled data is integrated into active learning.
Another exception is (Muslea et al., 2002), which uses a semi-supervised learning
method during training. In addition to this body of work from the machine learning
community, there is a large literature on the closely related topic of experimental
design in statistics; Chaloner and Verdinelli (1995) give a survey of experimental

35

36

CHAPTER5. ACTIVELEARNING

design from a Bayesian perspective.

The Gaussian random elds and harmonic functions framework allows a nat-
ural combination of active learning and semi-supervised learning.
In brief, the
framework allows one to efciently estimate the expected generalization error af-
ter querying a point, which leads to a better query selection criterion than naively
selecting the point with maximum label ambiguity. Then, once the queries are se-
lected and added to the labeled data set, the classier can be trained using both the
labeled and remaining unlabeled data. Minimizing the estimated generalization er-
ror was rst proposed by Roy and McCallum (2001). We independently discovered
the same idea (Zhu et al., 2003b), and the effective combination of semi-supervised
learning and active learning is novel.

We perform active learning with the Gaussian random eld model by greedily
selecting queries from the unlabeled data to minimize the risk of the harmonic
energy minimization function. The risk is the estimated generalization error of the
Bayes classier, and can be computed with matrix methods. We dene the true
risk R(h) of the Bayes classier based on the harmonic function h to be

R(h) =

[sgn(hi) 6= yi] p(yi)

nXi=1 Xyi=0,1

where sgn(hi) is the Bayes decision rule with threshold 0.5, such that (with a slight
abuse of notation) sgn(hi) = 1 if hi > 0.5 and sgn(hi) = 0 otherwise. Here p(yi)
is the unknown true label distribution at node i, given the labeled data. Because of
this, R(h) is not computable. In order to proceed, it is necessary to make assump-
tions. We begin by assuming that we can estimate the unknown distribution p(yi)
with the mean of the Gaussian eld model:

p(yi = 1)  hi

Intuitively, recalling hi is the probability of reaching 1 in a random walk on the
graph, our assumption is that we can approximate the distribution using a biased
coin at each node, whose probability of heads is hi. With this assumption, we can

[sgn(hi) 6= 0] (1  hi) + [sgn(hi) 6= 1] hi

min(hi, 1  hi)

(5.1)

compute the estimated risk bR(h) as

bR(h) =

=

nXi=1
nXi=1

If we perform active learning and query an unlabeled node k, we will receive an
answer yk (0 or 1). Adding this point to the training set and retraining, the Gaussian

5.1. COMBININGSEMI-SUPERVISEDANDACTIVELEARNING

37

eld and its mean function will of course change. We denote the new harmonic
function by h+(xk,yk). The estimated risk will also change:

bR(h+(xk,yk)) =

nXi=1

min(h+(xk,yk)

i

, 1  h+(xk,yk)

i

)

Since we do not know what answer yk we will receive, we again assume the proba-
bility of receiving answer p(yk = 1) is approximately hk. The expected estimated
risk after querying node k is therefore

bR(h+xk ) = (1  hk) bR(h+(xk,0)) + hk bR(h+(xk,1))

The active learning criterion we use in this paper is the greedy procedure of choos-
ing the next query k that minimizes the expected estimated risk:

(5.2)

k = arg mink0bR(h+xk0 )

To carry out this procedure, we need to compute the harmonic function h+(xk,yk)
after adding (xk, yk) to the current labeled training set. This is the retraining prob-
lem and is computationally intensive in general. However for Gaussian elds and
harmonic functions, there is an efcient way to retrain. Recall that the harmonic
function solution is

hU = 1

U U U LYL

What is the solution if we x the value yk for node k? This is the same as nding
the conditional distribution of all unlabeled nodes, given the value of yk. In Gaus-
sian elds the conditional on unlabeled data is multivariate Normal distributions
N (hU , 1
U U ). A standard result (a derivation is given in Appendix A) gives the
mean of the conditional once we x yk:

h+(xk,yk)
U

= hU + (yk  hk)

(1
(1

U U )k
U U )kk

where (1
U U )k is the k-th column of the inverse Laplacian on unlabeled data,
and (1
U U )kk is the k-th diagonal element of the same matrix. Both are already
computed when we compute the harmonic function h. This is a linear computation
and therefore can be carried out efciently.

To summarize, the active learning algorithm is shown in Figure 5.1. The time
complexity to nd the best query is O(n2). As a nal word on computational
efciency, we note that after adding query xk and its answer to L, in the next
iteration we will need to compute ((U U )k)1, the inverse of the Laplacian on
unlabeled data, with the row/column for xk removed. Instead of naively taking the
inverse, there are efcient algorithms to compute it from (U U )1; a derivation is
given in Appendix B.

38

CHAPTER5. ACTIVELEARNING

Input: L, U, weight matrix W
While more labeled data required:

Compute harmonic h using (4.11)
Find best query k using (5.2)
Query point xk, receive answer yk
Add (xk, yk) to L, remove xk from U

end
Output: L and classier h.

Figure 5.1: The active learning algorithm

2.5

2

1.5

1

0.5

0

0.5

1
1.5

a

1

0

B

1

0.5

0

0.5

1

1.5

Figure 5.2: Entropy Minimization selects the most uncertain point a as the next
query. Our method will select a point in B, a better choice.

5.2 Why not Entropy Minimization

We used the estimated generalization error to select queries. A different query
selection criterion, entropy minimization (or selecting the most uncertain instance),
has been suggested in some papers. We next show why it is inappropriate when
the loss function is based on individual instances. Such loss functions include the
widely used accuracy for classication and mean squared error for regression.

To illustrate the idea, Figure 5.2 shows a synthetic dataset with two labeled
data (marked 1, 0), an unlabeled point a in the center above and a cluster of 9
unlabeled points B below. B is slighted shifted to the right. The graph is fully
connected with weights wij = exp(d2
ij), where dij is the Euclidean distance be-
tween i, j. In this conguration, we have the most uncertainty in a: the harmonic
function at node a is h(a) = 0.43. Points in B have their harmonic func-

5.3. EXPERIMENTS

39

tion values around 0.32. Therefore entropy minimization will pick a as the query.
However, the risk minimization criterion picks the upper center point (marked with

a star) in B to query, instead of a. In fact the estimated risk is bR(a) = 2.9, and
bR(b  B)  1.1. Intuitively knowing the label of one point in B let us know the

label of all points in B, which is a larger gain. Entropy minimization is worse than
risk minimization in this example.

The root of the problem is that entropy does not account for the loss of mak-
ing a large number of correlated mistakes.
In a pool-based incremental active
learning setting, given the current unlabeled set U, entropy minimization nds the
query q  U such that the conditional entropy H(U \ q|q) is minimized. As
H(U \ q|q) = H(U )  H(q), it amounts to selecting q with the largest entropy,
or the most ambiguous unlabeled point as the query. Consider another example
where U = {a, b1, . . . , b100}. Let P (a = +) = P (a = ) = 0.5 and P (bi =
+) = 0.51, P (bi = ) = 0.49 for i = 1 . . . 100. Furthermore let b1 . . . b100 be
perfectly correlated so they always take the same value; Let a and bis be inde-
pendent. Entropy minimization will select a as the next query since H(a) = 1 >
H(bi) = 0.9997. If our goal were to reduce uncertainty about U, such query selec-
tion is good: H(b1 . . . b100|a) = 0.9997 < H(a, b1, . . . , bi1, bi+1, . . . , b100|bi) =
H(a|bi) = 1. However if our loss function is the accuracy on the remaining
instances in U, the picture is quite different. After querying a, P (bi = +) re-
mains at 0.51, so that each bi incurs a Bayes error of 0.49 by always predict
bi = +. The problem is that the individual error adds up, and the overall accuracy
is 0.51 100/100 = 0.51. On the other hand if we query b1, we know the labels of
b2 . . . b100 too because of their perfect correlation. The only error we might make is
on a with Bayes error of 0.5. The overall accuracy is (0.5 + 1  99)/100 = 0.995.
The situation is analogous to speech recognition in which one can measure the
word level accuracy or sentence level accuracy where a sentence is correct if all
words in it are correct. The sentence corresponds to the whole U in our example.
Entropy minimization is more aligned with sentence level accuracy. Nevertheless
since most active learning systems use instance level loss function, it can leads to
suboptimal query choices as we show above.

5.3 Experiments

Figure 5.3 shows a check-board synthetic dataset with 400 points. We expect active
learning to discover the pattern and query a small number of representatives from
each cluster. On the other hand, we expect a much larger number of queries if
queries are randomly selected. We use a fully connected graph with weight wij =
exp(d2
ij/4). We perform 20 random trials. At the beginning of each trial we

40

35

30

25

20

15

10

5

0

0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1
11

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

5

10

15

20

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

25

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

1
1
1
1
1

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

0
0
0
0
0

35

30

100

80

60

40

20

0

k
s
R

i

CHAPTER5. ACTIVELEARNING

Active Learning
Random Query
Most Uncertain Query

5

10

Labeled set size

15

20

y
c
a
r
u
c
c
A

1.1

1

0.9

0.8

0.7

0.6

0.5

0.4

Active Learning
Random Query
Most Uncertain Query

5

10

Labeled set size

15

20

Figure 5.3: A check-board example. Left: dataset and true labels; Center: esti-
mated risk; Right: classication accuracy.

randomly select a positive example and a negative example as the initial training
set. We then run active learning and compare it to two baselines: (1) Random
Query: randomly selecting the next query from U; (2) Most Uncertain Query:
selecting the most uncertain instance in U, i.e. the one with h closest to 0.5. In each
case, we run for 20 iterations (queries). At each iteration, we plot the estimated risk
(5.1) of the selected query (center), and the classication accuracy on U (right).
The error bars are 1 standard deviation, averaged over the random trials. As
expected, with risk minimization active learning we reduce the risk more quickly
than random queries or the most uncertain queries. In fact, risk minimization active
learning with about 15 queries (plus 2 initial random points) learns the correct
concept, which is nearly optimal given that there are 16 clusters. Looking at the
queries, we nd that active learning mostly selects the central points within the
clusters.

Next, we ran the risk minimization active learning method on several tasks
(marked active learning in the plots). We compare it with several alternative ways
of picking queries:

 random query. Randomly select the next query from the unlabeled set.
Classication on the unlabeled set is based on the harmonic function. There-
fore, this method consists of no active learning, but only semi-supervised
learning.

 most uncertain. Pick the most ambiguous point (h closest to 0.5 for binary

problems) as the query. Classication is based on the harmonic function.

 SVM random query. Randomly select the next query from the unlabeled
set. Classication with SVM. This is neither active nor semi-supervised
learning.

 SVM most uncertain. Pick the query closest to the SVM decision boundary.

5.3. EXPERIMENTS

41

one vs. two, active learning

active learning

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

active learning
most uncertain
random query
svm most uncertain
svm random query

10

20

30

40

50

60

70

80

90

100

labeled set size

(b) ten digits

active learning

active learning
most uncertain
random query
svm most uncertain
svm random query

0.5

0

5

10

15

20

25

30
labeled set size

35

40

45

50

y
c
a
r
u
c
c
a


t

l

e
s

d
e
e
b
a
n
u

l

active learning
most uncertain
random query
svm most uncertain
svm random query

0.5

0

5

10

15

20

25

30
labeled set size

35

40

45

50

(a) 1 vs. 2

active learning

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

10

20

30

40

50

60

70

80

90

100

labeled set size

active learning
most uncertain
random query
svm most uncertain
svm random query

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

(c) odd vs. even

(d) baseball vs. hockey

Figure 5.4: Active learning accuracy

Classication with SVM.

For each task, we use the best graph for harmonic functions, and the best kernel
for SVM, as in section 4.7. We run 30 trials and the plots are the average.
In
each trial, we start from a randomly selected labeled set, so that each class has
exactly one labeled example. The query selection methods mentioned above are
used independently to grow the labeled set until a predetermined size. We plot
the classication accuracy on the remaining unlabeled data in Figure 5.4. For the
FreeFoodCam task, there are two experiments: 1. We allow the queries to come
from all days; 2. From only the rst days of a persons rst appearance.

It is interesting to see what queries are selected by different methods. Figures
5.5 and 5.6 compare the rst few queries for the 1 vs. 2 and ten digits tasks. In
each case, the initial labeled set is the same.

The combined semi-supervised learning and risk minimization active learning
method performs well on the tasks. Compared to the results reported in (Roy &

42

CHAPTER5. ACTIVELEARNING

active learning

active learning

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

active learning
most uncertain
random query
svm most uncertain
svm random query

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

active learning
most uncertain
random query
svm most uncertain
svm random query

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

0.5

0

5

10

15

20

25

30
labeled set size

35

40

45

50

0.5

0

5

10

15

20

25

30
labeled set size

35

40

45

50

(e) PC vs. MAC

active learning, queries from all U

(f) religion vs. atheism

active learning, queries from first days only

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

60

70

80

90

100

110

labeled set size

active learning
most uncertain
random query
svm most uncertain
svm random query

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

0

10

20

30

40

50

60

70

80

90

100

110

labeled set size

active learning
most uncertain
random query

(g) freefoodcam, query from all days

(h) freefoodcam, query from the rst days

Figure 5.4: Active learning accuracy (continued)

initial labeled set
active learning
most uncertain
random query
SVM most uncertain

Figure 5.5: The rst few queries selected by different active learning methods on
the 1 vs. 2 task. All methods start with the same initial labeled set.

5.3. EXPERIMENTS

43

initial labeled set
active learning
most uncertain
random query
SVM most uncertain

Figure 5.6: The rst few queries selected by different active learning methods on
the ten digits task. All methods start with the same initial labeled set.

McCallum, 2001), we think that good semi-supervised learning algorithm is a key
to the success of the active learning scheme.

44

CHAPTER5. ACTIVELEARNING

Chapter 6

Connection to Gaussian Processes

A Gaussian process dene a prior p(f (x)) over function values f (x), where x
ranges over an innite input space. It is an extension to an n-dimensional Gaus-
sian distribution as n goes to innity. A Gaussian process is dened by its mean
function (x) (usually taken to be zero everywhere), and a covariance function
C(x, x0). For any nite set of points x1, . . . , xm, the Gaussian process on the
set reduces to an m-dimensional Gaussian distribution with a covariance matrix
Cij = C(xi, xj), for i, j = 1 . . . m. More information can be found in Chapter 45
of (MacKay, 2003).

Gaussian random elds are equivalent to Gaussian processes that are restricted
to a nite set of points. Thus, the standard machineries for Gaussian processes can
be used for semi-supervised learning. Through this connection, we establish the
link between the graph Laplacian and kernel methods in general.

6.1 A Finite Set Gaussian Process Model

Recall for any real-valued function f on the graph, the energy is dened as

E(f ) =

1

2Xi,j

wij (f (i)  f (j))2 = f >f

the corresponding Gaussian random eld is

p(f ) =

1
Z

eE(f ) =

ef >f

1
Z

(6.1)

(6.2)

The Gaussian random eld is nothing but a multivariate Gaussian distribution on
the nodes. Meanwhile a Gaussian process restricted to nite data is a multivariate
Gaussian distribution too (MacKay, 1998). This indicates a connection between

45

46

CHAPTER6. CONNECTIONTOGAUSSIANPROCESSES

Gaussian random elds and nite set Gaussian processes. Notice the nite set
Gaussian processes are not real Gaussian processes, since the kernel matrix is
only dened on L  U, not the whole input space X.
Equation (6.2) can be viewed as a Gaussian process restricted to L  U with
covariance matrix (2)1. However the covariance matrix is an improper prior.
The Laplacian  by denition has a zero eigenvalue with constant eigenvector 1.
To see this note that the degree matrix D is the row sum of W . This makes 
singular: we cannot invert  to get the covariance matrix. To make a proper prior
out of the Laplacian, we can smooth its spectrum to remove the zero eigenvalues,
as suggested in (Smola & Kondor, 2003). In particular, we choose to transform the
eigenvalues  according to the function r() =  + 1/2 where 1/2 is a small
smoothing parameter. This gives the regularized Laplacian

Using the regularized Laplacian, we dene a zero mean prior as

 + I/2

p(f )  exp(cid:18)

1
2

f > f(cid:19)

which corresponds to a kernel with Gram matrix (i.e. covariance matrix)

(6.3)

(6.4)

(6.5)

We note several important aspects of the resulting nite set Gaussian process:

K = 1 =(cid:0)2( + I/2)(cid:1)1

 f  N(cid:16)0, 1(cid:17);
 Unlike ,  gives a proper covariance matrix.
 The parameter  controls the overall sharpness of the distribution; large 

means p(f ) is more peaked around its mean.

 The parameter 2 controls the amount of spectral smoothing; large  smoothes

less.

 The kernel (covariance) matrix K = 1 is the inverse of a function of the
Laplacian . Therefore the covariance between any two point i, j in general
depends on all the points. This is how unlabeled data inuences the prior.

The last point warrants further explanation. In many standard kernels, the entries
are local. For example, in a radial basis function (RBF) kernel K, the matrix entry

kij = exp(cid:16)d2

ij/2(cid:17) only depends on the distance between i, j and not any other

6.2. INCORPORATINGANOISEMODEL

47

points. In this case unlabeled data is useless because the inuence of unlabeled
data in K is marginalized out. In contrast, the entries in kernel (6.4) depends on all
entries in , which in turn depends on all edge weights W . Thus, unlabeled data
will inuence the kernel, which is desirable for semi-supervised learning. Another
way to view the difference is that in RBF (and many other) kernels we parameterize
the covariance matrix directly, while with graph Laplacians we parameterize the
inverse covariance matrix.

6.2 Incorporating a Noise Model

In moving from Gaussian elds to nite set Gaussian processes, we no longer
assume that the soft labels fL for the labeled data are xed at the observed labels
YL. Instead we now assume the data generation process is x  f  y, where
f  y is a noisy label generation process. We use a sigmoid noise model between
the hidden soft labels fi and observed labels yi:

P (yi|fi) =

efiyi

efiyi + efiyi

=

1

1 + e2fiyi

(6.6)

where  is a hyperparameter which controls the steepness of the sigmoid. This
assumption allows us to handle noise in training labels, and is a common practice
in Gaussian process classication.

We are interested in p(YU|YL), the labels for unlabeled data. We rst need to

compute the posterior distribution p(fL, fU|YL). By Bayes theorem,

(6.7)

p(fL, fU|YL) = Ql

i=1 P (yi|fi)p(fL, fU )

P (YL)

Because of the noise model, the posterior is not Gaussian and has no closed form
solution. There are several ways to approximate the posterior. For simplicity we
use the Laplace approximation to nd the approximate p(fL, fU|YL). A deriva-
tion can be found in Appendix C, which largely follows (Herbrich, 2002) (B.7).
Bayesian classication is based on the posterior distribution p(YU|YL). Since un-
der the Laplace approximation this distribution is also Gaussian, the classication
rule depends only on the sign of the mean (which is also the mode) of fU .

6.3 Experiments

We compare the accuracy of Gaussian process classication with the 0.5-threshold
harmonic function (without CMN). To simplify the plots, we use the same graphs

48

CHAPTER6. CONNECTIONTOGAUSSIANPROCESSES

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1 vs. 2, Gaussian field

y
c
a
r
u
c
c
a


t

l

e
s

d
e
e
b
a
n
u

l

5

10

15

20

25

30

35

40

45

50

Gaussian field, 8x8 10NN weighted
harmonic, 8x8 10NN weighted
SVM RBF

labeled set size

(a) 1 vs. 2

ten digits, Gaussian field

Gaussian field, 8x8 10NN weighted
harmonic, 8x8 10NN weighted
SVM RBF

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

10

20

30

40

50

60

70

80

90

100

labeled set size

(c) odd vs. even

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

ten digits, Gaussian field

Gaussian field, 8x8 10NN weighted
harmonic, 8x8 10NN weighted
SVM linear

10

20

30

40

50

60

70

80

90

100

labeled set size

(b) ten digits

baseball vs. hockey, Gaussian field

Gaussian field, 10NN weighted
harmonic, 10NN weighted
SVM RBF

5

10

15

20

25

30

35

40

45

50

labeled set size

(d) baseball vs. hockey

Figure 6.1: Gaussian process accuracy

that give the best harmonic function accuracy (except FreeFoodCam). To aid com-
parison we also show SVMs with the best kernel among linear, quadratic or RBF.
In the experiments, the inverse temperature parameter , smoothing parameter 
and noise model parameter  are tuned with cross validation for each task. The
results are in Figure 6.1.

For FreeFoodCam we also use two other graphs with no face edges at all
(kf = 0). The rst one limits color edges to within 12 hours (t2 = 12 hour), thus
the rst days that contain the labeled data is disconnected from the rest. The second
one allows color edges on far away images (t2 = ). Neither has good accuracy,
indicating that face is an important feature to use.

6.3. EXPERIMENTS

49

PC vs. MAC, Gaussian field

religion vs. atheism, Gaussian field

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

y
c
a
r
u
c
c
a


t

l

e
s

d
e
e
b
a
n
u

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

Gaussian field, 10NN weighted
harmonic, 10NN weighted
SVM RBF

y
c
a
r
u
c
c
a


t

e
s


l

d
e
e
b
a
n
u

l

5

10

15

20

25

30

35

40

45

50

labeled set size

(e) PC vs. MAC

isolet, Gaussian process

Gaussian process, 100NN unweighted
harmonic, 100NN unweighted
SVM linear

y
c
a
r
u
c
c
a

t
e
s

d
e
e
b
a
n
u

l

l

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

Gaussian field, 10NN weighted
harmonic, 10NN weighted
SVM RBF

5

10

15

20

25

30

35

40

45

50

labeled set size

(f) religion vs. atheism

freefoodcam, Gaussian field

Gaussian field, t1=2sec,t2=12hr,kc=3,kf=1
Gaussian field, t1=2sec,t2=12hr,kc=3,kf=0
Gaussian field, t1=2sec,t2=inf,kc=3,kf=0
harmonic, t1=2sec,t2=12hr,kc=3,kf=1
SVM linear

60

70

80

90

100

110
labeled set size

120

130

140

150

0
20

40

60

80

100
120
labeled set size

140

160

180

200

(g) isolet

(h) freefoodcam

Figure 6.1: Gaussian process accuracy (continued)

50

CHAPTER6. CONNECTIONTOGAUSSIANPROCESSES

6.4 Extending to Unseen Data

We have so far restricted ourselves to the L  U nodes in the graph. In this nite
case Gaussian processes are nothing but n-dimensional multivariate normal distri-
butions, and are equivalent to Gaussian random elds. However Gaussian elds,
by denition, cannot handle unseen instances. Any new data points need to be-
come additional nodes in the graph. The Laplacian and kernel matrices need to
be re-computed, which is expensive. We would like to extend the framework to
allow arbitrary new points. Equivalently, this is the problem of induction instead
of transduction.

The simplest strategy is to divide the input space into Voronoi cells. The
Voronoi cells are centered on instances in L  U. We classify any new instance
x by the Voronoi cell it falls into. Let x  L  U be the point closest to x:

x = arg maxzLU wxz

(6.8)

where closeness is measured by weights wxz. From an algorithmic point of view,
we classify x by its 1-nearest-neighbor x. When the unlabeled data size is large,
the approximation is reasonable.

We will discuss more inductive methods in Chapter 10.

Chapter 7

Graph Hyperparameter Learning

Previously we assumed that the weight matrix W is given and xed. In this chapter
we investigate learning the weights from both labeled and unlabeled data. We
present three methods. The rst one is evidence maximization in the context of
Gaussian processes. The second is entropy minimization, and the third one is based
on minimum spanning trees. The latter ones are heuristic but also practical.

7.1 Evidence Maximization

We assume the edge weights are parameterized with hyperparameters . For in-
stance the edge weights can be

wij = exp 

DXd=1

(xi,d  xj,d)2

2
d

!

and  = {1, . . . , D}. To learn the weight hyperparameters in a Gaussian pro-
cess, one can choose the hyperparameters that maximize the log likelihood:  =
arg max log p(yL|). log p(yL|) is known as the evidence and the procedure is
also called evidence maximization . One can also assume a prior on  and nd the
maximum a posteriori (MAP) estimate  = arg max log p(yL|) + log p().
The evidence can be multimodal and usually gradient methods are used to nd a
mode in hyperparameter space. This requires the derivatives  log p(yL|)/. A
complete derivation is given in Appendix D.
In a full Bayesian setup, one would average over all hyperparameter values
(weighted by the posterior p(|yL)) instead of using a point estimate . This
usually involves Markov Chain Monte Carlo techniques, and is not pursued in this
paper.

51

52

CHAPTER7. GRAPHHYPERPARAMETERLEARNING

regularized evidence
before
-24.6
-40.5

after
-23.9
-39.9

accuracy

before
0.973
0.737

after
0.982
0.756

task
1 vs. 2
7 vs. 9

Table 7.1: the regularized evidence and classication before and after learning s
for the two digits recognition tasks

We use binary OCR handwritten digits recognition tasks as our example, since
the results are more interpretable. We choose two tasks: 1 vs. 2 which has been
presented previously, and  7 vs. 9 which are the two most confusing digits in
terms of Euclidean distance. We use fully connected graphs with weights

wij = exp 

64Xd=1

(xi,d  xj,d)2

2
d

!

(7.1)

The hyperparameters are the 64 length scales d for each pixel dimension on 8 8
images. Intuitively they determine which pixel positions are salient for the classi-
cation task: if d is close to zero, a difference at pixel position d will be magnied;
if it is large, pixel position d will be essentially ignored. The weight function
is an extension to eq (4.15) by giving each dimension its own length scale. For
each task there are 2200 images, and we run 10 trials, in each trial we randomly
pick 50 images as the labeled set. The rest is used as unlabeled set. For each
trial we start at i = 140, i = 1 . . . 64, which is the same as in eq (4.15). We
compute the gradients for i for evidence maximization. However since there are
64 hyperparameters and only 50 labeled points, regularization is important. We
use a Normal prior on the hyperparameters which is centered at the initial value:
p(i)  N (140, 302), i = 1 . . . 64. We use a line search algorithm to nd a (pos-
sibly local) optimum for the s.

Table 7.1 shows the regularized evidence and classication before and after
learning s for the two tasks. Figure 7.1 compares the learned hyperparameters
with the mean images of the tasks. Smaller (darker) s correspond to feature
dimensions in which the learning algorithm pays more attention. It is obvious, for
instance in the 7 vs. 9 task, that the learned hyperparameters focus on the gap on
the neck of the image, which is the distinguishing feature between 7s and 9s.

7.2. ENTROPYMINIMIZATION

53

240

220

200

180

160

140

120

100

80

60

250

200

150

100

240

220

200

180

160

140

120

250

200

150

100

141

140.8

140.6

140.4

140.2

140

139.8

139.6

139.4

139.2

139

141

140.8

140.6

140.4

140.2

140

139.8

139.6

139.4

139.2

139

146

144

142

140

138

136

134

132

130

128

126

144

142

140

138

136

134

132

130

128

126

a

b

c

d

Figure 7.1: Graph hyperparameter learning. The upper row is for the 1 vs. 2 task,
and the lower row for 7 vs. 9. The four images are: (a,b) Averaged digit images
for the two classes; (c) The 64 initial length scale hyperparameters , shown as an
8  8 array; (d) Learned hyperparameters.

7.2 Entropy Minimization

Alternatively, we can use average label entropy as a heuristic criterion for parame-
ter learning 1. This heuristic uses only the harmonic function and does not depend
on the Gaussian process setup.

The average label entropy H(h) of the harmonic function h is dened as

H(h) =

1
u

l+uXi=l+1

Hi(h(i))

(7.2)

where Hi(h(i)) = h(i) log h(i)(1h(i)) log(1h(i)) is the Shannon entropy
of individual unlabeled data point i. Here we use the random walk interpretation
of h, relying on the maximum principle of harmonic functions which guarantees
that 0  h(i)  1 for i  U. Small entropy implies that h(i) is close to 0 or 1; this
captures the intuition that a good W (equivalently, a good set of hyperparameters
) should result in a condent labeling. There are of course many arbitrary label-
ings of the data that have low entropy, which might suggest that this criterion will
not work. However, it is important to point out that we are constraining h on the
labeled datamost of these arbitrary low entropy labelings are inconsistent with
this constraint. In fact, we nd that the space of low entropy labelings achievable
by harmonic function is small and lends itself well to tuning the hyperparameters.

1We could have used the estimated risk, cf. Chapter 5. The gradient will be more difcult because

of the min function.

54

CHAPTER7. GRAPHHYPERPARAMETERLEARNING

As an example, let us consider the case where weights are parameterized as
(7.1). We can apply entropy minimization but there is a complication, namely H
has a minimum at 0 as d  0. As the length scale approaches zero, the tail of the
weight function (7.1) is increasingly sensitive to the distance. In the end, the label
predicted for an unlabeled example is dominated by its nearest neighbors label,
which results in the following equivalent labeling procedure: (1) starting from the
labeled data set, nd the unlabeled point xu that is closest to some labeled point
xl; (2) label xu with xls label, put xu in the labeled set and repeat. Since these are
hard labels, the entropy is zero. This solution is desirable only when the classes
are well separated, and is inferior otherwise. This complication can be avoided by
smoothing the transition matrix. Inspired by analysis of the PageRank algorithm
in (Ng et al., 2001b), we smooth the transition matrix P with the uniform matrix
U: Uij = 1/n. The smoothed transition matrix is P = U + (1  ) P .
gradient is computed as

We use gradient descent to nd the hyperparameters d that minimize H. The

H
d

=

1
u

l+uXi=l+1

log(cid:18) 1  h(i)

h(i) (cid:19) h(i)

d

(7.3)

where the values h(i)/d can be read off the vector hU /d, which is given
by

hU
d

= (I  PU U )1   PU U

d

hU +

 PU L
d

YL!

(7.4)

using the fact that dX 1 = X 1(dX)X 1. Both  PU U /d and  PU L/d
are sub-matrices of  P /d = (1  ) P
. Since the original transition matrix P
is obtained by normalizing the weight matrix W , we have that

d

win
d

(7.5)

pij
d

=

wij

d  pijPl+u

n=1
n=1 win

Pl+u

Finally, wij
d

= 2wij(xdi  xdj)2/3
d.

In the above derivation we use hU as label probabilities directly; that is, p(yi =
1) = hU (i). If we incorporate class proportion information, or combine the har-
monic function with other classiers, it makes sense to minimize entropy on the
combined probabilities. For instance, if we incorporate class proportions using
CMN, the probability is given by

h0(i) =

q(u P hU )hU (i)

q(u P hU )hU (i) + (1  q)P hU (1  hU (j))

(7.6)

7.2. ENTROPYMINIMIZATION

55

5

4

3

2

1

0

1

2

3
4

5

4

3

2

1

0

1

2

3
4

2

0
(a)

2

4

1

0.95

0.9

0.85

0.8

0.75

y
p
o
r
t

n
e

2

0
(b)

2

4

0.7

0.2

0.4

0.6

0.8


(c)

=0.1
=0.01
=0.001
=0.0001
unsmoothed

1

1.2

1.4

Figure 7.2: The effect of parameter  on the harmonic function.
(a) If not
smoothed, H  0 as   0, and the algorithm performs poorly. (b) Result at
optimal  = 0.67, smoothed with  = 0.01 (c) Smoothing helps to remove the
entropy minimum.

and we use this probability in place of h(i) in (7.2). The derivation of the gradient
descent rule is a straightforward extension of the above analysis.

We use a toy dataset in Figure 7.2 as an example for Entropy Minimization.
The upper grid is slightly tighter than the lower grid, and they are connected by a
few data points. There are two labeled examples, marked with large symbols. We
learn the optimal length scales for this dataset by minimizing entropy on unlabeled
data.

To simplify the problem, we rst tie the length scales in the two dimensions,
so there is only a single parameter  to learn. As noted earlier, without smoothing,
the entropy approaches the minimum at 0 as   0. Under such conditions,
the harmonic function is usually undesirable, and for this dataset the tighter grid
invades the sparser one as shown in Figure 7.2(a). With smoothing, the nuisance
minimum at 0 gradually disappears as the smoothing factor  grows, as shown
in Figure 7.2(c). When we set  = 0.01, the minimum entropy is 0.898 bits at
 = 0.67. The harmonic function under this length scale is shown in Figure 7.2(b),
which is able to distinguish the structure of the two grids.

If we allow separate s for each dimension, parameter learning is more dra-
matic. With the same smoothing of  = 0.01, x keeps growing toward innity
(we use x = 1016 for computation) while y stabilizes at 0.65, and we reach a
minimum entropy of 0.619 bits. In this case x   is legitimate; it means that
the learning algorithm has identied the x-direction as irrelevant, based on both the
labeled and unlabeled data. The harmonic function under these hyperparameters
gives the same classication as shown in Figure 7.2(b).

56

CHAPTER7. GRAPHHYPERPARAMETERLEARNING

7.3 Minimum Spanning Tree

If the graph edges are exp-weighted with a single hyperparameter  (Section 3.4),
we can set the hyperparameter  with the following heuristic. We construct a
minimum spanning tree over all data points with Kruskals algorithm (Kruskal,
1956). In the beginning no node is connected. During tree growth, the edges are
examined one by one from short to long. An edge is added to the tree if it connects
two separate components. The process repeats until the whole graph is connected.
We nd the rst tree edge that connects two components with different labeled
points in them. We regard the length of this edge d0 as a heuristic to the minimum
distance between different class regions. We then set  = d0/3 following the 3
rule of Normal distribution, so that the weight of this edge is close to 0, with the
hope that local propagation is then mostly within classes.

7.4 Discussion

Other ways to learn the weight hyperparameters are possible. For example one can
try to maximize the kernel alignment to labeled data. This criterion will be used to
learn a spectral transformation from the Laplacian to a graph kernel in Chapter 8.
There the graph weights are xed, and the hyperparameters are the eigenvalues of
the graph kernel. It is possible that one can instead x a spectral transformation but
learn the weight hyperparameters, or better yet jointly learn both. The hope is the
problem can be formulated as convex optimization. This remains future research.

Chapter 8

Kernels from the Spectrum of
Laplacians

We used the inverse of a smoothed Laplacian as kernel matrix in Chapter 6. In
fact, one can construct a whole family of graph kernels from the spectral decom-
position of graph Laplacians. These kernels combine labeled and unlabeled data in
a systematic fashion. In this chapter we devise the best one (in a certain sense) for
semi-supervised learning.

8.1 The Spectrum of Laplacians

i=1 ii>

Let us denote the Laplacian s eigen-decomposition by {i, i}, so that  =
Pn
i . We assume the eigenvalues are sorted in non-decreasing order. The
Laplacian  has many interesting properties (Chung, 1997); For example  has
exactly k zero eigenvalues 1 =  = k = 0, where k is the number of con-
nected subgraphs. The corresponding eigenvectors 1, . . . , k are constant over
the individual subgraphs and zero elsewhere. Perhaps the most important property
of the Laplacian related to semi-supervised learning is the following: a smaller
eigenvalue  corresponds to a smoother eigenvector  over the graph; that is, the

valuePij wij((i)  (j))2 is small. Informally, a smooth eigenvector has the

property that two elements of the vector have similar values if there are many large
weight paths between the nodes in the graph. In a physical system, the smoother
eigenvectors correspond to the major vibration modes. Figure 8.1(top) shows a
simple graph consisting of two linear segments. The edges have the same weight
1. Its Laplacian spectral decomposition is shown below, where the eigenvalues are
sorted from small to large. The rst two eigenvalues should be zero  there are
numerical errors in Matlab eigen computation. As the eigenvalues increase, the

57

58

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

4.5874e17

3.7245e16

0.043705

0.17291

0.38197

0.38197

0.66174

1

1.382

1.382

1.7909

2.2091

2.618

2.618

3

3.3383

3.618

3.618

3.8271

3.9563

Figure 8.1: A simple graph with two segments, and its Laplacian spectral decom-
position. The numbers are the eigenvalues, and the zigzag shapes are the corre-
sponding eigenvectors.

corresponding eigenvectors become less and less smooth.

8.2 From Laplacians to Kernels

Kernel-based methods are increasingly being used for data modeling and predic-
tion because of their conceptual simplicity and good performance on many tasks.
A promising family of semi-supervised learning methods can be viewed as con-
structing kernels by transforming the spectrum (i.e. eigen-decomposition) of the
graph Laplacian. These kernels, when viewed as regularizers, penalize functions
that are not smooth over the graph (Smola & Kondor, 2003).

Assuming the graph structure is correct, from a regularization perspective we

8.2. FROMLAPLACIANSTOKERNELS

59

want to encourage smooth functions, to reect our belief that labels should vary
slowly over the graph. Specically, Chapelle et al. (2002) and Smola and Kondor
(2003) suggest a general principle for creating a family of semi-supervised kernels
K from the graph Laplacian : transform the eigenvalues  into r(), where the
spectral transformation r is a non-negative and usually decreasing function1

K =

nXi=1

r(i) i>
i

(8.1)

Note it may be that r reverses the order of the eigenvalues, so that smooth is have

larger eigenvalues in K. With such a kernel, a soft labeling function f =P cii
in a kernel machine has a penalty term in the RKHS norm given by (||f||2
(P c2
f corresponding to eigenfunctions that are less smooth.

K) =
i /r(i)). If r is decreasing, a greater penalty is incurred for those terms of

In previous work r has often been chosen from a parametric family. For exam-

ple, the diffusion kernel (Kondor & Lafferty, 2002) corresponds to

r() = exp(

2
2

)

The regularized Gaussian process kernel in Chapter 6 corresponds to

r() =

1

 + 

(8.2)

(8.3)

Figure 8.2 shows such a regularized Gaussian process kernel, constructed from
the Laplacian in Figure 8.1 with  = 0.05. Cross validation has been used to
nd the hyperparameter  for these spectral transformations. Although the general
principle of equation (8.1) is appealing, it does not address the question of which
parametric family to use for r. Moreover, the degree of freedom (or the number of
hyperparameters) may not suit the task, resulting in overly constrained kernels.

We address these limitations with a nonparametric method. Instead of using
a parametric transformation r(), we allow the transformed eigenvalues i =
r(i), i = 1 . . . n to be almost independent. The only additional condition is that
is have to be non-increasing, to encourage smooth functions over the graph. Un-
der this condition, we nd the set of optimal spectral transformation  that maxi-
mizes the kernel alignment to the labeled data. The main advantage of using kernel
alignment is that it gives us a convex optimization problem, and does not suf-
fer from poor convergence to local minima. The optimization problem in general
is solved using semi-denite programming (SDP) (Boyd & Vandenberge, 2004);

1We use a slightly different notation where r is the inverse of that in (Smola & Kondor, 2003).

60

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

6

5

4

3

2

1

0
20

18

16

14

12

10

8

6

4

2

2

6

4

8

18

16

14

20

12

10

Figure 8.2: The kernel constructed from the Laplacian in Figure 8.1, with spectrum
transformation r() = 1/( + 0.05).

however, in our approach the problem can be formulated in terms of quadratically
constrained quadratic programming (QCQP), which can be solved more efciently
than a general SDP. We review QCQP next.

8.3 Convex Optimization using QCQP

Let Ki = i>
vectors. Our kernel K is a linear combination

i , i = 1 n be the outer product matrices of the Laplacians eigen-

K =

iKi

nXi=1

(8.4)

where i  0. We formulate the problem of nding the optimal spectral transfor-
mation as one that nds the interpolation coefcients {r(i) = i} by optimizing
some convex objective function on K. To maintain the positive semi-deniteness
constraint on K, one in general needs to invoke SDPs (Boyd & Vandenberge,
2004). Semi-denite optimization can be described as the problem of optimizing
a linear function of a symmetric matrix subject to linear equality constraints and
the condition that the matrix be positive semi-denite. The well known linear pro-
gramming problem can be generalized to a semi-denite optimization by replacing
the vector of variables with a symmetric matrix, and replacing the non-negativity
constraints with a positive semi-denite constraints. This generalization inherits
several properties: it is convex, has a rich duality theory and allows theoretically
efcient solution algorithms based on iterating interior point methods to either fol-
low a central path or decrease a potential function. However, a limitation of SDPs is
their computational complexity (Boyd & Vandenberge, 2004), which has restricted
their application to small-scale problems (Lanckriet et al., 2004). However, an
important special case of SDPs are quadratically constrained quadratic programs

8.4. SEMI-SUPERVISEDKERNELSWITHORDERCONSTRAINTS

61

(QCQP) which are computationally more efcient. Here both the objective func-
tion and the constraints are quadratic as illustrated below,

minimize

subject to

1
2
1
2

x>P0x + q>

0 x + r0

x>Pix + q>

i x + ri  0

i = 1 m

Ax = b

(8.5)

(8.6)

(8.7)

+, i = 1, . . . , m, where S n

where Pi  S n
+ denes the set of square symmetric
positive semi-denite matrices. In a QCQP, we minimize a convex quadratic func-
tion over a feasible region that is the intersection of ellipsoids. The number of
iterations required to reach the solution is comparable to the number required for
linear programs, making the approach feasible for large datasets. However, as ob-
served in (Boyd & Vandenberge, 2004), not all SDPs can be relaxed to QCQPs.
For the semi-supervised kernel learning task presented here solving an SDP would
be computationally infeasible.

Recent work (Cristianini et al., 2001a; Lanckriet et al., 2004) has proposed ker-
nel target alignment that can be used not only to assess the relationship between
the feature spaces generated by two different kernels, but also to assess the similar-
ity between spaces induced by a kernel and that induced by the labels themselves.
Desirable properties of the alignment measure can be found in (Cristianini et al.,
2001a). The crucial aspect of alignment for our purposes is that its optimization can
be formulated as a QCQP. The objective function is the empirical kernel alignment
score:

(8.8)

A(Ktr, T ) =

hKtr, TiF

phKtr, KtriFhT, TiF

where Ktr is the kernel matrix restricted to the training points, hM, NiF denotes
the Frobenius product between two square matrices hM, NiF = Pij mijnij =
trace(M N >), and T is the target matrix on training data, with entry Tij set to +1
if yi = yj and 1 otherwise. Note for binary {+1,1} training labels YL this
L . K is guaranteed to be positive semi-
is simply the rank one matrix T = YLY >
denite by constraining i  0. Our kernel alignment problem is special in that
the Kis were derived from the graph Laplacian with the goal of semi-supervised
learning. We require smoother eigenvectors to receive larger coefcients, as shown
in the next section.

8.4 Semi-Supervised Kernels with Order Constraints

As stated above, we would like to maintain a decreasing order on the spectral
transformation i = r(i) to encourage smooth functions over the graph. This

62

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

motivates the set of order constraints

i  i+1,

i = 1 n  1

(8.9)

We can specify the desired semi-supervised kernel as follows.

Denition 1 Anorder constrained semi-supervised kernel K isthesolutiontothe
followingconvexoptimizationproblem:

maxK
subjectto

A(Ktr, T )

K =Pn

i  0

i=1 iKi

trace(K) = 1

i  i+1,

i = 1 n  1

(8.10)
(8.11)
(8.12)
(8.13)
(8.14)

where T isthetrainingtargetmatrix, Ki = i>
thegraphLaplacian.

i and isaretheeigenvectorsof

The formulation is an extension to (Lanckriet et al., 2004) with order constraints,
and with special components Kis from the graph Laplacian. Since i  0 and
Kis are outer products, K will automatically be positive semi-denite and hence
a valid kernel matrix. The trace constraint is needed to x the scale invariance of
kernel alignment. It is important to notice the order constraints are convex, and as
such the whole problem is convex. This problem is equivalent to:

maxK

hKtr, TiF

subject to hKtr, KtriF  1
i=1 iKi

K =Pn

i  0

i  i+1, i

(8.15)
(8.16)
(8.17)
(8.18)
(8.19)

Let vec(A) be the column vectorization of a matrix A. Dening a l2  m matrix
(8.20)

M =(cid:2)vec(K1,tr) vec(Km,tr)(cid:3)

it is not hard to show that the problem can then be expressed as

max
subject to

vec(T )>M 
||M ||  1

i  0

i  i+1,

i = 1 n  1

(8.21)
(8.22)
(8.23)
(8.24)

8.4. SEMI-SUPERVISEDKERNELSWITHORDERCONSTRAINTS

63

The objective function is linear in , and there is a simple cone constraint, making
it a quadratically constrained quadratic program (QCQP) 2.

An improvement of the above order constrained semi-supervised kernel can be
obtained by taking a closer look at the Laplacian eigenvectors with zero eigenval-
ues. As stated earlier, for a graph Laplacian there will be k zero eigenvalues if the
graph has k connected subgraphs. The k eigenvectors are piecewise constant over
individual subgraphs, and zero elsewhere. This is desirable when k > 1, with the
hope that subgraphs correspond to different classes. However if k = 1, the graph is
connected. The rst eigenvector 1 is a constant vector over all nodes. The corre-
sponding K1 is a constant matrix, and acts as a bias term in (8.1). In this situation
we do not want to impose the order constraint 1  2 on the constant bias term,
rather we let 1 vary freely during optimization:

Denition 2 An improved order constrained semi-supervised kernel K istheso-
lution to the same problem in Denition 1, but the order constraints (8.14) apply
onlytonon-constanteigenvectors:

i  i+1,

i = 1 n  1, and i notconstant

(8.25)

In practice we do not need all n eigenvectors of the graph Laplacian, or equiva-
lently all n Kis. The rst m < n eigenvectors with the smallest eigenvalues work
well empirically. Also note we could have used the fact that Kis are from orthog-
onal eigenvectors i to further simplify the expression. However we neglect this
observation, making it easier to incorporate other kernel components if necessary.
It is illustrative to compare and contrast the order constrained semi-supervised
kernels to other semi-supervised kernels with different spectral transformation. We
call the original kernel alignment solution in (Lanckriet et al., 2004) a maximal-
alignment kernel. It is the solution to Denition 1 without the order constraints
(8.14). Because it does not have the additional constraints, it maximizes kernel
alignment among all spectral transformation. The hyperparameters  of the Diffu-
sion kernel and Gaussian elds kernel (described earlier) can be learned by max-
imizing the alignment score too, although the optimization problem is not neces-
sarily convex. These kernels use different information in the original Laplacian
eigenvalues i. The maximal-alignment kernels ignore i altogether. The order
constrained semi-supervised kernels only use the order of i and ignore their ac-
tual values. The diffusion and Gaussian eld kernels use the actual values.
In
terms of the degree of freedom in choosing the spectral transformation is, the
maximal-alignment kernels are completely free. The diffusion and Gaussian eld

2An alternative formulation results in a quadratic program (QP), which is faster than QCQP.

Details can be found at http://www.cs.cmu.edu/zhuxj/pub/QP.pdf

64

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

kernels are restrictive since they have an implicit parametric form and only one free
parameter. The order constrained semi-supervised kernels incorporates desirable
features from both approaches.

8.5 Experiments

We evaluate the order constrained kernels on seven datasets. baseball-hockey
(1993 instances / 2 classes), pc-mac (1943/2) and religion-atheism (1427/2) are
document categorization tasks taken from the 20-newsgroups dataset. The distance
measure is the standard cosine similarity between tf.idf vectors. one-two (2200/2),
odd-even (4000/2) and ten digits (4000/10) are handwritten digits recognition
tasks. one-two is digits 1 vs. 2; odd-even is the articial task of classify-
ing odd 1, 3, 5, 7, 9 vs. even 0, 2, 4, 6, 8 digits, such that each class has several
well dened internal clusters; ten digits is 10-way classication. isolet (7797/26)
is isolated spoken English alphabet recognition from the UCI repository. For these
datasets we use Euclidean distance on raw features. We use 10NN unweighted
graphs on all datasets except isolet which is 100NN. For all datasets, we use the
smallest m = 200 eigenvalue and eigenvector pairs from the graph Laplacian.
These values are set arbitrarily without optimizing and do not create a unfair ad-
vantage to the proposed kernels. For each dataset we test on ve different labeled
set sizes. For a given labeled set size, we perform 30 random trials in which a la-
beled set is randomly sampled from the whole dataset. All classes must be present
in the labeled set. The rest is used as unlabeled (test) set in that trial. We compare
5 semi-supervised kernels (improved order constrained kernel, order constrained
kernel, Gaussian eld kernel, diffusion kernel3 and maximal-alignment kernel),
and 3 standard supervised kernels (RBF (bandwidth learned using 5-fold cross val-
idation),linear and quadratic). We compute the spectral transformation for order
constrained kernels and maximal-alignment kernels by solving the QCQP using
standard solvers (SeDuMi/YALMIP). To compute accuracy we use these kernels in
a standard SVM. We choose the bound on slack variables C with cross validation
for all tasks and kernels. For multiclass classication we perform one-against-all
and pick the class with the largest margin.

Table 8.1 through Table 8.7 list the results. There are two rows for each cell:
The upper row is the average test set accuracy with one standard deviation; The
lower row is the average training set kernel alignment, and in parenthesis the av-
erage run time in seconds for QCQP on a 2.4GHz Linux computer. Each number
is averaged over 30 random trials. To assess the statistical signicance of the re-

3The hyperparameters  are learned with the fminbnd() function in Matlab to maximize kernel

alignment.

8.5. EXPERIMENTS

65

Training
set size

10

30

50

70

90

semi-supervised kernels

Improved

Order

95.7  8.9
0.90 ( 2)
98.0  0.2
0.91 ( 9)
97.9  0.5
0.89 (29)
97.9  0.3
0.90 (68)
98.0  0.5
0.89 (103)

Order

93.9 12.0

0.69 ( 1)
97.3  2.1
0.67 ( 9)
97.8  0.6
0.63 (29)
97.9  0.3
0.64 (64)
98.0  0.2
0.63 (101)

Gaussian

Field

Diffusion

Max-align

63.1 15.8

65.8 22.8

0.35

0.44

91.8  9.3

59.1 17.9

0.25

0.39

96.7  0.6

93.7  6.8

0.22

0.36

96.8  0.6

97.5  1.4

0.22

0.37

97.0  0.4

97.8  0.2

0.21

0.36

93.2  6.8
0.95 ( 1)
96.6  2.2
0.93 ( 6)
97.0  1.1
0.90 (27)
97.2  0.8
0.90 (46)
97.6  0.3
0.89 (90)

RBF

 = 200
53.6  5.5

0.11

standard kernels

Linear

Quadratic

68.1  7.6

68.1  7.6

0.29

0.23

69.3 11.2

78.5  8.5

77.8 10.6

0.03

0.17

0.11

77.7  8.3

84.1  7.8

75.6 14.2

0.02

0.15

0.09

83.9  7.2

87.5  6.5

76.1 14.9

0.01

0.13

0.07

88.5  5.1

89.3  4.4

73.3 16.8

0.01

0.12

0.06

Table 8.1: Baseball vs. Hockey

Training
set size

10

30

50

70

90

semi-supervised kernels

Improved

Order

87.0  5.0
0.71 ( 1)
90.3  1.3
0.68 ( 8)
91.3  0.9
0.64 (31)
91.5  0.6
0.63 (70)
91.5  0.6
0.63 (108)

Order

84.9  7.2
0.57 ( 1)
89.6  2.3
0.49 ( 8)
90.5  1.7
0.46 (31)
90.8  1.3
0.46 (56)
91.3  1.3
0.45 (98)

Gaussian

Field

Diffusion

Max-align

56.4  6.2

57.8 11.5

0.32

0.35

76.4  6.1

79.6 11.2

0.19

0.23

81.1  4.6

87.5  2.8

0.16

0.20

84.6  2.1

90.5  1.2

0.14

0.19

86.3  2.3

91.3  1.1

0.13

0.18

71.1  9.7
0.90 ( 1)
85.4  3.9
0.74 ( 6)
88.4  2.1
0.68 (25)
89.6  1.6
0.66 (59)
90.3  1.0
0.65 (84)

RBF

 = 100
51.6  3.4

0.11

standard kernels

Linear

Quadratic

63.0  5.1

62.3  4.2

0.30

0.25

62.6  9.6

71.8  5.5

71.2  5.3

0.03

0.18

0.13

67.8  9.0

77.6  4.8

75.7  5.4

0.02

0.14

0.10

74.7  7.4

80.2  4.6

74.3  8.7

0.01

0.12

0.08

79.0  6.4

82.5  4.2

79.1  7.3

0.01

0.11

0.08

Table 8.2: PC vs. MAC

Training
set size

10

30

50

70

90

Improved

Order

72.8 11.2

0.50 ( 1)
84.2  2.4
0.38 ( 8)
84.5  2.3
0.31 (28)
85.7  1.4
0.29 (55)
86.6  1.3
0.27 (86)

semi-supervised kernels

Order

Gaussian

Field

Diffusion

Max-align

70.9 10.9

55.2  5.8

60.9 10.7

0.42 ( 1)
83.0  2.9
0.31 ( 6)
83.5  2.5
0.26 (23)
85.3  1.6
0.25 (42)
86.4  1.5
0.24 (92)

0.31

0.31

71.2  6.3

80.3  5.1

0.20

0.22

80.4  4.1

83.5  2.7

0.17

0.20

83.0  2.9

85.4  1.8

0.16

0.19

84.5  2.1

86.2  1.6

0.15

0.18

60.7  7.5
0.85 ( 1)
74.4  5.4
0.60 ( 7)
77.4  6.1
0.48 (27)
82.3  3.0
0.43 (51)
82.8  2.6
0.40 (85)

RBF

 = 130
55.8  5.8

0.13

standard kernels

Linear

Quadratic

60.1  7.0

61.2  4.8

0.30

0.26

63.4  6.5

63.7  8.3

70.1  6.3

0.05

0.18

0.15

69.3  6.5

69.4  7.0

70.7  8.5

0.04

0.15

0.11

73.1  5.8

75.7  6.0

71.0 10.0

0.03

0.13

0.10

77.7  5.1

74.6  7.6

70.0 11.5

0.02

0.12

0.09

Table 8.3: Religion vs. Atheism

semi-supervised kernels

standard kernels

Diffusion

Max-align

RBF

Linear

Quadratic

 = 1000
78.7 14.3

0.38

85.1  5.7

85.7  4.8

0.26

0.30

90.4  4.6

86.0  9.4

90.9  3.7

0.33

0.22

0.25

93.6  3.1

89.6  5.9

92.9  2.8

0.30

0.17

0.24

94.0  2.7

91.6  6.3

94.9  2.0

0.29

0.18

0.21

96.1  2.4

93.0  3.6

95.8  2.3

0.28

0.17

0.20

Training
set size

10

20

30

40

50

Improved

Order

96.2  2.7
0.87 ( 2)
96.4  2.8
0.87 ( 3)
98.2  2.1
0.84 ( 8)
98.3  1.9
0.84 (13)
98.4  1.9
0.83 (31)

Order

Gaussian

Field

90.6 14.0

58.2 17.6

59.4 18.9

85.4 11.5

0.66 ( 1)
93.9  8.7
0.64 ( 4)
97.2  2.5
0.61 ( 7)
96.5  2.4
0.61 (15)
95.6  9.0
0.60 (37)

0.43

0.53

87.0 16.0

83.2 19.8

0.38

0.50

98.1  2.2

98.1  2.7

0.35

0.47

98.9  1.8

99.1  1.4

0.36

0.48

99.4  0.5

99.6  0.3

0.35

0.46

0.95 ( 1)
94.5  1.6
0.90 ( 3)
96.4  2.1
0.86 ( 6)
96.3  2.3
0.86 (11)
96.6  2.3
0.84 (25)

Table 8.4: One vs. Two

66

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

semi-supervised kernels

standard kernels

Diffusion

Max-align

RBF

Linear

Quadratic

Training
set size

10

30

50

70

90

Improved

Order

69.6  6.5
0.45 ( 1)
82.4  4.1
0.32 ( 6)
87.6  3.5
0.29 (24)
89.2  2.6
0.27 (65)
91.5  1.5
0.26 (94)

Order

68.8  6.1
0.41 ( 1)
82.0  4.0
0.28 ( 6)
87.5  3.4
0.26 (25)
89.0  2.7
0.24 (50)
91.4  1.6
0.23 (97)

Gaussian

Field

65.5  8.9

0.32

68.4  8.5

0.34

79.6  4.1

83.0  4.2

0.21

0.23

85.9  3.8

89.1  2.7

0.19

0.21

89.0  1.9

90.3  2.8

0.17

0.20

90.5  1.4

91.9  1.7

0.16

0.19

55.7  4.4
0.86 ( 1)
67.2  5.0
0.56 ( 6)
76.0  5.3
0.45 (26)
80.9  4.4
0.39 (51)
85.4  3.1
0.36 (88)

 = 1500
65.0  7.0

0.23

63.1  6.9

65.4  6.5

0.25

0.27

77.7  3.5

72.4  6.1

76.5  5.1

0.10

0.11

0.16

81.8  2.7

74.4  9.2

81.3  3.1

0.07

0.09

0.12

84.4  2.0

73.6 10.0

83.8  2.8

0.06

0.07

0.12

86.1  1.8

66.1 14.8

85.5  1.6

0.05

0.07

0.11

Table 8.5: Odd vs. Even

semi-supervised kernels

standard kernels

Diffusion

Max-align

RBF

Linear

Quadratic

Training
set size

50

100

150

200

250

Improved

Order

76.6  4.3
0.47 (26)
84.8  2.6
0.47 (124)
86.5  1.7
0.48 (310)
88.1  1.3
0.47 (708)
89.1  1.1
0.47 (942)

Order

71.5  5.0
0.21 (26)
83.4  2.6
0.17 (98)
86.4  1.3
0.18 (255)
88.0  1.3
0.16 (477)
89.3  1.0
0.16 (873)

Gaussian

Field

41.4  6.8

49.8  6.3

0.15

0.16

63.7  3.5

72.5  3.3

0.12

0.13

75.1  3.0

80.4  2.1

0.11

0.13

80.4  2.5

84.4  1.6

0.10

0.11

84.6  1.4

87.2  1.3

0.10

0.11

70.3  5.2
0.51 (25)
80.7  2.6
0.49 (100)
84.5  1.9
0.50 (244)
86.0  1.5
0.49 (523)
87.2  1.3
0.49 (706)

 = 2000
57.0  4.0

-0.62

50.2  9.0

66.3  3.7

-0.50

-0.25

69.4  1.9

56.0  7.8

77.2  2.3

-0.64

-0.52

-0.29

75.2  1.4

56.2  7.2

81.4  2.2

-0.66

-0.53

-0.31

78.3  1.3

60.8  7.3

84.3  1.7

-0.65

-0.54

-0.33

80.4  1.4

61.3  7.6

85.7  1.3

-0.65

-0.54

-0.33

Table 8.6: Ten Digits (10 classes)

Training
set size

50

100

150

200

250

Improved

Order

56.0  3.5
0.27 (26)
64.6  2.1
0.26 (105)
67.6  2.6
0.26 (249)
71.0  1.8
0.26 (441)
71.8  2.3
0.26 (709)

semi-supervised kernels

Order

42.0  5.2
0.13 (25)
59.0  3.6
0.10 (127)
65.2  3.0
0.09 (280)
70.9  2.3
0.08 (570)
73.6  1.5
0.08 (836)

Gaussian

Field

Diffusion

Max-align

41.2  2.9

29.0  2.7

0.03

0.11

58.5  2.9

47.4  2.7

-0.02

0.08

65.4  2.6

57.2  2.7

-0.05

0.07

70.6  1.9

64.8  2.1

-0.07

0.06

73.7  1.2

69.8  1.5

-0.07

0.06

50.1  3.7
0.31 (24)
63.2  1.9
0.29 (102)
67.9  2.5
0.27 (221)
72.3  1.7
0.27 (423)
74.2  1.5
0.27 (665)

RBF

 = 30
28.7  2.0

-0.89

standard kernels

Linear

Quadratic

30.0  2.7

23.7  2.4

-0.80

-0.65

46.3  2.4

46.6  2.7

42.0  2.9

-0.90

-0.82

-0.69

57.6  1.5

57.3  1.8

53.8  2.2

-0.90

-0.83

-0.70

63.9  1.6

64.2  2.0

60.5  1.6

-0.91

-0.83

-0.72

68.8  1.5

69.5  1.7

66.2  1.4

-0.91

-0.84

-0.72

Table 8.7: ISOLET (26 classes)

8.5. EXPERIMENTS

67

sults, we perform paired t-test on test accuracy. We highlight the best accuracy
in each row, and those that cannot be determined as different from the best, with
paired t-test at signicance level 0.05. The semi-supervised kernels tend to out-
perform standard supervised kernels. The improved order constrained kernels are
consistently among the best. Figure 8.3 shows the spectral transformation i of
the semi-supervised kernels for different tasks. These are for the 30 trials with the
largest labeled set size in each task. The x-axis is in increasing order of i (the
original eigenvalues of the Laplacian). The mean (thick lines) and 1 standard de-
viation (dotted lines) of only the top 50 is are plotted for clarity. The i values are
scaled vertically for easy comparison among kernels. As expected the maximal-
alignment kernels spectral transformation is zigzagged, diffusion and Gaussian
elds are very smooth, while order constrained kernels are in between. The or-
der constrained kernels (green) have large 1 because of the order constraint. This
seems to be disadvantageous  the spectral transformation tries to balance it out
by increasing the value of other is so that the constant K1s relative inuence is
smaller. On the other hand the improved order constrained kernels (black) allow
1 to be small. As a result the rest is decay fast, which is desirable.

In conclusion, the method is both computationally feasible and results in im-
provements to classication performance when used with support vector machines.

68

CHAPTER8. KERNELSFROMTHESPECTRUMOFLAPLACIANS

Baseball vs. Hockey

PC vs. MAC

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

0

0

5

10

15

20

25

rank

30

35

40

45

50

0

0

5

10

15

20

25

rank

30

35

40

45

50

Religion vs. Atheism

One vs. Two

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

0

0

5

10

15

20

25

rank

30

35

40

45

50

0

0

5

10

15

20

25

rank

30

35

40

45

50

Odd vs. Even

Ten Digits (10 classes)

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



Improved order
Order
Maxalign
Gaussian field
Diffusion

0

0

5

10

15

20

25

rank

30

35

40

45

50

0

0

5

10

15

20

25

rank

30

35

40

45

50

ISOLET (26 classes)

Improved order
Order
Maxalign
Gaussian field
Diffusion

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

l

d
e
a
c
s



0

0

5

10

15

20

25

rank

30

35

40

45

50

Figure 8.3: Spectral transformation of the 5 semi-supervised kernels.

Chapter 9

Sequences and Beyond

So far, we have treated each data point individually. However in many problems
the data has complex structures. For example in speech recognition the data is se-
quential. Most semi-supervised learning methods have not addressed this problem.
We use sequential data as an example in the following discussion because it is sim-
ple. Nevertheless the discussion applies to other complex data structures like grids,
trees etc.

It is important to clarify the setting. By sequential data we do not mean each
data item x is a sequence and we give a single label y to the whole sequence.
Instead we want to give individual labels to the constituent data points in the se-
quence.

There are generative and discriminative methods that can be used for semi-

supervised learning on sequences.

The Hidden Markov Model (HMM) is such a generative methods. Speci-
cally the standard EM training with forward-backward algorithm (also known as
Baum-Welch (Rabiner, 1989)) is a sequence semi-supervised learning algorithm,
although it is usually not presented that way. The training data typically consists
of a small labeled set with l labeled sequences {XL, YL} = {(x1, y1) . . . (xl, yl)},
and a much larger unlabeled set of sequences XU = {xl+1 . . . xl+u}. We use
bold font xi to represent the i-th sequence with length mi, whose elements are
xi1 . . . ximi. Similarly yi is a sequence of labels yi1 . . . yimi. The labeled set is
used to estimate initial HMM parameters. The unlabeled data is then used to run
the EM algorithm on, to improve the HMM likelihood P (XU ) to a local maxi-
mum. The trained HMM parameters thus are determined by both the labeled and
unlabeled sequences. This parallels the mixture models and EM algorithm in the
i.i.d. case. We will not discuss it further in the thesis.

For discriminative methods one strategy is to use a kernel machine for se-

69

70

CHAPTER9. SEQUENCESANDBEYOND

quences, and introduce semi-supervised dependency via the kernels in Chapter 8.
Recent kernel machines for sequences and other complex structures include Ker-
nel Conditional Random Fields (KCRFs) (Lafferty et al., 2004) and Max-Margin
Markov Networks (Taskar et al., 2003), which are generalization of logistic re-
gression and support vector machines respectively to structured data. These kernel
machines by themselves are not designed specically for semi-supervised learn-
ing. However we can use a semi-supervised kernel, for example the graph kernels
in Chapter 8, with the kernel machines. This results in semi-supervised learning
methods on sequential data.

The idea is straightforward. The remainder of the chapter focuses on KCRFs,
describing the formalism and training issues, with a synthetic example on semi-
supervised learning.

9.1 Cliques and Two Graphs

Before we start, it is useful to distinguish two kinds of graphs in KCRF for semi-
supervised learning. The rst graph (gs) represents the conditional random eld
structure, for example a linear chain graph for sequences. In this case the size of
gs is the length of the sequence. In general let x be the features on gss nodes and
y the labels. A clique c is a subset of the nodes which is fully connected, with
any pair of nodes joined by an edge. Let yc be the labels on the clique. We want
Mercer kernels K to compare cliques in different graphs,

K((gs, x, c, yc), (g0

s, x0, c0, y0

c0))  R

(9.1)

Intuitively, this assigns a measure of similarity between a labeled clique in one
graph and a labeled clique in a (possibly) different graph. We denote by HK the
associated reproducing kernel Hilbert space, and by kkK the associated norm.
In the context of semi-supervised learning, we are interested in kernels with
the special form:

K((gs, x, c, yc), (g0

s, x0, c0, y0

c), gs, yc, g0

s, y0

(9.2)

c0)) = (cid:0)K0(xc, x0

c0(cid:1)

i.e. some function  of a kernel K0, where K0 depends only on the features, not
the labels. This is where the second graph (denoted gk) comes in. gk is the semi-
supervised graph discussed in previous chapters. Its nodes are the cliques xc in
both labeled and unlabeled data, and edges represent similarity between the cliques.
The size of gk is the total number of cliques in the whole dataset.
It however
does not represent the sequence structure. gk is used to derive the Laplacian and
ultimately the kernel matrix K0(xc, x0

c), as in Chapter 8.

9.2. REPRESENTERTHEOREMFORKCRFS

71

9.2 Representer Theorem for KCRFs

We start from a function f which, looking at a clique (c) in graph (gs, x) and an
arbitrary labeling of the clique (yc), computes a compatibility score. That is,
f (gs, x, c, yc)  R. We dene a conditional random eld

f (gs, x, c, yc)!

(9.3)

(9.4)

The normalization factor is

p(y|gs, x) = Z1(gs, x, f ) exp Xc
exp Xc
Z(gs, x, f ) =Xy0

f (gs, x, c, y0

c)!

Notice we sum over all possible labelings of all cliques. The conditional random
eld induces a loss function, the negative log loss

(y|gs, x, f )

=  log p(y|gs, x)

= Xc

f (gs, x, c, yc) + logXy0

(9.5)
(9.6)

(9.7)

exp Xc

f (gs, x, c, y0

c)!

We now extend the standard representer theorem of kernel machines (Kimel-
dorf & Wahba, 1971) to conditional graphical models. Consider a regularized loss
function (i.e. risk) of the form

R(f ) =

lXi=1

(cid:16)y(i)|g(i)

s , x(i), f(cid:17) +  (kfkK)

(9.8)

on a labeled training set of size l.  is a strictly increasing function. It is important
to note that the risk depends on all possible assignments yc of labels to each clique,
not just those observed in the labeled data y(i). This is due to the normalization
factor in the negative log loss. We have the following representer theorem for
KCRFs:

Proposition (Representer theorem for CRFs). The minimizer f ? of the risk
(9.8),ifitexists,hastheform

f ?(gs, x, c, yc) =

lXi=1Xc0 Xy0

(i)
c0 (y0) K((g(i)

s , x(i), c0, y0), (gs, x, c, yc)) (9.9)

72

CHAPTER9. SEQUENCESANDBEYOND

where the sum y0 is over all labelings of clique c0. The key property distinguish-
ing this result from the standard representer theorem is that the dual parameters
(i)
c0 (y0) now depend on all assignments of labels. That is, for each training graph
i, and each clique c0 within the graph, and each labeling y0 of the clique, not just
the labeling in the training data, there is a dual parameter .

The difference between KCRFs and the earlier non-kernel version of CRFs is
the representation of f. In a standard non-kernel CRF, f is represented as a sum of
weights times feature functions

f (gs, x, c, yc) = >(gs, x, c, yc)

(9.10)

where  is a vector of weights (the primal parameters), and  is a set of xed
feature functions. Standard CRF learning nds the optimal . Therefore one ad-
vantage of KCRFs is the use of kernels which can correspond to innite features.
In addition if we plug in a semi-supervised learning kernel to KCRFs, we obtain a
semi-supervised learning algorithm on structured data.

Let us look at two special cases of KCRF. In the rst case let the cliques be the

vertices v, and with a special kernel

K((gs, x, v, yv), (g0

s, x0, v0, y0

v0)) = K0(xv, x0

v0)(yv, y0

v0)

The representer theorem states that

f ?(x, y) =

lXi=1 Xvg

(i)
s

v (y)K0(x, x(i)
(i)
v )

(9.11)

(9.12)

Under the probabilistic model 9.3, this is simply kernel logistic regression. It has
no ability to model sequences.

In the second case let the cliques be edges connecting two vertices v1, v2. Let

the kernel be

and we have

K((gs, x, v1v2, yv1yv2), (g0

= K0(xv1, x0

v1)(yv1, y0

2, y0
1v0
s, x0, v0
v2))
v1)(yv2, y0
v1) + (yv1, y0
v2)

v1y0

(9.13)
(9.14)

f ?(xv1, yv1yv2) =

lXi=1 Xug

(i)
s

(i)
u (yv1)K0(xv1, x(i)

u ) + (yv1, yv2)

(9.15)

which is a simple type of semiparametric CRF. It has rudimentary ability to model
sequences with (yv1, yv2), similar to a transition matrix between states. In both
cases, we can use a graph kernel K0 on both labeled and unlabeled data for semi-
supervised learning.

9.3. SPARSETRAINING:CLIQUESELECTION

73

9.3 Sparse Training: Clique Selection

The representer theorem shows that the minimizing function f is supported by la-
beled cliques over the training examples; however, this may result in an extremely
large number of parameters. We therefore pursue a strategy of incrementally select-
ing cliques in order to greedily reduce the risk. The resulting procedure is parallel
to forward stepwise logistic regression, and to related methods for kernel logistic
regression (Zhu & Hastie, 2001).

s , x(i), c, yc)o, each item uniquely
Our algorithm will maintain an active setn(g
species a labeled clique. Again notice the labelings yc are not necessarily those
appearing in the training data. Each labeled clique can be represented by a ba-
s , x(i), c, yc),)  HK, and is assigned a parameter
sis function h() = K((g
h = (i)

c (yc). We work with the regularized risk

(i)

(i)

R(f ) =

lXi=1

(cid:16)y(i)|g(i)

s , x(i), f(cid:17) +


2 kfk2

K

(9.16)

where  is the negative log loss of equation (9.5). To evaluate a candidate h, one
strategy is to compute the gain sup R(f )  R(f + h), and to choose the
candidate h having the largest gain. This presents an apparent difculty, since the
optimal parameter  cannot be computed in closed form, and must be evaluated nu-
merically. For sequence models this would involve forward-backward calculations
for each candidate h, the cost of which is prohibitive.

As an alternative, we adopt the functional gradient descent approach, which
evaluates a small change to the current function. For a given candidate h, consider
adding h to the current model with small weight ; thus f 7 f + h. Then
R(f + h) = R(f ) + dR(f, h) + O(2), where the functional derivative of
R at f in the direction h is computed as

(9.17)

dR(f, h) = Ef [h]  eE[h] + hf, hiK

where eE[h] =PiPc h(g
PiPyPc p(y|x(i), f )h(g

(i)
(i)
s , x(i), c, y
c ) is the empirical expectation and Ef [h] =
(i)
s , x(i), c, yc) is the model expectation conditioned on
x. The idea is that in directions h where the functional gradient dR(f, h) is large,
the model is mismatched with the labeled data; this direction should be added to the
model to make a correction. This results in the greedy clique selection algorithm,
as summarized in Figure 9.1.

An alternative to the functional gradient descent algorithm above is to estimate
parameters h for each candidate. When each candidate clique is a vertex, the

74

CHAPTER9. SEQUENCESANDBEYOND

Initialize with f = 0, and iterate:

1. For each candidate h  HK, supported by a single labeled

clique, calculate the functional derivative dR(f, h).

2. Select the candidate h = arg maxh|dR(f, h)| having the largest

gradient direction. Set f 7 f + hh.

3. Estimate parameters f for each active f by minimizing R(f ).

Figure 9.1: Greedy Clique Selection. Labeled cliques encode basis functions h
which are greedily added to the model, using a form of functional gradient descent.

e

t

a
r

r
o
r
r
e


t
s
e
T

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

semisupervised
RBF

50

100

150

200

250

300

350

400

Training set size

e
t
a
r

r
o
r
r
e

t
s
e
T

0.5

0.45

0.4

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

semisupervised
RBF

2

4

6

8

10

12

14

16

18

20

Training set size

Figure 9.2: Left: The galaxy data is comprised of two interlocking spirals together
with a dense core of samples from both classes. Center: Kernel logistic regres-
sion comparing two kernels, RBF and a graph kernel using the unlabeled data.
Right: Kernel conditional random elds, which take into account the sequential
structure of the data.

gain can be efciently approximated using a mean eld approximation. Under this
approximation, a candidate is evaluated according to the approximate gain

R(f )  R(f + h)

 Xi Xv

Z(f, x(i))1p(y(i)

v |x(i), f ) exp(h(x(i), y(i)

(9.18)
v )) + hf, hi(9.19)

which is a logistic approximation. Details can be found in Appendix E.

9.4 Synthetic Data Experiments

In the experiments reported below for sequences, the marginal probabilities p(yv =
1|x) and expected counts for the state transitions are required; these are computed

9.4. SYNTHETICDATAEXPERIMENTS

75

using the forward-backward algorithm, with log domain arithmetic to avoid un-
derow. A quasi-Newton method (BFGS, cubic-polynomial line search) is used to
estimate the parameters in step 3 of Figure 9.1.

To work with a data set that will distinguish a semi-supervised graph kernel
from a standard kernel, and a sequence model from a non-sequence model, we
prepared a synthetic data set (galaxy) that is a variant of spirals, see Figure 9.2
(left). Note data in the dense core come from both classes.

We sample 100 sequences of length 20 according to an HMM with two states,
where each state emits instances uniformly from one of the classes. There is a 90%
chance of staying in the same state, and the initial state is uniformly chosen. The
idea is that under a sequence model we should be able to use the context to deter-
mine the class of an example at the core. However, under a non-sequence model
without the context, the core region will be indistinguishable, and the dataset as a
whole will have about 20% Bayes error rate. Note the choice of semi-supervised
vs. standard kernels and sequence vs. non-sequence models are orthogonal; the
four combinations are all tested on.

We construct the semi-supervised graph kernel by rst building an unweighted
10-nearest neighbor graph. We compute the associated graph Laplacian , and

then the graph kernel K = 10(cid:0) + 106I(cid:1)1. The standard kernel is the radial

basis function (RBF) kernel with an optimal bandwidth  = 0.35.

First we apply both kernels to a non-sequence model: kernel logistic regression
(9.12), see Figure 9.2 (center). The sequence structure is ignored. Ten random
trials were performed with each training set size, which ranges from 20 to 400
points. The error intervals are one standard error. As expected, when the labeled
set size is small, the RBF kernel results in signicantly larger test error than the
graph kernel. Furthermore, both kernels saturate at the 20% Bayes error rate.

Next we apply both kernels to a KCRF sequence model 9.15. Experimental
results are shown in Figure 9.2 (right). Note the x-axis is the number of train-
ing sequences: Since each sequence has 20 instances, the range is the same as
Figure 9.2 (center). The kernel CRF is capable of getting below the 20% Bayes
error rate of the non-sequence model, with both kernels and sufcient labeled data.
However the graph kernel is able to learn the structure much faster than the RBF
kernel. Evidently the high error rate for small label data sizes prevents the RBF
model from effectively using the context.

Finally we examine clique selection in KCRFs. For this experiment we use 50
training sequences. We use the mean eld approximation and only select vertex
cliques. At each iteration the selection is based on the estimated change in risk for
each candidate vertex (training position). We plot the estimated change in risk for
the rst four iterations of clique selection, with the graph kernel and RBF kernel re-

76

CHAPTER9. SEQUENCESANDBEYOND

spectively in Figure 9.3. Smaller values (lower on z-axis) indicate good candidates
with potentially large reduction in risk if selected. For the graph kernel, the rst
two selected vertices are sufcient to reduce the risk essentially to the minimum
(note in the third iteration the z-axis scale is already 106). Such reduction does
not happen with the RBF kernel.

9.4. SYNTHETICDATAEXPERIMENTS

77

1st position candidates

2nd position candidates

0

0.5

1

2

0

2

2

2

0

2

0

2

0

2

2

3rd position candidates

x 106

4th position candidates

x 106

0

2

4

0

0.2

0.4

0

1

2

3

2

0

2

0

2

2

2

0

2

0

2

2

graph kernel

1st position candidates

2nd position candidates

0

10

20

2

0

10

20

2

0

0

2

0

2

2

3rd position candidates

2

0

2

2

0

10

20

2

0

10

20

2

0

0

2

0

2

2

4th position candidates

2

0

2

2

RBF kernel

Figure 9.3: Mean eld estimate of the change in loss function with the graph kernel
(top) and the RBF kernel (bottom) for the rst four iterations of clique selection on
the galaxy dataset. For the graph kernel the endpoints of the spirals are chosen as
the rst two cliques.

78

CHAPTER9. SEQUENCESANDBEYOND

Chapter 10

Harmonic Mixtures: Handling
Unseen Data and Reducing
Computation

There are two important questions to graph based semi-supervised learning meth-
ods:

1. The graph is constructed only on the labeled and unlabeled data. Many such
methods are transductive in nature. How can we handle unseen new data
points?

2. They often involve expensive manipulation on large matrices, for example
matrix inversion, which can be O(n3). Because unlabeled data is relatively
easy to obtain in large quantity, the matrix could be too big to handle. How
can we reduce computation when the unlabeled dataset is large?

In this chapter we address these questions by combining graph method with a mix-
ture model.

Mixture model has long been used for semi-supervised learning, e.g. Gaussian
mixture model (GMM) (Castelli & Cover, 1996) (Ratsaby & Venkatesh, 1995), and
mixture of multinomial (Nigam et al., 2000). Training is typically done with the
EM algorithm. It has several advantages: The model is inductive and handles un-
seen points naturally; It is a parametric model with a small number of parameters.
However when there is underlying manifold structure in the data, EM may have
difculty making the labels follow the manifold: An example is given in Figure
10.1. The desired behavior is shown in Figure 10.2, which can be achieved by the
harmonic mixture method discussed in this Chapter.

79

80

CHAPTER10. HARMONICMIXTURES

Mixture models and graph based semi-supervised learning methods make dif-
ferent assumptions about the relation between unlabeled data and labels. Neverthe-
less, they are not mutually exclusive. It is possible that the data ts the component
model (e.g. Gaussian) locally, while the manifold structure appears globally. We
combine the best from both. From a graph method point of view, the resulting
model is a much smaller (thus computationally less expensive) backbone graph
with supernodes induced by the mixture components; From a mixture model
point of view, it is still inductive and naturally handles new points, but also has the
ability for labels to follow the data manifold. Our approach is related to graph reg-
ularization in (Belkin et al., 2004b), and is an alternative to the induction method in
(Delalleau et al., 2005). It should be noted that we are interested in mixture models
with a large number (possibly more than the number of labeled points) of compo-
nents, so that the manifold structure can appear, which is different from previous
works.

10.1 Review of Mixture Models and the EM Algorithm

In typical mixture models for classication, the generative process is the follow-
ing. One rst picks a class y, then chooses a mixture component m  {1 . . . M}
by p(m|y), and nally generates a point x according to p(x|m). Thus p(x, y) =
PM
m=1 p(y)p(m|y)p(x|m). In this paper we take a different but equivalent param-
eterization,

p(x, y) =

p(m)p(y|m)p(x|m)

(10.1)

MXm=1

We allow p(y|m) > 0 for all y, enabling classes to share a mixture component.
lihood of observed data:

The standard EM algorithm learns these parameters to maximize the log like-

L() = log p(xL, xU , yL|)

= XiL
= XiL

log p(xi, yi|) +XiU
MXm=1

log

log p(xi|)

(10.2)

p(m)p(yi|m)p(xi|m) +XiU

log

MXm=1

p(m)p(xi|m)

We introduce arbitrary distributions qi(m|i) on mixture membership, one for each

10.1. REVIEWOFMIXTUREMODELSANDTHEEMALGORITHM 81

i. By Jensens inequality

qi(m|xi, yi)

p(m)p(yi|m)p(xi|m)

qi(m|xi, yi)

(10.3)

MXm=1
MXm=1

log

log

L() = XiL
+XiU
MXm=1
 XiL
MXm=1
+XiU

 F(q, )

qi(m|xi)

qi(m|xi, yi) log

p(m)p(xi|m)

qi(m|xi)
p(m)p(yi|m)p(xi|m)

qi(m|xi, yi)

qi(m|xi) log

p(m)p(xi|m)

qi(m|xi)

(10.4)

(10.5)

The EM algorithm works by iterating coordinate-wise ascend on q and  to max-
imize F(q, ). The E step xes  and nds the q that maximizes F(q, ). We
denote the xed  at iteration t by p(m)(t), p(y|m)(t) and p(x|m)(t). Since the
terms of F has the form of KL divergence, it is easy to see that the optimal q are
the posterior on m:

q(t)
i (m|xi, yi) = p(m|xi, yi) =

q(t)
i (m|xi) = p(m|xi) =

p(m)(t)p(yi|m)(t)p(xi|m)(t)
PM
k=1 p(k)(t)p(yi|k)(t)p(xi|k)(t)
p(m)(t)p(xi|m)(t)
PM
k=1 p(k)(t)p(xi|k)(t)

, i  U

, i  L

(10.6)

The M step xes q(t) and nds (t+1) to maximize F. Taking the partial deriva-
tives and set to zero, we nd

p(m)(t+1)  XiLU

qi(m)(t)

(t+1)

m  p(y = 1|m)(t+1) = PiL, yi=1 qi(m)(t)
PiL qi(m)(t)

p(xi|m)

= 0

1

qi(m)(t)

p(xi|m)

x

(10.7)

(10.8)

(10.9)

XiLU

The last equation needs to be reduced further with the specic generative model

82

CHAPTER10. HARMONICMIXTURES

for x, e.g. Gaussian or multinomial. For Gaussian, we have

In practice one can smooth the ML estimate of covariance to avoid degeneracy:

(t+1)
m

(t+1)

m

(t+1)

m

=

m )(xi  (t)
m )>

m )(xi  (t)
m )>

= PiLU qi(m)(t)xi
PiLU qi(m)(t)
= PiLU qi(m)(t)(xi  (t)
PiLU qi(m)(t)
I +PiLU qi(m)(t)(xi  (t)
 +PiLU qi(m)(t)
MXm=1
= PM

p(y = 1|m)p(m|x)

m=1 p(y = 1|m)p(x|m)p(m)

m=1 p(x|m)p(m)

PM

p(y = 1|x) =

After EM converges, the classication of a new point x is done by

(10.10)

(10.11)

(10.12)

(10.13)

(10.14)

(10.15)

10.2 Label Smoothness on the Graph

Graph-based semi-supervised learning methods enforce label smoothness over a
graph, so that neighboring labels tend to have the same label. The graph has n
nodes L  U. Two nodes are connected by an edge with higher weights if they
are more likely to be in the same class. The graph is represented by the n  n
symmetric weight matrix W , and is assumed given.
Label smoothness can be expressed in different ways. We use the energy of the

label posterior as the measure,

where f is the label posterior vector, dened as

E(f ) =

1
2

nXi,j=1

wij (fi  fj)2 = f >f

fi =(cid:26)

(yi, 1)
p(yi = 1|xi, )

i  L
i  U

That is, fi is the probability that point i having label 1 under the mixture model
. The energy is small when f varies smoothly on the graph.  = D  W
is the combinatorial Laplacian matrix, and D is the diagonal degree matrix with
Dii = Pj wij. See Chapter 4 for more details. Other smoothness measures are

possible too, for example those derived from the normalized Laplacian (Zhou et al.,
2004a) or spectral transforms (Zhu et al., 2005).

10.3. COMBININGMIXTUREMODELANDGRAPH

83

10.3 Combining Mixture Model and Graph

We want to train a mixture model that maximizes the data log likelihood (10.3) and
minimizes the graph energy (10.14) at the same time. One way of doing so is to
learn the parameters p(m), p(x|m), p(y|m) to maximize the objective

O = L  (1  )E

(10.16)

where   [0, 1] is a coefcient that controls the relative strength of the two terms.
The E term may look like a prior ef >f on the parameters. But it involves the
observed labels yL, and is best described as a discriminative objective, while L
is a generative objective. This is closely related to, but different from, the graph
regularization framework of (Belkin et al., 2004b). Learning all the parameters
together however is difcult. Because of the E term, it is similar to conditional
EM training which is more complicated than the standard EM algorithm. Instead
we take a two-step approach:

 Step 1: Train all parameters p(m), p(x|m), p(y|m) with standard EM, which

maximizes L only;

 Step 2: Fix p(m) and p(x|m), and only learn p(y|m) to maximize (10.16).
It is suboptimal in terms of optimizing the objective function. However it has two
advantages: We created a concave optimization problem in the second step (see
section 10.3.2); Moreover, we can use standard EM without modication. We call
the solution harmonic mixtures.

We focus on step 2. The free parameters are p(y|m) for m = 1 . . . M. To sim-
plify the notation, we use the shorthand m  p(y = 1|m), and   (1, . . . , M )>.
We rst look at the special case with  = 0 in the objective function (10.16), as it
has a particularly simple closed form solution and interpretation. Notice although
 = 0, the generative objective L still inuences  through p(m) and p(x|m)
learned in step 1.

10.3.1 The Special Case with  = 0

We need to nd the parameters  that minimize E.  are constrained in [0, 1]M .
However let us look at the unconstrained optimization problem rst. Applying the
chain rule:

E
m

= h

E
fU

,

fU
mi

(10.17)

84

CHAPTER10. HARMONICMIXTURES

The rst term is

E
fU

=

=


fU

fU

(f >f )

(f >

L LLfL + 2f >

L LU fU + f >

U U U fU )

= 2LU fL + 2U U fU

(10.18)

(10.19)

(10.20)

where we partitioned the Laplacian matrix into labeled and unlabeled parts respec-
tively. The second term is

fU
m

= (p(m|xl+1), . . . , p(m|xl+u))>  Rm

(10.21)

where we dened a u  M responsibility matrix R such that Rim = p(m|xi), and
Rm is its m-th column. We used the fact that for i  U,

fi = p(yi = 1|xi, )

Pm p(m)p(xi|m)

= Pm p(m)p(yi = 1|m)p(xi|m)
= Xm
= Xm

p(m|xi)p(yi = 1|m)

p(m|xi)m

Notice we can write fU = R. Therefore

E
m

= R>

m (2U U fU + 2U LfL)

= R>

m (2U U R + 2U LfL)

(10.22)

(10.23)

(10.24)

(10.25)

(10.26)

(10.27)

When we put all M partial derivatives in a vector and set them to zero, we nd

E


= R> (2U U R + 2U LfL) = 0

(10.28)

where 0 is the zero vector of length M. This is a linear system and the solution is

 =  (R>U U R)1 R>U LfL

(10.29)

Notice this is the solution to the unconstrained problem, where some  might be
out of the bound [0, 1]. If it happens, we set out-of-bound s to their corresponding
boundary values of 0 or 1, and use them as starting point in a constrained convex

10.3. COMBININGMIXTUREMODELANDGRAPH

85

optimization (the problem is convex, as shown in the next section) to nd the global
solution. In practice however we found most of the time the closed form solution
for the unconstrained problem is already within bounds. Even when some compo-
nents are out of bounds, the solution is close enough to the constrained optimum
to allow quick convergence.

With the component class membership , the soft labels for the unlabeled data

are given by

fU = R
Unseen new points can be classied similarly.

(10.30)

We can compare (10.29) with the (completely graph based) harmonic function
solution (Zhu et al., 2003a). The former is fU = R (R>U U R)1 R>U LfL;
The latter is fU = 1
U U U LfL. Computationally the former only needs to invert
a M  M matrix, which is much cheaper than the latter of u  u because typically
the number of mixture components is much smaller than the number of unlabeled
points. This reduction is possible because fU are now tied together by the mixture
model.

In the special case where R corresponds to hard clustering, we just created a
much smaller backbone graph with supernodes induced by the mixture compo-
nents. In this case Rim = 1 for cluster m to which point i belongs, and 0 for all
other M  1 clusters. The backbone graph has the same L labeled nodes as in the
original graph, but only M unlabeled supernodes. Let wij be the weight between
nodes i, j in the original graph. By rearranging the terms it is not hard to show that
in the backbone graph, the equivalent weight between supernodes s, t  {1 . . . M}
is
(10.31)

RisRjtwij

wst = Xi,jU
wsl =XiU

and the equivalent weight between a supernode s and a labeled node l  L is

Riswil

(10.32)

 is simply the harmonic function on the supernodes in the backbone graph. For
this reason   [0, 1]M is guaranteed. Let c(m) = {i|Rim = 1} be the cluster m.
The equivalent weight between supernodes s, t reduces to

wst = Xic(s), jc(t)

wij

(10.33)

The supernodes are the clusters themselves. The equivalent weights are the sum
of edges between the clusters (or the cluster and a labeled node). One can easily

86

CHAPTER10. HARMONICMIXTURES

Input: initial mixture model p(m), p(x|m), p(y|m), m = 1 . . . M

data xL, yL, xU
graph Laplacian 

1. Run standard EM on data and get converged model p(m), p(x|m), p(y|m)
2. Fix p(m), p(x|m). Compute m  p(y = 1|m) =  (R>U U R)1 R>U LfL
3. Set out-of-bound s to 0 or 1, run constrained convex optimization
Output: mixture model p(m), p(x|m), p(y|m), m = 1 . . . M

Table 10.1: The harmonic mixture algorithm for the special case  = 0

create such a backbone graph by e.g. k-means clustering. In the general case when
R is soft, the solution deviates from that of the backbone graph.

The above algorithm is listed in Table 10.1. In practice some mixture compo-
nents may have little or no responsibility (p(m)  0). They should be excluded
from (10.29) to avoid numerical problems. In addition, if R is rank decient we
use the pseudo inverse in (10.29).

10.3.2 The General Case with  > 0
The objective (10.16) is concave in . To see this, we rst write L as
L() = XiL
= XiL

p(m)p(xi|m)m + XiL

p(m)p(yi|m)p(xi|m) + const

MXm=1
MXm=1

MXm=1

yi=1

log

log

yi=1

(10.34)

log

p(m)p(xi|m)(1  m) + const

Since we x p(m) and p(x|m), the term within the rst sum has the form logPm amm.

We can directly verify the Hessian

H = (cid:20)  logPm amm

ij

(cid:21) = 

1

(Pm amm)2 aa> (cid:22) 0

(10.35)

is negative semi-denite. Therefore the rst term (i  L and yi = 1) is concave.
Similarly the Hessian for the second term is

H = (cid:20)  logPm am(1  m)

ij

(cid:21) = 

aa>

(Pm am(1  m))2 (cid:22) 0 (10.36)

10.4. EXPERIMENTS

87

L is the non-negative sum of concave terms and is concave. Recall fU = R, the
graph energy can be written as

E = f >f

= f >
= f >

L LLfL + 2f >
L LLfL + 2f >

L LU fU + f >
L LU R + >R>U U R

U U U fU

(10.37)
(10.38)
(10.39)

The Hessian is 2R>U U R (cid:23) 0 because U U (cid:23) 0. Therefore E is convex in .
Putting them together, O is concave in .
As m is in [0, 1], we perform constrained convex optimization in the general
case with  > 0. The gradient of the objective is easily computed:

O
m

= 

L
m  (1  )

E
m

(10.40)

(10.41)

L
m

= XiL

yi=1

p(m)p(xi|m)

k=1 p(k)p(xi|k)k  XiL
PM

yi=1

PM

p(m)p(xi|m)

k=1 p(k)p(xi|k)(1  k)

(10.42)

and E/ was given in (10.28). One can also use the sigmoid function to trans-
form it into an unconstrained optimization problem with

m = (m) =

1

em + 1

(10.43)

and optimize the s.

Although the objective is concave, a good starting point for  is still important
to reduce the computation time until convergence. We nd a good initial value for
 by solving an one-dimensional concave optimization problem rst. We have two
parameters at hand: em is the solution from the standard EM algorithm in step
1, and special is the special case solution in section 10.3.1. We nd the optimal
interpolated coefcient   [0, 1]

init = em + (1  )special

(10.44)

that maximizes the objective (the optimal  in general will not be ). Then we start
from init and use a quasi-Newton algorithm to nd the global optimum for .

88

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

CHAPTER10. HARMONICMIXTURES

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Initial random GMM settings

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

(a) M = 2 Gaussian components

(b) M = 36 Gaussian components

After EM converges

Figure 10.1: Gaussian mixture models learned with the standard EM algorithm
cannot make labels follow the manifold structure in an articial dataset. Small dots
are unlabeled data. The two labeled points are marked with red + and green (cid:3).
The left panel has M = 2 and right M = 36 mixture components. Top plots show
the initial settings of the GMM. Bottom plots show the GMM after EM converges.
The ellipses are the contours of covariance matrices. The colored central dots
have sizes proportional to the component weight p(m). Components with very
small p(m) are not plotted. The color stands for component class membership
m  p(y = 1|m): red for  = 1, green for  = 0, and intermediate yellow for
values in between  which did not occur in the converged solutions. Notice in the
bottom-right plot, although the density p(x) is estimated well by EM,  does not
follow the manifold.

10.4. EXPERIMENTS

89

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

2.5

2

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Figure 10.2: The GMM with the component class membership  learned as in the
special case  = 0. , color coded from red to yellow and green, now follow the
structure of the unlabeled data.

10.4 Experiments

We test harmonic mixture on synthetic data, image and text classication. The
emphases are on how harmonic mixtures perform on unlabeled data compared to
EM or the harmonic function; how they handle unseen data; and whether they
can reduce the problem size. Unless otherwise noted, the harmonic mixtures are
computed with  = 0.

10.4.1 Synthetic Data

First we look at a synthetic dataset in Figure 10.1. It has a Swiss roll structure,
and we hope the labels can follow the spiral arms. There is one positive and one
negative labeled point, at roughly the opposite ends. We use u = 766 unlabeled
points and an additional 384 points as unseen test data.
The mixture model and standard EM. We start with Figure 10.1(a, top), the
initial setting for a Gaussian mixture model with M = 2 components. The initial
means are set by running a k-means algorithm. The initial covariances are identity,
thus the circles. The initial  are all set to 0.5, represented by the yellow color. (a,
bottom) shows the GMM after EM converges. Obviously it is a bad model because
M is too small.

Next we consider a Gaussian mixture model (GMM) with M = 36 compo-

90

CHAPTER10. HARMONICMIXTURES

nents, each with full covariance. Figure 10.1(b, top) shows the initial GMM and
(b, bottom) the converged GMM after running EM. The GMM models the manifold
density p(x) well. However the component class membership m  p(y = 1|m)
(red and green colors) does not follow the manifold. In fact  takes the extreme
values of 0 or 1 along a somewhat linear boundary instead of following the spiral
arms, which is undesirable. The classication of data points will not follow the
manifold either.
The graph and harmonic mixtures. Next we combine the mixture model with
a graph to compute the harmonic mixtures, as in the special case  = 0. We
construct a fully connected graph on the L  U data points with weighted edges
wij = exp(cid:0)||xi  xj||2/0.01(cid:1). We then reestimate , which are shown in Figure
10.2. Note  now follow the manifold as it changes from 0 (green) to approximately
0.5 (yellow) and nally 1 (red). This is the desired behavior.

The particular graph-based method we use needs extra care. The harmonic
function solution f is known to sometimes skew toward 0 or 1. This problem is
easily corrected if we know or have an estimate of the proportion of positive and
negative points, with the Class Mass Normalization heuristic (Zhu et al., 2003a).
In this paper we use a similar but simpler heuristic. Assuming the two classes are
about equal in size, we simply set the decision boundary at the median. That is, let
f (l + 1), . . . , f (n) be the soft label values on the unlabeled nodes. Let m(f ) =
median(f (l + 1), . . . , f (n)). We classify point i as positive if f (i) > m(f ), and
negative otherwise.
Sensitivity to M. If the number of mixture components M is too small, the GMM
is unable to model p(x) well, let alone . In other words, the harmonic mixture
is sensitive to M. M has to be larger than a certain threshold so that the man-
ifold structure can appear. In fact M may need to be larger than the number of
labeled points l, which is unusual in traditional mixture model methods for semi-
supervised learning. However once M is over the threshold, further increase should
not dramatically change the solution. In the end the harmonic mixture may ap-
proach the harmonic function solution when M = u.

Figure 10.3(a) shows the classication accuracy on U as we change M. We
nd that the threshold for harmonic mixtures is M = 35, at which point the ac-
curacy (HM) jumps up and stabilizes thereafter. This is the number of mixture
components needed for harmonic mixture to capture the manifold structure. The
harmonic function on the complete graph (graph) is not a mixture model and
appears at. The EM algorithm (EM) fails to discover the manifold structure
regardless of the number of mixtures M.
Computational savings. The harmonic mixtures perform almost as well as the
harmonic function on the complete graph, but with a much smaller problem size.
As Figure 10.3(a) shows, we only need to invert a 35  35 matrix instead of a

10.4. EXPERIMENTS

91

766  766 one as required by the harmonic function solution. The difference can
be signicant if the unlabeled set size is even larger. There is of course the overhead
of EM training.
Handling unseen data. Because the harmonic mixture model is a mixture model,
it naturally handles unseen points. On 384 new test points harmonic mixtures
perform similarly to Figure 10.3(a), with accuracies around 95.3% after M  35.
10.4.2

Image Recognition: Handwritten Digits

We use the 1vs2 dataset which contains equal number of images of handwritten
digit of 1s and 2s. Each gray scale image is 8  8, which is represented by a 64
dimensional vector of pixel values. We use l + u = 1600 images as the labeled and
unlabeled set, and 600 additional images as unseen new data to test induction.
The mixture model. We use Gaussian mixture models. To avoid data sparse-
ness problem, we model each Gaussian component with a spherical covariance,
i.e. diagonal covariance matrix with the same variance in all dimensions. Different
components may have different variances. We set the initial means and variances
of the GMM with k-means algorithm before running EM.
The graph. We use a symmetrized 10-nearest-neighbor weighted graph on the
1600 images. That is, images i, j are connected if i is within js 10NN or vice

versa, as measured by Euclidean distance. The weights are wij = exp(cid:0)||xi  xj||2/1402(cid:1).

Sensitivity to M. As illustrated in the synthetic data, the number of mixture com-
ponents M needs to be large enough for harmonic mixture to work. We vary M
and observe the classication accuracies on the unlabeled data with different meth-
ods. For each M we perform 20 trials with random L/U split and plot the mean
and standard deviation of classication accuracies in Figure 10.3(b). The exper-
iments were performed with labeled set size xed at l = 10. We conclude that
harmonic mixtures need only M  100 components to match the performance of
the harmonic function method.
Computational savings. In terms of graph method computation, we invert a 100
100 matrix instead of the original 1590  1590 matrix for harmonic function. This
is good saving with little sacrice in accuracy. We x M = 100 in the experiments
that follow.
Handling unseen data. We systematically vary labeled set size l. For each l we
run 20 random trials. The classication accuracy on U (with 1600-l points) and
unseen data (600 points) are listed in Table 10.2. On U, harmonic mixtures (HM)
achieve the same accuracy as harmonic function (graph). Both are not sensitive to
l. The GMM trained with EM (EM) also performs well when l is not too small,
but suffers otherwise. On the unseen test data, the harmonic mixtures maintain
high accuracy.

92

CHAPTER10. HARMONICMIXTURES

The general case  > 0. We also vary the parameter  between 0 and 1, which
balances the generative and discriminative objectives. In our experiments  = 0
always gives the best accuracies.

10.4.3 Text Categorization: PC vs. Mac

We perform binary text classication on the two groups comp.sys.ibm.pc.hardware
vs. comp.sys.mac.hardware (982 and 961 documents respectively) in the 18828
version of the 20-newsgroups data. We use rainbow (McCallum, 1996) to prepro-
cess the data, with the default stopword list, no stemming, and keep words that
occur at least 5 times. We represent documents by tf.idf vectors with the Okapi
TF formula (Zhai, 2001), which was also used in (Zhu et al., 2003a). Of the 1943
documents, we use 1600 as L  U and the rest as unseen test data.
The mixture model. We use multinomial mixture models (bag-of-words naive
Bayes model), treating tf.idf as pseudo word counts of the documents. We found
this works better than using the raw word counts. We use k-means to initialize the
models.
The graph. We use a symmetrized 10NN weighted graph on the 1600 docu-
ments. The weight between documents u, v is wuv = exp ((1  cuv)/0.03),
where cuv = hu, vi/ (||u||  ||v||) is the cosine between the tf.idf vectors u, v.
Sensitivity to M. The accuracy on U with different number of components M
is shown in Figure 10.3(c).
l is xed at 10. Qualitatively the performance of
harmonic mixtures increases when M > 400. From the plot it may look like the
graph curve varies with M, but this is an artifact as we used different randomly
sampled L, U splits for different M. The error bars on harmonic mixtures are large.
We suspect the particular mixture model is bad for the task.
Computational savings. Unlike the previous tasks, we need a much larger M
around 600. We still have a smaller problem than the original u = 1590, but the
saving is limited.
Handling unseen data. We x M = 600 and vary labeled set size l. For each l we
run 20 random trials. The classication accuracy on U (with 1600-l documents)
and unseen data (343 documents) are listed in Table 10.3. The harmonic mixture
model has lower accuracies than the harmonic function on the L  U graph. The
harmonic mixture model performs similarly on U and on unseen data.

10.5 Related Work

Recently Delalleau et al. (2005) use a small random subset of the unlabeled data to
create a small graph. This is related to the Nystrom method in spectral clustering

10.5. RELATEDWORK

93

l

2
5
10
20
30

2
5
10
20
30

HM

98.7  0.0
98.7  0.0
98.7  0.1
98.7  0.2
98.7  0.2
96.1  0.1
96.1  0.1
96.1  0.1
96.1  0.1
96.1  0.1

EM
on U:

86.7  5.7
90.1  4.1
93.6  2.4
96.0  3.2
97.1  1.9
on unseen:
87.1  5.4
89.8  3.8
93.2  2.3
95.1  3.2
96.8  1.7

graph

98.7  0.0
98.7  0.1
98.7  0.1
98.7  0.2
98.8  0.2

-
-
-
-
-

Table 10.2: Image classication 1 vs. 2: Accuracy on U and unseen data. M =
100. Each number is the mean and standard deviation of 20 trials.

l

2
5
10
20
40

2
5
10
20
40

HM

75.9  14.3
74.5  16.6
84.5  2.1
83.3  7.1
85.7  2.3
73.6  13.0
73.2  15.2
82.9  2.9
82.0  6.5
84.7  3.3

EM
on U:

54.5  6.2
53.7  5.2
55.7  6.5
59.5  6.4
61.8  6.1
on unseen:
53.5  6.0
52.3  5.9
55.7  5.7
58.9  6.1
60.4  5.9

graph

84.6  10.9
87.9  3.9
89.5  1.0
90.1  1.0
90.3  0.6

-
-
-
-
-

Table 10.3: Text classication PC vs. Mac: Accuracy on U and unseen data.
M = 600. Each number is the mean and standard deviation of 20 trials.

94

CHAPTER10. HARMONICMIXTURES

(Fowlkes et al., 2004), and to the random landmarks in dimensionality reduction
(Weinberger et al., 2005). Our method is different in that

 It incorporates a generative mixture model, which is a second knowledge

source besides the graph;

 The backbone graph is not built on randomly selected points, but on mean-

ingful mixture components;

 When classifying an unseen point x, it does not need graph edges from land-
mark points to x. This is less demanding on the graph because the burden
is transferred to the mixture component models. For example one can now
use kNN graphs. In the other works one needs edges between x and the
landmarks, which are non-existent or awkward for kNN graphs.

In terms of handling unseen data, our approach is closely related to the regu-
larization framework of (Belkin et al., 2004b; Krishnapuram et al., 2005) as graph
regularization on mixture models. However instead of a regularization term we
used a discriminative term, which allows for the closed form solution in the special
case.

10.6 Discussion

To summarize, the proposed harmonic mixture method reduces the graph prob-
lem size, and handles unseen test points. It achieves comparable accuracy as the
harmonic function for semi-supervised learning.

There are several questions for further research. First, the component model
affects the performance of the harmonic mixtures. For example the Gaussian in the
synthetic task and 1 vs. 2 task seem to be more amenable to harmonic mixtures
than the multinomial in PC vs. Mac task. How to quantify the inuence remains a
question. A second question is when  > 0 is useful in practice. Finally, we want
to nd a way to automatically select the appropriate number of mixture components
M.

The backbone graph is certainly not the only way to speed up computation.
We list some other methods in literature review in Chapter 11. In addition, we
also performed an empirical study to compare several iterative methods, including
Label Propagation, loopy belief propagation, and conjugate gradient, which all
converge to the harmonic function. The study is presented in Appendix F.

10.6. DISCUSSION

95

graph
HM
EM

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55



U
n
o


y
c
a
r
u
c
c
A

0.5

0

5

10

15

20

30

35

40

45

50

25
M

(a) synthetic data

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65



U
n
o


y
c
a
r
u
c
c
A

graph
HM
EM

0.6

0

20

40

60

80

120

140

160

180

200

100
M

(b) 1 vs. 2

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55



U
n
o

y
c
a
r
u
c
c
A

graph
HM
EM

0.5

100

200

300

400

500

600

700

800

M

(c) PC vs. Mac

Figure 10.3: Sensitivity to M in three datasets. Shown are the classication accu-
racies on U as M changes. graph is the harmonic function on the complete L U
graph; HM is the harmonic mixture, and EM is the standard EM algorithm. The
intervals are 1 standard deviation with 20 random trials when applicable.

96

CHAPTER10. HARMONICMIXTURES

Chapter 11

Literature Review

We review some of the literature on semi-supervised learning. There has been a
whole spectrum of interesting ideas on how to learn from both labeled and un-
labeled data. The review is by no means comprehensive and the eld of semi-
supervised learning is evolving rapidly. The author apologizes in advance for any
inaccuracies in the descriptions, and welcomes corrections and comments. Please
send corrections and suggest papers to zhuxj@cs.cmu.edu. To make the review
more useful, we maintain an online version at
http://www.cs.cmu.edu/zhuxj/pub/semireview.html
which will be updated indenitely.

11.1 Q&A

Q: What is semi-supervised learning?
A: Its a special form of classication. Traditional classiers need labeled data
(feature / label pairs) to train. Labeled instances however are often difcult, ex-
pensive, or time consuming to obtain, as they require the efforts of experienced
human annotators. Meanwhile unlabeled data may be relatively easy to collect,
but there has been few ways to use them. Semi-supervised learning addresses this
problem by using large amount of unlabeled data, together with the labeled data,
to build better classiers. Because semi-supervised learning requires less human
effort and gives higher accuracy, it is of great interest both in theory and in practice.

Q: Can we really learn anything from unlabeled data? It looks like magic.
A: Yes we can  under certain assumptions. Its not magic, but good matching of
problem structure with model assumption.

97

98

CHAPTER11. LITERATUREREVIEW

Q: Does unlabeled data always help?
A: No, theres no free lunch. Bad matching of problem structure with model as-
sumption can lead to degradation in classier performance. For example, quite a
few semi-supervised learning methods assume that the decision boundary should
avoid regions with high p(x). These methods include transductive support vector
machines (SVMs), information regularization, Gaussian processes with null cate-
gory noise model, graph-based methods if the graph weights is determined by pair-
wise distance. Nonetheless if the data is generated from two heavily overlapping
Gaussian, the decision boundary would go right through the densest region, and
these methods would perform badly. On the other hand EM with generative mix-
ture models, another semi-supervised learning method, would have easily solved
the problem. Detecting bad match in advance however is hard and remains an open
question.

Q: How many semi-supervised learning methods are there?
A: Many. Some often-used methods include: EM with generative mixture models,
self-training, co-training, transductive support vector machines, and graph-based
methods. See the following sections for more methods.

Q: Which method should I use / is the best?
A: There is no direct answer to this question. Because labeled data is scarce, semi-
supervised learning methods make strong model assumptions. Ideally one should
use a method whose assumptions t the problem structure. This may be difcult
in reality. Nonetheless we can try the following checklist: Do the classes produce
well clustered data? If yes, EM with generative mixture models may be a good
choice; Do the features naturally split into two sets? If yes, co-training may be
appropriate; Is it true that two points with similar features tend to be in the same
class? If yes, graph-based methods can be used; Already using SVM? Transductive
SVM is a natural extension; Is the existing supervised classier complicated and
hard to modify? Self-training is a practical wrapper method.

Q: How do semi-supervised learning methods use unlabeled data?
A: Semi-supervised learning methods use unlabeled data to either modify or re-
prioritize hypotheses obtained from labeled data alone. Although not all methods
are probabilistic, it is easier to look at methods that represent hypotheses by p(y|x),
and unlabeled data by p(x). Generative models have common parameters for the
joint distribution p(x, y). It is easy to see that p(x) inuences p(y|x). Mixture
models with EM is in this category, and to some extent self-training. Many other
methods are discriminative, including transductive SVM, Gaussian processes, in-
formation regularization, and graph-based methods. Original discriminative train-

11.2. GENERATIVEMIXTUREMODELSANDEM

99

ing cannot be used for semi-supervised learning, since p(y|x) is estimated ignoring
p(x). To solve the problem, p(x) dependent terms are often brought into the ob-
jective function, which amounts to assuming p(y|x) and p(x) share parameters.
Q: Where can I learn more?
A: An existing survey can be found in (Seeger, 2001).

11.2 Generative Mixture Models and EM

This is perhaps the oldest semi-supervised learning method. It assumes a genera-
tive model p(x, y) = p(y)p(x|y) where p(x|y) is an identiable mixture distribu-
tion, for example Gaussian mixture models. With large amount of unlabeled data,
the mixture components can be identied; then ideally we only need one labeled
example per component to fully determine the mixture distribution. One can think
of the mixture components as soft clusters.

Nigam et al. (2000) apply the EM algorithm on mixture of multinomial for
the task of text classication. They showed the resulting classiers perform better
than those trained only from L. Baluja (1998) uses the same algorithm on a face
orientation discrimination task.

One has to pay attention to a few things:

Identiability

11.2.1
The mixture model ideally should be identiable. In general let {p} be a family of
distributions indexed by a parameter vector .  is identiable if 1 6= 2  p1 6=
p2, up to a permutation of mixture components. If the model family is identiable,
in theory with innite U one can learn  up to a permutation of component indices.
Here is an example showing the problem with unidentiable models. The
model p(x|y) is uniform for y  {+1,1}. Assuming with large amount of un-
labeled data U we know p(x) is uniform in [0, 1]. We also have 2 labeled data
points (0.1, +1), (0.9,1). Can we determine the label for x = 0.5? No. With
our assumptions we cannot distinguish the following two models:
p(y = 1) = 0.2, p(x|y = 1) = unif(0, 0.2), p(x|y = 1) = unif(0.2, 1) (11.1)
p(y = 1) = 0.6, p(x|y = 1) = unif(0, 0.6), p(x|y = 1) = unif(0.6, 1) (11.2)
which give opposite labels at x = 0.5, see Figure 11.1. It is known that a mixture of
Gaussian is identiable. Mixture of multivariate Bernoulli (McCallum & Nigam,
1998a) is not identiable. More discussions on identiability and semi-supervised
learning can be found in e.g. (Ratsaby & Venkatesh, 1995) and (Corduneanu &
Jaakkola, 2001).

100

CHAPTER11. LITERATUREREVIEW

p(x)=1












0

1

+ 0.8 *

p(x|y=1)=5




























= 0.2 *

0 0.2

1

0

= 0.6 *

p(x|y=1)=1.67


















+ 0.4 *

0

0.6

1

0

p(x|y=1)=1.25
















0.2

1

p(x|y=1)=2.5


























0.6

1

Figure 11.1: An example of unidentiable models. Even if we known p(x) (top)
is a mixture of two uniform distributions, we cannot uniquely identify the two
components. For instance, the mixtures on the second and third line give the same
p(x), but they classify x = 0.5 differently.

6

4

2

0

2

4

6
6

Class 1

Class 2

4

2

0

2

4

6

6

4

2

0

2

4

6
6

4

2

0

2

4

6

6

4

2

0

2

4

6
6

4

2

0

2

4

6

(a) Horizontal class separation

(b) High probability

(c) Low probability

Figure 11.2: If the model is wrong, higher likelihood may lead to lower classica-
tion accuracy. For example, (a) is clearly not generated from two Gaussian. If we
insist that each class is a single Gaussian, (b) will have higher probability than (c).
But (b) has around 50% accuracy, while (c)s is much better.

11.2.2 Model Correctness

If the mixture model assumption is correct, unlabeled data is guaranteed to improve
accuracy (Castelli & Cover, 1995) (Castelli & Cover, 1996) (Ratsaby & Venkatesh,
1995). However if the model is wrong, unlabeled data may actually hurt accuracy.
Figure 11.2 shows an example. This has been observed by multiple researchers.
Cozman et al. (2003) give a formal derivation on how this might happen.

It is thus important to carefully construct the mixture model to reect reality.
For example in text categorization a topic may contain several sub-topics, and will
be better modeled by multiple multinomial instead of a single one (Nigam et al.,
2000). Some other examples are (Shahshahani & Landgrebe, 1994) (Miller &
Uyar, 1997). Another solution is to down-weighing unlabeled data (Corduneanu &

11.3. SELF-TRAINING

101

Jaakkola, 2001), which is also used by Nigam et al. (2000), and by Callison-Burch
et al. (2004) who estimate word alignment for machine translation.

11.2.3 EM Local Maxima

Even if the mixture model assumption is correct, in practice mixture components
are identied by the Expectation-Maximization (EM) algorithm (Dempster et al.,
1977). EM is prone to local maxima. If a local maximum is far from the global
maximum, unlabeled data may again hurt learning. Remedies include smart choice
of starting point by active learning (Nigam, 2001).

11.2.4 Cluster and Label

We shall also mention that instead of using an probabilistic generative mixture
model, some approaches employ various clustering algorithms to cluster the whole
dataset, then label each cluster with labeled data, e.g. (Demiriz et al., 1999) (Dara
et al., 2000). Although they may perform well if the particular clustering algo-
rithms match the true data distribution, these approaches are hard to analyze due to
their algorithmic nature.

11.3 Self-Training

Self-training is a commonly used technique for semi-supervised learning. In self-
training a classier is rst trained with the small amount of labeled data. The
classier is then used to classify the unlabeled data. Typically the most condent
unlabeled points, together with their predicted labels, are added to the training
set. The classier is re-trained and the procedure repeated. Note the classier
uses its own predictions to teach itself. The procedure is also called self-teaching
or bootstrapping (not to be confused with the statistical procedure with the same
name). The generative model and EM approach of section 11.2 can be viewed as
a special case of soft self-training. One can imagine that a classication mistake
can reinforce itself. Some algorithms try to avoid this by unlearn unlabeled points
if the prediction condence drops below a threshold.

Self-training has been applied to several natural language processing tasks.
Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding
whether the word plant means a living organism or a factory in a give context.
Riloff et al. (2003) uses it to identify subjective nouns. Maeireizo et al. (2004)
classify dialogues as emotional or non-emotional with a procedure involving
two classiers.Self-training has also been applied to parsing and machine transla-
tion. Rosenberg et al. (2005) apply self-training to object detection systems from

102

CHAPTER11. LITERATUREREVIEW

+

+

+

+

+

+
+

+

+

+

+

+

+

+

+

+

+

+

+

+

+

+



+

+

+




+

+


















+

+

+

+

+

+
+

+

+

+

+

+
++
+

+

++
+

+

++

+

+





+

+

+













































 




(a) x1 view

(b) x2 view

Figure 11.3: Co-Training: Conditional independent assumption on feature split.
With this assumption the high condent data points in x1 view, represented by
circled labels, will be randomly scattered in x2 view. This is advantageous if they
are to be used to teach the classier in x2 view.

images, and show the semi-supervised technique compares favorably with a state-
of-the-art detector.

11.4 Co-Training

Co-training (Blum & Mitchell, 1998) (Mitchell, 1999) assumes that features can
be split into two sets; Each sub-feature set is sufcient to train a good classier;
The two sets are conditionally independent given the class. Initially two separate
classiers are trained with the labeled data, on the two sub-feature sets respectively.
Each classier then classies the unlabeled data, and teaches the other classier
with the few unlabeled examples (and the predicted labels) they feel most con-
dent. Each classier is retrained with the additional training examples given by the
other classier, and the process repeats.

In co-training, unlabeled data helps by reducing the version space size. In other
words, the two classiers (or hypotheses) must agree on the much larger unlabeled
data as well as the labeled data.

We need the assumption that sub-features are sufciently good, so that we can
trust the labels by each learner on U. We need the sub-features to be conditionally
independent so that one classiers high condent data points are iid samples for
the other classier. Figure 11.3 visualizes the assumption.

Nigam and Ghani (2000) perform extensive empirical experiments to compare
co-training with generative mixture models and EM. Their result shows co-training
performs well if the conditional independence assumption indeed holds. In addi-
tion, it is better to probabilistically label the entire U, instead of a few most con-
dent data points. They name this paradigm co-EM. Finally, if there is no natural
feature split, the authors create articial split by randomly break the feature set into

11.5. MAXIMIZINGSEPARATION

103

two subsets. They show co-training with articial feature split still helps, though
not as much as before. Jones (2005) used co-training, co-EM and other related
methods for information extraction from text.

Co-training makes strong assumptions on the splitting of features. One might
wonder if these conditions can be relaxed. Goldman and Zhou (2000) use two
learners of different type but both takes the whole feature set, and essentially use
one learners high condence data points, identied with a set of statistical tests, in
U to teach the other learning and vice versa. Recently Balcan et al. (2005) relax
the conditional independence assumption with a much weaker expansion condition,
and justify the iterative co-training procedure.

11.5 Maximizing Separation

11.5.1 Transductive SVM
Discriminative methods work on p(y|x) directly. This brings up the danger of
leaving p(x) outside of the parameter estimation loop, if p(x) and p(y|x) do not
share parameters. Notice p(x) is usually all we can get from unlabeled data. It is
believed that if p(x) and p(y|x) do not share parameters, semi-supervised learning
cannot help. This point is emphasized in (Seeger, 2001). Zhang and Oles (2000)
give both theoretical and experimental evidence of the same point specically on
transductive support vector machines (TSVM). However this is controversial as
empirically TSVMs seem benecial.

TSVM is an extension of standard support vector machines with unlabeled
data. In a standard SVM only the labeled data is used, and the goal is to nd a
maximum margin linear boundary in the Reproducing Kernel Hilbert Space. In a
TSVM the unlabeled data is also used. The goal is to nd a labeling of the unla-
beled data, so that a linear boundary has the maximum margin on both the original
labeled data and the (now labeled) unlabeled data. The decision boundary has the
smallest generalization error bound on unlabeled data (Vapnik, 1998). Intuitively,
unlabeled data guides the linear boundary away from dense regions. However
nding the exact transductive SVM solution is NP-hard. Several approximation al-
gorithms have been proposed and show positive results, see e.g. (Joachims, 1999)
(Bennett & Demiriz, 1999) (Demirez & Bennettt, 2000) (Fung & Mangasarian,
1999) (Chapelle & Zien, 2005).

The maximum entropy discrimination approach (Jaakkola et al., 1999) also
maximizes the margin, and is able to take into account unlabeled data, with SVM
as a special case.

The application of graph kernels (Zhu et al., 2005) to SVMs differs from
TSVM. The graph kernels are special semi-supervised kernels applied to a stan-

104

CHAPTER11. LITERATUREREVIEW

+

+

+

+

+









Figure 11.4: In TSVM, U helps to put the decision boundary in sparse regions.
With labeled data only, the maximum margin boundary is plotted with dotted lines.
With unlabeled data (black dots), the maximum margin boundary would be the one
with solid lines.

dard SVM; TSVM is a special optimization criterion regardless of the kernel being
used.

11.5.2 Gaussian Processes

Lawrence and Jordan (2005) proposed a Gaussian process approach, which can be
viewed as the Gaussian process parallel of TSVM. The key difference to a standard
Gaussian process is in the noise model. A null category noise model maps the
hidden continuous variable f to three instead of two labels, specically to the never
used label 0 when f is around zero. On top of that, it is restricted that unlabeled
data points cannot take the label 0. This pushes the posterior of f away from zero
for the unlabeled points. It achieves the similar effect of TSVM where the margin
avoids dense unlabeled data region. However nothing special is done on the process
model. Therefore all the benet of unlabeled data comes from the noise model. A
very similar noise model is proposed in (Chu & Ghahramani, 2004) for ordinal
regression.

This is different from the Gaussian processes in (Zhu et al., 2003c), where we
have a semi-supervised Gram matrix, and semi-supervised learning originates from
the process model, not the noise model.

11.5.3

Information Regularization

Szummer and Jaakkola (2002) propose the information regularization framework
to control the label conditionals p(y|x) by p(x), where p(x) may be estimated from
unlabeled data. The idea is that labels shouldnt change too much in regions where
p(x) is high. The authors use the mutual information I(x; y) between x and y as
a measure of label complexity. I(x; y) is small when the labels are homogeneous,

11.6. GRAPH-BASEDMETHODS

105

and large when labels vary. This motives the minimization of the product of p(x)
mass in a region with I(x; y) (normalized by a variance term). The minimization
is carried out on multiple overlapping regions covering the data space.

The theory is developed further in (Corduneanu & Jaakkola, 2003). Cor-
duneanu and Jaakkola (2005) extend the work by formulating semi-supervised
learning as a communication problem. Regularization is expressed as the rate of
information, which again discourages complex conditionals p(y|x) in regions with
high p(x). The problem becomes nding the unique p(y|x) that minimizes a regu-
larized loss on labeled data. The authors give a local propagation algorithm.

11.5.4 Entropy Minimization

The hyperparameter learning method in section 7.2 uses entropy minimization.
Grandvalet and Bengio (2005) used the label entropy on unlabeled data as a reg-
ularizer. By minimizing the entropy, the method assumes a prior which prefers
minimal class overlap.

11.6 Graph-Based Methods

Graph-based semi-supervised methods dene a graph where the nodes are labeled
and unlabeled examples in the dataset, and edges (may be weighted) reect the
similarity of examples. These methods usually assume label smoothness over the
graph. Graph methods are nonparametric, discriminative, and transductive in na-
ture. This thesis largely focuses on graph-based semi-supervised learning algo-
rithms.

11.6.1 Regularization by Graph

Many graph-based methods can be viewed as estimating a function f on the graph.
One wants f to satisfy two things at the same time: 1) it should be close to the
given labels yL on the labeled nodes, and 2) it should be smooth on the whole
graph. This can be expressed in a regularization framework where the rst term is
a loss function, and the second term is a regularizer.

Several graph-based methods listed here are similar to each other. They differ
in the particular choice of the loss function and the regularizer. Are these differ-
ences crucial? Probably not. We believe it is much more important to construct
a good graph than to choose among the methods. However graph construction, as
we will see later, is not a well studied area.

106

Mincut

CHAPTER11. LITERATUREREVIEW

Blum and Chawla (2001) pose semi-supervised learning as a graph mincut (also
known as st-cut) problem. In the binary case, positive labels act as sources and
negative labels act as sinks. The objective is to nd a minimum set of edges whose
removal blocks all ow from the sources to the sinks. The nodes connecting to the
sources are then labeled positive, and those to the sinks are labeled negative. Equiv-
alently mincut is the mode of a Markov random eld with binary labels (Boltzmann
machine). The loss function can be viewed as a quadratic loss with innity weight:

PiL(yi  yi|L)2, so that the values on labeled data are in fact clamped. The

labeling y minimizes

1

2Xi,j

wij|yi  yj| =

1

2Xi,j

wij(yi  yj)2

(11.3)

which can be thought of as a regularizer on binary (0 and 1) labels.

One problem with mincut is that it only gives hard classication without con-
dence. Blum et al. (2004) perturb the graph by adding random noise to the edge
weights. Mincut is applied to multiple perturbed graphs, and the labels are deter-
mined by a majority vote. The procedure is similar to bagging, and creates a soft
mincut.

Pang and Lee (2004) use mincut to improve the classication of a sentence into
either objective or subjective, with the assumption that sentences close to each
other tend to have the same class.

Gaussian Random Fields and Harmonic Functions

The Gaussian random elds and harmonic function methods in (Zhu et al., 2003a)
can be viewed as having a quadratic loss function with innity weight, so that
the labeled data are clamped, and a regularizer based on the graph combinatorial
Laplacian :

XiL
= XiL

(fi  yi)2 + 1/2Xi,j
(fi  yi)2 + f >f

wij(fi  fj)2

(11.4)

(11.5)

Recently Grady and Funka-Lea (2004) applied the harmonic function method to
medical image segmentation tasks, where a user labels classes (e.g. different or-
gans) with a few strokes. Levin et al. (2004) use essentially harmonic functions for
colorization of gray-scale images. Again the user species the desired color with

11.6. GRAPH-BASEDMETHODS

107

only a few strokes on the image. The rest of the image is used as unlabeled data,
and the labels propagation through the image. Niu et al. (2005) applied the label
propagation algorithm (which is equivalent to harmonic functions) to word sense
disambiguation.

Local and Global Consistency

The local and global consistency method (Zhou et al., 2004a) uses the loss function
Pn
i=1(fiyi)2, and the normalized Laplacian D1/2D1/2 = ID1/2W D1/2

in the regularizer,

1/2Xi,j

wij(fi/pDii  fj/pDjj)2 = f >D1/2D1/2f

(11.6)

Tikhonov Regularization

The Tikhonov regularization algorithm in (Belkin et al., 2004a) uses the loss func-
tion and regularizer:

1/kXi

(fi  yi)2 + f >Sf

(11.7)

where S =  or p for some integer p.

Graph Kernels

For kernel methods, the regularizer is a (typically monotonically increasing) func-
tion of the RKHS norm ||f||K = f >K1f with kernel K. Such kernels are derived
from the graph, e.g. the Laplacian.
Chapelle et al. (2002) and Smola and Kondor (2003) both show the spectral
transformation of a Laplacian results in kernels suitable for semi-supervised learn-
ing. The diffusion kernel (Kondor & Lafferty, 2002) corresponds to a spectrum
transform of the Laplacian with

r() = exp(

2
2

)

(11.8)

The regularized Gaussian process kernel  + I/2 in (Zhu et al., 2003c) corre-
sponds to

r() =

1

 + 

(11.9)

Similarly the order constrained graph kernels in (Zhu et al., 2005) are con-
structed from the spectrum of the Laplacian, with non-parametric convex opti-
mization. Learning the optimal eigenvalues for a graph kernel is in fact a way to

108

CHAPTER11. LITERATUREREVIEW

(at least partially) correct an imprecise graph. In this sense it is related to graph
construction.

Spectral Graph Transducer

The spectral graph transducer (Joachims, 2003) can be viewed with a loss function
and regularizer

c(f  )>C(f  ) + f >Lf

(11.10)

where i = pl/l+ for positive labeled data, pl+/l for negative data, l

being the number of negative data and so on. L can be the combinatorial or nor-
malized graph Laplacian, with a transformed spectrum.

Tree-Based Bayes
Kemp et al. (2003) dene a probabilistic distribution P (Y |T ) on discrete (e.g. 0
and 1) labelings Y over an evolutionary tree T . The tree T is constructed with
the labeled and unlabeled data being the leaf nodes. The labeled data is clamped.
The authors assume a mutation process, where a label at the root propagates down
to the leaves. The label mutates with a constant rate as it moves down along the
edges. As a result the tree T (its structure and edge lengths) uniquely denes the
label prior P (Y |T ). Under the prior if two leaf nodes are closer in the tree, they
have a higher probability of sharing the same label. One can also integrate over all
tree structures.

The tree-based Bayes approach can be viewed as an interesting way to incor-
porate structure of the domain. Notice the leaf nodes of the tree are the labeled and
unlabeled data, while the internal nodes do not correspond to physical data. This is
in contrast with other graph-based methods where labeled and unlabeled data are
all the nodes.

Some Other Methods

Szummer and Jaakkola (2001) perform a t-step Markov random walk on the graph.
The inuence of one example to another example is proportional to how easy the
random walk goes from one to the other. It has certain resemblance to the diffusion
kernel. The parameter t is important.

Chapelle and Zien (2005) use a density-sensitive connectivity distance between
nodes i, j (a given path between i, j consists of several segments, one of them
is the longest; now consider all paths between i, j and nd the shortest longest
segment). Exponentiating the negative distance gives a graph kernel.

11.6. GRAPH-BASEDMETHODS

109

Bousquet et al.

(2004) consider the continuous counterpart of graph-based
regularization. They dene regularization based on a known p(x) and provide
interesting theoretical analysis. However there seem to be problems in applying
the theoretical results to higher (D > 2) dimensional tasks.

11.6.2 Graph Construction

Although the graph is the heart and soul of graph-based semi-supervised learning
methods, its construction has not been studied carefully. The issue has been dis-
cussed informally in Chapter 3, and graph hyperparameter learning discussed in
Chapter 7. There are relatively few literatures on graph construction. For example
Carreira-Perpinan and Zemel (2005) build robust graphs from multiple minimum
spanning trees by perturbation and edge removal. It is possible that graph construc-
tion is domain specic because it encodes prior knowledge, and has thus far been
treated on an individual basis.

11.6.3

Induction

Most graph-based semi-supervised learning algorithms are transductive, i.e. they
cannot easily extend to new test points outside of L  U. Recently induction has
received increasing attention. One common practice is to freeze the graph on
L  U. New points do not (although they should) alter the graph structure. This
avoids expensive graph computation every time one encounters new points.
Zhu et al. (2003c) propose that new test point be classied by its nearest neigh-
bor in LU. This is sensible when U is sufciently large. In (Chapelle et al., 2002)
the authors approximate a new point by a linear combination of labeled and unla-
beled points. Similarly in (Delalleau et al., 2005) the authors proposes an induction
scheme to classify a new point x by

f (x) = PiLU wxif (xi)
PiLU wxi

(11.11)

This can be viewed as an application of the Nystrom method (Fowlkes et al., 2004).
In the regularization framework of (Belkin et al., 2004b), the function f does
not have to be restricted to the graph. The graph is merely used to regularize f
which can have a much larger support. It is necessarily a combination of an in-
ductive algorithm and graph regularization. The authors give the graph-regularized
version of least squares and SVM. Note such an SVM is different from the graph
kernels in standard SVM in (Zhu et al., 2005). The former is inductive with both
a graph regularizer and an inductive kernel. The latter is transductive with only
the graph regularizer. Following the work, Krishnapuram et al. (2005) use graph

110

CHAPTER11. LITERATUREREVIEW

regularization on logistic regression. These methods create inductive learners that
naturally handle new test points.

The harmonic mixture model in Chapter 10 naturally handles new points with

the help of a mixture model.

11.6.4 Consistency

The consistency of graph-based semi-supervised learning algorithms has not been
studied extensively according to the authors knowledge. By consistency we mean
whether the classication converges to the right solution as the number of labeled
and unlabeled data grows to innity. Recently von Luxburg et al. (2005) (von
Luxburg et al., 2004) study the consistency of spectral clustering methods. The au-
thors nd that the normalized Laplacian is better than the unnormalized Laplacian
for spectral clustering. The convergence of the eigenvectors of the unnormalized
Laplacian is not clear, while the normalized Laplacian always converges under
general conditions. There are examples where the top eigenvectors of the unnor-
malized Laplacian do not yield a sensible clustering. Although these are valuable
results, we feel the parallel problems in semi-supervised learning needs further
study. One reason is that in semi-supervised learning the whole Laplacian (nor-
malized or not) is often used for regularization, not only the top eigenvectors.

11.6.5 Ranking

Given a large collection of items, and a few query items, ranking orders the items
according to their similarity to the queries. It can be formulated as semi-supervised
learning with positive data only (Zhou et al., 2004b), with the graph induced simi-
larity measure.

11.6.6 Directed Graphs

Zhou et al. (2005) take a hub/authority approach, and essentially convert a directed
graph into an undirected one. Two hub nodes are connected by an undirected edge
with appropriate weight if they co-link to authority nodes, and vice versa. Semi-
supervised learning then proceeds on the undirected graph.

Lu and Getoor (2003) convert the link structure in a directed graph into per-
node features, and combines them with per-node object features in logistic regres-
sion. They also use an EM-like iterative algorithm.

11.7. METRIC-BASEDMODELSELECTION

111

11.6.7 Fast Computation

Fast computation with sparse graphs and iterative methods has been briey dis-
cussed in Chapter 10. Recently numerical methods for fast N-body problems have
been applied to dense graphs in semi-supervised learning, reducing the computa-
tional cost from O(n3) to O(n) (Mahdaviani et al., 2005). This is achieved with
Krylov subspace methods and the fast Gauss transform.

11.7 Metric-Based Model Selection

Metric-based model selection (Schuurmans & Southey, 2001) is a method to detect
hypotheses inconsistency with unlabeled data. We may have two hypotheses which
are consistent on L, for example they all have zero training set error. However they
may be inconsistent on the much larger U. If so we should reject at least one of
them, e.g. the more complex one if we employ Occams razor.

The key observation is that a distance metric is dened in the hypothesis space
H. One such metric is the number of different classications two hypotheses make
under the data distribution p(x): dp(h1, h2) = Ep[h1(x) 6= h2(x)]. It is easy to
verify that the metric satises the three metric properties. Now consider the true
classication function h and two hypotheses h1, h2. Since the metric satises the
triangle inequality (the third property), we have

dp(h1, h2)  dp(h1, h) + dp(h, h2)

Under the premise that labels in L is noiseless, lets assume we can approximate
dp(h1, h) and dp(h, h2) by h1 and h2s training set error rates dL(h1, h) and
dL(h2, h), and approximate dp(h1, h2) by the difference h1 and h2 make on a
large amount of unlabeled data U: dU (h1, h2). We get

dU (h1, h2)  dL(h1, h) + dL(h, h2)

which can be veried directly. If the inequality does not hold, at least one of the
assumptions is wrong. If |U| is large enough and U iid p(x), dU (h1, h2) will be
a good estimate of dp(h1, h2). This leaves us with the conclusion that at least one
of the training errors does not reect its true error. If both training errors are close
to zero, we would know that at least one model is overtting. An Occams razor
type of argument then can be used to select the model with less complexity. Such
use of unlabeled data is very general and can be applied to almost any learning
algorithms. However it only selects among hypotheses; it does not generate new
hypothesis based on unlabeled data.

The co-validation method (Madani et al., 2005) also uses unlabeled data for

model selection and active learning.

112

CHAPTER11. LITERATUREREVIEW

11.8 Related Areas

The focus of the thesis is on classication with semi-supervised methods. There
are some closely related areas with a rich literature.

11.8.1 Spectral Clustering

Spectral clustering is unsupervised. As such there is no labeled data to guide the
process. Instead the clustering depends solely on the graph weights W . On the
other hand semi-supervised learning for classication has to maintain a balance
between how good the clustering is, and how well the labeled data can be ex-
plained by it. Such balance is expressed explicitly in the regularization framework.
As we have seen in section 8.1 and 11.6.4, the top eigenvectors of the graph
Laplacian can unfold the data manifold to form meaningful clusters. This is the
intuition behind spectral clustering. There are several criteria on what constitutes
a good clustering (Weiss, 1999).

The normalized cut (Shi & Malik, 2000) seeks to minimize

N cut(A, B) =

cut(A, B)

assoc(A, V )

+

cut(A, B)

assoc(B, V )

(11.12)

The continuous relaxation of the cluster indicator vector can be derived from the
normalized Laplacian. In fact it is derived from the second smallest eigenvector of
the normalized Laplacian. The continuous vector is then discretized to obtain the
clusters.

The data points are mapped into a new space spanned by the rst k eigenvec-
tors of the normalized Laplacian in (Ng et al., 2001a), with special normalization.
Clustering is then performed with traditional methods (like k-means) in this new
space. This is very similar to kernel PCA.

Fowlkes et al. (2004) use the Nystrom method to reduce the computation cost
for large spectral clustering problems. This is related to our method in Chapter 10.

Chung (1997) presents the mathematical details of spectral graph theory.

11.8.2 Clustering with Side Information

This is the opposite of semi-supervised classication. The goal is clustering but
there are some labeled data in the form of must-links (two points must in the same
cluster) and cannot-links (two points cannot in the same cluster). There is a tension
between satisfying these constraints and optimizing the original clustering criterion
(e.g. minimizing the sum of squared distances within clusters). Procedurally one
can modify the distance metric to try to accommodate the constraints, or one can

11.8. RELATEDAREAS

113

bias the search. We refer readers to a recent short survey (Grira et al., 2004) for the
literatures.

11.8.3 Nonlinear Dimensionality Reduction

The goal of nonlinear dimensionality reduction is to nd a faithful low dimensional
mapping of the high dimensional data. As such it belongs to unsupervised learning.
However the way it discovers low dimensional manifold within a high dimensional
space is closely related to spectral graph semi-supervised learning. Representative
methods include Isomap (Tenenbaum et al., 2000), locally linear embedding (LLE)
(Roweis & Saul, 2000) (Saul & Roweis, 2003), Hessian LLE (Donoho & Grimes,
2003), Laplacian eigenmaps (Belkin & Niyogi, 2003), and semidenite embedding
(SDE) (Weinberger & Saul, 2004) (Weinberger et al., 2004) (Weinberger et al.,
2005).

11.8.4 Learning a Distance Metric

Many learning algorithms depend, either explicitly or implicitly, on a distance met-
ric on X. We use the term metric here loosely to mean a measure of distance or
(dis)similarity between two data points. The default distance in the feature space
may not be optimal, especially when the data forms a lower dimensional manifold
in the feature vector space. With a large amount of U, it is possible to detect such
manifold structure and its associated metric. The graph-based methods above are
based on this principle. We review some other methods next.

The simplest example in text classication might be Latent Semantic Indexing
(LSI, a.k.a. Latent Semantic Analysis LSA, Principal Component Analysis PCA,
or sometimes Singular Value Decomposition SVD). This technique denes a lin-
ear subspace, such that the variance of the data, when projected to the subspace,
is maximumly preserved. LSI is widely used in text classication, where the orig-
inal space for X is usually tens of thousands dimensional, while people believe
meaningful text documents reside in a much lower dimensional space. Zelikovitz
and Hirsh (2001) and Cristianini et al. (2001b) both use U, in this case unlabeled
documents, to augment the term-by-document matrix of L. LSI is performed on
the augmented matrix. This representation induces a new distance metric. By the
property of LSI, words that co-occur very often in the same documents are merged
into a single dimension of the new space. In the extreme this allows two docu-
ments with no common words to be close to each other, via chains of co-occur
word pairs in other documents.

Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) is an impor-
tant improvement over LSI. Each word in a document is generated by a topic (a

114

CHAPTER11. LITERATUREREVIEW

multinomial, i.e. unigram). Different words in the document may be generated by
different topics. Each document in turn has a xed topic proportion (a multino-
mial on a higher level). However there is no link between the topic proportions in
different documents.

Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is one step further.

It
assumes the topic proportion of each document is drawn from a Dirichlet distribu-
tion. With variational approximation, each document is represented by a posterior
Dirichlet over the topics. This is a much lower dimensional representation.

Some algorithms derive a metric entirely from the density of U. These are mo-
tivated by unsupervised clustering and based on the intuition that data points in the
same high density clump should be close in the new metric. For instance, if U
is generated from a single Gaussian, then the Mahalanobis distance induced by the
covariance matrix is such a metric. Tipping (1999) generalizes the Mahalanobis
distance by tting U with a mixture of Gaussian, and dene a Riemannian mani-
fold with metric at x being the weighted average of individual component inverse
covariance. The distance between x1 and x2 is computed along the straight line (in
Euclidean space) between the two points. Rattray (2000) further generalizes the
metric so that it only depends on the change in log probabilities of the density, not
on a particular Gaussian mixture assumption. And the distance is computed along
a curve that minimizes the distance. The new metric is invariate to linear transfor-
mation of the features, and connected regions of relatively homogeneous density
in U will be close to each other. Such metric is attractive, yet it depends on the
homogeneity of the initial Euclidean space. Their application in semi-supervised
learning needs further investigation.

We caution the reader that the metrics proposed above are based on unsuper-
vised techniques. They all identify a lower dimensional manifold within which the
data reside. However the data manifold may or may not correlate with a particular
classication task. For example, in LSI the new metric emphasizes words with
prominent count variances, but ignores words with small variances. If the classi-
cation task is subtle and depends on a few words with small counts, LSI might
wipe out the salient words all together. Therefore the success of these methods
is hard to guarantee without putting some restrictions on the kind of classication
tasks. It would be interesting to include L into the metric learning process.

In a separate line of work, Baxter (1997) proves that there is a unique optimal
metric for classication if we use 1-nearest-neighbor. The metric, named Canoni-
cal Distortion Measure (CDM), denes a distance d(x1, x2) as the expected loss if
we classify x1 with x2s label. The distance measure proposed in (Yianilos, 1995)
can be viewed as a special case. Yianilos assume a Gaussian mixture model has
been learned from U, such that a class correspond to a component, but the corre-
spondence is unknown. In this case CDM d(x1, x2) = p(x1, x2from same component)

11.8. RELATEDAREAS

115

and can be computed analytically. Now that a metric has been learned from U, we
can nd within L the 1-nearest-neighbor of a new data point x, and classify x with
the nearest neighbors label. It will be interesting to compare this scheme with EM
based semi-supervised learning, where L is used to label mixture components.

Weston et al. (2004) propose the neighborhood mismatch kernel and the bagged
mismatch kernel. More precisely both are kernel transformation that modies an
input kernel. In the neighborhood method, one denes the neighborhood of a point
as points close enough according to certain similarity measure (note this is not
the measure induced by the input kernel). The output kernel between point i, j is
the average of pairwise kernel entries between is neighbors and js neighbors. In
bagged method, if a clustering algorithm thinks they tend to be in the same cluster
(note again this is a different measure than the input kernel), the corresponding
entry in the input kernel is boosted.

11.8.5

Inferring Label Sampling Mechanisms

Most semi-supervised learning methods assume L and U are both i.i.d. from the
underlying distribution. However as (Rosset et al., 2005) points out that is not
always the case. For example y can be the binary label whether a customer is
satised, obtained through a survey.
It is conceivable survey participation (and
thus labeled data) depends on the satisfaction y.

Let si be the binary missing indicator for yi. The authors model p(s|x, y)
with a parametric family. The goal is to estimate p(s|x, y) which is the label
sampling mechanism. This is done by computing the expectation of an arbi-
trary function g(x) in two ways: on L  U as 1/nPn
i=1 g(xi), and on L only as
1/nPiL g(xi)/p(si = 1|xi, yi). By equating the two p(s|x, y) can be estimated.

The intuition is that the expectation on L requires weighting the labeled samples
inversely proportional to the labeling probability, to compensate for ignoring the
unlabeled data.

116

CHAPTER11. LITERATUREREVIEW

Chapter 12

Discussions

We have presented a series of semi-supervised learning algorithms, based on a
graph representation of the data. Experiments show that they are able to take ad-
vantage of the unlabeled data to improve classication. Contributions of the thesis
include:

 We proposed a harmonic function and Gaussian eld formulations for semi-
supervised problems. This is not the rst graph-based semi-supervised method.
The rst one was graph mincut. However our formulation is a continuous
relaxation to the discrete labels, resulting in a more benign problem. Sev-
eral variations of the formulation were proposed independently by different
groups shortly after.

 We addressed the problem of graph construction, by setting up parametric
edge weights and performing edge hyperparameter learning. Since the graph
is the input to all graph-based semi-supervised algorithms, it is important that
we construct graphs that best suit the task.

 We combined an active learning scheme that reduces expected error instead
of ambiguity, with graph-based semi-supervised learning. We believe that
active learning and semi-supervised learning will be used together for prac-
tical problems, because limited human annotation resources should be spent
wisely.

 We dened optimal semi-supervised kernels by spectral transformation of
the graph Laplacian. Such optimal kernels can be found with convex opti-
mization. We can use the kernels with any kernel machine, e.g. support vec-
tor machines, for semi-supervised learning. The kernel machines in general
can handle noisy labeled data, which is an improvement over the harmonic
function solution.

117

118

CHAPTER12. DISCUSSIONS

 We kernelized conditional random elds. CRFs were traditionally feature
based. We derived the dual problem and presented an algorithm for fast
sparse kernel CRF training. With kernel CRFs, it is possible to use a semi-
supervised kernel on instances for semi-supervised learning on sequences
and other structures.

 We proposed to solve large-scale problems with harmonic mixtures. Har-
monic mixtures reduce computation cost signicantly by grouping unlabeled
data into soft clusters, then carrying out semi-supervised learning on the
coarser data representation. Harmonic mixtures also handle new data points
naturally, making the semi-supervised learning method inductive.

Semi-supervised learning is a relatively new research area. There are many

open questions and research opportunities:

 The graph is the single most important quantity for graph-based semi-supervised

learning. Parameterizing graph edge weights, and learning weight hyperpa-
rameters, should be the rst step of any graph-based semi-supervised learn-
ing methods. Current methods in Chapter 7 are not efcient enough. Can we
nd better ways to learn the graph structure and parameters?

 Real problems can have millions of unlabeled data points. Anecdotal sto-
ries and experiments in Appendix F indicate that conjugate gradient with a
suitable pre-conditioner is one of the fastest algorithms in solving harmonic
functions. Harmonic mixture works along an orthogonal direction by reduc-
ing the problem size. How large a dataset can we process if we combine
conjugate gradient and harmonic mixture? What can we do to handle even
larger datasets?

 Semi-supervised learning on structured data, e.g. sequences and trees, is
largely unexplored. We have proposed the use of kernel conditional ran-
dom elds plus semi-supervised kernels. Much more work is needed in this
direction.

 In this thesis we focused on classication problems. The spirit of combining
some human effort with large amount of data should be applicable to other
problems. Examples include: regression with both labeled and unlabeled
data; ranking with ordered pairs and unlabeled data; clustering with cluster
membership knowledge. What can we do beyond classication?

 Because labeled data is scarce, semi-supervised learning methods depend
more heavily on their assumptions (see e.g. Table 1.1). Can we develop
novel semi-supervised learning algorithms with new assumptions?

119

 Applications of semi-supervised learning are emerging rapidly. These in-
clude text categorization, natural language processing, bioinformatics, im-
age processing, and computer vision. Many others are sure to come. Appli-
cations are attractive because they solve important practical problems, and
provide fertile test bed for new ideas in machine learning. What problems
can we apply semi-supervised learning? What applications were too hard
but are now feasible with semi-supervised learning?

 The theory of semi-supervised learning is almost absent in both the ma-
chine learning literature and the statistics literature. Is graph-based semi-
supervised learning consistent? How many labeled and unlabeled points are
needed to learn a concept with condence?

We expect advances in research will address these questions. We hope semi-
supervised learning become a fruitful area for both machine learning theory and
practical applications.

120

CHAPTER12. DISCUSSIONS

Appendix A

The Harmonic Function after
Knowing One More Label

uu ulfl = 1

Construct the graph as usual. We use f to denote the harmonic function. The
random walk solution is fu = 1
uu Wulfl. There are u unlabeled
nodes. We ask the question: what is the solution if we add a node with value f0 to
the graph, and connect the new node to unlabeled node i with weight w0? The new
node is a dongle attached to node i. Besides the usage here, dongle nodes can
be useful for handling noisy labels where one would put the observed labels on the
dongles, and infer the hidden true labels for the nodes attached to dongles. Note
that when w0  , we effectively assign label f0 to node i.
Since the dongle is a labeled node in the augmented graph,

f +
u = +
uu

ul f +

l = (D+

1W +

ul f +
= (w0ee> + Duu  Wuu)1(w0f0e + Wulfl)
= (w0ee> + uu)1(w0f0e + Wulfl)

uu  Wuu)1W +

l

where e is a column vector of length u with 1 in position i and 0 elsewhere. Note
that we can use the matrix inversion lemma here, to obtain

(w0ee> + uu)1 = 1

uu 

uu (w0e)(w0e)>1
1
uu (w0e)
1 + (w0e)>1

uu

= G 

1

1 + w0Gii

w0G|iG

where we use the shorthand G = 1
uu (the Greens function); Gii is the i-th row,
i-th column element in G; G|i is a square matrix with Gs i-th column and 0 else-

121

122

APPENDIXA. UPDATEHARMONICFUNCTION

where. Some calculation gives

f +
u = fu +

w0f0  w0fi
1 + w0Gii

Gi

where fi is the unlabeled nodes original solution, and Gi is the i-th column vector
in G. If we want to pin down the unlabeled node to value f0, we can let w0  
to obtain

f +
u = fu +

f0  fi
Gii

Gi

Appendix B

The Inverse of a Matrix with One
Row/Column Removed

Let A be an n n non-singular matrix. Given A1, we would like a fast algorithm
to compute A1
i , where Ai is the (n  1)  (n  1) matrix obtained by removing
the i-th row and column from A.
Let B = perm(A, i) be the matrix created by moving the i-th row in front of

the 1st row, and the i-th column in front of the 1st column of A. Then

A1

i = (perm(A, i)1)1 = (B1)1

Also note B1 = perm(A1, i). So we only need to consider the special case of

where B1 = (b12 . . . b1n) and B1 = (b21 . . . bn1)>. We will transform B into a

removing the rst row/column of a matrix. Write B out as B =(cid:20) b11 B1
B1 B1 (cid:21),
block diagonal form in two steps. First, let B0 =(cid:20) 1
B1 B1 (cid:21) = B +uv> where
u = (1, 0, . . . , 0)> and v = (b11  1, B1)>. We are interested in (B0)1 which
will be used in the next step. By the matrix inversion lemma (Sherman-Morrison-
Woodbury formula),

0

(B0)1 = (B + uv>)1 = B1 
0 B1 (cid:21) = B0 + wu> where w = (0, B1)>. Applying the

0

B1uv>B1
1 + v>B1u

Next let B00 = (cid:20) 1

matrix inversion lemma again,

(B00)1 = (B0 + wu>)1 = (B0)1 

(B0)1wu>(B0)1
1 + u>(B0)1w

123

124

APPENDIXB. MATRIXINVERSE

But since B00 is block diagonal, we know (B00)1 =(cid:20) 1

0 (B1)1 (cid:21). Therefore

(B1)1 = ((B00)1)1.

0

Appendix C

Laplace Approximation for
Gaussian Processes

This derivation largely follows (Herbrich, 2002) (B.7). The Gaussian process
model, restricted to the labeled and unlabeled data, is

f  N(cid:16), 1(cid:17)

(C.1)

(C.3)

(C.4)

(C.5)

(C.6)

We will use G = 1 to denote the covariance matrix (i.e. the Gram matrix). Let
y  {1, +1} be the observed discrete class labels. The hidden variable f and
labels y are connected via a sigmoid noise model

P (yi|fi) =

efiyi

efiyi + efiyi

=

1

1 + e2fiyi

(C.2)

where  is a hyperparameter which controls the steepness of the sigmoid. Given
the prior and the noise model, we are interested in the posterior p(fL, fU|yL). By
Bayes theorem,

p(fL, fU|yL) = Ql

i=1 P (yi|fi)p(fL, fU )

P (yL)

Because of the noise model, the posterior is not Gaussian and has no closed form
solution. We use the Laplace approximation.

First, we nd the mode of the posterior (6.7):

( fL, fU ) = arg maxfL,fUQl
lXi=1

= arg maxfL,fU

= arg maxfL,fU

Q1 + Q2

i=1 P (yi|fi)p(fL, fU )

P (yL)

ln P (yi|fi) + ln p(fL, fU )

125

126APPENDIXC. LAPLACEAPPROXIMATIONFORGAUSSIANPROCESSES

Note fU only appears in Q2, and we can maximize fU independently given fL. Q2
is the log likelihood of the Gaussian (C.1). Therefore given fL, fU follows the
conditional distribution of Gaussian:

p(fU| fL) = N(cid:16)GU LG1

LL

fL, GU U  GU LG1

LLGLU(cid:17)

Moreover, the mode is the conditional mean

fU = GU LG1
LL

fL

(C.7)

(C.8)

Its easy to see (C.8) has the same form as the solution for Gaussian Fields (4.11):
Recall G = 1. From partitioned matrix inversion theorem,

U U = S1
A

U L = S1

A GU LG1

LL

where SA = GU U  GU L(GLL)1GLU is the Schur complement of GLL. This
gives us

( U U )1 U L = SAS1

A GU LG1

LL = GU LG1

LL

Thus we have

fU =  1
= 1

U L fL
U U WU L fL

U U

(C.9)
(C.10)

which has the same form as the harmonic energy minimizing function in (Zhu et al.,
2003a). In fact the latter is the limiting case when 2   and there is no noise
model.
Substitute (C.8) back to Q2, using partitioned inverse of a matrix, it can be

shown that (not surprisingly)

Q2 = 

1
2

L G1
f >

LLfL + c

Now go back to Q1. The noise model can be written as

P (yi|fi) =

efiyi

efiyi + efiyi

= (cid:18)

efi

efi + efi(cid:19) yi+1

2 (cid:18)1 

efi

efi + efi(cid:19) 1yi

2

= (fi)

yi+1

2 (1  (fi))

1yi

2

(C.11)

(C.12)

(C.13)

(C.14)

ln P (yi|fi)

yi + 1

2

ln (fi) +

1  yi

2

ln(1  (fi))

ln(1 + e2fi)

therefore

Q1 =

=

lXi=1
lXi=1

= (yL  1)>fL 

Put it together,

fL = arg maxQ1 + Q2

= arg max(yL  1)>fL 

To nd the mode, we take the derivative,

lXi=1

lXi=1

127

(C.15)

(C.16)

(C.17)

(C.18)

ln(1 + e2fi) 

1
2

L G1
f >

LLfL (C.19)

(Q1 + Q2)

fL

= (yL  1) + 2(1  (fL))  G1

LLfL

(C.20)

Because of the term (fL) it is not possible to nd the root directly. We solve it
with Newton-Raphson algorithm,

f

(t+1)

L  f

where H is the Hessian matrix,

(t)

L  H 1 (Q1 + Q2)

fL

(cid:12)(cid:12)(cid:12)(cid:12)fL

(t)

H =" 2(Q1 + Q2)

fifj

(cid:12)(cid:12)(cid:12)(cid:12)fL#

Note d
dfi

(fi) = 2(fi)(1  (fi)), we can write H as

H = G1

LL  P

(C.21)

(C.22)

(C.23)

where P is a diagonal matrix with elements Pii = 42(fi)(1  (fi)).
Once Newton-Raphson converges we compute fU from fL with (C.8). Classi-
cation can be done with sgn( fU ) noting this is the Bayesian classication rule with
Gaussian distribution and sigmoid noise model.

128APPENDIXC. LAPLACEAPPROXIMATIONFORGAUSSIANPROCESSES

To compute the covariance matrix of the Laplace approximation, note by de-

nition the inverse covariance matrix of the Laplace approximation is

From (6.7) it is straightforward to conrm

fifj

1 =" 2  ln p(f|y)

(cid:12)(cid:12)(cid:12)(cid:12) fL, fU#
0 0 (cid:21) + G1 =(cid:20) P 0
0 0 (cid:21) + 

1 = (cid:20) P 0

Therefore the covariance matrix is

 =(cid:18)(cid:20) P 0

0 0 (cid:21) + (cid:19)1

where P is evaluated at the mode fL.

(C.24)

(C.25)

(C.26)

Appendix D

Hyperparameter Learning by
Evidence Maximization

This derivation largely follows (Williams & Barber, 1998). We want to nd the
MAP hyperparameters  which maximize the posterior

p(|yL)  p(yL|)p()

The prior p() is usually chosen to be simple, and so we focus on the term
p(yL|), known as the evidence. The denition

p(yL|) =Z p(yL|fL)p(fL|) dfL

is hard to compute analytically. However notice

p(yL|) =

p(yL|fL)p(fL|)

p(fL|yL, )

,fL

(D.1)

Since it holds for all fL, it holds for the mode of the Laplace approximation fL:

p(yL|) =

p(yL| fL)p( fL|)

p( fL|yL, )

The terms on the numerator are straightforward to compute; the denominator is
tricky. However we can use the Laplace approximation, i.e. the probability density
at the mode: p( fL|yL, )  N ( fL| fL, LL). Recall
0 0 (cid:21) +(cid:20) GLL GLU

GU L GU U (cid:21)1!1

(D.2)

 = (cid:20) P 0

129

130

APPENDIXD. EVIDENCEMAXIMIZATION

By applying Schur complement in block matrix decomposition twice, we nd

LL = (P + G1

LL)1

Therefore the evidence is

p(yL|) 

=

=

p(yL| fL)p( fL|)
N ( fL| fL, LL)
p(yL| fL)p( fL|)
2 |LL| 1
(2) n
p(yL| fL)p( fL|)
2 |(P + G1

(2) n

2

LL)1| 1

2

Switching to log domain, we have

log p(yL|)  ( fL) +
= ( fL) +

n
2
n
2

log 2 +

log 2 

1
2
1
2

log |LL|
log |P + G1
LL|

(D.3)

(D.4)

(D.5)

(D.6)

(D.7)

(D.8)

where (fL) = log p(yL|fL) + log p(fL|). Since f  N(cid:16), 1(cid:17) = N (, G),
we have fL  N (L, GLL). Therefore

( fL) = log p(yL| fL) + log p( fL|)

LXi=1

n
2

log(1 + exp(2 fiyi))
1
2

log |GLL| 

log 2 

1
2

(D.9)

( fL  L)>G1

LL( fL  L)(D.10)

= 



Put it together,

log p(yL|)  



= 



LXi=1

1
2

log(1 + exp(2 fiyi))

log |GLL| 
log(1 + exp(2 fiyi))

LXi=1
( fL  L)>G1

LL( fL  L) 

1
2

1
2

( fL  L)>G1

LL( fL  L) 

1
2

log |P + G1
LL|

1
2

log |GLLP + I|

(D.11)

131

This gives us a way to (approximately) compute the evidence.

To nd the MAP estimate of  (which can have multiple local maxima), we use
gradient methods. This involves the derivatives of the evidence  log p(yL|)/,
where  is the hyperparameter , ,  or the ones controlling W .

We start from




( fi) =




1

1 + e2 fi

= 2( fi)(1  ( fi))( fi




+ 

 fi


)

(D.12)

(D.13)

To compute  fL/, note the Laplace approximation mode fL satises

(fL)

fL (cid:12)(cid:12)(cid:12)(cid:12) fL

which means

= (yL + 1  2( fL))  G1

LL( fL  L) = 0

(D.14)

fL = GLL(yL + 1  2( fL)) + L

(D.15)

Taking derivatives on both sides,

GLL(yL + 1  2( fL))



GLL



GLL



 fL


=

=

=

which gives

(yL + 1  2( fL))  2GLL
(yL + 1  2( fL)) 

1


GLLP fL

( fL)



(D.16)

(D.17)


  GLLP

 fL


(D.18)

 fL


= (I + GLLP )1(cid:20) GLL



(yL + 1  2( fL)) 

1


GLLP fL



(cid:21)(D.19)

132

APPENDIXD. EVIDENCEMAXIMIZATION

Now it is straightforward to compute the gradient with (D.11):




log p(yL|)



= 

( fL  L)>G1
 fi


)

1



1
2

+ 

( fi




log(1 + exp(2 fiyi)) 

"
LXi=1
LXi=1
exp(2 fiyi)(2yi)
1 + exp(2 fiyi)
2"2(G1
LL( fL  L))>  fL
+ ( fL  L)> G1
tr(cid:18)(GLLP + I)1 GLLP
 (cid:19)
= tr(cid:18)A1 A
(cid:19)

 log |A|

1
2







LL





where we used the fact

LL( fL  L) 

log |GLLP + I|#

1
2

( fL  L)#

(D.20)

(D.21)

For example, if  = , the gradient can be computed by noting GLL

 = GLL,
 = 8( fi)(1  ( fi)) +

LL

 = 0, and GLLP

 = GLL

P

 where Pii



 = 1, G1
42(1  2( fi)) ( fi)



.



 = 0, G1

GLLP

 = (1/)GLL, 
For  = , we have GLL
LL/, and
 = 83( fi)(1( fi))(12( fi))  fi
 where Pii
P
 .
 = GLLP/+GLL
For  = , the computation is more intensive because the complex depen-
(cid:3)LL. Using the fact A1
 =(cid:2) G
dency between G and . We start from GLL
 =
A1 A
 = /3G2. Note the computation in-
volves the multiplication of the full matrix G and is thus more demanding. Once
GLL

 A1 and G = 1, we get G

 = G1

LL

is computed the rest is easy.

If we parameterize the weights W in Gaussian Fields with radial basis func-
tions (for simplicity we assume a single length scale parameter  for all dimen-
sions. Extension to multiple length scales is simple),

wij = exp 

d2
ij

2!

(D.22)

where dij is the Euclidean distance between xi, xj in the original feature space, we
can similarly learn the hyperparameter . Note wij
  W
 ,
 
 =  

 . The rest is the same as for  above.

 = D

d2
3 , 
ij

 = wij

Similarly with a tanh()-weighted weight function wij = (tanh(1(dij 
=

= (1  tanh2(1(dij  2)))(dij  2)/2 and wij

2)) + 1)/2, we have wij
1
(1  tanh2(1(dij  2)))1/2, and the rest follows.

133

2

134

APPENDIXD. EVIDENCEMAXIMIZATION

Appendix E

Mean Field Approximation for
Kernel CRF Training

In the basic kernel CRF model, each clique c is associated with |y||c| parameters
c
j(yc). Even if we only consider vertex cliques, there would be hundreds of thou-
sands of parameters for a typical protein dataset. This seriously affects the training
efciency.

To solve the problem, we adopt the notion of import vector machines by Zhu
and Hastie (2001). That is, we use a subset of the training examples instead of all
of them. The subset is constructed by greedily selecting training examples one at a
time to minimize the loss function:

where

arg minkR(fA{k}, )  R(fA, )
fA(x, y) =XjA

j(y)K(xj, x)

and A is the current active import vector set.

(E.1) is hard to compute: we need to update all the parameters for fA{k}.
Even if we keep old parameters in fA xed, we still need to use expensive forward-
backward algorithm to train the new parameters k(y) and compute the loss. Fol-
lowing McCallum (2003), we make a set of speed up approximations.

Approximation 1: Mean eld approximation. With the old fA we have an
A(x, y)) over a label sequence y. We

old distribution P (y|x) = 1/Z exp(Pc f c
approximate P (y|x) by the mean eld
Po(y|x) =Yi

Po(yi|xi)

135

(E.1)

(E.2)

(E.3)

136

APPENDIXE. MEANFIELDAPPROXIMATION

i.e. the mean eld approximation is the independent product of marginal distribu-
tions at each position i. It can be computed with the Forward-Backward algorithm
on P (y|x).
Approximation 2: Consider only the vertex kernel. In conjunction with the
mean eld approximation, we only consider the vertex kernel K(xi, xj) and ignore
edge or other higher order kernels. The loss function becomes



log Po(yi|xi) +

R(fA, ) = XiT
where T = {1, . . . , M} is the set of training positions on which to evaluate the
loss function. Once we add a candidate import vector xk to the active set, the new
model is

2 Xi,jAXy

i(y)j(y)K(xi, xj)

(E.4)

Pn(yi|xi) =

Po(yi|xi) exp(k(yi)K(xi, xk))

Py Po(y|xi) exp(k(y)K(xi, xk))

(E.5)

log Pn(yi|xi) +



2 Xi,jA{k}Xy

i(y)j(y)K(xi, xj)

(E.6)

The new loss function is

R(fA{k}, ) = XiT

And (E.1) can be written as

R(fA{k}, )  R(fA, ) = XiT
+XiT
logXy
+XjAXy

k(yi)K(xi, xk)

(E.7)

Po(y|xi) exp(k(y)K(xi, xk))

j(y)k(y)K(xj, xk) +

2

k(y)K(xk, xk)



2Xy

This change of loss is a convex function of the |y| parameters k(y). We can nd
the best parameters with Newtons method. The rst order derivatives are

R(fA{k}, )  R(fA, )

k(y)

= XiT
+XiT
+ XjA{k}

K(xi, xk)(yi, y)

Pn(y|xi)K(xi, xk)

(E.8)

(E.9)

j(y)K(xj, xk)

(E.10)

And the second order derivatives are
2R(fA{k}, )  R(fA, )

k(y)k(y0)

= XiT(cid:2)Pn(y|xi)K2(xi, xk)(y, y0)  Pn(y|xi)K2(xi, xk)Pn(y0|xi)(cid:3)

+K(xk, xk)(y, y0)

137

(E.11)

Approximation 1 and 2 allow us to estimate the change in loss function inde-
pendently for each position in T . This avoids the need of dynamic programming.
Although the time complexity to evaluate each candidate xk is still linear in |T|,
we save by a (potentially large) constant factor. Further more, they allow a more
dramatic approximation as shown next.

Approximation 3: Sparse evaluation of likelihood. A typical protein database
has around 500 sequences, with hundreds of amino acid residuals per sequence.
Therefore M, the total number of training positions, can easily be around 100,000.
Normally T = {1, . . . , M}, i.e. we need to sum over all training positions to
evaluate the log-likelihood. However we can speed up by reducing T . There are
several possibilities:

1. Focus on errors: T = {i|yi 6= arg maxyPo(y|xi)}
2. Focus on low condence: T = {i|Po(yi|xi) < p0}
3. Skip positions: T = {ai|ai  M ; a, i  N}
4. Random sample: T = {i|i  unif orm(1, M )}
5. Error/condence guided sample: errors / low condence positions have higher

probability to be sampled.

We need to scale the log likelihood term to maintain the balance between it and the
regularization term:

R(fA, ) = 

M

|T|XiT

log Po(yi|xi) +



2 Xi,jAXy

and scale the derivatives accordingly.

i(y)j(y)K(xi, xj) (E.12)

Other approximations: We may want to add more than one candidate import
vector to A at a time. However we need to eliminate redundant vectors, possibly
by the kernel distance. We may not want to fully train fA{k} once we selected k.

138

APPENDIXE. MEANFIELDAPPROXIMATION

Appendix F

An Empirical Comparison of
Iterative Algorithms

The single most signicant bottleneck in computing the harmonic function is to
invert a u  u matrix, as in fu = 1
uu ulfl. Done naively the cost is close
to O(n3), which is prohibitive for practical problems. For example Matlab inv()
function can only handle n in the range of several thousand. Clearly, we need to
nd ways to avoid the expensive inversion. One can go several directions:

i=1 ii>

i=1 1/ii>

i=1 1/ii>

1. One can approximate the inversion of a matrix by its top few eigenvalues
and eigenvectors. If a n n invertible matrix A has spectrum decomposition
A =Pn
i . The
top m < n eigenvectors i with the smallest eigenvalues i is less expensive
to compute than inverting the matrix. This has been used in non-parametric
transforms of graph kernels for semi-supervised learning in Chapter 8. A
similar approximation is used in (Joachims, 2003). We will not pursue it
further here.

i , then A1 =Pn

i Pm

2. One can reduced the problem size.

Instead of using all of the unlabeled
data, we can use a subset (or clusters) to construct the graph. The harmonic
solution on the remaining data can be approximated with a computationally
cheap method. The backbone graph in Chapter 10 is an example.

3. One can use iterative methods. The hope is that each iteration is O(n) and
convergence can be reached in relatively few iterations. There is a rich set of
iterative methods applicable. We will compare the simple label propagation
algorithm, loopy belief propagation and conjugate gradient next.

139

140

APPENDIXF. COMPARINGITERATIVEALGORITHMS

F.1 Label Propagation

The original label propagation algorithm was proposed in (Zhu & Ghahramani,
2002a). A slightly modied version is presented here. Let P = D1W be the
transition matrix. Let fl be the vector for labeled set (for multiclass problems it
can be an l  c matrix). The label propagation algorithm consists of two steps:
1.   f (t+1)

! = P  f (t)
u !

l
f (t+1)
u

l
f (t)

2. Clamp the labeled data f (t+1)

l

= fl

It can be shown fu converges to the harmonic solution regardless of initialization.
Each iteration needs a matrix-vector multiplication, which can be O(n) for sparse
graphs. However the convergence may be slow.

F.2 Conjugate Gradient

The harmonic function is the solution to the linear system

uufu = ulfl

(F.1)

Standard conjugate gradient methods have been shown to perform well (Argyriou,
2004). In particular, the use of Jacobi preconditioner was shown to improve con-
vergence. The Jacobi preconditioner is simply the diagonal of uu, and the pre-
conditioned linear system is

diag(uu)1uufu = diag(uu)1ulfl

(F.2)

We note this is exactly

(F.3)
i.e. the alternative denition of harmonic function fu = (IPuu)1Pulfl, where
P = D1W is the transition matrix.

(I  Puu)fu = Pulfl

F.3 Loopy belief propagation on Gaussian elds

The harmonic solution

fu = 1

uu ulfl

(F.4)

computes the mean of the marginals on unlabeled nodes u.  is the graph Lapla-
cian. The computation involves inverting a u  u matrix and is expensive for large

F.3. LOOPYBELIEFPROPAGATIONONGAUSSIANFIELDS

141

datasets. We hope to use loopy belief propagation instead, as each iteration is O(n)
if the graph is sparse, and loopy BP has a reputation of converging fast (Weiss &
Freeman, 2001) (Sudderth et al., 2003). It has been proved that if loopy BP con-
verges, the mean values are correct (i.e. the harmonic solution).

The Gaussian eld is dened as

p(y)  exp(

1
2

yy>)

And fu = Ep[yu]. Note the corresponding pairwise clique representation is

p(y)  Yi,j
= Yi,j
= Yi,j

ij(yi, yj)

exp(cid:18)
exp(cid:18)

1
2

1
2

wij(yi  yj)2(cid:19)
(yiyj)(cid:18) a b

c d (cid:19)(cid:18) yi

yj (cid:19)(cid:19)

(F.5)

(F.6)

(F.7)

(F.8)

where a = d = wij, b = c = wij, and wij is the weight of edge ij. Notice in
this simple model we dont have n nodes for hidden variables and another n for
observed ones; we only have n nodes with some of them observed. In other words,
there is no noise model.

The standard belief propagation messages are

mij(yj) = Zyi

ij(yi, yj) YkN (i)\j

mki(yi)dyi

(F.9)

where mij is the message from i to j, N (i)\j is the neighbors of i except j, and
 a normalization factor. Initially the messages are arbitrary (e.g. uniform) except
for observed nodes yl = fl, whose messages to their neighbors are

After the messages converge, the marginals (belief) is computed as

mlj(yj) = ij(yl, yj)

b(yi) =  YkN (i)

mki(yi)

(F.10)

(F.11)

For Gaussian elds with scalar-valued nodes, each message mij can be param-
eterized similar to a Gaussian distribution by its mean ij and inverse variance
(precision) Pij = 1/2

ij parameters. That is,

mij(xj)  exp(cid:18)

1
2

(xj  ij)2Pij(cid:19)

(F.12)

142

APPENDIXF. COMPARINGITERATIVEALGORITHMS

We derive the belief propagation iterations for this special case next.

mij(yj)

1

1
2

mki(yi)dyi

mki(yi)dyi

= Zyi
ij(yi, yj) YkN (i)\j
exp(cid:18)
(yiyj)(cid:18) a b
c d (cid:19)(cid:18) yi
= Zyi
exp
2(yiyj)(cid:18) a b
c d (cid:19)(cid:18) yi
= 2Zyi
j(cid:19)
= 3 exp(cid:18)
exp
Pki y2
2a + XkN (i)\j

(xi  ki)2Pki dyi
Pkiki yi dyi
where we use the fact b = c. Let A = a+PkN (i)\j Pki, B = byjPkN (i)\j Pkiki,

yj (cid:19)(cid:19) YkN (i)\j
yj (cid:19) + XkN (i)\j
i + 2byj  XkN (i)\j

Zyi

dy2

1
2

1

(F.13)

mij(yj)

= 3 exp(cid:18)
= 3 exp(cid:18)
= 3 exp(cid:20)

1

1
2

1
2

dy2

dy2

exp(cid:20)
j(cid:19)Zyi
i + 2Byi(cid:1)(cid:21) dyi
2(cid:0)Ay2
exp(cid:20)
j(cid:19)Zyi
2(cid:16)(Ayi + B/A)2  B2/A(cid:17)(cid:21) dyi
exp(cid:20)
j  B2/A(cid:1)(cid:21)Zyi
2(cid:16)(Ayi + B/A)2(cid:17)(cid:21) dyi
2(cid:0)dy2

1

1

1

Note the integral is Gaussian whose value depends on A, not B. However since A
is constant w.r.t. yj, the integral can be absorbed into the normalization factor,

mij(yj)

= 4 exp(cid:20)
= 4 exp"
= 5 exp"

1

1

b2y2

j 

2(cid:0)dy2
2 dy2
2  d 

j  B2/A(cid:1)(cid:21)
j  2bPkN (i)\j Pkikiyj + (PkN (i)\j Pkiki)2
a +PkN (i)\j Pki! y2
yj!#

bPkN (i)\j Pkiki
a +PkN (i)\j Pki

a +PkN (i)\j Pki

j + 2

b2

1

(F.14)

!#

F.3. LOOPYBELIEFPROPAGATIONONGAUSSIANFIELDS

143

Let C = d 

, D =

b2

mij(yj)

a+PkN (i)\j Pki
= 5 exp(cid:20)
= 5 exp(cid:20)
= 6 exp(cid:20)
= 6 exp(cid:20)

,

bPkN (i)\j Pkiki
a+PkN (i)\j Pki
j + 2Dyj(cid:1)(cid:21)

1

1

2(cid:0)Cy2
2(cid:18)(cid:16)Cyj + D/C(cid:17)2
2(cid:18)(cid:16)Cyj + D/C(cid:17)2(cid:19)(cid:21)
2(cid:16)(yj  (D/C))2 C(cid:17)(cid:21)

1

1

 D2/C(cid:19)(cid:21)

(F.15)

(F.16)

(F.17)

(F.18)

(F.19)

Thus we see the message mij has the form of a Gaussian density with sufcient
statistics

(F.20)

(F.21)

(F.22)

(F.23)

(F.24)

(F.25)

(F.26)

Pij = C

= d 

ij = D/C

b2

a +PkN (i)\j Pki
bPkN (i)\j Pkiki
a +PkN (i)\j Pki

P 1
ij

= 

For our special case of a = d = wij, b = c = wij, we get
wij +PkN (i)\j Pki

Pij = wij 
ij = D/C

w2
ij

=

wijPkN (i)\j Pkiki
wij +PkN (i)\j Pki

P 1
ij

For observed nodes yl = fl, they ignore any messages sent to them, while sending
out the following messages to their neighbors j:

lj = fl
Plj = wlj

(F.27)
(F.28)

144

APPENDIXF. COMPARINGITERATIVEALGORITHMS

The belief at node i is

mki(yi)

bi(yi)

=  YkN (i)
=  exp
= 2 exp
= 3 exp

1

1

1

Pkiy2

(yi  ki)2Pki
2 XkN (i)
2 XkN (i)
i  2 XkN (i)
2 yi PkN (i) Pkiki
PkN (i) Pki !2
i = PkN (i) Pkiki
PkN (i) Pki
Pi = XkN (i)

Pki

Pkikiyi
 XkN (i)

(F.29)
(F.30)

(F.31)

(F.32)

Pki (F.33)

(F.34)

(F.35)

This is a Gaussian distribution with mean and inverse variance

F.4 Empirical Results

We compare label propagation (LP), loopy belief propagation (loopy BP), conju-
gate gradient (CG) and preconditioned conjugate gradient (CG(p)) on eight tasks.
The tasks are small because we want to be able to compute the closed form solution
fu with matrix inversion. LP is coded in Matlab with sparse matrix. Loopy BP is
implemented in C. CG and CG(p) use Matlab cgs() function.

Figure F.1 compares the mean squared errorPiU(cid:0)f (t)(i)  fu(i)(cid:1)2 with dif-

ferent methods at iteration t. We assume that with good implementation, the cost
per iteration for different methods is similar. For multiclass tasks, it shows the
binary sub-task of the rst class vs. the rest. Note the y-axis is in log scale. We
observe that loopy BP always converges reasonably fast; CG(p) can catch up and
come closest to the closed form solution quickly, however sometimes it does not
converge (d,e,f); CG is always worse than CG(p); LP converges very slowly.

For classication purpose we do not need to wait for f (t)

quantity of interest is when does f (t)
form solution fu. For the binary case this means f (t)

u to converge. Another
u give the same classication as the closed
u and fu are on the same side

F.4. EMPIRICALRESULTS

r
o
r
r
e

d
e
r
a
u
q
s

n
a
e
m



u

f

r
o
r
r
e

d
e
r
a
u
q
s

n
a
e
m



u

f

r
o
r
r
e



d
e
r
a
u
q
s


n
a
e
m



u

f

r
o
r
r
e



d
e
r
a
u
q
s


n
a
e
m



u

f

105

100

105

1010

1015

1020

0

100

102

104

106

108

1010

1012

1014

1016

1018

1020

0

106

105

104

103

102

101

100

101

102

103

104

0

100

105

1010

1015

1020

1025

1030

0

loopy BP
CG
CG(p)
LP

r
o
r
r
e



d
e
r
a
u
q
s

n
a
e
m



u

f

200

400

600

800

1000

1200

1400

1600

1800

iteration

(a) 1 vs. 2

loopy BP
CG
CG(p)
LP

r
o
r
r
e



d
e
r
a
u
q
s


n
a
e
m



u

f

500

1000

1500
iteration

2000

2500

3000

(c) odd vs. even

loopy BP
CG
CG(p)
LP

r
o
r
r
e



d
e
r
a
u
q
s


n
a
e
m



u

f

200

400

600

800

1000

1200

1400

1600

1800

iteration

(e) PC vs. MAC

loopy BP
CG
CG(p)
LP

r
o
r
r
e



d
e
r
a
u
q
s


n
a
e
m



u

f

200

400

600

800

1000
iteration

1200

1400

1600

1800

2000

(g) isolet

145

loopy BP
CG
CG(p)
LP

500

1000

1500
iteration

2000

2500

3000

(b) ten digits

loopy BP
CG
CG(p)
LP

200

400

600

800

1000

1200

1400

1600

1800

iteration

(d) baseball vs. hockey

loopy BP
CG
CG(p)
LP

200

400

600

800

1000

1200

1400

iteration

(f) religion vs. atheism

loopy BP
CG
CG(p)
LP

500

1000

1500

2000
iteration

2500

3000

3500

4000

(h) freefoodcam

102

100

102

104

106

108

1010

0

100

101

102

103

0

103

102

101

100

101

102

0

100

105

1010

1015

1020

1025

1030

1035

0

Figure F.1: Mean squared error to the harmonic solution with various iterative
methods: loopy belief propagation (loopy BP), conjugate gradient (CG), conjugate
gradient with Jacobi preconditioner (CG(p)), and label propagation (LP). Note the
log-scale y-axis.

146

APPENDIXF. COMPARINGITERATIVEALGORITHMS

baseball vs. hockey

one vs. two
odd vs. even

nodes
2200
4000
1993
1943
religion vs. atheism 1427
4000
7797
5254

pc vs. mac

task

ten digits

isolet

freefoodcam

edges
17000
31626
13930
14288
10201
31595
550297
23098

loopy BP
0.02
0.03
0.02
0.02
0.01
0.03
5
0.02

CG CG(p)
0.001
0.0007
0.002
0.002
0.001
0.004
0.0003
7e-05

0.002
0.003
0.001
0.002
0.001
0.003
0.0005
0.0001

LP closed form
2e+01
1e+02
2e+01
2e+01
7
9e+01
2e+03
1e+02

0.0008
0.001
0.0007
0.0007
0.0005
0.008
1
0.008

Table F.1: Average run time per iteration for loopy belief propagation (loopy BP),
conjugate gradient (CG), conjugate gradient with Jacobi preconditioner (CG(p)),
and label propagation (LP). Also listed is the run time for closed form solution.
Time is in seconds. Loopy BP is implemented in C, others in Matlab.

of 0.5, if labels are 0 and 1. We dene classication agreement as the percentage of
unlabeled data whose f (t)
u and fu have the same label. Note this is not classication
accuracy. Ideally agreement should reach 100% long before f (t)
u converges. Figure
F.2 compares the agreement. Note x-axis is in log scale. All methods quickly
reach classication agreement with the closed form solution, except CG and CG(p)
sometimes do not converge; Task (f) has only 80% agreement.

Since loopy BP code is implemented in C and others in Matlab, their speed
may not be directly comparable. Nonetheless we list the average per-iteration run
time of different iterative methods in Table F.1. Also listed are the run time of the
closed form solution with Matlab inv().

F.4. EMPIRICALRESULTS

147

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

t
n
e
m
e
e
r
g
a

n
o
i
t
a
c
i
f
i
s
s
a
c


l

u

f

t
n
e
m
e
e
r
g
a

n
o
i
t
a
c
i
f
i
s
s
a
c


l

u

f

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

1

0.9

0.8

0.7

0.6

0.5

0.4

1

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

1

0.9

0.8

0.7

0.6

0.5

0.4

2

1.8

1.6

1.4

1.2

1

0.8

0.6

0.4

0.2

0

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

loopy BP
CG
CG(p)
LP

103

loopy BP
CG
CG(p)
LP

102

iteration

(a) 1 vs. 2

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

102

iteration

103

(c) odd vs. even

loopy BP
CG
CG(p)
LP

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

102

iteration

103

(e) PC vs. MAC

loopy BP
CG
CG(p)
LP

t

n
e
m
e
e
r
g
a



n
o

i
t

a
c
i
f
i
s
s
a
c


l

u

f

102

iteration

(g) isolet

103

1

0.99

0.98

0.97

0.96

0.95

0.94

1

0.95

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.9

0.85

0.8

0.75

0.7

0.65

0.6

0.55

0.5

0.45

1

0.9995

0.999

0.9985

0.998

0.9975

0.997

0.9965

0.996

0.9955

loopy BP
CG
CG(p)
LP

loopy BP
CG
CG(p)
LP

102

iteration

103

(b) ten digits

102

iteration

103

(d) baseball vs. hockey

loopy BP
CG
CG(p)
LP

103

loopy BP
CG
CG(p)
LP

102

iteration

(f) religion vs. atheism

102

iteration

103

(h) freefoodcam

Figure F.2: Classication agreement to the closed form harmonic solution with
various iterative methods: loopy belief propagation (loopy BP), conjugate gradient
(CG), conjugate gradient with Jacobi preconditioner (CG(p)), and label propaga-
tion (LP). Note the log-scale x-axis.

148

APPENDIXF. COMPARINGITERATIVEALGORITHMS

