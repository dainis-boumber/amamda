Abstract. We present a kernel-based framework for pattern recognition, regression estimation, function
approximation, and multiple operator inversion. Adopting a regularization-theoretic framework, the above are
formulated as constrained optimization problems. Previous approaches such as ridge regression, support vector
methods, and regularization networks are included as special cases. We show connections between the cost
function and some properties up to now believed to apply to support vector machines only. For appropriately
chosen cost functions, the optimal solution of all the problems described above can be found by solving a
simple quadratic programming problem.

Key Words. Kernels, Support vector machines, Regularization, Inverse problems, Regression, Pattern
Recognition.

1. Introduction. Estimating dependences from empirical data can be viewed as risk
minimization [43]: we are trying to estimate a function such that the risk, dened in terms
of some a priori chosen cost function measuring the error of our estimate for (unseen)
inputoutput test examples, becomes minimal. The fact that this has to be done based
on a limited amount of training examples comprises the central problem of statistical
learning theory. A number of approaches for estimating functions have been proposed in
the past, ranging from simple methods like linear regression over ridge regression (see,
e.g., [3]) to advanced methods like generalized additive models [16], neural networks, and
support vectors [4]. In combination with different types of cost functions, as for instance
quadratic ones, robust ones in Hubers sense [18], or -insensitive ones [41], these yield
a wide variety of different training procedures which at rst sight seem incompatible
with each other. The purpose of this paper, which was inspired by the treatments of [7]
and [40], is to present a framework which contains the above models as special cases
and provides a constructive algorithm for nding global solutions to these problems.
The latter is of considerable practical relevance insofar as many common models, in
particular neural networks, suffer from the possibility of getting trapped in local optima
during training.

Our treatment starts by giving a denition of the risk functional general enough to
deal with the case of solving multiple operator equations (Section 2). These provide a
versatile tool for dealing with measurements obtained in different ways, as in the case
of sensor fusion, or for solving boundary constrained problems. Moreover, we show that

1 This work was supported by the Studienstiftung des deutschen Volkes and a grant of the DFG #Ja 379/71.
2 GMD FIRST, Rudower Chaussee 5, 12489 Berlin, Germany. smola@rst.gmd.de.
3 Max Planck Institut fur biologische Kybernetik, Spemannstrasse 38, 72076 Tubingen, Germany. bs@mpik-
tueb.mpg.de.

Received January 31, 1997; revised June 1, 1997, and July 7, 1997. Communicated by P. Auer and W. Maass.

212

A. J. Smola and B. Scholkopf

they are useful for describing symmetries inherent to the data, be it by the incorporation
of virtual examples or by enforcing tangent constraints. To minimize risk, we adopt
a regularization approach which consists in minimizing the sum of training error and
a complexity term dened in terms of a regularization operator [38]. Minimization is
carried out over classes of functions written as kernel expansions in terms of the training
data (Section 3). Moreover, we describe several common choices of the regularization
operator. Following that, Section 4 contains a derivation of an algorithm for practically
obtaining a solution of the problem of minimizing the regularized risk. For appropriate
choices of cost functions, the algorithm reduces to quadratic programming. Section 5
generalizes a theorem by Morozov from quadratic cost functions to the case of convex
ones, which will give the general form of the solution to the problems stated above.
Finally, Section 6 contains practical applications of multiple operators to the case of
problems with prior knowledge. Appendices A and B contain proofs of the formulae
of Sections 4 and 5, and Appendix C describes an algorithm for incorporating prior
knowledge in the form of transformation invariances in pattern recognition problems.

2. Risk Minimization.
In regression estimation we try to estimate a functional depen-
dency f between a set of sampling points X = {x1, . . . , x(cid:96)} taken from a space V , and
target values Y = {y1, . . . , y(cid:96)}. We now consider a situation where we cannot observe
X, but some other corresponding points Xs = {xs1, . . . , xs(cid:96)s
}, nor can we observe Y , but
Ys = {ys1, . . . , ys(cid:96)s
}. We call the pairs (xss(cid:48) , yss(cid:48) ) measurements of the dependency f .
Suppose we know that the elements of Xs are generated from those of X by a (possibly
nonlinear) transformation T :
(1)

(cid:48) = 1, . . . , (cid:96)).

xss(cid:48) = T xs(cid:48)

(s

The corresponding transformation A T acting on f ,

(A T f )(x) := f ( T x),

(2)

is then generally linear: for functions f, g and coefcients ,  we have

(3)

(A T

( f + g))(x) = ( f + g)( T x)

=  f ( T x) + g( T x)
= (A T f )(x) + (A T g)(x).

Knowing A T , we can use the data to estimate the underlying functional dependency. For
several reasons, this can be preferable to estimating the dependencies in the transformed
data directly. For instance, there are cases where we specically want to estimate the
original function, as in the case of magnetic resonance imaging [42]. Moreover, we may
have multiple transformed data sets, but only estimate one underlying dependency. These
data sets might differ in size; in addition, we might want to associate different costs with
estimation errors for different types of measurements, e.g., if we believe them to differ in
reliability. Finally, if we have knowledge of the transformations, we may as well utilize it
to improve the estimation. Especially if the transformations are complicated, the original
function might be easier to estimate. A striking example is the problem of backing up a

Pattern Recognition, Regression, Approximation, and Operator Inversion

213

truck with a trailer to a given position [14]. This problem is a complicated classication
problem (steering wheel left or right) when expressed in cartesian coordinates; in polar
coordinates, however, it becomes linearly separable.

Without restricting ourselves to the case of operators acting on the arguments of f
only, but for general linear operators, we formalize the above as follows. We consider
pairs of observations (x , y ), with sampling points x and corresponding target values y .
The rst entry i of the multi-index  := (i, i
(cid:48)) denotes the procedure by which we have
(cid:48)
runs over the observations 1, . . . , (cid:96)i . In
obtained the target values; the second entry i
the following, it is understood that variables without a bar correspond to the appropriate
entries of the multi-indices. This helps us to avoid multiple summation symbols. We
may group these pairs in q pairs of sets Xi and Yi by dening
x  Vi ,
y  R,

Xi = {xi1, . . . , xi (cid:96)i
Yi = {yi1, . . . , yi (cid:96)i

with
with

(4)

}
}

with Vi being vector spaces.

We assume that these samples have been drawn independently from q corresponding
probability distributions with densities p1(x1, y1), . . . , pq (xq , yq ) for the sets Xi and Yi ,
respectively.

We further assume that there exists a Hilbert space of real-valued functions on V ,

denoted by H(V ), and a set of linear operators A1, . . . , Aq on H(V ) such that

Ai : H(V )  H(Vi )

(5)
for some Hilbert space H(Vi ) of real-valued functions on Vi . (In the case of pattern
recognition, we consider functions with values in {1} only.)

Our aim is to estimate a function f  H(V ) such that the risk functional

(6)

ci (( Ai f )(xi ), xi , yi ) pi (xi , yi ) dxi dyi

(cid:90)

R[ f ] = q(cid:88)

i=1

is minimized.4 (In some cases, we restrict ourselves to subsets ofH(V ) in order to control
the capacity of the admissible models.)

The functions ci are cost functions determining the loss for deviations between the
estimate generated by Ai f and the target value yi at the position xi . We require these

functions to be bounded from below and therefore, by adding a constant, we may as
well require them to be nonnegative. The dependence of ci on xi can, for instance,
accommodate the case of a measurement device whose precision depends on the location
of the measurement.

(cid:90)

4 A note on underlying functional dependences: for each Vi together with pi one might dene a function

(7)

yi (xi ) :=

yi pi (yi|xi ) dyi

and try to nd a corresponding function f such that Ai f = yi holds. This intuition, however, is misleading, as
yi need not even lie in the range of Ai , and Ai need not be invertible either. We resort to nding a pseudosolution

of the operator equation. For a detailed treatment see [26].

214

A. J. Smola and B. Scholkopf

EXAMPLE 1 (Vapniks Risk Functional). By specializing

A = 1

q = 1,
(cid:90)

we arrive at the denition of the risk functional of [40]:5

(9)

R[ f ] =

c( f, x, y) p(x, y) dx dy.

(8)

(11)

Specializing to c( f (x), x, y) = ( f (x)  y)2 leads to the denition of the least mean

square error risk [11].

As the probability density functions pi are unknown, we cannot evaluate (and mini-

mize) R[ f ] directly. Instead we only can try to approximate

(10)

fmin := argminH(V ) R[ f ]

by some function f , using the given data sets Xi and Yi . In practice, this requires

considering the empirical risk functional, which is obtained by replacing the integrals
over the probability density functions pi (see (6)) with summations over the empirical
data:

Remp[ f ] =

(cid:80)

1
(cid:96)i

ci (( Ai f )(x ), x , y )).
(cid:80)
(cid:80)(cid:96)i
i(cid:48)=1 with  = (i, i

q
i=1

 is a shorthand for

(cid:48)). The problem that
Here the notation
arises now is how to connect the values obtained from Remp[ f ] with R[ f ]: we can only
compute the former, but we want to minimize the latter. A naive approach is to minimize
Remp, hoping to obtain a solution f that is close to minimizing R, too. The ordinary

least mean squares method is an example for these approaches, exhibiting overtting
in the case of a high model capacity, and thus poor generalization [11]. Therefore it is
not advisable to minimize the empirical risk without any means of capacity control or
regularization [40].

(cid:88)



3. Regularization Operators and Additive Models. We assume a regularization term
in the spirit of [38] and [23], namely, a positive semidenite operator

(12)
mapping into a dot product space D (whose closure D is a Hilbert space), dening a
regularized risk functional

P: H(V )  D

(13)

Rreg[ f ] = Remp[ f ] + 
2

(cid:107) P f (cid:107)2D

5 Note that (9) already includes multiple operator equations for the special case where Vi = V and pi = p
for all i, even though this is not explicitly mentioned in [40]: c is a functional of f and therefore it may also
be a sum of functionals Ai f for several Ai .

Pattern Recognition, Regression, Approximation, and Operator Inversion

215

with a regularization parameter   0. This additional term effectively reduces our
model space and thereby controls the complexity of the solution. Note that the topic of
this paper is not nding the best regularization parameter, which would require model
selection criteria as, for instance, VC-theory [43], Bayesian methods [22], the minimum
description length principle [30], AIC [2], NIC [25]a discussion of these methods,
however, would go beyond the scope of this work. Instead, we focus on how and un-
der which conditions, given a value of , the function minimizing Rreg can be found
efciently.
We do not require positive deniteness of P, as we may not want to attenuate contri-
butions of functions stemming from a given class of models M (e.g., linear and constant
P such that M  Ker P. Making more specic as-
ones): in this case, we construct
sumptions about the type of functions used for minimizing (13), we assume f to have
(cid:48)  V ) with the property
a function expansion based on a symmetric kernel k(x, x
that, for all x  V , the function on V obtained by xing one argument of k to x is an
element of H(V ). To formulate the expansion, we use the tensor product notation for
operators on H(V )  H(V ),
(( A  B)k)(x, x
(cid:48)).
(14)
xed), and B vice versa. The class of
Here A acts on k as a function of x only (with x
(cid:48)
models that we investigate as admissible solutions for minimizing (13) are expansions
of the form

(cid:48))(x, x

f (x) =

 (( Ai  1)k)(x , x) + b,

with   R.

(cid:88)



(15)

(cid:80)



This may seem to be a rather arbitrary assumption; however, kernel expansions of the
 k(x , x) are quite common in regression and pattern recognition models [16],
type
and in the case of support vectors even follow naturally from optimality conditions with
respect to a chosen regularization [4], [42]. Moreover, an expansion with as many basis
functions as data points is rich enough to interpolate all measurements exactly, except for
some pathological cases, e.g., if the functions k (x) := k(x , x) are linearly dependent,
or if there are conicting measurements at one point (different target values for the same
x). Finally, using additive models is a useful approach insofar as the computations of the
coefcients may be carried out more easily.

To obtain an expression for (cid:107) P f (cid:107)2D in terms of the coefcients  , we rst note

 (( Ai  P)k)(x , x).

(16)
For simplicity we have assumed the constant function to lie in the null space of P, i.e.,
Pb = 0. Exploiting the linearity of the dot product in D, we can express (cid:107) P f (cid:107)2D as



(17)

( P f  P f ) =

   ((( Ai  P)k)(x , x)  (( Aj  P)k)(x  , x)).

( P f )(x) =
(cid:88)

 , 

(cid:88)

For a suitable choice of k and P, the coefcients
(18)

D  := ((( Ai  P)k)(x , .)  (( Aj  P)k)(x  , .))

216

A. J. Smola and B. Scholkopf

can be evaluated in closed form, allowing an efcient implementation (here, the dot in
k(x , .) means that k is considered as a function of its second argument, with x xed).
Positivity of (17) implies positivity of the regularization matrix D (arranging  and  in
dictionary order). Conversely, any positive semidenite matrix will act as a regularization
matrix. As we minimize the regularized risk (13), the functions corresponding to the
largest eigenvalue of D  will be attenuated most; functions with expansion coefcient
vectors lying in the null space of D, however, will not be dampened at all.

EXAMPLE 2 (Sobolev Regularization). Smoothness properties of functions f can be
enforced effectively by minimizing the Sobolev norm of a given order. Our exposition
at this point follows [15]: The Sobolev space H s, p(V ) (s  N, 1  p  ) is dened
as the space of those L p functions on V whose derivatives up to the order s are L p
functions. It is a Banach space with the norm

(19)

(cid:107) f (cid:107)H s, p (V ) =

(cid:107) D f (cid:107)L p

,

(cid:88)

||s

where  is a multi-index and D is the derivative of order  . A special case of the
Sobolev embedding theorem [37] yields

(20)

H s, p(V )  C k

k  N

for

and

s > k + d
2

.

Here d denotes the dimensionality of V and C k is the space of functions with continuous
derivatives up to order k. Moreover, there exists a constant c such that

(21)

max
||k

sup
xV

| D f (x)|  c(cid:107) f (cid:107)H s, p (V ),

i.e., convergence in the Sobolev norm enforces uniform convergence in the derivatives
up to order k.
For our purposes, we use p = 2, for which H s, p(V ) becomes a Hilbert space. In this

case, the coefcients of D are

(22)

D  =

((( Ai  D )k)(x , x)  (( Aj  D )k)(x  , x)).

(cid:88)

||s

EXAMPLE 3 (Support Vector Regularization). We consider functions which can be writ-
ten as linear functions in some Hilbert space H,

f (x) = (  (x)) + b

(23)
with : V  H and   H. The weight vector  is expressed as a linear combination
of the images of x
(24)

(cid:88)

 =

 (x ).

The regularization operator P is chosen such that P f =  for all  (in view of the
expansion (15), this denes a linear operator). Hence using the term (cid:107) P f (cid:107)2D = (cid:107)(cid:107)2

H



Pattern Recognition, Regression, Approximation, and Operator Inversion

217

corresponds to looking for the attest linear function (23) on H. Moreover,  is chosen
such that we can express the terms ((x )  (x)) in closed form as some symmetric
function k(x , x), thus the solution (23) reads

(cid:88)
 k(x , x) + b,
(cid:88)



   k(x , x  ).

 

(25)

f (x) =

and the regularization term becomes
(cid:107)(cid:107)2

=

(26)

H

(cid:90)

This leads to the optimization problem of [4]. The mapping  need not be known
explicitly: for any continuous symmetric kernel k satisfying Mercers condition [9]

if

(27)

f  L2\{0},

f (x)k(x, y) f (y) dx dy > 0

one can expand k into a uniformly convergent series k(x, y) = (cid:80)
(cid:80)
i i (x)i (y)
with positive coefcients i for i  N. Using this, it is easy to see that (x) :=

i i (x)ei ({ei} denoting an orthonormal basis of (cid:96)2) is a map satisfying ((x) 
i=1
(cid:48)). In particular, this implies that the matrix D  = k(x , x  ) is positive.
(cid:48))) = k(x, x
(x
Different choices of kernel functions allow the construction of polynomial classiers
[4] and radial basis function classiers [33]. Although formulated originally for the case
where f is a function of one variable, Mercers theorem also holds if f is dened on a
space of arbitrary dimensionality, provided that it is compact [12].6

i=1

In the next example, as well as in the remainder of the paper, we use vector notation;

e.g., (cid:69) denotes the vector with entries  , with  arranged in dictionary order.

If we dene P such that all functions used in the
EXAMPLE 4 (Ridge Regression).
(cid:88)
expansion of f are attenuated equally and decouple, D becomes the identity matrix,
D  =   . This leads to
(28)

   D  = (cid:107)(cid:69)(cid:107)2

(cid:107) P f (cid:107)2D =

 

and

(29)

Rreg[ f ] = Remp[ f ] + 
2

(cid:107)(cid:69)(cid:107)2,

which is exactly the denition of a ridge-regularized risk functional, known in the neural
network community as weight decay principle [3]. The concept of ridge regression
appeared in [17] in the context of linear discriminant analysis.

Poggio and Girosi [28] give an overview over some more choices of regularization

operators and corresponding kernel expansions.

6 The expansion of  in terms of the images of the data follows more naturally if viewed in the support vector
context [41]; however, the idea of selecting the attest function in a high-dimensional space is preserved in
the present exposition.

218

A. J. Smola and B. Scholkopf

4. Risk Minimization by Quadratic Programming. The goal of this section is to
transform the problem of minimizing the regularized risk (13) into a quadratic program-
ming problem which can be solved efciently by existing techniques. In the following
we only require the cost functions to be convex in the rst argument and
yi  R.

ci (yi , xi , yi ) = 0

xi  Vi

and all

for all

(30)
More specically, we require ci (., x , y ) to be zero exactly on the interval [

y ] with 0   , 

c ( ) := 1
(cid:96)i
) := 1
(



c
(cid:96)i

 , and C 1 everywhere else. For brevity we write
with   0,
 0,
with 


ci (y +  +  , x , y )
ci (y  


 


, x , y )

+ y ,  +

(31)

with x and y xed and

(32)

 := max(( Ai f )(x )  y   , 0),
 := max(( Ai f )(x ) + y  



, 0).

In pattern recognition problems, the intervals [


The asterisk is used for distinguishing positive and negative slack variables and corre-

sponding cost functions. The functions c and c
 describe the parts of the cost functions
ci at the location (x , y ) which differ from zero, split up into a separate treatment of
( Ai f ) y   and ( Ai f ) y  
 . This is done to avoid the (possible) discontinuity
in the rst derivative of ci at the point where it starts differing from zero.
,  ] are either [, 0] or [0,].
In this case, we can eliminate one of the two appearing slack variables, thereby getting a
simpler form for the optimization problem. In the following, however, we deal with the
more general case of regression estimation.
 and 

We may rewrite the minimization of Rreg as a constrained optimization problem, using

 , to render the subsequent calculus more amenable:

(33)

minimize

(cid:88)
(c ( ) + c


subject to ( Ai f )(x )  y +  +  ,
 


( Ai f )(x )  y  

 , 


Rreg = 1

 0.

1






,

(


)) + 1

2

(cid:107) P f (cid:107)2D

The dual of this problem can be computed using standard Lagrange multiplier techniques.
In the following, we make use of the results derived in Appendix A, and discuss some
special cases obtained by choosing specic loss functions.

EXAMPLE 5 (Quadratic Cost Function). We use (71) (Appendix A, Example 12) in the
special case p = 2,  = 0 to get the following unconstrained optimization problem:
1 K ( (cid:69)  (cid:69))

maximize

(34)

K D

(cid:88)
( (cid:69)  (cid:69))(cid:62)(cid:69)y  
2
  ) = 0,

((cid:107) (cid:69)(cid:107)2 + (cid:107) (cid:69)(cid:107)2)  1
 , 


( Ai 1)(


2

( (cid:69)  (cid:69))(cid:62)
 R+

.

0

subject to



Pattern Recognition, Regression, Approximation, and Operator Inversion

219

Transformation back to  is done by
(35)

(cid:69) = D

1 K ( (cid:69)  (cid:69)).

Here, the symmetric matrix K is dened as

K  := (( Ai  Aj )k)(x , x  ),

(36)

( Ai 1) is the operator Ai acting on the constant function with value 1. Of course there
would have been a simpler solution to this problem (by combining  and 

into one
variable resulting in an unconstrained optimization problem) but in combination with
other cost functions we may exploit the full exibility of our approach.

EXAMPLE 6 (-Insensitive Cost Function). Here we use (75) (Appendix A, Example 13)
for  = 0. This leads to
(cid:88)
( (cid:69)  (cid:69))(cid:62)(cid:69)y  ( (cid:69) + (cid:69))(cid:62)(cid:69)  1
(37)

1 K ( (cid:69)  (cid:69))

maximize

(cid:184)

(cid:183)
( (cid:69)  (cid:69))(cid:62)
 , 




2

0,

K D
1


subject to



( Ai 1)(


  ) = 0,

with the same back substitution rules (35) as in Example 5. For the special case of support
vector regularization, this leads to exactly the same equations as in support vector pattern
recognition or regression estimation [41]. In that case, one can show that D = K , and
1 K cancel out, with only the support vector equations remaining.
therefore the terms D
This follows directly from (25) and (26) with the denitions of D and K .

Note that the Laplacian cost function is included as a special case for  = 0.

EXAMPLE 7 (Hubers Robust Cost Function). Setting
 = 0

(38)

in Example 13 leads to the following optimization problem:

p = 2,
(cid:88)

(39)

maximize

subject to

( (cid:69) (cid:69))(cid:62)(cid:69)y 
(cid:88)
( 2
2
( Ai 1)(
  ) = 0,






+ 


2


) 1

2

(cid:183)
(cid:184)
( (cid:69) (cid:69))(cid:62)

 , 




0,

1


1 K ( (cid:69) (cid:69))

K D

with the same backsubstitution rules (35) as in Example 5.

The cost functions described in the Examples 5, 6, 7, 12, and 13 may be linearly com-
bined into more complicated ones. In practice, this results in using additional Lagrange
multipliers, as each of the cost functions has to be dealt with using one multiplier. Still,
by doing so computational complexity is not greatly increased as only the linear part
of the optimization problem is increased, whereas the quadratic part remains unaltered
(except for a diagonal term for cost functions of the Huber type). Muller et al. [24] report
excellent performance of the support vector regression algorithm for both -insensitive
and Huber cost function matching the correct type of the noise in an application to time
series prediction.

220

A. J. Smola and B. Scholkopf

5. A Generalization of a Theorem of Morozov. We follow and extend the proof of a
theorem originally stated by Morozov [23] as described in [28] and [7]. As in Section 4,
we require the cost functions ci to be convex and C 1 in the rst argument with the extra
requirement

ci (yi , xi , yi ) = 0

for all

(40)
We use the notation D for the closure of D, and P
P : H(V )  D,
P

:

D  P

(41)

 D  H(V ).

xi  Vi

and

yi  R.



to refer to the adjoint7 of P,

THEOREM 1 (Optimality Condition). Under the assumptions stated above, a necessary
and sufcient condition for

(42)

f = fopt := argminf H(V ) Rreg[ f ]

is that the following equation holds true:

(43)

P

 P f =  1



1ci (( Ai f )(x ), x , y ) A


i

.

x

1
(cid:96)i

(cid:88)



Here, 1 denotes the partial derivative of ci by its rst argument, and x is the Dirac
distribution, centered on x . For a proof of the theorem see Appendix B.
In order to illustrate the theorem, we rst consider the special case of q = 1 and A = 1,
i.e., the well-known setting of regression and pattern recognition. Greens functions

G(x, xj ) corresponding to the operator P

(44)

( P

 P satisfy
 PG)(x, xj ) = xj

(x),

as previously described in [28]. In this case we derive from (43) the following system of
equations which has to be solved in a self-consistent manner:

(45)

with

(46)

f (x) = (cid:96)(cid:88)

i=1

i G(x, xi ) + b

i =  1



1c( f (xi ), xi , yi ).

Here the expansion of f in terms of kernel functions follows naturally with i correspond-
ing to Lagrange multipliers. It can be shown that G is symmetric in its arguments, and

7 The adjoint of an operator O: HO  DO mapping from a Hilbert space HO to a dot product space DO is
the operator O



such that, for all f  HO and g  DO ,
= ( O

(g  O F )HO



g  f ) DO

.

Pattern Recognition, Regression, Approximation, and Operator Inversion

221

translation invariant for suitable regularization operators P. Equation (46) determines
the size of i according to how much f deviates from the original measurements yi .

For the general case, (44) becomes a little more complicated, namely we have q

functions Gi (x, x ) such that
( P

(47)

 PGi )(x, x ) = ( A


i

x

)(x)

holds. In [28] Greens formalism is used for nding suitable kernel expansions corre-
sponding to the chosen regularization operators for the case of regression and pattern
recognition. This may also be applied to the case of estimating functional dependen-
cies from indirect measurements. Moreover, (43) may also be useful for approximately
solving some classes of partial differential equations by rewriting them as optimization
problems.

6. Applications of Multiple Operator Equations.
In the following we discuss some
examples of incorporating domain knowledge by using multiple operator equations as
contained in (6).

EXAMPLE 8 (Additional Constraints on the Estimated Function). Suppose we have ad-
ditional knowledge on the function values at some points, for instance saying that
  f (0)  
for some ,  > 0. This can be incorporated by adding the points
as an extra set Xs = {xs1, . . . , xs(cid:96)s
}  X with corresponding target values Ys =
(cid:189)
{ys1, . . . , ys(cid:96)s
0
 otherwise

}  Y , an operator As = 1, and a cost function (dened on Xs)
cs ( f (xs ), xs , ys ) =

if  s  f (xs )  ys  
s

,

(48)
dened in terms of s1, . . . , s(cid:96)s and 

, . . . , 
s(cid:96)s

.

s1

These additional hard constraints result in optimization problems similar to those ob-
tained in the -insensitive approach of support vector regression [42]. See Example 14
for details.

Monotonicity and convexity of a function f , along with other constraints on deriva-

tives of f , can be enforced similarly. In that case, we use

(cid:181)

(cid:182) p


x

(49)

As =

instead of the As = 1 used above. This requires differentiability of the function expansion

of f . If we want to use general expansions (15), we have to resort to nite difference
operators.

EXAMPLE 9 (Virtual Examples). Suppose we have additional knowledge telling us that
the function to be estimated should be invariant with respect to certain transformations

Ti of the input. For instance, in optical character recognition these transformations might
sponding linear operators Ai acting on H(V ) as in (2).

be translations, small rotations, or changes in line thickness [34]. We then dene corre-

222

A. J. Smola and B. Scholkopf

As the empirical risk functional (11) then contains a sum over original and trans-
formed (virtual) patterns, this corresponds to training on an articially enlarged data
set. Unlike previous approaches such as the one of [32], we may assign different weight
to the enforcement of the different invariances by choosing different cost functions ci . If
the Ti comprise translations of different amounts, we may, for instance, use smaller cost

functions for bigger translations. Thus, deviations of the estimated function on these ex-
amples will be penalized less severely, which is reected by smaller Lagrange multipliers
(see (62)). Still, there are more general types of symmetries, especially nondeterministic
ones, which also could be taken care of by modied cost functions. For an extended
discussion of this topic see [21]. In Appendix C we give a more detailed description of
how to implement a virtual examples algorithm.

Much work on symmetries and invariances (e.g., [44]) is mainly concerned with
global symmetries (independent of the training data) that have a linear representation in
the domain of the input patterns. This concept, however, can be rather restrictive. Even
for the case of handwritten digit recognition, the above requirements can be fullled
for translation symmetries only. Rotations, for instance, cannot be faithfully represented
in this context. Moreover would a full rotation invariance not be desirable (thereby
transforming a 6 into a 9)only local invariances should be admitted. Some symmetries
only exist for a class of patterns (mirror symmetries are a reasonable concept for the
digits 8 and 0 only) and some can only be dened on the patterns themselves, e.g., stroke
changes, and do not make any sense on a random collection of pixels at all. This requires
a model capable of dealing with nonlinear, local, pattern dependent, and possibly only
approximate symmetries, all of which can be achieved by the concept of virtual examples.

EXAMPLE 10 (Hints). We can also utilize prior knowledge where target values or ranges
for the function are not explicitly available. For instance, we might know that f takes
the same value at two different points x1 and x2 [1]; e.g., we could use unlabeled data
together with known invariance transformations to generate such pairs of points. To
incorporate this type of invariance of the target function, we use a linear operator acting
on the direct sum of two copies of input space, computing the difference between f (x1)
and f (x2),
(50)
The technique of Example 8 then allows us to constrain ( As f ) to be small, on a set of

( As f )(x1  x2) := f (x1)  f (x2).

sampling points generated as direct sums of the given pairs of points.

As before (49), we can modify the above methods using derivatives of f . This will

lead to tangent regularizers as the ones proposed by [35], as we shall presently show.

EXAMPLE 11 (Tangent Regularizers). We assume that G is a Lie group of invariance
transformations. Similar to (2), we can dene an action of G on a Hilbert space H(V )
of functions on V , by

(51)

The generators in this representation, call them Si , i = 1, . . . , r, generate the group
Si ). As rst-order

in a neighborhood of the identity via the exponential map exp(

i

i

(g  f )(x) := f (gx)

g  G,

for

f  H(V ).
(cid:80)

Pattern Recognition, Regression, Approximation, and Operator Inversion

223

(tangential) invariance is a local property at the identity, we may enforce it by requiring

(52)

To motivate this, note that

( Si f )(x) = 0
(cid:195)(cid:88)
(cid:33)
(cid:33)

for

(cid:195)(cid:195)

(cid:195)

(cid:175)(cid:175)(cid:175)(cid:175)

i = 1, . . . , r.

(cid:175)(cid:175)(cid:175)(cid:175)

(cid:195)(cid:88)

(cid:33)(cid:33)

(cid:33)

f

(x)

Sj

j

x j Sj

f

exp

(53)

=0


i

=

i
= ( Si f )(x),
using (51), the chain rule, and the identity exp(0) = 1.

x

j

exp

j

=0

Examples of operators Si that can be used are derivative operators, which are the

generators of translations. Operator equations of the type (52) allow us to use virtual
examples which incorporate knowledge about derivatives of f . In the sense of [35],
this corresponds to having a regularizer enforcing invariance. Interestingly, our analy-
sis suggests that this case is not as different from a direct virtual examples approach
(Example 9) as it might appear supercially.

As in Example 8, prior knowledge could also be given in terms of allowed ranges or
cost functions [20] for approximate symmetries, rather than enforced equalities as (52).
Moreover, we can apply the approach of Example 11 to higher-order derivatives as well,
generalizing what we said above about additional constraints on the estimated function
(Example 8).

We conclude this section with an example of a possible application where the latter
could be useful. In three-dimensional surface mesh construction (e.g., [19]), one tries
to represent a surface by a mesh of few points, subject to the following constraints.
First, the surface points should be represented accuratelythis can be viewed as a
standard regression problem. Second, the normal vectors should be represented correctly,
to make sure that the surface will look realistic when rendered. Third, if there are specular
reections, say, geometrical optics comes into play, and thus surface curvature (i.e.,
higher-order derivatives) should be represented accurately.

7. Discussion. We have shown that we can employ fairly general types of regulariza-
tion and cost functions, and still arrive at a support vector type quadratic optimization
problem. An important feature of support vector machines, however, sparsity of the de-
compositions of f , is due to a special type of cost function used. The decisive part is the
, y + ] inside of which the cost for approximation, regres-
nonvanishing interval [y 

sion, or pattern recognition is zero. Therefore there exists a range of values ( Ai f )(x )
= 0 for some . By virtue of the KarushKuhnTucker
in which (32) holds with  , 

conditions, stating that the product of constraints and Lagrange multipliers have to vanish
at the point of optimality, (33) implies

(54)

(55)

 (y +  +   ( Ai f )(x )) = 0,
(( Ai f )(x )  y + 
+  ) = 0.




224

A. J. Smola and B. Scholkopf

Therefore, the  and 
inequalities. This causes sparsity in the solution of  and 
 .

 have to vanish for the constraints of (33) that become strict

As shown in Examples 3 and 6, the special choice of a support vector regularization
combined with the -insensitive cost function brings us to the case of support vector
pattern recognition and regression estimation. The advantage of this setting is that, in
the low noise case, it generates sparse decompositions of f (x) in terms of the training
data, i.e., in terms of support vectors. This advantage, however, vanishes for noisy data
as the number of support vectors increases with the noise (see [36] for details).

Unfortunately, independent of the noise level, the choice of a different regularization
1 K may not generally
prevents such an efcient calculation scheme due to (35), as D
be assumed to be diagonal. Consequently, the expansion of f is only sparse in terms of
 but not in . Yet this is sufcient for some encoding purposes as f is dened uniquely
by the matrix D

1 K and the set of  . Hence storing  is not required.

The computational cost of evaluating f (x0) also can be reduced. For the case of a
(cid:48)) satisfying Mercers condition (27), the reduced set method [6] can be
kernel k(x, x
applied to the initial solution. In that case, the nal computational cost is comparable
with the one of support vector machines, with the advantage of regularization in input
space (which is the space we are really interested in) instead of high-dimensional space.
The computational cost is approximately cubic in the number of nonzero Lagrange
multipliers  , as we have to solve a quadratic programming problem whose quadratic part
is as large as the number of basis functions of the functional expansion of f . Optimization
methods like the BunchKaufman decomposition [5], [10] have the property of incuring
computational cost only in the number of nonzero coefcients, whereas for cases with
a large percentage of nonvanishing Lagrange multipliers, interior point methods (e.g.,
[39]) might be computationally more efcient.

by n the number of functions of which f is a linear combination, f (x) =(cid:80)

We deliberately omitted the case of having fewer basis functions than constraints,
as (depending on the cost function) optimization problems of this kind may become
infeasible, at least for the case of hard constraints. However, it is not very difcult to see
how a generalization to an arbitrary number of basis functions could be achieved: denote
n
i=1 fi (x),
and by m the number of constraints or cost functions on f . Then D will be an n  n
matrix and K an n  m matrix, i.e., we have n variables i and m Lagrange multipliers
i . The calculations will lead to a similar class of quadratic optimization problems as
described in (33) and (56), with the difference that the quadratic part of the problem will
be at most of rank n, whereas the quadratic matrix will be of size m  m. A possible way
of dealing with this degeneracy is to use a singular value decomposition [29] and solve
the optimization equations in the reduced space.

To summarize, we have embedded the support vector method into a wider regulari-
zation-theoretic framework, which allow us to view a variety of learning approaches,
including but not limited to least mean squares, ridge regression, and support vector
machines as special cases of risk minimization using suitable loss functions. We have
shown that general ArseninTikhinov regularizers may be used while still preserving
important advantages of support vector machines. Specically, for particular choices
of loss function, the solution to the above problems (which can often be obtained only
through nonlinear optimization, e.g., in regression estimation by neural networks) was
reduced to a simple quadratic programming problem. Unlike many nonlinear optimiza-

Pattern Recognition, Regression, Approximation, and Operator Inversion

225

tion problems, the latter can be solved efciently without the danger of getting trapped
in local minima. Finally, we have shown that the formalism is powerful enough to deal
with indirect measurements stemming from different sources.

Acknowledgments. We would like to thank Volker Blanz, Leon Bottou, Chris Burges,
Patrick Haffner, Jorg Lemm, Klaus-Robert Muller, Noboru Murata, Sara Solla, Vladimir
Vapnik, and the referees for helpful comments and discussions. The authors are indebted
to AT&T and Bell Laboratories for the possibility to prot from an excellent research
environment during several research stays.



Appendix A. Optimization Problems for Risk Minimization. From (17) and (33)
we arrive at the following statement of the optimization problem:
D(cid:69)

(cid:88)
(c ( ) + c



minimize

(cid:69)(cid:62)

1


(56)

)) + 1
subject to ( Ai f )(x )  y +  +  ,
 


(

( Ai f )(x )  y  

for all  .
 , 

To this end, we introduce a Lagrangian:
)) + 1

(cid:88)
(cid:88)
(c ( ) + c


 (y +  +   ( Ai f )(x )) 

L = 1



 0
(cid:88)

(


(57)

 

(cid:88)
(cid:88)
   D  
(  + 


(( Ai f )(x )  y + 







)

+ 


)

2

,

2







with

 , 


,  , 


 0.

In (57), the regularization term is expressed in terms of the function expansion coefcients

 . We next do the same for the terms stemming from the constraints on ( Ai f )(x ), and
compute Ai f by substituting the expansion (15) to get

(58)

( Ai f )(x ) =

  (( Aj  Ai )k)(x  , x ) + Ai b =

  K  + Ai b.

(cid:88)



(cid:88)



See (36) for the denition of K . Now we can compute the derivatives with respect to the
primary variables  , b,  . These have to vanish for optimality.
)) = 0.

(cid:88)
(D    K  (  


L =

(59)


 
Solving (59) for (cid:69) yields
(60)



(cid:69) = D

1 K ( (cid:69)  (cid:69)),

226

A. J. Smola and B. Scholkopf

1 is the pseudoinverse in case D does not have full rank. We proceed to the

where D
next Lagrange condition, reading
 
b

(61)

1
b

L =

(cid:88)
( Ai 1)(




  ) = 0,

using Ai b = b Ai 1. Summands for which ( Ai 1) = 0 vanish, thereby removing the
constraint imposed by (61) on the corresponding variables. Partial differentiation with
respect to  and 
1
d
d


) = 


 yields

+ 


(


(62)



c

and

1


.

d
d


Now we may substitute (60), (61), and (62) back into (57), taking into account the
(cid:88)
substitution (58), and eliminate  and  , obtaining
)  
c ( ) + c
(




+( (cid:69)  (cid:69))(cid:62)(cid:69)y  ( (cid:69) + (cid:69))(cid:62)(cid:69)  1
( (cid:69)  (cid:69))(cid:62)


(


c
1 K ( (cid:69)  (cid:69)).

c ( )  

d
d

K D

L = 1

d
d

(63)





)

(cid:182)

2

c ( ) =  + 
(cid:181)

The next step is to ll in the explicit form of the cost functions c , which will enable us
to eliminate  , with programming problems in the  remaining. However (as one can

see), each of the c and c
 may have its own special functional form. Therefore we carry
out the further calculations with

(cid:181)

(cid:182)

(64)

and

T () := 1



c()  

d
d

c()

(65)
where ( ) and possible asterisks have been omitted for clarity. This leads to

c() =  + ,

1


d
d

L =

(66)

T ( )+T



(


)+( (cid:69) (cid:69))(cid:62)(cid:69)y( (cid:69)+ (cid:69))(cid:62)(cid:69) 1

2

( (cid:69) (cid:69))(cid:62)

K D

1 K ( (cid:69)B

 (cid:69)).

(cid:88)



EXAMPLE 12 (Polynomial Loss Functions). We assume the general case of functions
with -insensitive loss zone (which may vanish, if  = 0) and polynomial loss of degree
p > 1. In [8] this type of cost function was used for pattern recognition. This contains all
L p loss functions as special cases ( = 0), with p > 1, which is treated in Example 13.
We use

(67)

c() = 1
p

 p.

From (64), (65), and (67) it follows that

(68)

 p1 =  + ,

1


(cid:182)

(cid:181)

(cid:182)

(cid:181)

1
p

Pattern Recognition, Regression, Approximation, and Operator Inversion

227

T () = 1

 p   p1

= 

(69)
As we want to nd the maximum of L in terms of the dual variables we get  = 0 as T
is the only term where  appears and T becomes maximal for that value. This yields



1/( p1)( +  ) p/( p1).

1  1
p

(cid:181)

(cid:182)

(70)

T () = 

1  1
p

1/( p1) p/( p1)

with   R+

0

.

Moreover, we have the following relation between  and ;

(71)

 = ()1/( p1).

EXAMPLE 13 (Piecewise Polynomial and Linear Loss Functions). Here we discuss cost
functions with polynomial growth for [0,  ] with   0 and linear growth for [,)
such that c() is C 1 and convex. A consequence of the linear growth for large  is that
the range of the Lagrange multipliers becomes bounded, namely, by the derivative of
c(). Therefore we will have to solve box constrained optimization problems:

 1 p 1
 +

(cid:40)
(cid:179)
 1 p
(cid:179)
(cid:189)



 p

p
1
p

(cid:180)
 1
(cid:179)
(cid:180)
1  1
1  1
 1 p p1
1

p

p

(72)

(73)

c() =

T () = 1





(cid:180)

for  < ,
for   ,

 p

for  < ,
n  ,

for

 +  = 1


(74)
By the same reasoning as above we nd that the optimal solution is obtained for  = 0.
Furthermore, we can see through the convexity of c() that  <  iff  < 1/. Hence
we may easily substitute  for 1/ in the case of  >  .   [0, 1/] is always true as
  0. Combining these ndings leads to a simplication of (73):

for  < ,
for   .

(cid:181)

(cid:182)

T () = 1/( p1)

(75)
Analogously to Example 12 we can determine the error for   [0, 1/) by
(76)

 =  ()1/( p1).

0

.

 p/( p1)

for   R+

1  1
p

EXAMPLE 14 (Hard -Constraints). The simplest case to consider, however, are hard
constraints, i.e., the requirement that the approximation of the data is performed with
at most  deviation. In this case dening a cost function does not make much sense
in the Lagrange framework and we may skip all terms containing ()
i j . This leads to a
simplied optimization problem:
1 K ( (cid:69)  (cid:69))

(cid:88)
( (cid:69)  (cid:69))(cid:62)(cid:69)y  ( (cid:69) + (cid:69))(cid:62)(cid:69)  1

maximize

(77)

K D

2

( (cid:69)  (cid:69))(cid:62)
 R+
 , 


0

.

subject to



( Ai 1)(


  ) = 0,

228

A. J. Smola and B. Scholkopf

Another way to see this is to use the result of Example 6 and take the limit   0.
Loosely speaking, the interval [0, 1/] then converges to R+
0 .

Appendix B. Proof of Theorem 1. We modify the proof given in [7] to deal with the
more general case stated in Theorem 1. As Rreg is convex for all   0, minimization of
Rreg is equivalent to fullling the EulerLagrange equations. Thus a necessary and suf-
cient condition for f  H(V ) to minimize Rreg on H(V ) is that the Gateaux functional
derivative [13] (/ f )Rreg[ f, ] vanish for all   H(V ). We get

(78)


 f

Rreg[ f, ] = lim
k0
= lim
k0

1
k

Rreg[ f + k]  Rreg[ f ]

k

(cid:34)(cid:88)
(cid:88)




+ 
2

1
(cid:96)i

ci (( Ai ( f + k )))(x ), x , y )
(cid:35)
ci (( Ai f )(x ), x , y )

1
(cid:96)i

((cid:107) P( f + k )(cid:107)2D  (cid:107) P f (cid:107)2D)

.

(cid:88)



Expanding (78) in terms of k and taking the limit k  0 yields

(79)

Rreg[ f, ] =


 f

1ci (( Ai f )(x ), x , y )( Ai  )(x ) + ( P f  P )D.

1
(cid:96)i



Equation (79) has to vanish for f = fopt. As D is a Hilbert space, we can dene the
adjoint P
(80)

( P f  P )D = ( P
Similarly, we rewrite the rst term of (79) to get

 P f   )H(V ).

and get

(81)

1ci (( Ai f )(x ), x , y )(x

 Ai  )H(V ) + ( P

 P f   )H(V ).

(cid:88)

1
(cid:96)i


 Ai  )H(V ) = ( A


i

Using (x
dot product with . As  was arbitrary, this proves the theorem.8

xi

  )H(V ), the whole expression (81) can be written as a

8 Note that this can be generalized to the case of convex functions which need not be C 1. We next briey sketch
the modications in the proof. Partial derivatives of ci now become subdifferentials, with the consequence that
the equations only have to hold for some variables

i  1ci (( Ai f )(x ), x , y ).

In this case, 1 denotes the subdifferential of a function, which consists of an interval rather than just a single
number. For the proof, we convolve the non-C 1 cost functions with a positive C 1 smoothing kernel which
preserves convexity (thereby rendering them C 1), and take the limit to smoothing kernels with innitely small
support. Convergence of the smoothed cost functions to the nonsmooth originals is exploited.

Pattern Recognition, Regression, Approximation, and Operator Inversion

229

We start with an initial set of training data X0 = {x01, . . . , x0(cid:96)0

Appendix C. An Algorithm for the Virtual Examples Case. We discuss an appli-
cation of this algorithm to the problem of optical character recognition. For the sake
of simplicity we assume the case of a dichotomy problem, e.g., having to distinguish
between the digits 0 and 1, combined with a regularization operator of the support vector
type, i.e., D = K .
} together with class
labels Y0 = {y01, . . . , y0(cid:96)0
| y0i  {1, 1}}. Additionally we know that the decision
function should be invariant under small translations, rotations, changes of line thickness,
radial scaling, and slanting or deslanting operations.9
Assume transformations Ts associated with the aforementioned symmetries, together
with condence levels Cs  1 regarding whether Tsx0i will still belong to class y0i . As
in Example 9, we use Xs := X0, ( As f )(x) := f ( Tsx), and T0 := 1. As we are dealing
with the case of pattern recognition, i.e., we are only interested in sgn( f (x)), not in f (x)
itself, it is benecial to use a corresponding cost function, namely, the soft margin loss
as described in [8]:

(cid:189)

f (x)y  1,

for
otherwise.

c0( f (x), x, y) =

0
1  f (x)y

(82)
For the transformed data sets Xs we dene cost functions cs := Csc0 (i.e., we are going
to penalize errors on Xs less than on X0). As the effective cost functions (see (31)) are
0 for an interval unbounded in one direction (either (, 0] or [0,), depending on
the class labels), half of the Lagrange multipliers vanish. Therefore our setting can be
simplied by using  :=  y instead of  , i.e.,
(83)

f (x) =

(cid:88)

(cid:183)

(cid:184)

,

0,

Ci


This allows us to eliminate the asterisks in the optimization problem, reading
i 

y  = 0,

(cid:69) (cid:62) K (cid:69)

  1

subject to

(84)

maximize

2

(cid:88)





y  ( Ai k)(x , x) + b.
(cid:88)



with

(85)

K  := k( Ti x , Tj x  )y y  .

The fact that less condence has been put on the transformed samples Tsx0i leads to a
decrease in the upper boundary Cs / for the corresponding Lagrange multipliers. In this
point our algorithm differs from the virtual support vector algorithm as proposed in [32].
Moreover, their algorithm proceeds in two stages by rst nding the support vectors and
then training on a database generated only from the support vectors and their transforms.
If one was to tackle the quadratic programming problem with all variables at a time,
the proposed algorithm would incur a substantial increase of computational complexity.

9 Unfortunately no general rule can be given on the number or the extent of these transformations, as they
depend heavily on the data at hand. A database containing only a very few (but very typical) instances of a
class may benet from a large number of additional virtual examples. A large database instead possibly may
already contain realizations of the invariances in an explicit manner.

230

A. J. Smola and B. Scholkopf

However, only a small fraction of Lagrange multipliers corresponding to data relevant for
the classication problem will differ from zero (e.g., [31]). Therefore it is advantageous
to minimize the target function only on subsets of the i , keeping the other variables
xed (see [27]), possibly starting with the original data set X0.

