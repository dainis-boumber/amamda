Abstract

We consider the general problem of learning from labeled and unlabeled
data, which is often called semi-supervised learning or transductive in-
ference. A principled approach to semi-supervised learning is to design
a classifying function which is suf(cid:2)ciently smooth with respect to the
intrinsic structure collectively revealed by known labeled and unlabeled
points. We present a simple algorithm to obtain such a smooth solution.
Our method yields encouraging experimental results on a number of clas-
si(cid:2)cation problems and demonstrates effective use of unlabeled data.

1

Introduction

We consider the general problem of learning from labeled and unlabeled data. Given a
point set X = fx1; : : : ; xl; xl+1; : : : ; xng and a label set L = f1; : : : ; cg; the (cid:2)rst l points
have labels fy1; : : : ; ylg 2 L and the remaining points are unlabeled. The goal is to predict
the labels of the unlabeled points. The performance of an algorithm is measured by the
error rate on these unlabeled points only.
Such a learning problem is often called semi-supervised or transductive. Since labeling
often requires expensive human labor, whereas unlabeled data is far easier to obtain, semi-
supervised learning is very useful in many real-world problems and has recently attracted
a considerable amount of research [10]. A typical application is web categorization, in
which manually classi(cid:2)ed web pages are always a very small part of the entire web, and
the number of unlabeled examples is large.
The key to semi-supervised learning problems is the prior assumption of consistency, which
means: (1) nearby points are likely to have the same label; and (2) points on the same struc-
ture (typically referred to as a cluster or a manifold) are likely to have the same label. This
argument is akin to that in [2, 3, 4, 10, 15] and often called the cluster assumption [4, 10].
Note that the (cid:2)rst assumption is local, whereas the second one is global. Orthodox super-
vised learning algorithms, such as k-NN, in general depend only on the (cid:2)rst assumption of
local consistency.
To illustrate the prior assumption of consistency underlying semi-supervised learning, let us
consider a toy dataset generated according to a pattern of two intertwining moons in Figure
1(a). Every point should be similar to points in its local neighborhood, and furthermore,
points in one moon should be more similar to each other than to points in the other moon.
The classi(cid:2)cation results given by the Support Vector Machine (SVM) with a RBF kernel

(a) Toy Data (Two Moons)

(b) SVM  (RBF Kernel)

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

unlabeled point
labeled point  1
labeled point +1

1.5

1

0.5

0

0.5

1

1.5

2

2.5

(c) kNN

1.5

1

0.5

0

0.5

1

1.5

2

2.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

2

2.5

(c) Ideal Classification

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Figure 1: Classi(cid:2)cation on the two moons pattern. (a) toy data set with two labeled points;
(b) classifying result given by the SVM with a RBF kernel; (c) k-NN with k = 1; (d) ideal
classi(cid:2)cation that we hope to obtain.

and k-NN are shown in Figure 1(b) & 1(c) respectively. According to the assumption of
consistency, however, the two moons should be classi(cid:2)ed as shown in Figure 1(d).
The main differences between the various semi-supervised learning algorithms, such as
spectral methods [2, 4, 6], random walks [13, 15], graph mincuts [3] and transductive SVM
[14], lie in their way of realizing the assumption of consistency. A principled approach to
formalize the assumption is to design a classifying function which is suf(cid:2)ciently smooth
with respect to the intrinsic structure revealed by known labeled and unlabeled points. Here
we propose a simple iteration algorithm to construct such a smooth function inspired by the
work on spreading activation networks [1, 11] and diffusion kernels [7, 8, 12], recent work
on semi-supervised learning and clustering [2, 4, 9], and more speci(cid:2)cally by the work of
Zhu et al. [15]. The keynote of our method is to let every point iteratively spread its label
information to its neighbors until a global stable state is achieved.
We organize the paper as follows: Section 2 shows the algorithm in detail and also discusses
possible variants; Section 3 introduces a regularization framework for the method; Section
4 presents the experimental results for toy data, digit recognition and text classi(cid:2)cation,
and Section 5 concludes this paper and points out the next researches.

2 Algorithm

Given a point set X = fx1; : : : ; xl; xl+1; : : : ; xng (cid:26) Rm and a label set L = f1; : : : ; cg;
the (cid:2)rst l points xi(i (cid:20) l) are labeled as yi 2 L and the remaining points xu(l+1 (cid:20) u (cid:20) n)
are unlabeled. The goal is to predict the label of the unlabeled points.
Let F denote the set of n (cid:2) c matrices with nonnegative entries. A matrix F =
n ]T 2 F corresponds to a classi(cid:2)cation on the dataset X by labeling each
1 ; : : : ; F T
[F T
point xi as a label yi = arg maxj(cid:20)c Fij: We can understand F as a vectorial function
F : X ! Rc which assigns a vector Fi to each point xi: De(cid:2)ne a n(cid:2) c matrix Y 2 F with
Yij = 1 if xi is labeled as yi = j and Yij = 0 otherwise. Clearly, Y is consistent with the

initial labels according the decision rule. The algorithm is as follows:

1. Form the af(cid:2)nity matrix W de(cid:2)ned by Wij = exp((cid:0)kxi (cid:0) xjk2=2(cid:27)2) if i 6= j
2. Construct the matrix S = D(cid:0)1=2W D(cid:0)1=2 in which D is a diagonal matrix with

and Wii = 0:

its (i; i)-element equal to the sum of the i-th row of W:

in (0; 1):

3. Iterate F (t + 1) = (cid:11)SF (t) + (1(cid:0) (cid:11))Y until convergence, where (cid:11) is a parameter
4. Let F (cid:3) denote the limit of the sequence fF (t)g: Label each point xi as a label

yi = arg maxj(cid:20)c F (cid:3)
ij:

This algorithm can be understood intuitively in terms of spreading activation networks
[1, 11] from experimental psychology. We (cid:2)rst de(cid:2)ne a pairwise relationship W on the
dataset X with the diagonal elements being zero. We can think that a graph G = (V; E) is
de(cid:2)ned on X , where the the vertex set V is just X and the edges E are weighted by W: In
the second step, the weight matrix W of G is normalized symmetrically, which is necessary
for the convergence of the following iteration. The (cid:2)rst two steps are exactly the same as
in spectral clustering [9]. During each iteration of the third step each point receives the
information from its neighbors ((cid:2)rst term), and also retains its initial information (second
term). The parameter (cid:11) speci(cid:2)es the relative amount of the information from its neighbors
and its initial label information. It is worth mentioning that self-reinforcement is avoided
since the diagonal elements of the af(cid:2)nity matrix are set to zero in the (cid:2)rst step. Moreover,
the information is spread symmetrically since S is a symmetric matrix. Finally, the label of
each unlabeled point is set to be the class of which it has received most information during
the iteration process.
Let us show that the sequence fF (t)g converges and F (cid:3) = (1 (cid:0) (cid:11))(I (cid:0) (cid:11)S)(cid:0)1Y: Without
loss of generality, suppose F (0) = Y: By the iteration equation F (t + 1) = (cid:11)SF (t) + (1(cid:0)
(cid:11))Y used in the algorithm, we have

F (t) = ((cid:11)S)t(cid:0)1Y + (1 (cid:0) (cid:11))

t(cid:0)1

Xi=0

((cid:11)S)iY:

(1)

Since 0 < (cid:11) < 1 and the eigenvalues of S in [-1, 1] (note that S is similar to the stochastic
matrix P = D(cid:0)1W = D(cid:0)1=2SD1=2),

lim
t!1

((cid:11)S)t(cid:0)1 = 0; and lim
t!1

t(cid:0)1

Xi=0

((cid:11)S)i = (I (cid:0) (cid:11)S)(cid:0)1:

(2)

Hence

F (cid:3) = lim
t!1

F (t) = (1 (cid:0) (cid:11))(I (cid:0) (cid:11)S)(cid:0)1Y;

for classi(cid:2)cation, which is clearly equivalent to

F (cid:3) = (I (cid:0) (cid:11)S)(cid:0)1Y:

(3)
Now we can compute F (cid:3) directly without iterations. This also shows that the iteration
result does not depend on the initial value for the iteration. In addition, it is worth to notice
that (I (cid:0) (cid:11)S)(cid:0)1 is in fact a graph or diffusion kernel [7, 12].
Now we discuss some possible variants of this method. The simplest modi(cid:2)cation is to
repeat the iteration after convergence, i.e. F (cid:3) = (I (cid:0) (cid:11)S)(cid:0)1 (cid:1)(cid:1)(cid:1) (I (cid:0) (cid:11)S)(cid:0)1Y = (I (cid:0)
(cid:11)S)(cid:0)pY; where p is an arbitrary positive integer. In addition, since that S is similar to P;
we can consider to substitute P for S in the third step, and then the corresponding closed
form is F (cid:3) = (I (cid:0) (cid:11)P )(cid:0)1Y: It is also interesting to replace S with P T ; the transpose of P:
Then the classifying function is F (cid:3) = (I (cid:0)(cid:11)P T )(cid:0)1Y: It is not hard to see this is equivalent
to F (cid:3) = (D (cid:0) (cid:11)W )(cid:0)1Y: We will compare these variants with the original algorithm in the
experiments.

3 Regularization Framework

Here we develop a regularization framework for the above iteration algorithm. The cost
function associated with F is de(cid:2)ned to be

Q(F ) =

1

2(cid:18) n
Xi;j=1

Wij(cid:13)(cid:13)(cid:13)(cid:13)

1
pDii

Fi (cid:0)

1

Fj(cid:13)(cid:13)(cid:13)(cid:13)

pDjj
F 2F Q(F ):

F (cid:3) = arg min

n

2

+ (cid:22)

Xi=1 (cid:13)(cid:13)Fi (cid:0) Yi(cid:13)(cid:13)

2(cid:19);

(4)

(5)

Where (cid:22) > 0 is the regularization parameter. Then the classifying function is

The (cid:2)rst term of the right-hand side in the cost function is the smoothness constraint, which
means that a good classifying function should not change too much between nearby points.
The second term is the (cid:2)tting constraint, which means a good classifying function should
not change too much from the initial label assignment. The trade-off between these two
competing constraints is captured by a positive parameter (cid:22): Note that the (cid:2)tting constraint
contains labeled as well as unlabeled data.
We can understand the smoothness term as the sum of the local variations, i.e. the local
changes of the function between nearby points. As we have mentioned, the points involving
pairwise relationships can be be thought of as an undirected weighted graph, the weights
of which represent the pairwise relationships. The local variation is then in fact measured
on each edge. We do not simply de(cid:2)ne the local variation on an edge by the difference of
the function values on the two ends of the edge. The smoothness term essentially splits
the function value at each point among the edges attached to it before computing the local
changes, and the value assigned to each edge is proportional to its weight.
Differentiating Q(F ) with respect to F , we have

@Q

@F (cid:12)(cid:12)(cid:12)(cid:12)F =F (cid:3)

which can be transformed into

F (cid:3) (cid:0)
Let us introduce two new variables,

= F (cid:3) (cid:0) SF (cid:3) + (cid:22)(F (cid:3) (cid:0) Y ) = 0;

1

1 + (cid:22)

SF (cid:3) (cid:0)

(cid:22)

1 + (cid:22)

Y = 0:

(cid:11) =

1

1 + (cid:22)

; and (cid:12) =

(cid:22)

1 + (cid:22)

:

Note that (cid:11) + (cid:12) = 1: Then

(I (cid:0) (cid:11)S)F (cid:3) = (cid:12)Y;
F (cid:3) = (cid:12)(I (cid:0) (cid:11)S)(cid:0)1Y:

Since I (cid:0) (cid:11)S is invertible, we have
which recovers the closed form expression of the above iteration algorithm.
Similarly we can develop the optimization frameworks for the variants F (cid:3) = (I(cid:0)(cid:11)P )(cid:0)1Y
and F (cid:3) = (D (cid:0) (cid:11)W )(cid:0)1Y . We omit the discussions due to lack of space.
4 Experiments

(6)

We used k-NN and one-vs-rest SVMs as baselines, and compared our method to its two
variants: (1) F (cid:3) = (I (cid:0) (cid:11)P )(cid:0)1Y ; and (2) F (cid:3) = (D (cid:0) (cid:11)W )(cid:0)1Y: We also compared to
Zhu et al.s harmonic Gaussian (cid:2)eld method coupled with the Class Mass Normalization
(CMN) [15], which is closely related to ours. To the best of our knowledge, there is no
reliable approach for model selection if only very few labeled points are available. Hence
we let all algorithms use their respective optimal parameters, except that the parameter (cid:11)
used in our methods and its variants was simply (cid:2)xed at 0.99.

(a) t = 10

(b) t = 50

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

2

2.5

(c) t = 100

1.5

1

0.5

0

0.5

1

1.5

2

2.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

2

2.5

(d)  t = 400

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Figure 2: Classi(cid:2)cation on the pattern of two moons. The convergence process of our
iteration algorithm with t increasing from 1 to 400 is shown from (a) to (d). Note that the
initial label information are diffused along the moons.

Figure 3: The real-valued classifying function becomes (cid:3)atter and (cid:3)atter with respect to
the two moons pattern with increasing t. Note that two clear moons emerge in (d).

(a) SVM  (RBF Kernel)

(b) Smooth with Global Consistency

1.5

1

0.5

0

0.5

1

1.5

labeled point  1
labeled point +1

1.5

1

0.5

0

0.5

1

1.5

2

2.5

1.5

1

0.5

0

0.5

1

1.5

1.5

1

0.5

0

0.5

1

1.5

2

2.5

Figure 4: Smooth classi(cid:2)cation results given by supervised classi(cid:2)ers with the global con-
sistency: (a) the classi(cid:2)cation result given by the SVM with a RBF kernel; (b) smooth the
result of the SVM using the consistency method.

4.1 Toy Problem

i2)=(F (cid:3)

i1 + F (cid:3)

i1 (cid:0) F (cid:3)

In this experiment we considered the toy problem mentioned in Section 1 (Figure 1).
The af(cid:2)nity matrix is de(cid:2)ned by a RBF kernel but the diagonal elements are set to zero.
The convergence process of our iteration algorithm with t increasing from 1 to 400 is
shown in Figure 2(a)-2(d). Note that the initial label information are diffused along the
moons. The assumption of consistency essentially means that a good classifying func-
tion should change slowly on the coherent structure aggregated by a large amount of
data. This can be illustrated by this toy problem very clearly. Let us de(cid:2)ne a function
i2) and accordingly the decision function is sign(f (xi));
f (xi) = (F (cid:3)
which is equivalent to the decision rule described in Section 2. In Figure 3, we show that
f (xi) becomes successively (cid:3)atter with respect to the two moons pattern from Figure 3(a)-
3(d) with increasing t. Note that two clear moons emerge in the Figure 3(d).
The basic idea of our method is to construct a smooth function. It is natural to consider
using this method to improve a supervised classi(cid:2)er by smoothing its classifying result. In
other words, we use the classifying result given by a supervised classi(cid:2)er as the input of
our algorithm. This conjecture is demonstrated by a toy problem in Figure 4. Figure 4(a) is
the classi(cid:2)cation result given by the SVM with a RBF kernel. This result is then assigned
to Y in our method. The output of our method is shown in Figure 4(b). Note that the points
classi(cid:2)ed incorrectly by the SVM are successfully smoothed by the consistency method.

4.2 Digit Recognition

In this experiment, we addressed a classi(cid:2)cation task using the USPS handwritten 16x16
digits dataset. We used digits 1, 2, 3, and 4 in our experiments as the four classes. There
are 1269, 929, 824, and 852 examples for each class, for a total of 3874.
The k in k-NN was set to 1. The width of the RBF kernel for SVM was set to 5, and
for the harmonic Gaussian (cid:2)eld method it was set to 1.25. In our method and its variants,
the af(cid:2)nity matrix was constructed by the RBF kernel with the same width used as in
the harmonic Gaussian method, but the diagonal elements were set to 0. The test errors
averaged over 100 trials are summarized in the left panel of Figure 5. Samples were chosen
so that they contain at least one labeled point for each class. Our consistency method and
one of its variant are clearly superior to the orthodox supervised learning algorithms k-NN
and SVM, and also better than the harmonic Gaussian method.
Note that our approach does not require the af(cid:2)nity matrix W to be positive de(cid:2)nite. This
enables us to incorporate prior knowledge about digit image invariance in an elegant way,
e.g., by using a jittered kernel to compute the af(cid:2)nity matrix [5]. Other kernel methods are

0.35

0.3

0.25

r
o
r
r
e


t
s
e

t

0.2

0.15

0.1

0.05

0
4

10

15

kNN (k = 1)
SVM (RBF kernel)
harmonic Gaussian
consistency method
variant consistency (1)
variant consistency (2)

kNN (k = 1)
SVM (RBF kernel)
harmonic Gaussian
consistency method
variant consistency (1)
variant consistency (2)

0.7

0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3

0.25

r
o
r
r
e


t
s
e

t

20

25

# labeled points

30

40

50

0.2
4

10

15

20

25

# labeled points

30

40

50

Figure 5: Left panel: the error rates of digit recognition with USPS handwritten 16x16
digits dataset for a total of 3874 (a subset containing digits from 1 to 4). Right panel: the
error rates of text classi(cid:2)cation with 3970 document vectors in a 8014-dimensional space.
Samples are chosen so that they contain at least one labeled point for each class.

known to have problems with this method [5]. In our case, jittering by 1 pixel translation
leads to an error rate around 0.01 for 30 labeled points.

4.3 Text Classi(cid:2)cation

In this experiment, we investigated the task of text classi(cid:2)cation using the 20-newsgroups
dataset. We chose the topic rec which contains autos, motorcycles, baseball, and hockey
from the version 20-news-18828. The articles were processed by the Rainbow software
package with the following options: (1) passing all words through the Porter stemmer
before counting them; (2) tossing out any token which is on the stoplist of the SMART
system; (3) skipping any headers; (4) ignoring words that occur in 5 or fewer documents.
No further preprocessing was done. Removing the empty documents, we obtained 3970
document vectors in a 8014-dimensional space. Finally the documents were normalized
into TFIDF representation.
The distance between points xi and xj was de(cid:2)ned to be d(xi; xj) = 1(cid:0)hxi; xji=kxikkxjk
[15]. The k in k-NN was set to 1: The width of the RBF kernel for SVM was set to 1:5, and
for the harmonic Gaussian method it was set to 0:15. In our methods, the af(cid:2)nity matrix
was constructed by the RBF kernel with the same width used as in the harmonic Gaussian
method, but the diagonal elements were set to 0. The test errors averaged over 100 trials
are summarized in the right panel of Figure 5. Samples were chosen so that they contain at
least one labeled point for each class.
It is interesting to note that the harmonic method is very good when the number of labeled
points is 4, i.e. one labeled point for each class. We think this is because there are almost
equal proportions of different classes in the dataset, and so with four labeled points, the pro-
portions happen to be estimated exactly. The harmonic method becomes worse, however, if
slightly more labeled points are used, for instance, 10 labeled points, which leads to pretty
poor estimation. As the number of labeled points increases further, the harmonic method
works well again and somewhat better than our method, since the proportions of classes
are estimated successfully again. However, our decision rule is much simpler, which in
fact corresponds to the so-called naive threshold, the baseline of the harmonic method.

5 Conclusion

The key to semi-supervised learning problems is the consistency assumption, which essen-
tially requires a classifying function to be suf(cid:2)ciently smooth with respect to the intrinsic
structure revealed by a huge amount of labeled and unlabeled points. We proposed a sim-
ple algorithm to obtain such a solution, which demonstrated effective use of unlabeled data
in experiments including toy data, digit recognition and text categorization. In our further
research, we will focus on model selection and theoretic analysis.

Acknowledgments

We would like to thank Vladimir Vapnik, Olivier Chapelle, Arthur Gretton, and Andre Elis-
seeff for their help with this work. We also thank Andrew Ng for helpful discussions about
spectral clustering, and the anonymous reviewers for their constructive comments. Special
thanks go to Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty who communicated with
us on the important post-processing step class mass normalization used in their method and
also provided us with their detailed experimental data.

