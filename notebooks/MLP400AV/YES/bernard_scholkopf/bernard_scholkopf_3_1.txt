Abstract

This paper describes an algorithm for nding faces within
an image. The basis of the algorithm is to run an observa-
tion window at all possible positions, scales and orientation
within the image. A non-linear support vector machine is
used to determine whether or not a face is contained within
the observation window. The non-linear support vector ma-
chine operates by comparing the input patch to a set of sup-
port vectors (which can be thought of as face and anti-face
templates). Each support vector is scored by some non-
linear function against the observation window and if the
resulting sum is over some threshold a face is indicated. Be-
cause of the huge search space that is considered, it is im-
perative to investigate ways to speed up the support vector
machine. Within this paper we suggest a method of speeding
up the non-linear support vector machine. A set of reduced
set vectors (RVs) are calculated from the support vectors.
By considering the RVs sequentially, and if at any point a
face is deemed too unlikely to cease the sequential evalua-
tion, obviating the need to evaluate the remaining RVs. The
idea being that we only need to apply a subset of the RVs to
eliminate things that are obviously not a face (thus reducing
the computation). The key then is to explore the RVs in the
right order and a method for this is proposed.

1 Introduction

In this paper we consider the problem of face detection
within a large collection of images, such as a large pho-
tographic database, images bandied about in emails or dis-
played on the internet. We consider the most general prob-
lem with no constraint on the position of the face, further-
more we allow the images to be monochrome or colour so
that colour information alone cannot be used to reduce the
search (leaving the exploration of colour cues to others).

This is a well researched problem and there have been
a large number of different approaches to it. The most suc-
cessful have included those of Osuna and Girosi [3] who ap-
plied support vectors (SVs) to the problem, that of Rowley
et al [5] who used a neural network, and that of Schneider-
man and Kanade [6] who pursued a maximum likelihood
approach based on histograms of feature outputs. The one
common thing to all these methods is that they are all based
on running a 20 (cid:2) 20 pixel observation window across the
image at all possible locations, scales and orientations. This
involves a high degree of computation as (a) the observation
window is a 400 dimensional vector that has to be classied
in a very non-linear space (b) there are hundreds of thou-
sands of positions to search.

Within this paper we follow the support vector machine
approach of Osuna and Girosi [3], our new contribution be-
ing the sequential application of the support vectors to speed
up the algorithm, and an algorithm to determine the or-
der of this evaluation. Nonlinear Support Vector Machines
are known to lead to excellent classication accuracies in
a wide range of tasks [7, 10], including face detection [3].
They utilize a set of support vectors to dene a boundary
between two classes, this boundary depending on a ker-
nel function that denes a distance between two vectors.
They are, however, usually slower classiers than neural
networks. The reason for this is that their run-time complex-
ity is proportional to the number of SVs, i.e. to the number
of training examples that the SVM algorithm utilizes in the
expansion of the decision function. Whilst it is possible to
construct classication problems, even in high-dimensional
spaces, where the decision surface can be described by two
SVs only, it is normally the case that the set of SVs forms
a substantial subset of the whole training set. This is the
case for face detection where several hundred support vec-
tors can be needed.

There has been a fair amount of research on methods for
reducing the run-time complexity of SVMs [2, 8]. In the

1

present article, we employ one of these methods and adapt
it to the case where the reduced expansion is not evaluated
at once, but rather in a sequential way, such that in most
cases a very small number of SVs are applied.

The paper is organised as follows: In Section 2 the gen-
eral theory of support vector machines is reviewed with em-
phasis on non-linear support vector machines. In Section 3
it is explained how to compute a set of reduced support vec-
tors and how to deduce a suitable order for their evaluation.
The training is explained in Section 4 and the face nding
algorithm in Section 5. Results are given in Section 6 and
conclusion plus avenues for future work suggested in Sec-
tion 7.

spanned by all order d products of input features, and the
Gaussian RBF kernel

k(x; x

0) = exp(cid:18) (cid:0)kx (cid:0) x0k2

2 (cid:27)2

(cid:19) :

(4)

Performance-wise, they have been found to do similarly
well; in the present paper, we focus on the latter of the two.
This means that support vectors act as templates for faces
and anti-faces, thus relating non-linear SVs to vector quan-
tization.

3 Reduced Set Vectors

2 Non-linear Support Vector Ma-

Assume we are given a vector (cid:9) 2 F , expanded in images
of input patterns xi 2 X ,

(cid:9) =

(cid:11)i(cid:8)(xi);

(5)

with (cid:11)i 2 R; xi 2 X . To reduce the complexity of evalu-
ating it, one can approximate it by a reduced set expansion
[2]

(cid:9)0 =

(cid:12)i(cid:8)(zi);

(6)

with Nz (cid:28) Nx, (cid:12)i 2 R, and reduced set vectors zi 2 X .
To this end, one can minimize [2]

k(cid:9) (cid:0) (cid:9)0k2 =

Nx

Xi;j=1

(cid:11)i(cid:11)jk(xi; xj) +

(cid:0)2

Nx

Xi=1

Nz

Nz

Xi;j=1
Xj=1

(cid:12)i(cid:12)jk(zi; zj)

(7)

(cid:11)i(cid:12)jk(xi; zj):

The key point of that method is that although (cid:8) is not given
explicitly, (7) can be computed (and minimized) in terms of
the kernel.

The sequential approach used here requires an extension
of the reduced set method, to compute a whole sequence of
reduced set approximations

Nx

Xi=1

Nz

Xi=1

m

Xi=1

chines

classiers

Support Vector
implicitly map the data
(x1; y1); : : : ; (x; y) 2 X (cid:2) f(cid:6)1g (in our case, X is
the 20 (cid:2) 20 observation window being a 400 dimensional
integer valued vector) into a dot product space F via a
(usually nonlinear) map (cid:8) : X ! F; x 7! (cid:8)(x): F is
often referred to as the feature space. Although F can be
high-dimensional, it is usually not necessary to explicitly
work in that space [1]. There exists a class of kernels
k(x; x0) which can be shown to compute the dot products
in associated feature spaces, i.e. k(x; x0) = ((cid:8)(x) (cid:1) (cid:8)(x0)):
The SV algorithm computes a hyperplane which separates
the data in F by a large margin. Once this geometrical
problem is cast in terms of dot products, the kernel trick
is used and thus all computations in F are reduced to the
evaluation of the kernel. It can be shown that the resulting
training problem consists of computing (for some positive
value of the parameter C determining the trade-off between
margin maximization and training error minimization)

max

(cid:11)

(cid:11)i (cid:0)



Xi=1

1
2



Xi;j=1

(cid:11)i(cid:11)jyiyjk(xi; xj)

(1)

subject to 0 (cid:20) (cid:11)i (cid:20) C; i = 1; : : : ; ;

and that the solution has an expansion



Xi=1

f (x) = sgn  
Xi=1

(cid:11)iyik(x; xi) + b! :

Those training examples xi with (cid:11)i > 0 are called Support
Vectors.

Kernels commonly used include polynomials k(x; x0) =
(x (cid:1) x0)d, which can be shown to map into a feature space

(cid:11)iyi = 0;

(cid:9)0

m =

(cid:12)m;i(cid:8)(zi);

(8)

(2)

(3)

for m = 1; : : : ; Nz. The reduced set vectors zi and
the coefcients (cid:12)m;i are computed by iterative optimiza-
tion [8]. For the rst vector, we need to approximate
i=1 (cid:11)i(cid:8)(xi) by (cid:9)0 = (cid:12)(cid:8)(z). Minimizing the dis-
tance k(cid:9) (cid:0) (cid:9)0k2 between (cid:9) and (cid:9)0, with respect to z; (cid:12), to
give the rst reduced set vector z1 and its coefcient (cid:12)1;1,
using the method set out in the appendix.

(cid:9) = PNx

Recall that the aim of the reduced set algorithm is to ap-
proximate a vector (cid:9) as in equation (5) by an expansion of

2

Xi=1

the type (6) with Nz > 1. The required higher order re-
duced set vectors zi; i > 1 and their coefcients (cid:12)i, are
obtained in recursive fashion by dening a residual vector

m(cid:0)1

(cid:9)m = (cid:9) (cid:0)

(cid:12)m(cid:0)1;i(cid:8)(zi);

(9)

where (cid:9) is the original feature-space vector dened in (5).
Then the procedure for obtaining the rst reduced set vec-
tor z1 is repeated, now with (cid:9)m in place of (cid:9) to obtain zm.
However, the optimal (cid:12) from this step is not used, instead
optimal (cid:12)m;i; i = 1; : : : ; m are jointly computed [8]. Fig-
ure 1 demonstrates the effects on the classication boundary
of sequential reduced set vector evaluation. Note that there
is a law of diminishing returns, the rst few RVs yielding
the greatest increase in discrimination.

Figure 1: The result of the sequential application of RVs (stars)
to a classication problem, showing the result of using 1,2,3,4,9
and 13 RVs Darker regions indicate strong support for the classi-
cation.

Thresholds. For any Nz, the obtained expansion can be
plugged into the SVM decision function (3) to yield f (x) =

sgn(cid:16)PNz

j=1 (cid:12)jk(x; zj) + b(cid:17) : It is, however, not optimal to

simply re-use the offset b stemming from the original SV
machine. Reduced set approximations of decision functions
can be improved by recomputing the thresholds bj based on
the training set or some validation set [8], to get

Nz

fNz (x) = sgn0
Xj=1
@

(cid:12)jk(x; zj) + bNz1
A :

This is especially true in the present setting, as will become
clear in the following.

(10)

4 Training

Initially the SVM was trained on 3600 frontal face and
25000 non-face examples using Platts Sequential Minimal

3

Optimisation [4]. The kernel used was Gaussian (Equa-
tion 4) with a standard deviation (cid:27) of 3.5. The trade-off
between margin maximization and training error minimiza-
tion, was set to 1. The non-face patches were taken ran-
domly on a set of 1000 images containing no faces. The
SVM selected 1742 support vectors.

To improve the performance of the classier a second
bout of training was initiated: To decrease the number of
false positives the face detector was applied on a new set of
100 images which did not contain any faces. This gener-
ated 110000 false positive patches which were then added
to the training. To decrease the number of false negatives,
virtual faces were generated and added to the training set.
These virtual faces were computed by modifying the con-
trast or by adding an illumination plane to the faces of the
original training set. This alleviates the need of computing
a pre-processing at detection time and increase the run-time
performance of our algorithm. The SVM was then retrained
using this new training set which yielded 8291 support vec-
tors. These were subsequently decreased to 100 reduced set
vectors. Note that a retraining using the misclassications
of a previous training has been shown in [5] to produce a
greatly improved classier.

Figure 2: First 10 reduced set vectors. Note that all vectors can
be interpreted as either faces (e.g. the rst one) or anti-faces (e.g.
the second one)
.

5 Face Detection by Sequential Eval-

uation

At detection time, each pixel of an input image is a po-
tential face (a large number). To detect faces at different
scales an image pyramid is constructed. If w and h are the
width and the height of the input image and L and s the
number of sub-sampling levels and the sub-sampling rate,
respectively, the total number of patches to be evaluated is
l=1 whs2(l(cid:0)1). Evaluating the full SVM or even
the whole set of reduced vectors on all patches would be
slow. A large portion of the patches can be easily classied
using only a few reduced set vectors. Hence we propose the
following Sequential Evaluation algorithm, to be applied to

Np = PL

each overlapping patch x of an input image.

1. Set the hierarchy level to m = 1.

2. Evaluate ym = sgn(cid:16)Pm

k(x; zj):

j=1 (cid:12)m;jKj + bm(cid:17) where Kj =

3.

(cid:15) if ym < 0, x is classied as a non-face and the

algorithm stops.

(cid:15) if ym (cid:21) 0, m is incremented. If m = Nz the
algorithm stops, otherwise evaluation continues
at step 2.

4. if yj (cid:21) 0 and j = Nz, the full SVM is applied on the
patch x, using equation 3. If the evaluation is positive
the patch is classied as a face.

The main feature of this approach is that on average, rela-
tively few kernels Kj have to be evaluated at any given im-
age location  i.e., for most patches, the algorithm above
stops at a level j (cid:28) Nz. This speeds up the algorithm rela-
tive to the full reduced set (by more than an order of magni-
tude in the face classication experiments reported below).
Note that in the case of gaussian kernels, the application of
one reduced set vector amounts to a simple template match-
ing operation.

Setting offsets. The offsets bm are xed to obtain a de-
sired point on the R.O.C. for the overall sequential scheme.
Suppose an overall false negative rate (cid:23) is required, then,
given a decay rate (cid:11), we express (cid:23) as a geometric series
by setting false negative rates (cid:23)m for the mth level in the hi-
erarchy to (cid:23)j = (cid:11)(cid:23)j(cid:0)1 where (cid:23)1 = (cid:23)(1(cid:0)(cid:11)): Now each bm
is xed to achieve the desired (cid:23)m over a validation set. The
free parameter (cid:11) can now be set to maximize the overall
true positive rate over the validation set.

6 Results

Within this section the new sequential evaluation algorithm
is tested for speed and accuracy.

Speed Improvement. At detection time, due to the se-
quential evaluation of the patches, very few reduced set vec-
tors are applied. Figure 3 shows the number of reduced set
vectors evaluated per patches for different methods (SVM,
RSM and SRSM (Sequential Reduced Set Machine)), when
the algorithm is applied to the photo in Fig 4. The Full
SVM and the RSM evaluate all their support or reduced set
vectors on all the patches, while the SRSM uses on average
only 2.8 reduced set vectors per patch. Figure 4 shows the
patches of an input image which remain after 1, 10, 20 and
30 sequential reduced set evaluations on an image with one

104

103

102

101

)
z
N

(

s
n
o

i
t

l

a
u
a
v
E


r
o

t
c
e
V


t
r
o
p
p
u
S


f

o


r
e
b
m
u
N

100

102

Full SVM

RS

Hierarchical RS

Mean of Hierarchical RS

103

104

Number of Patches (Np)

105

106

Figure 3: Number of reduced set vectors used per patch for the
full SVM (8291 support vectors), Reduced Set SVM and Sequential
Reduced Set SVM (both at 100 reduced set vector)

Figure 4: From left to right: input image, followed by portions
of the image which contain un-reject patches after the sequential
evaluation of 1 (13.3% patches remaining), 10 (2.6%), 20 (0.01%)
and 30 (0.002%) support vectors. Note that in these images, a
pixel is displayed if it is part of any remaining un-rejected patch
at any scale, orientation or position This explains the apparent
discrepancy between the above percentages and the visual impres-
sion.

face, gure 5 shows the results on an image with multiple
faces.

Figure 7 shows the number of reduced set vectors used
to classify each patch of an image. The intensities values
of the pixels of the right image are proportional to the num-
ber of reduced set vectors used to classify the corresponding
spot in the left image (note that the intensities are displayed
at the center of the corresponding patches only). The uni-
form parts of the input image are easily rejected using a sin-
gle reduced set vector, whereas the cluttered background re-
quires more reduced set vectors. Note that very few patches
needed all the reduced set vectors (only the patches contain-
ing the faces used all the reduced set vectors).

Accuracy. Figure 6 shows a comparison of the accuracy
of the different methods. These R.O.C. were computed on
a test set containing 800 faces and 5000 non-faces. The ac-
curacy of the SRSM (100 reduced set vectors) is very sim-
ilar to the accuracy of the full SVM (8291 support vectors)
and the RS (100 reduced set vectors) which perform equally
well.

To compare our system with others, we used the Row-

4

than Rowleys, Sungs and Osunas results, although they
are hard to compare due to the fact that they pre-process
the patches before feeding them into their classier (his-
togram equalisation, background pixel removal and illumi-
nation gradient compensation). Our main objective was
speed, hence no pre-processing was made. Secondly, we
used a different training set as their training set was partly
proprietary. Speed gures are also hard to compare, but
from the information given, we conjecture that the Osuna
et al. RS system is comparable in speed to our RS system,
which in turn is 30 times slower than our sequential evalu-
ation system (32(cid:22)s for the sequential evaluation, 1.2ms for
the reduced set evaluation and 26ms for the full SVM per
patch on a 500MHz Pentium).

Figure 7: Top left: The intensity values of the pixels of the left
image are proportional to the number of reduced set vectors used
to classify their associated patches of the middle image. Light
grey corresponds to the use of a single reduced set vector, black
to the use of all the vectors. Top middle: 153(cid:2)263 middle im-
age contains 76108 patches and was detected in 2:58s. Top right:
A 601(cid:2)444 image containing 518801 patches detected in 27.9s.
Bottom Left: 1280(cid:2)1024 contains 2562592 patches and was
detected in 80:1s. Bottom right: A 320(cid:2)240 image containing
147289 patches detected in 10.4s (Note the false positives).

7 Conclusion and Future Work

Pattern detection systems usually have to scan large images.
Therefore, the greatest challenge in engineering systems for
real-world applications is that of reducing computational
complexity. Within this paper we have demonstrated com-
putational savings in classication by the use of a sequential

5

Figure 5: Input image, followed by patches which remain after
the evaluation of 1 (19.8% patches remaining), 10 (0.74%), 20
(0.06%) and 30 (0.01%) : : : 70 (0.007%)support vectors. Note the
comment in the caption of Fig 4.

0.1

0.09

0.08

0.07

e
v
i
t
i
s
o
P

e
s
a
F

l

0.06

0.05

0.04

0.03

0.02

0.01

0

0

0.7

0.6

0.5

0.4

0.3

0.2

0.1

e
v
i
t
i
s
o
P
e
s
a
F



l

0.005

0.01

0.015

0.02

0.025

0.03

0.035

0.04

0.045

0.05

False Negative

0

0

0.1

0.2

0.3
0.4
False Negative

0.5

0.6

0.7

Figure 6: Left: R.O.C. for the SVM using 8291 support vectors
(dotted line), the RS using 100 reduced set vectors (dashed line)
and SRSM using also 100 reduced set vectors (solid line). Note
that the SVM and RS curves are so close that they are not distin-
guishable. Right: R.O.C. for an SRSM using 1 (dashed line), 2
(dash-dot line), 3 (dotted line) and 4 (solid line) reduced set vec-
tors.

ley et al. [5] test set (which also includes the Sung et
al. [9] and the Osuna et al. [3] test images). This set con-
sists of 130 images containing 507 faces. We used a sub-
sampling ratio of s = 0:7 and the input images were sub-
sampled as long as their width and height was larger than
20 (i.e. the number of levels in the sub-sampling pyramid is

min(cid:16)(cid:13)oor(cid:16) log(20=w)

log 0:7 (cid:17) ; (cid:13)oor(cid:16) log(20=h)

are, respectively, the width and the height of the input im-
age). We obtained a detection rate of 80.7% with a false
detection rate of 0.001%. These numbers are slightly worse

log 0:7 (cid:17)(cid:17) where w and h

reduced support vector evaluation. There are several av-
enues for future research. (a) We have explored the use of
the Gaussian kernel as a distance metric, however it may be
possible to tailor the kernel to something much more suited
to facial detection. (b) It may be that the criteria for choos-
ing the reduced set of support vectors can be improved. At
present the reduced set of support vectors is chosen to min-
imize (7), which affects classication error only indirectly.
However, it might be advantageous to choose a reduced set
that minimizes classication error directly. (c) It would be
interesting to adapt the thresholds based on contextual in-
formation: for instance, if a face is detected in the image,
this places strong priors on the scale and orientation of any
other faces we expect to see. This could further speed up
the detection. Finally, although the method has been im-
plemented for the task of face detection, it could be readily
applied to a wide class of other detection and classications
problems.

Acknowledgments

Thanks to Henry Rowley for assisting us and for providing
test images. Thanks to Mike Tipping, Kentaro Toyama and
Ben Bradshaw for useful conversations.

