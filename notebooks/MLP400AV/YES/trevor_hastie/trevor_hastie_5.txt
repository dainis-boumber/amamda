Abstract

Nearest neighbor classification  expects the class  con-
ditional probabilities  to be locally  constant, and suf-
fers  from bias  in  high  dimensions We propose a lo-
cally  adaptive form of  nearest neighbor classification
to  try  to  finesse this  curse of  dimensionality. We use
a local linear  discriminant analysis to estimate an ef-
fective  metric  for  computing neighborhoods. We de-
termine the  local  decision  boundaries from centroid
information, and then  shrink neighborhoods in  direc-
tions  orthogonal to  these local  decision boundaries,
and elongate them parallel  to  the  boundaries. There-
after,  any neighborhood-based classifier  can be em-
ployed, using the  modified neighborhoods. The poste-
rior  probabilities  tend to  be more homogeneous in  the
modified  neighborhoods.  We also  propose a  method
for  global  dimension reduction,  that  combines local
dimension information.  In  a number of  examples, the
methods demonstrate the  potential  for  substantial  im-
provements over nearest  neighbour classification.

Introduction

observations.

We consider  a  discrimination  problem  with  d  classes
and  N  training
ob-
consist  of  predictor  measurements  x  =
servations
(zl,z2,...zp)
and  the  known class
memberships.  Our goal  is  to  predict
bership  of  an observation  with predictor  vector  x0

on  p  predictors

The  training

the  class  mem-

Nearest  neighbor classification

is  a  simple and ap-
pealing  approach  to  this  problem.  We find  the  set
of  K nearest  neighbors  in  the  training  set  to  x0  and
then  classify  x0 as  the  most frequent  class  among the
K neighbors.  Nearest  neighbors  is  an  extremely  flex-
ible  classification
scheme, and does  not  involve  any
pre-processing  (fitting)  of  the  training  data.  This can
offer  both  space  and  speed  advantages  in  very  large
problems:  see  Cover (1968),  Duda & Hart  (1973),
McLachlan (1992)  for  background material  on nearest
neighborhood classification.

Cover  & Hart  (1967)  show  that

the  one  nearest

neighbour rule  has  asymptotic  error  rate  at  most twice
the  Bayes rate.  However in  finite  samples  the  curse
of  dimensionality  can severely  hurt  the  nearest  neigh-
bor  rule.  The relative  radius  of  the  nearest-neighbor

142  KDD--95

sphere  grows like  r 1/p  where p  is  the  dimension  and
r  the  radius  for  p = 1,  resulting  in  severe  bias  at  the
target  point  x.  Figure 1 illustrates
the  situation  for  a
simple  example.

Figure 1:  The vertical  strip  denotes the  NN region using
only the  X coordinate to  find  the  nearest neighbor for  the
target  point  (solid  dot).  The sphere shows the  NN region
using both coordinates, and we see in  this  case it  has ex-
tended into  the  class  1 region (and found the  wrong class
in  this instance).

Our illustration  here  is  based on a 1-NN rule,  but  the
same phenomenon occurs  for  k-NN rules  as  well.  Near-
est  neighbor  techniques  are  based  on  the  assumption
that  locally  the  class  posterior  probabilities  are  con-
stant.  While that  is  clearly  true  in  the  vertical  strip
using  only  coordinate  X,  using  X and  Y this
is  no
longer true.

The techniques  outlined  in  the  abstract  are  designed
to  overcome these  problems.  Figure  2  shows an  exam-
ple.  There  are  two classes
in  two dimensions,  one of
which almost completely  surrounds  the  other.  The left
panel  shows a  nearest  neighborhood of  size  25  at  the
target  point  (shown as  origin),  which is  chosen to
near  the  class  boundary.  The right  panel  shows the
same size  neighborhood using  our  discriminant  adap-

 components of  distance  in  the  null  space  of  B* are

ignored;

  other  components  are  weighted  according

to  the
eigenvalues  of  B* when there  are  more than  2  classes
--  directions
in  which the  centroids  are  more spread
out  are  weighted  more than  those  in  which they  are
close

in  neighborhoods  sim-
Thus this  metric  would result
long in
ilar  to  the  narrow strip  in  figure  2:  infinitely
the  null  space  of  B,  and then  deformed appropriately
in  the  centroid  subspace  according  to  how they  are
placed.  It  is  dangerous to  allow  neighborhoods to  ex-
tend infinitely
in  any direction,  so we need to  limit  this
stretching.  Our proposal  is

+eI]W

-x /2
=  W-~/2[W-V2BW-I/~
= W-X/~[B*  +el]W-1/~

(2)
where e  is  some small  tuning  parameter  to  be  deter-
mined. The metric  shrinks  the  neighborhood in  direc-
tions  in  which the  local  class  centroids  differ,  with the
intention  of  ending  up  with  a  neighborhood  in  which
the  class  centroids  coincide  (and  hence nearest  neigh-
is  appropriate).  With this  goal  in
bor  classification
the  procedure,  and
mind one  can  think  of  iterating
thus  successively  shrinking  in  directions
in  which the
class  centroids  do not  coincide.

Here is  a  summary of  the  proposal.

Discriminant  Adaptive  Nearest  Neighbor Classifier

O. Initialize
1.

the  metric ~ = I,  the  identity  matriz.

Spread  out  a  nearest  neighborhood  of  KM points

around the  test  point  x0, in  the  metric ~,,.

the  weighted  within  and  between  sum of
in  the

2.  Calculate
squares  matrices  W and  B  using  the  points
neighborhood (see  formula (8  below).
3.  Define  a  new  metric  ~,,  =  W-~/~[W-1/2BW-~/2  +
2.
eI]W--
4.  Iterate  steps  1,  2,  and 3.
5.  At  completion,  use  the  metric  ~ for  K-nearest neigh-

1[

bor classification  at  the  test  point x0.

The metric  (2)  can be given  a  more formal  justifica-

tion.  Suppose we are  classifying  at  a  test  point  x0 and
find  a single  nearest  neighbor X according to  a  metric
d(X,  x0).  Let  p(j]x)  be the  true  probability  of  class
at  point  x.

We consider  the  Chi-squared  distance

which measures  the  distance  (appropriately  weighted)
between  the  true  and  estimated  posteriors.
Small

Hastie

143

Figure  2:  The leJt  panel  shows a  spherical  neighborhood
containing ~5 points.  The right panel shows the  ellipsoidal
neighborhood found by the  DANN procedure, also  contain-
ing ~5 points.  The latter  is  elongated along the true decision
boundary, and flattened  orthogonal to it.

tive  nearest  neighbour procedure.  Notice  how the  mod-
ified  neighborhood extends further  in  the  direction  par-
to  the  decision  boundary.  As we will  see  in  our
allel
simulation  studies,
this  new neighborhood  can  often
provide  improvement in  classification

performance.

While the  idea  of  local  adaptation  of  the  nearest

neighbour  metric  may seem obvious,  we could  find  few
proposMs along  these  lines
in  the  literature.  A sum-
mary of  previous  work in  given in  section  .

Discriminant

adaptive  nearest

neighbors

Our proposal  is  motivated as  follows.  Consider first  a
standard  linear  discriminant  (LDA) classification  pro-
cedure  with  K classes.  Let  B and  W denote  the  be-
tween  and  within  sum of  squares  matrices.
the  data  are  first  sphered  with  respect  to  W, then  the
target  point is  classified  to  the class  of  the  closest  cen-
troid  (with  a  correction  for  the  class  prior  membership
probabilities).  Since only relative  distances  are  rele-
vant,  any distances  in  the  complement of  the  subspace
spanned by the  sphered  centroids  can  be ignored.  This
complement corresponds  to  the  null  space  of  B.

In  LDA

We propose  to  estimate  B and  W locally,

and  use
them to  form a  local  metric  that  approximately  be-
haves  like  the  LDA metric.  One such  candidate  is

-1
=  W-IBW
=  W-1/2(W-
-t/2.
= W-I/2B*W

1/2BW-

1/2)W-I/2

(i)

where B* is the between sum-of-squares
in the sphered
space. Consider the action of ~ as a metric for com-
puting distances  (x  -  x0)T~(x --  x0):
* it  first  spheres  the  space using  W;

r(X,  x0)  implies  that  the  misclassification  error  rate
will  be  close  to  the  asymptotic  error  rate  for  INN,
which is  achieved  when X = x0 or  more generally  when
p(jlX)  = p(jlxo).  We show that  the  first
metric  (2)  approximates  r(X,  x0).

term  in

Assuming that  in  the  neighborhood  xlj  has  a  Gaus-
sian  distribution  with  mean pj  and covariance  E,  we
obtain  by  a simple  first  order  Taylor  approximation

p(jIX)  ~ p(jlx0)  p( jlxo)(#j

-  p) T~[~-l(x --

X0)

where #  = ~j  p(jlx0)pj.  Plugging  this  into  (3)  we

J

,(X,x0)  ~  Ep(jlx0)

[(#j

j=l

-  #)TE-I(x  2  (5)

(4)

is

Thus  the  approximately  best  distance  metric
-  p)(pj  p) TE-I. Es timating E

E-I  ~j  p(j[xo)(pj
by  W and  ~j  p(jlxo)(l~j
first

term in  the  metric (2).

-#)(Pi  by B  giv esthe

By allowing prior  uncertainty  for  the  class  means Pi,

that  is,  assume pj  ,.~  N(v/, eI)  in  the  sphered space,
obtain  the  second term in  the  metric  (2).

Details
implementation
Define a  weight  function  at  x0 by

the

of

-- x0)ll).

k(x, x0; h)

(6)
Here E0 is  an initial  non-negative metric (often  I),  and
Ch is  a  symmetric real-valued  function  depending on a
parameter  h.  We use  a  lri-cube  function  defined  over  a
K-nearest  neighborhood  NK(x0) of  x0.  Formally,
define  di  = IIE1/2(xi
define

-xo)ll,  h  maxiegK(xo)di  and

k(xi, xo; h) = [1 (ddh)3]aI(Id l <

(7)
Let  B(xo;  Eo,  h)  and W(xo; Eo,  h)  be  the  weighted

between  and  within  class  sum of  squares  matrices,
where the  weights  assigned  to  the  ith  observation  are
given by wi = k(xi,  xo;  Eo, h).  That is,

U( 0; r,0, h)

J

j=l

-

-

Eyi=j ,oi

EiN

I  Wi

~j

--

(8)

to  start  with  E0 = I  (the  identity  matrix)  and iterate
is  a  metric  E for  use  in
this  procedure.  The result
a  nearest  neighbor  classification
rule  at  x0.  In  our
examples we try  either  a  single  step  of  this  procedure,
or  larger  number of  iterations.

about

remarks

the  DANN metric

to  ask  whether  the  mapping g(.)  has

Some
It  is  natural
fixed  point,  and if  it  does,  whether an iteration  of  the
form E ,--  g(E)  converges to  it.  These questions  seem
difficult
to  answer in  general.  To get  some insight,
it  is  helpful  to  consider  an equivalent  form of  the  it-
eration.  At each  step  we take  a  spherical  neighbor-
hood around the  test  point,  estimate  the  metric I3,  and
then  transform  the  predictors  via  x"~~  = El/2xta.  At
completion we use  a  spherical  nearest  neighbor rule  in
the  final
transformed  space.  It  is  easy  to  show that
this  procedure is  equivalent  to  the  one given  above.  If
the  metrics estimated in j  iterations  are  El,  E2,       Ej,
then  the  effective  metric for  the  original  coordinates is
El12v,112

Expressed in  this  way, the  fixed  points  of  the  itera-

v,112~,  v.112 v,  ll2  vdl2
j
"
tion  satisfy  W-1BW-1  +  eW-l  =  cI.
a  fixed  point  occurs  when B is  zero  and  W is  pro-
portional  to  the  identity  matrix,  in  the  space  of  the
transformed  coordinates.

In  particular

"~j-l~j

~lZ~2

"z"2

"-~j-1

In  practice  we find  it  more effective  to  estimate  only
the  diagonal  elements  of  W, and  assume that  the  off
diagonal  elements  are  zero.  This  is  especially  true  if
the  dimension of  the  predictor  space is  large,  as  there
will  be insufficient  data  locally  to  estimate  the  O(p2)
elements  of  W. With  the  diagonal  approximation,
the  two forms of  the  algorithm  are  not  equivalent:  we
use  the  version  that  transforms  the  space  at  each step
since  a  diagonal  approximation  makes most  sense  in
the  transformed  coordinates.

If  the  predictors  are  spatially  or  temporally related,
we might  use  a  penalized  estimate  of  W that  down-
weights  components of  the  covariance  that  correspond
to  spatially  noisy  signals  (Hastie,  Buja & Tibshirani
1994). A related  approach is  to  pre-filter
the  predictors
using  a smooth basis,  and then  operate  in  the  reduced
domain.

In  the  final  neighborhood  we perform  K nearest

neighbor classification.  An alternative  approach would
be to  use  discriminant  analysis  to  perform the  classi-
fication,  using  the  locally  determined  parameters.  We
are  currently  investigating  this  approach.

W(x0;

~0,

h)

E;=I  Eyi:j  Wi(Xi

--

:Xj)(XI

--  ~j)T

EN=I  wi

(9)

where 2j  is  the  weighted mean of  the  Nj  observations
in  the  jth  group.  Finally,  we let  B(x0;E0,  h)  and
B(x0; E0,  h)  determine the  metric  E in  (2).

Notice  that  equations  (8)  and (2)  produce a  mapping

S0 -*  E,  say  E  = g(E0).  An approach  we explore

tuning

Choice  of
The DANN procedure  has  a  number of  adjustable
ing  parameters:

parameters

tun-

KM :  the  number of  nearest  neighbors  in  the  neighborhood

NKM (x0)  for  estimation  of  the  metric;

K :  the  number of  neighbors in  the  final  nearest  neighbor

rule;

e :  the  "softening"  parameter in  the  metric.

144  KDD-95

Test  set  or  cross  validation  could  be used to  estimate
an optimal  values  for  these  parameters.  In  the  exam-
ples  in  the  next  section  we instead  use  fixed  choices.
The value  of  Km must  be  reasonably  large  since
the
initial  neighborhood is  used to  estimate  a  covariance:
we use  KM =  max(N/5,  50).  To  ensure  consistency
one  should  take  KM to  be  a  vanishing  fraction  of  N,
and should also  use larger  values for  higher dimensional
problems.  A smaller  number of  neighbors  is  preferable
rule  to  avoid bias:  we used
for  the  final  classification
K = 5,  and compared it
to  standard  5  nearest  neigh-
bors.  Note that  the  metric  (2)  is  invariant  under non-
singular  transformations  of  the  predictors,  and hence
it  makes sense to  consider fixed  values of  e.  After  some
experimentation,  we found  that  the  value  e  = 1  works
well,  and we use  this  value  in  the  examples below.

where the  B(i)  are  the  local  between sum of  squares
matrices.  This  latter  problem  is  solved  by  finding
the  largest  eigenvectors  of  the  average  between  sum
of  squares  matrix  ~1  B(i)/N.

LDA a~l Lo~l  Subspac~ --  K = 25

J
"

L

I

2222
2

22

9

1

|1
I1 I

I 1

12//"

Local

Figure 3:  [Left  Panel] Two dimensional gaussian data with
two classes  and correlation  0.65.  The solid  lines  are the
LDA decision boundary and its  equivalent subspace for clas-
sification.  The dashed lines  were produced by the  local pro-
cedure described in  this  section.  [Right panel] Each line
segment represents  the  local  between information centered
at that point.

Figure  3  shows a  simple  illustrative

example.  The
two classes  are  Gaussian with  substantial  within  class
covariance  between  the  two predictors  Xl and  X2. In
the  left  panel,  the  solid  line  is  the  Gaussian decision
boundary  that  optimally  separates
the  classes.  The
orthogonal  vector  labeled  S is  a  one dimensional  sub-
space  onto  which we can  project
the  data  and perform
classification.  Using the  knowledge that  the  data  are
Gaussian,  it  is  the  leading  discriminant  direction.  The
broken  lines  are  the  boundaries  and equivalent  sub-
space  produced by  our  procedure.  In  the  right  panel,
each  line  segment represents
the  local  between infor-
mation  centered  at  that  point.  Our procedure  uses  a
principal  components analysis  of  these  N x  J  line  seg-
ments to  produce the  broken line  subspace  in  the  left
panel.

in  a  meaningful  way,  notice

To allow  combination  of  the  local  between informa-
tion
that  we have  not
sphered  the  data  locally  before  computing  the  mean
deviations.  A justification
for  this  is  that  any local
spherical  window containing  two classes,  say,  will  have
approximately  a  linear  decision  boundary orthogonal
to  the  vector  joining  the  two means.

Figure  4  shows the  eigenvalues  of  the  average  be-
tween matrix  for  an  instance  of  a  two class,  4  dimen-
sional  sphere  model with  6  noise  dimensions.  The de-
cision  boundary  is  a  4  dimensional  sphere,  although
locally  linear  (full  details  of  this  example are  given in
the  next  section).  For this  demonstration  we randomly
rotated
the  10 dimensional  data,  so  that  the  dimen-
sions  to  be trimmed are  not  coordinate  directions.  The
eigenvalues  show a distinct  change after  4  (the  correct

Hastie

145

Dimension

Reduction

Discriminant

using
Information

So  far  our  technique  has  been  entirely
"memory
based",  in  that  we locally  adapt  a  neighborhood about
a  query  point  at  the  time  of  classification.  Here we
describe  a  method for  performing  a  global  dimension
reduction,  by  pooling  the  local  dimension information
over all  points in  the  training  set.  In  a nutshell  we con-
sider  subspaces  corresponding  to  eigenvectors  of  the
average  local  between sum-of-squares  matrices.

Consider  first  how linear  discriminant

analysis
(LDA) works.  After  sphering  the  data,  it  concentrates
in  the  space  spanned by the  class  means ~j  or  a reduced
rank space  that  lies  close  to  these  means. If  ~ denote
the  overall  mean, this  subspace is  exactly  the  princi-
pal  component hyperplane  for  the  data  points  ~j  -  ~,
weighted by the  class  proportions.

Our idea  to  compute the  deviations  :~j  i  locally

in
a  neighborhood around  each  of  the  N training  points,
and then  do  an overall  principal  components analysis
for  the  N x J  deviations.  Here are  the  details.  Let xi  (i)
be the  mean of  class  j  vectors  in  a  neighborhood of  the
ith  training  point,  and ~(i)  be the  overall  mean. All
means are  weighted by the  local  class  membership pro-
portions zr1 (i),
the  local  centroid  deviations.  We seek  a subspace that
gets  close  in  average  weighted squared  distance  to  all
N x  J  of  these.  Denoting by  U (p  x  J)  an orthonormal
basis  for  the  k  < p  dimensional  subspace,we  minimize
the  criterion

j  = 1,...,  d.  Let xi(i)  = xj  (i)  -

ass(u)

N

J

i=1

j=l

uuT) (0,

or  the  total  weighted  residual  sum of  squares.  It  is
not  hard  to  show  that  minimizing  RSS(U)  amounts
to  maximizing

tr  UT

B(i)

4D Sphere with 6 noise Variables

d

2

4

6

Order

8

10

Figure 4:  The eigenvalues  of  the  average between matrix
for  the 4D sphere -I-  6 noise variable problem. Using these
first
four dimensions followed by our DANN nearest neigh-
bor routine,  we get  better  performance than 5NN in  the  real
4D subspaee.

dimension),  and  using  our  DANN classifier
in  these
four  dimensions  actually  beats  ordinary  5NN in  the
known four  dimensional  sphere  subspace.

information

It  is  desirable  to  automate the  dimension reduction
is  based  on
in  high  dimen-

operation.  Since  our  local
spherical  neighborhoods  (potentially
sions),  we find  an  iterative
approach  most success-
ful.  We apply  this  procedure  in  the  full  space,  and
use  cross-validated  DANN to  find  the  best  nested  sub-
space  (with  a built  in  bias  towards larger  subspaces).
in  the
We then  successively  repeat  these  operations
new subspaces,  until  no further
is  deemed
suitable  by CV. Using DANN in  this  final  subspace  is
what  we have  labelled
sub-DANN in  the  boxplots  of
figures 5.

reduction

Examples

The  methods
In  the  following  examples we compare several  classifi-
cation  approaches:
  LDA--linear  discriminant  analysis


reduced  LDA--linear  discriminant  restricted
(known) relevant  subspace,  where appropriate.

to  the

 5-NN: 5  nearest  neighbor classification

 DANN-- Discriminant  adaptive  nearest  neighbor,

reduced  5-NN



one iteration.
iter-DANN--  Discriminant  adaptive  nearest  neigh-
bor,  five iterations.

  sub-DANN-- Discriminant  adaptive  nearest  neigh-
bor,  with automatic  subspace reduction.  This  is  de-
scribed in  section  .

146  KDD-95

For all  methods, the  predictors  were first  standard-
ized  so  as  to  have zero  mean and unit  variance  over the
training  set,  and the  test  set  predictors  was standard-
ized  by  the  corresponding  training  mean and variance.
The training  and test  set  sizes  were 200 and 500, unless
indicated  otherwise.

The  problems
1. 2  Dimensional  Gaussian  with  1.~  Noise.Two  Gaus-
in  two  dimensions  (X1,X2)  separated
sian  classes
by 2  units  in  X1. The predictors  have variance  (1,2)
and correlation  0.75.  The additional  14  predictors
are  independent  standard  Gaussians.

2. Unstructured  with  8 Noise.  There are  4  classes  each
with  3  spherical  bivariate  normal subclasses,  hav-
ing  standard  deviation  0.25.  The means of  the  12
subclasses  were chosen  at  random (without  replace-
ment) from the  integers  [1,  2,...  5]  x [1,  2,...5].  Each
training  sample had 20 observations  per  subclass,  for
a total  of  240 observations.  The additional  8 predic-
tors  are  independent  standard  Gaussians.

3. 4  Dimensional Spheres  with  6  Noise.  In  this  exam-
ple  there  are  10 predictors  and 2  classes.  The last  6
predictors  are  noise  variables,  with  standard  Gaus-
independent of  each  other  and the
sian  distributions,
class  membership. The first
four  predictors  in  class
1  are  independent  standard  normal,  conditioned  on
the  radius  being greater  than  3,  while the  first  four
predictors  in  class  2  are  independent  standard  nor-
mal without  the  restriction.  The first  class  almost
completely surrounds the  second class  in  the  four  di-
mensional subspace of  the  first  four  predictors.  This
example was designed  to  see  if  DANN could  improve
upon nearest  neighbors in  the  presence of  noise  vari-
ables.

4. 10  Dimensional  Spheres.  As in  the  previous  exam-
ple  there  are  10 predictors  and 2  classes.  Now all  10
predictors  in  class  1 are  independent  standard  nor-
mal,  conditioned  on  the  radius  being  greater
than
22.4 and less  than  40,  while the  predictors  in  class
2  are  independent  standard  normal without  the  re-
In  this  example there  are  no pure  noise
striction.
variables,
the  kind  that  a  nearest  neighbor  subset
selection  rule  might be able  weed out.  At any given
point  in  the  feature  space,  the  class  discrimination
occurs  along  only one direction.  However this  direc-
tion  changes as  we move across  the  feature  space  and
all  variables  are  important  somewhere in  the  space.
The first  class  almost completely surrounds the  sec-
ond class  in  the  full  ten-dimensional space.

of  results

Discussion
The results  for  the  simulated  examples are  summarized
in  Figures 5.

DANN seems to  do as  well  as  5-NN across  the  board,
and offers  significant
improvements in  problems  with
noise  variables.  DANN does  not  do as  well  as  reduced

Two Gausslane with  Noise

Unstructured with Noise

I

T

I

o

"i

)fii

,,

1

4-D Sphere in  10-D

10-D sphere in  10-D

1

i

1

l

.

1

,itii

!E3

1
o

.+- ,,,+  <>//

Figure 5:  Boxplots of  error rates  over ~0 simulations.

nearest neighbors in  problems 1 and 3:  this  is  not sur-
prising since  in  effect  we are giving  the nearest neigh-
bor rule  the  information that  DANN is  trying  to  infer
from the  training  data.  A nearest  neighbor  method
with variable selection  might do well  in  these problems:
however this  procedure can be foiled  by by rotating the
relevant  subspace away from the  coordinate directions.
On the  average  there  seems to  be  no advantage in
carrying out  more than one iteration  of  the  DANN pro-
cedure.  The subspace  DANN procedure  is  the  over-
all  winner, producing big  gains  in  problems admitting
global  dimension reduction.

The top panel of  Figure 6 shows error rates  relative

to  5-NN, accumulated across  8  x  20 simulated problems
(these  4  and another 4 described in  Hastie & Tibshirani
(1995).  The bottom panel  shows the  rates  relative
LDA.

We see  that  DANN is  20-30% better  than  5-NN on
the  average,  and is  at  most 20% worse.  DANN is  also
better  than LDA on the  average but can be three  times
worse (in  problem 2).

Image  Classification

Example

Image scene classification  is  an important and difficult
problem. The example we consider  here  is  classifying
satellite
images of  an area of  the  earths  surface into
land and vegetation  type.  Other examples include clas-
sification  of  X-rays, such as  mammograms,
or calcified  into  normal or cancerous regions.

into  normal

Figure  7  shows four  spectral  bands of  a  section  of
spectrum (red  and green)

land,  two in  the  visible

Enor rell fCalJve

Io ,,~-~1

E/TOt r~ r~ll  to LDA

I



T

.... f-I_;

............

, ...............
i ............
!
i
!

.

770I

l

[

l



T

- I

|

=
-

,

--

l



I
T
I

l

*

I

I

l

i

I

I

+* ~ i

Figure 6:  Relative  error rates  ol  the methods across the
8 simulated problems. In the  top panel the error  rate  has
been divided  by the  error  rate  5-NN, on a  simulation by
simulation basis.  In the  bottom panel we have divided  by
the error rate  ol  LDA.

Spectral  band 1

Speclml  band  2

Spectral  band 3

Spectral  band  4

Land  use (Actual)

Land use  (Predicted)

Figure 7:  The first  four images are the  satellite
images
in  the four spectral  bands. The fifth  image is  the  known
classification,  and the final image is  the classification map
produced by linear discrlmlnant analysis.

N

N

N

N

X

N

N

N

N

A Pixel  and  its  8  neighbors

Figure 8:  The pixel intensities of  the 8-neighbors of  a pixel
(and itself)  are used as features for classification

Hastie

147

and  two  in  the  infra  red  spectrum.  These  data  are
taken  from  the  STATLOG projects  archive
(Michie,
Spigelhalter  & Taylor  1994)1.  The goal  is  to  clas-
sify  each  pixel  into  one of  7  land  types:  red  soil,
cotton,  vegetalion  stubble,  mixture,  grey  soil,  damp
grey  soil,  very  damp grey  soil.  We extract  for  each
pixel  its  8-neighbors,  as  depicted in  figure  8,  giving us
(8+ 1)x  4 = 36 features  (the  pixel  intensities)  per pixel
to  be classified.  The data  come scrambled,  with  4435
training  pixels  and 2000 test  pixels,  each  with  their
36 features  and the  known classification.
Included  in
figure  7 is  the  true  classification,  as  well as  that  pro-
duced by linear  discriminant  analysis.  Figure  9  shows

Satellite Image Classification

..h  o

o

0.

5

10

15

o

20

25

~rnee, s~n

LDA

Figure 10: Miscla.ssification  results  as  a function of sub-
space size,  for the  satellite  image data

STATLOG results

ogistic
SMART

QDA

NewlDC4.5

CART

Nueral

ALLOCSO

RBF

LVQ

K-NN

8
<5

)AN

i
2

i
4

i

6

i

8

i

10

t

12

i

14

Method

Figure 9: Misclassification results  of  a variety of classifi-
cation  procedures on the  satellite
image test  data (taken
from Michie et  al.  (1994)).

the  results  reported  in  Michie et  al.  (1994) for  a  va-
riety  of  classifiers;
they  reported  the  best  result  for
5-NN classification.
Included in  the  figure  is  the  result
for  DANN, which has  outperformed  5-NN. We also  ran
the  subspace  version  of  DANN, and figure  10 shows the
sequence of  of  test-error
results  as  a function  of  sub-
space  size.  Again, a  low-dimensional subspace actually
improves the  misclassification  error.

Discussion

that  can  offer  substantial

We have developed  an  adaptive  form of  nearest  neigh-
bor  classification
improve-
ments over  standard  nearest  neighbors  method in  some
problems.  We have  also  proposed  a  method that  uses
local  discrimination  information  to  estimate  a  subspace
for  global  dimension reduction.

Short  & Fukanaga (1980)  proposed a  technique  close
to  ours  for  the  two class  problem.  In  our  terminology
they  used  our  metric  with  W = I  and  e  =  0,  with  B

1The authors  thank C. Taylor and D. Spiegelhalter  for

making these  images and data  available

148  KDD-95

this  extends  the  neighborhood infinitely

in  a  neighborhood of  size  KM. In
determined  locally
effect
in  the
null  space  of  the  local  between class  directions,  but
this  neighborhood to  the  original  KM ob-
they  restrict
servations.  This  amounts to  projecting
the  local  data
onto  the  line  joining  the  two local  centroids.
In  our
experiments  this  approach tended  to  perform  on  aver-
age  10% worse than  our  metric,  and we did  not  pursue
it  further.  Short  & Fukanaga (1981)  extended  this
J  > 2  classes,  but  here  their  approach  differs  even
more  from  ours.  They  computed a  weighted  average
of  the  J  local  centroids  from the  overall  average,  and
project  the  data  onto  it,  a one dimensional projection.
Even with  e  = 0  we project
the  data  onto  the  sub-
space  containing  the  local  centroids,  and deform the
metric  appropriately
in  that  subspace.  Myles & Hand
(1990) recognized a  shortfall  of  the  Short  and Fukanaga
approach,  since  the  averaging  can  cause  cancellation,
and  proposed  other  metrics  to  avoid  this.  Although
their  metrics  differ  from ours,  the  Chi-squared  moti-
vation  for  our  metric  (3)  was inspired  by  the  metrics
developed in  their  paper.  We have not  tested  out  their
proposals,  but  they  report  results  of  experiments with
far  more modest  improvements over  standard  nearest
neighbors  than  we achieved.

Friedman (1994) proposes  a  number of  techniques  for
flexible  metric  nearest  neighbor classification.  These
techniques  use  a  recursive  partitioning  style  strategy
to  adaptively  shrink  and shape  rectangular  neighbor-
hoods  around  the  test  point.  Friedman also  uses  de-
rived  variables  in  the  process,  including  discriminant
variates.  With the  latter  variables,  his  procedures have
some similarity
to  the  discriminant  adaptive  nearest
neighbor  approach.

Other recent  work that  is  somewhat related  to  this
is  that  of  Lowe (1993).  He estimates
the  covariance
matrix in  a variable  kernel classifier  using a neural net-
work approach.

There  are  a  number of  ways in  which this  work might

In  some discrimination  problems,  it
to  use  specialized  distance  measures  that
in  the  feature  space.  For  exam-
Simard

be generalized.
is  natural
capture  invariances
ple  Simard,  LeCun & Denker (1993),  IIastie,
& Sackinger  (1993),  use  a  transformation-invariant
metric  to  measure  distance  between  digitized
of  handwritten  numerals  in  a  nearest  neighbor  rule.
The invariances
ages  such  as  rotation,
invariant  distance  measure might  be used  in  a  linear
discriminant  analysis  and  hence  in  the  DANN proce-
dare.

include  local  transformations  of  im-
shear  and stroke-thickness.  An

images

Another interesting  possibility  would be to  apply  the
techniques of  this  paper to  regression  problems. In  this
case  the  response  variable  is  quantitative
rather  than
a class  label.  Natural  analogues  of  the  local  between
and within  matrices  exist,  and  can  be  used  to  shape
the  neighborhoods  for  near-neighbor  and local  poly-
nomial regression  techniques.  Likewise,  the  dimension
reduction  ideas  of  section  can also  be  applied.  There
is  a strong  connection between the  latter  and the  Sliced
Inverse  Regression  technique  of  Duan & Li  (1991)  for-
subspace  identification.  We are  currently  exploring
these directions.
Acknowledgments  We thank  Jerome  Friedman  for
sharing  his  recent  work, which stimulated  us  to  embark
on this  project,  and for  many enjoyable  conversations.
The second  author  was supported  by  a  grant  from
the  Natural  Sciences  and Engineering  Research  Coun-
cil  of  Canada.

