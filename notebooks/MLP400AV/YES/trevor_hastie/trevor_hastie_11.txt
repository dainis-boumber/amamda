1. Introduction. The starting point for this paper is an interesting pro-
cedure called boosting, which is a way of combining the performance of many
weak classiers to produce a powerful committee. Boosting was proposed in
the computational learning theory literature [Schapire (1990), Freund (1995),
Freund and Schapire (1997)] and has since received much attention.

While boosting has evolved somewhat over the years, we describe the most
commonly used version of the AdaBoost procedure [Freund and Schapire

Received August 1998; revised December 1999.
1Also at Stanford Linear Accelerator Center, Stanford, CA 94305. Supported in part by Dept.

of Energy Contract DE-AC03-76 SF 00515 and NSF Grant DMS-97-64431.

2Also at Division of BioStatistics, Dept. of Health, Research and Policy, Stanford University,

Stanford, CA 94305.

3Supported in part by NSF Grants DMS-95-04495, DMS-98-03645 and NIH Grant

ROI-CA-72028-01.

4Supported in part by Natural Sciences and Engineering Research Council of Canada.
AMS 1991 subject classications. 62G05, 62G07, 68T10, 68T05.
Key words and phrases. classication, tree, nonparametric estimation, stagewise tting,

machine learning.

337

338

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

(1996b)], which we call Discrete AdaBoost. This is essentially the same as
AdaBoost.M1 for binary data in Freund and Schapire. Here is a concise descrip-
tion of AdaBoost in the two-class classication setting. We have training data
dene F(cid:1)x(cid:2) = (cid:1)M
(cid:1)x1(cid:1) y1(cid:2)(cid:1) (cid:4) (cid:4) (cid:4) (cid:1)(cid:1)xN(cid:1) yN(cid:2) with xi a vector valued feature and yi = 1 or 1. We
1 cmfm(cid:1)x(cid:2) where each fm(cid:1)x(cid:2) is a classier producing val-
ues plus or minus 1 and cm are constants; the corresponding prediction is
sign(cid:1)F(cid:1)x(cid:2)(cid:2). The AdaBoost procedure trains the classiers fm(cid:1)x(cid:2) on weighted
versions of the training sample, giving higher weight to cases that are cur-
rently misclassied. This is done for a sequence of weighted samples, and then
the nal classier is dened to be a linear combination of the classiers from
each stage. A detailed description of Discrete AdaBoost is given in the boxed
display titled Algorithm 1.

Much has been written about the success of AdaBoost in producing accurate
classiers. Many authors have explored the use of a tree-based classier for
fm(cid:1)x(cid:2) and have demonstrated that it consistently produces signicantly lower
error rates than a single decision tree. In fact, Breiman (1996) (referring to
a NIPS workshop) called AdaBoost with trees the best off-the-shelf classier
in the world [see also Breiman (1998b)]. Interestingly, in many examples the
test error seems to consistently decrease and then level off as more classiers
are added, rather than ultimately increase. For some reason, it seems that
AdaBoost is resistant to overtting.

Figure 1 shows the performance of Discrete AdaBoost on a synthetic clas-
sication task, using an adaptation of CARTTM [Breiman, Friedman, Olshen
and Stone (1984)] as the base classier. This adaptation grows xed-size trees
in a best-rst manner (see Section 8). Included in the gure is the bagged
tree [Breiman (1996)] which averages trees grown on bootstrap resampled
versions of the training data. Bagging is purely a variance-reduction tech-
nique, and since trees tend to have high variance, bagging often produces
good results.

Discrete AdaBoost [Freund and Schapire (1996b)]

1. Start with weights wi = 1/N(cid:1) i = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N.
2. Repeat for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M:

(a) Fit the classier fm(cid:1)x(cid:2)  (cid:6)1(cid:1) 1(cid:7) using weights wi on the training data.
(b) Compute errm = Ew(cid:8)1(cid:1)y(cid:9)=fm(cid:1)x(cid:2)(cid:2)(cid:10), cm = log(cid:1)(cid:1)1  errm(cid:2)/errm(cid:2).
(c) Set wi  wi exp(cid:8)cm1(cid:1)yi(cid:9)=fm(cid:1)xi(cid:2)(cid:2)(cid:10), i = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N, and renormalize so that
3. Output the classier sign(cid:8)(cid:1)M

(cid:1)
i wi = 1.

m=1 cmfm(cid:1)x(cid:2)(cid:10).

Algorithm 1. Ew represents expectation over the training data with
weights w = (cid:1)w1(cid:1) w2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) wN(cid:2), and 1(cid:1)S(cid:2) is the indicator of the set S. At each
iteration, AdaBoost increases the weights of the observations misclassied by
fm(cid:1)x(cid:2) by a factor that depends on the weighted training error.

ADDITIVE LOGISTIC REGRESSION

339

Fig. 1. Test error for Bagging, Discrete AdaBoost and Real AdaBoost on a simulated two-class
nested spheres problem (see Section 6). There are 2000 training data points in ten dimensions, and
the Bayes error rate is zero. All trees are grown best-rst without pruning. The leftmost iteration
corresponds to a single tree.

Early versions of AdaBoost used a resampling scheme to implement step 2
of Algorithm 1, by weighted sampling from the training data. This suggested a
connection with bagging and that a major component of the success of boosting
has to do with variance reduction.

However, boosting performs comparably well when:

1. A weighted tree-growing algorithm is used in step 2 rather than weighted
resampling, where each training observation is assigned its weight wi. This
removes the randomization component essential in bagging.

2. Stumps are used for the weak learners. Stumps are single-split trees with
only two terminal nodes. These typically have low variance but high bias.
Bagging performs very poorly with stumps [Figure 1 (top right panel)].

340

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

These observations suggest that boosting is capable of both bias and vari-

ance reduction, and thus differs fundamentally from bagging.
The base classier in Discrete AdaBoost produces a classication rule
fm(cid:1)x(cid:2)(cid:12) (cid:1) (cid:13) (cid:6)1(cid:1) 1(cid:7), where (cid:1) is the domain of the predictive features x.
Freund and Schapire (1996b), Breiman (1998a) and Schapire and Singer
(1998) have suggested various modications to improve the boosting algo-
rithms.
A generalization of Discrete AdaBoost appeared in Freund and Schapire
(1996b), and was developed further in Schapire and Singer (1998), that uses
real-valued condence-rated predictions rather than the (cid:6)1(cid:1) 1(cid:7) of Discrete
AdaBoost. The weak learner for this generalized boosting produces a map-
ping fm(cid:1)x(cid:2): (cid:1) (cid:13) R; the sign of fm(cid:1)x(cid:2) gives the classication, and (cid:15)fm(cid:1)x(cid:2)(cid:15) a
measure of the condence in the prediction. This real-valued contribution is
combined with the previous contributions with a multiplier cm as before, and
a slightly different recipe for cm is provided.

We present a generalized version of AdaBoost, which we call Real AdaBoost
in Algorithm 2, in which the weak learner returns a class probability estimate
pm(cid:1)x(cid:2) = Pw(cid:1)y = 1(cid:15)x(cid:2)  (cid:8)0(cid:1) 1(cid:10). The contribution to the nal classier is half the
logit-transform of this probability estimate. One form of Schapire and Singers
generalized AdaBoost coincides with Real AdaBoost, in the special case where
the weak learner is a decision tree. Real AdaBoost tends to perform the best
in our simulated examples in Figure 1, especially with stumps, although we
see with 100 node trees Discrete AdaBoost overtakes Real AdaBoost after 200
iterations.

In this paper we analyze the AdaBoost procedures from a statistical per-
(cid:1)
spective. The main result of our paper rederives AdaBoost as a method for
m fm(cid:1)x(cid:2) in a forward stagewise manner. This sim-
tting an additive model
ple fact largely explains why it tends to outperform a single base learner.
By tting an additive model of different and potentially simple functions, it
expands the class of functions that can be approximated.

Real AdaBoost
1. Start with weights wi = 1/N(cid:1) i = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N.
2. Repeat for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M:

(a) Fit the classier to obtain a class probability estimate pm(cid:1)x(cid:2) = Pw(cid:1)y =
(b) Set fm(cid:1)x(cid:2)  1
(c) Set wi  wi exp(cid:8)yifm(cid:1)xi(cid:2)(cid:10), i = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N, and renormalize so that
3. Output the classier sign(cid:8)(cid:1)M

1(cid:15)x(cid:2)  (cid:8)0(cid:1) 1(cid:10), using weights wi on the training data.
(cid:1)
i wi = 1.

2 log pm(cid:1)x(cid:2)/(cid:1)1  pm(cid:1)x(cid:2)(cid:2)  R.

m=1 fm(cid:1)x(cid:2)(cid:10).

Algorithm 2. The Real AdaBoost algorithm uses class probability esti-

mates pm(cid:1)x(cid:2) to construct real-valued contributions fm(cid:1)x(cid:2).

ADDITIVE LOGISTIC REGRESSION

341

1

Given this fact, Discrete and Real AdaBoost appear unnecessarily compli-
squared-error loss E(cid:1)y (cid:1)
cated. A much simpler way to t an additive model would be to minimize
fm(cid:1)x(cid:2)(cid:2)2 in a forward stagewise manner. At the
= E(cid:1)y(cid:1)m1
mth stage we x f1(cid:1)x(cid:2) fm1(cid:1)x(cid:2) and minimize squared error to obtain fm(cid:1)x(cid:2)
fj(cid:1)x(cid:2)(cid:15)x(cid:2). This is just tting of residuals and is commonly used
in linear regression and additive modeling [Hastie and Tibshirani (1990)].
However squared error loss is not a good choice for classication (see
Figure 2 in Section 4.2) and hence tting of residuals doesnt work very
well in that case. We show that AdaBoost ts an additive model using a bet-
ter loss function for classication. Specically we show that AdaBoost ts an
additive logistic regression model, using a criterion similar to, but not the
same as, the binomial log-likelihood. [If pm(cid:1)x(cid:2) are the class probabilities, an
(cid:1)
additive logistic regression approximates log pm(cid:1)x(cid:2)/(cid:1)1  pm(cid:1)x(cid:2)(cid:2) by an addi-
m fm(cid:1)x(cid:2).] We then go on to derive a new boosting procedure
tive function
LogitBoost that directly optimizes the binomial log-likelihood.
The original boosting techniques [Schapire (1990), Freund (1995)] prov-
ably improved or boosted the performance of a single classier by produc-
ing a majority vote of similar classiers. These algorithms then evolved
into more adaptive and practical versions such as AdaBoost, whose success
was still explained in terms of boosting individual classiers by a weighted
majority vote or weighted committee. We believe that this view, along with
the appealing name boosting inherited by AdaBoost, may have led to some
of the mystery about how and why the method works. As mentioned above,
we instead view boosting as a technique for tting an additive model.

Section 2 gives a short history of the boosting idea. In Section 3 we briey
review additive modeling. Section 4 shows how boosting can be viewed as an
additive model estimator and proposes some new boosting methods for the two-
class case. The multiclass problem is studied in Section 5. Simulated and real
data experiments are discussed in Sections 6 and 7. Our tree-growing imple-
mentation, using truncated best-rst trees, is described in Section 8. Weight
trimming to speed up computation is discussed in Section 9, and we briey
describe generalizations of boosting in Section 10. We end with a discussion
in Section 11.

2. A brief history of boosting. Schapire (1990) developed the rst sim-
ple boosting procedure in the PAC-learning framework [Valiant (1984), Kearns
and Vazirani (1994)]. Schapire showed that a weak learner could always
improve its performance by training two additional classiers on ltered ver-
sions of the input data stream. A weak learner is an algorithm for producing
a two-class classier with performance guaranteed (with high probability) to
be signicantly better than a coinip. After learning an initial classier h1 on
the rst N training points:
1. h2 is learned on a new sample of N points, half of which are misclassied

by h1.

2. h3 is learned on N points for which h1 and h2 disagree.
3. The boosted classier is hB = Majority Vote(cid:1)h1(cid:1) h2(cid:1) h3(cid:2).

342

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Schapires Strength of Weak Learnability theorem proves that hB has
improved performance over h1.

Freund (1995) proposed a boost by majority variation which combined
many weak learners simultaneously and improved the performance of the sim-
ple boosting algorithm of Schapire. The theory supporting both of these algo-
rithms requires the weak learner to produce a classier with a xed error rate.
This led to the more adaptive and realistic AdaBoost [Freund and Schapire
(1996b)] and its offspring, where this assumption was dropped.

Freund and Schapire (1996b) and Schapire and Singer (1998) provide some
theory to support their algorithms, in the form of upper bounds on generaliza-
tion error. This theory has evolved in the computational learning community,
initially based on the concepts of PAC learning. Other theories attempting
to explain boosting come from game theory [Freund and Schapire (1996a),
Breiman (1997)] and VC theory [Schapire, Freund, Bartlett and Lee (1998)].
The bounds and the theory associated with the AdaBoost algorithms are inter-
esting, but tend to be too loose to be of practical importance. In practice, boost-
ing achieves results far more impressive than the bounds would imply.

additive model F(cid:1)x(cid:2) = (cid:1)M

3. Additive models. We show in the next section that AdaBoost ts an
m=1 cmfm(cid:1)x(cid:2). We believe that viewing current boost-
ing procedures as stagewise algorithms for tting additive models goes a long
way toward understanding their performance. Additive models have a long
history in statistics, and so we rst give some examples here.

3.1. Additive regression models. We initially focus on the regression prob-
lem, where the response y is quantitative, x and y have some joint distribu-
tion, and we are interested in modeling the mean E(cid:1)y(cid:15)x(cid:2) = F(cid:1)x(cid:2). The additive
model has the form

(1)

F(cid:1)x(cid:2) = p(cid:2)

j=1

fj(cid:1)xj(cid:2)(cid:4)

There is a separate function fj(cid:1)xj(cid:2) for each of the p input variables xj. More
generally, each component fj is a function of a small, prespecied subset of
the input variables. The backtting algorithm [Friedman and Stuetzle (1981),
Buja, Hastie and Tibshirani (1989)] is a convenient modular GaussSeidel
algorithm for tting additive models. A backtting update is

(2)

fj(cid:1)xj(cid:2)  E

for j = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) p(cid:1) 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:4)

(cid:3)

y  (cid:2)

k(cid:9)=j

(cid:5)

(cid:4)(cid:4)(cid:4)xj

fk(cid:1)xk(cid:2)

Any method or algorithm for estimating a function of xj can be used to
obtain an estimate of the conditional expectation in (2). In particular, this
can include nonparametric smoothing algorithms, such as local regression or
smoothing splines. In the right-hand side, all the latest versions of the func-
tions fk are used in forming the partial residuals. The backtting cycles are
repeated until convergence. Under fairly general conditions, backtting can

ADDITIVE LOGISTIC REGRESSION

343
be shown to converge to the minimizer of E(cid:1)y  F(cid:1)x(cid:2)(cid:2)2 [Buja, Hastie and
Tibshirani (1989)].

3.2. Extended additive models. More generally, one can consider additive
models whose elements (cid:6)fm(cid:1)x(cid:2)(cid:7)M
1 are functions of potentially all of the input
features x. Usually in this context the fm(cid:1)x(cid:2) are taken to be simple functions
characterized by a set of parameters  and a multiplier m,

The additive model then becomes

(3)

(4)

fm(cid:1)x(cid:2) = mb(cid:1)x(cid:18) m(cid:2)(cid:4)
FM(cid:1)x(cid:2) = M(cid:2)

mb(cid:1)x(cid:18) m(cid:2)(cid:4)

m=1

For example, in single hidden layer neural networks b(cid:1)x(cid:18) (cid:2) = (cid:1)tx(cid:2) where
(cid:1)(cid:2) is a sigmoid function and  parameterizes a linear combination of the
input features. In signal processing, wavelets are a popular choice with 
parameterizing the location and scale shifts of a mother wavelet b(cid:1)x(cid:2).
In these applications (cid:6)b(cid:1)x(cid:18) m(cid:2)(cid:7)M
1 are generally called basis functions since
they span a function subspace.

If least-squares is used as a tting criterion, one can solve for an optimal
set of parameters through a generalized backtting algorithm with updates,

(cid:3)

y  (cid:2)

k(cid:9)=m

(cid:5)2

(5)

(cid:6)m(cid:1) m(cid:7)  arg min

(cid:1) 

E

kb(cid:1)x(cid:18) k(cid:2)  b(cid:1)x(cid:18) (cid:2)

for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M in cycles until convergence. Alternatively, one can use a
greedy forward stepwise approach,
(cid:6)m(cid:1) m(cid:7)  arg min

y  Fm1(cid:1)x(cid:2)  b(cid:1)x(cid:18) (cid:2)(cid:7)2

(6)

(cid:6)

E

(cid:1) 

1

for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M, where (cid:6)k(cid:1) k(cid:7)m1
are xed at their corresponding solu-
tion values at earlier iterations. This is the approach used by Mallat and
Zhang (1993) in matching pursuit, where the b(cid:1)x(cid:18) (cid:2) are selected from an
over-complete dictionary of wavelet bases. In the language of boosting, f(cid:1)x(cid:2) =
b(cid:1)x(cid:18) (cid:2) would be called a weak learner and FM(cid:1)x(cid:2) (4) the committee. If
decision trees were used as the weak learner, the parameters  would repre-
sent the splitting variables, split points, the constants in each terminal node
and number of terminal nodes of each tree.

Note that the backtting procedure (5) or its greedy cousin (6) only require
an algorithm for tting a single weak learner (3) to data. This base algorithm
is simply applied repeatedly to modied versions of the original data

ym  y  (cid:2)

k(cid:9)=m

fk(cid:1)x(cid:2)(cid:4)

344

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

In the forward stepwise procedure (6), the modied output ym at the mth iter-
ation depends only on its value ym1 and the solution fm1(cid:1)x(cid:2) at the previous
iteration,

ym = ym1  fm1(cid:1)x(cid:2)(cid:4)

(7)
At each step m, the previous output values ym1 are modied (7) so that
the previous model fm1(cid:1)x(cid:2) has no explanatory power on the new outputs
ym. One can therefore view this as a procedure for boosting a weak learner
f(cid:1)x(cid:2) = b(cid:1)x(cid:18) (cid:2) to form a powerful committee FM(cid:1)x(cid:2) (4).

3.3. Classication problems. For the classication problem, we learn from
Bayes theorem that all we need is P(cid:1)y = j(cid:15)x(cid:2), the posterior or conditional class
probabilities. One could transfer all the above regression machinery across
to the classication domain by simply noting that E(cid:1)1(cid:8)y=j(cid:10)(cid:15)x(cid:2) = P(cid:1)y = j(cid:15)x(cid:2),
where 1(cid:8)y=j(cid:10) is the 0/1 indicator variable representing class j. While this works
fairly well in general, several problems have been noted [Hastie, Tibshirani
and Buja (1994)] for constrained regression methods. The estimates are typ-
ically not conned to (cid:8)0(cid:1) 1(cid:10), and severe masking problems can occur when
there are more than two classes. A notable exception is when trees are used
as the regression method, and in fact this is the approach used by Breiman,
Friedman, Olshen and Stone (1984).

Logistic regression is a popular approach used in statistics for overcom-
ing these problems. For a two-class problem, an additive logistic model has
the form

(8)

log

P(cid:1)y = 1(cid:15)x(cid:2)

P(cid:1)y = 1(cid:15)x(cid:2) = M(cid:2)

m=1

fm(cid:1)x(cid:2)(cid:4)

of F(cid:1)x(cid:2) = (cid:1)M

The monotone logit transformation on the left guarantees that for any values
m=1 fm(cid:1)x(cid:2)  R, the probability estimates lie in (cid:8)0(cid:1) 1(cid:10); inverting

we get

(9)

p(cid:1)x(cid:2) = P(cid:1)y = 1(cid:15)x(cid:2) = eF(cid:1)x(cid:2)

1 + eF(cid:1)x(cid:2) (cid:4)

Here we have given a general additive form for F(cid:1)x(cid:2); special cases exist that
are well known in statistics. In particular, linear logistic regression [McCullagh
and Nelder (1989), e.g.] and additive logistic regression [Hastie and Tibshirani
(1990)] are popular. These models are usually t by maximizing the binomial
log-likelihood and enjoy all the associated asymptotic optimality features of
maximum likelihood estimation.

A generalized version of backtting (2), called Local Scoring in Hastie and
Tibshirani (1990), can be used to t the additive logistic model by maximum
fk(cid:1)xk(cid:2) and p(cid:1)x(cid:2)

likelihood. Starting with guesses f1(cid:1)x1(cid:2) fp(cid:1)xp(cid:2), F(cid:1)x(cid:2) = (cid:1)

dened in (9), we form the working response:

(10)

z = F(cid:1)x(cid:2) + 1(cid:8)y=1(cid:10)  p(cid:1)x(cid:2)
p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2) (cid:4)

ADDITIVE LOGISTIC REGRESSION

345
We then apply backtting to the response z with observation weights p(cid:1)x(cid:2)(cid:1)1
p(cid:1)x(cid:2)(cid:2) to obtain new fk(cid:1)xk(cid:2). This process is repeated until convergence. The
forward stagewise version (6) of this procedure bears a close similarity to the
LogitBoost algorithm described later in the paper.

4. AdaBoost: an additive logistic regression model.

In this section
we show that the AdaBoost algorithms (Discrete and Real) can be interpreted
as stagewise estimation procedures for tting an additive logistic regression
model. They optimize an exponential criterion which to second order is equiva-
lent to the binomial log-likelihood criterion. We then propose a more standard
likelihood-based boosting procedure.

4.1. An exponential criterion. Consider minimizing the criterion

J(cid:1)F(cid:2) = E(cid:1)eyF(cid:1)x(cid:2)(cid:2)

(11)
for estimation of F(cid:1)x(cid:2). Here E represents expectation; depending on the con-
text, this may be a population expectation (with respect to a probability dis-
tribution) or else a sample average. Ew indicates a weighted expectation.
Lemma 1 shows that the function F(cid:1)x(cid:2) that minimizes J(cid:1)F(cid:2) is the symmetric
logistic transform of P(cid:1)y = 1(cid:15)x(cid:2).

Lemma 1. E(cid:1)eyF(cid:1)x(cid:2)(cid:2) is minimized at

(12)

Hence

(13)

(14)

F(cid:1)x(cid:2) = 1
2

log

P(cid:1)y = 1(cid:15)x(cid:2)
P(cid:1)y = 1(cid:15)x(cid:2) (cid:4)

P(cid:1)y = 1(cid:15)x(cid:2) =

P(cid:1)y = 1(cid:15)x(cid:2) =

eF(cid:1)x(cid:2)

eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2) (cid:1)

eF(cid:1)x(cid:2)

eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2) (cid:4)

Proof. While E entails expectation over the joint distribution of y and x,

it is sufcient to minimize the criterion conditional on x:

(cid:8)
(cid:9) = P(cid:1)y = 1(cid:15)x(cid:2)eF(cid:1)x(cid:2) + P(cid:1)y = 1(cid:15)x(cid:2)eF(cid:1)x(cid:2)(cid:1)
eyF(cid:1)x(cid:2)(cid:15)x
(cid:8)
(cid:9)
eyF(cid:1)x(cid:2)(cid:15)x
= P(cid:1)y = 1(cid:15)x(cid:2)eF(cid:1)x(cid:2) + P(cid:1)y = 1(cid:15)x(cid:2)eF(cid:1)x(cid:2)(cid:4)
F(cid:1)x(cid:2)

E

E

The result follows by setting the derivative to zero. 

This exponential criterion appeared in Schapire and Singer (1998), moti-
vated as an upper bound on misclassication error. Breiman (1997) also used
this criterion in his results on AdaBoost and prediction games. The usual

346

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

logistic transform does not have the factor 1
2 as in (12); by multiplying the
numerator and denominator in (13) by eF(cid:1)x(cid:2), we get the usual logistic model

(15)

p(cid:1)x(cid:2) = e2F(cid:1)x(cid:2)

1 + e2F(cid:1)x(cid:2) (cid:4)

Hence the two models are equivalent up to a factor 2.

Corollary 1.

If E is replaced by averages over regions of x where F(cid:1)x(cid:2) is
constant (as in the terminal node of a decision tree), the same result applies to
the sample proportions of y = 1 and y = 1.

Results 1 and 2 show that both Discrete and Real AdaBoost, as well as the
Generalized AdaBoost of Freund and Schapire (1996b), can be motivated as
iterative algorithms for optimizing the (population based) exponential crite-
rion. The results share the same format.
1. Given an imperfect F(cid:1)x(cid:2), an update F(cid:1)x(cid:2) + f(cid:1)x(cid:2) is proposed based on the

population version of the criterion.

2. The update, which involves population conditional expectations, is imper-
fectly approximated for nite data sets by some restricted class of estima-
tors, such as averages in terminal nodes of trees.

Hastie and Tibshirani (1990) use a similar derivation of the local scoring
algorithm used in tting generalized additive models. Many terms are typi-
cally required in practice, since at each stage the approximation to conditional
expectation is rather crude. Because of Lemma 1, the resulting algorithms
can be interpreted as a stagewise estimation procedure for tting an additive
logistic regression model. The derivations are sufciently different to warrant
separate treatment.

Result 1. The Discrete AdaBoost algorithm (population version) builds
an additive logistic regression model via Newton-like updates for minimizing
E(cid:1)eyF(cid:1)x(cid:2)(cid:2).

Proof. Let J(cid:1)F(cid:2) = E(cid:8)eyF(cid:1)x(cid:2)(cid:10). Suppose we have a current estimate F(cid:1)x(cid:2)
and seek an improved estimate F(cid:1)x(cid:2) + cf(cid:1)x(cid:2). For xed c (and x), we expand
J(cid:1)F(cid:1)x(cid:2) + cf(cid:1)x(cid:2)(cid:2) to second order about f(cid:1)x(cid:2) = 0,

J(cid:1)F + cf(cid:2) = E(cid:8)ey(cid:1)F(cid:1)x(cid:2)+cf(cid:1)x(cid:2)(cid:2)(cid:10)

 E(cid:8)eyF(cid:1)x(cid:2)(cid:1)1  ycf(cid:1)x(cid:2) + c2y2f(cid:1)x(cid:2)2/2(cid:2)(cid:10)
= E(cid:8)eyF(cid:1)x(cid:2)(cid:1)1  ycf(cid:1)x(cid:2) + c2/2(cid:2)(cid:10)(cid:1)

since y2 = 1 and f(cid:1)x(cid:2)2 = 1. Minimizing pointwise with respect to f(cid:1)x(cid:2)
(cid:6)1(cid:1) 1(cid:7), we write
(16)

Ew(cid:1)1  ycf(cid:1)x(cid:2) + c2/2(cid:15)x(cid:2)(cid:4)

f(cid:1)x(cid:2) = arg min

f

ADDITIVE LOGISTIC REGRESSION

347
Here the notation Ew(cid:1)(cid:15)x(cid:2) refers to a weighted conditional expectation, where
w = w(cid:1)x(cid:1) y(cid:2) = eyF(cid:1)x(cid:2), and

Ew(cid:8)g(cid:1)x(cid:1) y(cid:2)(cid:15)x(cid:10) def= E(cid:8)w(cid:1)x(cid:1) y(cid:2)g(cid:1)x(cid:1) y(cid:2)(cid:15)x(cid:10)

E(cid:8)w(cid:1)x(cid:1) y(cid:2)(cid:15)x(cid:10)
For c > 0, minimizing (16) is equivalent to maximizing

(cid:4)

(17)

Ew(cid:8)yf(cid:1)x(cid:2)(cid:10)(cid:4)

The solution is
f(cid:1)x(cid:2) =

(18)

(cid:10)

1(cid:1)
1(cid:1)

if Ew(cid:1)y(cid:15)x(cid:2) = Pw(cid:1)y = 1(cid:15)x(cid:2)  Pw(cid:1)y = 1(cid:15)x(cid:2) > 0,
otherwise.

Note that

Ew(cid:8)yf(cid:1)x(cid:2)(cid:10) = Ew(cid:8)y  f(cid:1)x(cid:2)(cid:10)2/2  1

(19)
[again using f(cid:1)x(cid:2)2 = y2 = 1]. Thus minimizing a quadratic approximation to
the criterion leads to a weighted least-squares choice of f(cid:1)x(cid:2)  (cid:6)1(cid:1) 1(cid:7), and
this constitutes the Newton-like step.
Given f(cid:1)x(cid:2)  (cid:6)1(cid:1) 1(cid:7), we can directly minimize J(cid:1)F + cf(cid:2) to determine c:

(20)

Ewecyf(cid:1)x(cid:2)

c = arg min
= 1
2

log

c

1  err

(cid:1)

err

where err = Ew(cid:8)1(cid:8)y(cid:9)=f(cid:1)x(cid:2)(cid:10)(cid:10). Note that c can be negative if the weak learner
does worse than 50%, in which case it automatically reverses the polarity.
Combining these steps we get the update for F(cid:1)x(cid:2),
1  err

F(cid:1)x(cid:2)  F(cid:1)x(cid:2) + 1
2

log

err

f(cid:1)x(cid:2)(cid:4)

In the next iteration the new contribution cf(cid:1)x(cid:2) to F(cid:1)x(cid:2) augments the weights

Since yf(cid:1)x(cid:2) = 2  1(cid:8)y(cid:9)=f(cid:1)x(cid:2)(cid:10)  1, we see that the update is equivalent to

w(cid:1)x(cid:1) y(cid:2)  w(cid:1)x(cid:1) y(cid:2)ecf(cid:1)x(cid:2)y(cid:4)
(cid:13)
(cid:12) 1  err

(cid:11)
w(cid:1)x(cid:1) y(cid:2)  w(cid:1)x(cid:1) y(cid:2)exp

log

(cid:14)

1(cid:8)y(cid:9)=f(cid:1)x(cid:2)(cid:10)

(cid:4)

err

Thus the function and weight updates are of an identical form to those used

in Discrete AdaBoost.

This population version of AdaBoost translates naturally to a data version
using trees. The weighted conditional expectation in (18) is approximated by
the terminal-node weighted averages in a tree. In particular, the weighted
least-squares criterion is used to grow the tree-based classier f(cid:1)x(cid:2), and given
f(cid:1)x(cid:2), the constant c is based on the weighted training error.

348

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Note that after each Newton step, the weights change, and hence the tree
conguration will change as well. This adds an adaptive twist to the data
version of a Newton-like algorithm.

Parts of this derivation for AdaBoost can be found in Breiman (1997) and
Schapire and Singer (1998), but without making the connection to additive
logistic regression models.

Corollary 2. After each update to the weights, the weighted misclassi-

cation error of the most recent weak learner is 50%.

Proof. This follows by noting that the c that minimizes J(cid:1)F+ cf(cid:2) satises

J(cid:1)F + cf(cid:2)

(21)
The result follows since yf(cid:1)x(cid:2) is 1 for a correct and 1 for an incorrect
classication. 

= E(cid:8)ey(cid:1)F(cid:1)x(cid:2)+cf(cid:1)x(cid:2)(cid:2)yf(cid:1)x(cid:2)(cid:10) = 0(cid:4)

c

Schapire and Singer (1998) give the interpretation that the weights are
updated to make the new weighted problem maximally difcult for the next
weak learner.
The Discrete AdaBoost algorithm expects the tree or other weak learner
to deliver a classier f(cid:1)x(cid:2)  (cid:6)1(cid:1) 1(cid:7). Result 1 requires minor modications to
accommodate f(cid:1)x(cid:2)  R, as in the generalized AdaBoost algorithms [Freund
and Schapire (1996b), Schapire and Singer (1998)]; the estimate for cm differs.
Fixing f, we see that the minimizer of (20) must satisfy
(22)
If f is not discrete, this equation has no closed-form solution for c, and requires
an iterative solution such as NewtonRaphson.

Ew(cid:8)yf(cid:1)x(cid:2)ecyf(cid:1)x(cid:2)(cid:10) = 0(cid:4)

We now derive the Real AdaBoost algorithm, which uses weighted proba-
bility estimates to update the additive logistic model, rather than the classi-
cations themselves. Again we derive the population updates and then apply it
to data by approximating conditional expectations by terminal-node averages
in trees.

Result 2. The Real AdaBoost algorithm ts an additive logistic regression

model by stagewise and approximate optimization of J(cid:1)F(cid:2) = E(cid:8)eyF(cid:1)x(cid:2)(cid:10).

Proof. Suppose we have a current estimate F(cid:1)x(cid:2) and seek an improved esti-

mate F(cid:1)x(cid:2) + f(cid:1)x(cid:2) by minimizing J(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2) at each x.

J(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2) = E(cid:1)eyF(cid:1)x(cid:2)eyf(cid:1)x(cid:2)(cid:15)x(cid:2)

= ef(cid:1)x(cid:2)E(cid:8)eyF(cid:1)x(cid:2)1(cid:8)y=1(cid:10)(cid:15)x(cid:10) + ef(cid:1)x(cid:2)E(cid:8)eyF(cid:1)x(cid:2)1(cid:8)y=1(cid:10)(cid:15)x(cid:10)(cid:4)

Dividing through by E(cid:8)eyF(cid:1)x(cid:2)(cid:15)x(cid:10) and setting the derivative w.r.t. f(cid:1)x(cid:2) to zero
we get

(23)

f(cid:1)x(cid:2) = 1
2

log

Ew(cid:8)1(cid:8)y=1(cid:10)(cid:15)x(cid:10)
Ew(cid:8)1(cid:8)y=1(cid:10)(cid:15)x(cid:10)

ADDITIVE LOGISTIC REGRESSION

349

(24)
where w(cid:1)x(cid:1) y(cid:2) = exp(cid:1)yF(cid:1)x(cid:2)(cid:2). The weights get updated by

log

= 1
2

Pw(cid:1)y = 1(cid:15)x(cid:2)
Pw(cid:1)y = 1(cid:15)x(cid:2) (cid:1)

w(cid:1)x(cid:1) y(cid:2)  w(cid:1)x(cid:1) y(cid:2)eyf(cid:1)x(cid:2)(cid:4)

The algorithm as presented would stop after one iteration. In practice we
use crude approximations to conditional expectation, such as decision trees or
other constrained models, and hence many steps are required.

Corollary 3. At the optimal F(cid:1)x(cid:2)(cid:1) the weighted conditional mean of y is 0.

Proof.

(cid:1)25(cid:2)

If F(cid:1)x(cid:2) is optimal, we have

J(cid:1)F(cid:1)x(cid:2)(cid:2)
F(cid:1)x(cid:2) = EeyF(cid:1)x(cid:2)y = 0(cid:4)



We can think of the weights as providing an alternative to residuals for the
binary classication problem. At the optimal function F, there is no further
information about F in the weighted conditional distribution of y. If there is,
we use it to update F.

An iteration M in either the Discrete or Real AdaBoost algorithms, we have

composed an additive function of the form

(26)

F(cid:1)x(cid:2) = M(cid:2)

m=1

fm(cid:1)x(cid:2)(cid:1)

where each of the components are found in a greedy forward stagewise fash-
ion, xing the earlier components. Our term stagewise refers to a similar
approach in statistics:
1. Variables are included sequentially in a stepwise regression.
2. The coefcients of variables already included receive no further adjustment.

4.2. Why EeyF(cid:1)x(cid:2)? So far the only justication for this exponential crite-
rion is that it has a sensible population minimizer, and the algorithm described
above performs well on real data. In addition:
1. Schapire and Singer (1998) motivate eyF(cid:1)x(cid:2) as a differentiable upper bound

to misclassication error 1(cid:8)yF<0(cid:10) (see Figure 2).

2. The AdaBoost algorithm that it generates is extremely modular, requir-
ing at each iteration the retraining of a classier on a weighted training
database.
Let y = (cid:1)y + 1(cid:2)/2, taking values 0, 1, and parametrize the binomial prob-

abilities by

p(cid:1)x(cid:2) =

eF(cid:1)x(cid:2)

eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2) (cid:4)

The binomial log-likelihood is

l(cid:1)y(cid:1) p(cid:1)x(cid:2)(cid:2) = y log(cid:1)p(cid:1)x(cid:2)(cid:2) + (cid:1)1  y(cid:2)log(cid:1)1  p(cid:1)x(cid:2)(cid:2)

(27)

=  log(cid:1)1 + e2yF(cid:1)x(cid:2)(cid:2)(cid:4)

350

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Fig. 2. A variety of loss functions for estimating a function F(cid:1)x(cid:2) for classication. The horizontal
axis is yF(cid:1) which is negative for errors and positive for correct classications. All the loss functions
are monotone in yF(cid:1) and are centered and scaled to match eyF at F = 0. The curve labeled Log-
likelihood is the binomial log-likelihood or cross-entropy y log p + (cid:1)1  y(cid:2)log(cid:1)1  p(cid:2). The curve
labeled Squared Error(p) is (cid:1)y  p(cid:2)2. The curve labeled Squared Error(F) is (cid:1)y  F(cid:2)2 and
increases once yF exceeds 1(cid:1) thereby increasingly penalizing classications that are too correct.

Hence we see that:

3. The population minimizers of El(cid:1)y(cid:1) p(cid:1)x(cid:2)(cid:2) and EeyF(cid:1)x(cid:2) coincide. This is
easily seen because the expected log-likelihood is maximized at the true
probabilities p(cid:1)x(cid:2) = P(cid:1)y = 1(cid:15)x(cid:2), which dene the logit F(cid:1)x(cid:2). By Lemma 1
we see that this is exactly the minimizer of EeyF(cid:1)x(cid:2). In fact, the exponential
criterion and the (negative) log-likelihood are equivalent to second order in
a Taylor series around F = 0,

(28)

 l(cid:1)y(cid:1) p(cid:2)  exp(cid:1)yF(cid:2) + log(cid:1)2(cid:2)  1(cid:4)

Graphs of exp(cid:1)yF(cid:2) and log(cid:1)1 + e2yF(cid:1)x(cid:2)(cid:2) are shown in Figure 2, as a
function of yFpositive values of yF imply correct classication. Note
that  exp(cid:1)yF(cid:2) itself is not a proper log-likelihood, as it does not equal
the log of any probability mass function on plus or minus 1.

4. There is another way to view the criterion J(cid:1)F(cid:2). It is easy to show that

(29)

eyF(cid:1)x(cid:2) =

(cid:15)

(cid:15)y  p(cid:1)x(cid:2)(cid:15)
p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2) (cid:1)

2 log(cid:1)p(cid:1)x(cid:2)/(cid:1)1  p(cid:1)x(cid:2)(cid:2)(cid:2). The right-hand side is known as the
with F(cid:1)x(cid:2) = 1
 statistic in the statistical literature. 2 is a quadratic approximation to
the log-likelihood, and so  can be considered a gentler alternative.

ADDITIVE LOGISTIC REGRESSION

351

1

One feature of both the exponential and log-likelihood criteria is that they
are monotone and smooth. Even if the training error is zero, the criteria will
drive the estimates towards purer solutions (in terms of probability estimates).
If Fm1(cid:1)x(cid:2) = (cid:1)m1
Why not estimate the fm by minimizing the squared error E(cid:1)y  F(cid:1)x(cid:2)(cid:2)2?
fj(cid:1)x(cid:2) is the current prediction, this leads to a forward
stagewise procedure that does an unweighted t to the response y  Fm1(cid:1)x(cid:2)
at step m as in (6). Empirically we have found that this approach works quite
well, but is dominated by those that use monotone loss criteria. We believe
that the nonmonotonicity of squared error loss (Figure 2) is the reason. Correct
classications, but with yF(cid:1)x(cid:2) > 1, incur increasing loss for increasing values
of (cid:15)F(cid:1)x(cid:2)(cid:15). This makes squared-error loss an especially poor approximation to
misclassication error rate. Classications that are too correct are penalized
as much as misclassication errors.

4.3. Direct optimization of the binomial log-likelihood.

In this section we
explore algorithms for tting additive logistic regression models by stagewise
optimization of the Bernoulli log-likelihood. Here we focus again on the two-
class case and will use a 0/1 response y to represent the outcome. We repre-
sent the probability of y = 1 by p(cid:1)x(cid:2), where

(30)

p(cid:1)x(cid:2) =

eF(cid:1)x(cid:2)

eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2) (cid:4)

Algorithm 3 gives the details.

LogitBoost (two classes)

1. Start with weights wi = 1/N i= 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N, F(cid:1)x(cid:2)= 0 and probability esti-
2. Repeat for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M:

mates p(cid:1)xi(cid:2) = 1
2 .

(a) Compute the working response and weights
i  p(cid:1)xi(cid:2)
y

zi =
p(cid:1)xi(cid:2)(cid:1)1  p(cid:1)xi(cid:2)(cid:2) (cid:1)
wi = p(cid:1)xi(cid:2)(cid:1)1  p(cid:1)xi(cid:2)(cid:2)(cid:4)

xi using weights wi.

(b) Fit the function fm(cid:1)x(cid:2) by a weighted least-squares regression of zi to
(c) Update F(cid:1)x(cid:2)  F(cid:1)x(cid:2) + 1
3. Output the classier sign(cid:8)F(cid:1)x(cid:2)(cid:10) = sign(cid:8)(cid:1)M

2 fm(cid:1)x(cid:2) and p(cid:1)x(cid:2)  (cid:1)eF(cid:1)x(cid:2)(cid:2)/(cid:1)eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2)(cid:2).

m=1 fm(cid:1)x(cid:2)(cid:10).

Algorithm 3. An adaptive Newton algorithm for tting an additive logis-

tic regression model.

352

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Result 3. The LogitBoost algorithm (two classes, population version) uses
Newton steps for tting an additive symmetric logistic model by maximum
likelihood.

Derivation. Consider the update F(cid:1)x(cid:2) + f(cid:1)x(cid:2) and the expected log-

likelihood
(31)
Conditioning on x, we compute the rst and second derivative at f(cid:1)x(cid:2) = 0,

2y(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)  log(cid:8)1 + e2(cid:1)F(cid:1)x(cid:2)+f(cid:1)x(cid:2)(cid:2)(cid:10)(cid:4)

El(cid:1)F + f(cid:2) = E

(cid:6)

(32)

(33)

(34)

f(cid:1)x(cid:2)

s(cid:1)x(cid:2) = El(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)
= 2E(cid:1)y  p(cid:1)x(cid:2)(cid:15)x(cid:2)(cid:1)
H(cid:1)x(cid:2) = 2El(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)

f(cid:1)x(cid:2)2

(cid:4)(cid:4)(cid:4)(cid:4)

(cid:4)(cid:4)(cid:4)(cid:4)

f(cid:1)x(cid:2)=0

f(cid:1)x(cid:2)=0
= 4E(cid:1)p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2)(cid:15)x(cid:2)(cid:1)

F(cid:1)x(cid:2)  F(cid:1)x(cid:2)  H(cid:1)x(cid:2)1s(cid:1)x(cid:2)

= F(cid:1)x(cid:2) + 1
2
= F(cid:1)x(cid:2) + 1
2

E(cid:1)p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2)(cid:15)x(cid:2)

E(cid:1)y  p(cid:1)x(cid:2)(cid:15)x(cid:2)
(cid:11)

y  p(cid:1)x(cid:2)

(cid:14)

(cid:4)(cid:4)(cid:4)(cid:4)x

where p(cid:1)x(cid:2) is dened in terms of F(cid:1)x(cid:2). The Newton update is then

(35)
where w(cid:1)x(cid:2) = p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2). Equivalently, the Newton update f(cid:1)x(cid:2) solves
the weighted least-squares approximation [about F(cid:1)x(cid:2)] to the log-likelihood

p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2)

Ew

(cid:1)

(36)

min
f(cid:1)x(cid:2) Ew(cid:1)x(cid:2)

F(cid:1)x(cid:2) + 1
2

y  p(cid:1)x(cid:2)
p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2)  (cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)

(cid:14)2

(cid:4)

(cid:11)

The population algorithm described here translates immediately to an
implementation on data when E(cid:1)(cid:15)x(cid:2) is replaced by a regression method, such
as regression trees [Breiman, Friedman, Olshen and Stone (1984)]. While the
role of the weights are somewhat articial in the population case, they are
not in any implementation; w(cid:1)x(cid:2) is constant when conditioned on x, but the
w(cid:1)xi(cid:2) in a terminal node of a tree, for example, depend on the current values
F(cid:1)xi(cid:2), and will typically not be constant.
Sometimes the w(cid:1)x(cid:2) get very small in regions of (x) perceived [by F(cid:1)x(cid:2)] to be
purethat is, when p(cid:1)x(cid:2) is close to 0 or 1. This can cause numerical problems
in the construction of z, and lead to the following crucial implementation
protections:
1. If y = 1, then compute z = (cid:1)(cid:1)y  p(cid:2)/p(cid:1)1  p(cid:2)(cid:2) as 1/p. Since this number
can get large if p is small, threshold this ratio at z max. The particu-
lar value chosen for z max is not crucial; we have found empirically that

ADDITIVE LOGISTIC REGRESSION

353
z max  (cid:8)2(cid:1) 4(cid:10) works well. Likewise, if y = 0, compute z = 1/(cid:1)1 p(cid:2) with
a lower threshold of z max.
2. Enforce a lower threshold on the weights: w = max(cid:1)w(cid:1) 2  machine-zero(cid:2).

4.4. Optimizing EeyF(cid:1)x(cid:2) by Newton stepping. The population version of
the Real AdaBoost procedure (Algorithm 2) optimizes E exp(cid:1)y(cid:1)F(cid:1)x(cid:2)+ f(cid:1)x(cid:2)(cid:2)(cid:2)
exactly with respect to f at each iteration. In Algorithm 4 we propose the
Gentle AdaBoost procedure that instead takes adaptive Newton steps much
like the LogitBoost algorithm just described.

Result 4. The Gentle AdaBoost algorithm (population version) uses

Newton steps for minimizing EeyF(cid:1)x(cid:2).

Derivation.

J(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)

f(cid:1)x(cid:2)

2J(cid:1)F(cid:1)x(cid:2) + f(cid:1)x(cid:2)(cid:2)

f(cid:1)x(cid:2)2

Hence the Newton update is

(cid:4)(cid:4)(cid:4)(cid:4)
(cid:4)(cid:4)(cid:4)(cid:4)

f(cid:1)x(cid:2)=0

f(cid:1)x(cid:2)=0

= E(cid:1)eyF(cid:1)x(cid:2)y(cid:15)x(cid:2)(cid:1)

= E(cid:1)eyF(cid:1)x(cid:2)(cid:15)x(cid:2) since y2 = 1(cid:4)

F(cid:1)x(cid:2)  F(cid:1)x(cid:2) + E(cid:1)eyF(cid:1)x(cid:2)y(cid:15)x(cid:2)
E(cid:1)eyF(cid:1)x(cid:2)(cid:15)x(cid:2)

= F(cid:1)x(cid:2) + Ew(cid:1)y(cid:15)x(cid:2)(cid:1)

where w(cid:1)x(cid:1) y(cid:2) = eyF(cid:1)x(cid:2).

The main difference between this and the Real AdaBoost algorithm is how
it uses its estimates of the weighted class probabilities to update the functions.
Here the update is fm(cid:1)x(cid:2) = Pw(cid:1)y = 1(cid:15)x(cid:2) Pw(cid:1)y = 1(cid:15)x(cid:2), rather than half the

1. Start with weights wi = 1/N(cid:1) i = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N(cid:1) F(cid:1)x(cid:2) = 0(cid:4)
2. Repeat for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M:

Gentle AdaBoost

with weights wi.

(a) Fit the regression function fm(cid:1)x(cid:2) by weighted least-squares of yi to xi
(b) Update F(cid:1)x(cid:2)  F(cid:1)x(cid:2) + fm(cid:1)x(cid:2).
(c) Update wi  wi exp(cid:1)yifm(cid:1)xi(cid:2)(cid:2) and renormalize.
3. Output the classier sign(cid:8)F(cid:1)x(cid:2)(cid:10) = sign(cid:8)(cid:1)M
m=1 fm(cid:1)x(cid:2)(cid:10).

Algorithm 4. A modied version of the Real AdaBoost algorithm, using

Newton stepping rather than exact optimization at each step.

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

354
log-ratio as in (24): fm(cid:1)x(cid:2) = 1
2 log(cid:1)Pw(cid:1)y = 1(cid:15)x(cid:2)(cid:2)/(cid:1)Pw(cid:1)y = 1(cid:15)x(cid:2)(cid:2). Log-ratios
can be numerically unstable, leading to very large updates in pure regions,
while the update here lies in the range (cid:8)1(cid:1) 1(cid:10). Empirical evidence suggests
(see Section 7) that this more conservative algorithm has similar performance
to both the Real AdaBoost and LogitBoost algorithms, and often outperforms
them both, especially when stability is an issue.
There is a strong similarity between the updates for the Gentle AdaBoost
algorithm and those for the LogitBoost algorithm. Let P = P(cid:1)y = 1(cid:15)x(cid:2), and
p(cid:1)x(cid:2) = eF(cid:1)x(cid:2)/(cid:1)eF(cid:1)x(cid:2) + eF(cid:1)x(cid:2)(cid:2). Then

(37)

E(cid:1)eyF(cid:1)x(cid:2)y(cid:15)x(cid:2)
E(cid:1)eyF(cid:1)x(cid:2)(cid:15)x(cid:2) = eF(cid:1)x(cid:2)P  eF(cid:1)x(cid:2)(cid:1)1  P(cid:2)
eF(cid:1)x(cid:2)P + eF(cid:1)x(cid:2)(cid:1)1  P(cid:2)

P  p(cid:1)x(cid:2)

=

(cid:1)1  p(cid:1)x(cid:2)(cid:2)P + p(cid:1)x(cid:2)(cid:1)1  P(cid:2) (cid:4)

The analogous expression for LogitBoost from (34) is

(38)

P  p(cid:1)x(cid:2)

p(cid:1)x(cid:2)(cid:1)1  p(cid:1)x(cid:2)(cid:2) (cid:4)

1
2

2 these are nearly the same, but they differ as the p(cid:1)x(cid:2) become
At p(cid:1)x(cid:2)  1
extreme. For example, if P  1 and p(cid:1)x(cid:2)  0, (38) blows up, while (37) is
about 1 (and always falls in (cid:8)1(cid:1) 1(cid:10)).

5. Multiclass procedures. Here we explore extensions of boosting to
classication with multiple classes. We start off by proposing a natural gen-
eralization of the two-class symmetric logistic transformation, and then con-
sider specic algorithms. In this context Schapire and Singer (1998) dene
J responses yj for a J class problem, each taking values in (cid:6)1(cid:1) 1(cid:7). Simi-
larly the indicator response vector with elements y
j is more standard in the
statistics literature. Assume the classes are mutually exclusive.

Definition 1. For a J class problem let pj(cid:1)x(cid:2) = P(cid:1)yj = 1(cid:15)x(cid:2). We dene

the symmetric multiple logistic transformation

(39)

Equivalently,

(40)

Fj(cid:1)x(cid:2) = log pj(cid:1)x(cid:2)  1

J

log pk(cid:1)x(cid:2)(cid:4)

J(cid:2)
k=1

pj(cid:1)x(cid:2) =

eFj(cid:1)x(cid:2)
(cid:1)J
k=1 eFk(cid:1)x(cid:2) (cid:1)

J(cid:2)
k=1

Fk(cid:1)x(cid:2) = 0(cid:4)

The centering condition in (40) is for numerical stability only; it simply pins
the Fj down, else we could add an arbitrary constant to each Fj and the
probabilities remain the same. The equivalence of these two denitions is
easily established, as well as the equivalence with the two-class case.

ADDITIVE LOGISTIC REGRESSION

355

Schapire and Singer (1998) provide several generalizations of AdaBoost for
the multiclass case, and also refer to other proposals [Freund and Schapire
(1997), Schapire (1997)]; we describe their AdaBoost.MH algorithm (see Algo-
rithm 5), since it seemed to dominate the others in their empirical studies. We
then connect it to the models presented here. We will refer to the augmented
variable in Algorithm 5 as the class variable C. We make a few observations:
(cid:1)J
j=1 EeyjFj(cid:1)x(cid:2), which
is equivalent to running separate population boosting algorithms on each
of the J problems of size N obtained by partitioning the N  J samples
in the obvious fashion. This is seen trivially by rst conditioning on C = j,
and then x(cid:15)C = j, when computing conditional expectations.

1. The population version of this algorithm minimizes

2. The same is almost true for a tree-based algorithm. We see this because:
(a) If the rst split is on C, either a J-nary split if permitted, or else J 1
binary splits, then the subtrees are identical to separate trees grown to
each of the J groups. This will always be the case for the rst tree.
(b) If a tree does not split on C anywhere on the path to a terminal node,
then that node returns a function fm(cid:1)x(cid:1) j(cid:2) = gm(cid:1)x(cid:2) that contributes
nothing to the classication decision. However, as long as a tree includes
a split on C at least once on every path to a terminal node, it will make
a contribution to the classier for all input feature values.

The advantage or disadvantage of building one large tree using class label
as an additional input feature is not clear. No motivation is provided.
(cid:1)J
We therefore implement AdaBoost.MH using the more traditional direct
j=1 E exp(cid:1)yjFj(cid:1)x(cid:2)(cid:2)
approach of building J separate trees to minimize

We have thus shown

Result 5. The AdaBoost.MH algorithm for a J-class problem ts J uncou-

pled additive logistic models, Gj(cid:1)x(cid:2) = 1
against the rest.

2 log pj(cid:1)x(cid:2)/(cid:1)1  pj(cid:1)x(cid:2)(cid:2), each class

AdaBoost.MH [Schapire and Singer (1998)]

1. Expand the original N observations into N J pairs (cid:1)(cid:1)xi(cid:1) 1(cid:2)(cid:1) yi1(cid:2),
is the (cid:6)1(cid:1) 1(cid:7)
(cid:1)(cid:1)xi(cid:1) 2(cid:2)(cid:1) yi2(cid:2)(cid:1) (cid:4) (cid:4) (cid:4) (cid:1)(cid:1)(cid:1)xi(cid:1) J(cid:2)(cid:1) yiJ(cid:2)(cid:1)
response for class j and observation i.
F(cid:12) (cid:1)  (cid:1)1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J(cid:2) (cid:13) R(cid:18) F(cid:1)x(cid:1) j(cid:2) = (cid:1)
2. Apply Real AdaBoost to the augmented dataset, producing a function
3. Output the classier arg maxj F(cid:1)x(cid:1) j(cid:2).

i= 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N. Here yij

m fm(cid:1)x(cid:1) j(cid:2).

Algorithm 5. The AdaBoost.MH algorithm converts the J class problem
into that of estimating a two class classier on a training set J times as large,
with an additional feature dened by the set of class labels.

356

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

In principal this parametrization is ne, since Gj(cid:1)x(cid:2) is monotone in pj(cid:1)x(cid:2).
However, we are estimating the Gj(cid:1)x(cid:2) in an uncoupled fashion, and there is
no guarantee that the implied probabilities sum to 1. We give some examples
where this makes a difference, and AdaBoost.MH performs more poorly than
an alternative coupled likelihood procedure.

Schapire and Singers AdaBoost.MH was also intended to cover situations
where observations can belong to more than one class. The MH represents
Multi-Label Hamming, Hamming loss being used to measure the errors in
the space of 2J possible class labels. In this context tting a separate classier
for each label is a reasonable strategy. However, Schapire and Singer also
propose using AdaBoost.MH when the class labels are mutually exclusive,
which is the focus in this paper.

Algorithm 6 is a natural generalization of Algorithm 3 for tting the J-class

logistic regression model (40).

Result 6. The LogitBoost algorithm (J classes, population version) uses
quasi-Newton steps for tting an additive symmetric logistic model by
maximum-likelihood.

LogitBoost (J classes)

1. Start with weights wij = 1/N(cid:1) i = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) N(cid:1) j = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J(cid:1) Fj(cid:1)x(cid:2) = 0 and

pj(cid:1)x(cid:2) = 1/J  j.

2. Repeat for m = 1(cid:1) 2(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) M:
(a) Repeat for j = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J:

(i) Compute working responses and weights in the jth class,

(b) Set fmj(cid:1)x(cid:2)  J1
(c) Update pj(cid:1)x(cid:2) via (40).

fmj(cid:1)x(cid:2).

J

(cid:1)fmj(cid:1)x(cid:2)  1

J

3. Output the classier arg maxj Fj(cid:1)x(cid:2).

Algorithm 6. An adaptive Newton algorithm for tting an additive mul-

tiple logistic regression model.

ij  pj(cid:1)xi(cid:2)
y

zij =
pj(cid:1)xi(cid:2)(cid:1)1  pj(cid:1)xi(cid:2)(cid:2) (cid:1)
wij = pj(cid:1)xi(cid:2)(cid:1)1  pj(cid:1)xi(cid:2)(cid:2)(cid:4)

(ii) Fit the function fmj(cid:1)x(cid:2) by a weighted least-squares regression of zij

to xi with weights wij.

(cid:1)J
k=1 fmk(cid:1)x(cid:2)(cid:2), and Fj(cid:1)x(cid:2)  Fj(cid:1)x(cid:2) +

ADDITIVE LOGISTIC REGRESSION

357

Derivation.

1. We rst give the score and Hessian for the population Newton algorithm

corresponding to a standard multilogit parametrization

Gj(cid:1)x(cid:2) = log

P(cid:1)y
P(cid:1)y

j = 1(cid:15)x(cid:2)
J = 1(cid:15)x(cid:2)

with GJ(cid:1)x(cid:2) = 0 (and the choice of J for the base class is arbitrary). The
expected conditional log-likelihood is:

E(cid:1)l(cid:1)G + g(cid:2)(cid:15)x(cid:2) = J1(cid:2)

j=1

E(cid:1)y
(cid:11)

j(cid:15)x(cid:2)(cid:1)Gj(cid:1)x(cid:2) + gj(cid:1)x(cid:2)(cid:2)
(cid:14)
1 + J1(cid:2)
j  pj(cid:1)x(cid:2)(cid:15)x(cid:2)(cid:1) j = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J  1(cid:1)

eGk(cid:1)x(cid:2)+gk(cid:1)x(cid:2)

k=1

(cid:1)

 log
sj(cid:1)x(cid:2) = E(cid:1)y

Hj(cid:1) k(cid:1)x(cid:2) = pj(cid:1)x(cid:2)(cid:1)jk  pk(cid:1)x(cid:2)(cid:2)(cid:1) j(cid:1) k = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J  1(cid:4)

2. Our quasi-Newton update amounts to using a diagonal approximation to

the Hessian, producing updates:

gj(cid:1)x(cid:2)  E(cid:1)y

j  pj(cid:1)x(cid:2)(cid:15)x(cid:2)

pj(cid:1)x(cid:2)(cid:1)1  pj(cid:1)x(cid:2)(cid:2) (cid:1) j = 1(cid:1) (cid:4) (cid:4) (cid:4) (cid:1) J  1(cid:4)

and set fj(cid:1)x(cid:2) = gj(cid:1)x(cid:2)  (cid:1)1/J(cid:2)(cid:1)J
3. To convert to the symmetric parametrization, we would note that gJ = 0,
k=1 gk(cid:1)x(cid:2). However, this procedure could
be applied using any class as the base, not just the Jth. By averaging over
all choices for the base class, we get the update

(cid:14)(cid:11)

(cid:11)

J  1

J

fj(cid:1)x(cid:2) =

j  pj(cid:1)x(cid:2)(cid:15)x(cid:2)

E(cid:1)y
pj(cid:1)x(cid:2)(cid:1)1  pj(cid:1)x(cid:2)(cid:2)  1

J

E(cid:1)y
k  pk(cid:1)x(cid:2)(cid:15)x(cid:2)
pk(cid:1)x(cid:2)(cid:1)1  pk(cid:1)x(cid:2)(cid:2)

J(cid:2)
k=1

(cid:14)

(cid:4)

For more rigid parametric models and full Newton stepping, this sym-
metrization would be redundant. With quasi-Newton steps and adaptive (tree
based) models, the symmetrization removes the dependence on the choice of
the base class.

6. Simulation studies.

In this section the four avors of boosting out-
lined above are applied to several articially constructed problems. Compar-
isons based on real data are presented in Section 7.

An advantage of comparisons made in a simulation setting is that all as-
pects of each example are known, including the Bayes error rate and the
complexity of the decision boundary. In addition, the population expected error
rates achieved by each of the respective methods can be estimated to arbitrary
accuracy by averaging over a large number of different training and test data

358

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

sets drawn from the population. The four boosting methods compared here
are:

DAB: Discrete AdaBoostAlgorithm 1.

RAB: Real AdaBoostAlgorithm 2.

LB: LogitBoostAlgorithms 3 and 6.

GAB: Gentle AdaBoostAlgorithm 4.

DAB, RAB and GAB handle multiple classes using the AdaBoost.MH

approach.

In an attempt to differentiate performance, all of the simulated examples
involve fairly complex decision boundaries. The ten input features for all exam-
ples are randomly drawn from a ten-dimensional standard normal distribution
x  N10(cid:1)0(cid:1) I(cid:2). For the rst three examples the decision boundaries separating
successive classes are nested concentric ten-dimensional spheres constructed
by thresholding the squared-radius from the origin

r2 = 10(cid:2)

j=1

x2
j(cid:4)

(41)

1

i < tk(cid:7)

Ck = (cid:6)xi(cid:15)tk1  r2

Each class Ck (cid:1)1  k  K(cid:2) is dened as the subset of observations
(42)
with t0 = 0 and tK = . The (cid:6)tk(cid:7)K1
for each example were chosen so as to
put approximately equal numbers of observations in each class. The training
sample size is N = K 1000 so that approximately 1000 training observations
are in each class. An independently drawn test set of 10,000 observations was
used to estimate error rates for each training set. Averaged results over ten
such independently drawn trainingtest set combinations were used for the
nal error rate estimates. The corresponding statistical uncertainties (stan-
dard errors) of these nal estimates (averages) are approximately a line width
on each plot.
Figure 3 (top left) compares the four algorithms in the two-class (cid:1)K = 2(cid:2)
case using a two-terminal node decision tree (stump) as the base classier.
Shown is error rate as a function of number of boosting iterations. The upper
(black) line represents DAB and the other three nearly coincident lines are
the other three methods (dotted red = RAB, short-dashed green = LB, and
long-dashed blue = GAB). Note that the somewhat erratic behavior of DAB,
especially for less than 200 iterations, is not due to statistical uncertainty. For
less than 400 iterations LB has a minuscule edge, after that it is a dead heat
with RAB and GAB. DAB shows substantially inferior performance here with
roughly twice the error rate at all iterations.
Figure 3 (lower left) shows the corresponding results for three classes (cid:1)K =
3(cid:2) again with two-terminal node trees. Here the problem is more difcult as
represented by increased error rates for all four methods, but their relation-
ship is roughly the same: the upper (black) line represents DAB and the other

ADDITIVE LOGISTIC REGRESSION

359

Fig. 3. Test error curves for the simulation experiment with an additive decision boundary, as
described in (cid:1)42(cid:2). In all panels except the top right, the solid curve (representing Discrete AdaBoost)
lies alone above the other three curves.

three nearly coincident lines are the other three methods. The situation is
somewhat different for larger number of classes. Figure 3 (lower right) shows
results for K = 5 which are typical for K  4. As before, DAB incurs much
higher error rates than all the others, and RAB and GAB have nearly iden-
tical performance. However, the performance of LB relative to RAB and GAB
has changed. Up to about 40 iterations it has the same error rate. From 40 to
about 100 iterations LBs error rates are slightly higher than the other two.
After 100 iterations the error rate for LB continues to improve whereas that

360

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

for RAB and GAB level off, decreasing much more slowly. By 800 iterations the
error rate for LB is 0.19 whereas that for RAB and GAB is 0.32. Speculation
as to the reason for LBs performance gain in these situations is presented
below.

In the above examples a stump was used as the base classier. One might
expect the use of larger trees would do better for these rather complex prob-
lems. Figure 3 (top right) shows results for the two-class problem, here boost-
ing trees with eight terminal nodes. These results can be compared to those for
stumps in Figure 3 (top left). Initially, error rates for boosting eight-node trees
decrease much more rapidly than for stumps, with each successive iteration,
for all methods. However, the error rates quickly level off and improvement
is very slow after about 100 iterations. The overall performance of DAB is
much improved with the bigger trees, coming close to that of the other three
methods. As before RAB, GAB and LB exhibit nearly identical performance.
Note that at each iteration the eight-node tree model consists of four times
the number of additive terms as does the corresponding stump model. This
is why the error rates decrease so much more rapidly in the early iterations.
In terms of model complexity (and training time), a 100-iteration model using
eight-terminal node trees is equivalent to a 400-iteration stump model.

Comparing the top two panels in Figure 3, one sees that for RAB, GAB
and LB the error rate using the bigger trees (0.072) is in fact 33% higher
than that for stumps (0.054) at 800 iterations, even though the former is four
times more complex. This seemingly mysterious behavior is easily understood
by examining the nature of the decision boundary separating the classes. The
Bayes decision boundary between two classes is the set

x(cid:12) log

P(cid:1)y = 1(cid:15)x(cid:2)
P(cid:1)y = 1(cid:15)x(cid:2) = 0

(43)
or simply (cid:6)x(cid:12) B(cid:1)x(cid:2) = 0(cid:7). To approximate this set it is sufcient to estimate
the logit B(cid:1)x(cid:2), or any monotone transformation of B(cid:1)x(cid:2), as closely as possible.
As discussed above, boosting produces an additive logistic model whose com-
ponent functions are represented by the base classier. With stumps as the
base classier, each component function has the form
m1(cid:8)xj>tm(cid:10)

m1(cid:8)xjtm(cid:10) + cR

fm(cid:1)x(cid:2) = cL

(44)

= fm(cid:1)xj(cid:2)

(45)
if the mth stump chose to split on coordinate j. Here tm is the split-point,
and cL
m are the weighted means of the response in the left and right
terminal nodes. Thus the model produced by boosting stumps is additive in
the original features,

m and cR

(cid:10)

(cid:16)

(46)

F(cid:1)x(cid:2) = p(cid:2)

j=1

gj(cid:1)xj(cid:2)(cid:1)

where gj(cid:1)xj(cid:2) adds together all those stumps involving xj (and is 0 if none
exist).

ADDITIVE LOGISTIC REGRESSION

361

Examination of (41) and (42) reveals that an optimal decision boundary for
j +
the above examples is also additive in the original features, with fj(cid:1)xj(cid:2)= x2
constant. Thus, in the context of decision trees, stumps are ideally matched
to these problems; larger trees are not needed. However boosting larger trees
need not be counterproductive in this case if all of the splits in each individual
tree are made on the same predictor variable. This would also produce an
additive model in the original features (46). However, due to the forward
greedy stagewise strategy used by boosting, this is not likely to happen if the
decision boundary function involves more than one predictor; each individual
tree will try to do its best to involve all of the important predictors. Owing to
the nature of decision trees, this will produce models with interaction effects;
most terms in the model will involve products in more than one variable. Such
nonadditive models are not as well suited for approximating truly additive
decision boundaries such as (41) and (42). This is reected in increased error
rate as observed in Figure 3.

The above discussion also suggests that if the decision boundary separating
pairs of classes were inherently nonadditive in the predictors, then boosting
stumps would be less advantageous than using larger trees. A tree with m
terminal nodes can produce basis functions with a maximum interaction order
of min(cid:1)m  1(cid:1) p(cid:2) where p is the number of predictor features. These higher
order basis functions provide the possibility to more accurately estimate those
decision boundaries B(cid:1)x(cid:2) with high-order interactions. The purpose of the next
example is to verify this intuition. There are two classes (cid:1)K = 2(cid:2) and 5000
training observations with the (cid:6)xi(cid:7)5000
drawn from a ten-dimensional normal
distribution as in the previous examples. Class labels were randomly assigned
to each observation with log-odds
Pr(cid:8)y = 1(cid:15)x(cid:10)
Pr(cid:8)y = 1(cid:15)x(cid:10)

1 + 6(cid:2)

= 10

(cid:1)1(cid:2)lxl

(cid:4)

(47)

(cid:11)

6(cid:2)
j=1

xj

(cid:11)

log

1

(cid:14)

(cid:14)

l=1

Approximately equal numbers of observations are assigned to each of the
two classes, and the Bayes error rate is 0.046. The decision boundary for this
problem is a complicated function of the rst six predictor variables involving
all of them in second-order interactions of equal strength. As in the above
examples, test sets of 10,000 observations was used to estimate error rates for
each training set, and nal estimates were averages over ten replications.

Figure 4 (top left) shows test-error rate as a function of iteration number for
each of the four boosting methods using stumps. As in the previous examples,
RAB and GAB track each other very closely. DAB begins very slowly, being
dominated by all of the others until around 180 iterations, where it passes
below RAB and GAB. LB mostly dominates, having the lowest error rate until
about 650 iterations. At that point DAB catches up and by 800 iterations it
may have a very slight edge. However, none of these boosting methods perform
well with stumps on this problem, the best error rate being 0.35.

Figure 4 (top right) shows the corresponding plot when four terminal node
trees are boosted. Here there is a dramatic improvement with all of the four

362

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Fig. 4. Test error curves for the simulation experiment with a nonadditive decision boundary, as
described in (47).

methods. For the rst time there is some small differentiation between RAB
and GAB. At nearly all iterations the performance ranking is LB best, followed
by GAB, RAB and DAB in order. At 800 iterations LB achieves an error rate of
0.134. Figure 4 (lower left) shows results when eight terminal node trees are
boosted. Here, error rates are generally further reduced with LB improving
the least (0.130), but still dominating. The performance ranking among the
other three methods changes with increasing iterations; DAB overtakes RAB

ADDITIVE LOGISTIC REGRESSION

363

at around 150 iterations and GAB at about 230 becoming fairly close to LB
by 800 iterations with an error rate of 0.138.

Although limited in scope, these simulation studies suggest several trends.
They explain why boosting stumps can sometimes be superior to using larger
trees, and suggest situations where this is likely to be the case; that is when
decision boundaries B(cid:1)x(cid:2) can be closely approximated by functions that are
additive in the original predictor features. When higher order interactions
are required, stumps exhibit poor performance. These examples illustrate the
close similarity between RAB and GAB. In all cases the difference in perfor-
mance between DAB and the others decreases when larger trees and more
iterations are used, sometimes overtaking the others. More generally, relative
performance of these four methods depends on the problem at hand in terms
of the nature of the decision boundaries, the complexity of the base classier
and the number of boosting iterations.

The superior performance of LB in Figure 3 (lower right) appears to be a
consequence of the multiclass logistic model (Algorithm 6). All of the other
methods use the asymmetric AdaBoost.MH strategy (Algorithm 5) of building
separate two-class models for each individual class against the pooled com-
plement classes. Even if the decision boundaries separating all class pairs
are relatively simple, pooling classes can produce complex decision bound-
aries that are difcult to approximate [Friedman (1996)]. By considering all
of the classes simultaneously, the symmetric multiclass model is better able
to take advantage of simple pairwise boundaries when they exist [Hastie and
Tibshirani (1998)]. As noted above, the pairwise boundaries induced by (41)
and (42) are simple when viewed in the context of additive modeling, whereas
the pooled boundaries are more complex; they cannot be well approximated
by functions that are additive in the original predictor variables.

The decision boundaries associated with these examples were deliberately
chosen to be geometrically complex in an attempt to elicit performance differ-
ences among the methods being tested. Such complicated boundaries are not
likely to often occur in practice. Many practical problems involve compara-
tively simple boundaries [Holte (1993)]; in such cases performance differences
will still be situation dependent, but correspondingly less pronounced.

7. Some experiments with real world data.

In this section we show
the results of running the four tting methods: LogitBoost, Discrete AdaBoost,
Real AdaBoost and Gentle AdaBoost on a collection of datasets from the UC-
Irvine machine learning archive, plus a popular simulated dataset. The base
learner is a tree in each case, with either two or eight terminal nodes. For com-
parison, a single decision tree was also t (using the tree function in Splus),
with the tree size determined by 5-fold cross-validation.

The datasets are summarized in Table 1. The test error rates are shown in
Table 2 for the smaller datasets, and in Table 3 for the larger ones. The vowel,
sonar, satimage and letter datasets come with a prespecied test set. The
waveform data is simulated, as described in Breiman, Friedman, Olshen and

364

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Table 1

Datasets used in the experiments

Data set

Vowel
Breast cancer
Ionosphere
Glass
Sonar
Waveform
Satimage
Letter

# Train

528
699
351
214
210
300
4435
16000

# Test

462
5-fold CV
5-fold CV
5-fold CV
5-fold CV
5000
2000
4000

# Inputs

# Classes

10
9
34
10
60
21
36
16

11
2
2
7
2
3
6
26

Stone (1984). For the others, 5-fold cross-validation was used to estimate the
test error.

It is difcult to discern trends on the small datasets (Table 2) because all
but quite large observed differences in performance could be attributed to
sampling uctuations. On the vowel, breast cancer, ionosphere, sonar and
waveform data, purely additive stump models seem to perform comparably to
the larger (eight-node) trees. The glass data seems to benet a little from
larger trees. There is no clear differentiation in performance among the boost-
ing methods.

On the larger data sets (Table 3) clearer trends are discernible. For the
satimage data the eight-node tree models are only slightly, but signicantly,
more accurate than the purely additive models. For the letter data there is
no contest. Boosting stumps is clearly inadequate. There is no clear differ-
entiation among the boosting methods for eight-node trees. For the stumps,
LogitBoost, Real AdaBoost and Gentle AdaBoost have comparable perfor-
break mance, distinctly superior to Discrete AdaBoost. This is consistent with
the results of the simulation study (Section 6).

Except perhaps for Discrete AdaBoost, the real data examples fail to demon-
strate performance differences between the various boosting methods. This is
in contrast to the simulated data sets of Section 6. There LogitBoost gener-
ally dominated, although often by a small margin. The inability of the real
data examples to discriminate may reect statistical difculties in estimat-
ing subtle differences with small samples. Alternatively, it may be that their
underlying decision boundaries are all relatively simple [Holte (1993)] so that
all reasonable methods exhibit similar performance.

8. Additive logistic trees.

In most applications of boosting the base clas-
sier is considered to be a primitive, repeatedly called by the boosting proce-
dure as iterations proceed. The operations performed by the base classier
are the same as they would be in any other context given the same data and
weights. The fact that the nal model is going to be a linear combination of
a large number of such classiers is not taken into account. In particular,
when using decision trees, the same tree growing and pruning algorithms are

ADDITIVE LOGISTIC REGRESSION

365

Test error rates on small real examples

Table 2

2 Terminal Nodes

8 Terminal Nodes

Iterations

50

100

200

50

100

200

CART error = 0(cid:4)642

0.532
0.565
0.556
0.563

0.028
0.038
0.037
0.042

0.074
0.068
0.085
0.088

0.266
0.276
0.276
0.285

0.231
0.154
0.183
0.154

0.196
0.193
0.190
0.188

0.524
0.561
0.571
0.535

0.031
0.038
0.037
0.040

0.077
0.066
0.074
0.080

0.257
0.247
0.261
0.285

0.231
0.163
0.183
0.144

0.195
0.197
0.188
0.185

0.511
0.548
0.584
0.563

0.029
0.040
0.041
0.040

0.071
0.068
0.077
0.080

0.266
0.257
0.252
0.271

0.202
0.202
0.173
0.183

0.206
0.195
0.193
0.191

0.517
0.496
0.515
0.511

0.034
0.032
0.032
0.032

0.068
0.054
0.066
0.068

0.243
0.234
0.219
0.238

0.163
0.173
0.154
0.163

0.192
0.185
0.185
0.186

0.517
0.496
0.496
0.500

0.038
0.034
0.031
0.035

0.063
0.054
0.063
0.063

0.238
0.234
0.233
0.234

0.154
0.173
0.154
0.144

0.191
0.182
0.185
0.183

0.517
0.496
0.496
0.500

0.038
0.034
0.031
0.037

0.063
0.054
0.063
0.063

0.238
0.234
0.238
0.243

0.154
0.173
0.154
0.144

0.191
0.182
0.186
0.183

Method

Vowel

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Breast

CART error = 0(cid:4)045

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Ion

CART error = 0(cid:4)076

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Glass

CART error = 0(cid:4)400

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Sonar

CART error = 0(cid:4)596

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Waveform

CART error = 0(cid:4)364

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

366

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Test error rates on larger data examples

Table 3

Iterations

Method

Satimage

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Letter

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

LogitBoost
Real AdaBoost
Gentle AdaBoost
Discrete AdaBoost

Terminal

Nodes
20
CART error = 0(cid:4)148
0.140
2
0.148
2
0.148
2
2
0.174

8
0.096
8
0.105
8
0.106
8
0.122
CART error = 0(cid:4)124
0.250
2
0.244
2
2
0.246
0.310
2

8
8
8
8

0.075
0.068
0.068
0.080

50

100

200

Fraction

0.120
0.126
0.129
0.156

0.095
0.102
0.103
0.107

0.182
0.181
0.187
0.226

0.047
0.041
0.040
0.045

0.112
0.117
0.119
0.140

0.092
0.092
0.095
0.100

0.159
0.160
0.157
0.196

0.036
0.033
0.030
0.035

0.102
0.119
0.119
0.128

0.088
0.091
0.089
0.099

0.145
0.150
0.145
0.185

0.033
0.032
0.028
0.029

0.06
0.12
0.14
0.18

0.03
0.03
0.03
0.03

generally employed. Sometimes alterations are made (such as no pruning) for
programming convenience and speed.

When boosting is viewed in the light of additive modeling, however, this
greedy approach can be seen to be far from optimal in many situations. As
discussed in Section 6 the goal of the nal classier is to produce an accurate
approximation to the decision boundary function B(cid:1)x(cid:2). In the context of boost-
ing, this goal applies to the nal additive model, not to the individual terms
(base classiers) at the time they were constructed. For example, it was seen
in Section 6 that if B(cid:1)x(cid:2) was close to being additive in the original predictive
features, then boosting stumps was optimal since it produced an approxima-
tion with the same structure. Building larger trees increased the error rate
of the nal model because the resulting approximation involved high-order
interactions among the features. The larger trees optimized error rates of the
individual base classiers, given the weights at that step, and even produced
lower unweighted error rates in the early stages. However, after a sufcient
number of boosts, the stump-based model achieved superior performance.

More generally, one can consider an expansion of the decision boundary

function in a functional ANOVA decomposition [Friedman (1991)]

(48)

j

j(cid:1) k

j(cid:1) k(cid:1) l

B(cid:1)x(cid:2) = (cid:2)

fj(cid:1)xj(cid:2) + (cid:2)

fjk(cid:1)xj(cid:1) xk(cid:2) + (cid:2)

fjkl(cid:1)xj(cid:1) xk(cid:1) xl(cid:2) +  (cid:4)

ADDITIVE LOGISTIC REGRESSION

367
The rst sum represents the closest function to B(cid:1)x(cid:2) that is additive in
the original features, the rst two represent the closest approximation involv-
ing at most two-feature interactions, the rst three represent three-feature
interactions, and so on. If B(cid:1)x(cid:2) can be accurately approximated by such an
expansion, truncated at low interaction order, then allowing the base classi-
er to produce higher order interactions can reduce the accuracy of the nal
boosted model. In the context of decision trees, higher order interactions are
produced by deeper trees.

In situations where the true underlying decision boundary function admits
a low order ANOVA decomposition, one can take advantage of this structure
to improve accuracy by restricting the depth of the base decision trees to be
not much larger than the actual interaction order of B(cid:1)x(cid:2). Since this is not
likely to be known in advance for any particular problem, this maximum depth
becomes a meta-parameter of the procedure to be estimated by some model
selection technique, such as cross-validation.

One can restrict the depth of an induced decision tree by using its stan-
dard pruning procedure, starting from the largest possible tree, but requiring
it to delete enough splits to achieve the desired maximum depth. This can
be computationally wasteful when this depth is small. The time required to
build the tree is proportional to the depth of the largest possible tree before
pruning. Therefore, dramatic computational savings can be achieved by sim-
ply stopping the growing process at the maximum depth, or alternatively at
a maximum number of terminal nodes. The standard heuristic arguments in
favor of growing large trees and then pruning do not apply in the context of
boosting. Shortcomings in any individual tree can be compensated by trees
grown later in the boosting sequence.

If a truncation strategy based on number of terminal nodes is to be
employed, it is necessary to dene an order in which splitting takes place. We
adopt a best-rst strategy. An optimal split is computed for each currently
terminal node. The node whose split would achieve the greatest reduction in
the tree building criterion is then actually split. This increases the number
of terminal nodes by one. This continues until a maximum number M of ter-
minal notes is induced. Standard computational tricks can be employed so
that inducing trees in this order requires no more computation than other
orderings commonly used in decision tree induction.

The truncation limit M is applied to all trees in the boosting sequence. It is
thus a meta-parameter of the entire boosting procedure. An optimal value can
be estimated through standard model selection techniques such as minimizing
cross-validated error rate of the nal boosted model. We refer to this combi-
nation of truncated best-rst trees, with boosting, as additive logistic trees
(ALT). Best-rst trees were used in all of the simulated and real examples.
One can compare results on the latter (Tables 2 and 3) to corresponding results
reported by Dietterich [(1998), Table 1] on common data sets. Error rates
achieved by ALT with very small truncation values are seen to compare quite
favorably with other committee approaches using much larger trees at each
boosting step. Even when error rates are the same, the computational savings

368

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

Fig. 5. Coordinate functions for the additive logistic tree obtained by boosting (Logitboost) with
stumps, for the two-class nested sphere example from Section 6.

associated with ALT can be quite important in data mining contexts where
large data sets cause computation time to become an issue.

Another advantage of low order approximations is model visualization. In
particular, for models additive in the input features (46), the contribution of
each feature xj can be viewed as a graph of gj(cid:1)xj(cid:2) plotted against xj. Figure 5
shows such plots for the ten features of the two-class nested spheres example
of Figure 3. The functions are shown for the rst class concentrated near the
origin; the corresponding functions for the other class are the negatives of
these functions.

The plots in Figure 5 clearly show that the contribution to the log-odds of
each individual feature is approximately quadratic, which matches the gener-
ating model (41) and (42).

When there are more than two classes, plots similar to Figure 5 can be made
for each class and analogously interpreted. Higher order interaction models
are more difcult to visualize. If there are at most two-feature interactions,
the two-variable contributions can be visualized using contour or perspective
mesh plots. Beyond two-feature interactions, visualization techniques are even
less effective. Even when noninteraction (stump) models do not achieve the
highest accuracy, they can be very useful as descriptive statistics owing to the
interpretability of the resulting model.

9. Weight trimming.

In this section we propose a simple idea and show
that it can dramatically reduce computation for boosted models without sacri-
cing accuracy. Despite its apparent simplicity, this approach does not appear
to be in common use [although similar ideas have been proposed before:
Schapire (1990), Freund (1995)]. At each boosting iteration there is a
distribution of weights over the training sample. As iterations proceed, this
distribution tends to become highly skewed towards smaller weight values.
A larger fraction of the training sample becomes correctly classied with
increasing condence, thereby receiving smaller weights. Observations with
very low relative weight have little impact on training of the base classier;
only those that carry the dominant proportion of the weight mass are inuen-

ADDITIVE LOGISTIC REGRESSION

369

tial. The fraction of such high weight observations can become very small in
later iterations. This suggests that at any iteration one can simply delete from
the training sample the large fraction of observations with very low weight
without having much effect on the resulting induced classier. However, com-
putation is reduced since it tends to be proportional to the size of the training
sample, regardless of weights.
At each boosting iteration, training observations with weight wi less than
a threshold wi < t(cid:1)(cid:2) are not used to train the classier. We take the value of
t(cid:1)(cid:2) to be the th quantile of the weight distribution over the training data
at the corresponding iteration. That is, only those observations that carry
the fraction 1   of the total weight mass are used for training. Typically
  (cid:8)0(cid:4)01(cid:1) 0(cid:4)1(cid:10) so that the data used for training carries from 90 to 99% of
the total weight mass. Note that the weights for all training observations are
recomputed at each iteration. Observations deleted at a particular iteration
may therefore reenter at later iterations if their weights subsequently increase
relative to other observations.

Figure 6 (left panel) shows test-error rate as a function of iteration number
for the letter recognition problem described in Section 7, here using Gentle
AdaBoost and eight-node trees as the base classier. Two error rate curves are
shown. The black solid one represents using the full training sample at each
iteration (cid:1) = 0(cid:2), whereas the blue dashed curve represents the corresponding
error rate for  = 0(cid:4)1. The two curves track each other very closely, especially
at the later iterations. Figure 6 (right panel) shows the corresponding frac-
tion of observations used to train the base classier as a function of iteration
number. Here the two curves are not similar. With  = 0(cid:4)1 the number of
observations used for training drops very rapidly reaching roughly 5% of the
total at 20 iterations. By 50 iterations it is down to about 3% where it stays
throughout the rest of the boosting procedure. Thus, computation is reduced

Fig. 6. The left panel shows the test error for the letter recognition problem as a function of
iteration number. The black solid curve uses all the training data, the red dashed curve uses a
subset based on weight thresholding. The right panel shows the percent of training data used for
both approaches. The upper curve steps down, because training can stop for an entire class if it is
t sufciently well (see text).

370

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

by over a factor of 30 with no apparent loss in classication accuracy. The
reason why sample size in this case decreases for  = 0 after 150 iterations is
that if all of the observations in a particular class are classied correctly with
very high condence (cid:8)Fk > 15+ log(cid:1)N(cid:2)(cid:10) training for that class stops, and con-
tinues only for the remaining classes. At 400 iterations, 12 classes remained
of the original 26 classes.

The last column labeled fraction in Table 3 for the letter-recognition problem
shows the average fraction of observations used in training the base classiers
over the 200 iterations, for all boosting methods and tree sizes. For eight-node
trees, all methods behave as shown in Figure 6. With stumps, LogitBoost uses
considerably less data than the others and is thereby correspondingly faster.
This is a genuine property of LogitBoost that sometimes gives it an advan-
tage with weight trimming. Unlike the other methods, the LogitBoost weights
wi = pi(cid:1)1  pi(cid:2) do not in any way involve the class outputs yi; they simply
measure nearness to the currently estimated decision boundary FM(cid:1)x(cid:2) = 0.
Discarding small weights thus retains only those training observations that
are estimated to be close to the boundary. For the other three procedures the
weight is monotone in yiFM(cid:1)xi(cid:2). This gives highest weight to currently mis-
classied training observations, especially those far from the boundary. If after
trimming, the fraction of observations remaining is less than the error rate,
the subsample passed to the base learner will be highly unbalanced contain-
ing very few correctly classied observations. This imbalance seems to inhibit
learning. No such imbalance occurs with LogitBoost since near the decision
boundary, correctly and misclassied observations appear in roughly equal
numbers.

As this example illustrates, very large reductions in computation for boost-
ing can be achieved by this simple trick. A variety of other examples (not
shown) exhibit similar behavior with all boosting methods. Note that other
committee approaches to classication such as bagging [Breiman (1996)] and
randomized trees [Dietterich (1998)] while admitting parallel implementa-
tions, cannot take advantage of this approach to reduce computation.

10. Further generalizations of boosting. We have shown above that
AdaBoost ts an additive model, optimizing a criterion similar to binomial
log-likelihood, via an adaptive Newton method. This suggests ways in which
the boosting paradigm may be generalized. First, the Newton step can be
replaced by a gradient step, slowing down the tting procedure. This can
reduce susceptibility to overtting and lead to improved performance. Sec-
ond, any smooth loss function can be used: for regression, squared error is
natural, leading to the tting of residuals boosting algorithm mentioned
in the introduction. However, other loss functions might have benets, for
example, tapered squared error based on Hubers robust inuence function
[Huber (1964)]. The resulting procedure is a fast, convenient method for resis-
tant tting of additive models. Details of these generalizations may be found
in Friedman (1999).

ADDITIVE LOGISTIC REGRESSION

371

11. Concluding remarks.

In order to understand a learning procedure
statistically it is necessary to identify two important aspects: its structural
model and its error model. The former is most important since it determines
the function space of the approximator, thereby characterizing the class of
functions or hypotheses that can be accurately approximated with it. The
error model species the distribution of random departures of sampled data
from the structural model. It thereby denes the criterion to be optimized in
the estimation of the structural model.

We have shown that the structural model for boosting is additive on the
logistic scale with the base learner providing the additive components. This
understanding alone explains many of the properties of boosting. It is no sur-
prise that a large number of such (jointly optimized) components denes a
much richer class of learners than one of them alone. It reveals that in the
context of boosting all base learners are not equivalent, and there is no uni-
versally best choice over all situations. As illustrated in Section 6, the base
learners need to be chosen so that the resulting additive expansion matches
the particular decision boundary encountered. Even in the limited context of
boosting decision trees the interaction order, as characterized by the number of
terminal nodes, needs to be chosen with care. Purely additive models induced
by decision stumps are sometimes, but not always, the best. However, we con-
jecture that boundaries involving very high-order interactions will rarely be
encountered in practice. This motivates our additive logistic trees (ALT) pro-
cedure described in Section 8.

The error model for two-class boosting is the obvious one for binary vari-
ables, namely the Bernoulli disribution. We show that the AdaBoost proce-
dures maximize a criterion that is closely related to expected log-Bernoulli
likelihood, having the identical solution in the distributional (L2) limit of
innite data. We derived a more direct procedure for maximizing this log-
likelihood (LogitBoost) and show that it exhibits properties nearly identical to
those of Real AdaBoost.

In the multiclass case, the AdaBoost procedures maximize a separate
Bernoulli likelihood for each class versus the others. This is a natural choice
and is especially appropriate when observations can belong to more than one
class [Schapire and Singer (1998)]. In the more usual setting of a unique
class label for each observation, the symmetric multinomial distribution is
a more appropriate error model. We develop a multiclass LogitBoost proce-
dure that maximizes the corresponding log-likelihood by quasi-Newton step-
ping. We show through simulated examples that there exist settings where
this approach leads to superior performance, although none of these situa-
tions seems to have been encountered in the set of real data examples used
for illustration; the performance of both approaches had quite similar perfor-
mance over these examples.

The concepts developed in this paper suggest that there is very little, if any,
connection between (deterministic) weighted boosting and other (randomized)
ensemble methods such as bagging [Breiman (1996)] and randomized trees
[Dietterich (1998)]. In the language of least-squares regression, the latter are

372

J. FRIEDMAN, T. HASTIE AND R. TIBSHIRANI

purely variance reducing procedures intended to mitigate instability, espe-
cially that associated with decision trees. Boosting on the other hand seems
fundamentally different. It appears to be mainly a bias reducing procedure,
intended to increase the exibility of stable (highly biased) weak learners by
incorporating them in a jointly tted additive expansion.

The distinction becomes less clear [Breiman (1998a)] when boosting is
implemented by nite weighted random sampling instead of weighted opti-
mization. The advantages or disadvantages of introducing randomization into
boosting by drawing nite samples is not clear. If there turns out to be an
advantage with randomization in some situations, then the degree of random-
ization, as reected by the sample size, is an open question. It is not obvious
that the common choice of using the size of the original training sample is
optimal in all (or any) situations.

One fascinating issue not covered in this paper is the fact that boosting,
whatever avor, seems resistant to overtting. Some possible explanations are:

1. As the LogitBoost iterations proceed, the overall impact of changes intro-
duced by fm(cid:1)x(cid:2) reduces. Only observations with appreciable weight deter-
mine the new functionsthose near the decision boundary. By denition
these observations have F(cid:1)x(cid:2) near zero and can be affected by changes,
while those in pure regions have large values of (cid:15)F(cid:1)x(cid:2)(cid:15) and are less likely
to be modied.

2. The stagewise nature of the boosting algorithms does not allow the full col-
lection of parameters to be jointly t, and thus has far lower variance than
the full parameterization might suggest. In the computational learning the-
ory literature this is explained in terms of VC dimension of the ensemble
compared to that of each weak learner.

Fig. 7. Real AdaBoost (stumps) on a noisy concentric-sphere problem, with 400 observations per
class and Bayes error 25%. The test error (upper curve) increases after about fty iterations.

ADDITIVE LOGISTIC REGRESSION

373

3. Classiers are hurt less by overtting than other function estimators
[e.g., the famous risk bound of the 1-nearest-neighbor classier, Cover and
Hart (1967)].

Figure 7 shows a case where boosting does overt. The data are gener-
ated from two ten-dimensional spherical Gaussians with the same mean, and
variances chosen so that the Bayes error is 25% (400 samples per class). We
used Real AdaBoost and stumps (the results were similar for all the boosting
algorithms). After about 50 iterations the test error (slowly) increases.

Schapire, Freund, Bartlett and Lee (1998) suggest that the properties of
AdaBoost, including its resistance to overtting, can be understood in terms of
classication margins. However, Breiman (1997) presents evidence counter to
this explanation. Whatever the explanation, the empirical evidence is strong;
the introduction of boosting by Schapire, Freund and colleagues has brought
an exciting and important set of new ideas to the table.

Acknowledgments. We thank Andreas Buja for alerting us to the recent
work on text classication at AT&T laboratories, Bogdan Popescu for illu-
minating discussions on PAC learning theory and Leo Breiman and Robert
Schapire for useful comments on an earlier version of this paper. We also
thank two anonymous referees and an Associate Editor for detailed and use-
ful comments on an earlier draft of the paper.

