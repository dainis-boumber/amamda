1. Introduction

One of the rst problems infants must solve as they are
acquiring language is word segmentation:
identifying
word boundaries in continuous speech. About 9% of utter-
ances directed at English-learning infants consist of iso-
lated words (Brent & Siskind, 2001), but there is no
obvious way for children to know from the outset which
utterances these are. Since multi-word utterances gener-
ally have no apparent pauses between words, children
must be using other cues to identify word boundaries. In
fact, there is evidence that infants use a wide range of weak
cues for word segmentation. These cues include phonotac-

* Corresponding author. Tel.: +44 131 651 5609.

E-mail addresses: sgoldwat@inf.ed.ac.uk, sgwater@gmail.com (S. Gold-

water).

0010-0277/$ - see front matter  2009 Elsevier B.V. All rights reserved.
doi:10.1016/j.cognition.2009.03.008

tics (Mattys, Jusczyk, Luce, & Morgan, 1999), allophonic
variation (Jusczyk, Hohne, & Bauman, 1999), metrical
(stress) patterns (Jusczyk, Houston, & Newsome, 1999;
Morgan, Bonamo, & Travis, 1995), effects of coarticulation
(Johnson & Jusczyk, 2001), and statistical regularities in
the sequences of syllables found in speech (Saffran, Aslin,
& Newport, 1996). This last source of information can be
used in a language-independent way, and seems to be used
by infants earlier than most other cues, by the age of
7 months (Thiessen & Saffran, 2003). These facts have
caused some researchers to propose that strategies based
on statistical sequencing information are a crucial rst step
in bootstrapping word segmentation (Thiessen & Saffran,
2003), and have provoked a great deal of interest in these
strategies (Aslin, Saffran, & Newport, 1998; Saffran, New-
port, & Aslin, 1996; Saffran et al., 1996; Toro, Sinnett, &
Soto-Faraco, 2005). In this paper, we use computational

22

S. Goldwater et al. / Cognition 112 (2009) 2154

modeling techniques to examine some of the assumptions
underlying much of the research on statistical word
segmentation.

Most previous work on statistical word segmentation
is based on the observation that transitions from one syl-
lable or phoneme to the next tend to be less predictable
at word boundaries than within words (Harris, 1955; Saf-
fran et al., 1996). Behavioral research has shown that in-
fants are indeed sensitive to this kind of predictability, as
measured by statistics such as transitional probabilities
(Aslin et al., 1998; Saffran et al., 1996). This research,
however, is agnostic as to the mechanisms by which in-
fants use statistical patterns to perform word segmenta-
tion. A number of researchers in both cognitive science
and computer science have developed algorithms based
on transitional probabilities, mutual
information, and
similar statistics of predictability in order to clarify how
these statistics can be used procedurally to identify words
or word boundaries (Ando & Lee, 2000; Cohen & Adams,
2001; Feng, Chen, Deng, & Zheng, 2004; Swingley,
2005). Here, we take a different approach: we seek to
identify the assumptions the learner must make about
the nature of language in order to correctly segment nat-
ural language input.

Observations about predictability at word boundaries
are consistent with two different kinds of assumptions
about what constitutes a word: either a word is a unit that
is statistically independent of other units, or it is a unit that
helps to predict other units (but to a lesser degree than the
beginning of a word predicts its end). In most articial lan-
guage experiments on word segmentation,
the rst
assumption is adopted implicitly by creating stimuli
through random (or near-random) concatenation of nonce
words. This kind of random concatenation is often neces-
sary for controlled experiments with human subjects,
and has been useful in demonstrating that humans are
sensitive to the statistical regularities in such randomly
generated sequences. However, it obviously abstracts away
from many of the complexities of natural language, where
regularities exist not only in the relationships between
sub-word units, but also in the relationships between
words themselves. We know that humans are able to use
sub-word regularities to begin to extract words; it is natu-
ral to ask whether attending to these kinds of regularities
is sufcient for a statistical learner to succeed with word
segmentation in a more naturalistic setting. In this paper,
we use computer simulations to examine learning from
natural, rather than articial, language input. We ask what
kinds of words are identied by a learner who assumes
that words are statistically independent, or (alternatively)
by a learner who assumes as well that words are predictive
of later words. We investigate this question by developing
two different Bayesian models of word segmentation
incorporating each of these two different assumptions.
These models can be seen as ideal learners: they are de-
signed to behave optimally given the available input data,
in this case a corpus of phonemically transcribed child-di-
rected speech.

Using our ideal learning approach, we nd in our rst
set of simulations that the learner who assumes that words
are statistically independent units tends to undersegment

the corpus, identifying commonly co-occurring sequences
of words as single words. These results seem to conict
with those of several earlier models (Batchelder, 2002;
Brent, 1999; Venkataraman, 2001), where systematic
undersegmentation was not found even when words were
assumed to be independent. However, we argue here that
these previous results are misleading. Although each of
these learners is based on a probabilistic model that de-
nes an optimal solution to the segmentation problem,
we provide both empirical and analytical evidence that
the segmentations found by these learners are not the opti-
mal ones. Rather, they are the result of limitations imposed
by the particular learning algorithms employed. Further
mathematical analysis shows that undersegmentation is
the optimal solution to the learning problem for any rea-
sonably dened model that assumes statistical indepen-
dence between words.

Moving on to our second set of simulations, we nd
that permitting the learner to gather information about
word-to-word dependencies greatly reduces the problem
of undersegmentation. The corpus is segmented in a
much more accurate, adult-like way. These results indi-
cate that, for an ideal learner to identify words based on
statistical patterns of phonemes or syllables, it is impor-
tant to take into account that frequent predictable pat-
terns may occur either within words or across words.
This kind of dual patterning is a result of the hierarchical
structure of language, where predictable patterns occur at
many different levels. A learner who considers predict-
ability at only one level (sub-word units within words)
will be less successful than a learner who considers also
the predictability of larger units (words) within their sen-
tential context. The second, more nuanced interpretation
of the statistical patterns in the input leads to better
learning.

depends

crucially

on

segmentation

Our work has important implications for the under-
standing of human word segmentation. We show that suc-
cessful
the
assumptions that the learner makes about the nature of
words. These assumptions constrain the kinds of infer-
ences that are made when the learner is presented with
naturalistic input. Our ideal learning analysis allows us to
examine the kinds of constraints that are needed to suc-
cessfully identify words, and suggests that infants or young
children may need to account for more subtle statistical ef-
fects than have typically been discussed in the literature.
To date, there is little direct evidence that very young lan-
guage learners approximate ideal learners. Nevertheless,
this suggestion is not completely unfounded, given the
accumulating evidence in favor of humans as ideal learners
in other domains or at other ages (Frank, Goldwater, Mans-
inghka, Grifths, & Tenenbaum, 2007; Schulz, Bonawitz, &
Grifths, 2007; Xu & Tenenbaum, 2007). In order to further
examine whether infants behave as ideal learners, or the
ways in which they depart from the ideal, it is important
to rst understand what behavior to expect from an ideal
learner. The theoretical results presented here provide a
characterization of this behavior, and we hope that they
will provide inspiration for future experimental work
investigating the relationship between human learners
and ideal learners.

S. Goldwater et al. / Cognition 112 (2009) 2154

23

The remainder of this paper is organized as follows.
First, we briey review the idea of transitional probabilities
and how they relate to the notion of words, and provide
some background on the probabilistic modeling approach
taken here. We draw a distinction between two kinds of
probabilistic model-based systems  those based on maxi-
mum-likelihood and Bayesian estimation  and argue in fa-
vor of the Bayesian approach. We discuss in some detail
the strengths and weaknesses of the Model-Based Dy-
namic Programming (MBDP-1) system, a Bayesian learner
described by Brent (1999). Next, we introduce our own
Bayesian unigram model and learning algorithm, which
address some of the weaknesses of MBDP-1. We provide
the results of simulations using this model and compare
them to the results of previously proposed models. We
then generalize our unigram modeling results using addi-
tional empirical and theoretical arguments, revealing some
deep mathematical similarities between our unigram mod-
el and MBDP-1. Finally, we extend our model to incorpo-
rate bigram dependencies, present the results of this
bigram model, and conclude by discussing the implications
of our work.

2. Words and transitional probabilities

The question of how infants begin to segment words
from continuous speech has inspired a great deal of re-
search over the years (Jusczyk, 1999). While many differ-
ent cues have been shown to be important, here we focus
on one particular cue: statistical regularities in the se-
quences of sounds that occur in natural language. The
idea that word and morpheme boundaries may be discov-
ered through the use of statistical information is not new,
but originally these methods were seen primarily as ana-
lytic tools for linguists (Harris, 1954, 1955). More re-
cently, evidence that infants are sensitive to statistical
dependencies between syllables has lent weight to the
idea that this kind of information may actually be used
by human learners for early word segmentation (Saffran
et al., 1996; Thiessen & Saffran, 2003). In particular, re-
search on statistical word segmentation has focused on
the notion of transitional probabilities between sub-word
units (e.g., segments or syllables). The transitional proba-
bility from (say) syllable x to syllable y is simply the con-
ditional probability of y given x. In natural language, there
is a general tendency towards lower transitional probabil-
ities at word boundaries than within words (Harris, 1954;
Saffran et al., 1996), a tendency which infants seem able
to exploit in order to segment word-like units from con-
tinuous speech (Aslin et al., 1998; Saffran et al., 1996).
While other cues are also important for word segmenta-
tion, and may in fact take precedence over transitional
probabilities in older infants, transitional probabilities
seem to be one of the earliest cues that infants are able
to use for this task (Johnson & Jusczyk, 2001; Thiessen
& Saffran, 2003).

Much of the experimental work devoted to studying
word segmentation and related linguistic tasks has fo-
cused on exploring the kinds of statistical information
that human learners are or are not sensitive to, e.g., tran-
sitional probabilities vs. frequencies (Aslin et al., 1998),

syllables vs. phonemes (Newport, Weiss, Aslin, & Wonna-
cott,
in preparation), adjacent vs. non-adjacent depen-
dencies (Newport & Aslin, 2004), and the ways in
which transitional probabilities interact with other kinds
of cues (Johnson & Jusczyk, 2001; Thiessen & Saffran,
2003, 2004). In addition, many researchers have explored
the extent to which word segmentation based on transi-
tional probabilities can be viewed as a special case of
more general pattern- or sequence-learning mechanisms
that operate over a range of cognitive domains (Creel,
Newport, & Aslin, 2004; Fiser & Aslin, 2002). A question
that has received less explicit attention is how the notion
of
transitional probabilities relates to the notion of
words. Transitional probabilities are a property of the
boundaries between words (or units within words), but
ultimately it is the words themselves, rather than the
boundaries, that are of interest to the language learner/
user.
It behooves us, then, to consider what possible
properties of words (or, more accurately, word se-
quences) could give rise to the patterns of transitional
probabilities that are typically discussed in the literature,
i.e. lower probabilities at word boundaries and higher
probabilities within words.

Given a lexicon of words, one way that the standard
patterns of transitional probabilities can arise is by choos-
ing words independently at random from the lexicon and
stringing them together to form a sequence. To a rst
approximation, this is the procedure that is typically used
to generate stimuli for most of the experiments mentioned
above.1 There are many good reasons to generate experi-
mental stimuli in this way, especially when the focus of re-
search is on transitional probabilities: choosing words at
random controls for possible ordering effects and other con-
founds, and leads to simple and systematic patterns of tran-
sitional probabilities. However, there is clearly another way
we could generate a sequence of words, by choosing each
word conditioned on the previous word or words. Depend-
ing on the strength of the word-to-word dependencies, tran-
sitional probabilities between words may be low or high. In
general, if the strength of the dependencies between words
is variable, then in a non-independent sequence of words,
word boundaries will still tend to be associated with lower
transitional probabilities (since many pairs of words will
not be highly dependent). However, there will also be word
boundaries with relatively high transitional probabilities
(where two words are highly associated, as in rubber ducky
or thats a).

The models that we develop in this paper are designed
to examine the results of making these two different
assumptions about the nature of language: that words
are statistically independent units, or that they are predic-
tive units. In thinking about the differences between learn-
ers making each of these two kinds of assumptions, we
frame the issue in terms of the space of linguistic hypoth-

1 The words in experimental stimuli are never chosen completely
independently, due to restrictions against immediate repetition of words.
When the lexicon is small, this leads to signicant deviations from
independence. However, as the lexicon size grows, sequences without
repetition will become more and more similar to truly independent
sequences.

24

S. Goldwater et al. / Cognition 112 (2009) 2154

eses (loosely, grammars) that each learner considers. No-
tice that a learner who assumes that utterances are formed
from sequences of independently chosen words is more re-
stricted than a learner who assumes that words may pre-
dict other words. The second learner is able to learn
grammars that describe either predictive or non-predictive
sequences of words, while the rst learner can only learn
grammars for non-predictive sequences of words. If words
are truly independent, then the rst learner may have an
advantage due to the presence of the stronger constraint,
because this learner has a much smaller space of hypothe-
ses to consider. On the other hand, the second learner will
have an advantage in the case where words are not inde-
pendent, because the learner who assumes independence
will never be able to converge on the correct hypothesis.
Before describing our implementation of these two kinds
of learners, we rst outline our general approach and pro-
vide a summary of related work.

2.1. Probabilistic models for word segmentation

Behavioral work in the vein of Saffran et al. (1996) has
provided a wealth of information regarding the kinds of
statistics human learners are sensitive to, at what ages,
and to what degree relative to other kinds of segmentation
cues. Computational modeling provides a complementary
method of investigation that can be used to test specic
hypotheses about how statistical information might be
used procedurally to identify word boundaries or what
underlying computational problem is being solved. Using
the terminology of Marr (1982), these two kinds of ques-
tions can be investigated by developing models at (respec-
tively) the algorithmic level or computational level of the
acquisition system. Typically, researchers investigate algo-
rithmic-level questions by implementing algorithms that
are believed to incorporate cognitively plausible mecha-
nisms of information processing. Algorithmic-level ap-
proaches to word segmentation include a variety of
neural network models (Allen et al., 1996; Cairns & Shill-
cock, 1997; Christiansen, Allen, & Seidenberg, 1998; Elman,
1990) as well as several learning algorithms based on tran-
sitional probabilities, mutual information, and similar sta-
tistics (Ando & Lee, 2000; Cohen & Adams, 2001; Feng
et al., 2004; Swingley, 2005) (with most of the latter group
coming from the computer science literature).

In contrast to these proposals, our work provides a
computational-level analysis of the word segmentation
problem. A computational-level approach focuses on
identifying the problem facing the learner and determin-
ing the logic through which it can be solved. For problems
of induction such as those facing the language learner,
probability theory provides a natural
framework for
developing computational-level models. A probabilistic
is a set of declarative mathematical statements
model
specifying the goals of the learning process and the kinds
of information that will be used to achieve them. Of
course, these declarative statements must be paired with
some algorithm that can be used to achieve the specic
goal, but generally the algorithm is not seen as the focus
of research. Rather, computational-level investigations of-
ten take the form of ideal learner analyses, examining the

behavior of a learner who behaves optimally given the
assumptions of the model.2

Very generally, we can view the goal of a language lear-
ner as identifying some abstract representation of the ob-
served data (e.g., a grammar)
that will allow novel
linguistic input to be correctly interpreted, and novel out-
put to be correctly produced. Many different representa-
tions are logically possible, so the learner must have
some way to determine which representation is most
likely to be correct. Probabilistic models provide a natural
way to make this determination, by creating a probability
distribution over different hypothesized representations
given the observed data. A learner who is able to correctly
identify this posterior distribution over hypotheses can use
this information to process future input and output in an
optimal way (i.e., in a way that is as similar as possible
to the correct generating process  the adult grammar).
Under this view, then, the posterior distribution over
grammars is the outcome of the learning process.

How does the learner go about identifying the posterior
distribution? Bayes rule tells us that the probability of a
hypothesized grammar h given the observed data d can
be computed as

P
Phjd  PdjhPh
h0 Pdjh0Ph0

where the sum in the denominator ranges over all hypoth-
eses h0 within the hypothesis space. Here, Pdjh (known as
the likelihood) is the probability of the observed data given
a particular hypothesis, and tells us how well that hypoth-
esis explains the data. Ph (the prior probability of h) tells
us how good a linguistic hypothesis h is, regardless of any
data. The prior can be viewed as a learning bias or measure
of linguistic naturalness: hypotheses with high prior prob-
ability may be adopted based on less evidence than
hypotheses with low prior probability. Bayes rule states
that the posterior probability Phjd is proportional to the
product of the likelihood and the prior, with the denomina-
tor acting as a normalizing constant to ensure that Phjd
sums to one over all hypotheses. The learner can compute
the posterior probabilities of different hypotheses by eval-
uating each one according to its explanatory power (likeli-
hood) and the learners prior expectations.

Bayes rule answers the question of how to determine
the posterior probability of a hypothesis if the prior prob-
ability and likelihood are known, but it does not tell us
how to compute those terms in the rst place. We turn rst
to the calculation of the likelihood. Typically, the likelihood
is computed by dening a generative model: a probabilistic

2 A note on terminology: the word model unfortunately encompasses
two related but (importantly) distinct senses. It can be used to describe
either (1) a proposal about the nature of learning or its implementation (as
in connectionist model, exemplar model); or (2) a specic mathemat-
ical statement regarding the process generating a set of data (as in
probabilistic model, generative model). A probabilistic model (second
sense) together with its learning algorithm can be viewed as an instance of
a learning model (rst sense). To avoid confusion, we will generally use the
term model only for the second sense, and the terms system or
learner to describe the fully implemented combination of a probabilistic
model and learning algorithm.

S. Goldwater et al. / Cognition 112 (2009) 2154

25

process for generating the observed data given the hypoth-
esis under consideration. As a simple non-linguistic exam-
ple, imagine the data d consists of the results of 10 coin
ips and we wish to determine the probability of d given
hypotheses h that differ in the probability of ipping heads.
We assume a generative model in which each observation
is the result of ipping the same coin, and that coin always
has probability p of landing on heads independently of any
previous outcomes. The set of hypotheses under consider-
ation consists of all possible values for p. We therefore
have Pdjh  p  pnH1  pnT , where nH and nT are the
number of heads and tails observed in a particular se-
quence of ips.

2.1.1. Maximum-likelihood estimation

A standard method of using a probabilistic generative
model for learning is to perform maximum-likelihood esti-
mation, i.e., to choose the hypothesis ^h that maximizes
the value of the likelihood function. This is equivalent to
choosing the hypothesis with maximum posterior proba-
bility, assuming a uniform prior distribution over hypoth-
eses with respect to the given parameterization. In the
coin ip example, it is easy to show using elementary cal-
culus that the likelihood is maximized when h  nH
.
nHnT
Thus, if we observe a sequence consisting of four tails
and six heads, the maximum-likelihood estimate for h is
^h  :6.

For more complex generative models such as those typ-
ically used in language modeling, it is usually impossible to
identify the maximum-likelihood hypothesis analytically.
If the number of possible hypotheses is small, it may be
feasible to explicitly compute the likelihood for each possi-
ble hypothesis and choose the best one. However, in gen-
eral, it will be necessary to design some sort of algorithm
for searching through the space of hypotheses without
evaluating all of them. The ideal algorithm would be one

that is guaranteed to nd the globally optimal hypothesis.
In many cases, however, approximate search algorithms are
used. These algorithms generally work by seeking the opti-
mal hypothesis within some local region of the search
space. Approximate search algorithms may be used for
practical reasons (when an exact procedure is not known,
as in Anderson (1991)) or for theoretical reasons (if the re-
searcher wishes to incorporate particular assumptions
about human learning, as in Sanborn, Grifths, & Navarro
(2006)). In either case, certain hypotheses are excluded
from consideration by the algorithm itself. Consequently,
the use of an approximate search procedure can make a
purely computational-level analysis difcult. The kinds of
generalizations made by the learner are determined both
by the explicit constraints specied by the probabilistic
model, and the implicit constraints specied by the search
procedure. Examples of this type of learning system are de-
scribed by Venkataraman (2001) and Batchelder (2002).
The models underlying these systems are very similar;
we describe only Venkataramans work in detail.

Venkataraman proposes a method of word segmenta-
tion based on maximum-likelihood estimation. He dis-
cusses three different generative models of increasing
complexity; we focus our analysis on the simplest of these,
although our argument can be extended to all three. This
model is a standard unigram model, i.e., it assumes that
words are generated independently at
random. The
observed data consists of a corpus of phonemically tran-
scribed child-directed speech, where utterance boundaries
(corresponding to pauses in the input) are known, but
word boundaries are unknown (see Fig. 1). The probabilis-
tic model underlying this system describes how to gener-
ate a corpus given U, the number of utterances in the
corpus; the distinguished symbol $, which is used to mark
utterance boundaries; and R, the phonemic symbol alpha-
bet (which does not include $):

Fig. 1. An excerpt from the beginning of the corpus used as input to Venkataramans (2001) word segmentation system, showing (a) the actual input corpus
and (b) the corresponding standard orthographic transcription. The corpus was originally prepared by Brent and Cartwright (1996) using data from
Bernstein-Ratner (1987), and was also used as input to Brents (1999) MBDP-1 system.

26

S. Goldwater et al. / Cognition 112 (2009) 2154

Repeat U times:

Repeat until $ is generated:
1. Generate the next word, w, with probability Pww.
2. Generate $ with probability p$.

where Pw is some probability distribution over R, the set
of all possible words. As each word is generated, it is con-
catenated onto the previously generated sequence of
words. No boundary marker is added unless the end-of-
utterance marker, $, is generated. Under this model, the
probability of generating words w1 . . . wn as a single utter-
ance is

Pw1 . . . wn$ 

Pwwnp$

"
Y

n1

#
Pwwi1  p$
Y

i1
 p$
1  p$

n

i1

Pwwi1  p$

1

and the probability of generating the unsegmented utter-
ance u is found by summing over all possible sequences
of words that could be concatenated to form u:
Pu 

Pw1 . . . wn$

X

w1...wnu

The probability of the entire corpus is the product of the
probabilities of the individual utterances. The hypothesis
space for this model consists of all the possible assign-
ments of probability values to words and the utterance
i.e., possible values for p$ and the
boundary marker,
parameters of Pw.

Notice that in the hypothesis space just dened, some
choices of Pw may assign non-zero probability to only a -
nite subset of potential words. Different hypotheses will
have different sizes, i.e., different numbers of words will
have non-zero probability. Crucially, however, no prefer-
ence is given to hypotheses of any particular size  the
maximum-likelihood assumption states that we should
choose whichever hypothesis assigns the highest probabil-
ity to the observed data.

What is the maximum-likelihood hypothesis under this
model? It is straightforward to show that, in general, the
maximum-likelihood solution for a model is the probabil-
ity distribution that is closest to the empirical distribution
(relative frequencies) of observations in the corpus, where
the distance is computed by an information theoretic
measure called the KullbackLeibler divergence (Bishop,
2006, p. 57, inter al.). In the above model, there is one
hypothesis that is able to match the empirical distribution
of the corpus exactly. This hypothesis treats each utterance
as a single word, with probability equal to the empirical
probability of that utterance in the corpus, and assumes
p$  1. In other words, this solution memorizes the entire
data set without segmenting utterances at all, and assigns
zero probability to any unobserved utterance. Intuitively,
any solution that does hypothesize word boundaries will
require p$ < 1, which means that some unobserved utter-
ances will have non-zero probability  those that can be
created by, for example, concatenating two observed utter-
ances, or rearranging the hypothesized words into novel

orderings. Since some probability mass is allocated to
these unobserved utterances, the probability of the ob-
served data must be lower than in the case where p$  1
and no generalization is possible.

For a maximum-likelihood learner using the model in
Eq. (1), then, only a trivial segmentation will be found un-
less some constraint is placed on the kinds of hypotheses
that are considered. Crucially, however, this argument
does not depend on the particular form of Pw used in Eq.
(1), where words are assumed to be generated indepen-
dent of context. Many other possible distributions over
words would yield the same result. Venkataraman, for
example, presents two other models in which the unigram
distribution Pw is replaced with a bigram or trigram distri-
bution: rather than generating words independent of con-
text, each word is generated conditioned on either one or
two previous words. That is, the bigram model denes

"
Y
Pw1 . . . wn$  Pw1j$

n

#
P$jwn
Pwijwi1

i2

lexical

Essentially, the reason that all of these models yield the
same maximum-likelihood solution (an unsegmented cor-
pus) is that they are allowed to consider hypotheses with
arbitrary numbers of
items. When comparing
hypotheses with different levels of complexity (corre-
sponding here to the number of word types in the hypoth-
esis), a maximum-likelihood learner will generally prefer a
more complex hypothesis over a simple one. This leads to
the problem of overtting, where the learner chooses a
hypothesis that ts the observed data very well, but gener-
alizes very poorly to new data. In the case of word segmen-
tation, the solution of complete memorization allows the
learner to t the observed data perfectly. Since we know
that this is not the solution found by Venkataramans
learners, we must conclude that the algorithm he proposes
to search the space of possible hypotheses must be impos-
ing additional constraints beyond those of the models
themselves. It should be clear from the previous discussion
that this is not the approach advocated here, since it ren-
ders constraints implicit and difcult to examine. Batchel-
ders (2002) maximum-likelihood learning system uses an
explicit external constraint to penalize lexical items that
are too long. This approach is a step in the right direction,
but is less mathematically principled than Bayesian model-
ing,
in which a (non-uniform) prior distribution over
hypotheses is used within the model itself to constrain
learning. We now review several of the Bayesian models
that served as inspiration for our own work.

2.1.2. Bayesian models

In the previous section, we argued that unconstrained
maximum-likelihood estimation is a poor way to choose
between hypotheses with different complexities. In Bayes-
ian modeling, the effect of the likelihood can be counter-
balanced by choosing a prior distribution that favors
simpler hypotheses. Simpler hypotheses will tend not to
t the observed data as well, but will tend to generalize
more successfully to novel data. By considering both the
likelihood and prior in determining the posterior probabil-
ity of each hypothesis, Bayesian learners naturally avoid

S. Goldwater et al. / Cognition 112 (2009) 2154

27

the kind of overtting that maximum-likelihood learners
encounter. The trade-off between t and generalization
will depend on exactly how the prior is dened; we now
describe several methods that have been used to dene
priors in previous Bayesian models.

Perhaps the most well-known framework for dening
Bayesian models is known as minimum description length
(MDL) (Rissanen, 1989), and is exemplied by the work
of de Marcken (1995) and Brent and Cartwright (1996).
MDL is a particular formulation of Bayesian learning that
has been used successfully in a number of other areas of
language acquisition as well (Creutz & Lagus, 2002; Dow-
man, 2000; Ellison, 1994; Goldsmith, 2001; Goldwater &
Johnson, 2004). The basic idea behind MDL is to dene
some encoding scheme that can be used to encode the cor-
pus into a more compact representation. In word segmen-
tation, for example, a code might consist of a list of lexical
items along with a binary representation for each one.
With appropriate choices for the lexical items and binary
representations (with shorter representations assigned to
more common words), the length of the corpus could be
reduced by replacing each word with its binary code. In
this framework, the learners hypotheses are different pos-
sible encoding schemes. The minimum description length
principle states that the optimal hypothesis is the one that
minimizes the combined length, in bits, of the hypothesis it-
self (the codebook) and the encoded corpus. Using results
from information theory, it can be shown that choosing a
hypothesis using the MDL principle is equivalent to choos-
ing the maximum a posteriori (MAP) hypothesis  the
hypothesis with the highest posterior probability  under
a Bayesian model where the prior probability of a hypoth-
esis decreases exponentially with its length.
In other
words, MDL corresponds to a particular choice of prior dis-
tribution over hypotheses, where hypotheses are preferred
if they can be described more succinctly.

Although MDL models can in principle produce good
word segmentation results, there are no standard search
algorithms for these kinds of models, and it is often dif-
cult to design efcient model-specic algorithms. For
example, Brent and Cartwright (1996) were forced to limit
their analysis to a very short corpus (about 170 utterances)
due to efciency concerns. In later research, Brent devel-
oped another Bayesian model for word segmentation with
a more efcient search algorithm (Brent, 1999). He named
this system Model-Based Dynamic Programming (MBDP-
1).3 Since we will be returning to this model at various
points throughout this paper, we now describe MBDP-1 in
more detail.

Unlike models developed within the MDL framework,
where hypotheses correspond to possible encoding meth-
ods, MBDP-1 assumes that the hypotheses under consider-
ation are actual sequences of words, where each word is a
sequence of phonemic symbols. The input corpus consists
of phonemically transcribed utterances of child-directed
speech, as in Fig. 1. Some word sequences, when concate-
nated together to remove word boundaries, will form ex-

3 The 1 in MBDP-1 was intended as a version number, although Brent

never developed any later versions of the system.

actly the string of symbols found in the corpus, while
others will not. The probability of the observed data given
a particular hypothesized sequence of words will therefore
either be equal to 1 (if the concatenated words form the
corpus) or 0 (if not). Consequently, only hypotheses that
are consistent with the corpus must be considered. For
each possible segmentation of the corpus, the posterior
probability of that segmentation will be directly propor-
tional to its prior probability. The prior probability, in turn,
is computed using a generative model. This model assumes
that the sequence of words in the corpus was created in a
sequence of four steps:4

Step 1: Generate the number of types that will be in the

lexicon.

Step 2: Generate a token frequency for each lexical type.
Step 3: Generate the phonemic representation of each
type (except for the single distinguished utter-
ance boundary type, $).

Step 4: Generate an ordering for the set of tokens.

Each step in this process is associated with a probability
distribution over the possible outcomes of that step, so
together these four steps dene the prior probability distri-
bution over all possible segmented corpora. We discuss the
specic distributions used in each step in Appendix B; here
it is sufcient to note that these distributions tend to
assign higher probability to segmentations containing
fewer and shorter lexical items, so that the learner will
prefer to split utterances into words.

To search the space of possible segmentations of the
corpus, Brent develops an efcient online algorithm. The
algorithm makes a single pass through the corpus, seg-
menting one utterance at a time based on the segmenta-
tions found for all previous utterances. The online nature
of this algorithm is intended to provide a more realistic
simulation of human word segmentation than earlier batch
learning algorithms (Brent & Cartwright, 1996; de Marc-
ken, 1995), which assume that the entire corpus of data
is available to the learner at once (i.e., the learner may iter-
ate over the data many times).

In the remainder of this paper, we will describe two
new Bayesian models of word segmentation inspired, in
part, by Brents work. Like Brent, we use a generative mod-
el-based Bayesian framework to develop our learners.
Moreover, as we prove in Appendix B, our rst (unigram)
model is mathematically very similar to the MBDP-1 mod-
el. However, our work differs from Brents in two respects.
First, our models are more exible, which allows us to
more easily investigate the effects of different modeling
assumptions. In theory, each step of Brents model can be
individually modied, but in practice the mathematical
statement of the model and the approximations necessary
for the search procedure make it difcult to modify the
model in any interesting way. In particular, the fourth step
assumes a uniform distribution over orderings, which

4 Our presentation involves a small change from Brent (1999), switching
the order of Steps 2 and 3. This change makes no difference to the model,
but provides a more natural grouping of steps for purposes of our analysis
in Appendix B.

28

S. Goldwater et al. / Cognition 112 (2009) 2154

creates a unigram constraint that cannot easily be changed.
We do not suppose that Brent was theoretically motivated
in his choice of a unigram model, or that he would be op-
posed to introducing word-to-word dependencies, merely
that the modeling choices available to him were limited
by the statistical techniques available at the time of his
work. In this paper, we make use of more exible recent
techniques that allow us to develop both unigram and bi-
gram models of word segmentation and explore the differ-
ences in learning that result.

The second key contribution of this paper lies in our fo-
cus on analyzing the problem of word segmentation at the
computational level by ensuring, to the best of our ability,
that the only constraints on the learner are those imposed
by the model itself. We have already shown that the mod-
el-based approaches of Venkataraman (2001) and Batchel-
der (2002) are constrained by their choice of search
algorithms; in the following section we demonstrate that
the approximate search procedure used by Brent (1999)
prevents his learner, too, from identifying the optimal
solution under his model. Although in principle one could
develop a Bayesian model within the MDL or MBDP frame-
works that could account for word-to-word dependencies,
the associated search procedures would undoubtedly be
even more complex than those required for the current
unigram models, and thus even less likely to identify opti-
mal solutions. Because our own work is based on more re-
cent Bayesian techniques, we are able to develop search
procedures using a standard class of algorithms known as
Markov chain Monte Carlo methods (Gilks, Richardson, &
Spiegelhalter, 1996), which produce samples from the pos-
terior distribution over hypotheses. We provide evidence
that the solutions identied by our algorithms are indeed
optimal or near-optimal, which allows us to draw conclu-
sions using ideal observer arguments and to avoid the
obfuscating effects of ad hoc search procedures.

3. Unigram model

3.1. Generative model

Like MBDP-1, our models assume that the hypotheses
under consideration by the learner are possible segmenta-
tions of the corpus into sequences of words. Word se-
quences that are consistent with the corpus have a
likelihood of 1, while others have a likelihood of 0, so the
posterior probability of a segmentation is determined by
its prior probability. Also as in MBDP-1, we compute the
prior probability of a segmentation by assuming that the
sequence of words in the segmentation was created
according to a particular probabilistic generative process.
Let w  w1 . . .w N be the words in the segmentation. Set-
ting aside the complicating factor of utterance boundaries,
our unigram model assumes that the ith word in the se-
quence, wi, is generated as follows:

(1) Decide if wi is a novel lexical item.
(2) a.
x1 . . .x M) for wi.

If so, generate a phonemic form (phonemes

b. If not, choose an existing lexical form  for wi.

We assign probabilities to each possible choice as follows:

Q
, Pwi is not novel  n
(1) Pwi is novel  a0
na0
na0
(2) a. Pwi  x1 . . .x M j wi is novel 
j1Pxj
b. Pwi   j wi is not novel  n

p#1  p#M1

M

n

where a0 is a parameter of the model, n is the number of
previously generated words ( i  1), n is the number of
times lexical item  has occurred in those n words, and p#
is the probability of generating a word boundary. Taken
together, these denitions yield the following distribution
over wi given the previous words wi  fw1 . . .w i1g:
Pwi  jwi 

 a0P0wi  
i  1  a0

i  1  a0

2

n

where we use P0 to refer to the unigram phoneme distribu-
tion in Step 2a. (The p# and 1  p# factors in this distribu-
tion result from the process used to generate a word from
constituent phonemes: after each phoneme is generated, a
word boundary is generated with probability p# and the
process ends, or else no word boundary is generated with
probability 1  p# and another phoneme is generated.)

We now provide some intuition for the assumptions
that are built into this model. First, notice that in Step 1,
when n is small, the probability of generating a novel
lexical item is relatively large. As more word tokens are
generated and n increases, the relative probability of
generating a novel item decreases, but never disappears
entirely. This part of the model means that segmentations
with too many different lexical items will have low proba-
bility, providing pressure for the learner to identify a seg-
mentation consisting of relatively few lexical items. In
Step 2a, we dene the probability of a novel lexical item
as the product of the probabilities of each of its phonemes.
This ensures that very long lexical items will be strongly
dispreferred. Finally, in Step 2b, we say that the probability
of generating an instance of the lexical item  is propor-
tional to the number of times  has already occurred. In ef-
fect, the learner assumes that a few lexical items will tend
to occur very frequently, while most will occur only once
or twice. In particular, the distribution over word frequen-
cies produced by our model becomes a power-law distri-
bution for large corpora (Arratia, Barbour, & Tavare,
1992), the kind of distribution that is found in natural lan-
guage (Zipf, 1932).

The model we have just described is an instance of a
kind of model known in the statistical literature as a Dirich-
let process (Ferguson, 1973). The Dirichlet process is com-
monly used in Bayesian statistics as a non-parametric
prior for clustering models, and is closely related to Ander-
sons (1991) rational model of categorization (Sanborn
et al., 2006). The Dirichlet process has two parameters:
the concentration parameter a0 and the base distribution
P0. The concentration parameter determines how many
clusters will typically be found in a data set of a particular
size (here, how many word types for a particular number
of tokens), and the base distribution determines the typical
characteristics of a cluster (here, the particular phonemes
in a word type). A more detailed mathematical treatment

S. Goldwater et al. / Cognition 112 (2009) 2154

29

of our model and its relationship to the Dirichlet process is
provided in Appendix A, but this connection leads us to re-
fer to our unigram model of word segmentation as the
Dirichlet process (DP) model.

So far, the model we have described assigns probabili-
ties to sequences of words where there are no utterance
boundaries. However, because the input corpus contains
utterance boundaries, we need to extend the model to ac-
count for them. In the extended model, each hypothesis
consists of a sequence of words and utterance boundaries,
and hypotheses are consistent with the input if removing
word boundaries (but not utterance boundaries) yields
the input corpus. To compute the probability of a sequence
of words and utterance boundaries, we assume that this
sequence was generated using the model above, with the
addition of an extra step: after each word is generated,
an utterance boundary marker $ is generated with proba-
bility p$ (or not, with probability 1  p$). For simplicity,
we will suppress this portion of the model in the main
body of this paper, and refer the reader to Appendix A for
full details.

3.2. Inference

We have now dened a generative model that allows
us to compute the probability of any segmentation of the
input corpus. We are left with the problem of inference,
or actually identifying the highest probability segmenta-
tion from among all possibilities. We used a method
known as Gibbs sampling (Geman & Geman, 1984), a type
of Markov chain Monte Carlo algorithm (Gilks et al.,
1996) in which variables are repeatedly sampled from
their conditional posterior distribution given the current
values of all other variables in the model. Gibbs sampling
is an iterative procedure in which (after a number of
iterations used as a burn-in period to allow the sam-
pler to converge) each successive iteration produces a
sample from the full posterior distribution Phjd.
In
our sampler, the variables of interest are potential word
boundaries, each of which can take on two possible val-
ues, corresponding to a word boundary or no word
boundary. Boundaries may be initialized at random or
using any other method; initialization does not matter
since the sampler will eventually converge to sampling
from the posterior distribution.5 Each iteration of the
sampler consists of
stepping through every possible
boundary location and resampling its value conditioned
on all other current boundary placements. Since each set
of assignments to the boundary variables uniquely deter-
mines a segmentation, sampling boundaries is equivalent
to sampling sequences of words as our hypotheses.
Although Gibbs sampling is a batch learning algorithm,
where the entire data set is available to the learner at
once, we note that there are other sampling techniques
known as particle lters
(Doucet, Andrieu, & Godsill,
2000; Sanborn et al., 2006) that can be used to produce

5 Of course, our point that initialization does not matter is a theoretical
one; in practice, some initializations may lead to faster convergence than
others, and checking that different initializations lead to the same results is
one way of testing for convergence of the sampler, as we do in Appendix A.

approximations of the posterior distribution in an online
fashion (examining each utterance in turn exactly once).
We return in the General Discussion to the question of
how a particle lter might be developed for our own
model in the future. Full details of our Gibbs sampling
algorithm are provided in Appendix A.

3.3. Simulations

3.3.1. Data

To facilitate comparison to previous models of word
segmentation, we report results on the same corpus used
by Brent (1999) and Venkataraman (2001). The data is de-
rived from the BernsteinRatner corpus (Bernstein-Rat-
ner, 1987) of the CHILDES database (MacWhinney &
Snow, 1985), which contains orthographic transcriptions
of utterances directed at 13- to 23-month-olds. The data
was post-processed by Brent, who removed disuencies
and non-words, discarded parental utterances not direc-
ted at the children, and converted the rest of the words
into a phonemic representation using a phonemic dictio-
nary (i.e. each orthographic form was always given the
same phonemic form). The resulting corpus contains
9790 utterances, with 33,399 word tokens and 1321 un-
ique types. The average number of words per utterance
is 3.41 and the average word length (in phonemes) is
2.87. The word boundaries in the corpus are used as the
gold standard for evaluation, but are not provided in the
input to the system (except for word boundaries that
are also utterance boundaries).

The process used to create this corpus means that it is
missing many of the complexities of real child-directed
speech. Not the least of these is the acoustic variability
with which different tokens of the same word are pro-
duced, a factor which presumably makes word segmenta-
tion more difcult. On the other hand, the corpus is also
missing many cues which could aid in segmentation, such
as coarticulation information, stress, and duration. While
this idealization of child-directed speech is somewhat
unrealistic, the corpus does provide a way to investigate
the use of purely distributional cues for segmentation,
and permits direct comparison to other word segmenta-
tion systems.

3.3.2. Evaluation procedure

For quantitative evaluation, we adopt the same mea-
sures used by Brent (1999) and Venkataraman (2001): pre-
cision (number of correct items found out of all items
found) and recall (number of correct items found out of
all correct items). These measures are widespread in the
computational linguistics community; the same measures
are often known as accuracy and completeness in the cogni-
tive science community (Brent & Cartwright, 1996; Chris-
tiansen et al., 1998). We also report results in terms of
F0 (another common metric used in computational lin-
guistics, also known as F-measure or F-score). F0 is the geo-
metric average of precision and recall, dened as
2  precision
, and penalizes results where precision and
recall are very different. We report the following scores
for each model we propose:

precisionrecall

recall

30

S. Goldwater et al. / Cognition 112 (2009) 2154

aries must be correctly identied to count as correct.

 P, R, F: precision, recall, and F0 on words: both bound-
 LP, LR, LF: precision, recall, and F0 on the lexicon, i.e.
 BP, BR, BF: precision, recall, and F0 on potentially ambig-
uous boundaries (i.e. utterance boundaries are not
included in the counts).

word types.

As an example, imagine a one-utterance corpus whose cor-
rect segmentation is look at the big dog there, where
instead we nd the segmentation look at the bigdo g
the re. There are seven words in the found segmentation,
and six in the true segmentation; three of these words
match. We report all scores as percentages, so P = 42.9%
(3/7), R = 50.0% (3/6), and F = 46.2%. Similarly, BP = 66.7%
(4/6), BR = 80.0% (4/5), BF = 72.7%,
LP = 50.0% (3/6),
LR = 50.0% (3/6), and LF = 50.0%. Note that if the learner
correctly identies all of the boundaries in the true solu-
tion, but also proposes extra boundaries (oversegmenta-
tion), then boundary recall will reach 100%, but boundary
precision and boundary F0 will be lower. Conversely, if
the learner proposes no incorrect boundaries, but fails to
identify all of the true boundaries (undersegmentation),
then boundary precision will be 100%, but boundary recall
and F0 will be lower. In either case, scores for word tokens
and lexical items will be below 100%.

For comparison, we report scores as well for Brents
MBDP-1 system (Brent, 1999) and Venkataramans n-gram
segmentation systems (Venkataraman, 2001), which we
will refer to as NGS-u and NGS-b (for the unigram and bi-
gram models). Both Brent and Venkataraman use online
search procedures (i.e., their systems make a single pass
through the data, segmenting each utterance in turn), so
in their papers they calculate precision and recall sepa-
rately on each 500-utterance block of the corpus and graph
the results to show how scores change as more data is pro-
cessed. They do not report lexicon recall or boundary pre-
cision and recall. Their results are rather noisy, but
performance seems to stabilize rapidly, after about 1500
utterances. To facilitate comparison with our own results,
we calculated scores for MBDP-1 and NGS over the whole
corpus, using Venkataramans implementations of these
algorithms.6

Since our algorithm produces random segmentations
sampled from the posterior distribution rather than a sin-
gle optimal solution, there are several possible ways to
evaluate its performance. For most of our simulations, we
evaluated a single sample taken after 20,000 iterations.
We used a method known as simulated annealing (Aarts &
Korst, 1989) to speed convergence of the sampler, and in
some cases (noted below) to obtain an approximation of
the MAP solution by concentrating samples around the
mode of the posterior. This allowed us to examine possible
differences between a random sample of the posterior and
a sample more closely approximating the MAP segmenta-
tion. Details of the annealing and MAP approximation pro-
cedures can be found in Appendix A.

6 The implementations are available at http://www.speech.sri.com/

people/anand/.

(a) Varying p# with 

0 = 20

0.3

0.5

value of p#

0.7

0.9

(b) Varying 

0 with p# = .5

60%

55%

50%

60%

55%

50%

LF
F
0.1

LF
F

1

2

5

10

50
20
value of 
0

100 200

500

Fig. 2. F0 for words (F) and lexical items (LF) in the DP model (a) as a
function of p#, with a0  20 and (b) as a function of a0, with p#  :5.

3.3.3. Results and discussion

The DP model we have described has two free parame-
ters: p# (the prior probability of a word boundary), and a0
(which affects the number of word types proposed).7 Fig. 2
shows the effects of varying of p# and a0. Lower values of p#
result in more long words, which tends to improve recall
(and thus F0) in the lexicon. The accompanying decrease in
token accuracy is due to an increasing tendency for the mod-
el to concatenate short words together, a phenomenon we
discuss further below. Higher values of a0 allow more novel
words, which also improves lexicon recall, but begins to de-
grade precision after a point. Due to the negative correlation
between token accuracy and lexicon accuracy, there is no
single best value for either p# or a0. In the remainder of this
section, we focus on the results for p#  :5; a0  20 (though
others are qualitatively similar  we discuss these briey
below).

In Table 1, we compare the results of our system to
those of MBDP-1 and NGS-u. Although our system has
higher lexicon accuracy than the others, its token accuracy
is much worse. Performance does not vary a great deal be-
tween different samples, since calculating the score for a
single sample already involves averaging over many ran-
dom choices  the choices of whether to place a boundary
at each location or not. Table 2 shows the mean and stan-
dard deviation in F0 scores and posterior probabilities over
samples taken from 10 independent runs of the algorithm
with different random initializations. The same statistics

7 The DP model actually contains a third free parameter, q, used as a
prior over the probability of an utterance boundary (see Appendix A). Given
the large number of known utterance boundaries, the value of q should
have little effect on results, so we simply xed q  2 for all simulations.

S. Goldwater et al. / Cognition 112 (2009) 2154

Table 1
Word segmentation accuracy of unigram systems.

Model

Performance measure

NGS-u
MBDP-1
DP

P

67.7
67.0
61.9

R

70.2
69.4
47.6

F

68.9
68.2
53.8

BP

80.6
80.3
92.4

BR

84.8
84.3
62.2

BF

82.6
82.3
74.3

LP

52.9
53.6
57.0

LR

51.3
51.3
57.5

31

LF

52.0
52.4
57.2

Note: P, R, and F are precision, recall, and F0 for word tokens; BP, LP, etc. are the corresponding scores for ambiguous boundaries and lexical items. Best
scores are shown in bold. DP results are with p#  :5 and a0  20.

Table 2
Results of the DP model, averaged over multiple samples.

Samples (10 runs)
Samples (1 run)
MAP approx.

F

53.9 (.32)
53.5 (.07)
53.7 (.26)

LF

57.8 (.60)
57.7 (.43)
58.7 (.56)

 log Pw
200,587 (192)
200,493 (25)
199,853 (228)

Note: Token F0 (F), lexicon F0 (LF), and negative log posterior probability
were averaged over 10 samples from independent runs of our Gibbs
sampler, over 10 samples from a single run, and over 10 samples from
independent runs of our MAP approximation (see text). Standard devia-
tions are shown in parentheses.

are also provided for ten samples obtained from a single
run of the sampler. Samples from a single run are not inde-
pendent, so to reduce the amount of correlation between
these samples they were taken at 100-iteration intervals
(at iterations 19100,19200, . . .,20000). Nevertheless, they
show less variability than the truly independent samples.
In both cases, lexicon accuracy is more variable than token
accuracy, probably because there are far fewer lexical
items to average over within a single sample. Finally, Table
2 provides results for the approximate MAP evaluation
procedure. This procedure is clearly imperfect, since if it
were able to identify the true MAP solution, there would
be no difference in results across multiple runs of the algo-
rithm. In fact, compared to the standard sampling proce-
dure, there is only slightly less variation in F0 scores, and
greater variation in probability.8 Nevertheless, the MAP
approximation does succeed in nding solutions with signif-
icantly higher probabilities. These solutions also have higher
lexicon accuracy, although token accuracy remains low.

The reason that token accuracy is so low with the DP
model is that it often mis-analyzes frequently occurring
words. Many instances of these words occur in common
collocations such as whats that and do you, which the sys-
tem interprets as a single words. This pattern of errors is
apparent in the boundary scores: boundary precision is
very high, indicating that when the system proposes a
boundary, it is almost always correct. Boundary recall is
low, indicating undersegmentation.

We analyzed the behavior of the system more carefully
by examining the segmented corpus and lexicon. A full 30%
of the proposed lexicon and nearly 30% of tokens consist of

8 The large standard deviation in the probabilities of the approximate
MAP solutions is due to a single outlier. The standard deviation among the
remaining nine solutions is 160, well below the standard deviation in the
sample solutions, where there are no outliers.

undersegmentation (collocation) errors, while only 12% of
types and 5% of tokens are other non-words. (Some addi-
tional token errors, under 4%, are caused by proposing a
correct word in an incorrect location.) About 85% of collo-
cations (both types and tokens) are composed of two
words, nearly all the rest are three words. To illustrate
the phenomenon, we provide the systems segmentation
of the rst 40 utterances in the corpus in Fig. 3, and the
35 most frequently found lexical items in Fig. 4. The 70
most frequent collocations identied as single words by
the system are shown in Fig. 5.

It is interesting to examine the collocations listed in
Fig. 5 with reference to the existing literature on childrens
early representation of words. Peters (1983), in particular,
provides a number of examples of childrens underseg-
mentation errors (using their productions as evidence).
Several of the full sentences and social conventions in
Fig. 5 (e.g., thank you, thats right, bye bye, look at this) are
included among her examples. In addition, some of the
other collocation errors found by our unigram system
match the examples of formulaic frames given by Peters:
the verb introducer can you and noun introducers its a,
this is, those are, and see the. Phonological reductions in
adult speech also suggest that a few of the collocations
found by the system (e.g., did you, what do you) may even
be treated as single units by adults in some circumstances.
However, the extent and variety of collocations found by
the system is certainly much broader than what research-
ers have so far found evidence for in young children.

We will defer for the moment any further discussion of
whether childrens early word representations are similar
to those found by our DP model (we return to this issue
in the General Discussion), and instead turn to the question
of why these units are found. The answer seems clear:
groups of words that frequently co-occur violate the uni-
gram assumption in the model, since they exhibit strong
word-to-word dependencies. The only way the learner
can capture these dependencies is by assuming that these
collocations are in fact words themselves. As an example,
consider the word that. In our corpus, the empirical proba-
bility of the word that is 798/33399  .024. However, the
empirical probability of that following the word whats is
far higher: 263/569  .46. Since the strong correlation be-
tween whats and that violates the independence assump-
tion of the model, the learner concludes that whatsthat
must be a single word.

Note that by changing the values of the parameters a0
and p#, it is possible to reduce the level of undersegmenta-
tion, but only slightly, and at the cost of introducing other

32

S. Goldwater et al. / Cognition 112 (2009) 2154

Fig. 3. The rst 40 utterances in the corpus as segmented by the DP model (represented orthographically for readability), illustrating that the model
undersegments the corpus. The stochastic nature of the Gibbs sampling procedure is apparent: some sequences, such as youwantto and getit, receive two
different analyses.

errors. For example, raising the value of p# to 0.9 strongly
increases the models preference for short lexical items,
but collocations still make up 24% of both types and tokens
in this case. Measures of token accuracy increase by a few
points, but are still well below those of previous systems.
The main qualitative difference between results with
p#  :5 and p#  :9 is that with the higher value, infre-
quent words are more likely to be oversegmented into very
short one- or two- phoneme chunks (reected in a drop in
lexicon accuracy). However, frequent words still tend to be
undersegmented as before.

It is also worth noting that, although the proportion of
collocations in the lexicons found by MBDP-1 and NGS-u
is comparable to the proportion found by our own model
(24%), only 6% of tokens found by these systems are collo-
cation errors. This fact seems to contradict our analysis of
the failures of our own unigram model, and raises a num-
ber of questions. Why dont these other unigram models
exhibit the same problems as our own? Is there some other
weakness in our model that might be causing or com-
pounding the problems with undersegmentation? Is it pos-
sible to design a successful unigram model for word
segmentation? We address these questions in the follow-
ing section.

4. Other unigram models

4.1. MBDP-1 and search

In the previous section, we showed that the optimal
segmentation under our unigram model is one that identi-
es common collocations as individual words. Our earlier

discussion of Venkataramans (2001) NGS models demon-
strated that the optimal solution under those models is a
completely unsegmented corpus. What about Brent
(1999) MBDP-1 model? While the denition of this uni-
gram model makes it difcult to determine what the opti-
mal solution is, our main concern was whether it exhibits
the same problems with undersegmentation as our own
unigram model. The results presented by Brent do not indi-
cate undersegmentation, but it turns out that these results,
like Venkataramans, are inuenced by the approximate
search procedure used. We determined this by calculating
the probability of various segmentations of the corpus un-
der each model, as shown in Table 3. The results indicate
that the MBDP-1 model assigns higher probability to the
solution found by our Gibbs sampler than to the solution
found by Brents own incremental search algorithm. In
other words, the model underlying MBDP-1 does favor
the lower-accuracy collocation solution, but Brents
approximate search algorithm nds a different solution
that has higher accuracy but lower probability under the
model.

We performed two simulations suggesting that our own
inference procedure does not suffer from similar problems.
First, we initialized the Gibbs sampler in three different
ways: with no utterance-internal boundaries, with a
boundary after every character, and with random bound-
aries. The results were virtually the same regardless of ini-
tialization (see Appendix A for details). Second, we created
an articial corpus by randomly permuting all the words in
the true corpus and arranging them into utterances with
the same number of words as in the true corpus. This
manipulation creates a corpus where the unigram assump-

S. Goldwater et al. / Cognition 112 (2009) 2154

33

responsible for the poor segmentation performance on
the natural language corpus. In particular, the unigram
assumption of the model seems to be at fault. In the fol-
lowing section we present some additional simulations de-
signed to further test this hypothesis. In these simulations,
we change the model of lexical items used in Step 2a of the
model, which has so far assumed that lexical items are cre-
ated by choosing phonemes independently at random. If
the original poor lexical model is responsible for the DP
models undersegmentation of the corpus, then improving
the lexical model should improve performance. However,
if the problem is that the unigram assumption fails to ac-
count for sequential dependencies in the corpus, then a
better lexical model will not make much difference.

4.2. The impact of the lexical model on word segmentation

One possible improvement to the lexical model is to re-
place the assumption of a uniform distribution over pho-
nemes with the more realistic assumption that phonemes
have different probabilities of occurrence. This assumption
is more in line with the MBDP-1 and NGS models. In NGS,
phoneme probabilities are estimated online according to
their empirical distribution in the corpus. In MBDP-1, pho-
neme probabilities are also estimated online, but according
to their empirical distribution in the current lexicon. For
models like MBDP-1 and the DP model, where the pho-
neme distribution is used to generate lexicon items rather
than word tokens, the latter approach makes more sense. It
is relatively straightforward to extend the DP model to in-
fer the phoneme distribution in the lexicon simultaneously
with inferring the lexicon itself. Before implementing this
extension, however, we tried simply xing the phoneme
distribution to the empirical distribution in the true lexi-
con. This procedure gives an upper bound on the perfor-
mance that could be expected if the distribution were
learned. We found that this change improved lexicon F0
somewhat (to 60.5, with a0  20 and p#  :5), but made al-
most no difference on token F0 (53.6). Inference of the pho-
neme distribution was therefore not implemented.

Fig. 4. The 35 most frequent items in the lexicon found by the DP model
(left) and in the correct lexicon (right). Except for the phonemes z and s,
lexical items are represented orthographically for readability. Different
possible spellings of a single phonemic form are separated by slashes. The
frequency of each lexical item is shown to its left. Items in the segmented
lexicon are indicated as correct (+) or incorrect (). Frequencies of correct
items in the segmented lexicon are lower than in the true lexicon because
many occurrences of these items are accounted for by collocations.

tion is correct. If our inference procedure works properly,
the unigram system should be able to correctly identify
the words in the permuted corpus. This is exactly what
we found, as shown in Table 4. The performance of the
DP model jumps dramatically, and most errors occur on
infrequent words (as evidenced by the fact that token
accuracy is much higher than lexicon accuracy). In con-
trast, MBDP-1 and NGS-u receive a much smaller benet
from the permuted corpus, again indicating the inuence
of search.

These results imply that the DP model itself, rather than
the Gibbs sampling procedure we used for inference, is

Other changes could be made to the lexical model in or-
der to create a better model of word shapes. For example,
using a bigram or trigram phoneme model would allow
the learner to acquire some notion of phonotactics. Basing
the model on syllables rather than phonemes could incor-
porate constraints on the presence of vowels or syllable
weight. Rather than testing all these different possibilities,
we designed a simulation to determine an approximate
upper bound on performance in the unigram DP model.
In this simulation, we provided the model with informa-
tion that no infant would actually have access to: the set
of word types that occur in the correctly segmented cor-
pus. The lexical model is dened as follows:
 2 L
 R L

Ptruewi    1   1

(

jLj  P0wi  

P0wi  

where L is the true set of lexical items in the data, and  is
some small mixing constant. In other words, this model is a
mixture between a uniform distribution over the true lex-
ical items and the basic model P0. If   0, the model is

34

S. Goldwater et al. / Cognition 112 (2009) 2154

Fig. 5. The 70 most frequently occurring items in the segmented lexicon that consist of multiple words from the true lexicon. These items are all identied
as single words; the true word boundaries have been inserted for readability. The frequency of each item is shown to its left.

Table 3
Negative log probabilities of various segmentations under each unigram
model.

Table 4
Accuracy of the various systems on the permuted corpus.

Model

Performance measure

Model

Segmentation

NGS-u
MBDP-1
DP

True

204.5
208.2
222.4

None

90.9
321.7
393.6

MBDP-1

NGS-u

210.7
217.0
231.2

210.8
218.0
231.6

DP

183.0
189.8
200.6

Note: Row headings identify the models used to evaluate each segmen-
tation. Column headings identify the different segmentations: the true
segmentation, the segmentation with no utterance-internal boundaries,
and the segmentation found by each system. Actual log probabilities are
1000 those shown.

constrained so that segmentations may only contain words
from the true lexicon. If  > 0, a small amount of noise is
introduced so that new lexical items are possible, but have
much lower probability than the true lexical items. If the
model still postulates collocations when  is very small,
we have evidence that the unigram assumption, rather

P

76.6
77.0
94.2

R

85.8
86.1
97.1

F

81.0
81.3
95.6

BP

83.5
83.7
95.7

BR

97.6
97.7
99.8

BF

90.0
90.2
97.7

LP

60.0
60.8
86.5

LR

52.4
53.0
62.2

LF

55.9
56.6
72.4

NGS-u
MBDP-1
DP

Note: P, R, and F are precision, recall, and F0 for word tokens; BP, LP, etc.
are the corresponding scores for ambiguous boundaries and lexical items.
Best scores are shown in bold. DP results are with p#  :5 and a0  20.

than any failure in the lexicon model, is responsible for
the problem.

The results from this model are shown in Table 5. Not
surprisingly, the lexicon F0 scores in this model are very
high, and there is a large improvement in token F0 scores
against previous models. However,
considering the
amount of information provided to the model, its scores
are still surprisingly low, and collocations remain a prob-
lem, especially for frequent items.

S. Goldwater et al. / Cognition 112 (2009) 2154

35

Table 5
Results of the DP model using Ptrue.

Mixing constant

Accuracy

% Collocations

  102
  103
  104
  105
  106

F

60.5
62.7
64.5
65.5
68.2

LF

81.7
83.4
84.8
85.3
85.6

Tokens

Lexicon

27.7
25.8
24.6
23.7
21.4

21.6
19.1
16.9
15.5
13.1

Note: Shown, for each value of , is token F0 (F), lexicon F0 (LF), and the
percentage of tokens and lexical items that are multi-word collocations.

Considering the case where   106 yields some in-
sight into the performance of these models with improved
lexical models. The solution found, with a lexicon consist-
ing of 13.1% collocations, has higher probability than the
true solution. This is despite the fact that the most proba-
ble incorrect lexical items are about ve orders of magni-
items.9 These
tude less probable than the true lexical
incorrect lexical items are proposed despite their extremely
low probability because only the rst occurrence of each
word is accounted for by the lexical model. Subsequent
occurrences are accounted for by the part of the model that
generates repeated words, where probabilities are propor-
tional to the number of previous occurrences. Therefore,
low-probability lexical items incur no penalty (beyond that
of any other word) after the rst occurrence. This is why
the collocations remaining in the DP model using Ptrue are
the highest-frequency collocations: over many occurrences,
the probability mass gained by modeling these collocations
as single words outweighs the mass lost in generating the
rst occurrence.

The results of this simulation suggest that the many of
collocations found by the unigram DP model are not due
to the weakness of the lexical model. Regardless of how
good the lexical model is, it will not be able to completely
overcome the inuence of the unigram assumption gov-
erning word tokens when modeling the full corpus. In or-
der to reduce the number of collocations, it is necessary
to account for sequential dependencies between words.
Before showing how to do so, however, we rst present
theoretical results regarding the generality of our conclu-
sions about unigram models.

4.3. MBDP-1, the DP model, and other unigram models

The probabilistic models used in MBDP-1 and our
Dirichlet process model appeal to quite different genera-
tive processes. To generate a corpus using MBDP, the num-
ber of word types is sampled, then the token frequencies,
then the forms of the words in the lexicon, and nally an
ordering for the set of tokens. Using the DP model, the
length of the corpus (number of word tokens) must be cho-

9 There are 1321 lexical items in the corpus, so under the lexical model,
the probability of each of these is approximately 103. There are 50
phonemes and p#  :5, so a single-character word has probability .01 under
P0. Multiplying by the discount factor   106 yields Ptrue  108 for one-
character words not in the true lexicon. Longer incorrect words will have
much lower probability.

sen, and then the sequence of words is generated, implic-
itly determining the number of word types and the
lexicon. Although these two approaches to generating a
corpus are very different, it is possible to show that, by
varying the specic distributions assumed at each step of
the MBDP-1 generative process, the two approaches can
result in exactly the same distribution over word se-
quences. In Appendix B we show that by changing how
the size of the lexicon and the token frequencies are cho-
sen in Steps 1 and 2 of the MBDP model, we can produce
distributions over words that are equivalent to the distri-
bution given by the DP model when conditioned on the to-
tal number of words.

This formal correspondence between MBDP-1 and the
DP model suggests that the two models might express sim-
