Abstract

Interpreting brain image experiments requires analysis of complex, multivariate data.

In recent years, one
analysis approach that has grown in popularity is the use of machine learning algorithms to train classiers to
decode stimuli, mental states, behaviors and other variables of interest from fMRI data and thereby show the
data contain enough information about them. In this tutorial overview we review some of the key choices faced
in using this approach as well as how to derive statistically signicant results, illustrating each point from a case
study. Furthermore, we show how, in addition to answering the question of is there information about a variable
of interest (pattern discrimination), classiers can be used to tackle other classes of question, namely where is
the information (pattern localization) and how is that information encoded (pattern characterization).

1 Introduction

In the last few years there has been growing interest in the use of machine learning classiers for analyzing fMRI
data. A growing number of studies has shown that machine learning classiers can be used to extract exciting new
information from neuroimaging data (see [32] and [17] for selective reviews). Along with the growth in interest
and breadth of application, the methods underlying the use of classiers with fMRI have continuously evolved
and ramied (see [34] for a historical overview). Given the novelty of the approach, there have been few attempts
to organize and interrelate available methods in a single place. The present article strives to rectify that situation
by providing a tutorial introduction to classier methods in fMRI.

Our presentation will be organized around the idea that classier- based analyses, like traditional fMRI anal-
yses, can be characterized in terms of a series of specic choices over a series of decision points in the analysis
process, starting from selection of the scientic question to be asked and ending with choices among tests for
hypotheses. We begin by laying out an illustrative example of a classier-based analysis, and then dissect the
analysis process, examining the set of choices it implicitly involves, and the alternatives available at each stage.
There have been other proposals for a staged procedure for using classiers to analyse fMRI data (e.g.
[40]),
though with an emphasis on a single type of classier and requiring the use of dimensionality reduction. We
broaden this idea to include several kinds of classier and also the use of feature selection. We conclude with a
discussion of the sorts of scientic questions that may be fruitfully addressed using classiers, and the ways in
which the choice of question impacts subsequent analysis decisions.

Where possible, we discuss strengths and weaknesses of competing options. However, it is important to
acknowledge from the outset that rm grounds for such evaluations are in many cases not yet available. Where
possible, we oer recommendations based on the results of formal principles or controlled empirical tests (from
[35]). Where these are lacking, we shall sometimes inject impressions drawn from our own personal experience
with classier-based analysis of fMRI data. Before entering into the discussion of the analysis proper, we begin
in the next section by briey introducing machine learning classiers in their own right.

1.1 What is a classier?

Classication is the analogue of regression when the variable being predicted is discrete, rather than continuous. A
classier is a function that takes the values of various features (independent variables or predictors, in regression)
in an example (the set of independent variable values) and predicts the class that that example belongs to (the
dependent variable). In a neuroimaging setting, the features could be voxels and the class could be the type of
stimulus the subject was looking at when the voxel values were recorded (see Figure 1). We will denote an example
by the row vector x = [x1 . . . xv] and its class label as y. A classier has a number of parameters that have to be

1

example

dataset



tools

voxels (features)

class label

a)

b)








labels

group 1

...

group 6

Figure 1: An example where voxels are features as a row vector (left) and a dataset as matrix of such vectors (right).

learned from training data  a set of examples reserved for this purpose  similarly to how regression parameters
are estimated using least squares. The learned classier is essentially a model of the relationship between the
features and the class label in the training set. More formally, given an example x, the classier is a function f
that predicts the label y = f (x).

Once trained the classier can be used to determine whether the features used contain information about the
class of the example. This relationship is tested by using the learned classier on a dierent set of examples,
the test data.
Intuitively, the idea is that, if the classier truly captured the relationship between features
and class, it ought to be able to predict the classes of examples it hasnt seen before. The typical assumption
for classier learning algorithms is that the training (and testing) examples are independently drawn from an
example distribution; when judging a classier on a test set we are obtaining an estimate of its performance on
any test set from the same distribution. This is depicted in part c) of Figure ??. We will denote the training and
test sets by Xtrain and Xtest, matrices with respectively ntrain and ntest examples as their rows, and the example
labels by the column vectors ytrain and ytest. The most commonly used measure of how well a classier does on
the test set is its accuracy. This is simply the fraction of examples in the test set for which the correct label was
predicted, i.e. Pntest
, where I(f (xi), y) = 1 if f (xi) = yi (the label of the ith example was predicted
ntest
correctly) or 0 otherwise.

I(f (xi),yi)

i=1

As we shall discuss in Section 3.2, there are several types of classier. However, for reasons that will become
clear, our emphasis will be on linear classiers; in this type the classication function relies on a linear combination
of the features, i.e. f (x) = g(w1x1 + . . . + wV xV ), and the weights wi in that combination are the parameters to
be learned.

2 Classier analysis: an illustrative example

In this section we will introduce an illustrative example of a classier-based study, which will provide a basis for
subsequent discussion.

The experiment that gave rise to our dataset was designed to test the question of whether the activation as
a result of seeing words that were either kinds of tool or kinds of building could be distinguished by a classier.
1 The subject was shown one word per trial and performed the following task: the subject should think about
the item and its properties while the word was displayed (3 seconds) and clear her mind afterwards (8 seconds of
blank screen). There were 7 dierent items belonging to each of the two categories and 6 experimental epochs.
In each epoch all 14 items were presented, without repetition; all 6 epochs had the same items.

Images were acquired with a TR of 1 second, with voxel dimensions 3  3  5mm. The dataset underwent a
typical fMRI preprocessing stream from DICOM les using SPM [38]. The steps followed were volume registration
and slice acquisition timing correction, followed by a voxelwise linear detrending. A brain mask was extracted
automatically and used to restrict voxels considered in the subsequent steps. Although the original dataset
contained several subjects, we will focus on a single one.

Each trial was converted into an example by taking the average image during a 4 second span while the subject
was thinking about the stimulus shown a few seconds earlier; these 4 seconds contained the expected peak of the
signal during the trial. Each example was thus a vector of between 10000 and 20000 values, feature values, each
corresponding to the 4 second average signal at a voxel. In each average image only between 10000 and 20000 of
the voxels were contained in the brain mask. Using each trial to obtain an example yielded a total of 42 examples

1Data kindly provided by Robert Mason and Marcel Just, Center for Cognitive Brain Imaging, CMU, who ran the experiment.

2

c)

training data




test data





labels

predicted

labels

true
labels

+

classifier

=

vs

Figure 2: A classier is learned from the training set, examples whose labels it can see, and used to predict labels
for a test set, examples whose labels it cannot see. The predicted labels are then compared to the true labels and
the classiers accuracy computed.

for each of the tools and buildings categories. Each example vector was normalized to have mean 0 and standard
deviation 1. Examples were further grouped by which of the six epochs they belonged, as depicted in Figure 1.

With examples in hand, the dataset was divided into a training and a test set, with a goal of learning a classier
whose task was to predict the category of an example. As mentioned previously, the parameters of a classier are
learned from the examples in the training set, and the classier is then evaluated on the examples in a separate
test set (see Figure 2). In the example study we divided the dataset for one subject into examples coming from
either the even or the odd epochs, assigning those in epochs 1,3 and 5 to be the training set (42 examples) and
those in epochs 2,4 and 6 to be the test set (42 examples). Given this, we trained a Gaussian Naive Bayes classier
(see Section 3.2) and applied it to the test set. For 65% of the examples the label was predicted correctly. Given
this result, could we conclude that the classier learnt from the training set was capable of predicting the labels
of the test set better than chance and, therefore, that the fMRI features contained relevant information about the
stimulus category? Or could it just have done this well on the test set by chance? In order to test the statistical
signicance of this result, we considered the null hypothesis that the classier was performing at chance level (e.g.
the accuracy of prediction on average would be 50%) and tested it using the procedure we describe in Section 3.4.
For an accuracy of 65% with 42 test examples the test yielded a p-value of 0.0218 and the null hypothesis was
rejected.

3 Classier analysis: stages and options

The foregoing example, though presented as an unsegmented narrative, can be broken down into a set of stages,
beginning with the conversion of raw data into a set of examples and proceeding though choice of classier,
training and test sets and interpretation of results. At each stage, the researcher faces a choice among multiple
alternatives.
In what follows we examine each stage and the choices it presents, as well as how those choices
interact and are conditioned by practical factors.

3.1 Creating examples

Creating examples in general requires deciding what to use as features, how to extract their values and what we
would like to predict. In our illustrative example we chose to average several TRs worth of images in a single trial
to create an example; as a single word stimulus maps to a single trial this is a natural choice. Many other options
are available, however. We could instead have used each individual TR within a trial as an example by itself,
or averaged many trials with words belonging to the same category into a single example. One is not limited to
using voxels as features. We could use the average of several voxels in one ROI as a single feature (essentially,
the ROI becomes the feature) or use voxels at a particular time point in a trial. The latter is used in [29] in a
situation where what distinguishes the activation during processing of ambiguous and unambiguous sentences is
the time course of activation in various voxels active in both conditions. Another possibility is to forsake creating
examples directly from the activation signal and instead do it from deconvolution coecient images, if each class
can be identied with a regressor.

3

The question of what to use as class labels depends on the purposes of the researcher, but we note that one is
not limited to using stimuli classes. One could also use subject responses or decisions in face of a stimulus (e.g.
did the subject get this right or wrong?). A particularly creative example of this is the study [16], where the
prediction is of which of two possible images a subject reports perceiving in a binocular rivalry situation.

In creating examples, there is an important tradeo associated with the number of examples produced. This is
the tradeo between having many noisy examples (e.g. one per trial) or fewer, cleaner ones (e.g. one of each class
per run), as a result of averaging images in the same class. Although there is no hard and fast number of examples
necessary to train a classier, having more is generally better, to the degree that the classier can see through
the noise present (whereas averaging eliminates both noise and natural variability of informative activation in the
examples belonging to a class). Having more examples helps in the training set side in that some of the classiers
we consider require a certain number of examples to obtain good estimates of their parameters. Conversely, there
are some classiers that are particularly suitable for a situation with very few examples, and we discuss this at
greater length in Section 3.2. From the test set standpoint, having more test examples increases the power of the
test for signicance of the accuracy obtained. In Section 3.3 we will introduce cross-validation, a procedure that
makes very ecient use of examples for both training and testing.

One issue to be cognizant of is the desirability of having the same number of examples in each class. WHen
this is not the case a classier learning algorithm may tend to focus on the most numerous class, to the detriment
of the others; this will also aect the interpretation of an accuracy result (e.g. 80% accuracy may not be very
good in a situation where 9 of 10 examples belong to one class and 1 of 10 to the other, if it means the classier
simply predicts the most numerous class by default). This balance between classes can be achieved by using only
a subset of the examples in the most numerous class, repeating the procedure for various subsets and averaging
the results.

Note that using consecutive images as individual training examples should be avoided, as the strong temporal
autocorrelation in the signal will virtually ensure those examples are not independent. Intuitively, it also means
that having several very similar examples will not bring in much new information. This does not bar us from
using consecutive images in the test set, depending on the purpose (see Section 4 for an example where training
examples come from block data and test examples from event-related data). Non-independent test examples
cannot, however, be used for the signicance test described earlier (the binomial distribution outcome is the sum
of the results of independent Bernoulli trials).

Given that there are generally many more features than examples, it is often advantageous to reduce the
number of features considered to focus on a subset of particular interest; this is called feature selection. For
instance, if using voxels as features, we may just want the voxels in a particular region of interest (ROI) or their
values at a particular point in a trial. The crucial issue to keep in mind is that the choice of features at this stage
must not depend on the labels that will be used to train a classier. To see why, imagine a situation where we
are interested in predicting the orientation of a stimulus grating. It is acceptable to restrict the voxels considered
to those in visual cortex, demarcated anatomically. It is also acceptable to restrict them further to those showing
some activity during task trials relative to a baseline. What is not acceptable to select voxels that appear to
distinguish one orientation from another in the entire dataset. The reason for this is that it is a subtle form of
letting the test set information aect the learning of the classier in the training set, and it leads to optimistic
accuracy estimates. Looking at the labels for the entire dataset is sometimes called peeking, Note that this does
not mean that the class labels cannot be used at all in feature selection. They can be used once the data have
been divided into training and test sets, considering solely the training set, and we discuss all practical aspects of
doing it in Section 3.3.3.

An alternative path to reducing the number of features a classier has to consider is dimensionality reduction
using a method such as Singular Value Decomposition/Principal Component Analysis [12] or Independent Com-
ponents Analysis [3] on the entire dataset matrix (rows are examples, columns are features). The commonality
between these methods is that they transform the original feature space (e.g. voxels) into a new, low-dimensional
feature space, yielding a new dataset matrix with the same number of rows and a small number of columns. This
is always worth trying and a necessary step for particular classiers, as we will see in Section 3.2, but not at all
guaranteed to improve results, partially because most dimensionality reduction techniques ignore class labels in
their criteria (though see SVDM [36] and PLS [27]).

A nal issue to consider in the construction of examples is that of preprocessing. By this we do not mean
the usual preprocessing of data, e.g. motion correction or detrending, a topic covered in great depth in [39]. We
mean, rather, that done on the examples, considered as a matrix with N rows (where each row is an example)
and V columns (features). In the example study, we normalized each example (row) to have mean 0 and standard
deviation 1. The idea in this case is to reduce the eect of large, image-wide signal changes. Another possibility
would be to normalized each feature (column) to have mean 0 and standard deviation 1, either across the entire
experiment or within examples coming from the same run. This is worth considering if there is a chance that
some voxels will have much wider variation in signal amplitude than others. Although a linear classier can in

4

principle compensate for this to some degree by scaling the coecient for each voxel, there are situations where it
will not and thus this normalization will help. Our own experience suggests either row or column normalization
is generally benecial and should be tried in turn, whereas combining them may sometimes have adverse eects.

3.2 Choosing a classier

Earlier we introduced the idea of a classier as a function f that takes an example x and generates a class label
prediction y = f (x). The specic kind of function being learnt  and the assumptions built into it  is what
distinguishes the various types of classier. In this section we review and contrast the classiers most commonly
used with fMRI data, as well as the factors at stake in choosing which kind to use. We supply much additional
detail in Appendix A.

The simplest classication procedure is called nearest-neighbour and it doesnt even involve explicitly learning
a classication function. Classication of a test example is done by nding the training set example that is most
similar to it by some measure (e.g. lowest euclidean distance, considering the entire feature vector) and assigning
the label of this nearest neighbour to the test example. Variants of this idea include averaging all the training
set examples in each class into a single class prototype example or assigning the majority label in the k-nearest
neighbours (k odd). Both variants minimize the eect of noise (if the examples of dierent classes are all relatively
close to each other) but can be susceptible to destroying some information if there is actual non-noise variability
in the examples in each class (e.g. examples in one class come in two clusters that are relatively dissimilar to each
other). Whereas nearest-neighbour classication can work very well if there is a small number of features, it most
likely will not in a situation where there is a large number and only a few are informative. Hence it is generally
used in conjunction with some form of feature selection, as in [15] or [29].

Classiers that do learn an actual function divide into what are called discriminative and generative models.
In the former, a prediction function with a given parametric form is learnt directly from the training data by
setting its parameters.
In the latter, what is learned is essentially a statistical model that could generate an
example belonging to a particular class (i.e. it models the distributions of feature values conditional on example
class, P (x|y = A) and P (X|y = B)), which is then inverted via Bayes Rule to classify (i.e. yields P (y = A|x) and
P (y = B|x), and the prediction is the class for which that probability is largest).
In the typical fMRI study, we generally have many more features than examples; if using voxels in the whole
brain as features, its not unusual to have a few tens of examples and tens of thousands of features (or at least a
few hundreds, if using feature selection as described in Section 3.3.2). The eect of this is that it will generally be
possible to nd a function that can classify the examples in the training set well, without this necessarily meaning
that it will do well in the test set (for a more detailed explanation of the reason why see [28]): this phenomenon
is called overtting 2. One way of trying to avoid overtting is to use as simple a function as possible that does
well in the training set.

The most natural choice for a simple function is to have the prediction depend on a linear combination of the
features that can dampen or increase the inuence of each one of them, just as linear regression. To make the
idea of a linear classier more concrete, the classier is parameterized by a set of weights w and, for an example
x with V features

xw = x1w1 + . . . + xV wV

the classier predicts class A if xw > 0 or class B if xw < 0 (with ties broken randomly). One nice property of
linear classiers, beyond their simplicity, is the fact that each feature aects the prediction solely via its weight
and without interaction with other features, giving us a measure of its inuence on that prediction. We discuss
ways to use this, as well as some caveats, in Section 4.1.

A more geometric intuition for this is given in Figure 3, in a rather simplied two voxel brain, with three
examples of each class (each example is characterized by the value of the two voxels). Learning a linear classier
is equivalent to learning a line that separates points in the two classes as well as possible, the decision boundary.
Examples of classiers that learn this are Logistic Regression (LR) and linear Support Vector Machines (SVM).
Whereas this might seem remote from the generative model idea introduced earlier, the classication decisions of
Gaussian Naive Bayes (GNB) and Fishers Linear Discriminant Analysis (LDA) can be expressed in this manner
as well, and thus properly included in the linear classier category. Note that there are many possible linear
discriminants that perfectly classify the six training examples shown in Figure 3. The various classiers have
dierent rationales for choosing among these, which correspond to dierent denitions of separating points as
well as possible. Consider also that, even for a specic type of classier, there may still be many possible settings
of its parameters that lead to equally good predictions in the training set. The process of guiding the procedure
that sets the parameters to a solution with desirable properties  small weights overall, for instance  is called

2An example of this would be to use a very high-degree polynomial to model points in a time series coming from a low-degree
polynomial with added noise. The very high-degree polynomial would likely be worse at approximating new points than a low-degree
polynomial would.

5

linear discriminant A

tools1

linear discriminant B

d)

cross-validation fold 1

cross-validation fold 6

tools2

tools3

1

l
e
x
o
v

buildings1

buildings3

buildings2

voxel 2









test

train









train

test

Figure 3: left: Learning a linear classier is equivalent to learning a line that separates examples in the two classes
(vectors in a 2-voxel brain) as well as possible. right: During cross-validation each of 6 groups of examples takes a
turn as the test set while the rest serve as the training set.

regularization. Both the rationales and the way regularization is done for the various kinds of classiers are
described in Appendix A.

All this brings us to the question of which classier to use, which we will attempt to answer based on our
experience and a multi-study comparison of classiers and feature selection methods [35]. In cases where there
are a large number of features (e.g. all the voxels in the cortex), GNB is generally inferior to LR and linear SVM,
as the regularization in the latter clearly helps weigh down the eect of noisy features that are highly correlated
with each other. The disadvantage mostly disappears if using feature selection, hence GNB is a useful classier
for procedures that need to be repeated many times, such as permutation tests, due to being much faster to train
than the others. LDA needs to be used either in conjunction with extreme feature selection or with dimensionality
reduction ([40]), as otherwise there are typically not enough examples to estimate its covariance matrix reliably.
If choosing between LR and linear SVM, they are roughly equivalent other than for the fact that the rst
lends itself more naturally to cases where there are more than two classes. So far, and for the sake of simplicity,
we have discussed classiers in a situation where examples belong to one of two classes but, in general, we might
have more (several dierent stimulus classes, subject responses, etc). Most of the classiers described above work
without modication in a multiple class scenario. The one exception are the SVM classiers; although multiclass
versions do exist, most packages in current use require you to create several two class problems (e.g. each class
versus all the others, or all pairs of classes) and combine them to produce a multiway classication decision. We
describe some approaches to doing this in Appendix A. That said, often the object of interest in a multiple
class situation is not so much the raw accuracy (do we believe that hundreds of types of object stimuli would be
distinguishable based on fMRI activation, for instance?) as which classes can be distinguished from which others,
and how. Hence, it might make more sense to consider all pairs of classes, train and test a classier for each
pairwise distinction and produce what is called a confusion matrix, a #classes  #classes matrix where entry
(i, j) contains the accuracy of the distinction between classes i and j. Often this reveals groups of classes that
are hard to distinguish from each other but that are distinguishable from another group (e.g. several small object
stimuli versus animal and human faces).

There are more complex classiers, such as nonlinear SVMs or articial neural networks, which can let interac-
tions between features and nonlinear functions thereof drive the prediction; we will not discuss them at length for
two main reasons. The rst is that it is unclear that they provide a signicant advantage in practical performance
relative to linear classiers (in extant studies or the multi-study comparison in [35]). In our opinion this is more
a reection of the fact that the number of examples available is so small than of the inexistence of complicated
relationships between features. The second reason is that the relationship between features and the prediction
becomes harder to interpret (though see [13] which derives a class taxonomy from the hidden layer node activity
in a neural network, for instance). This doesnt mean that these classiers cannot be used fruitfully when the
number of features is smaller, in particular when doing information mapping as discussed in Section 4.2. We
describe these classiers and their relationship to linear classiers in more detail Appendix A.

6

3.3 Training and testing in cross-validation

3.3.1 Cross-validation

In our illustrative example we divided the dataset in two halves, one used for training and the other for testing.
As we later discussed, we would like to train a classier with as much data as possible; sadly, we cannot train and
test on the same data if the goal is obtain a useful estimate of the classiers true accuracy, i.e. the probability
that it would label any future test example correctly. Furthermore, using just a few examples for testing will not
lead to a good estimate: it will be inordinately variable, as well as constrained to a few possible values. Splitting
the dataset in half to have more test examples, as we did in the example study, means we might be learning a
much worse classier than we potentially could with more training examples. Fortunately, there is a procedure
that allows us to have our cake and eat (almost) all of it too... The procedure is called cross-validation. In its
most extreme variant, cross-validation follows these steps:

 leave one example out, train on the remaining ones, make a prediction for this example
 repeat for each example in turn
 compute the accuracy of the predictions made for all the examples

This variant is called leave-one-out cross-validation. Although each classier trained is technically dierent, one
can expect them to be similar and predict similarly, since they share so much training data. Hence we treat the
accuracy estimate obtained in this manner as the expected accuracy of a classier trained on a dataset with all
but one example, following [23]. Note this also provides a slightly conservative estimate of the accuracy we expect
if we were to train a classier using all the available examples.

In practice, leaving each example out can be computationally expensive because instead of a single classier
we are training as many classiers as there are examples. One compromise is a procedure called a k-fold cross-
validation, where k is the number of parts into which the dataset is divided; common choices are k = 10 or k = 5,
corresponding to leaving out 10% or 20% of the examples on each fold. Fortunately, fMRI datasets are often
naturally divided in a way that addresses all these issues, be it in runs or epochs within a run where all the
possible classes or stimuli appear. Our illustrative dataset divides naturally into six groups, corresponding to the
six epochs of stimuli in the experiment; each group is used in turn as the test set in cross-validation, with the
remaining groups used as the training set (on the right of Figure 3). Hence we advise the reader to take advantage
of those natural divisions at least on a rst attempt, possibly moving to a ner grain division that still has the
desired properties if more training examples are needed.

Aside from the number of examples left out there are other important considerations. The rst is that the
training data in each fold must contain examples of all classes, as otherwise the classier for that fold will not
be able to predict the absent ones. As mentioned earlier, classes should be balanced, i.e. have roughly the same
number of examples. Furthermore, examples that are correlated - by being close in time, for instance - should
end up in the same fold. Otherwise, the classier may be able to predict accurately for test examples with a
correlated counterpart in the training set.

3.3.2 Feature selection

In Section 3.1 we introduced the idea of feature selection, in situations where we have some a priori reason for
picking them, e.g. wishing to test whether voxels in a particular ROI have information. Feature selection is often
deemed necessary in order to train classiers in domains where datasets have very large numbers of features.
Why might it help? Intuitively, the goal is to reduce the ratio of features to examples  decreasing the chance
of overtting  as well as to getting rid of uninformative features to let the classier focus on informative ones.
Almost all the fMRI decoding results reported in the literature used either feature selection or some other form
of dimensionality reduction ([32],[17],[4],[40]). Feature selection in general is too vast a topic to broach in great
detail here, but a useful review of both terminology and the thrust of various approaches is given in ([11]).

One distinction often made is between scoring/ltering and wrapper methods. The former involves ranking the
features by a given criterion - each feature is scored by itself, a bit like a univariate test of a voxel - and selecting
the best in the ranking. The latter consists broadly of picking new features by how much impact they have on the
classier given the features already selected. This can be done by repeatedly training and applying the classier in
cross-validation within the training set. Given the often prohibitive computational expense of wrapper methods
 due to the large number of features  we will be concerned solely with feature scoring methods.

There are several generic methods for selecting informative features. In parallel, features in the fMRI domain
are often voxels and there is a desire to be able to look for certain types of voxel behaviour, e.g. is a voxel very
selective for one particular condition or discriminating between two groups of conditions. Hence most extant
papers resort to methods that draw from both these sources, which we summarize below. Note that method

7

method

number of voxels

method

NCV

accuracy
searchlight

activity
ANOVA

100
0.81
0.81
0.79
0.77

200
0.81
0.82
0.80
0.75

400
0.75
0.82
0.77
0.75

800
0.73
0.77
0.73
0.73

1000
0.74
0.79
0.74
0.71

all
0.65
0.65
0.65
0.65

accuracy
searchlight

activity
ANOVA

0.81
0.82
0.77
0.77

Table 1: Classication results using each of several selection methods with either various xed numbers of voxels
(left) or with the number determined using nested cross-validation (NCV) inside the training set of each fold (right).

rationales are described in terms of voxels and conditions (classes), for clarity, but could be adapted for other
kinds of features:

 Activity - This method selects voxels that are active in at least one condition relative to a control-task
baseline, scoring a voxel as measured by a t-test on the dierence in mean activity level between condition
and baseline.

 Accuracy - This method scores a voxel by how accurately a Gaussian Bayesian classier can predict the
condition of each example in the training set, based only on the voxel. It does a cross-validation within the
training set for the voxel and the accuracy is the voxels score.

 Searchlight Accuracy - The same as Accuracy, but instead of using the data from a single voxel (GNB
classier with one feature) we use the data from the voxel and its immediately adjacent neighbours in three
dimensions (a classier with up to 27 features, classier can be GNB, LDA or SVM).

 ANOVA - This method looks for voxels where there are reliable dierences in mean value across conditions

(e.g. A is dierent from B C D, or AB from CD, etc), as measured by an ANOVA.

 Stability - This method picks voxels that react to the various conditions consistently across cross-validation
groups in the training set (e.g. if the mean value in A is less than B and greater than C, is this repeated in
all groups).

Precise details of how these methods can be implemented and other alternatives are provided in Appendix B.
Note that the methods above are, with exception of searchlight accuracy, univariate. There are promising results
that show that, in some circumstances, multivariate feature selection can work better, which are discussed in
Section 4. Finally, the methods listed above provide rankings of voxels; one might still have to decide on how
many of the top-ranked voxels to use from the ranking for each method. For methods that are based on a statistical
criterion (e.g. t-tests of activation), it is common to use a multiple comparison criterion such as Bonferroni or
False Discovery Rate [8] and keep just the features deemed signicant at a given level. This is reasonable but
problematic if few or no voxels are signicant (possible given the amount of noise and very small sample sizes) or
the score produced by the method does not map to a p-value (though nonparametric methods can still be used
to obtain one, see Appendix C).

3.3.3 Feature selection and cross-validation

Up till now we have discussed feature selection as a topic by itself. Often, however, feature selection is used in
conjunction with cross-validation and this entails particular precautions. The rst is that selection must happen
on the training set. As mentioned earlier, for every fold in cross-validation we have a dierent training set and thus
features have to be selected separately for each fold. A second reason is that, as mentioned earlier, the methods
used in practice rank features and we still have to decide how many to use, unless the method provides a rationale
(e.g. the number of voxels where a t-test is signicant at a certain level). A typical approach is to not commit to
a specic number, but to instead produce a feature ranking by each method for each fold. This can then be used
to obtain, for each method, an accuracy score using each of the numbers of voxels desired (e.g. top 20, top 50,
top 100, etc). The end product is a #methods  #voxels table of accuracy results, such as Table 1 (left) for our
sample dataset.

If feature selection could be done prior to cross-validation (e.g. by selecting voxels from one ROI, or by some
other method that avoids examimining the labels of the test examples), the whole procedure can be run as if the
dataset contained only the selected features. In this and the case where the method determines the number of
features, the end product is a #methods  1 vector of accuracy results.

An alternative to considering all numbers of voxels is to let the training set in each cross-validation suggest
which number to pick. To do this, we can run a nested cross-validation (NCV) inside the training set. Consider
our illustrative example, which had 6 groups of examples based on epochs. During the rst cross-validation fold,

8

we tested on group 1 anded train on groups 2-5. Using a nested cross-validation means performing the cross-
validation procedure described above inside groups 2-5 by testing on 2 and training on 3-5, then testing on 3
while training on 2,4,5,6, etc. The result for this training set is, again, #methods  #voxels table of results. For
each method, this table will yield the number of voxels at which the accuracy is highest; this is the number that
will be select from the ranking produced from groups 2-5. The end product is a #methods  1 vector of accuracy
results, as shown in Table 1 (right) for our sample dataset.

3.4 Evaluating results

The person training a classier on fMRI data is concerned with establishing that a variable of interest can be
decoded from it, i.e. a classier can be trained whose true accuracy is better than that of a classier deciding at
random. A signicant result is thus one where one can reject the null hypothesis that there is no information about
the variable of interest. Establishing signicance is generally done by determining how improbable a classication
results would be were the null hypothesis to be true (this probability is called a p-value).

The accuracy on the test set is an estimate of the true accuracy of this classier. Formally, this is the probability
that it will correctly label a new example drawn at random from the same distribution that the other examples
came from; it can also be viewed as the accuracy one would get if one had an innite number of examples in the
test set. Hence, how good an estimate it is depends on the size of the test set. It is an unbiased estimate because
the test examples have not been seen by the classier during training.

In this situation the probability of obtaining the result under this null hypothesis is easy to calculate. It can
be viewed as the outcome of tossing a coin for each of the examples to be classied, with the coins bias reecting
the probability of labelling an example correctly by chance (50% in a two class problem, 25% in a four class
problem, etc). Thus, the p-value under this null hypothesis of k correctly labelled test examples is, if we dene X
to be the number of correctly labeled examples, simply P (X  k) under a binomial distribution with n trials and
probability of success 0.5 (two class), 0.25 (four class), etc; if the p-value is below a certain threshold (typically
0.05 or 0.01) the result will be declared signicant.

The accuracy estimate obtained through cross-validation is a combination of results produced by classiers
trained on mostly overlapping training sets, hence we will treat that estimate as if it came from a single classier,
following common practice [23]. This allows us to test it in exactly the same way.

Extending the signicance test to multiple, independent results is straightforward, in that one can simply
resort to a multiple comparison correction criterion and adjust the signicance threshold accordingly. This is the
situation we have when we consider the nested cross-validation (NCV) results, as each one is the product of training
a classier using voxels selected with a dierent method. If, instead, we are just producing a #methods #voxels
table of results, we could use the same approach to simultaneously test all the results in the table. This would,
however, be conservative, in that the results for a given method are correlated to some extent; this follows from
the fact that the half the voxels used by the classier based on the top 100 voxels are also used by the one based on
the top 50, say. A common idea to deal with this issue is to select the maximum result among all those obtained
using a particular selection method (a table row); it is often accompanied by the mistake of testing that maximum
result for signicance in the way described. To see why this can be a problem, consider a situation where the
classiers are using dierent numbers of features and, in each case, have a true accuracy (over the underlying
example distribution) of 0.5. It is possible even in this situation that, at least for one number of features, the
experimentally observed accuracy will be fairly high; this becomes more likely the smaller the dataset is. If we are
just deeming the best number of features to be the one at which the maximum accuracy is attained, we would
pick this result and possibly deem it signicant. Hence one possibility is to report the entire table and the number
of examples used, and correct for all of them, at the risk of being conservative. The other possibility is to report
the maximum accuracy result and test it dierently using a permutation test, as follows.

Assuming there is no class information in the data, the labels can be permuted without altering the expected
accuracy using a given classier and number of features (this would be chance level). This is exactly the kind
of situation where one can use a permutation test, by repeating the entire process of producing a result table
from a dataset with permuted labels (see [9], which reviews the use of permutation tests in classication with one
functional neuroimaging application, and also Appendix C for more details). Over many permutations this yields
a sample of accuracy results for each cell in the table under the null hypothesis but also, more importantly, for
any derived measures such as the maximum of each table row. The p-value under the null hypothesis for each
entry of the original result table is the fraction of the corresponding sample that is greater than or equal to it,
and this applies to the maximum as well. The main problem of resorting to this approach is its computational
cost.

9

4 Applications of pattern classication

We have been treating a classier-based analysis as a sequence of stages with choices at each stage. One choice that
we have not yet directly addressed is perhaps the most important: the choice of the initial scientic question to be
answered. Our focus so far has been on the use of classiers for pattern discrimination. Here, the basic question
being addressed is simply whether fMRI data carry information about a given variable of interest. Determining
the answer to this question is of interest, for example, in clinical diagnosis or lie detection [5]. We can view
traditional univariate methods as answering this question on a voxel by voxel or ROI by ROI basis. Classiers
bring to the question an increase in sensitivity of detection, both by pooling information across many voxels but
also by relaxing constraints on spatial contiguity or the need for voxel responses to be similar. In addition to
the question of whether fMRI data carry class information, there are at least two other basic questions to which
classiers can be applied, namely where or when is class information represented (pattern localization), how
is class information encoded and how does its encoding relate to known relationships between stimuli (pattern
characterization). This section examines the extent to which questions these questions can be tackled using the
machinery of linear classiers and feature selection, as well as the limitations and caveats. For the rest of the
section we will assume features are voxels, as it will simplify discussion.

4.1 Pattern Localization

Once it has been established that class information is present in a dataset, one may go on to ask where in the brain
this information resides. A natural approach for addressing this is to ask which voxels contribute most strongly
and reliably to the classiers success. There are two aspects to this: determining which voxels are being selected
and also how their weight aects the classier prediction.

When cross-validation is being used, a particular voxel selection method can and very likely will rank dierent
voxels at the top from fold to fold. It may come as a surprise that, for many datasets, the overlap between the
set of voxels selected in all folds versus the set selected in an individual fold is fairly small (between 1
10 and 1
3
is typical) even if selecting a relatively large fraction of the total number of voxels [35]. This is not a problem,
intrinsically, it just reects the fact that whatever voxels are selected contain sucient information. There might
be redundancy, in that many voxels encode the same things and a few get picked in each fold, and there might
be a small necessary subset, present in all folds. One possible strategy for nding the maximal subset of voxels
containing information would be to start from the ones present in all or almost all folds and use them as seeds in
a clustering algorithm that would nd voxels with similar response proles across examples.

A related issue is whether the information carried by voxels is the same in the sets identied by various selection
methods. There are two ways of considering this issue: one is to look at overlap in selected sets, now between
methods, and the other to consider whether a given classier using dierent voxel sets performs dierently (in
particular, whether it makes dierent prediction errors). For problems with two classes, most selection criteria
will nd similar voxels, as there arent many ways that a voxel could respond dierently to the two classes. In
multiclass problems results suggest there are dierent types of voxel behaviour and the various voxel selection
methods described earlier can pick on dierent types [35]. Hence, if we wish to use classier prediction success
as an indication that the voxel set it uses contains informative voxels, it is wise to consider dierent selection
methods; it is also a good idea to determine whether there is any overlap between the voxel sets selected in all
folds by each method.

Given a set of voxels, a dierent question is which of those voxels most strongly aect classier predictions.
The use of linear classiers makes it relatively simple to answer this question, as each voxel aects the decision only
through the magnitude of its weight, and not though the interaction with other voxels. Appendix A explains how
to get these weights for the various linear classiers, as well as for nearest neighbour classiers with particular
distance measures (although in this case the weights are test example specic, rather than the same for any
example), and also shows what a typical image of voxel weights looks like. Note that you can take this further by
multiplying the weights by the average example of each class, giving you class-specic images.

A practical issue if using cross-validation is that, again, we have a dierent set of voxels and thus a dierent
classier in each fold. A heuristic approach is to group all the voxels that were selected in at least one of the
folds and run the cross-validation again on that set; if the accuracy is comparable to what was obtained with the
original cross-validation, the classier weights can be averaged across folds and considered. Alternatively, if the
accuracy using all the voxels available is high  more common in problems with few classes - we can just consider
the weights over all those voxels.

One point to be aware concerning linear classiers is that the regularization for LR or linear SVMs will cause
the weights on correlated voxels to go down proportionally to the number of correlated voxels. This is benecial
when the correlation is mostly driven by a noise source and is the reason why, in general, both types of classier
outperform GNB if all voxels are used [35] (though not if coupled with voxel selection). However, it is quite

10

classier accuracy at each voxel using various classiers

0

0

0

1

0

0

0

20

14

12

20

29

17

23

1

2

0

0

3

2

0

2

1

4

2

4

3

1

classier weights for a linear SVM trained on the entire image

0

9

1

1









1

0.5

0

1

0.5

0

1

0.5

0

1

0.5

0

3

x 10
2



0

2











B
N
G

e
s
w
v



i

B
N
G


t
h
g

i
l


s

A
D
L


t

h
g

i
l


s

M
V
S


t

h
g

i
l


s

M
V
S


r
a
e
n

i
l

Figure 4: top: Comparison of accuracy maps obtained with 6-fold cross-validation. Each row contains eight slices
from an accuracy map, inferior to superior, top of each slice is posterior. The classiers used in each row are single
voxel GNB and radius 1 neighbourhood searchlight GNB, LDA and linear SVM. On top of each slice is the number
of voxels for which accuracy was deemed signicant using FDR q = 0.01, which are highlighted in dark red. bottom:
Classier weights for a linear SVM classier trained on the entire image.

11

possible that there will be voxels that contribute to the decision but whose weights are smaller than those of other
voxels contributing voxels.

Note that, in addition to asking where class information is present, we can also ask when classes are represented
 temporal as opposed to spatial localization. This discussed less commonly in the literature and appears in two
dierent guises. The rst is to either select a feature space corresponding to voxels at all time points in a trial or
simply restrict the time points data is extracted from, as in [29] or [31]. A more sophisticated approach is used in
[37], where classiers are trained on data where classes are cleanly separated between blocks and then deployed
as sensors of emergence of class-specic patterns in event-related test data.

The issues outlined make localization of information to specic voxels less straightforward than one might
wish and, furthermore, the use of linear classiers means that no nonlinear relationships between voxels can be
learned. There is, however, an alternative approach to answering the localization question, information-based
functional brain mapping [24], which can ameliorate these problems and also address characterization questions
to some extent.

A general version of this idea can be summarized as training classiers on many small voxel sets which, put
together, cover the whole brain. One natural candidate for this is to train a distinct classer for each voxel,
using only the voxels spatially adjacent to it. This is akin to shining a searchlight on every voxel neighbourhood
in succession, and is often referred to as training searchlight classiers. Common variants for the classier are
versions of gaussian classiers (GNB, LDA) or linear SVM (the original paper used the Mahalanobis distance,
which is part of the multivariate gaussian distribution in LDA). The result is a brain map where, for every voxel,
we have the cross-validation accuracy of a classier trained on the neighbourhood surrounding it (typically, the
26 neighbours in 3D, although a wider radius could be considered). This accuracy can be converted into a p-value
using the analytical method in Section ?? or by using a permutation test as described in Section 3.4. In either
case, we will obtain a p-value map that can be thresholded using a multiple comparison correction criterion, to
identify voxel neighbourhoods of signicant accuracy.

We again have a choice of which classier to use, although in this case the tradeos are dierent. Given the
relatively small number of voxels in a neighbourhood, it is now feasible to train more complex classiers. LDA
is the rst obvious choice, in that a covariance matrix of voxels captures relationships between them, but one
could also attempt a nonlinear classier if more complex relationships are hypothesized. This will possibly be
more advantageous if there are more than two classes, although with the caveat that it is unlikely that activation
distinguishing all the classes from each other can be found in a small region.

We provide a comparison of single voxel GNB and searchlight GNB, LDA and linear SVM in Figure 4 for
our illustrative study. Whereas there are reassuring similarities in the location of voxels deemed signicantly
informative, there are also interesting dierences between the maps which we discuss in Appendix B. For reference,
the gure also provides the weights of a linear SVM classier on each voxel.

4.2 Pattern Characterization

Beyond asking whether and where class information is represented, classiers can also be used to answer the
question of how classes are represented in the brain. It is perhaps here that classiers and other machine learning
methods stand to be of greater value.

The work on information mapping in the previous section can also be considered from the point of view of
pattern characterization. Each neighbourhood classier attempts to nd relationships between the voxels in a
small set that contain information. As said earlier, given the small number of voxels, we have the possibility of
training nonlinear classiers to learn fairly complex relationships. But just how complex should one expect these
relationships to be? One extreme in the range of answers is the situation in [20], where voxels are barely more
sensitive to one class of stimulus than another, and thus do not reect an encoding of those stimuli. There is no
advantage in training a classier that does more than weigh a vote of each voxel to pool information. The other
extreme is what [13] calls a combinatorial code: the information is in the complex combination of voxel activities
in dierent classes.
In between lie situations where voxels have very selective responses (e.g. FFA or PPA in
[21]) or voxels are selective for groups of classes (e.g. semantic category classes that correspond to animate things
versus those that do not).

Thus far, we chose to describe the work done resorting to linear classiers or information mapping, both
because it is simple and a good starting point to address characterization questions. There are, however, several
recent papers that tackle this kind of question in more sophisticated ways that elude a unied view, and hence
we will cover them only briey. The common points between them are the wish to correspond relationships
between stimulus with those in fMRI data, either descriptively and actually changing classiers to reect domain
information, such as such as models of neuron populations behaviour or psychophysics division of the stimulus
space. On the rst camp, [13] shows that a neural network trained to distinguish examples of eight semantic
categories learns a representation on its hidden layer that captures a plausible taxonomy of the classes and [33],

12

on the same dataset, that the ability to distinguish the category stimuli correlates with the ability of the classier
to distinguish the corresponding classes. [2] shows that that the similarity matrix of phonetic stimuli corresponds
to the similarity matrix of patterns of activation while processing them and [1] relates perceptual similarity
of stimuli to the structure of their neural representation.
[18] introduces hidden process models, a technique
to identify multiple cognitive processes present simultaneously in the brain with overlapping spatial activation,
taking advantage of domain information and class labels to constrain their causal ordering.

Finally, two recent studies leverage domain knowledge to predict fMRI activation for new stimuli, something
which can be used to classify by comparing the predicted activation to the activation of stimuli in the training
[22] does this for visual stimuli, using a model of the responses of neuron populations in the visual cortex
set.
and predicting the fMRI signal from the model populations for each voxel.
[30] learns a model that predicts
fMRI activation for new words, based on their similarity in a space of semantic features to words for which fMRI
activation is known.

5 Conclusions

In this paper we have described the various stages in a machine learning classier analysis of fMRI data. Aside
from discussing the choices available at each analysis stage, their interactions and the practical factors conditioning
them, we explored the use of this kind of analysis to answer three types of scientic question. These are is there
information about a variable of interest (pattern discrimination), where is the information (pattern localization)
and how is that information encoded (pattern characterization). We purposefully focused on linear classiers
and the discrimination and localization questions, as these are both better understood and the ones most readers
are likely to want to answer rst. Space constraints meant that some important practical considerations had
to be left out of the main text. Various appendices are provided in a longer version of this paper available at
http://www.cs.cmu.edu/~fpereira/research.html. Software for performing a classier analysis (the Princeton
Multi-Voxel Pattern Analysis toolbox) can be found at http://www.csbmb.princeton.edu/mvpa, together with
tutorials and a very responsive online community.

We are well aware that the current frontier of research is in pattern characterization, and provided pointers to
what we think some of the most thought-provoking work in this area. We are condent that, regardless of what
progress may come, it will have machine learning methods at its root, classier-based or otherwise.

