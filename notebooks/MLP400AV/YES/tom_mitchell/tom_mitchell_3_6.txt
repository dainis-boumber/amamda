Abstract

How can artificial neural nets generalize better from  fewer examples?  In order
to  generalize  successfully,  neural  network  learning  methods  typically require
large  training data sets.  We  introduce a  neural  network learning  method  that
generalizes  rationally  from  many  fewer  data  points,  relying  instead  on  prior
knowledge encoded in previously learned neural networks. For example, in robot
control learning tasks reported here, previously learned networks that model the
effects  of robot actions  are used  to guide subsequent learning of robot control
functions.  For each  observed training example of the target function (e.g.  the
robot control policy), the learner explains the observed example in terms of its
prior knowledge, then analyzes this explanation to infer additional information
about the shape, or slope, of the target function.  This shape knowledge is  used
to bias  generalization when learning the target function.  Results are presented
applying this approach to a simulated robot task based on reinforcement learning.

1  Introduction

Neural network learning methods generalize from observed training data to new cases based
on an  inductive bias  that is  similar to smoothly interpolating between observed  training
points.  Theoretical results [Valiant, 1984], [Baum and Haussler,  1989] on learnability, as
well as practical experience, show that such purely inductive methods require significantly
larger training data sets to learn functions of increasing complexity.  This paper introduces
explanation-based neural network learning (EBNN), a method that generalizes successfully
from  fewer training examples, relying instead on prior knowledge encoded in previously
learned neural networks.
EBNN is a neural network analogue to symbolic explanation-based learning methods (EBL)
[DeJong and Mooney,  1986], [Mitchell et al.,  19861  Symbolic EBL methods generalize
based upon pre-specified domain knowledge represented by collections of symbolic rules.

287

288

Mitchell  and Thrun

For example.  in  the  task  of learning general  rules  for  robot control  EBL  can  use  prior
know ledge about the effects of robot actions to analytically generalize from specific training
examples of successful control actions. This is achieved by a. observing a sequence of states
and actions  leading to some goal. b.  explaining (i.e .  post-facto predicting) the outcome
of this  sequence  using  the  domain  theory.  then  c.  analyzing  this  explanation  in  order
to determine which features  of the initial state are relevant  to achieving  the goal of the
sequence. and which are nol In previous approaches to EBL. the initial domain knowledge
has been represented symbolically. typically by propositional rules or hom clauses. and has
typically been assumed to be complete and correct

2  EBNN: Integrating inductive and analytical learning

EBNN extends  explanation-based learning to cover situations in which prior knowledge
(also called the domain theory) is approximate and is itself1earned from scratch.  In EBNN,
this domain theory is represented by real-valued neural networks. By using neural network
representations. it becomes possible to learn the domain theory using training algorithms
such  as  the Backpropagation algorithm [Rumelhart et al.,  19861  In the robot domains
addressed  in  this paper.  such  domain theory networks correspond to action  models.  i.e.,
networks that model the effect of actions on the state of the world M:s  x  a  -+ Sf  (here
a denotes an action,  s a state. and Sf  the successor state).  This  domain theory is  used by
EBNN to bias the learning of the 'robot control function.  Because the action models may be
only approximately correct. we require that EBNN be robust with respect to severe errors
in the domain theory.
The remainder of this  section describes  the EBNN learning algorithm.  Assume that the
robot agent's  action space is  discrete. and that its domain knowledge is represented by a
collection of pre-trained action models  Mi:S  -+ Sf. one for each discrete action i.  The
learning task of the robot is to learn a policy for action selection that maximizes the reward,
denoted by R. which defines the task.  More specifically. the agent has to learn an evaluation
function  Q(s, a). which measures the cumulativefuture expected reward when action a is
executed at state s.  Once learned. the function  Q(s, a)  allows the agent to select actions
that maximize the reward  R (greedy policy).  Hence learning control reduces  to learning
the evaluation function  Q. 1
How can  the agent  use its previously learned action models  to focus  its  learning of Q?
To  illustrate. consider the episode shown in Figure 1.  The EBNN learning algorithm for
learning the target function Q consists of two components, an inductive learning component
and an analytical learning component.

2.1  The inductive component of EBNN
The observed episode is  used by the agent to construct training examples, denoted by Q,
for the evaluation function Q:

Q(sl,ad:= R

Q(s2,a2):=  R

Q(s3,a3):= R

Q could for example be realized by a monolithic neural network, or by a collection of net(cid:173)
works trained with the Backpropagation training procedure.  As observed training episodes
are accumulated,  Q will become increasingly accurate.  Such pure inductive learning typ-

IThis  approach to  learning  a policy is  adopted from  recent research on reinforcenuml learning

[Barto et al., 1991].

Explanation-Based Neural Network Learning for  Robot Control

289

_-----~~ reward:  R
(goal state)

Figure 1:  Episode:  Starting with the initial state SI. the action sequence aI, az, a3  was observed to
produce the final reward R.  The domain knowledge represented by neural network action models is
used to post-facto predict and analyze each step of the observed episode.

ically requires  large amounts of training data (which will be costly in the case of robot
learning).

2.2  The analytical component or EBNN

In EBNN, the agent exploits its domain lrnowledge to extract additional shape lrnowledge
about  the  target  function  Q.  to  speed  convexgence  and  reduce  the  number  of training
examples required. This shape lrnowledge. represented by the estimated slope of the target
function  Q.  is  then  used  to  guide the generalization process.  More specifically.  EBNN
combines the above inductive learning component with an analytical learning component
that performs the following three steps for each observed training episode:

1.  Explain:  Post-facto predict the obsexved episode (states and final reward), using the
action models  Mi  (c.f. Fig. 1).  Note that thexe may be a deviation between predicted
and observed states. since the domain lrnowledge is only approximately correct.

2.  Analyze:  Analyze  the explanation to estimate  the slope  of the target function  for
each observed state-action pair (81:, a1:)  (k  = 1..3). i.e .  extract the derivative of the
final  reward  R  with respect to  the features  of the states  81:.  according to  the action
models  Mi.  For instance. consider the explanation of the episode shown in Fig.  1.
The domain theory networks  Mi  represent differentiable functions.  Therefore it is
possible to extract the derivative of the final reward  R with respect to the preceding
state 83. denoted by "V '3R.  Using the chain rule of differentiation. the derivatives of
the final  reward  R  with respect to all  states  81:  can be extracted.  These derivatives
"V,,, R describe the dependence of the final reward upon features of the previous states.
They provide the target slopes. denoted by "V,,, Q. for the target function Q:

-

-
"V '3 Q  83, a3)

(

"V '3 R

"V'2 R

8M43 (S3)

0
83

oM4,(83)  OM42 (82)

OS3

082

OM43 (83)  8M42 (82)  8M41 (81)

883

082

881

3.  Learn: Update the learned target function to better fit both the target values and target
slopes.  Fig. 2 illustrates training information extracted by both the inductive (values)
and the analytical (slopes) components ofEBNN. Assume that the "true" Q-function

290

Mitchell  and Thrun

Figure2: Fitting slopes: Let/bea target function for which tbreeexampies (Xl, I(xt)}. (X2, I(X2)).
and (X3, 1 (X3))  are known. Based on these points the learner might generate the hypothesis g. If the
slopes are also known. the learner can do much better:  h.

is shown in Fig. 2a, and that three training instances at Xl,  X2 and  X3  are given.  When
only values  are used for learning, i.e.,  as  in standard inductive learning, the learner
might conclude the hypothesis g depicted in Fig. 2b.  If the slopes are known as  well,
the learner can better estimate the target function (Fig.  2c).  From this example it is
clear that the analysis in EBNN may reduce the need for training data, provided that
the estimated slopes extracted from the explanations are sufficiently accurate.
In EBNN, the function Q is learned by a real-valued function approximator that fits both
the target values and target slopes.  If this approximator is a neural network, an extended
version of the Backpropagation algorithm can be employed to fit these slope constraints
as well, as originally shown by [Simard et al., 19921  Their algorithm "Tangent Prop"
extends  the  Backpropagation error function  by a  second  term  measuring  the  mean
square error of the slopes.  Gradient descent  in  slope space  is  then combined  with
Backpropagation to minimize both error functions.  In the experiments reported here,
however,  we  used  an  instance-based function approximation technique described in
Sect. 3.

2.3  Accommodating imperfect domain theories

Notice that the slopes extracted from explanations will be only approximately correct, since
they are  derived from  the approximate action  models  Mi.  If this  domain knowledge is
weak, the slopes can be arbitrarily poor, which may mislead generalization.
EBNN  reduces  this  undesired  effect by estimating the accuracy  of the extracted  slopes
and  weighting the analytical component of learning by these estimated slope accuracies.
Generally speaking, the accuracy of slopes is estimated by the prediction accuracy of the
explanation (this heuristic has been named LOB *).  More specifically, each time the domain
theory is used to post-facto predict a state sk+1, its prediction st~icted may deviate from the
observed state sr+ied   Hence the I-step prediction accuracy at state Sk, denoted by Cl (i),
is defined as  1 minus the normalized prediction error:

( .)

Cl  Z

:=

1 _  II st~cted - skb+red II
max..prediction...error

For a  given  episode  we  define  the  n-step  accuracy  cn(i)  as  the product  of the  I-step
accuracies  in the next n steps.  The n-step accuracy, which measures  the accuracy of the
deri ved slopes n steps away from the end of the episode, posseses three desireable properties:
a.  It is  I if the learned domain theory is perfectly correct, b. it decreases  monotonically as
the length of the chain of inferences increases, and c. it is bounded below by O.  The n-step
accuracy is used to determine the ratio with which the analytical and inductive components

Explanation-Based Neural  Network  Learning  for  Robot  Control

291

are weighted when learning the target concept.  If an observation is  n steps away from the
end of the episode.  the analytically derived training information (slopes) is  weighted by
the n-step accuracy  times the weight of the inductive component (values).  Although the
experimental results reported in section 3 are promising. the generality of this approach is
an open question. due to the heuristic nature of the assumption LOB *.

2.4  EBNN and Reinforcement Learning

To  make EBNN  applicable to  robot learning,  we extend  it here to  a  more sophisticated
scheme  for  learning  the evaluation function  Q. namely  Watkins'  Q-Learning  [Watkins,
1989]  combined with Sutton's  temporal difference  methods  [Sutton,  19881  The reason
for doing so is  the problem 0/ suboptimal action choices in robot learning:  Robots must
explore their environment. i.e.,  they must select non-optimal actions.  Such non-optimal
actions can have a negative impact on the final reward of an episode which results in both
underestimating target values and misleading slope estimates.
Watkins'  Q-Learning  [Watkins,  1989]  permits  non-optimal actions during  the course of
In  his  algorithm  targets  for  Q  are  constructed  recursively,  based  on  the
learning  Q.
maximum possible Q-value at the next state:2

......
Q(Sk, ak)  =

{  R

if k is the final step and R final reward

,  m~ Q(Sk+l, a)  otherwise

a acuon

Here ,  (O~,~I) is a discount/actor that discounts reward over time, which is commonly
used for minimizing the number of actions.  Sutton's TD(A) [Sutton, 1988] can be used to
combine both Watkins' Q-Learning and the non-recursive Q-estimation scheme underlying
the previous section.  Here the parameter A (0 ~ A ~ 1) determines the ratio between recursive
and non-recursive components:

{  R

......
Q(sk,ak)  =

if k final step
)
(I-A),max a Q(sk+l,a)  +  A,Q(sk+l,ak+d  otherwise  (1
Eq. (1) describes the extended inductive component of the EBNN learning algoriLhm.  The
extension of the analytical component in EBNN is  straightforward.  Slopes are extracted
via the derivative of Eq. (1), which is computed via the derivative of both the models  !IIi
and the derivative of Q.

if k last step

otherwise

3  Experimental results

EBNN  has  been  evaluated  in a  simulated robot navigation  domain.  The world and  the
action space are depicted in Fig. 3a&b.  The learning task is to find a Q function, for which
the greedy policy navigates  the agent to  its  goal location (circle)  from  arbitrary starting
locations,  while avoiding collisions  with  the  walls  or the obstacle  (square).  States  are

210 order to simplify the notation. we assume that reward is only received at the end of the episode,

and is  also modeled by the action models. The extension to more general cases is straightforward.

292

Mitchell  and Thrun

(a)

(b)

0'*'

robot

(c)

error

0.2

0.15

0.1

0.05

0

number of
training examples

Figure 3:  a.  The simulated  robot world.  b.  Actions.  c.  The squared generalization  error  of the
domain theory  networks decreases monotonically as the  amount of training  data increases.  These
nine alternative domain theories were used in the experiments.

described by the local view of the agent. in terms of distances and angles to the center of
the goal  and to  the center of the obstacle.  Note that the world  is  deterministic  in  these
experiments, and that there is no sensor noise.
We  applied  Watkins'  Q-Learning  and  TD(~) as  described  in  the previous  section  with
A=0.7 and a  discount factor ;=0.8.  Each of the five  actions was  modeled by a separate
neural  network  (12  hidden  units) and  each  had  a  separate  Q  evaluation  function.  The
latter functions were represented by a instance-based local approximation technique.  In a
nutshell, this technique memorizes all training instances and their slopes explicitly, and fits
a local quadratic model over the [=3 nearest neighbors to the query point, fitting both target
values and target slopes.  We found empirically that this technique outperformed Tangent
Prop in the domain at hand.3  We also applied an experience replay technique proposed by
Lin [Lin, 19911 in order to optimally exploit the information given by the observed training
episodes.
Fig. 4 shows average performance curves for EBNN using nine different domain theories
(action models) trained to different accuracies, with (Fig. 4a) and without (Fig. 4b) taking
the n-step accuracy of the slopes into account.  Fig. 4a shows the main result.  It shows
clearly  that (1) EBNN outperfonns purely inductive learning,  (2)  more accurate  domain
theories yield better performance than less  accurate theories, and (3) EBNN learning de(cid:173)
grades gracefully as  the accuracy of the domain theory decreases, eventually matching the
performance of purely inductive learning.  In the limit, as  the size of the training data set
grows, we expect all methods to converge to the same asymptotic performance.

4  Conclusion

Explanation-based neural network learning. compared  to purely inductive learning, gen(cid:173)
eralizes  more  accurately  from  less  training data.  It replaces  the need for large training
data sets by relying instead on a previously learned domain theory. represented by neural
networks.  In this paper, EBNN has been described and evaluated in terms of robot learning
tasks.  Because the learned action models Mi are independent of the particular control task
(reward function), this knowledge acquired during one task transfers directly to other tasks.

3Note  that  in  a  second experiment not reported  here,  we  applied EBNN using  neural  network

representation for  Q and Tangent Prop successfully in a real robot domain.

Explanation-Based  Neural  Network Learning for  Robot  Control

293

(a)

Prob(success)

1

0 . 8

0.6

0 . 4

0.2

(b)

Prob(success)

1

0.8

0 . 6

0.4

0.2

0

n\lI1lber of

100  epiSOdes

" ...... .. ~.

20

40

60

80

,"",_10

"  ~

nU!Jlberof

100  episodes

Figure 4:  How  does domain  knowledge  improve  generalization?  a.  Averaged results  for  EBNN
domain theories of differing accuracies, pre-trained with from  5 to 8 192 training examples for each
action  model network.  In contrast,  the bold grey line reflects the learning curve for pure inductive
learning,  i.e.,  Q-Leaming  and TD(A).  b.  Same experiments, but without weighting  the  analytical
component of EBNN by its  accuracy,  illustrating  the importance of the WB* heuristic.  All curves
are averaged over 3 runs and are also locally window-averaged.  The perfonnance (vertical axis) is
measured on an independent test set of starting positions.

EBNN differs  from  other approaches  to knowledge-based  neural network learning,  such
as  Shavlik/fowell's KBANNs  [Shavlik and Towell,  1989]. in that the domain knowledge
and  the  target  function  are strictly separated,  and  that both are learned  from  scratch.  A
major difference  from  other model-based approaches  to robot learning,  such  as  Sutton's
DYNA  architecture [Sutton,  1990] or Jordan!Rumelhart's  distal  teacher  method  [Jordan
and Rumelhart,  1990], is the ability of EBNN to operate across  the spectrum of strong to
weak domain theories (using LOB*).  EBNN has been  found to degrade gracefully as  the
accuracy of the domain theory decreases.
We  have demonstrated the ability of EBNN to  transfer knowledge among  robot learning
tasks.  However,  there are several  open  questions  which  will  drive future research,  the
most significant of which  are:  a.  Can EBNN  be extended to  real-valued,  parameterized

294

Mitchell  and  Thrun

action spaces?  So far we assume discrete actions.  b.  Can EBNN be extended to  handle
first-order predicate logic. which is common in symbolic approaches to EBL? c. How will
EBNN perform in highly stochastic domains?  d.  Can knowledge other than slopes (such
as higher order derivatives) be extracted via explanations? e. Is it feasible to automatically
partition/modularize the domain theory as well as the target function, as this is the case with
symbolic EBL methods?  More research on these issues is warranted.

Acknowledgments
We thank Ryusuke Masuoka, Long-Ji Lin. the CMU Robot Learning GrouP. Jude Shavlik,
and Mike Jordan for invaluable discussions and suggestions. This research was sponsored in
part by the Avionics Lab, Wright Research and Development Center. Aeronautical Systems
Division (APSC). U. S. Air Force. Wright-Patterson AFB. OH 45433-6543 under Contract
F33615-90-C-1465.Arpa Order No.  7597 and by a grant from Siemens Corporation.

