ABSTRACT
We consider the problem of semi-supervised learning to ex-
tract categories (e.g., academic elds, athletes) and relations
(e.g., PlaysSport(athlete, sport)) from web pages, starting
with a handful of labeled training examples of each category
or relation, plus hundreds of millions of unlabeled web doc-
uments. Semi-supervised training using only a few labeled
examples is typically unreliable because the learning task is
underconstrained. This paper pursues the thesis that much
greater accuracy can be achieved by further constraining
the learning task, by coupling the semi-supervised training
of many extractors for dierent categories and relations. We
characterize several ways in which the training of category
and relation extractors can be coupled, and present exper-
imental results demonstrating signicantly improved accu-
racy as a result.

Categories and Subject Descriptors
I.2.6 [Articial Intelligence]: Learningknowledge acqui-
sition; I.2.7 [Articial Intelligence]: Natural Language
Processingtext analysis

General Terms
Algorithms, Experimentation

Keywords
Semi-supervised learning, bootstrap learning, information
extraction, web mining

1.

INTRODUCTION

Machine learning approaches have been shown to be very
useful for information extraction from text, including ap-
proaches that learn to extract various categories of entities
(e.g., Athlete, City) and relations (e.g., CompanyProduce-
sProduct) from structured and unstructured text [3, 28].
However, supervised training of accurate entity and rela-
tion extractors is costly, requiring a substantial number of
labeled training examples for each type of entity and rela-
tion to be extracted. Because of this, many researchers have
explored semi-supervised learning methods that use only a
small number of labeled examples of the predicate to be
extracted, along with a large volume of unlabeled text [5,
19, 1]. While such semi-supervised learning methods are
promising, they often exhibit unacceptable accuracy because
the limited number of initial labeled examples is insucient
to reliably constrain the learning process.

Figure 1: We show that signicant improvements in
accuracy result from coupling the training of information
extractors for many interrelated categories and relations
(B), compared with the simpler but much more dicult
task of learning a single information extractor (A).

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for prot or commercial advantage and that copies
bear this notice and the full citation on the rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specic
permission and/or a fee.
WSDM10, February 46, 2010, New York City, New York, USA.
Copyright 2010 ACM 978-1-60558-889-6/10/02 ...$10.00.

The thesis explored in this paper is that we can achieve
much higher accuracy in semi-supervised learning by cou-
pling the simultaneous training of many extractors, as sug-
gested in Figure 1. The intuition here is that the undercon-
strained semi-supervised learning task can be made easier by
adding new constraints that arise from coupling the training
of many extractors.

We present an approach in which the input to the semi-
supervised learner is an ontology dening a set of target
categories and relations to be learned, a handful of seed ex-
amples for each, and a set of constraints that couple the
various categories and relations (e.g., Person and Sport are
mutually exclusive). We show that given this input and
millions of unlabeled documents, a semi-supervised learning
procedure can achieve very signicant accuracy improve-
ments by coupling the training of extractors for dozens of
categories and relations. We show that our general ap-
proach improves accuracies when training both contextual-
pattern extractors that extract information from freeform
text (e.g., the pattern mayor of arg1 as an extractor for
the category City) and wrappers which extract informa-
tion from semi-structured documents (e.g., the wrapper <td
class="cty">arg1 </td> from some specic URL).

Based on results reported here, we hypothesize that even
greater accuracy improvements will be possible by forming a
more dense network of inter-constrained learning tasks. To-
ward this end, we explore two more specic points. First,
we identify three general types of coupling among target
functions that can be combined to form a dense network of
coupled learning problems. Second, we explore the impact
of coupling the training of extractors that use freeform text
with extractors that leverage semi-structured web pages,
based on the intuition that these dierent techniques should
make independent errors.

We believe that the novel contributions of this work are
as follows: Our work is the rst to couple the simultaneous
semi-supervised training of category and relation extractors.
It is also the rst to couple the training of multiple wrap-
per inducers by using mutual exclusion and type checking
relationships. Finally, this work is the rst to couple the
training (rather than the nal outputs) of freeform-text ex-
tractors and semi-structured web page wrapper inducers by
assuming that they make independent errors, a method that
we show provides higher accuracies than using either method
alone. More generally, this paper advocates large-scale cou-
pled training as a strategy to signicantly improve accuracy
in semi-supervised learning, identies three distinct types of
coupling, and experimentally evaluates their utility.

2. RELATED WORK

In this paper, we focus on a bootstrapping method for
semi-supervised learning. Bootstrapping approaches start
with a small number of labeled seed examples and itera-
tively grow the set of labeled examples using high-condence
labels from the current model. Such approaches have shown
promise in applications such as web page classication [4],
named entity classication [9], parsing [16], and machine
translation [24]. Bootstrapping approaches to information
extraction can yield impressive results [5, 9, 1]. However,
after many iterations, accuracy typically declines because er-
rors in labeling accumulate, a problem that has been called
semantic drift [10].

To reduce errors introduced in underconstrained semi-
supervised learning, several methods have been considered.
Coupling the learning of category extractors by using posi-
tive examples of one category as negative examples for oth-
ers has been shown to help limit this decline in accuracy [19,
27]. Co-Training methods exploit conditionally independent
partitions of the feature space to avoid labeling errors [4].
Other non-bootstrapping techniques have used the intuition

that dierent extraction methods should make independent
errors to motivate combining predictions from multiple ex-
tractors to improve extraction accuracy [22, 6, 18]. Type
checking relation arguments using available entity recogniz-
ers can help avoid incorrect labels [17, 20]. Our work builds
on these dierent ideas and uses them to couple the simulta-
neous bootstrapped training of many category and relation
extractors in many dierent ways.

Several machine learning frameworks have been proposed
that penalize violations of constraints on unlabeled data [2,
8, 11]. While similar in spirit, our work diers in that we
consider using many dierent kinds of constraints to learn
many dierent functions simultaneously.

In multitask learning, supervised training of related func-
tions together can yield higher accuracy than learning them
separately [23, 7]. Semi-supervised multitask learning has
used a prior that encourages related models to have similar
parameters [15]. These methods require that related tasks
share similar representations; our work exploits additional
ways in which functions can be related, and makes no as-
sumptions regarding similarity of representations across the
dierent functions being learned.

3. COUPLED TRAINING

Central to this work is the idea of coupling the semi-
supervised learning of multiple functions to constrain our
learning problem. Our method iteratively trains classiers
in a self-supervised manner. It starts by training classiers
using a small amount of labeled data, then uses these classi-
ers to label unlabeled data. The most condent new labels
are added to the pool of data used to train the models (we
say that these new labeled examples are promoted), and the
process repeats. The iterative training is coupled by con-
straints that restrict allowable candidates and promotions.
3.1 Types of Coupling

We have identied three general types of coupling:
1. Output constraints: For two functions fa : X  Ya and
fb : X  Yb, if we know some constraint on values ya
and yb for an input x, we can require fa and fb to satisfy
this constraint. For example, if fa and fb are Boolean-
valued functions and fa(x)  fb(x), we could constrain
fb(x) to have value 1 whenever fa(x) = 1.
2. Compositional constraints: For two functions f1 : X1 
Y1 and f2 : X1  X2  Y2, we may have a constraint on
valid y1 and y2 pairs for a given x1 and any x2. We can
require f1 and f2 to satisfy this constraint. For example,
f1 could type check valid rst arguments of f2, so that
x1,x2, f2(x1, x2)  f1(x1).
3. Multi-view-agreement constraints: For a function f :
X  Y , if X can be partitioned into two views where
we write X = (cid:104)X1, X2(cid:105) and we assume that both X1 and
X2 can predict Y , then we can learn f1 : X1  Y and
f2 : X2  Y and constrain them to agree. For example,
Y could be a set of possible categories for a web page,
X1 could represent the words in a page, and X2 could
represent words in hyperlinks pointing to that page (this
example was used for the Co-Training setting [4]).

3.2 Coupling Constraints Used in this Paper
In this work, the functions that we learn are category and
relation extractors, which decide if a noun phrase or pair

of noun phrases is an instance of some category or relation
(generally referred to as a predicate in the rest of this paper).
The general types of coupling discussed above are used to
learn these functions in these specic ways:

1. Mutual Exclusion: The input to our learner has a list of
pairs of predicates which are mutually exclusive. These
relationships are used to enforce an output constraint
over instances: mutually exclusive predicates cannot both
be satised by the same input x.

2. Relation Argument Type Checking: We couple the learn-
ing of relation extractors with the learning of category
extractors using type checking. For example, the argu-
ments of the CompanyIsInEconomicSector relation are
declared to be of the categories Company and Economic-
Sector. This is an example of a compositional constraint.

3. Unstructured and Semi-structured Text Features: Noun
phrases on the web appear in two types of contexts:
freeform textual contexts and semi-structured contexts.
For example, Pittsburgh occurs on the web with a dis-
tribution of freeform textual contexts such as mayor of
arg1, and it also appears with a distribution of semi-
structured contexts such as the HTML tags for a list
item at a particular URL. We assume that either of these
distributions is sucient to classify a noun phrase, and
that the two distributions are conditionally independent
given the class label of the noun phrase. We therefore
train two noun phrase classiers, one using each type of
context distribution, and require that the two classiers
agree on the label for each given noun phrase. This is an
example of a multi-view constraint.

4. ALGORITHMS

In this section, we present algorithms with which we inves-
tigate the feasibility of improving semi-supervised learning
for information extraction with coupling. The general prob-
lem addressed by these algorithms is to learn extractors to
automatically populate the predicates of a specied ontology
with high-condence instances, starting from a small set of
seed instances for each predicate and a large corpus of web
pages. We focus on extracting facts that are stated multi-
ple times, which we can assess probabilistically using corpus
statistics. We do not resolve strings to real-world entities:
the problems of synonym resolution and disambiguation of
strings that can refer to multiple entities are left for future
work. We focus our consideration of predicates on unary
relations (categories) and binary relations (ones with two
arguments, referred to as relations in this paper).

The specic inputs to our algorithms are: a large text cor-
pus and an initial ontology with predened categories, re-
lations, mutual-exclusion relationships between same-arity
predicates, and seed instances for all predicates. Each rela-
tion has an ordered pair of argument types, which specify
categories that relation instance arguments must be mem-
bers of. Some additional information is only used by freeform-
text extraction methods: seed extraction patterns for cate-
gories, and a ag for each category indicating whether in-
stances must be proper nouns, common nouns, or can be
either (e.g., instances of City are proper nouns).

The rst algorithm, Coupled Pattern Learner (CPL), is
a bootstrapping algorithm that leverages mutual-exclusion
and type-checking constraints to learn high-precision con-
textual patterns that are accurate extractors of predicate

Algorithm 1: Coupled Pattern Learner (CPL)
Input: An ontology O, and text corpus C
Output: Trusted instances/contextual patterns for

each predicate

for i = 1, 2, . . . , do

foreach predicate p  O do

Extract new candidate instances/contextual
patterns using recently promoted
patterns/instances;
Filter candidates that violate coupling;
Rank candidate instances/patterns;
Promote top candidates;

end

end

instances. The second, Coupled SEAL (CSEAL), is a set
expansion algorithm that learns wrappers to extract in-
stances from semi-structured documents and exploits the
same two types of coupling constraints. These two algo-
rithms serve to demonstrate that output and compositional
coupling techniques can improve the accuracy of bootstrapped
training of multiple types of extractors. The nal algorithm,
Meta-Bootstrap Learner (MBL), couples the training of sub-
ordinate extraction algorithms like CPL and CSEAL using
multi-view-agreement constraints.
4.1 Coupled Pattern Learner

The Coupled Pattern Learner (CPL) algorithm learns to
extract category and relation instances from unstructured
text, and is summarized in Algorithm 1. CPL learns con-
textual patterns that are high-precision extractors for each
predicate (e.g., arg1 and other software rms and arg1
scored a goal for arg2) and uses them to grow a set of high-
precision predicate instances. Noun phrases that ll in the
arg1 and arg2 blanks of patterns in sentences in the text
corpus are said to co-occur with those patterns.

At the start of execution, CPL initializes sets of pro-
moted instances and patterns with the seed instances and
patterns provided as input. In each iteration, CPL expands
these sets of promoted instances and patterns for each predi-
cate while obeying mutual exclusion and type checking con-
straints. This is accomplished by ltering out candidates
that co-occur with instances or patterns from mutually ex-
clusive classes and by requiring arguments of candidate rela-
tions to be candidates for the relevant categories. Each step
of CPL is discussed in more detail below:
4.1.1 Extracting Candidates
To start each iteration, CPL nds new candidate instances
by using the patterns promoted in the last iteration to ex-
tract noun phrases that co-occur with those patterns in the
text corpus (in the rst iteration, the seed patterns are
used). To keep the size of this set manageable, for each
predicate, CPL selects the 1000 candidates that occur with
the most patterns. An analogous procedure is used to ex-
tract candidate patterns using recently promoted instances.
We use part-of-speech-tag heuristics to limit extraction to
instances that appear to be noun phrases and patterns that
are likely to be informative. These are described next:
 Category Instances: In the blank of a category pattern,
It uses part-of-speech

CPL looks for a noun phrase.

tags to segment noun phrases and ignores determiners.
Proper noun phrases containing prepositions or conjunc-
tions are segmented using a reimplementation of the Lex
algorithm [13]. Category instances are required to obey
the proper/common noun specication of the category.
 Category Patterns: When a promoted category instance
is found, CPL extracts the preceding words as a can-
didate pattern if they are verbs followed by a sequence
of adjectives, prepositions, or determiners and option-
ally preceded by nouns (e.g., being acquired by arg1
or companies acquired by arg1) or nouns and adjec-
tives followed by a sequence of adjectives, prepositions,
or determiners (e.g., former CEO of arg1). CPL ex-
tracts the words following the instance as a candidate
pattern if they are verbs followed optionally by a noun
phrase (e.g., arg1 broke the home run record), or verbs
followed by a preposition (e.g., arg1 said that).

 Relation Instances: If a promoted relation pattern (e.g.,
arg1 is mayor of arg2) is found, a candidate relation
instance is extracted if both placeholders are valid noun
phrases (according to our part-of-speech-tag heuristics),
and if they obey the proper/common specications for
their categories.

 Relation Patterns: If both arguments from a promoted
relation instance are found in a sentence then the in-
tervening sequence of words is extracted as a candidate
relation pattern if it contains no more than ve tokens,
has a content word, and has an uncapitalized word.

4.1.2 Filtering Candidates using Coupling
Candidate instances and patterns are ltered to enforce
mutual exclusion and type checking constraints. A candi-
date instance is rejected unless the number of times it co-
occurs with a promoted pattern is at least three times more
than the number of times it co-occurs with patterns from
mutually exclusive predicates. This soft constraint is much
more tolerant of the inevitable noise in web text as well as
ambiguous noun phrases than a hard constraint. Candidate
patterns are ltered in the same manner using promoted
instances.

4.1.3 Ranking Candidates
Next, for each predicate CPL ranks candidate instances
using the number of promoted patterns that they co-occur
with so that candidates that occur with more patterns are
ranked higher. Candidate patterns are ranked using an es-
timate of the precision of each pattern p:

P

P recision(p) =

iI count(i, p)

count(p)

where I is the set of promoted instances for the predicate
under consideration, count(i, p) is the number of times in-
stance i co-occurs with pattern p in the text corpus, and
count(p) is the number of times pattern p occurs in the cor-
pus.

4.1.4 Promoting Candidates
For each predicate, CPL then promotes at most 100 in-
stances and 5 patterns according to the rankings from the
previous step. Instances and patterns are only promoted if
they co-occur with at least two promoted patterns or in-
stances, respectively. Relation instances are only promoted

Algorithm 2: Coupled SEAL (CSEAL)
Input: An ontology O, and text corpus C
Output: Trusted instances/wrappers for each predicate
for i = 1, 2, . . . , do

foreach predicate p  O do

begin Call existing SEAL code to:

Query for documents containing recently
promoted instances;
Learn wrappers for each document returned;
Extract new candidates using wrappers;

end
Filter wrappers that extract candidates that
violate coupling;
Rank candidate instances;
Promote top candidates;

end

end

if their arguments are candidates for the specied categories
(that is, they co-occur with at least one promoted pattern
for the category, and are not promoted instances of a mutu-
ally exclusive category).

4.1.5 Large-Scale Implementation
CPL was designed to allow ecient learning of many pred-
icates simultaneously from a large corpus of sentences ex-
tracted from web text. Gathering the statistics needed from
the text corpus is the most expensive part of the algorithm.
The statistics needed come from two types of queries. First,
in the extraction step, CPL has a list of promoted instances
and patterns, and needs to know which patterns and in-
stances co-occur with those instances and patterns. Sec-
ond, in the ltering and ranking steps, CPL needs to know
which candidate patterns occur with which promoted in-
stances, and which candidate instances occur with which
promoted patterns. CPL gathers these statistics from a pre-
processed text corpus which species how many times each
noun phrase occurs with each category pattern in the cor-
pus, and also how many times each pair of noun phrases
occurs with each relation pattern. The preprocessing can be
done quickly using using the MapReduce framework [12]. In
each iteration of CPL, CPL gathers corpus statistics from
this data set by scanning through the preprocessed data in
two passes: one for extracting candidates and one for count-
ing co-occurrences. CPL can perform one pass in about 15
minutes from a data set derived from 200 million web pages
(see Section 5.1.2 for details on the corpus).

4.1.6 Uncoupled Pattern Learner
In our experiments, we use a variant of CPL called Un-
coupled Pattern Learner (UPL) which removes the coupling
constraints from CPL. Candidates are not ltered using mu-
tual exclusion with other predicates, and relation arguments
are not type checked. UPL is equivalent to independent
semi-supervised learning of each extractor.
4.2 Coupled SEAL

CPL is an example of a semi-supervised text pattern learn-
ing algorithm that is aided by coupling. To demonstrate how
coupling can improve a dierent, already implemented ex-
traction algorithm we consider SEAL [26], a wrapper induc-

URL:
Wrapper:
Content:
URL:
Wrapper:
Content:
URL:
Wrapper:
Content:

http://www.shopcarparts.com/
.html" CLASS="shopcp">arg1 Parts</A> <br>
acura, audi, bmw, buick, cadillac, chevrolet, chevy, chrysler, daewoo, daihatsu, dodge, eagle, ford, ...
http://www.allautoreviews.com/
</a><br> <a href="auto_reviews/arg1/
acura, audi, bmw, buick, cadillac, chevrolet, chrysler, dodge, ford, gmc, honda, hyundai, inniti, isuzu, ...
http://www.hertrichs.com/
<li class="franchise arg1"> <h4><a href="#">
buick, chevrolet, chrysler, dodge, ford, gmc, isuzu, jeep, lincoln, mazda, mercury, nissan, pontiac, scion, ...

Table 1: Examples of wrappers constructed by CSEAL for various web pages given the seeds: Ford, Nissan,
Toyota. In the table, arg1 is a placeholder for extracting instances.

tion algorithm, and how we can add coupling constraints on
top of an existing implementation that we treat as a black
box. First, we will describe the existing algorithm, and
then we will describe how we add coupling constraints.

SEAL is a set-expansion system that accepts input ele-
ments (seeds) of some target set S and automatically nds
other probable elements of S in semi-structured documents
such as web pages by querying the web using the seeds. The
algorithm implemented in SEAL constructs page-specic ex-
traction rules, or wrappers, that are independent of the hu-
man language and markup language of the web pages. SEAL
can expand sets of category instances as well as binary rela-
tion instances. Every category wrapper is dened by char-
acter strings, which specify the left context and right con-
text necessary for an entity to be extracted from a page.
Relation instance wrappers also are dened using an inx
context that separates the two arguments of the instance.
These context strings are selected to be maximally-long con-
texts that bracket at least one occurrence of every seed on
a page. Table 1 shows a few examples of such wrappers for
categories. An instance is extracted by a wrapper if it is
found anywhere in the document with left and right con-
text identical to that of the wrapper. When given large sets
of seeds, SEAL can be congured to subsample the seeds
some number of times [25]. Subsampling samples a subset of
the seeds and uses that subset as a query to a search engine,
which is necessary because using all examples in one query
would typically not yield any matched results.

SEAL does not have a mechanism for exploiting mutual-
exclusion or type-checking constraints. Wrappers for each
predicate are learned independently in SEAL. Our algo-
rithm, Coupled SEAL (CSEAL), adds these constraints on
top of SEAL. CSEAL is summarized in Algorithm 2.
In
each iteration of bootstrapping, we invoke SEAL using the
recently promoted instances. SEAL returns a list of new
candidate instances and documents that they were extracted
from. CSEAL lters out any document that extracts a can-
didate instance that is a member of a mutually exclusive
predicate. Additionally, CSEAL only considers candidate
relation instances if their arguments are candidate instances
for the respective category types. These forms of coupling
should lter out cases where a subsampled set of seeds hap-
pens to occur on a page but that page does not in fact con-
tain a valid list of predicate instances. They should also
lter out cases where instances of a predicate that is more
general than the one being learned are listed (e.g., if a long
list of locations of various types is present on a page, but we
are learning some specic type of location).

After ltering, CSEAL ranks all candidate instances by
the number of unltered wrappers that extracted them, and
promotes the top 100 instances that were extracted by at

Algorithm 3: Meta-Bootstrap Learner (MBL)
Input: An ontology O, a set of extractors E
Output: Trusted instances for each predicate
for i = 1, 2, . . . , do

foreach predicate p  O do
foreach extractor e  E do

Extract new candidates for p using e with
recently promoted instances;

end
Filter candidates that violate mutual-exclusion or
type-checking constraints;
Promote candidates that were extracted by all
extractors;

end

end

least two wrappers. To deal with web pages from the same
domain that repeat the same list, only one page from a do-
main is counted in ranking candidates. Without limiting
consideration to domains, navigational and other template-
generated elements that repeat many times can dramatically
skew the results.

In our experiments below, CSEAL refers to the algorithm
described here, and SEAL refers to CSEAL without the l-
tering step: SEAL does not lter out wrappers that ex-
tract candidates that violate mutual-exclusion relations, and
SEAL does not enforce relation instance type checking.
4.3 Meta-Bootstrap Learner

Meta-Bootstrap Learner (MBL) couples the training of
multiple extraction techniques using a multi-view constraint
that requires them to agree. MBL is summarized in Algo-
rithm 3. MBL is based on the intuition that the errors made
by dierent extraction techniques should be independent.

In this paper, the subordinate algorithms used with MBL
are CSEAL and CPL. When using CSEAL and CPL with
MBL, the subordinate algorithms do not promote instances
on their own.
Instead, they skip the promotion step and
report evidence about each candidate to MBL, and MBL
is responsible for promoting instances. MBL uses a simple
combination method: MBL promotes any instance that has
been recommended by both techniques while obeying the
mutual-exclusion and type-checking constraints specied in
the ontology.

5. EXPERIMENTAL EVALUATION

We designed experiments to explore three main questions:
First, does coupling learning using mutual-exclusion and

type-checking constraints improve the performance of CPL
relative to uncoupled, independent learning using UPL? Sec-
ond, do mutual-exclusion and type-checking constraints im-
prove the performance of CSEAL relative to the uncoupled
methods of SEAL? Finally, does MBL achieve better perfor-
mance than CPL and CSEAL by combining their outputs
with a multi-view constraint?

To answer these questions, we ran CPL, UPL, CSEAL,
SEAL, and MBL with CPL and CSEAL as subordinate ex-
tractors for 10 iterations of learning. We then compared the
dierences in performance between several pairs of methods
to see the eects of coupling.

Direct comparison to previous work is dicult for a num-
ber of reasons, including the lack of availability of implemen-
tations and the lack of a large shared web corpus. However,
our evaluation directly tests the usefulness of the coupled
approach that we are advocating in this paper. We believe
that the uncoupled baselines are reasonable and competitive
large-scale uncoupled approaches.
5.1 Experimental Methodology

Input Ontology

5.1.1
The ontology used in all experiments contained categories
and relations from two main domains: companies and sports.
Extra categories were added to provide negative evidence to
the domain-related categories (e.g., Hobby for EconomicSec-
tor; Actor, Politician, and Scientist for Athlete and Coach;
and BoardGame for Sport) and also to provide wider variety
for experiments (e.g., Shape, Emotion). Table 2 lists all of
the categories in the leftmost column, and Table 3 lists the
relations in the leftmost column. Categories were initialized
with 15 seed instances and 5 seed patterns. The seed in-
stances were specied by a human, and the seed patterns
for each category were derived from the generic patterns
of Hearst [14]. Relations were initialized with 15 seed in-
stances, 5 seed negative instances (typically incorrect varia-
tions of positive seed examples), and no seed patterns (since
it is not obvious how to generate good seed patterns from
relation names). Most predicates were declared as mutually
exclusive with one another (examples of exceptions include
SportsTeam and University; KitchenItem and ProductType;
and Company and Product).

5.1.2 Corpus for CPL
The text corpus used by CPL was from a 200-million-page
web crawl. We parsed the HTML, ltered out non-English
pages using a stop-word-ratio threshold, then ltered out
web spam and adult content using a bad word list. The
pages were then segmented into sentences, tokenized, and
tagged with parts-of-speech using the OpenNLP package.
Finally, we ltered the sentences to eliminate those that were
likely to be noisy and not useful for learning (e.g., sentences
without a verb, without any lowercase words, with too many
words that were all capital letters). This yielded a corpus of
roughly 514 million sentences.

As discussed in Section 4.1.5, we processed these sentences
to create a data set of noun phrase and contextual pattern
co-occurrence counts. To manage the size of the data set, we
ltered out all noun phrases and contexts that only occurred
once in the corpus. This yielded a data set that contained
14.9 million unique contextual patterns for categories, 24.6
million unique noun phrases, 232.0 million unique pairs of

noun phrases that co-occur together, and 35.7 million unique
contextual patterns for relations.

5.1.3 Parameters for SEAL
In our experiments with CSEAL and SEAL, we used an
implementation provided by the original authors of SEAL.
SEAL was congured to subsample the examples provided 5
times for categories and 10 times for relations to mitigate the
relatively higher sparsity of relations. SEAL downloaded up
to 50 web pages for each search query using results from the
Google search engine. Thus, the corpus for SEAL was the
web as indexed by Google. The minimum context length
for a wrapper was set to 2, which meant that each part of a
wrapper needed to be at least 2 characters long.

5.1.4 General Experimental Procedure
When comparing two algorithms, we ran each algorithm
for 10 iterations of bootstrapping, and then assessed the
instances promoted by the algorithms. To evaluate the pre-
cision of all instances promoted by an algorithm on a per-
predicate basis, we sampled 30 instances from the set of
promoted instances for each predicate, pooled together the
samples, and submitted the instances to Mechanical Turk
for labeling. This gave an estimate of how accurate all of
the instances were and measured the degree to which a par-
ticular method avoided semantic drift. We also compared
algorithms at matching levels of recall. For each predicate,
we only considered the rst k instances promoted by each
algorithm, where k was the minimum number of instances
promoted for that predicate between the two algorithms. We
refer to this method of comparison as the minimum recall
method in the results below. We sampled 30 instances from
each of these two sets of instances, and also submitted them
to Mechanical Turk.

While samples of 30 instances do not produce tight con-
dence intervals for individual estimates of precision for a
single predicate, they are sucient for testing for the eects
in which we are interested.

CPL can reliably extract the proper case of an instance,
but lists of items on the web often use arbitrary case con-
ventions, so CSEAL cannot reliably extract the proper case
of an instance. Because of this, our evaluation ignored case,
and presented all instances to the evaluators in lower case.

5.1.5 Mechanical Turk Labeling
The various estimates of precision required for our eval-
uation yielded 10717 unique instances. We submitted each
of these instances to Mechanical Turk for labeling and had
three dierent individuals label each instance. Mechanical
Turk has been shown to be an inexpensive and fast method
for obtaining labels for language tasks [21]. To estimate
the accuracy of the labels produced by this procedure, we
sampled 100 instances at random, and manually judged the
accuracy of their labels. We found that 96 out of the 100
were correctly labeled using the majority vote. The four er-
rors were: a false positive with entomology there labeled
as an AcademicField (the labelers ignored the segmentation
error), and three false negatives: informs as a Profession-
alOrganization, love seats as Furniture, and the relation in-
stance CompanyCompetesWithCompany(bhp, rio). This
suggests that the labels may be biased towards false nega-
tives, which in turn suggests that our precision estimates in
the remainder of the paper may be pessimistic.

Precision (%)

Promoted Instances (#)

Predicate
AcademicField
Actor
Animal
Athlete
AwardTrophyTournament
BoardGame
BodyPart
Building
Celebrity
CEO
City
Clothing
Coach
Company
Conference
Country
EconomicSector
Emotion
Food
Furniture
Hobby
KitchenItem
Mammal
Movie
NewspaperCompany
Politician
Product
ProductType
Profession
ProfessionalOrganization
Reptile
Room
Scientist
Shape
Sport
SportsEquipment
SportsLeague
SportsTeam
Stadium
StateOrProvince
Tool
Trait
University
Vehicle
Average
Weighted average

97
97
70
87
7
77
63
0

90
100
90
100
53
70
97
30
100
100
97
43
100
100
97
97
100
87
97
57
77
88
93
97
60
97
-
27
-

CPL UPL CSEAL SEAL MBL
100
100
97
100
77
90
93
93
97
100
97
97
100
97
100
93
77
83
100
90
90
100
90
100
100
100
70
50
93
87
100
100
100
85
73
23
86
87
90
77
97
97
93
77
90
91

70
100
80
87
57
80
77
33
100
33
97
97
93
97
93
57
60
77
90
100
77
73
83
97
90
80
90
73
73
93
95
64
97
77
77
20
100
90
93
77
40
53
93
67
78
79

83
33
50
17
7
13
17
50
90
30
100
20
63
83
53
33
23
53
70
0
33
3
50
57
60
60
83
63
53
63
3
0
30
7
13
10
7
30
57
63
13
40
97
30
41
42

100
77
87
27
83
100
90
37
10
60
80
57
50
13
50
100
97
37
77
63
57
77
27
7
17
7
83
23
27
87
63
93
90
47
90
13
59
59

7
63
57
80
87
53
83
93
52
100
50
78
86

100
90
33
100

3

46
199
741
132
86
10
176
597
347

CPL UPL CSEAL SEAL MBL
181
380
307
555
79
31
61
14
514
30
603
102
242
784
92
207
138
211
272
95
127

203
1000
144
276
146
126
80
57
72
322
368
167
619
245
437
130
34
183
89
215
77
8

83
188
1000

1000

95

2

1000
1000
974
1000
1000
1000
1000
1000
747
1000
1000
1000
1000
1000
928
1000
1000
1000
1000
1000
1000
960
1000
1000
1000
1000
999
1000
1000
1000
1000
643
1000
733
1000
1000
1000
944
1000
1000
1000
1000
1000
1000
976

169
183
241
101
127
159
171
163
54
3

130
26
284
174
14
506
343
161
59
44
516
98
199

903
1000
1000
930
902
907
922
1000
1000
902
1000
973
838
1000
990
1000
1000
992
1000
963
936
900
1000
1000
1000
990
1000
1000
973
943
912
913
971
985
1000
902
901
903
767
1000
1000
1000
1000
1000
960

1000
1000
483
811
55
357
11
224
718
179
178
1000
712
916
104
19
25
83
43
283
58
11
301
102
202
561
234
1000
460
360

154
566
1000

30
0
31
0
58
149
12
928
28
225
52
10
864
944
114
713
21
961
50
271

Table 2: Precision (%) and counts of promoted instances for each category using CPL, UPL, CSEAL, SEAL, and
MBL.

Predicate
CompanyAcquiredCompany
AthletePlaysForTeam
AthletePlaysInLeague
AthletePlaysSport
CEOOfCompany
CityLocatedInCountry
CityLocatedInState
CoachCoachesInLeague
CoachCoachesTeam
CompanyIsInEconomicSector
CompanyCompetesWithCompany
CompanyHasOceInCity
CompanyHasOceInCountry
CompanyHeadquarteredInCity
LeaguePlaysGamesInStadium
CompanyProducesProduct
ProductInstanceOfProductType
SportUsesSportsEquipment
StadiumLocatedInCity
StateHasCapitalCity
StateLocatedInCountry
TeamHasHomeStadium
TeamPlaysAgainstTeam
TeamHasHomeCity
TeamPlaysInLeague
TeamPlaysSport
TeamWonAwardTrophyTournament
Average
Weighted Average

Precision (%)

Promoted Instances (#)

CPL UPL CSEAL SEAL MBL

CPL UPL CSEAL SEAL MBL

97
100

-

100
100
93
100

-

100
93
100

-
-
50
-
97
73
33
100
60
97
100
100

-

100

-
90
89
91

77
93
78
47
100
57
70
-

100
97
67
63
90
53
-
93
67
3
20
70
40
87
80
57
67
70
70
69
61

100
100
100

-

-

100
100

0
-
-
-
-
-

100

-
-
-

-
-

100
77
-

100
100

100
100

-
91
92

-
76
57
100
100
100
93
-
-
-
-

100

-

100
100

-
-
87
70
73
97
100

-
93
100
100

-
91
90

-

100

-

100
100
100
100

100

-

-
-
-
-
-
-

100

-
33
90
-

100
100

-

93
9
0
83
18
185
76
0

324
583
28
0
0
2
0
54
153
15
7

266
194
97
238

100
100
100

-
95
99

0
7
0

128
95

230
269
18
258
18
787
194

0

668
889
123
526
195
532

0

215
484
1330
600
188
1299
208
2088
680
255
177
262
463

0
4
14
1
0
9
34
1
0
0
0
0
0
1
0
0
0
5

200

0
46
179

0
0

104
30
0
23

0
17
82
1
1

577
537

0
0
0
0
4
0
2

177

0
0
15
554
495
653
106

0
29
749
30
0

149

0
96
0

109

1

136
54
0
6
0
0
0
0
0
0
8
0
6
56
0
61
92
0
11
23
37
0
26

Table 3: Precision (%) and counts of promoted instances for each relation using CPL, UPL, CSEAL, SEAL, and MBL.

5.1.6 Supplementary Online Materials
Several dierent types of materials from our evaluation are
posted online at http://rtw.ml.cmu.edu/wsdm10_online:

 Seeds for all predicates.
 All instances promoted by MBL, CPL, UPL, CSEAL,

and SEAL.

 All textual patterns promoted by pattern learning in the

MBL, CPL, and UPL experiments.

 Browseable knowledge bases in XML format of all pro-
moted instances and candidate instances from the runs of
MBL, CPL, and CSEAL, with patterns and URLs that
extracted each instance.

 All judgments obtained from Mechanical Turk.
 An example screenshot from a Mechanical Turk task.
 Templates used to create the Mechanical Turk tasks,

which may be of general use.

5.2 Mutual Exclusion and Type Checking

To explore the eects of coupling predicates using mutual-
exclusion and type-checking constraints, we compared cou-
pled and uncoupled methods for learning contextual pat-
terns for freeform text:
 CPL: The algorithm as described in Section 4.1.
 UPL: This method is an uncoupled version of CPL; it
does not couple predicates using mutual-exclusion con-
straints or type checking. Relation instance arguments
are not ltered using their categories and candidate in-

stances and patterns are not ltered out based on vio-
lations of mutual exclusion. The common/proper noun
specications of arguments are used to lter out implau-
sible instances.

We also compared coupled and uncoupled methods for
learning wrappers to extract lists of instances from semi-
structured web pages:
 CSEAL: The algorithm as described in Section 4.2.
 SEAL: This method uses the implementation of SEAL
provided by the authors of SEAL. As in the UPL method,
it does not couple the learning of predicates using mutual-
exclusion constraints or type checking.

5.2.1 Results
Table 2 gives estimates of the precision of promoted in-
stances for each category for each algorithm, as well as the
number of promoted instances for each category after 10 iter-
ations. The Average row averages across all predicates for
which instances were promoted. The Weighted Average is
an estimate of the instance-level precision across all predi-
cates obtained by weighting the precision for each predicate
by the number of instances promoted for that predicate. Ta-
ble 3 gives this information for each relation, as well. Across
all categories and relations, CPL has higher average preci-
sion than UPL, and CSEAL has higher average precision
than SEAL. These results suggest that coupling using type
checking and mutual exclusion signicantly reduces the error
rates of the learned extractors.

Another method of comparing which algorithms perform
the best is to use the sign test, which is a non-parametric

Comparison

All Promotions
Wins

p-value

Minimum Recall
Wins

p-value

CPL vs. UPL
CSEAL vs. SEAL
MBL vs. CPL
MBL vs. CSEAL

55 vs. 12
36 vs. 15
34 vs. 18
31 vs. 14

1.03e-07
0.00460
0.0365
0.0161

37 vs. 8
17 vs. 12
28 vs. 6
18 vs. 6

1.54e-05

0.458

0.000195

0.0227

Table 4: Various pairs of methods compared based on
the precision of all promotions for each predicate (All
Promotions) and the precision of the instances promoted
cut o at the minimum recall out of the pair for each
predicate (Minimum Recall). Wins record how many
predicates had superior precision for each method, and
the p-value according to a sign test is given. All re-
sults are statistically signicant at the 5% level except
for CSEAL vs. SEAL at minimum recall.

Figure 2: Examples of extracted facts. The generaliza-
tions of Software were given in the seed ontology; all
other facts were discovered by CPL. Values shown for
productInstances and companiesInSector for Soft-
ware are a subset of the full set of promoted values.

hypothesis test. The test statistic needed to compare, for ex-
ample, CPL with UPL, is obtained by counting the number
of predicates for which CPL performed better than UPL,
and vice versa, ignoring ties. This test gracefully handles
predicates where only one method promoted instances: we
prefer the method which extracted some instances rather
than none for such predicates.

Table 4 compares CPL vs. UPL and CSEAL vs. SEAL for
the precision of all promoted instances for each predicate, as
well as the minimum recall sample discussed above. CPL
performs statistically signicantly better than UPL for both
methods of sampling. CSEAL is signicantly better than
SEAL with respect to the precision of all promotions, but
is not signicantly better when thresholding recall to the
minimum recall for each predicate. These results conrm
that coupling yields signicantly higher accuracies across all
predicates than using independent, uncoupled learning. The
results for CSEAL vs. SEAL suggest that coupling prevents
CSEAL from promoting some incorrect instances but with
some loss in recall.

Figure 2 gives some examples of the type of information
extracted in our experiments. The initial seed examples pro-
vided specied that software is a ProductType and an Eco-
nomicSector; the rest of the information in the gure was
extracted by CPL.
5.3 Multiple Extraction Techniques

Tables 2 and 3 also give estimates of the precision of pro-
moted instances for each predicate for MBL after 10 itera-
tions. Across both relations and categories, MBL has the
highest precision of promoted instances out of all of the al-

gorithms considered, which indicates that adding the multi-
view-agreement constraint results in further avoidance of se-
mantic drift. Table 4 gives sign test results for comparing
MBL vs. CPL and MBL vs. CSEAL, which allows us to
judge whether or not MBL improves over its subordinate
algorithms. All sign tests show statistically signicant dif-
ferences: MBL is superior to both CPL and CSEAL when
comparing both the precision of all promoted instances as
well as the precision of promoted instances at the minimum
recall of either method. This suggests that coupling CPL
and CSEAL with a multi-view coupling constraint that as-
sumes independent errors yields more accurate learning than
either method used alone.

5.4 Discussion

The results presented here demonstrate that adding more

coupling improves the accuracy of our learned extractors.

One of the biggest challenges in applying bootstrap learn-
ing algorithms is determining when to stop the bootstrap-
ping process. Ideally, an algorithm would be able to respect
the boundaries of a closed set. In this respect, the results for
the Country category for MBL are particularly compelling.
MBL promoted 207 instances of countries with an estimated
precision of 93%. CSEAL promoted 130 instances with an
estimated precision of 97%. Without coupling, Country per-
forms poorly, drifting into a more general Location category.
The categories for which the coupled algorithms still have
the most diculty (e.g., ProductType, SportsEquipment,
Traits, Vehicles) tend to be common nouns. We expect that
a more complete hierarchy of common nouns would better
constrain these categories and yield better accuracies.

The coupled algorithms generally had high accuracies for
relations, but suered from sparsity. SportUsesSportsEquip-
ment suered because the SportsEquipment category per-
formed poorly, resulting in bad type checking. StateHasCap-
ital and CompanyHeadquarteredInCity drifted to the more
general relations of StateContainsCity and CompanyHas-
OperationsInCity. These latter two cases can be improved
by adding the ability to infer negative examples using the
knowledge that these are functional relations: patterns that
extract multiple capitals for the same city could be ltered
out using this knowledge.

Our experiments included ve relations for which no in-
stances were promoted by any algorithms: CoachCoaches-
Athlete, AthletePlaysInStadium, CoachWonAwardTrophy-
Tournament, SportPlaysGamesInStadium, and AthleteIs-
TeammateOfAthlete. These relations show that some re-
lations are not easy to extract using the extraction methods
used in this paper. However, many of these relations could
be inferred from instances promoted in our current work.
We plan to investigate learning to infer such relations.

6. CONCLUSION

We have presented methods of coupling the semi-
supervised learning of category and relation instance ex-
tractors and demonstrated empirically that coupling fore-
stalls the problem of semantic drift associated with boot-
strap learning methods. This empirical evidence leads us to
advocate large-scale coupled training as a strategy to signif-
icantly improve accuracy in semi-supervised learning.

Software  isA: Product Type, Economic Sector  productInstances: iTunes, Excel, Adobe       Photoshop, Microsoft Outlook, AutoCAD,       Kazaa  companiesInSector: Infosys, SAP, Microsoft,       IBM, Wipro, SymantecTigers  isA: Mammal, Sports Team  teamHomeStadium: Comerica Park  teamCoach: Les Miles  teamWonTrophy: World Series  teamPlaysAgainstTeam: Yankees, Royals,       Sox, White Sox, Red Sox, WarriorsAcknowledgments
This work is supported in part by DARPA, Google, a Yahoo!
Fellowship to Andrew Carlson, and the Brazilian research
agencies CNPq and CAPES. We also gratefully acknowl-
edge Jamie Callan for making available his collection of web
pages, Yahoo! for use of their M45 computing cluster, and
the anonymous reviewers for their comments.

