Abstract

Most learning algorithms that  are  applied to  text  cat-
egorization  problems rely  on a bag-of-words document
representation,  i.e.,  each word occurring in  the  docu-
ment is  considered as  a separate feature.  In this  paper,
we investigate the use of linguistic  phrases as input fea-
tures  for  text  categorization problems. These features
are  based on information extraction  patterns  that  are
generated  and  used  by  the  AUTOSLOG-TS
system.  We
present experimental results  on using such features  as
background knowledge for  two machine learning  algo-
rithms  on a  classification
task  on the  WWW. The re-
sults  show that  phrasal  features  can improve the  pre-
cision of  learned theories at  the  expense of  coverage.

Introduction

Most machine learning  algorithms  for  text  categoriza-
tion  represent  documents as  a  bag of  words.  Typically,
each  word is  treated  as  a  separate  feature.  For  exam-
ple,  a  HTML-document might  be  classified
as  the  home
page of  a  student  if  the  word student  occurs on the  page.
However, student  will  also  occur  on many other  pages
on  the  WWW, including  most  pages  of  computer  sci-
ence  departments.  On the  other  hand,  a  sentence  like
"I  am a  student  of  computer science  at  Carnegie  Mel-
lon  University"  is  clearly  indicative  of  a  students  home
page.  It  is  worth noting  that  none of  the  words that  ap-
pear  in  this  sentence  are  good predictors  of  a  student
home page,  nor  is  any subset  of  these  words, because all
of  them are  quite  common on pages  related
to  Carnegie
Mellons  School  of  Computer Science. 1  What makes a
difference  is  the  linguistic  role  that  these words have in
this  sentence.

AuToSLoG-TS, originally

conceived  for  information
extraction  (Riloff  1996a),  is  able  to  provide  a learner
with  features  that  capture  some of  the  syntactic  struc-
ture  of  natural  language text.  For  the  above input  sen-
tence,  it  extracts  the  following four  features:  I  am <_>,

1 In this  sentence, the  most predictive words for a student
home page are  I  and am. They are  quite  reliable  features
for  the  problem of  discriminating between student pages and
other  departmental pages and,  for  this  task,  should not  be
removed by a stop list  (see  also  (Craven et  al.  1998a)).

5

the  last

<_> is  student,  student  of  <_>,  and student  at  <_>.2
All  four  of  them are  highly  indicative  of  student  pages
and at  least
three  of  them are  quite  unlikely
to  appear  on  other  types  of  pages.  Note that  the  last
of  these,  student  at  <_>, does  not  match a  contiguous
piece  of  text,  but  is  based on prepositional  attachment
to  the  word student.

In  previous  work (e.g.,

(Riloff  & Lorenzen 1998)),
promising results  on the  usefulness  of  such  phrases  for
text  categorization
tasks  were obtained  using  simple
thresholding  methods to  find  the  best  classi-
statistical
terms.  In  this  paper,  we report  on some exper-
fication
the  utility  of  such
iments that  aimed at  investigating
phrases  as  features
machine
learning  algorithms,  namely the  Naive Bayes classifier
RAINBOW and  the  separate-and-conquer
learning
algorithm  I~IPPER. The case  study  is  performed  in  a
text  classification

for  two  state-of-the-art

task  on  the  WWW.

rule

Generating  Phrasal  Features

i.e.,

training

AUTOSLOG was  developed  as  a  method  for  automati-
cally  constructing  domain-specific  extraction  patterns
corpus.  As  input,  Au-
from  an  annotated
the  in-
ToSLoG  requires  a  set  of  noun phrases,
formation that  should  be  extracted
from the  training
documents.  AUTOSLOG
then  uses  syntactic  heuristics
to  create  linguistic  patterns  that  can extract  the  desired
information  from the  training  documents (and  from un-
typically  rep-
seen  documents). The extracted  patterns
resent  subject-verb  or  verb-direct-object
relationships
(e.g.,  <subject> teaches  or  teaches  <direct-object>)  as
well  as  prepositional  phrase  attachments  (e.g.,
teaches
at  <noun-phrase> or  teacher  at  <noun-phrase>).  See
(Riloff  1996b) for  a  more detailed  description  of  Au-
ToSLOG.

The  key  difference

between  AUTOSLOG and
the  removal  of  the
corpus.  AuToSLoG-TS

1996a)  is
training

AuToSLoG-TS (Riloff
need  for  an  annotated
for  all  noun
simply  generates  extraction
role
phrases  in  the  training  corpus  whose syntactic
matches  one  of  the  heuristics.
Table  1  shows  three
of  the  16 syntactic  heuristics  employed in  the  current

patterns

2We use  angle  brackets  <>  to  represent

variables.

Syntactic  Heuristic
noun  aux-verb
<d-obj>
<sub  j>  aux-verb  noun
noun  prep  <noun-phrase>

Phrasal  Feature
I  am  <_>
<_> is  student
student  of  <_>
student  at  <_>

Table h  Syntactic  heuristics
that  are  used  for  finding
phrasal  features  in  the  sentence "I  am a student  of  com-
puter  science  at  Carnegie Mellon  University."

of  words occurring  in  documents of  that

probabilities
class  independent  of  their  context.  By doing so,  RAIN-
BOW makes the  naive  independence  assumption.  More
precisely,
the  probability  of  document d  belonging  to
class  C is  estimated  by multiplying  the  prior  probability
Pr(C)  of  class  C with  the  product  of  the  probabilities
Pr(wiIC)  that  the  word wi  occurs  in  documents of  this
class.  This  product  is  then  normalized  by the  product
of  the  prior  probabilities  Pr(wi) of  all  words.

that  can  be  detected

version  of  AUTOSLoG-TS as  well  as  their  correspond-
ing  extraction  patterns
in  the
example sentence  "I  am a  student  of  computer science
at  Carnegie  Mellon  University".  Note  that  the  third
twice  in  the  same
syntactic  heuristic  of  table  1  fires
sentence  using  two different  prepositional  attachments
forms  of  the
to  the  noun student.  Also,  the  different
verb to  be are  not  discerned  for  the  first
two patterns.
However, in  general  words that  occur  in  different  forms
will  not  be  treated
in  the  same way. For  example,  the
sentence  "Here  are  the  students  at  our  department"
will  not  match the  last  extraction  pattern  of  table  1,
because  the  word student  occurs  in  plural
instead  of
that  the  plural  version  stu-
singular.
dents  at  <_> occurs  more frequently  on  departmental
pages  than  on  student  home pages.  That  such  small
differences  in  the  use  of  words can make a big  difference
for  text  classification  was previously  observed in  (Riloff
1995).  A set  of  experiments  demonstrated
the
occurrence  or  absence  of  linguistic
phrases  of  the
above  form  can  be  successfully  used  for  recognizing
relevant  documents of  the  terrorist
domain of  the  4th
Message  Understanding  Conference  (MUC-4) (Riloff
& Lorenzen  1998).

It  seems likely

that

patterns

In  this  paper,  we explore  the  potential  use  of  the
generated  by  AuToSLoG-TS as
extraction
phrasal  features  for  state-of-the-art
learning  algorithms.
In  the  following, we will  briefly  describe the  learning  al-
gorithms  and  the  test  domain (classifying  WWW pages
related  to  computer science  departments),  and then  dis-
cuss  the  results  of  our  experiments.

Description

of

the  Experiments

features  with  two

the  use  of  phrasal

learning  algorithms,

We evaluated
the  naive  Bayes classi-
different
fier  RAINBOW and  the  rule
learning  algorithm  RIP-
PER. The data  set  used  for  our  experiments  is  the  4-
universities  dataset,  which has  been collected  for  the
WebKB project  at  Carnegie  Mellon  University.

Rainbow
RAINBOW
fication
McCallum at  CMU3.  It  estimates
that
a  document is  a  member of  a  certain  class  using  the

is  a  Naive  Bayes classifier
tasks  (Mitchell  1997),  developed  by  Andrew

for  text  classi-

the  probability

3  Available

from http ://www. cs. cmu. edu/af s/cs/

Pr(Cld)  :=  Pr(C)

Pr(wilC)

As many of  the  probabilities

4=1  Pr(wi)

(1)
Pr(wi[C)  are  typi-

cally  0.0  (hence  their  product  will  be  0.0),  RAINBOW
smoothes the  estimates  using  the  technique  proposed
by  Witten  & Bell  (1991).  A related  problem  --
the
fact  that  for  text  classification
tasks  many estimates  of
Pr(C[d)  for  the  winning class  tend  to  be  close  to  1.0
and often  will  be exactly  1.0  because of  floating-point
round-off  errors  --  is  addressed by providing  an option
to  incorporate  a  correction
term  based  on  Kullback-
Leibler  Divergence.  This  correction  does  not  change
the  classification
of  the  documents, but  provides  more
realistic  probability  estimates.  This option  was used in
our  experiments  to  obtain  better  confidence  estimates
for  the  predictions,  which we used  for  generating  re-
graphs.  A more detailed  description  of
call/precision
this  smoothing  technique  and  of  RAINBOW
in  general
can  be found in  (Craven et  al.  1998a).

(Quinlan  1990))  until

Ripper
RIPPER4  (Cohen 1995) is  an efficient,  noise-tolerant  rule
learning  algorithm  based  on  the  incremental  reduced
error  pruning  algorithm  (Fiirnkranz  ~  Widmer 1994;
Ffirnkranz  1997).  It  learns  single  rules  by greedily
adding  one condition  at  a  time  (using  FOILs informa-
tion  gain  heuristic
longer  makes incorrect  predictions  on the  growing set,
a  randomly chosen  subset  of  the  training  set.  There-
after,
the  learned  rule  is  simplified  by deleting  condi-
tions  as  long  as  the  performance of  the  rule  does  not
decrease  on the  remaining set  of  examples (the  pruning
set).  All  examples covered by the  resulting  rule  are  then
removed from the  training  set  and a  new rule  is  learned
in  the  same way until  all  examples are  covered  by  at
least  one rule.  Thus,  RIPPER is  a  member of  the  fam-
ily  of  separate-and-conquer  (or  covering)  rule  learning
algorithms  (Fiirnkranz  1998).

the  rule

What makes RIPPER particularly  well-suited  for  text
categorization  problems is  its  ability
to  use  set-valued
features  (Cohen 1996).  For  conventional  machine learn-
ing  algorithms,  a document is  typically  represented  as
a  set  of  binary  features,  each encoding the  presence  or
absence  of  a  particular  word in  that  document.  This
results  in  a very inefficient  encoding of  the  training  ex-
amples because much space  is  wasted for  specifying  the
com/

://www.  research,

4  Available

from

http

art.

project/theo- 1 I/www/naive-bayes. html.

~wcohen/ripperd.html.

Class
Student
Faculty
Staff
Project
Department
Course
Other
Total

All  Examples

(19.82%)
(13.57%)
(1.65%)
(6.09%)
(2.20%)
(11.23%)
(45.45%)
(100.00%)

Test  Examples
558
(13.42%)
153
(3.68%)
46
(1.11%)
(2.07%)
s6
4
(0.10%)
244
(5.87%)
(73.76%)
3067
(100.00%)
4158

1641
1124
137
504
182
930
3764
8282

Table 2:  Class  Distribution
set

in  the  4  Universities  data

absence  of  words in  a  document. RIPPER allows  to  rep-
resent  a  document as  a  single  set-valued  feature
that
simply  lists  all  the  words occurring  in  the  text.  Con-
ceptually,  RIPPERs  use  of  such  a  set-valued  feature  is
no different
than  the  use  of  binary  features  in  conven-
tional
learning  algorithms,  although  RIPPER makes use
of  some optimizations.  For  the  remainder of  this  paper,
we will  continue  to  think  of  each word (or  phrase)  as
separate  binary  feature.

The  WebKB  Project
The goal  of  the  WebKB Project  (Craven  et  al.  1998b) is
to  extract  a  computer-understandable  knowledge base
from  the  WWW whose contents  mirrors
the  contents  of
could  be  imagined  for
the  WWW. Many applications
such  a  knowledge base.  For  example,  it  could  enhance
the  capabilities  of  currently  available  search  engines
that  can only  answer word-occurrence queries  (e.g.,  AL-
TAVISTA)  or  that  rely  on  a  manually constructed  knowl-
edge  base  about  the  contents  of  WWW pages  (e.g.,  YA-
HOO) by  enabling  them to  answer  questions
like  "Who
teaches
course  X  at  university  Y?"  or  "How many
students  are  in  department  Z?".  Currently,  a  proto-
type  system  uses  an  ontology  of  common entities
in
faculty,  staff,
computer science  departments (students,
courses,  projects,  departments)  and relations  between
them (e.g.,  professor  X is  the  instructor  of  course
and  the  advisor  of  student  Z).  The prototype  crawls
the  net  and uses  learned  knowledge to  classify  pages of
computer science  departments  into  that  ontology.

In  order  to  furnish  the  crawler  with  some learned  do-
main knowledge,  a  set  of  8,282  training  documents was
collected
from  the  WWW pages  of  various  computer
science  departments. 5  About half  of  the  pages  are  a
fairly  exhaustive  set  of  pages from the  computer science
departments  of  four  universities:  Cornell,  Texas,  Wis-
consin,  and  Washington.  The remaining  4,120  pages
are  more or  less  randomly collected  pages  from various
computer  science  departments.  The  pages  are  manu-
ally  classified  into  the  categories Student, Faculty, Staff,
Course,  Project,  Department,  and Other.  Table  2  shows
the  frequency distributions  of  these  classes  in  the  data

5Available

from  http

://www.  cs.  cmu.  edu/afs/cs,

cmu.

edu/proj

ect/theo-20/www/data/

set.  The Other class  is  a very  heterogeneous class  that
contains  pages  found at  these  departments  that  are  not
classified  as  any of  the  six  relevant  classes.  Note, how-
ever,
that  one  of  the  assumptions  made in  manually
classifying  the  pages was that  each  real-world  entity  is
represented  by  only  one page.  Thus,  if,  e.g.,  a  fac-
ulty  member organizes  his  personal  home page  as  a
hypertext  document containing  separate  pages  for  his
research  interests,  his  publications,  his  CV, and point-
ers  to  the  research  projects  he is  involved in,  only the
top  page  that  links  these  informations  together  would
be used  to  represent  him in  the  class  Faculty,  while  all
other  pages would be classified  as  Other.  This is  clearly
not  a  perfect  solution,  as  other  people  might organize
the  same information  into  a  single  page.  Thus it  can
be  expected  to  be  hard  to  discriminate  between  cer-
tain  pages of  the  Other class  and pages of  the  relevant
classes.

A more detailed  description  of  this  data  set  and of
some previous  work can  be  found  in  (Craven  et  al.
1998b; 1998a).

Setup

Experimental
For  our  experiments,  all  pages  were stripped  of  their
HTML tags,  converted to  lower case,  and all  digits  were
replaced  with  a  D.  AuToSLoc-TS was run  on  each  doc-
ument and  the  generated  extraction  patterns  were en-
coded as  one-word tokens  and put  into  a  separate  file.
We compared three  different
for  each
algorithm:  One where each  word was treated  as  a  fea-
ture,  one where each  phrase  (extraction  pattern)  was
treated  as  a  feature,  and one where both  were consid-
ered  as  features.  For the  last  case,  corresponding files
were simply  appended  to  produce  the  input  files
for
RAINBOW, while  for  RIPPER we  encoded  each  docu-
ment with  two set-valued  features,  one containing  the
words in  the  document, the  other  containing  the  tokens
that  represent  the  discovered  phrasal  features.

representations

The algorithms  were evaluated  using  the  same proce-
dure  as  in  (Craven et  al.  1998b):  Each experiment  con-
sists  of  four  runs,  in  which the  pages of  one of  the  four
universities  are  left  out in  turn.  Thus, each training  set
consists  of  the  4,120 pages  from miscellaneous  univer-
sities  plus the  pages from three  of  the  four  universities.
The results  of  each  of  the  four  runs  were combined us-
ing  micro-averaging, i.e.,
the  predictions  made for  the
four  test  sets  were thrown together  and all  evaluation
measures  were computed on  this  combined set.  The fre-
quency distribution  of  the  classes  in  this  combined test
set  is  shown in  the  second column of  table  2.

Unless  noted  otherwise,  RIPPER was used  with  its
default  options.  It  should be noted that  in  this  setting,
RIPPER sorts  the  classes  according to  their  inverse  fre-
quency and learns  decision  lists
for  discriminating  one
class  against  all  classes  ranked  below it.  Hence, the
biggest  class,
is  treated  as
a default  class  and no rules  are  learned for  it.  In  the  ex-
periments  with  RAINBOW, we made use  of  its  built-in
stemming,  and features

that  occurred  only  once  were

in  our  case  the  Other class,

Representation
words
phrases
words+phrases

RAINBOW

RIPPER

45.70% 77.78%
51.22% 74.51%
46.79% 77.10%

Table 3:  Overall  predictive  accuracy

not  considered.  The  experiments  with  RIPPER were
performed  without  stemming and  without  any  form  of
feature  subset  selection.

Results

language  phrases  and  are  classified

Accuracy
in  terms  of  predictive  accu-
Table  3 shows the  results
racy  on the  4  test  sets  combined. The results  are  quite
diverse.  For  RAINBOW, the  use  of  phrases  increases  pre-
dictive  accuracy,  while  for  RIPPER,  the  opposite  is  the
case.  An investigation  of  the  confusion  matrices  (not
shown) shows that  this  is  mostly  due to  an increase  in
the  number of  pages  that  are  classified
as  Other.  For
example,  about  4.5% of  the  pages  do  not  contain  any
natural
as  Other
by  default.  This  decrease  in  the  number of  pages  for
which a  commitment to  one of  the  six  relevant  classes
is  made, in  some sense,  confirms  that  the  phrasal  fea-
tures  are  much more conservative  in  their  predictions:
They tend  to  appear  less  frequently,  but  some of  them
are  highly  typical  for  certain  classes.  This has  a  posi-
tive  effect  on the  overall  accuracy,  because RAINBOW
in
general  overpredicts
using  word-based features  classifies  only  1216 pages as
Other,  while  the  test  set  contains  3067 pages  of  this
class.  RAINBOW using  phrasal  features  classifies
1578
examples as  Other  without  significantly
changing  the
classifiers  precision  in  the  six  relevant  classes.  This
means that  the  majority  of  Other pages is  misclassified
into  one of  the  six  relevant  classes,
thus  producing a
low overall  accuracy and a  low precision,  as  we will  see
in  the  next  section.

the  6  minority  classes.  RAINBOW

The situation

is  converse  for  RIPPER,  which classi-
fies  3008 pages as  Other using  words, while  classifying
3517 pages  as  Other  when using  only  phrases.  Hence,
for  RIPPER,  whose classification
accuracy is  above the
default  accuracy  (73.76%),  predicting  more examples
Other has  a  negative  effect  on its  performance.

These differences

in  the  number of  pages  classified
with  the  default  class  are  also  the  main reason  for
the  huge performance differences  of  the  two algorithms.
One reason  for  this  phenomenon might  be  the  nature
of  of  the  Other class,  which is  likely
pages  that  are  very  similar
classes.
It  might  well  be  the  case  that  this  problem
imposes greater  difficulties  upon a linear  classifier  such
as  RAINBOW, whereas the  rule  learner  RIPPER  is  better
able  to  focus  on the  small  differences  between similar
pages in  different  classes.  It  might also  be the  case  that
the  differences
in  the  experimental  setup  (stemming,

to  pages  of  the  relevant

to  contain  many

Rainbow

"-....o.

i

100.00 -
95.00 -
90.00 -
85.00 -
80.00-
75.00- :
70,00- i
65.00
60.00-
55.00 -
50.00
45,00
40.00
35.00
30.00
25,00
20.00 -
15.00 -
I0,00
5.00-
0.00-

0.00

20.00

40.00

00.00

80.00

100.00

Recall

Words

, %~ ................

Precision

100,00 -
95.00 -
90.00-
85.00 -
80,00 --
75.00 --
70.00 --
65.00 --
00.00--
55.00 --
50,00 -
45.OO --
40.00 -
35.60
30.00
25.00
20.00 -
15.00  -- ----
10.00
5.00
0.00--

0,00

20.00

40.00

00.00

80.00

100.00

Rec~ll

Figure  1:  Combined  precision/recall
(bottom)  and  RIPPER (top).

for  RAINBOW

pruning  of  features
that  occur  only  once)  contributed
to  this  effect,  but  it  cannot be its  sole  cause,  because
no stemming was used  on  the  phrasal  features
in  both
cases.

vs.  Recall

Precision
More interesting,  however, is  a closer  look at  the  pre-
cision/recall
graphs  shown in  figure  1.  In  this  graph,
recall  means the  percentage  of  correctly  predicted  ex-
amples of  the  six  relevant  classes  (not  including  Other),
while  precision  means the  percentage  of  correct  predic-
tions  in  all  predictions  for  examples of  these classes.

For generating  these  graphs,  we associated  a  confi-
dence  score  with  each  prediction.  The confidence  as-
sociated  with  a  predictions  of  RAINBOW
is  simply  the

class

student

faculty

staff

project

department

course

phrase

5 I am <_>
7 <_> is student
14 student in <_>
12 university of <_>
26 professor of <_>
37 <_> is professor
19 I am <_>

<_> is associate

30  manager of <_>
18  associated <_>
28  related <_>
58  affiliated <_>
12  department of <_>
13  undergraduate <_>
16  graduate <_>
25  <_> due
34  due <_>
40  fall <_>

Table 4:  The best  three  phrases  for  each class  and their
rank  in  a  sorted  list  of  features  in  the  words+phrases
representation.

student :- my,

student,
am,
DDD_DDDD.

Training Accuracy:
Test Accuracy (Washington): 9

149 2

student :- I am <_>,

<_> is student,
institute of <_>.

Training Accuracy:
Test Accuracy (Texas):

43 0
5 0

student :- my,

student,
<_> is student,
DDD_DDDD.

Training Accuracy:
Test Accuracy (Texas):

125 0
12 0

estimated  probability  of  the  example being  in  the  pre-
dicted  class.  RIPPER does not  provide  probability  esti-
mates,  so  we associated  with  each  prediction  of  RIPPER
a confidence value  c_A_+_L_ where e  (i)  is  the  number
training  examples that  are  correctly  (incorrectly)  clas-
sified  by the  rule  that  predicted  the  class  for  this  exam-
ple. 6  We chose  the  Laplace-estimate  for  estimating  the
accuracy of  a rule  in  order  to  penalize  rules  that  cover
only  a  few examples.

c+i+2 

We measured  the  precision  of  the  algorithms  at  5%
increments in  recall.  A recall  level  of  5% is  equivalent
to  correctly  classifying  about  55 of  the  1091 examples
of  the  six  relevant  classes.  The precision  at  a  recall
level  of  n% was measured by first  sorting  the  predictions
for  examples of  the  relevant  classes  according to  their
confidence  score.  Then we went down the  list  until
the
number of  correct  predictions  exceeded n% of  the  total
number of  examples of  the  relevant  classes,  i.e.,  until  we
had recalled  n% of  the  examples of  these  classes.  The
number of  correct  predictions  over  the  total  number of
predictions  in  that  segment of  the  list
is  the  precision
score  associated  with that  recall  level.  If  a recall  of  n%
was reached within a  series  of  predictions  with identical
confidence scores,  we continued to  process  the  list  until
the  confidence  score  changed.  Hence,  in  some cases,
the  points  for  the  precision/recall  curves are  not  exactly
lined  up  at  5% increments.

~Note that  these  are  not  necessarily  equivalent  to  the
number of  examples that  are  correctly  or  incorrectly  classi-
fied  by the  rule  in  isolation,  because RIPPER learns  decision
lists.  This means that  the  learned rules  are  processed in
order,  and only examples that  are  not classified  by previous
rules can be classified  by subsequent rules.

Table 5:  The rules  with  highest  confidence  scores  for
words,  phrases,  and phrases+words respectively,
along
with  the  number of  correct  and incorrect  predictions
they  make on their  respective  training  and test  sets.

In  both  graphs,  it  is  apparent  that  at  lower  recall

the  phrasal  features  outperform  the  word-based
levels,
representation.  This  supports  the  hypothesis  that  some
phrasal features  are  highly predictive  for  certain  classes,
but  in  general  have low coverage.  This  is  particularly
obvious in  the  significant  decrease  of  the  maximum re-
call  for  RIPPER if  it  uses only phrasal  features.

The results

for  the  combined representation  are  more
diverse:  RAINBOW assigns  higher  weights  to  word-based
features  than  to  phrasal  features,  so  that  the  results
for  the  combined representation  are  mostly  determined
by the  words.  Table 4  illustrates
this  by showing the
top  three  phrases  for  each class,  and their  rank in  the
list  of  features  ordered  by a  weighted  log-odds  ratio

log

(~).  But  even

the  word-
Pr(wi,c)
based  features  determine  the  shape  of  the  curve,  the
addition  of  phrasal  features  results
in  small  improve-
ments at  all  recall  levels.

though

The situation  is  quite  similar  for  RIPPER in  the  sense
that  only  a  few of  the  learned  rules  actually  use  the
phrasal  features.  However, the  phrases  frequently  oc-
cur  in  rules  with  a  high  confidence  score  and  make a
crucial  difference  at  the  low recall  end of  the  graph.
Table  5  shows, for  each  of  the  three  representations,
the  rule  that  was assigned the  highest  confidence  score
based  on  its  performance  on  the  respective
training

set.  It  is  very interesting  to  observe that  the  best  rule
for  the  word-based feature  set  and  the  best  rule  for
the  words+phrases representation  are  almost  identical.
Both require  the  presence  of  the  word my and a  seven-
digit  number (most  likely  a  phone number).  However,
requires  the  pres-
while  the  word-based representation
ence  of  the  words am and  student,
the  feature-based
requires  the  presence  of  the  phrase  <_>
representation
is  student.  Recall  that  all  documents containing
the
sentence  "I  am a  student."  match both  conditions,  be-
cause  the  phrase  matches all  forms of  the  auxiliary  verb
to  be.  Looking at  the  accuracies  of  these  two rules  shows
that  the  first  one matches 163 examples in  the  entire
data  set,  5  of  which being  non-student  pages,  while  the
second rule  matches only  137 examples,  but  all  of  them
are  of  the  class  Student.

Bigrams vs.  Phrases

"Ph+ W {Rainbow)

\

35.60
30,C0
25,C~

-,4
\

The rule  that  was formed using  only  the  phrasal  fea-
tures  can  be loosely  interpreted  as  classifying  all  doc-
uments  that  contain
the  sentence  "I  am a  student  at
of  <_>."  as  student  home pages.  While
the  institute
this  rule  is  sensible  and accurate,  it  has  a much lower
coverage than  both  other  rules.

It  is  also  interesting  to  note  that  the  third  rule  con-

the  redundant  condition

tains
the  redundant  condition  student.  Apparently,
RIPPERs information  gain  heuristic  first  preferred  this
condition  over  the  more accurate  phrase  that  contains
the  word,  because  the  word feature  had higher  cover-
age.  After  it  discovered  that  the  phrase  has  to  be added
nevertheless,
is  not  removed
because  RIPPERs pruning  algorithm  only  considers  the
removal of  a  final  sequence of  conditions  from a  rule/
It  should  be remarked that  it  is  no coincidence  that
all  three  rules  are  predicting  the  class  Student.  In  fact,
most of  the  improvements at  the  low recall  end of  the
curves  is  due to  respective
improvements in  the  pre-
diction  of  the  Student  class.  Precision/recall
graphs
for  this  class  look  very similar  to  the  graphs shown in
figure  1,  while for  the  other five  relevant  classes  no dra-
matic  improvements could  be observed.  We have seen  in
table  4  that  RAINBOW attributes
a  somewhat lower  im-
portance to  phrasal  features  in  the  other  5 classes,  and
an investigation  of  the  learned  rules  shows that  only  a
few of  the  top-ranked  rules  for  classes  other  than  Stu-
dent  actually  use  phrasal  features.  This  may be partly
due to  the  fact  that  there  are  too  few training  examples
for  some of  these  classes.  We plan  to  further  investigate
this  on a more balanced data  set,  like  the  20 newsgroups
s
data  used  in  (Lang 1995).

Two other  interesting

ure  1  are  the  differences

observations

to  make in  fig-
in  maximum recall  between

7Thls

is  one  of

the  differences

between  RIPPER  and

the

original  incremental reduced error  pruning algorithm, which
--  less  efficiently  --  considers all  conditions as candidates
for  pruning (Ffirnkranz  & Widmer 1994).

awe have also  investigated  whether the  Student  class
contains  a higher percentage of  natural  language text,  but
we could not  empirically confirm this  hypothesis (using  the
crude measure number of  phrases per  class  over  number of
words per class).

O.O0

20.00

40.00

60.00

80.00

I00.00

Figure  2:  Precision/recall
(top)
and  RIPPER (bottom)  using  phrases+words  versus  bi-
grams+words.

curves  for  RAINBOW

classifies

RAINBOW and  RIPPER,  and  the  consistent
decline  of
precision  at  the  low recall  end for  RAINBOW. The for-
mer is  due to  fact  that  RAINBOW
less  pages as
Other,  thus  increasing  the  percentage  of  recalled  pages
in  the  relevant  classes  at  the  expense of  precision.  We
conjecture  that  the  latter  phenomenon is  caused  by vio-
lations  of  the  independence assumption of  Naive Bayes,
which leads  to  overly  optimistic  confidence  scores  for
some predictions.
For  example,  if  the  phrase  <_> is
student  occurs  in  a  document, it  is  quite  likely  that  at
least  one of  the  phrases  student  at/in/of  <_> will  oc-
cur  in  the  same sentence,  which will  unduly boost  the
probability
that  this  document is  a  student  home page.

to  Bigrams

Comparison
While the  results  of  the  last  section  have shown that
phrasal  features  can improve the  precision  of  text  clas-
sifters
at  the  expense  of  recall,  one can  ask  whether
similar  results  could  have been obtained  by using  se-
quences  of  words instead  of  single  words as  features.
To this  end,  we have  repeated  some of  the  above  ex-
periments  using  a representation
that  considers  single
words and pairs  of  words (bigrams)  as  features.

the  same  phenomenon
For  RAINBOW, we observed
the  shape  of  the  re-
as  with  the  use  of  phrases:
call/precision
curve  is  determined  by the  word-based
features,  but  the  precision  is  slightly  higher.  The two
curves at  the  bottom of  figure  2 are  the  recall/precision
curves  for  bigrams+words  versus  phrases+words  for
RAINBOW. There  are  no  notable  differences
except  a
small peak for  the  phrase representation  at  a recall  level
of  10%. A comparison of  the  best  bigram features  (ta-
ble  6)  to  the  best  phrase  features  (table  4)  shows that
the  average  rank  of  the  top  three  features  among the

class

student

department

faculty

staff

project

course

bigram

depart

4 home page
7 comput scienc
17 depart of
2  comput  scienc
4  the  depart
11  scienc
8 comput scienc
10 of comput
12  univ  of
4 satoshi  sekin
5 rice edu
8 in japanes
12 research group
16 audio latex
17 latex postscript
9  will  be
14  offic hour
19  the cours

Table 6:  The best  three  bigrams for  each  class  and their
rank  in  a  sorted  list  of  features
representation.

in  the  bigrams+words

student  :-  my,

student,
i_aal,
science,
research.

Training Accuracy:
184 3
Test Accuracy (Washington): 9 3

Table  7:  A highly  ranked  rule  using  bigrams  with  the
number of  correct  and incorrect  predictions  it  makes on
training  and test  examples.

word-based features
appear  to  be less  sensible  (cf.,  e.g.,
classes  Staff  and Project).

is  higher  for  bigrams,  while  they
the  features  for

is  different.

For  RIPPER, however, the  situation

In
the  upper two curves of  figure  2,  the  phrase  representa-
tion  outperforms  the  bigram representation  at  the  low
recall  end.  Looking at  the  rules  that  apply  there,  we
find  that,  unlike  the  rules  of  table  5,  for  the  bigrams
the  rule  with the  highest  confidence is  not  one of  the  4
top-ranked  rules  of  the  Student  class  in  the  respective
folds  of  the  4-fold  cross-validation.  The rule  shown in
table  7 is  most similar  to  the  rules  shown in  table  5.  It
is  ranked  number 6  by our  confidence  measure.

RAINBOW
RIPPER

Words  Phrases  Bigrams
26,628
224,751
92,666  116,990  872,275

36,216

Table  8:  Number of  features  considered  by  RAINBOW
and  RIPPER

tures  it  uses  is  much smaller  than  the  number features
RIPPER considers.  It  can  be  seen  that,  although  there
are  slightly  more different  phrases  than  words,  their
numbers  are  in  about  the  same order  of  magnitude,
while  the  number of  bigrams  found  in  the  documents
is  one order  of  magnitude larger.

Conclusions

Our experiments  have shown that  the  use  of  linguistic
features  can improve the  precision  of  text  categorization
at  the  low recall  end.  For the  rule  learning  algorithm
P~IPPER,  adding such features  was able  to  boost the  pre-
cision  by  about  10% to  more than  90% when recalling
about  5% of  the  1091 test  examples of  a  text  catego-
rization
task  on  the  WWW. Although  phrasal  features
require  more engineering effort  (e.g.,
the  syntax of  both
the  training  and the  test  documents has  to  be parsed),
they  seemingly  provide  a  better  focus  for  rule  learn-
ing  algorithms.  This  effect  was less  remarkable for  the
naive Bayes classifier

that  we used.

text?

language

Nevertheless,  it  should be noted that  we were not  able
to  improve the  precision  of  the  classifiers  at  high recall
levels,
the  reason being  that  the  phrasal  features  typi-
cally  have a  very  narrow focus.  However, it  should  also
be noted  that  the  test  domain used  in  our  case  study
may not  have  been  an  ideal  choice  for  evaluating
the
utility  of  phrasal  features,  because significant  parts  of
WWW pages  do  not  contain  natural
Thus,  we plan  to  further  evaluate
commonly used  text  categorization
as  the  20  newsgroups  dataset
REUTERS newswire  collection

technique  on
this
benchmarks,  such
(Lang  1995)  and  the
(Cohen & Singer  1996).
On the  other  hand,  we are  also  working  on  further
improving the  classification  accuracy on the  4 universi-
ties  data  set  used  in  this  case  study.  For example, all
approaches used  in  this  study  performed very  poorly  on
the  Project  class  (precision  was typically  below 20%).
The reason for  this  is  most likely  the  heterogeneous na-
ture  of  topics  that  are  dealt  with on project  pages.  We
believe  that  this  problem can be solved  by looking at  the
information  that  is  contained  on or  near  the  links  that
point  to  such  pages  and plan  to  investigate
this  topic
to  those  employed in
further  using  techniques  similar
this  study.

Finally,

it  is  worth to  take  a look  at  the  number of
different  features  that  are  considered by the  learning  al-
gorithms  (table  8).  Recall  that  we used  RAINBOW with
stemming on  words and  bigrams,  as  well  as  pruning  of
all  features  that  occur  only once,  so  the  number of  fea-

9For example, one of  the  most characteristic  words for
the  classes  Faculty and Staff  is  the  word "Fax", which is
more likely  to  occur in  addresses than in  the  natural  lan-
guage portion  of  a  Web-page.

Acknowledgements

is  supported  by  a  SchrSdinger-
(J1443-INF)  of  the  Austrian  Fonds  zur
Forsehung  (FWF).
support  by  the  Darpa

Johannes  Ffirnkranz
Stipendium
FSrderung  der  wissenschaftlichen
Tom Mitchell
HPKB program  under  contract
Ellen  Riloff  acknowledges support  from NSF grants  IRI-
9509820  and  IRI-9704240.  We wish  to  thank  Mark
Schmelzenbach  for  generating

the  AUTOSLoG-TS data.

F30602-97-1-0215.

acknowledges

