Abstract

@cs.berkeley.edu

Many algorithms rely critically on being given a good metric over their
inputs. For instance, data can often be clustered in many plausible
ways, and if a clustering algorithm such as K-means initially fails to nd
one that is meaningful to a user, the only recourse may be for the user to
manually tweak the metric until sufciently good clusters are found. For
these and other applications requiring good metrics, it is desirable that
we provide a more systematic way for users to indicate what they con-
sider similar. For instance, we may ask them to provide examples. In
this paper, we present an algorithm that, given examples of similar (and,
, learns a distance metric over
if desired, dissimilar) pairs of points in
that respects these relationships. Our method is based on posing met-

ric learning as a convex optimization problem, which allows us to give
efcient, local-optima-free algorithms. We also demonstrate empirically
that the learned metrics can be used to signicantly improve clustering
performance.



1 Introduction

The performance of many learning and datamining algorithms depend critically on their
being given a good metric over the input space. For instance, K-means, nearest-neighbors
classiers and kernel algorithms such as SVMs all need to be given good metrics that reect
reasonably well the important relationships between the data. This problem is particularly
acute in unsupervised settings such as clustering, and is related to the perennial problem of
there often being no right answer for clustering: If three algorithms are used to cluster a
set of documents, and one clusters according to the authorship, another clusters according
to topic, and a third clusters according to writing style, who is to say which is the right
answer? Worse, if an algorithm were to have clustered by topic, and if we instead wanted it
to cluster by writing style, there are relatively few systematic mechanisms for us to convey
this to a clustering algorithm, and we are often left tweaking distance metrics by hand.
In this paper, we are interested in the following problem: Suppose a user indicates that
) are considered by them to be similar. Can we
certain points in an input space (say,

automatically learn a distance metric over
that respects these relationships, i.e., one that

assigns small distances between the similar pairs? For instance, in the documents example,
we might hope that, by giving it pairs of documents judged to be written in similar styles,
it would learn to recognize the critical features for determining style.










One important family of algorithms that (implicitly) learn metrics are the unsupervised
ones that take an input dataset, and nd an embedding of it in some space. This includes
algorithms such as Multidimensional Scaling (MDS) [2], and Locally Linear Embedding
(LLE) [9]. One feature distinguishing our work from these is that we will learn a full metric
over the input space, rather than focusing only on (nding an embed-
ding for) the points in the training set. Our learned metric thus generalizes more easily to
previously unseen data. More importantly, methods such as LLE and MDS also suffer from
the no right answer problem: For example, if MDS nds an embedding that fails to cap-
ture the structure important to a user, it is unclear what systematic corrective actions would
be available. (Similar comments also apply to Principal Components Analysis (PCA) [7].)
As in our motivating clustering example, the methods we propose can also be used in a
pre-processing step to help any of these unsupervised algorithms to nd better solutions.
In the supervised learning setting, for instance nearest neighbor classication, numerous
attempts have been made to dene or learn either local or global metrics for classication.
In these problems, a clear-cut, supervised criterionclassication erroris available and
can be optimized for. (See also [11], for a different way of supervising clustering.) This
literature is too wide to survey here, but some relevant examples include [10, 5, 3, 6],
and [1] also gives a good overview of some of this work. While these methods often
learn good metrics for classication, it is less clear whether they can be used to learn
good, general metrics for other algorithms such as K-means, particularly if the information
available is less structured than the traditional, homogeneous training sets expected by
them.
In the context of clustering, a promising approach was recently proposed by Wagstaff et
al. [12] for clustering with similarity information. If told that certain pairs are similar or
dissimilar, they search for a clustering that puts the similar pairs into the same, and dis-
similar pairs into different, clusters. This gives a way of using similarity side-information
to nd clusters that reect a users notion of meaningful clusters. But similar to MDS and
LLE, the (instance-level) constraints that they use do not generalize to previously unseen
data whose similarity/dissimilarity to the training set is not known. We will later discuss
this work in more detail, and also examine the effects of using the methods we propose in
conjunction with these methods.

2 Learning Distance Metrics

, and are given information that certain

(1)
that respects this;

pairs of them are similar:

Suppose we have some set of points 	



 
How can we learn a distance metric 
"#)$+*,*

'&(







if
and! are similar
"#! between points 
and #
7-.#98
1-2#4365
 -.#/*,*

$+0

specically, so that similar points end up close to each other?
Consider learning a distance metric of the form

(2)

of Mahalanobis distances over

to be diagonal, this corresponds to learning a metric in which

To ensure that this be a metricsatisfying non-negativity and the triangle inequality

"#!%$

we require that 5
be positive semi-denite, 5;:=< .1 Setting 5>$@? gives Euclidean
distance; if we restrict 5
the different axes are given different weights; more generally, 5 parameterizes a family
to nding a rescaling of a data that replaces each point  with 5
1Technically, this also allows pseudometrics, whereDFEHGJILKNMPO/QSR does not implyITQUM
2Note that, but putting the original dataset through a non-linear basis functionV and considering
GXV/GJI!OY7VZGJMPONO\[L]^GXV/GJI!OY_VZGJMPONO , non-linear distance metrics can also be learned.

.2 Learning such a distance metric is also equivalent
and applying the

ACB



.






&



W
*,*

say,

have,

standard Euclidean metric to the rescaled data; this will later be useful in visualizing the
learned metrics.
A simple way of dening a criterion for the desired metric is to demand that
pairs of points
small squared distance between them:

does not
can be a set of pairs of points known to be
dissimilar if such information is explicitly available; otherwise, we may take it to be all


	

&



in 
. This is trivially solved with 5
*,*
	


useful, and we add the constraint

collapse the dataset into a single point. Here,!
pairs not in


*,*

"
*,*


/-2!'*,*
L
/-2!'*,*

. This gives the optimization problem:


-S!!**

& 



< , which is not

to ensure that 5

(4)
(5)

s.t.

(3)

*,*

BCB

)$

%$

8Z5



an efcient algorithm using the Newton-Raphson method. Dene

always being rank 1 (i.e., the data are always projected onto a line). 3

The choice of the constant 1 in the right hand side of (4) is arbitrary but not important, and
. Also,
, and both of the constraints
are also easily veried to be convex. Thus, the optimization problem is convex, which
enables us to derive efcient, local-minima-free algorithms to solve it.


-
**
'  would not be a good choice despite its giving a simple linear constraint. It
 , we can derive
-2

&#$
changing it to any other positive constant% results only in5 being replaced by%
this problem has an objective that is linear in the parameters 5
We also note that, while one might consider various alternatives to (4), 
&
!'*,*
would result in5
2.1 The case of diagonal5
$)(
C5
In the case that we want to learn a diagonal 5
C
*,+
-0/1
*,*
-2
*,*
C
+32

(subject to 5
It is straightforward to show that minimizing-
multiplication of 5
thus use Newton-Raphson to efciently optimize-
2.2 The case of full5
In the case of learning a full matrix 5
to enforce, and Newtons method often becomes prohibitively expensive (requiring 8
time to invert the Hessian over9
PD:EQRHBH
HBH
HBH
DFEGIHBH
C
Q
STU	VXW]ZY
N[STUVXWL]ZY
ONO=@?BA
IKJ"Y
IL
IKJ"Y
IL
imizing G>=@?BA
DFE
C
GJIKJHYSIL
OCGJI`J
YSIL
Y]\
Q'=^?_A
Qa=cb
. Decomposing ]
as ]
Jdfehg
sible since ]ai
R ), this gives=
J:NO=
J , which we recognize as a Rayleigh-
Jg
Jjg
e for the principal eigenvector, and settingg
Q#kKY
Q@lXlXl
QSR .
4To ensure that]miR , which is true iff the diagonal elements]
JJ are non-negative, we actually
eqsr bytun]o
eqsr
replace the Newton updatenpo
, wheret
line-search to give the largest downhill step subject to]ZJJuv.R .

, the constraint that 5
< becomes slightly trickier
:9<;
B parameters). Using gradient descent and the idea of
C

3The proof is reminiscent of the derivation of Fishers linear discriminant. Briey, consider max-
, where
(always pos-

8ZC5
&65

< ) is equivalent, up to a

iterative projections (e.g., [8]) we derive a different algorithm for this setting.

quotient like quantity whose solution is given by (say) solving the generalized eigenvector problem

by a positive constant, to solving the original problem (35). We can

is a step-size parameter optimized via a





*,*

*,*

.4










-


B
&
$
&


B
&



5
:
<
8
B
5




B
&

5

8
8


-

5
-

5

8
8


.








B
&
4
.








7
:
:

A
M
E
A
M
E
G
Q
A
\
O
[
J
g
[
J
[
J
Y
G
g
[
J
Y
Q
g
Y
G
g
e
Q
g
M
Q
g
b
Iterate

Iterate

**
**

*,*
**

	5

HBH

G:=

converges

&

!**
!**

*

s.t.

until5





5"


5
-25
-25


until convergence

is the Frobenius norm on




We pose the equivalent problem:

Figure 1: Gradient ascent + Iterative projection algorithm. Here,HBHhHBH
matrices (HBH
L
!'*,*
&
*,*


*P+
*P+
5

e
M ).



5)$

)$
We will use a gradient ascent step on-
5
repeatedly take a gradient step 5
the sets 
	
*,*
L
"-U'**
onto 
5"(#
*!

a single linear constraint; the solution to this is easily found by solving (in 8
a sparse system of linear equations. The second projection step onto 
B , the space of all
positive-semi denite matrices, is done by rst nding the diagonalization 5
where &
is a diagonal matrix of 5
8"
s corresponding eigenvectors, and taking 5+
$,$
*)

<
 . (E.g., see [4].)
8Z
*,+

(7)
(8)
to optimize (6), followed by the method of
iterative projections to ensure that the constraints (7) and (8) hold. Specically, we will
into
. This gives the

B can be done inexpensively. Specically, the rst projection step 5
F9
3'&
$%$
-$

algorithm shown in Figure 1.5
The motivation for the specic choice of the problem formulation (68) is that projecting

	5
 or 
5-
*,*
(
*,+
contains 5
*!

<

5 , and then repeatedly project 5
andB

involves minimizing a quadratic objective subject to
time)

,
s eigenvalues and the columns of
, where

3 Experiments and Examples



**

(6)

We begin by giving some examples of distance metrics learned on articial data, and then
show how our methods can be used to improve clustering performance.

1.036

87

, we obtain:

12354016

3.1 Examples of learned distance metrics
Consider the data shown in Figure 2(a), which is divided into two classes (shown by the
different symbols and, where available, colors). Suppose that points in each class are sim-
reecting this.6 Depending on whether we learn a

ilar to each other, and we are given 
diagonal or a full5
&.0/
1.007 :
To visualize this, we can use the fact discussed earlier that learning *,*CB*,*
to nding a rescaling of the data 
direction of the projection ofq
disrupt the constraintE
e . Empirically, this modication often signicantly speeds up convergence.
6In the experiments with synthetic data,F was a randomly sampled 1% of all pairs of similar

is equivalent
, that hopefully moves the similar pairs
5The algorithm shown in the gure includes a small renement that the gradient step is taken the
, so that it will minimally

r onto the orthogonal subspace ofq

&?A@06

87

3.245
3.286
0.081

3.286
3.327
0.082

0.081
0.082
0.002

points.

ACB

E'D

<>=

5

$




5

$
&

5



5

B

5

$
&
-

5

Y

Q
J
=
L
Y
M
J
L
O
&
-





&


5






B

5
:
<
8

5


$

&
-


$



B
&



$

5
:
<

5

$
+
&


5
B




B

$
$
(


8
8
(


$


3
&
&

$
(

(



8
8
(


9
:
:
:
:
:
:
;
6
9
;
<
&

5

E
2class data (original)

2class data projection (Newton)

2class data projection (IP)

5

0

5

z

5

0

5

z

5

0

y

5

5

5

0

x

5

0

y

5

5

5

0

x

(a)

(b)

z

5

0

5

20

20

0

y

20

20

(c)

0

x

Figure 2: (a) Original data, with the different classes indicated by the different symbols (and col-
(c) Rescaling
ors, where available).

.

.

(b) Rescaling of data corresponding to learned diagonal]

3class data projection (Newton)

3class data projection (IP)

corresponding to full]

3class data (original)

2

0

2

z

2

0

2

z

2

0

2

z

5

0

y

5

5

5

0

x

5

0

y

5

5

5

0

x

2

0

y

2

2

2

0

x

(a)

(b)

(c)

. (c) Rescaling corre-

.

Figure 3: (a) Original data. (b) Rescaling corresponding to learned diagonal]
sponding to full]
together. Figure 2(b,c) shows the result of plotting 5

successfully brought together the similar points, while keeping dissimilar ones apart.
Figure 3 shows a similar result for a case of three clusters whose centroids differ only
in the x and y directions. As we see in Figure 3(b), the learned diagonal metric correctly
, the algorithm nds a surprising

ignores the z direction. Interestingly, in the case of a full 5

projection of the data onto a line that still maintains the separation of the clusters well.

. As we see, the algorithm has

ACB

3.2 Application to clustering

One application of our methods is clustering with side information, in which we learn
a distance metric using similarity information, and cluster data using that metric. Speci-

3. K-means + metric: K-means but with distortion dened using the distance metric

 meansL
and belong
B between points 

and


always being

U

assigned to the same cluster [12].7

to the same cluster. We will consider four algorithms for clustering:

cally, suppose we are given
Z
C!	
, and told that each pair 
1. K-means using the default Euclidean metric *,*
-*,*
to dene distortion (and ignoring ).
cluster centroids 
2. Constrained K-means: K-means but subject to points 
**
*,*

"-
learned from
GJIKJ
T

M . More generally, if we imagine drawing an edge between each pair of points in

7This is implemented as the usual K-means, except ifGJI
O
KNI
points are assigned to cluster centroids
, we assign bothIKJ andI
Y		O
GJI
the points in each resulting connected componentE
we pick to beU	T
	O=

learned from

to clusterU

GJIKJY

E

4. Constrained K-means + metric: Constrained K-means using the distance metric

, then all
are constrained to lie in the same cluster, which

, then during the step in which

M .

Y		O

M

.

.





B






B
&
J
L
F
L
L
A
O
Original 2class data

Porjected 2class data

10

z

0

10

20

0

y

20

20

0

x

20

10

z

0

10

20

(a)

1. K-means: Accuracy = 0.4975
2. Constrained K-means: Accuracy = 0.5060
3. K-means + metric: Accuracy = 1
4. Constrained K-means + metric: Accuracy = 1

0

y

20

20

0

x

20

(b)

<8






tle side-information 

Figure 4: (a) Original dataset (b) Data scaled according to learned metric.


s match the%
/

	 s result is
(]
gave visually indistinguishable results.)
shown, but]


89
) be the cluster to which point

is assigned by an automatic clustering

(
Let 

be some correct or desired clustering of the data. Following [?], in
algorithm, and let%

s according to
the case of 2-cluster data, we will measure how well the 
Accuracy$
where
< ). This is equivalent to
the probability that for two points L
, ! drawn randomly from the dataset, our clustering
% agrees with the true clustering % on whether Z
and ! belong to same or different
their  -coordinate, but where the data in its original space seems to cluster much better
according to their # -coordinate. As shown by the accuracy scores given in the gure, both

clusters.8
As a simple example, consider Figure 4, which shows a clustering problem in which the
true clusters (indicated by the different symbols/colors in the plot) are distinguished by



is the indicator function (

K-means and constrained K-means failed to nd good clusterings. But by rst learning
a distance metric and then clustering according to that metric, we easily nd the correct
clustering separating the true clusters from each other. Figure 5 gives another example
showing similar results.
We also applied our methods to 9 datasets from the UC Irvine repository. Here, the true
clustering is given by the datas class labels. In each, we ran one experiment using lit-
, and one with much side-information. The results are given in



 ,

almost any clustering will correctly predict that most pairs are in different clusters. In this setting,

Figure 6.9
We see that, in almost every problem, using a learned diagonal or full metric leads to
signicantly improved performance over naive K-means. In most of the problems, using
, 6th bar for full 5
a learned metric with constrained K-means (the 5th bar for diagonal 5
)
also outperforms using constrained K-means alone (4th bar), sometimes by a very large
8In the case of many ( ) clusters, this evaluation metric tends to give inated scores since
we therefore modied the measure averaging not only IJ ,I
L drawn uniformly at random, but from
! ) with chance 0.5, and from different clusters with chance 0.5, so
the same cluster (as determined by
that matches and mis-matches are given the same weight. All results reported here used K-means
with multiple restarts, and are averages over at least 20 trials (except for wine, 10 trials).
9F was generated by picking a random subset of all pairs of points sharing the same class!
J . In
resulting connected components "$#

the case of little side-information, the size of the subset was chosen so that the resulting number of
(see footnote 7) would be very roughly 90% of the size of the

original dataset. In the case of much side-information, this was changed to 70%.

%
$


8
8
%
.




%


$
%


$



%


$

%





-




B


$
*


$

Original data

Projected data

50

z

0

50

50

0

y

50

50

0

x

50

50

z

0

50

50

(a)

1. K-means: Accuracy = 0.4993
2. Constrained K-means: Accuracy = 0.5701
3. K-means + metric: Accuracy = 1
4. Constrained K-means + metric: Accuracy = 1

0

y

50

50

0

x

50

(b)

Figure 5: (a) Original dataset (b) Data scaled according to learned metric.

shown, but]



gave visually indistinguishable results.)

	 s result is

(]

Boston housing (N=506, C=3, d=13)

ionosphere (N=351, C=2, d=34)

Iris plants (N=150, C=3, d=4)

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Kc=447

Kc=354

wine (N=168, C=3, d=12)

Kc=153

Kc=127

soy bean (N=47, C=4, d=35)

Kc=41

Kc=34

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Kc=269

Kc=187

balance (N=625, C=3, d=4)

Kc=548

Kc=400

protein (N=116, C=6, d=20)

Kc=92

Kc=61

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

Kc=133

Kc=116

breast cancer (N=569, C=2, d=30)

Kc=482

Kc=358

diabetes (N=768, C=2, d=8)

Kc=694

Kc=611

Figure 6: Clustering accuracy on 9 UCI datasets. In each panel, the six bars on the left correspond to
an experiment with little side-informationF
, and the six on the right to much side-information.
From left to right, the six bars in each set are respectively K-means, K-means 
diagonal met-
ric, K-means 
diagonal metric, and
C-Kmeans 
: number of classes/clusters;D : di-
mensionality of data; "$# : mean number of connected components (see footnotes 7, 9). 1 s.e. bars
are also shown.

full metric, Constrained K-means (C-Kmeans), C-Kmeans 
full metric. Also shown are

: size of dataset; E


Performance on Protein dataset

Performance on Wine dataset

1

0.9

0.8

0.7

e
c
n
a
m
r
o

f
r
e
p

0.6

0.5

0

kmeans
ckmeans
kmeans + metric (diag A)
ckmeans + metric (diag A)
kmeans + metric (full A)
ckmeans + metric (full A)

0.1

ratio of constraints

0.2

(a)

1

0.9

0.8

0.7

e
c
n
a
m
r
o

f
r
e
p

0.6

0.5

0

kmeans
ckmeans
kmeans + metric (diag A)
ckmeans + metric (diag A)
kmeans + metric (full A)
ckmeans + metric (full A)

0.1

ratio of constraints

0.2

(b)

Figure 7: Plots of accuracy vs. amount of side-information. Here, theI -axis gives the fraction of all
pairs of points in the same class that are randomly sampled to be included inF
margin. Not surprisingly, we also see that having more side-information in 

leads to metrics giving better clusterings.
Figure 7 also shows two typical examples of how the quality of the clusterings found in-
creases with the amount of side-information. For some problems (e.g., wine), our algo-
rithm learns good diagonal and full metrics quickly with only a very small amount of
side-information; for some others (e.g., protein), the distance metric, particularly the full
metric, appears harder to learn and provides less benet over constrained K-means.

typically

.

4 Conclusions

We have presented an algorithm that, given examples of similar pairs of points in
, learns
a distance metric that respects these relationships. Our method is based on posing metric
learning as a convex optimization problem, which allowed us to derive efcient, local-
optima free algorithms. We also showed examples of diagonal and full metrics learned
from simple articial examples, and demonstrated on articial and on UCI datasets how
our methods can be used to improve clustering performance.

