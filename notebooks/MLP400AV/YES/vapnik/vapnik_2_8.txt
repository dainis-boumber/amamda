Abstract

The method of Structural Risk  Minimization refers to tuning the capacity
of  the  classifier  to  the  available  amount  of  training  data.  This  capac(cid:173)
ity  is  influenced  by  several  factors,  including:  (1)  properties of the input
space, (2) nature and structure of the classifier, and (3) learning algorithm.
Actions based on  these  three factors  are  combined  here  to control  the  ca(cid:173)
pacity  of linear  classifiers  and  improve  generalization  on  the  problem  of
handwritten digit  recognition.

1  RISK  MINIMIZATION AND  CAPACITY

1.1  EMPIRICAL RISK  MINIMIZATION

A  common  way  of training  a  given  classifier  is  to adjust  the  parameters  w  in  the
classification  function  F( x, w)  to  minimize  the  training  error Etrain,  i.e.  the  fre(cid:173)
quency of errors on a set of p  training examples.  Etrain  estimates the expected risk
based  on  the empirical data provided  by  the p  available  examples.  The method  is
thus  called  Empirical  Risk  Minimization.  But  the  classification  function  F(x, w*)
which minimizes the empirical risk  does not necessarily minimize the generalization
error,  i.e.  the expected value of the risk over the full  distribution of possible inputs
and their corresponding outputs.  Such generalization error  Egene  cannot in  general
be computed,  but it can be estimated on a  separate test set  (Ete$t).  Other ways  of

471

472

Guyon, Vapnik,  Boser,  Bottou, and Solla

estimating  Egene  include  the  leave-one-out or  moving  control method  [Vap82]  (for
a  review,  see  [Moo92]).

1.2  CAPACITY AND  GUARANTEED  RISK

Any family of classification functions {F(x, w)} can be characterized by its capacity.
The Vapnik-Chervonenkis dimension  (or VC-dimension)  [Vap82]  is such a  capacity,
defined as the maximum number h of training examples which can be learnt without
error,  for  all  possible  binary  labelings.  The  VC-dimension  is  in  some  cases  simply
given by  the number of free  parameters of the classifier, but in most practical cases
it is  quite difficult  to determine it analytically.
The VC-theory  provides bounds.  Let  {F(x, w)}  be a  set of classification functions
of capacity  h.  With probability  (1  - 71),  for  a  number of training examples p > h,
simultaneously for  all classification functions F{x, w), the generalization error Egene
is  lower  than a  guaranteed  risk defined  by:

Eguarant  =  Etrain + ((p, h, Etrain, 71)  ,

(1)

where ((p, h, Etrain, 71)  is proportional to (0  = [h(ln2p/h+ 1) - 71l/p for small Etrain,
and  to Fa for  Etrain  close  to one  [Vap82,Vap92].
For  a  fixed  number  of training examples p,  the  training error  decreases  monoton(cid:173)
ically  as  the  capacity  h  increases,  while  both  guaranteed  risk  and  generalization
error go  through a  minimum.  Before  the minimum,  the problem is  overdetermined:
the  capacity  is  too small  for  the  amount  of training  data.  Beyond  the  minimum
the problem is  underdetermined.  The key  issue  is  therefore  to  match  the  capacity
of the  classifier  to the  amount  of training  data in  order  to get  best  generalization
performance.  The method of Structural  Risk  Minimization  (SRM)  [Vap82,Vap92]
provides a  way  of achieving this goal.

1.3  STRUCTURAL RISK  MINIMIZATION

Let  us  choose  a  family  of classifiers  {F(x, w)},  and define  a  structure consisting of
nested  subsets of elements of the family:  S1  C  S2  c  ...  C  Sr  c  ....  By  defining
such a structure, we ensure that the capacity hr of the subset of classifiers Sr  is  less
than hr+l  of subset Sr+l.  The method of SRM  amounts to finding  the subset sopt
for  which  the  classifier  F{x, w*)  which  minimizes  the  empirical  risk  within  such
subset yields  the best overall generalization performance.

Two problems arise in implementing SRM:  (I)  How  to select  sopt?  (II) How  to find
a  good  structure?  Problem  (I)  arises  because  we  have  no  direct  access  to  Egene.
In  our  experiments,  we  will  use  the  minimum of either  E te3t  or  Eguarant  to select
sopt,  and show  that these two minima are  very  close.  A good structure reflects  the
a priori knowledge of the designer,  and only few  guidelines can be provided from  the
theory to solve problem (II).  The designer  must find  the best compromise  between
two  competing  terms:  Etrain  and  i.  Reducing  h  causes  (  to  decrease,  but  Etrain
to  increase.  A  good  structure should  be  such  that  decreasing  the  VC-dimension
happens at the expense of the smallest  possible increase in  training error.  We  now
examine several ways  in  which such a structure can  be built.

Structural Risk  Minimization for  Character Recognition

473

2  PRINCIPAL COMPONENT ANALYSIS,  OPTIMAL

BRAIN DAMAGE, AND  WEIGHT DECAY

Consider  three  apparently  different  methods  of  improving  generalization  perfor(cid:173)
mance:  Principal  Component  Analysis  (a  preprocessing  transformation  of input
space)  [The89],  Optimal  Brain  Damage  (an  architectural  modification  through
weight  pruning)  [LDS90],  and  a  regularization  method,  Weight  Decay  (a modifi(cid:173)
cation of the  learning  algorithm)  [Vap82].  For  the  case  of a  linear  classifier,  these
three  approaches  are  shown  here  to  control  the  capacity  of  the  learning  system
through  the same  underlying mechanism:  a  reduction  of the  effective  dimension of
weight space, based on the curvature properties of the Mean Squared Error (M SE)
cost function  used for  training.

2.1  LINEAR CLASSIFIER AND  MSE  TRAINING

Consider a binary linear classifier F(x, w) =  (}o(wT x), where w T  is the transpose of
wand the function  {}o  takes two values 0 and  1 indicating to which  class x  belongs.
The VC-dimension  of such  classifier  is  equal  to the dimension  of input space  1  (or
the number of weights):  h = dim(w) = dim(x) =  n.
The empirical  risk  is  given  by:

Etrain  =  ! L(yk - {}o(wT xk2  ,

p

p  k=l

(2)

where  xk  is  the  kth  example,  and  yk  is  the  corresponding  desired  output.  The
problem  of minimizing  Etrain  as  a  function  of w  can  be  approached  in  different
ways  [DH73],  but it is often  replaced  by  the problem of minimizing a  Mean Square
Error (MSE)  cost function,  which  differs from  (2)  in  that the nonlinear function  (}o
has been  removed.

2.2  CURVATURE PROPERTIES  OF THE  MSE  COST  FUNCTION

The three structures  that  we  investigate  rely  on  curvature  properties of the  M S E
cost  function.  Consider  the  dependence  of  MSE  on  one  of  the  parameters  Wi.
Training  leads  to  the  optimal  value  wi  for  this  parameter.  One  way  of reducing
the  capacity  is  to  set  Wi  to  zero.  For  the  linear  classifier,  this  reduces  the  VC(cid:173)
dimension  by  one:  h'  =  dim(w) - 1 = n  - 1.  The  MSE  increase  resulting from
setting Wi  =  0 is  to lowest order  proportional to the  curvature of the  M SEat wi.
Since the decrease in capacity should be achieved at the smallest possible expense in
M S E  increase,  directions  in  weight  space  corresponding  to small  M S E  curvature
are good  candidates for  elimination.
The curvature of the M S E is specified by the Hessian matrix H  of second derivatives
of the M SE with respect to the weights.  For a linear classifier,  the Hessian matrix is
given by twice the correlation matrix of the training inputs, H  =  (2/p) 2:~=1 xkxkT.
The Hessian matrix is symmetric,  and can be diagonalized  to get rid of cross terms,

1 We  assume, for  simplicity,  that the first  component of vector x  is constant and set  to

1,  so  that the corresponding  weight introduces the bias  value.

474

Guyon, Vapnik,  Boser, Bottou, and SaBa

to  facilitate  decisions  about  the  simultaneous  elimination  of several  directions  in
weight  space.  The  elements  of the  Hessian  matrix  after  diagonalization  are  the
eigenvalues  Ai;  the  corresponding  eigenvectors  give  the  principal  directions  wi  of
the  MSE.  In  the  rotated  axis,  the  increase  IlMSE due to setting w:  = 0  takes a
simple form:

(3)

The  quadratic  approximation  becomes  an  exact  equality  for  the  linear  classifier.
Principal directions wi  corresponding to small eigenvalues  Ai  of H  are good candi(cid:173)
dates for  elimination.

2.3  PRINCIPAL  COMPONENT ANALYSIS

One common way  of reducing the capacity of a  classifier  is  to reduce the dimension
of the  input  space  and  thereby  reduce  the  number  of necessary  free  parameters
(or  weights).  Principal  Component  Analysis (PCA)  is  a  feature extraction  method
based on eigenvalue analysis.  Input vectors x of dimension  n  are approximated by a
linear combination of m  ~ n vectors forming an ortho-normal basis.  The coefficients
of this  linear  combination  form  a  vector  x' of dimension  m.  The optimal  basis in
the least square sense is given by the m eigenvectors corresponding to the m  largest
eigenvalues of the correlation matrix of the training inputs (this matrix is 1/2 of H).
A structure is obtained by ranking the classifiers according to m.  The VC-dimension
of the classifier  is  reduced to:  h' = dim(x/) = m.

2.4  OPTIMAL  BRAIN  DAMAGE

For  a  linear  classifier,  pruning can  be  implemented  in  two  different  but equivalent
ways:  (i)  change  input  coordinates  to  a  principal  axis  representation,  prune  the
components corresponding  to  small  eigenvalues  according  to  PCA,  and  then  train
with  the  M SE  cost  function;  (ii)  change  coordinates to a  principal  axis  represen(cid:173)
tation,  train  with  M S E  first,  and  then  prune  the  weights,  to get  a  weight  vector
w'  of  dimension  m  <  n.  Procedure  (i)  can  be  understood  as  a  preprocessing,
whereas procedure (ii)  involves  an  a posteriori modification  of the structure of the
classifier (network architecture).  The two procedures become identical if the weight
elimination in  (ii)  is  based on  a  'smallest eigenvalue'  criterion.

Procedure (ii)  is  very  reminiscent of Optimal Brain Damage (OBD), a  weight prun(cid:173)
ing procedure  applied  after  training.  In  OBD,  the best  candidates for  pruning are
those  weights  which  minimize the increase  IlM SE defined  in  equation (3).  The m
weights  that  are  kept  do  not  necessarily  correspond  to  the  largest  m  eigenvalues,
due  to  the  extra  factor  of (wi*)2  in  equation  (3).  In  either  implementation,  the
VC-dimension  is  reduced  to h' = dim(w /) = dim(x/) = m.

2.5  WEIGHT  DECAY

Capacity can also be controlled  through an additional term in  the cost function,  to
be minimized simultaneously with  Al S E.  Linear classifiers can be ranked according
to  the  norm  IIwll2  = L1=1 wJ  of the  weight  vector.  A  structure  is  constructed

Structural Risk Minimization for Character Recognition

475

by  allowing  within  the  subset  Sr  only  those  classifiers  which  satisfy  IIwll2  <  Cr.
The  positive  bounds  Cr  form  an  increasing  sequence:  Cl  <  C2  <  '"  <  Cr  <  ...
This sequence can be matched with a monotonically decreasing sequence of positive
Lagrange multipliers 11  ~ 12  ~ ...  ~ Ir > ... , such that our training problem stated
as  the  minimization  of M S E  within  a  specific  set  Sr  is  implemented  through  the
minimization  of a  new  cost  function:  MSE + 'rllwIl 2   This  is  equivalent  to  the
Weight  Decay  procedure  (WD).  In  a  mechanical  analogy,  the  term ,rllwll2  is  like
the energy of a spring of tension Ir  which pulls the weights to zero.  As it is easier to
pull in the directions of small curvature of the  MSE, WD pulls the weights  to zero
predominantly  along  the  principal  directions  of the  Hessian  matrix  H  associated
with small eigenvalues.

In  the  principal  axis  representation,  the  minimum  w-Y  of  the  cost  function
MSE  +  ,lIwIl2,  is  a  simple  function  of the  minimum  wO  of the  MSE  in  the
I  -+  0+  limit:  wI  =  w? Ad(Ai + I)'  The  weight  w?  is  attenuated  by  a  factor
Ad (Ai  + I)'  Weights  become  negligible  for  I  ~ Ai,  and  remain  unchanged  for
I  : Ai  The effect  of this attenuation can  be compared  to that of weight  pruning.
Pruning all  weights  such that  Ai  < I  reduces  the  capacity  to:

n

h' = L: 8-y(Ai)  ,

i=1

where 8-y(u)  =  1 if U  > I  and  8-y(u)  =  0 otherwise.
By  analogy,  we  introduce the Weight  Decay  capacity:

h' = t  Ai

i=1  Ai + I

.

(4)

(5)

This  expression  arises  in  various  theoretical  frameworks
valid  only for  broad spectra of eigenvalues.

[Moo92,McK92]'  and  is

3  SMOOTHING, HIGHER-ORDER UNITS, AND

REGULARIZATION

Combining several  different structures achieves further performance improvements.
The combination of exponential smoothing (a preprocessing transformation of input
space) and regularization (a modification of the learning algorithm) is shown here to
improve  character recognition .  The generalization ability  is  dramatically improved
by  the further  introduction of second-order  units  (an architectural modification).

3.1  SMOOTHING

Smoothing  is  a  preprocessing  which  aims  at  reducing  the  effective  dimension  of
input space by  degrading the resolution:  after smoothing, decimation of the inputs
could be performed without further image degradation.  Smoothing is achieved here
through  convolution  with  an exponential kernel:

Lk Ll PIXEL(i + k,j + I)  exp[-~Jk2 + 12]

BLURRED.PIXEL(i,j) =

Lk Ll exp[-fj  k2 + 12]

IJ

'

476

Guyon, Vapnik,  Boser,  Bottou, and Soil a

where {3  is  the smoothing parameter which determines the structure.
Convolution  with  the  chosen  kernel  is  an invertible linear operation.  Such  prepro(cid:173)
cessing results in no capacity change for  a  MSE-trained linear classifier.  Smoothing
only modifies the spectrum of eigenvalues and must be combined with an eigenvalue(cid:173)
based regularization procedure such as OBD or WD, to obtain performance improve(cid:173)
ment through capacity decrease.

3.2  HIGHER-ORDER UNITS

Higher-order (or sigma-pi)  units can  be substituted for  the linear units to get poly(cid:173)
nomial  classifiers:  F(x, w)  =  6o(wTe(x)),  where e(x)  is  an  m-dimensional  vector
(m > n)  with  components:  x}, X2, ... , Xn ,  (XIXt), (XIX2), . , (xnx n ),  ,  (X1X2 . Xn) .
The structure is geared towards increasing the capacity, and is controlled by the or(cid:173)
der of the polynomial:  Sl contains all the linear terms, S2  linear plus quadratic, etc.
Computations are kept  tractable with  the method  proposed  in  reference  [Pog75].

4  EXPERIMENTAL RESULTS

Experiments were performed on the benchmark problem of handwritten digit recog(cid:173)
nition  described  in  reference  [GPP+S9].  The  database  consists  of 1200  (16  x  16)
binary pixel  images,  divided  into 600  training examples and 600  test examples.

In  figure  1,  we  compare  the  results  obtained  by  pruning  inputs  or  weights  with
PCA  and  the  results  obtained  with  WD.  The overall  appearance  of the  curves  is
very similar.  In  both cases,  the capacity (computed from  (4)  and  (5))  decreases as
a  function  of r, whereas  the  training error  increases.  For  the  optimum  value  r*,
the  capacity  is  only  1/3 of the  nominal  capacity,  computed solely  on  the  basis  of
the network  architecture.  At  the price  of some error on  the  training set,  the error
rate on  the test set is only half the error rate obtained with r = 0+ .
The  competition  between  capacity  and  training  error  always  results  in  a  unique
minimum  of the  guaranteed  risk  (1).  It is  remarkable  that  our  experiments show
the minimum of Eguarant  coinciding with the minimum of E te1t   Any of these  two
quantities can therefore be used to determine r*.  In principle, another independent
test  set  should  be  used  to  get  a  reliable  estimate  of Egene  (cross-validation).  It
seems therefore advantageous  to determine r* using the minimum of Eguarant  and
use  the test set to predict the generalization  performance.
Using Eguarant to determine r* raises the problem of determining the capacity of the
system.  The capacity can  be measured  when  analytic computation is  not  possible.
Measurements performed with the method proposed by Vapnik, Levin,  and Le  Cun
yield  results  in  good  agreement  with  those obtained  using  (5).  The method yields
an  effective  VCdimension  which  accounts  for  the  global  capacity  of the  system,
including the effects of input data,  architecture,  and learning algorithm  2.

2 Schematically,  measurements  of  the  effective  VC.dimension  consist  of splitting  the

training  data into  two  subsets.  The  difference  between  Etrain  in  these  subsets  is  maxi(cid:173)
mized.  The value of h is extracted from  the fit  to a theoretical prediction for such maximal
discrepancy.

Structural Risk Minimization for Character Recognition

477

%error

1

12
11
10
9
8

a  ,

1
0
-5

Etest

Etrain

--------

I
I
I
I
I

-4

-3

-2

-1

*0

1

-4

-3

-2

-1

*0

1

10CJ-;.-&

"1

10;-; __

"1

%error

h'

b

1
12
11
10
9
8
7
6
5
4
3
2
1
0
-5

I
Etest

Etrain

-4

-3

-2

10;-;'-&

0

"1*

260
250
240
230
220
210
200
190
180
170
160
150
140
130
120
110
100
90
80
60
50
40
30
20
10
0
-5

70  -------------

-4

-3

-2

-1

0

1

loq-qamma

"1*

Figure  1:  Percent  error and  capacity  h'  as  a  function  of log r  (linear  classifier,  no
smoothing):  (a) weight/input pruning via peA (r is  a threshold), (b) WD (r is  the
decay  parameter).  The guaranteed  risk  has  been  rescaled  to fit  in  the figure.

478

Guyon, Vapnik,  Boser,  Bottou, and Solla

Table  1:  Eteat  for  Smoothing,  WD,  and  Higher-Order  Combined.

I  (3
0
1
2
10
any

II  "I
"1*
"1*
"1*
'Y'~
0+

II  13t  order  I 2nd  order  I

6.3
5.0
4.5
4.3
12.7

1.5
0.8
1.2
1.3
3.3

In table 1 we  report results obtained when several structures are combined.  Weight
decay  with  'Y  = "1*  reduces  E te3t  by  a  factor  of 2.  Input  space smoothing used  in
conjunction  with  WD  results  in  an  additional  reduction  by  a  factor  of  1.5.  The
best  performance  is  achieved for  the highest  level  of smoothing,  (3  =  10,  for  which
the  blurring  is  considerable.  As  expected,  smoothing  has  no  effect  in  the  absence
ofWD.
The use of second-order  units provides an  additional factor  of 5 reduction in  Ete3t 
For second order units,  the number of weights scales like  the square of the number
of inputs n 2 = 66049.  But the capacity (5)  is  found  to be only 196, for  the optimum
values  of "I  and (3.

5  CONCLUSIONS  AND  EPILOGUE

Our  results  indicate  that  the  VC-dimension  must  measure  the  global  capacity  of
the system.  It is crucial to incorporate the effects of preprocessing of the input data
and modifications  of the learning algorithm.  Capacities  defined  solely on  the basis
of the network  architecture give overly pessimistic  upper bounds.

The  method  of SRM  provides  a  powerful  tool  for  tuning  the  capacity.  We  have
shown  that structures acting  at  different  levels  (preprocessing,  architecture,  learn(cid:173)
ing mechanism)  can produce similar effects.  We  have then combined three different
structures  to  improve  generalization.  These  structures  have  interesting  comple(cid:173)
mentary  properties.  The introduction  of higher-order  units increases  the capacity.
Smoothing  and weight  decay  act in  conjunction  to decrease  it.

Elaborate neural networks for  character recognition  [LBD+90,GAL +91]  also incor(cid:173)
porate similar complementary structures.  In  multilayer sigmoid-unit networks,  the
capacity  is  increased  through  additional  hidden  units.  Feature extracting  neurons
introduce smoothing, and regularization follows  from prematurely stopping training
before  reaching  the  M S E  minimum.  When  initial  weights  are chosen  to be small,
this stopping technique produces effects similar  to those of weight  decay.

Structural  Risk Minimization for  Character Recognition

479

Acknowledgments

We  wish  to  thank  L.  Jackel's  group  at  Bell  Labs  for  useful  discussions,  and  are
particularly  grateful  to  E.  Levin  and  Y.  Le  Cun for  communicating  to us  the  un(cid:173)
published method of computing the effective  VC-dimension.

