Abstract.  The support-vector  network is a new learning  machine for two-group  classification  problems.  The
machine  conceptually  implements  the  following  idea:  input  vectors  are  non-linearly  mapped  to  a  very  high-
dimension  feature space.  In this feature space a linear decision surface  is constructed.  Special properties  of the
decision  surface ensures high generalization  ability  of the learning  machine.  The idea behind the  support-vector
network  was previously  implemented  for the restricted  case where  the training  data can be separated  without
errors.  We here extend this result to non-separable  training data.

High generalization  ability  of support-vector  networks  utilizing  polynomial  input transformations  is  demon-
strated.  We also compare the performance  of the support-vector  network to various classical learning  algorithms
that all took part in a benchmark study of Optical  Character  Recognition.

Keywords:  pattern recognition,  efficient  learning  algorithms, neural networks, radial basis  function  classifiers,
polynomial  classifiers.

1.  Introduction

More than 60 years ago R.A. Fisher (Fisher,  1936) suggested the first algorithm for pattern
recognition.  He considered  a model  of two normal distributed populations, N(m1,  EI)
and N(m2,  E2)  of n dimensional vectors x with mean vectors m1 and m2  and co-variance
matrices E1 and E2, and showed that the optimal (Bayesian) solution is a quadratic decision
function:

In the case where  E1  =  E2  =  E the quadratic decision function (1) degenerates to a linear
function:

To estimate the quadratic decision function one has to determine  "("+3) free parameters.  To
estimate the linear function only n free parameters have to be determined.  In the case where
the number of observations  is small (say less than 10n2)  estimating o(n2) parameters is not
reliable.  Fisher  therefore  recommended,  even in the case of EI  ^  2,  to use the linear
discriminator function  (2) with   of the form:

where  T is  some constant1.  Fisher also recommended  a linear decision  function  for  the
case  where  the  two  distributions  are  not  normal.  Algorithms  for  pattern  recognition

274

CORTES  AND VAPNIK

Figure  1.  A  simple feed-forward perceptron  with 8 input units, 2 layers of hidden units, and 1 output unit.  The
gray-shading of the vector entries  reflects their numeric value.

were  therefore  from  the  very  beginning associated  with  the  construction  of  linear  deci-
sion  surfaces.

In  1962  Rosenblatt  (Rosenblatt,  1962) explored  a different  kind of learning  machines:
perceptrons  or neural networks. The perceptron  consists of connected neurons, where each
neuron  implements  a separating  hyperplane,  so the perceptron  as a whole  implements  a
piecewise linear separating  surface.  See Fig. 1.

No algorithm that minimizes the error on a set of vectors by adjusting  all the weights of
the network was found in Rosenblatt's  time, and Rosenblatt suggested a scheme where only
the weights  of the output unit were  adaptive.  According  to the fixed setting  of the  other
weights the input vectors are non-linearly transformed into the feature space, Z,  of the last
layer of units.  In this space  a linear decision function  is  constructed:

by adjusting the weights ai  from the ith hidden unit to the output unit so as to minimize some
error  measure  over the training data.  As a result of Rosenblatt's  approach, construction of
decision  rules  was  again  associated  with the  construction  of  linear  hyperplanes  in some
space.

An algorithm that allows for all weights of the neural network to adapt in order locally to
minimize the error on a set of vectors belonging to a pattern recognition problem was found
in 1986 (Rumelhart, Hinton & Williams, 1986,1987; Parker,  1985; LeCun, 1985) when the
back-propagation  algorithm  was discovered.  The solution  involves a slight  modification
of the mathematical  model of neurons. Therefore,  neural networks implement "piece-wise
linear-type" decision  functions.

In this article we construct a new type of learning machine, the so-called  support-vector
network.  The  support-vector  network  implements  the  following idea:  it maps  the input
vectors  into  some  high  dimensional  feature  space  Z  through  some  non-linear  mapping
chosen  a priori.  In this space a linear decision surface is constructed with special  properties
that ensure high generalization  ability of the network.

SUPPORT-VECTOR  NETWORKS

275

Figure 2.  An example of a separable problem in a 2 dimensional  space. The support vectors, marked with grey
squares, define the margin of largest separation between the two classes.

EXAMPLE.  To obtain a decision surface corresponding to a polynomial of degree two, one
can create a feature space,  Z, which has N  =  2&21 coordinates  of the form:

where x =  (x\,..., xn).  The hyperplane is then constructed in this space.

Two problems arise in the above approach:  one conceptual and one technical. The con-
ceptual problem is how to find a separating hyperplane that will generalize well: the dimen-
sionality of the feature space will be huge, and not all hyperplanes that separate the training
data will necessarily  generalize  well2.  The technical problem is how computationally to
treat  such  high-dimensional  spaces:  to construct  polynomial of degree 4  or 5 in a 200
dimensional  space  it may  be necessary  to construct hyperplanes in a billion dimensional
feature space.

The conceptual  part of this problem was solved  in  1965  (Vapnik, 1982) for the case of
optimal  hyperplanes  for separable classes.  An optimal hyperplane is here defined  as  the
linear decision  function  with maximal margin between the vectors  of the two classes,  see
Fig. 2.  It was observed  that to construct such optimal hyperplanes one only has to take into
account a small amount of the training data, the so called support  vectors, which determine
this margin.  It  was shown that if the training vectors  are separated  without errors  by an
optimal hyperplane the expectation value of the probability of committing an error on a test
example is bounded by the ratio between the expectation  value of the number of support
vectors  and the number of training vectors:

276

CORTES  AND VAPNIK

Note that this bound does not explicitly contain the dimensionality of the space of separation.
It follows from  this bound, that if the optimal hyperplane can be constructed  from  a small
number of support vectors  relative  to the training set size the generalization  ability will be
higheven in an infinite dimensional space.  In Section 5 we will demonstrate that the ratio
(5) for a real  life problems  can be as low as 0.03  and the optimal hyperplane  generalizes
well in a billion dimensional  feature  space.

Let

be  the  optimal  hyperplane  in  feature  space.  We  will show,  that  the  weights W0 for  the
optimal  hyperplane  in  the  feature  space  can  be  written  as  some  linear  combination of
support  vectors

The linear decision function  / (z) in the feature space will accordingly  be of the form:

where zi-  z is the dot-product between  support vectors zi and vector z in feature space.  The
decision  function  can therefore be described  as a two layer network (Fig.  3).

However,  even if the optimal  hyperplane  generalizes  well the technical problem  of how
to treat the high dimensional  feature space remains.  In  1992 it was shown (Boser, Guyon,
&  Vapnik,  1992),  that  the  order  of  operations  for  constructing  a  decision  function  can
be interchanged:  instead  of making a non-linear  transformation of the  input vectors  fol-
lowed  by  dot-products  with  support vectors  in feature  space,  one can first compare  two
vectors  in  input space (by e.g.
taking their dot-product  or some  distance  measure),  and
then  make  a  non-linear  transformation  of  the  value  of  the  result  (see  Fig.  4).  This  en-
ables the construction of rich classes  of decision surfaces, for example polynomial decision
surfaces  of arbitrary  degree.  We will  call  this type of learning machine a  support-vector
network3.

The technique of support-vector  networks was first developed  for the restricted  case of
separating  training data without errors.  In this article  we extend  the approach  of support-
vector networks to cover when separation without error on the training vectors is impossible.
With  this  extension  we consider  the  support-vector  networks as a new class  of learning
machine, as powerful and universal as neural networks.  In Section 5 we will  demonstrate
how well it generalizes  for high degree  polynomial decision  surfaces (up to order 7) in a
high dimensional space (dimension 256).  The performance  of the algorithm is  compared
to that of classical  learning machines e.g.  linear classifiers, ^-nearest neighbors classifiers,
and neural networks.  Sections  2, 3, and 4 are devoted to the major  points of the derivation
of the  algorithm  and a discussion of some of its properties.  Details  of the derivation  are
relegated  to an appendix.

SUPPORT-VECTOR  NETWORKS

277

Figure 3.  Classification by a support-vector  network of an unknown pattern is conceptually done by first trans-
forming the pattern  into some high-dimensional feature space.  An optimal hyperplane constructed in this feature
space determines  the output. The similarity to a two-layer  perceptron can be seen by comparison to Fig. 1.

2.  Optimal  Hyperplanes

In this section  we review the method of optimal hyperplanes (Vapnik, 1982) for separation
of training data without errors.  In the next section  we introduce a notion of soft margins,
that will allow for an analytic treatment of learning with errors on the training set.

2.1.  The Optimal Hyperplane  Algorithm

The set of labeled  training patterns

is said to be linearly separable if there exists a vector w and a scalar b such that the inequalities

278

CORTES AND VAPNIK

Figure  4.  Classification  of  an  unknown  pattern  by  a  support-vector  network.  The  pattern  is in  input space
compared  to  support  vectors.  The  resulting  values  are  non-linearly  transformed.  A  linear  function  of  these
transformed  values determine the output of the classifier.

are valid for all elements  of the training set (8).  Below  we write the inequalities  (9) in the
form4:

The optimal hyperplane

is the unique one  which separates the training data  with a maximal margin:  it  determines
the direction  w/|w|  where the distance between  the projections of the training vectors of
two different classes is maximal, recall Fig. 2.  This distance p ( w , b)  is given by

The optimal hyperplane  (W0, b0) is the arguments that maximize the distance (12). It follows
from  (12)  and (10)  that

SUPPORT-VECTOR  NETWORKS

279

This  means that the optimal  hyperplane  is the unique one that minimizes w  w under the
constraints (10).  Constructing an optimal hyperplane is therefore a quadratic  programming
problem.

Vectors xi for which yi (w  x, +  6) =  1 will be termed  support  vectors. In Appendix A. 1
we show that the vector WQ that determines the optimal hyperplane can be written as a linear
combination  of training vectors:

where a  >  0.  Since a  > 0 only for support vectors  (see  Appendix),  the expression  (14)
represents a compact form of writing w0.  We also show that to find the vector of parameters
ai:

one has to solve the following quadratic programming  problem:

with respect to Ar  =  (a 1,..., at),  subject to the constraints:

where 1T  =  (1,...,  1) is an  -dimensional unit vector, Yr  =  (yi
sional vector of labels, and D is a symmetric I  x  ^-matrix with elements

y^) is the -dimen-

The inequality (16) describes the nonnegative quadrant. We therefore have to maximize the
quadratic form (15)  in the nonnegative quadrant, subject to the constraints (17).

When the training data (8) can be separated  without errors we also show in Appendix A
the following relationship between the maximum of the functional  (15), the pair (Ao,  b0),
and the maximal margin po from (13):

If for some A* and large constant  W0 the inequality

is valid, one can accordingly assert that all hyperplanes that separate  the training data (8)
have a margin

280

CORTES  AND VAPNIK

If the  training  set  (8) cannot  be separated  by a hyperplane, the  margin between  patterns
of the two classes becomes  arbitrary  small, resulting in the value of the functional  W(A)
turning  arbitrary  large.  Maximizing  the  functional  (15)  under  constraints  (16)  and  (17)
one therefore  either  reaches a maximum  (in this case one has constructed  the  hyperplane
with the maximal  margin po), or one finds that the maximum exceeds some given  (large)
constant  Wo (in  which  case  a  separation  of  the  training  data  with  a  margin  larger  then
V^/Wo  is  impossible).

The problem  of maximizing functional (15) under constraints (16) and (17) can be solved
very  efficiently  using  the  following  scheme. Divide the  training  data  into  a  number  of
portions  with a reasonable small number  of training vectors  in each portion.  Start out by
solving the quadratic programming problem determined by the first portion of training data.
For this problem  there are two possible outcomes:  either this portion  of the data cannot  be
separated  by a hyperplane (in which case the full  set of data as well cannot be  separated),
or the optimal hyperplane  for separating  the first portion of the training data is found.

Let the vector that maximizes functional (15) in the case of separation of the first portion
be A1.  Among  the coordinates  of vector A1 some  are equal  to zero.  They  correspond  to
non-support  training vectors  of this portion.  Make  a new set of training data containing
the  support  vectors  from  the  first  portion of  training data  and  the  vectors  of  the  second
portion  that do  not  satisfy  constraint  (10),  where  w is determined  by A1.  For this  set a
new  functional  W2(A) is constructed  and  maximized  at  A2.  Continuing this process of
incrementally  constructing  a solution  vector  A* covering all the portions  of the  training
data one either finds  that it is impossible  to separate the training set without error, or one
constructs  the optimal  separating  hyperplane for the full  data  set,  A,  =  A0-.  Note,  that
during this  process  the  value of the  functional  W(A)  is monotonically increasing,  since
more and more training vectors are considered  in the optimization, leading to a smaller and
smaller separation  between the two  classes.

3.  The Soft Margin  Hyperplane

Consider the case where the training data cannot be separated  without error.  In this case
one may want to separate the training set with a minimal number of errors. To express this
formally let us introduce some non-negative variables  ,- >  0,  i =  I,...,(,.

We can now minimize the  functional

for  small a  > 0, subject to the constraints

For sufficiently  small a  the functional  (21) describes  the number of the training errors5.

Minimizing (21) one finds some minimal subset of training errors:

SUPPORT-VECTOR  NETWORKS

281

If these  data are excluded  from  the training set one can separate  the remaining part of the
training  set  without errors.  To separate  the  remaining  part  of the  training data  one can
construct  an optimal  separating hyperplane.

This idea can be expressed formally as: minimize the  functional

subject to constraints (22) and (23), where F(u)  is a monotonic convex function  and C is
a constant.

For  sufficiently  large  C  and  sufficiently  small a,  the  vector  wo and constant  b0,  that
minimize  the  functional  (24)  under constraints  (22)  and  (23),  determine  the hyperplane
that minimizes the number of errors on the training set and separate the rest of the elements
with maximal margin.

Note,  however,  that  the  problem  of  constructing a  hyperplane  which minimizes the
number of errors on the training set is in general NP-complete.  To avoid  NP-completeness
of  our  problem  we  will  consider  the case of a   1 (the  smallest  value  of a  for which
the  optimization  problem  (15)  has  a  unique solution).  In  this  case  the  functional  (24)
describes  (for  sufficiently  large  C)  the problem  of constructing a  separating  hyperplane
which  minimizes  the sum  of  deviations, ,  of  training errors  and maximizes  the margin
for  the correctly  classified vectors.  If the training data can be separated  without errors  the
constructed hyperplane coincides with the optimal margin hyperplane.

In contrast to the case with a  <  I there exists an efficient method for finding the solution

of (24) in the case of a  =  1. Let us call this solution the soft  margin  hyperplane.

In Appendix A we consider the problem of minimizing the  functional

subject to the constraints (22)  and (23), where F(u)  is a monotonic convex function  with
F(0)  =  0. To simplify the formulas we only describe the case of F(u)  = u2 in this section.
For this function the optimization problem remains a quadratic programming problem.

In Appendix A we show that the vector w, as for the optimal hyperplane algorithm, can

be written as a linear combination of support vectors  x,:

To find the vector  AT  =  (i,..., at)  one has to solve the dual quadratic programming
problem of maximizing

subject to constraints

282

CORTES  AND VAPNIK

where  1, A, Y,  and  D  are  the  same  elements  as  used  in  the  optimization  problem  for
constructing  an  optimal  hyperplane,  S is  a scalar,  and (29) describes  coordinate-wise  in-
equalities.

Note that (29) implies that the smallest admissible value S in functional  (26) is

Therefore  to find a soft margin classifier one has to find a vector A that maximizes

under the constraints A  >  0 and (27).  This problem differs from the problem of constructing
an optimal  margin  classifier only  by the additional term with amax  in the functional (30).
Due  to this  term  the solution  to the problem  of constructing  the soft  margin  classifier is
unique and exists  for any data set.

The  functional (30) is not quadratic  because  of the term  with amax.  Maximizing (30)
subject  to the constraints  A  >  0 and (27) belongs  to the group  of so-called  convex pro-
gramming  problems.  Therefore,  to construct a soft  margin classifier  one can either solve
the convex programming problem  in the ^-dimensional space  of the parameters  A, or one
can solve the quadratic programming problem in the dual t  + 1 space of the parameters  A
and  S.  In  our experiments  we construct  the  soft  margin hyperplanes by  solving the dual
quadratic programming  problem.

4.  The Method  of Convolution  of the Dot-Product in Feature  Space

The algorithms described  in the previous sections construct hyperplanes in the input space.
To construct  a hyperplane  in a feature  space  one first has to transform the  n-dimensional
input vector x into an //-dimensional  feature vector through a choice of an  N-dimensional
vector  function  0:

An  N  dimensional  linear  separator  w  and  a  bias  b  is  then  constructed  for  the  set  of

transformed  vectors

Classification  of an unknown vector x is done by first transforming the vector to the  sepa-
rating space (x  i--  0 (x)) and then taking the sign of the  function

According  to  the  properties  of  the  soft  margin  classifier  method  the  vector  w  can  be

written as a linear combination of support vectors  (in the feature space).  That means

SUPPORT-VECTOR  NETWORKS

283

The linearity of the dot-product implies, that the classification  function  /  in (31) for an

unknown vector x only depends on the dot-products:

The idea  of constructing support-vector networks comes from  considering general  forms
of the dot-product in a Hilbert space (Anderson & Bahadur, 1966):

According  to  the  Hilbert-Schmidt  Theory  (Courant  &  Hilbert,  1953)  any symmetric

function  K(u, v), with K ( u , v) e  LI,  can be expanded in the form

where A, e  SK and  fa  are eigenvalues and eigenfunctions

of the integral operator  defined by the kernel K(u,  v).  A sufficient  condition to ensure that
(34) defines a dot-product in a feature space is that all the eigenvalues in the expansion (35)
are positive.  To guarantee that these coefficients  are positive, it is necessary  and  sufficient
(Mercer's Theorem) that the condition

is satisfied for all g such that

Functions that satisfy Mercer's theorem can therefore be used as dot-products.  Aizerman,
Braverman and Rozonoer (1964) consider  a convolution of the dot-product  in the feature
space given by function of the  form

which they call Potential Functions.

However, the convolution of the dot-product in feature space can be given by any function
satisfying  Mercer's condition; in particular, to construct a polynomial classifier of  degree
d  in n-dimensional  input space one can use the following  function

284

CORTES AND VAPNIK

Using different dot-products  K(u,  v) one can construct different  learning machines with
arbitrary types of decision  surfaces (Boser, Guyon & Vapnik, 1992).  The decision  surface
of these machines  has a form

where  xi  is the image of a support vector in input space and  ai  is the weight of a  support
vector in the feature space.

To find the vectors  xi  and weights  ai  one follows the same  solution scheme  as for the
original  optimal  margin  classifier  or  soft  margin  classifier.  The  only  difference  is  that
instead of matrix D (determined  by (18))  one uses the matrix

5.  General Features of Support-Vector  Networks

5.1.  Constructing  the Decision Rules by Support-Vector  Networks  is  Efficient

To construct a support-vector network decision rule one has to solve a quadratic optimization
problem:

under the simple  constraints:

where matrix

is determined  by the elements  of the training set, and K(u,  v) is the function determining
the convolution of the  dot-products.

The solution to the optimization problem can be found efficiently  by solving intermediate
optimization problems determined  by the training data, that currently constitute the support
vectors.  This technique is described  in Section  3.  The obtained  optimal  decision  function
is unique6.

Each optimization problem can be solved using any standard  techniques.

5.2.  The Support-Vector  Network  is a  Universal Machine

By changing the function  K (u, v) for the convolution of the dot-product one can implement
different  networks.

SUPPORT-VECTOR  NETWORKS

285

In the next section we will consider support-vector network machines that use polynomial
decision  surfaces.  To specify  polynomials of different  order  d  one can use  the following
functions  for convolution of the  dot-product

Radial Basis Function machines  with decision  functions of the form

can be implemented  by using convolutions of the type

In  this case the  support-vector  network machine  will  construct both  the centers  xi  of the
approximating  function and the weights  ai.

One  can  also  incorporate  a priori  knowledge  of  the problem  at  hand  by  constructing
special convolution functions. Support-vector  networks are therefore a rather general  class
of learning machines which changes  its set of decision  functions  simply by changing the
form of the  dot-product.

5.3.  Support- Vector Networks and Control of Generalization Ability

To control the generalization  ability of a learning machine one has to control two  different
factors:  the  error-rate  on  the  training  data  and  the  capacity  of  the  learning  machine  as
measured  by its VC-dimension (Vapnik, 1982).  There exists a bound for the probability of
errors  on the test set of the following form:  with probability  1  r; the inequality

is valid.  In the bound  (38)  the confidence  interval  depends  on the VC-dimension  of  the
learning machine, the number of elements  in the training set, and the value of n.

The  two  factors  in (38)  form  a trade-off:  the  smaller  the VC-dimension  of  the set of
functions  of the  learning  machine,  the smaller  the confidence interval,  but the  larger  the
value of the error frequency.

A general  way for resolving this trade-off was proposed  as the principle of structural risk
minimization:  for the given data  set one has to find a solution that minimizes their  sum.
A particular  case of structural risk minimization principle is the Occam-Razor  principle:
keep  the first term equal to zero and minimize the second  one.

It is known that the VC-dimension of the set of linear indicator  functions

with  fixed  threshold  b  is  equal  to  the  dimensionality of  the  input  space.  However,  the
VC-dimension  of the subset

286

CORTES  AND VAPNIK

(the set of functions with bounded norm of the weights) can be less than the dimensionality
of the input space and will depend  on  Cw.

From  this point of view the optimal margin classifier method executes  an  Occam-Razor
principle.  It keeps  the  first  term  of  (38)  equal to  zero  (by  satisfying  the  inequality  (9))
and it minimizes  the second term  (by minimizing the functional  w  w).  This minimization
prevents  an over-fitting problem.

However,  even  in the case where the training data  are separable one may obtain better
generalization  by  minimizing the  confidence term  in  (38)  even  further  at the  expense  of
errors on the training set.  In the soft margin classifier method this can be done by choosing
appropriate values  of the parameter C.  In the  support-vector  network algorithm  one can
control the trade-off between complexity of decision rule and frequency of error by changing
the parameter  C,  even  in the more  general  case where  there exists no solution  with zero
error on the training set.  Therefore  the support-vector network can control both factors for
generalization  ability of the learning machine.

6.  Experimental Analysis

To demonstrate  the support-vector  network method  we conduct two types of experiments.
We construct  artificial sets  of patterns in the plane and experiment  with 2nd degree  poly-
nomial decision  surfaces,  and we conduct experiments  with the real-life problem  of digit
recognition.

6.1.  Experiments  in the Plane

Using dot-products  of the form

with d  =  2 we construct decision  rules for different  sets of patterns in the plane.  Results
of these  experiments  can be  visualized and provide nice illustrations of the power of  the
algorithm.  Examples are shown in Fig.  5. The 2 classes are represented  by black and white

Figure  5.  Examples  of the dot-product  (39)  with d  =  2.  Support  patterns  are indicated with  double  circles,
errors with a cross.

SUPPORT-VECTOR NETWORKS

287

Figure 6.  Examples of patterns with labels from the US Postal Service digit database.

bullets.  In the figure we indicate support patterns with a double circle,  and errors with a
cross.  The solutions are optimal in the sense that no 2nd degree polynomials exist that make
less errors.  Notice  that the numbers of support patterns relative to the number of training
patterns are small.

6.2.  Experiments  with Digit Recognition

Our experiments for constructing support-vector networks make use of two different  data-
bases for bit-mapped digit recognition, a small and a large database.  The small one is a US
Postal Service  database that contains 7,300 training patterns and 2,000 test patterns.  The
resolution of the database is 16 x  16 pixels, and some typical examples are shown in Fig. 6.
On this database we report experimental research  with polynomials of various degree.

The large database consists  of 60,000 training and 10,000 test patterns, and is a 50-50
mixture of  the NIST7  training and test sets.  The resolution of these patterns is 28  x  28
yielding an input dimensionality of 784.  On this database  we have only constructed a 4th
degree polynomial classifier. The performance of this classifier is compared to other types
of learning machines that took part in a benchmark study (Bottou, 1994).

In all our experiments ten separators, one for each class,  are constructed.  Each hyper-
surface makes use of the same dot product and pre-processing  of the data.  Classification of
an unknown patterns is done according to the maximum output of these ten classifiers.

6.2.1.  Experiments  with US Postal Service Database.  The US Postal Service  Database
has been recorded from actual mail pieces and results from this database have been reported
by several  researchers.  In Table 1 we list the performance of various classifiers  collected

