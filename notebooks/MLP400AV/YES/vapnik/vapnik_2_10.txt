Abstract

We explore methods for incorporating prior knowledge about a problem
at hand in Support Vector learning machines. We show that both invari-
ances under group transformations and prior knowledge about locality in
images can be incorporated by constructing appropriate kernel functions.

1 INTRODUCTION

When we are trying to extract regularities from data, we often have additional knowledge
about functions that we estimate. For instance, in image classication tasks, there exist
transformations which leave class membership invariant (e.g. local translations); moreover,
it is usually the case that images have a local structure in that not all correlations between
image regions carry equal amounts of information.

The present study investigates the question how to make use of these two sources of know-
ledge by designing appropriate Support Vector (SV) kernel functions. To this end, we start
by giving a brief introduction to SV machines, following Vapnik & Chervonenkis (1979)
and Vapnik (1995) (Sec. 2). Regarding prior knowledge about invariances, we develop an
idea of Vapnik (1995) and present a method to design kernel functions which lead to invari-
ant classication hyperplanes (Sec. 3). The method is applicable to invariances under the
action of differentiable local 1parameter groups of local transformations, e.g. translational
invariance in pattern recognition. In Sec. 4, we describe kernels which take into account
image locality by using localized receptive elds. Sec. 5 presents experimental results on
both types of kernels, followed by a discussion (Sec. 6).

2 OPTIMAL MARGIN HYPERPLANES

For linear hyperplane decision functions f (cid:0)x(cid:1) (cid:2) sgn (cid:0)(cid:0)w (cid:0) x(cid:1) (cid:3) b(cid:1), the VCdimension
can be controlled by controlling the norm of the weight vector w. Given training data
(cid:0)x(cid:0) y(cid:1)(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) (cid:0)x(cid:0)(cid:0) y(cid:0)(cid:1)(cid:0) xi  RN (cid:0) yi  f(cid:2)g(cid:0) a separating hyperplane which generalizes

well can be found by minimizing




kwk subject to yi (cid:0) (cid:0)(cid:0)xi (cid:0) w(cid:1) (cid:3) b(cid:1) (cid:3)  for i (cid:2) (cid:0) (cid:1) (cid:1) (cid:1) (cid:0) (cid:2)(cid:0)

(1)

the latter being the conditions for separating the training data with a margin. Nonseparable
cases are dealt with by introducing slack variables (Cortes & Vapnik 1995), but we shall
omit this modication to simplify the exposition. All of the following also applies for the
nonseparable case.

To solve the above convex optimization problem, one introduces a Lagrangian with multi-
pliers (cid:3)i and derives the dual form of the optimization problem: maximize

(cid:0)

(cid:0)

(cid:3)iyi(cid:3)kyk(cid:0)xi (cid:0) xk(cid:1) subject to (cid:3)i (cid:3) (cid:0)

(cid:3)iyi (cid:2) (cid:1)

(2)

(cid:3)i (cid:4)




(cid:0)

Xi(cid:2)

Xi(cid:1)k(cid:2)

Xi(cid:2)

It turns out that the solution vector has an expansion in terms of training examples, w (cid:2)
i(cid:2) (cid:3)iyixi, where only those (cid:3)i corresponding to constraints (1) which are met can
become nonzero; the respective examples xi are called Support Vectors. Substituting this
expansion for w yields the decision function

P(cid:0)

f (cid:0)x(cid:1) (cid:2) sgn(cid:0) (cid:0)
Xi(cid:2)

(cid:3)iyi(cid:0)x (cid:0) xi(cid:1) (cid:3) b(cid:1) (cid:1)

(3)

It can be shown that minimizing (2) corresponds to minimizing an upper bound on the VC
dimension of separating hyperplanes, or, equivalently, to maximizing the separation margin
between the two classes. In the next section, we shall depart from this and modify the dot
product used such that the minimization of (2) corresponds to enforcing transformation
invariance, while at the same time the constraints (1) still hold.

3 INVARIANT HYPERPLANES

Invariance by a selfconsistency argument. We face the following problem: to express
the condition of invariance of the decision function, we already need to know its coef-
cients which are found only during the optimization, which in turn should already take into
account the desired invariances. As a way out of this circle, we use the following ansatz:
consider decision functions f (cid:2) (cid:0)sgn (cid:5) g(cid:1), where g is dened as

(cid:0)

g(cid:0)xj(cid:1) (cid:7)(cid:2)

(cid:3)iyi(cid:0)Bxj (cid:0) Bxi(cid:1) (cid:3) b(cid:0)

(4)

with a matrix B to be determined below. This follows Vapnik (1995), who suggested to
incorporate invariances by modifying the dot product used. Any nonsingular B denes a
dot product, which can equivalently be written as (cid:0)xj (cid:0) Axi(cid:1), with a positive denite matrix
A (cid:2) B(cid:0)B.
Clearly, invariance of g under local transformations of all xj is a sufcient condition for
the local invariance of f, which is what we are aiming for. Strictly speaking, however,
invariance of g is not necessary at points which are not Support Vectors, since these lie in
a region where (cid:0)sgn (cid:5) g(cid:1) is constant  however, before training, it is hard to predict which
examples will turn out to become SVs. In the Virtual SV method (Scholkopf, Burges, &
Vapnik, 1996), a rst run of the standard SV algorithm is carried out to obtain an initial SV
set; similar heuristics could be applied in the present case.
Local invariance of g for each pattern xj under transformations of a differentiable local
1parameter group of local transformations Lt,

Xi(cid:2)

g(cid:0)Ltxj(cid:1) (cid:2) (cid:0)

(5)

(cid:4)

(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

can be approximately enforced by minimizing the regularizer


(cid:2)

(cid:0)

Xj(cid:2)(cid:3) (cid:4)
(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

g(cid:0)Ltxj(cid:1)(cid:4)

(cid:1)

(6)

Note that the sum may run over labelled as well as unlabelled data, so in principle one could
also require the decision function to be invariant with respect to transformations of elements
of a test set. Moreover, we could use different transformations for different patterns.

For (4), the local invariance term (5) becomes

(cid:4)

(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)(cid:0) (cid:0)
Xi(cid:2)

(cid:3)iyi(cid:0)BLtxj (cid:0) Bxi(cid:1) (cid:3) b(cid:1) (cid:2)

(cid:0)

Xi(cid:2)

(cid:3)iyi(cid:4)(cid:0)BLxj (cid:0) Bxi(cid:1) (cid:0) B

Ltxj (cid:0) (7)

(cid:4)

(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

using the chain rule. Here, (cid:4)(cid:0)BLxj (cid:0) Bxi(cid:1) denotes the gradient of (cid:0)x (cid:0) y(cid:1) with respect to
x, evaluated at the point (cid:0)x (cid:0) y(cid:1) (cid:2) (cid:0)BLxj (cid:0) Bxi(cid:1). Substituting (7) into (6), using the facts
that L (cid:2) I and (cid:4)(cid:0)x(cid:0) y(cid:1) (cid:2) y(cid:0), yields the regularizer


(cid:2)

(cid:0)

Xj(cid:2)(cid:0) (cid:0)
Xi(cid:2)

where

(cid:3)iyi(cid:0)Bxi(cid:1)(cid:0)B

Q (cid:7)(cid:2) B


(cid:2)

(cid:4)

(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)
Xj(cid:2)(cid:3) (cid:4)
(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

(cid:0)

(cid:2)



(cid:0)

Ltxj(cid:1)

Xi(cid:1)k(cid:2)
Ltxj(cid:4)(cid:3) (cid:4)
(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

(cid:3)iyi(cid:3)kyk(cid:0)Bxi (cid:0) QBxk(cid:1)

(8)

Ltxj(cid:4)(cid:0)

B(cid:0)(cid:1)

(9)

We now choose Q such that (8) reduces to the quadratic term in the standard SV optimiza-
tion problem (2) utilizing the dot product chosen in (4), i.e. such that (cid:0)Bx i (cid:0) QBxk(cid:1) (cid:2)
(cid:0)Bxi (cid:0) Bxk(cid:1)(cid:1) Assuming that the xi span the whole space, this condition becomes B (cid:0)QB (cid:2)
B(cid:0)B(cid:0) or, by requiring B to be nonsingular, i.e. that no information get lost during the
preprocessing, Q (cid:2) I. This can be satised by a preprocessing matrix

the nonnegative square root of the inverse of the nonnegative matrix

B (cid:2) C (cid:1) 
 (cid:0)

(10)

(11)

C (cid:7)(cid:2)
(cid:6)


(cid:2)

(cid:0)

Xj(cid:2)(cid:3) (cid:4)
(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

Ltxj(cid:4)(cid:3) (cid:4)
(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

Ltxj(cid:4)(cid:0)
A

(cid:1)

In practice, we use a matrix

(12)
 (cid:6) (cid:5) (cid:6) (cid:0) instead of C. As C is nonnegative, C(cid:2) is invertible. For (cid:5) (cid:2) , we recover the
standard SV optimal hyperplane algorithm, other values of (cid:5) determine the tradeoff be-
tween invariance and model complexity control. It can be shown that using C (cid:2) corresponds

C(cid:2) (cid:7)(cid:2) (cid:0) (cid:4) (cid:5)(cid:1)C (cid:3) (cid:5)I(cid:0)

(cid:3)t jt(cid:2)Ltxi(cid:1) (cid:3) (cid:5)kwk.

to using an objective function (cid:8)(cid:0)w(cid:1) (cid:2) (cid:0) (cid:4) (cid:5)(cid:1)Pi(cid:0)w (cid:0) (cid:3)

By choosing the preprocessing matrix B according to (10), we have obtained a formulation
of the problem where the standard SV quadratic optimization technique does in effect min-
imize the tangent regularizer (6): the maximum of (2), using the modied dot product as in
(4), coincides with the minimum of (6) subject to the separation conditions y i (cid:0) g(cid:0)xi(cid:1) (cid:3) ,
where g is dened as in (4).
Note that preprocessing with B does not affect classication speed: since (cid:0)Bxj (cid:0) Bxi(cid:1) (cid:2)
(cid:0)xj (cid:0) B(cid:0)Bxi(cid:1), we can precompute B (cid:0)Bxi for all SVs xi and thus obtain a machine (with
modied SVs) which is as fast as a standard SV machine (cf. (4)).

The nonlinear case. To construct nonlinear SV machines, kernel functions k(cid:0)x(cid:0) y(cid:1) are
substituted for every occurence of a dot product (in (2) and (3)), corresponding to a dot
product in some feature space which is nonlinearly related to input space (Boser, Guyon,
& Vapnik, 1992). In that case, the above analysis leads to the regularizer


(cid:2)

(cid:0)

Xj(cid:2)(cid:0) (cid:0)
Xi(cid:2)

(cid:3)iyi(cid:4)k(cid:0)Bxj (cid:0) Bxi(cid:1) (cid:0) B



(cid:1)

Ltxj(cid:1)

(13)

(cid:4)

(cid:4)t(cid:2)(cid:2)(cid:2)t(cid:2)

The derivative of k must be evaluated for specic kernels, e.g. for k(cid:0)x(cid:0) y(cid:1) (cid:2) (cid:0)x (cid:0) y(cid:1) d,
(cid:4)k(cid:0)x(cid:0) y(cid:1) (cid:2) d (cid:0) (cid:0)x (cid:0) y(cid:1)d(cid:1) (cid:0) y(cid:0)(cid:1) To obtain a kernelspecic constraint on the matrix B, one
has to equate the result with the quadratic term in the nonlinear objective function,

(cid:3)iyi(cid:3)kykk(cid:0)Bxi(cid:0) Bxk(cid:1)(cid:1)

(14)

Relationship to Principal Component Analysis (PCA). Let us presently return to the
linear case, and provide some interpretation of (10) and (11). If we sum over derivatives
(cid:3)t jt(cid:2)Ltxj have zero mean and C is a sample estimate
in both directions, the vectors (cid:2) (cid:3)
(cid:3)t jt(cid:2)Ltx. Being positive, C can be
of the covariance matrix of the random vector (cid:2) (cid:3)
diagonalized, C (cid:2) SDS (cid:0), with an orthogonal matrix S consisting of Cs Eigenvectors
and a diagonal matrix D containing the Eigenvalues. Then we can compute B (cid:2) C (cid:1) 
 (cid:2)
SD(cid:1) 
 being diagonal and nonnegative. Since the dot product is invariant
under orthogonal transformations, we may drop the leading S and (4) becomes

 S(cid:0), with D(cid:1) 

Xi(cid:1)k

(cid:0)

Xi(cid:2)

g(cid:0)xj(cid:1) (cid:2)

(cid:3)iyi(cid:0)D(cid:1) 

 S(cid:0)xj (cid:0) D(cid:1) 

 S(cid:0)xi(cid:1) (cid:3) b(cid:1)

(15)

A given pattern x is thus rst transformed by projecting it onto the Eigenvectors of the
tangent covariance matrix C, which are the rows of S (cid:0). The resulting feature vector is then
rescaled by dividing by the square roots of Cs Eigenvalues.1 In other words, the directions
(cid:3)t jt(cid:2)Ltx are scaled back, thus more emphasis is
of main variance of the random vector (cid:3)
put on features which are less variant under Lt. For example, in image analysis, if Lt
represent the group of translations, more emphasis is put on the relative proportions of ink
in the image rather than the positions of lines. The PCA interpretation of our preprocessing
matrix suggests the possibility to regularize and reduce dimensionality by discarding part
of the features, as it is common usage when doing PCA.
Going nonlinear by using nonlinear PCA. We are now in a position to describe the way
how to generalize to the nonlinear case. To this end, we use kernel principal component
analysis (Scholkopf, Smola, & Muller, 1996). This technique allows us to compute prin-
cipal components in a space F nonlinearly related to input space, just as nonlinear SV
machines construct decision rules in F . The kernel function k plays the role of the dot
product in F , i.e. k(cid:0)x(cid:0) y(cid:1) (cid:2) (cid:0)(cid:8)(cid:0)x(cid:1) (cid:0) (cid:8)(cid:0)y(cid:1)(cid:1). Kernel PCA consists in diagonalizing the ma-
trix (cid:0)k(cid:0)xi(cid:0) xj(cid:1)(cid:1)ij in order to obtain the Eigenvalues of the covariance matrix of the images
of the data in F and an implicit expression of its Eigenvectors: since they live in F , the
Eigenvectors cannot in general be given explicitely (F may be very high or even innite
dimensional). However, kernel PCA nds coefcients (cid:3)k

i such that

i k(cid:0)xi(cid:0) x(cid:1)
(cid:3)k

(16)

(cid:0)

Xi(cid:2)

1As an aside, note that our goal to build invariant SV machines has thus serendipitously provided
us with an approach for an open problem in SV learning, namely the one of scaling: in SV machines,
there has so far been no way of automatically assigning different weight to different directions in
input space: in a trained SV machine, the weights of the rst layer form a subset of the training
set. Choosing these Support Vectors from the training set only gives rather limited possibilities for
appropriately dealing with different scales in different directions of input space.

is the projection of x onto the kth Eigenvector of the covariance matrix in F . To generalize
(15) to the nonlinear case, we thus need to compute the tangent covariance matrix C (Eq.
11) in F and its projection onto the subspace of F given by the linear span of the images of
our data. Here the considerations of the linear case apply. Without going into further detail
on this issue, we state that the whole procedure reduces to computing dot products in F ,
which can be done using k, without explicitly mapping into F .

4 KERNELS USING LOCAL CORRELATIONS

By using a kernel k(cid:0)x(cid:0) y(cid:1) (cid:2) (cid:0)x (cid:0) y(cid:1)d, one implicitly constructs a decision boundary in
the space of all possible products of d pixels. This may not be desirable, since in natural
images, correlations over short distances are much more reliable features than longrange
correlations. To take this into account, we dene a kernel k d(cid:1)d

as follows:

p

1. compute a third image z, dened as the pixelwise product of x and y
2. sample z with a pyramidal receptive eld of diameter p, centered at all locations

(cid:0)i(cid:0) j(cid:1), to obtain the values zij

3. raise each zij to the power d, to take into account local correlations within the

range of the pyramid

4. sum zd

ij over the whole image, and raise the result to the power d to allow for
some longerange correlations between the outputs of the pyramidal receptive
elds

The resulting kernel will be of order dd, however, it will not contain all possible correla-
tions of dd pixels.

5 EXPERIMENTAL RESULTS

In the experiments, we used a subset of the MNIST data base of handwritten characters
(Bottou at al., 1994), consisting of 5000 training examples and 10000 test examples at a
resolution of 20x20 pixels, with entries in (cid:9)(cid:4)(cid:0) (cid:10). Using a linear SV machine (i.e. a sep-
arating hyperplane), we obtain a test error rate of 	(cid:1)(cid:13) (training 10 binary classiers, and
using the maximum value of g (Eq. 4) for 10class classication); by using a polynomial
kernel of degree 4, this drops to (cid:1)(cid:13). In all of the following experiments, we used degree
4 kernels of various types. The number 4 was chosen as it can be written as a product of
two integers, thus we could compare results to a kernel kp with d (cid:2) d (cid:2) . For the
considered classication task, results for higher polynomial degrees are very similar.
In a series of experiments with a homogeneous polynomial kernel k(cid:0)x(cid:0) y(cid:1) (cid:2) (cid:0)x (cid:0) y(cid:1) , using
preprocessing with Gaussian smoothing kernels of standard deviation (cid:1)(cid:0) (cid:1)(cid:0) (cid:1) (cid:1) (cid:1) (cid:0) (cid:1), we
obtained error rates which gradually increased from (cid:1)(cid:13) to (cid:1)(cid:13); thus no improvement
of this performance was possible by a simple smoothing operation. Applying the Virtual
SV method (retraining the SV machine on translated SVs; Scholkopf, Burges, & Vapnik,
1996) to this problem results in an improved error rate of (cid:1)(cid:13).
Invariant hyperplanes. Table 1 reports results obtained by preprocessing all patterns with
B (cf. (10)), choosing different values of (cid:5) (cf. Eq. (12)). In the experiments, the patterns
were rst rescaled to have entries in (cid:9)(cid:0) (cid:10), then B was computed, using horizontal and
vertical translations, and preprocessing was carried out; nally, the resulting patterns were
scaled back again. This was done to ensure that patterns and derivatives lie in comparable
regions of RN (note that if the pattern background level is a constant (cid:4), then its derivative
is ). The results show that even though (11) was derived for the linear case, it can lead to
improvements in the nonlinear case (here, for a degree 4 polynomial), too.

Table 1: Classication error rates for modifying the kernel k(cid:0)x(cid:0) y(cid:1) (cid:2) (cid:0)x (cid:0) y(cid:1)  with the
(cid:2) ; cf. Eqs. (10)  (12). Enforcing
invariant hyperplane preprocessing matrix B(cid:2) (cid:2) C
invariance with (cid:1) (cid:6) (cid:5) (cid:6)  leads to improvements over the original performance ((cid:5) (cid:2) ).

(cid:1) 


0.4
3.6

0.5
3.7

0.6
3.8

0.7
3.8

0.8
3.9

0.9
3.9

1.0
4.0

(cid:5)

0.1
error rate in % 4.2

0.2
3.8

0.3
3.6

Dimensionality reduction. The above (cid:9)(cid:0) (cid:10) scaling operation is afne rather than linear,
hence the argument leading to Eq. 15 does not hold for this case. We thus only report
results on dimensionality reduction for the case where the data is kept in (cid:9)(cid:0) (cid:10) scaling from
the very beginning on. Dropping principal components which are less important leads to
substantial improvements (Table 2); cf. the explanation following Eq. (15)).

Table 2: Dropping directions corresponding to small Eigenvalues of C (cf. (15)) leads to
substantial improvements. All results given are for the case (cid:5) (cid:2) (cid:1) (cf. Table 1); degree 4
homogeneous polynomial kernel.
principal components discarded

0
8.7

50
5.4

100
4.9

150
4.4

200
4.2

250
3.9

300
3.7

350
3.9

error rate in %

p

Kernels using local correlations. To exploit locality in images, we used pyramidal re-
ceptive eld kernel kd(cid:1)d
with diameter p (cid:2) 	 (cf. Sec. 4). For d (cid:2) d (cid:2) , we ob-
tained an improved error rate of (cid:1)(cid:13), another degree 4 kernel with only local correlations
(d (cid:2) (cid:0) d (cid:2) ) led to (cid:1)(cid:13). Albeit signicantly better than the (cid:1)(cid:13) for the degree 4
homogeneous polynomial (the error rates on the 10000 element test set have an accuracy
of about (cid:1)(cid:13), cf. Bottou at al., 1994), this is still worse than the Virtual SV result of (cid:1)(cid:13).
As the two methods, however, exploit different types of prior knowledge, it could be ex-
pected that combining them leads to still better performance; and indeed, this yielded the
best performance of all ((cid:1)(cid:13)).
For the purpose of benchmarking, we also ran our system on the US postal service database
of 7300(cid:3)2000 handwritten digits at a resolution of  (cid:7) . In that case, we obtained the
following test error rates: SV with degree 4 polynomial kernel (cid:1)(cid:13), Virtual SV (same ker-
nel) (cid:1)(cid:13), SV with k(cid:1)
 (cid:1)(cid:13). The latter compares favourably
to almost all known results on that data base, and is second only to a memorybased
tangentdistance nearest neighbour classier at (cid:1)(cid:13) (Simard, LeCun, & Denker, 1993).

 3.6%, Virtual SV with k(cid:1)

6 DISCUSSION

With its rather general class of admissible kernel functions, the SV algorithm provides am-
ple possibilities for constructing taskspecic kernels. We have considered an image classi-
cation task and used two forms of domain knowledge: rst, pattern classes were required
to be locally translationally invariant, and second, local correlations in the images were
assumed to be more reliable than longrange correlations. The second requirement can be
seen as a more general form of prior knowledge  it can be thought of as arising partially
from the fact that patterns possess a whole variety of transformations; in object recognition,
for instance, we have object rotations and deformations. Typically, these transformations
are continuous, which implies that local relationships in an image are fairly stable, whereas
global relationships are less reliable.

We have incorporated both types of domain knowledge into the SV algorithm by construct-

ing appropriate kernel functions, leading to substantial improvements on the considered
pattern recognition tasks. Our method for constructing kernels for transformation invari-
ant SV machines, put forward to deal with the rst type of domain knowledge, so far has
only been applied in the linear case, which partially explains why it only led to moder-
ate improvements (also, we so far only used translational invariance). It is applicable for
differentiable transformations  other types, e.g. for mirror symmetry, have to be dealt
with using other techniques (e.g. Scholkopf, Burges, & Vapnik, 1996). Its main advantages
compared to the latter technique is that it does not slow down testing speed, and that using
more invariances leaves training time almost unchanged. The proposed kernels respecting
locality in images led to large improvements; they are applicable not only in image classi-
cation but in all cases where the relative importance of subsets of products features can
be specied appropriately. They do, however, slow down both training and testing by a
constant factor which depends on the specic kernel used.

Both described techniques should be directly applicable to other kernelbased methods
as SV regression (Vapnik, 1995) and kernel PCA (Scholkopf, Smola, & Muller, 1996).
Future work will include the nonlinear case (cf. our remarks in Sec. 3), the incorporation
of invariances other than translation, and the construction of kernels incorporating local
feature extractors (e.g. edge detectors) different from the pyramids described in Sec. 4.

Acknowledgements. We thank Chris Burges and Leon Bottou for parts of the code and for
helpful discussions.

