Abstract

Multiple realizations of continuous-valued time series from a stochastic
process often contain systematic variations in rate and amplitude. To
leverage the information contained in such noisy replicate sets, we need
to align them in an appropriate way (for example, to allow the data to be
properly combined by adaptive averaging). We present the Continuous
Prole Model (CPM), a generative model in which each observed time
series is a non-uniformly subsampled version of a single latent trace, to
which local rescaling and additive noise are applied. After unsupervised
training, the learned trace represents a canonical, high resolution fusion
of all the replicates. As well, an alignment in time and scale of each
observation to this trace can be found by inference in the model. We
apply CPM to successfully align speech signals from multiple speakers
and sets of Liquid Chromatography-Mass Spectrometry proteomic data.

1 A Prole Model for Continuous Data

When observing multiple time series generated by a noisy, stochastic process, large sys-
tematic sources of variability are often present. For example, within a set of nominally
replicate time series, the time axes can be variously shifted, compressed and expanded,
in complex, non-linear ways. Additionally, in some circumstances, the scale of the mea-
sured data can vary systematically from one replicate to the next, and even within a given
replicate.

We propose a Continuous Prole Model (CPM) for simultaneously analyzing a set of such
time series. In this model, each time series is generated as a noisy transformation of a
single latent trace. The latent trace is an underlying, noiseless representation of the set
of replicated, observable time series. Output time series are generated from this model
by moving through a sequence of hidden states in a Markovian manner and emitting an
observable value at each step, as in an HMM. Each hidden state corresponds to a particular
location in the latent trace, and the emitted value from the state depends on the value of
the latent trace at that position. To account for changes in the amplitude of the signals
across and within replicates, the latent time states are augmented by a set of scale states,
which control how the emission signal will be scaled relative to the value of the latent trace.
During training, the latent trace is learned, as well as the transition probabilities controlling
the Markovian evolution of the scale and time states and the overall noise level of the

observed data. After training, the latent trace learned by the model represents a higher
resolution fusion of the experimental replicates. Figure 1 illustrate the model in action.

Unaligned, Linear Warp Alignment and CPM Alignment

e
d
u

t
i
l

p
m
A

e
d
u

t
i
l

p
m
A

40

30

20

10

0
50

40

30

20

10

0
30

e
d
u

t
i
l

p
m
A

20

10

0

Time
a)

b)

Figure 1: a) Top: ten replicated speech energy signals as described in Section 4), Middle:
same signals, aligned using a linear warp with an offset, Bottom: aligned with CPM (the
learned latent trace is also shown in cyan). b) Speech waveforms corresponding to energy
signals in a), Top: unaligned originals, Bottom: aligned using CPM.

2 Dening the Continuous Prole Model (CPM)

1 ; xk

2 ; :::; xk

N k ). The
The CPM is generative model for a set of K time series, ~xk = (xk
temporal sampling rate within each ~xk need not be uniform, nor must it be the same across
the different ~xk. Constraints on the variability of the sampling rate are discussed at the end
of this section. For notational convenience, we henceforth assume N k = N for all k, but
this is not a requirement of the model.
The CPM is set up as follows: We assume that there is a latent trace, ~z = (z1; z2; :::; zM ), a
canonical representation of the set of noisy input replicate time series. Any given observed
time series in the set is modeled as a non-uniformly subsampled version of the latent trace
to which local scale transformations have been applied. Ideally, M would be innite, or at
least very large relative to N so that any experimental data could be mapped precisely to the
correct underlying trace point. Aside from the computational impracticalities this would
pose, great care to avoid overtting would have to be taken. Thus in practice, we have used
M = (2 + (cid:15))N (double the resolution, plus some slack on each end) in our experiments
and found this to be sufcient with (cid:15) < 0:2. Because the resolution of the latent trace is
higher than that of the observed time series, experimental time can be made effectively to
speed up or slow down by advancing along the latent trace in larger or smaller jumps.

The subsampling and local scaling used during the generation of each observed time se-
ries are determined by a sequence of hidden state variables. Let the state sequence for
observation k be ~(cid:25)k. Each state in the state sequence maps to a time state/scale state pair:
i g. Time states belong to the integer set (1::M ); scale states belong to an
(cid:25)k
i ! f(cid:28) k
(In our experiments we have used Q=7, evenly spaced scales in
ordered set ((cid:30)1::(cid:30)Q).
i , are related by the emission
logarithmic space). States, (cid:25)k
probability distribution: A(cid:25)k
i uk; (cid:27)), where (cid:27)
(cid:30)k

i , and observation values, xk
(xk

i ; ~z; (cid:27); uk) (cid:17) N (xk

i ; (cid:30)k

i j~z) (cid:17) p(xk

i j(cid:25)k

i

i ; z(cid:28) k

i

is the noise level of the observed data, N (a; b; c) denotes a Gaussian probability density
for a with mean b and standard deviation c. The uk are real-valued scale parameters, one
per observed time series, that correct for any overall scale difference between time series k
and the latent trace.

To fully specify our model we also need to dene the state transition probabilities. We
dene the transitions between time states and between scale states separately, so that
(cid:25)i(cid:0)1;(cid:25)i (cid:17) p((cid:25)ij(cid:25)i(cid:0)1) = p((cid:30)ij(cid:30)i(cid:0)1)pk((cid:28)ij(cid:28)i(cid:0)1). The constraint that time must move
T k
forward, cannot stand still, and that it can jump ahead no more than J(cid:28) time states is en-
forced. (In our experiments we used J(cid:28) = 3.) As well, we only allow scale state transitions
between neighbouring scale states so that the local scale cannot jump arbitrarily. These
constraints keep the number of legal transitions to a tractable computational size and work
well in practice. Each observed time series has its own time transition probability dis-
tribution to account for experiment-specic patterns. Both the time and scale transition
probability distributions are given by multinomials:

pk((cid:28)i = aj(cid:28)i(cid:0)1 = b) =

8>>>>>><
>>>>>>:
p((cid:30)i = aj(cid:30)i(cid:0)1 = b) =8>><
>>:

dk
1 ;
dk
2 ;
...
dk
J(cid:28) ;
0;

if a (cid:0) b = 1
if a (cid:0) b = 2

if a (cid:0) b = J(cid:28)
otherwise

s0;
s1;
s1;
0;

if D(a; b) = 0
if D(a; b) = 1
if D(a; b) = (cid:0)1
otherwise

where D(a; b) = 1 means that a is one scale state larger than b, and D(a; b) = (cid:0)1 means
that a is one scale state smaller than b, and D(a; b) = 0 means that a = b. The distributions

are constrained by: PJ(cid:28)

i=1 dk

i = 1 and 2s1 + s0 = 1.

J(cid:28) determines the maximum allowable instantaneous speedup of one portion of a time
series relative to another portion, within the same series or across different series. However,
the length of time for which any series can move so rapidly is constrained by the length of
the latent trace; thus the maximum overall ratio in speeds achievable by the model between
any two entire time series is given by min(J(cid:28) ; M
After training, one may examine either the latent trace or the alignment of each observable
time series to the latent trace. Such alignments can be achieved by several methods, in-
cluding use of the Viterbi algorithm to nd the highest likelihood path through the hidden
states [1], or sampling from the posterior over hidden state sequences. We found Viterbi
alignments to work well in the experiments below; samples from the posterior looked quite
similar.

N ).

3 Training with the Expectation-Maximization (EM) Algorithm

As with HMMs, training with the EM algorithm (often referred to as Baum-Welch in the
context of HMMs [1]), is a natural choice. In our model the E-Step is computed exactly
using the Forward-Backward algorithm [1], which provides the posterior probability over
s (i) (cid:17) p((cid:25)i = sj~x) and also
states for each time point of every observed time series, (cid:13) k
the pairwise state posteriors, (cid:24)s;t(i) (cid:17) p((cid:25)i(cid:0)1 = s; (cid:25)i = tj~xk). The algorithm is modied

only in that the emission probabilities depend on the latent trace as described in Section 2.
The M-Step consists of a series of analytical updates to the various parameters as detailed
below.

Given the latent trace (and the emission and state transition probabilities), the complete log
likelihood of K observed time series, ~xk, is given by Lp (cid:17) L + P. L is the likelihood term
arising in a (conditional) HMM model, and can be obtained from the Forward-Backward
algorithm. It is composed of the emission and state transition terms. P is the log prior
(or penalty term), regularizing various aspects of the model parameters as explained below.
These two terms are:

L (cid:17)

log A(cid:25)i (xk

i j~z) +

N

Xi=1

K

Xk=1 log p((cid:25)1) +
Xj=1

(cid:28) (cid:0)1

K

Xk=1

log T k

(cid:25)i(cid:0)1;(cid:25)i!

N

Xi=2

P (cid:17) (cid:0)(cid:21)

(zj+1 (cid:0) zj)2 +

log D(dk

vjf(cid:17)k

v g) + log D(svjf(cid:17)0

vg);

(1)

(2)

where p((cid:25)1) are priors over the initial states. The rst term in Equation 2 is a smoothing
penalty on the latent trace, with (cid:21) controlling the amount of smoothing. (cid:17) k
v are
Dirichlet hyperprior parameters for the time and scale state transition probability distribu-
tions respectively. These ensure that all non-zero transition probabilities remain non-zero.
v corresponds to the pseudo-count data for
For the time state transitions, v 2 f1; J(cid:28) g and (cid:17)k
v corresponds
the parameters d1, d2 . . . dJ(cid:28) . For the scale state transitions, v 2 f0; 1g and (cid:17)k
to the pseudo-count data for the parameters s0 and s1.
Letting S be the total number of possible states, that is, the number of elements in the
cross-product of possible time states and possible scale states, the expected complete log
likelihood is:

v and (cid:17)0

<Lp>(cid:25)=P +

(cid:13)k
s (1) log T k

0;s +

K

S

N

Xk=1

Xs=1

Xi=1

(cid:13)k
s (i) log As(xk

i j~z) + : : :

: : : +

(cid:24)k
s;s0(i) log T k

s;s0

K

S

K

Xs=1
Xk=1
Xs=1
Xk=1

S

S

N

Xs0=1

Xi=2

0;s (cid:17) p((cid:25)1 = s), and where (cid:13)k

using the notation T k
s;s0(i) are the posteriors over
states as dened above. Taking derivatives of this quantity with respect to each of the
parameters and nding the critical points provides us with the M-Step update equations. In
updating the latent trace ~z we obtain a system of M simultaneous equations, for j = 1::M:

s (i) and (cid:24)k

@ <Lp>(cid:25)

@zj

= 0 =

K

Xk=1 Xfsj(cid:28)s=jg

N

Xi=1(cid:20)(cid:13)k

s (i)(cid:30)suk (xk

i (cid:0) zjuk(cid:30)s)

(cid:27)2

(cid:21)(cid:0) (cid:21)(4zj (cid:0) 2zj(cid:0)1 (cid:0) 2zj+1)

For the cases j = 1; N, the terms 2zj(cid:0)1 and 2zj+1, respectively, drop out. Considering
all such equations we obtain a system of M equations in M unknowns. Each equation
depends only linearly on three variables from the latent trace. Thus the solution is easily
obtained numerically by solving a tridiagonal linear system.

Analytic updates for (cid:27)2 and uk are given by:

(cid:27)2 = PS

s=1PN

N

i=1 (cid:13)k

s (i)(xk

i (cid:0) z(cid:28)s uk(cid:30)s)2

;

s=1 z(cid:28)s (cid:30)sPN
uk = PS
PS
s=1(z(cid:28)s (cid:30)s)2PN

i=1 (cid:13)k

s (i)xk
i
s (i)

i=1 (cid:13)k

Lastly, updates for the scale and state transition probability distributions are given by:

dk
v =

sv =

(cid:17)k

i=2 (cid:24)k

v +PS
j +PJ(cid:28)
j +PK
j +PK

s=1Pfs0j(cid:28)s0 (cid:0)(cid:28)s=vgPN
j=1PS
k=1PS
k=1PS

s=1Pfs0j(cid:28)s0 (cid:0)(cid:28)s=jgPN
s=1Pfs002H(s;v)gPN
s=1Pfs002H(s;1);H(s;0)gPN

s;s00 (i)
i=2 (cid:24)k
i=2 (cid:24)k

j=1 (cid:17)k
(cid:17)0
j=0 (cid:17)0

PJ(cid:28)
P1

s;s00 (i)

s;s00 (i)
i=2 (cid:24)k

s;s00 (i)

vg) =Q1

where H(s; j) (cid:17) fs0js0is exactly j scale states away from sg.
not normalize the Dirichlets, and omit
v and D(svjf(cid:17)0
D(dk

Note that we do
the traditional minus one in the exponent:

v=0(sv)(cid:17)0
v .

v=1(dk

v)(cid:17)k

vjf(cid:17)k

The M-Step updates uk, (cid:27), and ~z are coupled. Thus we arbitrarily pick an order to update
them and as one is updated, its new values are used in the updates for the coupled parameter
updates that follow it. In our experiments we updated in the following order: (cid:27), ~z, uk. The
other two parameters, dk

v and sv, are completely decoupled.

v g) =QJ(cid:28)

4 Experiments with Laboratory and Speech Data

We have applied the CPM model to analyze several Liquid Chromatography - Mass Spec-
trometry (LC-MS) data sets from an experimental biology laboratory. Mass spectrometry
technology is currently being developed to advance the eld of proteomics [2, 3]. A mass
spectrometer takes a sample as input, for example, human blood serum, and produces a
measure of the abundance of molecules that have particular mass/charge ratios. In pro-
teomics the molecules in question are small protein fragments. From the pattern of abun-
dance values one can hope to infer which proteins are present and in what quantity. For
protein mixtures that are very complex, such as blood serum, a sample preparation step
is used to physically separate parts of the sample on the basis of some property of the
molecules, for example, hydrophobicity. This separation spreads out the parts over time
so that at each unique time point a less complex mixture is fed into the mass spectrometer.
The result is a two-dimensional time series spectrum with mass/charge on one axis and
time of input to the mass spectrometer on the other. In our experiments we collapsed the
data at each time point to one dimension by summing together abundance values over all
mass/charge values. This one-dimensional data is referred to as the Total Ion Count (TIC).
We discuss alternatives to this in the last section. After alignment of the TICs, we assessed
the alignment of the LC-MS data by looking at both the TIC alignments, and also the cor-
responding two-dimensional alignments of the non-collapsed data, which is where the true
information lies.

The rst data set was a set of 13 replicates, each using protein extracted from lysed E. coli
cells. Proteins were digested and subjected to capillary-scale LC-MS coupled on-line to an
ion trap mass spectrometer. First we trained the model with no smoothing (i.e., (cid:21) = 0) on
the 13 replicates. This provided nice alignments when viewed in both the TIC space and
the full two-dimensional space. Next we used leave-one-out cross-validation on six of the
v are time series
replicates in order to choose a suitable value for (cid:21). Because the uk and dk
specic, we ran a restricted EM on the hold-out case to learn these parameters, holding the
other parameters xed at the values found from learning on the training set. Sixteen values
of (cid:21) over ve orders of magnitude, and also zero, were used. Note that we did not include
the regularization likelihood term in the calculations of hold-out likelihood. One of the
non-zero values was found to be optimal (statistically signicant at a p=0.05 level using a
paired sample t-test to compare it to no smoothing). Visually, there did not appear to be
a difference between no regularization and the optimal value of (cid:21), in either the TIC space

or the full two-dimensional space. Figure 2 shows the alignments applied to the TICs and
also the two-dimensional data, using the optimal value of (cid:21).

x 108

Unaligned and Aligned Time Series

e
d
u

t
i
l

p
m
A

10

8

6

4

2

x 108

0

e
d
u

t
i
l



p
m
A
e
c
a
p
S


t

n
e
a
L

t

6

4

2

0

e
d
u

t
i
l

p
m
A

9

8

7

6

5

4

3

2

1

0

x 108

Replicate 5

Original Time Series

Latent Trace
Aligned Experimental Time Series

100

Residual

200

300

400

500

600

700

800

3 Time Jump From Previous State
2
1

Scale States

100

200

300

400

Time

a)

500

600

700

800

200

600

800

400
Latent Time

b)

c)

d)

Figure 2: Figure 2: a) Top: 13 Replicate pre-processed TICs as described in Section 4),
Bottom: same as top, but aligned with CPM (the learned latent trace is also shown). b) The
fth TIC replicate aligned to the learned latent trace (inset shows the original, unaligned).
Below are three strips showing, from top-to-bottom, i) the error residual, ii) the number
of time states moved between every two states in the Viterbi alignment, and iii) the local
scaling applied at each point in the alignment. c) A portion of the two-dimensional LC-MS
data from replicates two (in red) and four (in green). d) Same as c), but after alignment (the
same one dimensional alignment was applied to every Mass/Charge value). Marker lines
labeled A to F show how time in c) was mapped to latent time using the Viterbi alignment.

We also trained our model on ve different sets of LC-MS data, each consisting of human
blood serum. We used no smoothing and found the results visually similar in quality to the
rst data set.

To ensure convergence to a good local optimum and to speed up training, we pre-processed
the LC-MS data set by coarsely aligning and scaling each time series as follows: We 1)
translated each time series so that the center of mass of each time series was aligned to the
median center of mass over all time series, 2) scaled the abundance values such that the
sum of abundance values in each time series was equal to the median sum of abundance
values over all time series.

We also used our model to align 10 speech signals, each an utterance of the same sentence

spoken by a different speaker. The short-time energy (using a 30ms Hanning window) was
computed every 8ms for each utterance and the resulting vectors were used as the input
to CPM for alignment. The smoothing parameter (cid:21) was set to zero. For comparison, we
also performed a linear warping of time with an offset. (i.e. each signal was translated so
as to start at the same time, and the length of each signal was stretched or compressed so
as to each occupy the same amount of time). Figure 1 shows the successful alignment of
the speech signals by CPM and also the (unsuccessful) linear warp. Audio for this exam-
ple can be heard at http://www.cs.toronto.edu/jenn/alignmentStudy,
which also contains some supplemental gures for the paper.

Initialization for EM training was performed as follows: (cid:27) was set to 15% of the difference
between the maximum and minimum values of the rst time series. The latent trace was
initialized to be the rst observed time series, with Gaussian, zero-mean noise added, with
standard deviation equal to (cid:27). This was then upsampled by a factor of two by repeating
every value twice in a row. The additional slack at either end of the latent trace was set to
be the minimum value seen in the given time series. The uk were each set to one and the
multinomial scale and state transition probabilities were set to be uniform.

5 Related Algorithms and Models

Our proposed CPM has many similarities to Input/Output HMMs (IOHMMs), also called
Conditional HMMs [4]. IOHMMs extend standard HMMs [1] by conditioning the emission
and transition probabilities on an observed input sequence. Each component of the output
sequence corresponds to a particular component of the input. Training of an IOHMM
is supervised  a mapping from an observed input sequence to output target sequence
is learned. Our CPM also requires input and thus is also a type of conditional HMM.
However, the input is unobserved (but crucially it is shared between all replicates) and
hence learning is unsupervised in the CPM model. One could also take the alternative view
that the CPM is simply an HMM with an extra set of parameters, the latent trace, that affect
the emission probabilities and which are learned by the model.

The CPM is similar in spirit to Prole HMMs which have been used with great success
for discrete, multiple sequence alignment, modeling of protein families and their con-
served structures, gene nding [5], among others. Prole HMM are HMMs augmented
by constrained-transition Delete and Insert states, with the former emitting no observa-
tions. Multiple sequences are provided to the Prole HMM during training and a summary
of their shared statistical properties is contained in the resulting model. The development
of Prole HMMs has provided a robust, statistical framework for reasoning about sets of
related discrete sequence data. We put forth the CPM as a continuous data, conditional
analogue.

Many algorithms currently used for aligning continuous time series data are variations of
Dynamic Time Warping (DTW) [6], a dynamic programming based approach which origi-
nated in the speech recognition community as a robust distance measure between two time
series. DTW works on pairs of time series, aligning one time series to a specied reference
time series. DTW does not take in to account systematic variations in the amplitude of the
signal. Our CPM can be viewed as a rich and robust extension of DTW that can be applied
to many time series in parallel and which automatically uncovers the underlying template
of the data.

6 Discussion and Conclusion

We have introduced a generative model for sets of continuous, time series data. By training
this model one can leverage information contained in noisy, replicated experimental data,

and obtain a single, superior resolution fusion of the data. We demonstrated successful
use of this model on real data, but note that it could be applied to a wide range of problems
involving time signals, for example, alignment of gene expression time proles, alignment
of temporal physiological signals, alignment of motion capture data, to name but a few.

Certain assumptions of the model presented here may be violated under different ex-
perimental conditions. For example, the Gaussian emission probabilities treat errors in
large amplitudes in the same absolute terms as in smaller amplitudes, whereas in real-
ity, it may be that the error scales with signal amplitude. Similarly, the penalty term
j=1 (zj+1 (cid:0) zj)2 does not scale with the amplitude; this might result in the model
arbitrarily preferring a lower amplitude latent trace. (However, in practice, we did not nd
this to be a problem.)

(cid:0)(cid:21)P(cid:28) (cid:0)1

One immediate and straight-forward extension to the model would be to allow the data at
each time point to be a multi-dimensional feature vector rather than a scalar value. This
could easily be realized by allowing the emission probabilities to be multi-dimensional. In
this way a richer set of information could be used: either the raw, multi-dimensional feature
vector, or some transformation of the feature vectors, for example, Principal Components
Analysis. The rest of the model would be unchanged and each feature vector would move
as a coherent piece. However, it might also be useful to allow different dimensions of the
feature vector to be aligned differently. For example, with the LC-MS data, this might
mean allowing different mass/charge peptides to be aligned differently at each time point.
However, in its full generality, such a task would be extremely computational intense.

A perhaps more interesting extension is to allow the model to work with non-replicate data.
For example, suppose one had a set of LC-MS experiments from a set of cancer patients,
and also a set from normal persons. It would be desirable to align the whole set of time
series and also to have the model tease out the differences between them. One approach
is to consider the model to be semi-supervised - the model is told the class membership
of each training example. Then each class is assigned its own latent trace, and a penalty
is introduced for any disagreements between the latent traces. Care needs to be taken to
ensure that the penalty plateaus after a certain amount of disagreement between latent trace
points, so that parts of the latent trace which are truly different are able to whole-heartedly
disagree. Assuming that the time resolution in the observed time series is sufciently high,
one might also want to encourage the amount of disagreement over time to be Markovian.
That is, if the previous time point disagreed with the other latent traces, then the current
point should be more likely to disagree.

