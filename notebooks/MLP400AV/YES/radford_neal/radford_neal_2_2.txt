Abstract

The  attempt  to  find  a  single  "optimal"  weight  vector  in  conven(cid:173)
tional network training can lead to overfitting and poor generaliza(cid:173)
tion.  Bayesian methods avoid  this,  without the  need  for  a  valida(cid:173)
tion set,  by  averaging the outputs of many  networks with  weights
sampled  from  the  posterior  distribution  given  the  training  data.
This sample can be obtained by simulating a  stochastic dynamical
system that has the posterior as  its stationary distribution.

1  CONVENTIONAL AND  BAYESIAN  LEARNING

I view neural networks as probabilistic models, and learning as statistical inference.
Conventional  network  learning  finds  a  single  "optimal"  set  of network  parameter
values,  corresponding  to maximum likelihood or maximum penalized likelihood in(cid:173)
ference.  Bayesian  inference  instead  integrates  the  predictions of the  network  over
all  possible  values  of the  network  parameters,  weighting each  parameter set  by  its
posterior probability in light of the training data.

1.1  NEURAL  NETWORKS  AS  PROBABILISTIC MODELS

Consider a network taking a vector of real-valued inputs, x, and producing a  vector
of real-valued  outputs,  y,  perhaps  computed  using  hidden  units.  Such  a  network
architecture corresponds  to a function,  I,  with y = I(x, w),  where w  is  a  vector of
connection weights.  If we assume the observed outputs, y, are equal to y  plus Gaus(cid:173)
sian noise  of standard deviation  (j,  the network  defines  the conditional probability

475

476

Neal

for  an observed  output vector given an input vector  as follows:
ex:  exp( -IY - !(x, w)12 /20"2)

P(y I x, 0")

(1)

The probability of the outputs in a training set (Xl, yt), ... , (Xn, Yn)  given this fixed
noise level  is therefore

P(Yl, ... , Yn I Xl,., Xn, 0")

ex:  exp( - E lYe  - !(Xe, w)12 /20"2)

e

(2)

Often 0"  is  unknown.  A  Bayesian approach  to handling this is  to assign  0"  a  vague
prior distribution and  then  .ntcgrating it away, giving the following probability for
the training set  (see  (Buntine and Weigend,  1991) or (Neal,  1992) for  details):

P(Yl,""  Yn I Xl, ... , Xn)

ex:

(so + E lYe  - !(Xe, w)12) -

e

mpnD

2

(3)

where  So  and mo  are parameters of the prior for  0".

1.2  CONVENTIONAL  LEARNING

Conventional backpropagation learning tries  to find  the weight  vector  that  assigns
the highest  probability to the training data, or equivalently, that minimizes minus
the log probability of the training data.  When 0"  is assumed known,  we  can use  (2)
to obtain the following objective function  to minimize:

M(w)  =  E lYe  - !(Xe, w)12 /  20"2

e

When 0"  is  unknown,  we  can instead minimize the following,  derived from  (3):

M(w)

e

(4)

(5)

Conventional  learning  often  leads  to  the  network  over fitting  the  training  data -
modeling  the  noise,  rather  than  the  true  regularities.  This  can  be  alleviated  by
stopping learning when the the performance of the network on a separate validation
set  begins  to  worsen,  rather  than improve.  Another  way  to  avoid overfitting  is  to
include a  weight  decay  term in the objective function,  as  follows:

M'(w)  =  Alwl 2  +  M(w)

(6)

Here,  the data fit  term,  M(w), may come from either (4) or (5).  We must somehow
find  an appropriate value for  A,  perhaps,  again, using a  separate validation set.

1.3  BAYESIAN LEARNING  AND  PREDICTION

Unlike conventional training, Bayesian learning does not look for a single "optimal"
set  of network  weights.  Instead,  the  training  data  is  used  to  find  the  posterior
probability distribution over  weight  vectors.  Predictions for  future  cases  are  made
by  averaging the outputs obtained with  all possible  weight  vectors,  with each  con(cid:173)
tributing in proportion to its posterior probability.

To obtain the posterior, we must first  define a  prior distribution for weight vectors.
We  might, for  example, give each  weight a  Gaussian prior of standard deviation w:

(7)

Bayesian  Learning via  Stochastic Dynamics

477

We can then obtain the posterior distribution over weight vectors given the training
cases  (Xl, yt), ... , (Xn, Yn)  using  Bayes' Theorem:

P(w I (Xl, yt}, ... , (Xn, Yn))

oc  P(w) P(YI, ... , Yn  I Xl, ... , Xn, w)

(8)

Based on the training data, the  best  prediction for  the output vector in a  test  case
with  input vector X.,  assuming squared-error loss,  is

Y.  =  J /(x.,w)P(w I (xI,yd, .. ,(xn,Yn))dw

(9)

A  full  predictive distribution for  the outputs in the test  case  can  also  be obtained,
quantifying the uncertainty in the above prediction.

2

INTEGRATION BY MONTE  CARLO METHODS

Integrals such  as  that of (9)  are  difficult  to evaluate.  Buntine and Weigend  (1991)
and MacKay (1992) approach this problem by approximating the posterior distribu(cid:173)
tion by  a  Gaussian.  Instead, I evaluate such  integrals using  Monte  Carlo methods.
If we  randomly select  weight  vectors,  wo, ... , WN-I, each  distributed  according to
the  posterior,  the  prediction  for  a  test  case  can  be  found  by  approximating  the
integral of (9)  by  the average output of networks with  these  weights:

y.  ~  ~ L/(x.,Wt)

(10)

This  formula  is  valid  even  if the  Wt  are  dependent,  though  a  larger  sample  may
then  be  needed  to  achieve  a  given  error  bound.  Such  a  sample  can  be  obtained
by  simulating  an  ergodic  Markov  chain  that  has  the  posterior  as  its  stationary
distribution.  The  early  part  of the  chain,  before  the  stationary  distribution  has
been reached,  is  discarded.  Subsequent  vectors  are used  to estimate the integral.

t

2.1  FORMULATING THE PROBLEM IN  TERMS  OF ENERGY

Consider  the  general  problem  of obtaining  a  sample  of  (dependent)  vectors,  qt,
with  probabilities  given  by  P( q).  For  Bayesian  network  learning,  q  will  be  the
weight  vector,  or  other  parameters  from  which  the  weights  can  be  obtained,  and
the distribution of interest  will be the posterior.

It will be  convenient to express  this probability distribution in  terms of a  potential
energy  function,  E( q), chosen  so  that

P(q)

oc  exp(-E(q))

(11)

A  momentum vector, p, of the same dimensions as q, is also introduced, and defined
to have  a  kinetic  energy  of ~ \pI2.  The sum of the potential and kinetic energies  is
the  Hamiltonian:

H(q,p)  =  E(q)  +  ~lpl2

(12)

(13)

From the Hamiltonian, we define ajoint probability distribution over q and p  (phase
space)  as  follows:

P(q,p)  oc  exp(-H(q,p))

The marginal distribution for q in (13) is that of (11), from which we wish to sample.

478

Neal

We can therefore proceed  by sampling from this joint distribution for  q  and p, and
then just ignoring the values obtained for  p.

2.2  HAMILTONIAN  DYNAMICS

Sampling  from  the  distribution  (13)  can  be  split  into  two  subproblems  -
to  sample  uniformly  from  a  surface  where  H,  and  hence  the  probability,  is  con(cid:173)
stant,  and second,  to visit points of differing  H  with the correct  probabilities.  The
solutions to these subproblems can  then be interleaved to give  an overall solution.

first,

The  first  subproblem  can  be  solved  by  simulating  the  Hamiltonian  dynamics  of
the system,  in which  q  and p  evolve  through  a  fictitious  time,  r, according  to the
following equations:

dq
dr

8H
8p  = p,

dp
- = -- = -VE(q)
dr

8H
8q

(14)

This  dynamics  leaves  H  constant,  and  preserves  the  volumes  of regions  of phase
space.  It therefore visits points on a surface of constant H  with uniform probability.

When  simulating this  dynamics,  some  discrete  approximation must  be  used.  The
leapfrog  method exactly  maintains the preservation of phase space  volume.  Given
a  size for  the time step,  E,  an iteration of the leapfrog method goes  as follows:

p(r+ E/2)
q(r+ E)
p(r + E)

per) - (E/2)VE(q(r)
q(r)+Ep
p(r + E)  - (E/2)V E(q(r + E

(15)

2.3  THE STOCHASTIC  DYNAMICS  METHOD

To create a  Markov chain that converges  to the distribution of (13),  we  must inter(cid:173)
leave leapfrog  iterations,  which  keep  H  (approximately)  constant,  with steps  that
can change H.  It is convenient for  the latter to affect  only p, since it enters into H
in a  simple way.  This general approach is  due to Anderson  (1980).
I  use  stochastic steps of the following form  to change H:

p'

(16)

where  0 < (l'  < 1,  and n  is  a  random vector with components picked independently
from  Gaussian  distributions  of mean  zero  and  standard  deviation  one.  One  can
show  that  these  steps  leave  the  distribution  of (13)  invariant.  Alternating  these
stochastic  steps  with  dynamical  leapfrog  steps  will  therefore  sample  values  for  q
and p  with close  to the desired  probabilities.  In so far  as  the discretized  dynamics
does  not keep  H exactly constant, however,  there will be some degree of bias, which
will be eliminated only in the limit as  E  goes  to zero.

It is  best  to use  a  value of (l'  close  to one,  as  this reduces  the  random walk  aspect
of the dynamics.  If the random term in (16)  is omitted, the procedure is equivalent
to ordinary batch mode backpropagation learning with momentum.

Bayesian  Learning via Stochastic Dynamics

479

2.4  THE HYBRID  MONTE  CARLO  METHOD

The bias introduced into the stochastic  dynamics method  by  using  an approxima(cid:173)
tion  to the  dynamics  is eliminated  in  the  Hybrid  Monte  Carlo method  of Duane,
Kennedy,  Pendleton,  and Roweth (1987).

This  method  is  a  variation  on  the  algorithm  of Metropolis,  et  al  (1953),  which
generates  a  Markov  chain  by  considering  randomly-selected  changes  to  the  state.
A  change  is  always accepted  if it lowers  the energy  (H), or leaves  it unchanged.  If
it  increases  the energy,  it is  accepted  with  probability exp( -LlH), and  is  rejected
otherwise,  with the old  state then being repeated.
In  the  Hybrid  Monte  Carlo method,  candidate  changes  are produced  by  picking  a
random value for p  from its distribution given by (13) and then performing some pre(cid:173)
determined  number of leapfrog steps.  If the leapfrog method were  exact,  H  would
be  unchanged,  and  these  changes  would  always  be  accepted.  Since  the  method
is  actually  only  approximate,  H  sometimes  increases,  and  changes  are  sometimes
rejected,  exactly cancelling the bias introduced by  the  approximation.

Of course,  if the errors are very  large,  the acceptance  probability will  be  very  low,
and  it  will  take  a  long  time  to  reach  and  explore  the  stationary  distribution.  To
avoid  this,  we  need  to choose a  step size  (f) that is  small enough.

3  RESULTS  ON A  TEST PROBLEM

I  use  the  "robot  arm"  problem of MacKay  (1992) for  testing.  The task  is  to learn
the mapping from two real-valued inputs, Xl  and X2,  to two real-valued outputs,  YI
and Y2,  given by

ih  =  2.0  cos(xI)  +  1.3 COS(XI  +  X2)
Y2  =  2.0  sin(xI)  +  1.3 sin(xi + X2)

(17)
(18)
Gaussian noise of mean zero and standard deviation 0.05 is added to (YI' Y2)  to give
the observed position, (YI,  Y2).  The training and test sets each consist of 200 cases,
with  Xl  picked  randomly  from  the  ranges  [-1.932, -0.453]  and  [+0.453, +1.932],
and  X2  from  the range  [0.534,3.142].
A  network with  16 sigmoidal hidden units  was  used.  The output units were  linear.
Like  MacKay,  I  group  weights  into  three  categories  -
input  to  hidden,  bias  to
hidden,  and  hidden/bias  to  output.  MacKay  gives  separate  priors  to  weights  in
I  fix  w  to  one,  but
each  category,  finding  an  appropriate  value  of w  for  each.
multiply each  weight  by  a  scale  factor associated  with  its category  before  using it,
giving  an equivalent  effect.  For  conventional  training with  weight  decay,  I  use  an
analogous scheme  with three weight decay  constants  (.\  in (6.

In all cases,  I  assume  that the true value of u  is  not known.  I  therefore  use  (3)  for
the  training set  probability, and  (5)  for  the  data fit  term in conventional training.
I  set  80 = rno  = 0.1,  which  corresponds  to a  very  vague prior for  u.

3.1  PERFORMANCE OF  CONVENTIONAL  LEARNING

Conventional  backpropagation  learning  was  tested  on  the  robot  arm  problem  to
gauge how  difficult  it  is  to obtain good generalization with standard methods.

480

Neal

\..

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  o.

.........

1.0-"',

.0060~~~.-... -... ~.~-r---~~---~

(a) .006.5 +-l,-----t--___ ==!r.:-::,===*" (b) .006.5 +--+-,--t-----+----_+_
.0000+-~~ ... -.. -... -. r .. -.. -... -.. -... -.. -... -... ~.~---~
........................
. ~5+--4,,----+-------4-~~--~
.0050 +-----'''''''"--t----==-1I_---;(cid:173)
.~.5+----~---~~~====~

.~.5+-~~ ___ --r----~---~

-~O+-r----t----~I_---~

--.....

.~o

o

50
Herations  X  1000

~

~

.oow+----~---~~---~
~

0

50
100
Iterations  X  1000

Figure  1:  Conventional  backpropagation  learning -
(a)  with  no  weight  decay,  (b)  with
carefully-chosen  weight  decay  constants.  The  solid  lines  give  the  squared  error  on  the
training data,  the dotted lines  the squared error on the  test data.

Fig.  l(a) shows  results obtained without using weight  decay.  Error on  the  test  set
declined initially, but then increased with further  training.  To achieve good results,
the point where the test error reaches its minimum would have to be identified using
a  separate validation set.
Fig.  l(b)  shows  results  using  good  weight  decay  constants,  one  for  each  category
of weights,  taken from  the Bayesian  runs  described  below.  In this  case  there  is no
need  to stop learning early,  but finding  the  proper weight  decay  constants by  non(cid:173)
Bayesian methods would be  a problem.  Again, a validation set  seems  necessary,  as
well  as  considerable computation.
Use  of a  validation set  is  wasteful,  since  data that  could otherwise  be  included  in
the  training set  must  be excluded.  Standard  techniques  for  avoiding  this,  such  as
"N-fold"  cross-validation, are  difficult to apply to neural networks.

3.2  PERFORMANCE OF BAYESIAN LEARNING

Bayesian learning was first  tested  using the unbiased  Hybrid Monte Carlo method.
The  parameter  vector  in  the  simulations  (q)  consisted  of the  unsealed  network
weights  together with  the scale  factors for  the  three  weight  categories.  The  actual
weight  vector  (w)  was  obtained  by  multiplying each  unsealed  weight  by  the  scale
factor for  its category.

Each  Hybrid  Monte  Carlo run  consisted  of 500  Metropolis  steps.  For each step,  a
trajectory  consisting  of 1000  leapfrog  iterations  with  f  =  0.00012  was  computed,
and  accepted  or  rejected  based  on  the  change  in  H  at  its  end-point.  Each  run
therefore required 500,000 batch gradient evaluations, and took approximately four
hours on  a  machine rated  at about 25  MIPS.
Fig. 2(a) shows the training and test error for the early portion of one Hybrid Monte
Carlo run.  After initially declining, these values fluctuate about an average.  Though
not  apparent  in  the  figure,  some  quantities  (notably  the  scale  factors)  require  a
hundred or more steps  to reach  their final  distribution.  The first  250  steps of each
run were  therefore  discarded  as  not being from  the stationary distribution.

Fig.  2(b) shows  the  training and test  set errors  produced  by  networks with  weight
vectors  taken from  the  last  250  steps of the same run.  Also  shown  is  the error on
the  test  set  using  the  average  of the  outputs of all  these  networks  -
that  is,  the
estimate given  by  (10)  for  the  Bayesian  prediction of (9).  For the  run shown,  this

Bayesian Learning via  Stochastic Dynamics

481

(b)  .0070 --I.----t-------1I---+--+-----II-----!(cid:173)
.0061  --li-----t---__II----t.-+-----II-t----!(cid:173)
~~~-~~-~-M~~~~--~~~~~
.0064  --Ih--;--t--t-rh-i:'lIIt-i1lr--te!H-+--~!-!1I5B-+._i:_r_-!
.0062  ~~~~-+:III'!I''HI:4---::.fII-+Iit-'-i''*''-.+-__..H*<~II'-+:~:.H!t_.;+
.0060  --I~14.~!H-;;1fH-~~fml!--lF-Hi:-:t-_i_f.;i__'i~f--i!h"lt~!tt

(a)  .0140

.0120

.01110

.1lO8O

.1lO6O

.0040

I

"'.,"; ...... ~.J! 
..J>v-A

,J ! .........  ~,.
,

" .. """'-' ~

d

o

50

100

.~.--I~~~~-~~~__Ir+~~~~~--~
.00545  --Ia---;J--=.~"*-~---I~---'--+-.!i=_--1~-~-t
.00S4  --Ift-;;,-;nr-t----:---jih---Jr:--+----r---IHt--JbT;rl(cid:173)
.0052  ~-'\+~'fliIllnl~HI\,rbItl-"'AiIA>tI-Wl~'"T1I'cyJ-y;I-fif'L'\.-tti-tf'l~
.0050 --I-lf---'--f+-'~'---'..:&.!1---'-''---F>LL.:'--....,I--...J.:....--!-

300

350

400

450

I&era&ions  X  1000

I&era&ions  x  1000
Figure 2:  Bayesian learning using Hybrid Mon~e Carlo -
(a) early portion of run,  (b) last
250  iterations.  The solid  lines  give  the squared  error on  the  training set,  the dotted lines
the  squared  error  on  the  test  set,  for  individual  networks.  The dashed  line  in  (b)  is  the
test error when  using  the average of the outputs of all  250  networks.

Figure  3:  Predictive  distribution  for
outputs.  The  two  regions  from  which
training  data was  drawn  are  outlined.
Circles indicate the true, noise-free out-
puts  for  a  grid  of cases  in  the  input
space.  The dots in the vicinity of each
circle  (often  piled on  top of it) are  the
outputs of every fifth  network from  the
last  250  iterations  of a  Hybrid  Monte
Carlo run.

+3 -

+2 -

' 

",.

+1 -

0.0-

LD  -

2.0-

u-



::,

.. ~.,
:.:~:.'.
.. '



10

1.0

0

+LD

+10

+3.0

test set error using averaged outputs is  0.00559, which is  (slightly) better than any
results  obtained  using  conventional  training.  Note  that with  Bayesian  training no
validation  set  is  necessary.  The  analogues  of the  weight  decay  constants  -
the
weight scale factors -

are found during the course of the simulation.

Another  advantage  of the  Bayesian  approach  is  that  it  can  provide  an  indication
of how  uncertain  the  predictions  for  test  cases  are.  Fig.  3  demonstrates  this.  As
one would expect,  the uncertainty  is  greater  for  test  cases  with inputs outside  the
region  where  training data was  supplied.

3.3  STOCHASTIC DYNAMICS  VS.  HYBRID  MONTE  CARLO

The uncorrected  stochastic  dynamics  method will have some degree  of systematic
bias,  due  to inexact  simulation of the  dynamics.  Is  the amount of bias introduced
of any practical importance,  however?

482

Neal

(a)  .IXY1O  ~------+---~-++-----~----~-----4-(b)

.0068
.0066
.0064
.D062
.0060
.005.
.oo5ti
.0054
.0052
.0050
.0048

250

~  400
Iterations  X  1000





..

\.  ~



21

~
\ ~

II



Iterations  X  1000

Figure  4:  Bayesian  learning  using  uncorrected  stochastic  dynamics  -
(a)  Training  and
test error for  the  last  250  iterations of a  run  with  c = 0.00012,  (b)  potential  energy  (E)
for  a  run with  c =  0.00030.  Note  the two peaks where the dynamics  became  unstable.

To  help  answer  this  question,  the  stochastic  dynamics  method  was  run  with  pa(cid:173)
rameters analogous to those used in the Hybrid Monte Carlo runs.  The step size of
( = 0.00012 used  in  those  runs was  chosen  to be as  large as  possible  while  keeping
the number of trajectories rejected  low  (about 10%).  A smaller step size  would not
give competitive results,  so this value was  used for  the stochastic dynamics runs as
well.  A  value of 0.999 for  0'  in  (16)  was  chosen  as  being (loosely) equivalent to the
use  of trajectories  1000 iterations long in  the  Hybrid  Monte  Carlo runs.

The results shown in Fig. 4(a) are comparable to those obtained using Hybrid Monte
Carlo  in  Fig.  2(b).  Fig.  4(b)  shows  that  with  a  larger  step  size  the  uncorrected
stochastic dynamics method becomes unstable.  Large step sizes also cause problems
for  the Hybrid Monte Carlo method,  however,  as  they lead to high rejection  rates.

The  Hybrid  Monte  Carlo method may  be  the  more  robust  choice  in some  circum(cid:173)
stances,  but  uncorrected  stochastic  dynamics  can  also  give  good  results.  As  it is
simpler,  the  stochastic  dynamics  method  may  be  better  for  hardware  implemen(cid:173)
tation,  and  is  a  more  plausible  starting  point  for  any  attempt  to  relate  Bayesian
methods  to  biology.  Numerous  other  variations on  these  methods  are  possible  as
well,  some of which  are discussed  in (Neal,  1992).

