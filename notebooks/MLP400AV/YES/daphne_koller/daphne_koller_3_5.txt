Abstract

We present a novel discriminative approach to parsing
inspired by the large-margin criterion underlying sup-
port vector machines. Our formulation uses a factor-
ization analogous to the standard dynamic programs for
parsing. In particular, it allows one to e(cid:14)ciently learn
a model which discriminates among the entire space of
parse trees, as opposed to reranking the top few candi-
dates. Our models can condition on arbitrary features of
input sentences, thus incorporating an important kind of
lexical information without the added algorithmic com-
plexity of modeling headedness. We provide an e(cid:14)cient
algorithm for learning such models and show experimen-
tal evidence of the models improved performance over
a natural baseline model and a lexicalized probabilistic
context-free grammar.

1

Introduction

Recent work has shown that discriminative
techniques frequently achieve classi(cid:12)cation ac-
curacy that is superior to generative techniques,
over a wide range of tasks. The empirical utility
of models such as logistic regression and sup-
port vector machines (SVMs) in (cid:13)at classi(cid:12)ca-
tion tasks like text categorization, word-sense
disambiguation, and relevance routing has been
repeatedly demonstrated. For sequence tasks
like part-of-speech tagging or named-entity ex-
traction, recent top-performing systems have
also generally been based on discriminative se-
quence models, like conditional Markov mod-
els (Toutanova et al., 2003) or conditional ran-
dom (cid:12)elds (La(cid:11)erty et al., 2001).

A number of recent papers have consid-
ered discriminative approaches for natural lan-
guage parsing (Johnson et al., 1999; Collins,
2000; Johnson, 2001; Geman and Johnson,

2002; Miyao and Tsujii, 2002; Clark and Cur-
ran, 2004; Kaplan et al., 2004; Collins, 2004).
Broadly speaking, these approaches fall into two
categories, reranking and dynamic programming
approaches.
In reranking methods (Johnson
et al., 1999; Collins, 2000; Shen et al., 2003),
an initial parser is used to generate a number
of candidate parses. A discriminative model
is then used to choose between these candi-
dates.
In dynamic programming methods, a
large number of candidate parse trees are repre-
sented compactly in a parse tree forest or chart.
Given su(cid:14)ciently \local" features, the decod-
ing and parameter estimation problems can be
solved using dynamic programming algorithms.
For example, (Johnson, 2001; Geman and John-
son, 2002; Miyao and Tsujii, 2002; Clark and
Curran, 2004; Kaplan et al., 2004) describe ap-
proaches based on conditional log-linear (max-
imum entropy) models, where variants of the
inside-outside algorithm can be used to e(cid:14)-
ciently calculate gradients of the log-likelihood
function, despite the exponential number of
trees represented by the parse forest.

In this paper, we describe a dynamic pro-
gramming approach to discriminative parsing
that is an alternative to maximum entropy
estimation. Our method extends the max-
margin approach of Taskar et al.
(2003) to
the case of context-free grammars. The present
method has several compelling advantages. Un-
like reranking methods, which consider only
a pre-pruned selection of \good" parses, our
method is an end-to-end discriminative model
over the full space of parses. This distinction
can be very signi(cid:12)cant, as the set of n-best
parses often does not contain the true parse. For

example, in the work of Collins (2000), 41% of
the correct parses were not in the candidate pool
of (cid:24)30-best parses. Unlike previous dynamic
programming approaches, which were based on
maximum entropy estimation, our method in-
corporates an articulated loss function which
penalizes larger tree discrepancies more severely
than smaller ones.1

Moreover, like perceptron-based learning, it
requires only the calculation of Viterbi trees,
rather than expectations over all trees (for ex-
ample using the inside-outside algorithm).
In
practice, it converges in many fewer iterations
than CRF-like approaches. For example, while
our approach generally converged in 20-30 iter-
ations, Clark and Curran (2004) report exper-
iments involving 479 iterations of training for
one model, and 1550 iterations for another.

The primary contribution of this paper is the
extension of the max-margin approach of Taskar
et al.
(2003) to context free grammars. We
show that this framework allows high-accuracy
parsing in cubic time by exploiting novel kinds
of lexical information.

2 Discriminative Parsing

In the discriminative parsing task, we want to
learn a function f : X ! Y, where X is a set
of sentences, and Y is a set of valid parse trees
according to a (cid:12)xed grammar G. G maps an
input x 2 X to a set of candidate parses G(x) (cid:18)
Y.2

We assume a loss function L : X (cid:2) Y (cid:2)
Y ! R+. The function L(x; y; ^y) measures the
penalty for proposing the parse ^y for x when y
is the true parse. This penalty may be de(cid:12)ned,
for example, as the number of labeled spans on
which the two trees do not agree. In general we
assume that L(x; y; ^y) = 0 for y = ^y. Given
labeled training examples (xi; yi) for i = 1 : : : n,
we seek a function f with small expected loss
on unseen sentences.

The functions we consider take the following

linear discriminant form:

fw(x) = arg max
y2G(x)

hw; (cid:8)(x; y)i;

where h(cid:1); (cid:1)i denotes the vector inner product,
w 2 Rd and (cid:8) is a feature-vector representation
of a parse tree (cid:8) : X (cid:2) Y ! Rd (see examples
below).3

Note that this class of

functions includes
Viterbi PCFG parsers, where the feature-vector
consists of the counts of the productions used
in the parse, and the parameters w are the log-
probabilities of those productions.

2.1 Probabilistic Estimation

The traditional method of estimating the pa-
rameters of PCFGs assumes a generative gram-
mar that de(cid:12)nes P (x; y) and maximizes the
joint log-likelihood Pi log P (xi; yi) (with some
regularization).
A alternative probabilistic
approach is to estimate the parameters dis-
criminatively by maximizing conditional
log-
likelihood. For example, the maximum entropy
approach (Johnson, 2001) de(cid:12)nes a conditional
log-linear model:

Pw(y j x) =

1

Zw(x)

expfhw; (cid:8)(x; y)ig;

where Zw(x) = Py2G(x) expfhw; (cid:8)(x; y)ig, and
maximizes the conditional log-likelihood of the
sample, Pi log P (yi j xi), (with some regular-
ization).

2.2 Max-Margin Estimation

In this paper, we advocate a di(cid:11)erent estima-
tion criterion, inspired by the max-margin prin-
ciple of SVMs. Max-margin estimation has been
used for parse reranking (Collins, 2000). Re-
cently, it has also been extended to graphical
models (Taskar et al., 2003; Altun et al., 2003)
and shown to outperform the standard max-
likelihood methods. The main idea is to forego
the probabilistic interpretation, and directly en-
sure that

yi = arg max

y2G(xi)

hw; (cid:8)(xi; y)i;

for all i in the training data. We de(cid:12)ne the
margin of the parameters w on the example i
and parse y as the di(cid:11)erence in value between
the true parse yi and y:

1This articulated loss is supported by empirical suc-
cess and theoretical generalization bound in Taskar et al.
(2003).

2For all x, we assume here that G(x) is (cid:12)nite. The
space of parse trees over many grammars is naturally in-
(cid:12)nite, but can be made (cid:12)nite if we disallow unary chains
and empty productions.

hw; (cid:8)(xi; yi)i (cid:0) hw; (cid:8)(xi; y)i = hw; (cid:8)i;yi (cid:0) (cid:8)i;yi;

3Note that in the case that two members y1 and y2
have the same tied value for hw; (cid:8)(x; y)i, we assume that
there is some (cid:12)xed, deterministic way for breaking ties.
For example, one approach would be to assume some
default ordering on the members of Y.

where (cid:8)i;y = (cid:8)(xi; y), and (cid:8)i;yi = (cid:8)(xi; yi). In-
tuitively, the size of the margin quanti(cid:12)es the
con(cid:12)dence in rejecting the mistaken parse y us-
ing the function fw(x), modulo the scale of the
parameters jjwjj. We would like this rejection
con(cid:12)dence to be larger when the mistake y is
more severe, i.e. L(xi; yi; y) is large. We can ex-
press this desideratum as an optimization prob-
lem:

max (cid:13)
s:t: hw; (cid:8)i;yi (cid:0) (cid:8)i;yi (cid:21) (cid:13)Li;y 8i; y 2 G(xi);

(1)

jjwjj2 (cid:20) 1;

where Li;y = L(xi; yi; y). This quadratic pro-
gram aims to separate each y 2 G(xi) from
the target parse yi by a margin that is propor-
tional to the loss L(xi; yi; y). After a standard
transformation, in which maximizing the mar-
gin is reformulated as minimizing the scale of
the weights (for a (cid:12)xed margin of 1), we get the
following program:

min

1
2

kwk2 + C X

(cid:24)i

i

(2)

s:t: hw; (cid:8)i;yi (cid:0) (cid:8)i;yi (cid:21) Li;y (cid:0) (cid:24)i 8i; y 2 G(xi):

The addition of non-negative slack variables (cid:24)i
allows one to increase the global margin by pay-
ing a local penalty on some outlying examples.
The constant C dictates the desired trade-o(cid:11)
between margin size and outliers. Note that this
formulation has an exponential number of con-
straints, one for each possible parse y for each
sentence i. We address this issue in section 4.

2.3 The Max-Margin Dual
In SVMs, the optimization problem is solved by
working with the dual of a quadratic program
analogous to Eq. 2. For our problem, just as for
SVMs, the dual has important computational
advantages, including the \kernel trick," which
allows the e(cid:14)cient use of high-dimensional fea-
tures spaces endowed with e(cid:14)cient dot products
(Cristianini and Shawe-Taylor, 2000). More-
over, the dual view plays a crucial role in cir-
cumventing the exponential size of the primal
problem.

In Eq. 2, there is a constraint for each mistake
y one might make on each example i, which rules
out that mistake. For each mistake-exclusion
constraint, the dual contains a variable (cid:11)i;y. In-
tuitively, the magnitude of (cid:11)i;y is proportional
to the attention we must pay to that mistake in
order not to make it.

The dual of Eq. 2 (after renormalizing by C)

is given by:

max C X
i;y

(cid:11)i;yLi;y (cid:0)

1
2

(Ii;y (cid:0) (cid:11)i;y)(cid:8)i;y

C X
i;y

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

s:t: X
y

(cid:11)i;y = 1;

8i; (cid:11)i;y (cid:21) 0; 8i; y;

(3)

where Ii;y = I(xi; yi; y) indicates whether y is
the true parse yi. Given the dual solution (cid:11)(cid:3),
the solution to the primal problem w(cid:3) is sim-
ply a weighted linear combination of the feature
vectors of the correct parse and mistaken parses:

w(cid:3) = C X
i;y

(Ii;y (cid:0) (cid:11)(cid:3)

i;y)(cid:8)i;y:

This is the precise sense in which mistakes with
large (cid:11) contribute more strongly to the model.

3 Factored Models

There is a major problem with both the pri-
mal and the dual formulations above: since each
potential mistake must be ruled out, the num-
ber of variables or constraints is proportional to
jG(x)j, the number of possible parse trees. Even
in grammars without unary chains or empty el-
ements, the number of parses is generally ex-
ponential in the length of the sentence, so we
cannot expect to solve the above problem with-
out any assumptions about the feature-vector
representation (cid:8) and loss function L.

For that matter,

for arbitrary representa-
tions, to (cid:12)nd the best parse given a weight vec-
tor, we would have no choice but to enumerate
all trees and score them. However, our gram-
mars and representations are generally struc-
tured to enable e(cid:14)cient inference. For exam-
ple, we usually assign scores to local parts of
the parse such as PCFG productions. Such
factored models have shared substructure prop-
erties which permit dynamic programming de-
compositions. In this section, we describe how
this kind of decomposition can be done over the
dual (cid:11) distributions. The idea of this decom-
position has previously been used for sequences
and other Markov random (cid:12)elds in Taskar et
al. (2003), but the present extension to CFGs
is novel.

For clarity of presentation, we restrict the
grammar to be in Chomsky normal form (CNF),
where all rules in the grammar are of the form
hA ! B Ci or hA ! ai, where A; B and C are
non-terminal symbols, and a is some terminal

S

NP

VP

DT

NN

VBD

NP

The

screen

was

NP

PP

DT

NN

IN

NP

a

sea

of

NN

red

0

1

2

3

4

5

6

q = hS ! NP VP; 0; 2; 7i

DT

NP

NN

VBD

S

VP

DT

NP

NN

r = hNP; 3; 5i

IN

PP

NN

NP

0 1 2

3 4 5 6 7

(a)

(b)

Figure 1: Two representations of a binary parse tree: (a) nested tree structure, and (b) grid of labeled spans.

symbol. For example (cid:12)gure 1(a) shows a tree
in this form.

We will represent each parse as a set of two
types of parts. Parts of the (cid:12)rst type are sin-
gle constituent tuples hA; s; e; ii, consisting of
a non-terminal A, start-point s and end-point
e, and sentence i, such as r in (cid:12)gure 1(b). In
this representation, indices s and e refer to po-
sitions between words, rather than to words
themselves. These parts correspond to the tra-
ditional notion of an edge in a tabular parser.

Parts of the second type consist of CF-rule-
tuples hA ! B C; s; m; e; ii. The tuple speci(cid:12)es
a particular rule A ! B C, and its position,
including split point m, within the sentence i,
such as q in (cid:12)gure 1(b), and corresponds to the
traditional notion of a traversal in a tabular
parser. Note that parts for a basic PCFG model
are not just rewrites (which can occur multiple
times), but rather anchored items.

Formally, we assume some countable set of
parts, R. We also assume a function R which
maps each object (x; y) 2 X (cid:2) Y to a (cid:12)nite
subset of R. Thus R(x; y) is the set of parts be-
longing to a particular parse. Equivalently, the
function R(x; y) maps a derivation y to the set
of parts which it includes. Because all rules are
in binary-branching form, jR(x; y)j is constant
across di(cid:11)erent derivations y for the same input
sentence x. We assume that the feature vector
for a sentence and parse tree (x; y) decomposes

into a sum of the feature vectors for its parts:

(cid:8)(x; y) = X

(cid:30)(x; r):

r2R(x;y)

In CFGs, the function (cid:30)(x; r) can be any func-
tion mapping a rule production and its posi-
tion in the sentence x, to some feature vector
representation. For example, (cid:30) could include
features which identify the rule used in the pro-
duction, or features which track the rule iden-
tity together with features of the words at po-
sitions s; m; e, and neighboring positions in the
sentence x.

In addition, we assume that the loss function
L(x; y; ^y) also decomposes into a sum of local
loss functions l(x; y; r) over parts, as follows:

L(x; y; ^y) = X

l(x; y; r):

r2R(x;^y)

One approach would be to de(cid:12)ne l(x; y; r) to
be 0 only if the non-terminal A spans words
s : : : e in the derivation y and 1 otherwise. This
would lead to L(x; y; ^y) tracking the number of
\constituent errors" in ^y, where a constituent is
a tuple such as hA; s; e; ii. Another, more strict
de(cid:12)nition would be to de(cid:12)ne l(x; y; r) to be 0
if r of the type hA ! B C; s; m; e; ii is in the
derivation y and 1 otherwise. This de(cid:12)nition
would lead to L(x; y; ^y) being the number of CF-

rule-tuples in ^y which are not seen in y.4

Finally, we de(cid:12)ne indicator variables I(x; y; r)
which are 1 if r 2 R(x; y), 0 otherwise. We
also de(cid:12)ne sets R(xi) = [y2G(xi)R(xi; y) for the
training examples i = 1 : : : n. Thus, R(xi) is
the set of parts that is seen in at least one of
the objects f(xi; y) : y 2 G(xi)g.

4 Factored Dual

The dual in Eq. 3 involves variables (cid:11)i;y for
all i = 1 : : : n, y 2 G(xi), and the objec-
tive is quadratic in these (cid:11) variables. In addi-
tion, it turns out that the set of dual variables
(cid:11)i = f(cid:11)i;y : y 2 G(xi)g for each example i is
constrained to be non-negative and sum to 1.
It is interesting that, while the parameters w
lose their probabilistic interpretation, the dual
variables (cid:11)i for each sentence actually form a
kind of probability distribution. Furthermore,
the objective can be expressed in terms of ex-
pectations with respect to these distributions:

C X
i

E(cid:11)i [Li;y] (cid:0)

1
2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

C X
i

(cid:8)i;yi (cid:0) E(cid:11)i [(cid:8)i;y]

:

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

We now consider how to e(cid:14)ciently solve
the max-margin optimization problem for a
factored model. As shown in Taskar et al.
(2003), the dual in Eq. 3 can be reframed using
\marginal" terms. We will also (cid:12)nd it useful to
consider this alternative formulation of the dual.
Given dual variables (cid:11), we de(cid:12)ne the marginals
(cid:22)i;r((cid:11)) for all i; r, as follows:

(cid:22)i;r((cid:11)i) = X
y

(cid:11)i;yI(xi; y; r) = E(cid:11)i [I(xi; y; r)] :

Since the dual variables (cid:11)i form probability dis-
tributions over parse trees for each sentence i,
the marginals (cid:22)i;r((cid:11)i) represent the proportion
of parses that would contain part r if they were
drawn from a distribution (cid:11)i. Note that the
number of such marginal terms is the number
of parts, which is polynomial in the length of
the sentence.

Now consider the dual objective Q((cid:11)) in
Eq. 3.
It can be shown that the original ob-
jective Q((cid:11)) can be expressed in terms of these

4The constituent loss function does not exactly cor-
respond to the standard scoring metrics, such as F1 or
crossing brackets, but shares the sensitivity to the num-
ber of di(cid:11)erences between trees. We have not thoroughly
investigated the exact interplay between the various loss
choices and the various parsing metrics. We used the
constituent loss in our experiments.

marginals as Qm((cid:22)((cid:11))), where (cid:22)((cid:11)) is the vector
with components (cid:22)i;r((cid:11)i), and Qm((cid:22)) is de(cid:12)ned
as:

C X
i;r2R(xi)

(cid:22)i;rli;r (cid:0)

1
2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

C X

(Ii;r (cid:0) (cid:22)i;r)(cid:30)i;r

i;r2R(xi)

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

where li;r = l(xi; yi; r), (cid:30)i;r = (cid:30)(xi; r) and Ii;r =
I(xi; yi; r).

This follows from substituting the factored
de(cid:12)nitions of the feature representation (cid:8) and
loss function L together with de(cid:12)nition of
marginals.

Having expressed the objective in terms of a
polynomial number of variables, we now turn to
the constraints on these variables. The feasible
set for (cid:11) is

(cid:1) = f(cid:11) : (cid:11)i;y (cid:21) 0; 8i; y X
y

(cid:11)i;y = 1;

8ig:

Now let (cid:1)m be the space of marginal vectors
which are feasible:

(cid:1)m = f(cid:22) : 9(cid:11) 2 (cid:1) s:t: (cid:22) = (cid:22)((cid:11))g:

Then our original optimization problem can be
reframed as max(cid:22)2(cid:1)m Qm((cid:22)).

Fortunately, in case of PCFGs, the domain
(cid:1)m can be described compactly with a polyno-
mial number of linear constraints. Essentially,
we need to enforce the condition that the ex-
pected proportions of parses having particular
parts should be consistent with each other. Our
marginals track constituent parts hA; s; e; ii and
CF-rule-tuple parts hA ! B C; s; m; e; ii The
consistency constraints are precisely the inside-
outside probability relations:

(cid:22)i;A;s;e = X

(cid:22)i;A!B C;s;m;e

B;C

s<m<e

and

(cid:22)i;A;s;e = X

(cid:22)i;B!A C;s;m;e + X

(cid:22)i;B!C A;s;m;e

B;C

e<m(cid:20)ni

B;C

0(cid:20)m<s

where ni is the length of the sentence. In ad-
dition, we must ensure non-negativity and nor-
malization to 1:

(cid:22)i;r (cid:21) 0; X
A

(cid:22)i;A;0;ni = 1:

The number of variables in our factored dual
for CFGs is cubic in the length of the sentence,

P

Model
87.70
GENERATIVE
87.51
BASIC
LEXICAL
88.15
LEXICAL+AUX 89.74

R

88.06
88.44
88.62
90.22

F1

87.88
87.98
88.39
89.98

Figure 2: Development set results of the various
models when trained and tested on Penn treebank
sentences of length (cid:20) 15.

P

Model
88.25
GENERATIVE
88.08
BASIC
LEXICAL
88.55
LEXICAL+AUX 89.14
COLLINS 99
89.18

R

F1

87.99
87.73
88.20
88.31
88.34
88.44
89.10 89.12
88.20
88.69

Figure 3: Test set results of the various models when
trained and tested on Penn treebank sentences of
length (cid:20) 15.

while the number of constraints is quadratic.
This polynomial size formulation should be con-
trasted with the earlier formulation in Collins
(2004), which has an exponential number of
constraints.

5 Factored SMO

We have reduced the problem to a polynomial
size QP, which, in principle, can be solved us-
ing standard QP toolkits. However, although
the number of variables and constraints in the
factored dual is polynomial in the size of the
data, the number of coe(cid:14)cients in the quadratic
term in the objective is very large: quadratic in
the number of sentences and dependent on the
sixth power of sentence length. Hence, in our
experiments we use an online coordinate descent
method analogous to the sequential minimal op-
timization (SMO) used for SVMs (Platt, 1999)
and adapted to structured max-margin estima-
tion in Taskar et al. (2003).

We omit the details of the structured SMO
procedure, but the important fact about this
kind of training is that, similar to the basic per-
ceptron approach, it only requires picking up
sentences one at a time, checking what the best
parse is according to the current primal and
dual weights, and adjusting the weights.

6 Results

We used the Penn English Treebank for all of
our experiments. We report results here for

each model and setting trained and tested on
only the sentences of length (cid:20) 15 words. Aside
from the length restriction, we used the stan-
dard splits: sections 2-21 for training (9753 sen-
tences), 22 for development (603 sentences), and
23 for (cid:12)nal testing (421 sentences).

As a baseline, we trained a CNF transforma-
tion of the unlexicalized model of Klein and
Manning (2003) on this data. The resulting
grammar had 3975 non-terminal symbols and
contained two kinds of productions: binary non-
terminal rewrites and tag-word rewrites.5 The
scores for the binary rewrites were estimated us-
ing unsmoothed relative frequency estimators.
The tagging rewrites were estimated with a
smoothed model of P (wjt), also using the model
from Klein and Manning (2003). Figure 3 shows
the performance of this model (generative):
87.99 F1 on the test set.

For the basic max-margin model, we used
exactly the same set of allowed rewrites (and
therefore the same set of candidate parses) as in
the generative case, but estimated their weights
according to the discriminative method of sec-
tion 4. Tag-word production weights were (cid:12)xed
to be the log of the generative P (wjt) model.
That is, the only change between genera-
tive and basic is the use of the discriminative
maximum-margin criterion in place of the gen-
erative maximum likelihood one. This change
alone results in a small improvement (88.20 vs.
87.99 F1).

On top of the basic model, we (cid:12)rst added lex-
ical features of each span; this gave a lexical
model. For a span hs; ei of a sentence x, the
base lexical features were:

(cid:15) xs, the (cid:12)rst word in the span
(cid:15) xs(cid:0)1, the preceding adjacent word
(cid:15) xe(cid:0)1, the last word in the span
(cid:15) xe, the following adjacent word
(cid:15) hxs(cid:0)1; xsi
(cid:15) hxe(cid:0)1; xei
(cid:15) xs+1 for spans of length 3

These base features were conjoined with the
span length for spans of length 3 and below,
since short spans have highly distinct behaviors
(see the examples below). The features are lex-
ical in the sense than they allow speci(cid:12)c words

5Unary rewrites were compiled into a single com-
pound symbol, so for example a subject-gapped sentence
would have label like s+vp. These symbols were ex-
panded back into their source unary chain before parses
were evaluated.

and word pairs to in(cid:13)uence the parse scores, but
are distinct from traditional lexical features in
several ways. First, there is no notion of head-
word here, nor is there any modeling of word-to-
word attachment. Rather, these features pick
up on lexical trends in constituent boundaries,
for example the trend that in the sentence The
screen was a sea of red., the (length 2) span
between the word was and the word of
is un-
likely to be a constituent. These non-head lex-
ical features capture a potentially very di(cid:11)er-
ent source of constraint on tree structures than
head-argument pairs, one having to do more
