Abstract

Two of the most important threads of work in knowledge
representation today are frame-based representation systems
(FRSs) and Bayesian networks (BNs). FRSs provide an ex-
cellent representation for the organizational structure of large
complex domains, but their applicability is limited because of
their inability to deal with uncertainty and noise. BNs pro-
vide an intuitive and coherent probabilistic representation of
our uncertainty, but are very limited in their ability to handle
complex structured domains. In this paper, we provide a lan-
guage that cleanly integrates these approaches, preserving the
advantages of both. Our approach allows us to provide natural
and compact denitions of probability models for a class, in
a way that is local to the class frame. These models can be
instantiated for any set of interconnected instances, resulting
in a coherent probability distribution over the instance proper-
ties. Our language also allows us to represent important types
of uncertainty that cannot be accomodated within the frame-
work of traditional BNs: uncertainty over the set of entities
present in our model, and uncertainty about the relationships
between these entities. We provide an inference algorithm for
our language via a reduction to inference in standard Bayesian
networks. We describe an implemented system that allows
most of the main frame systems in existence today to annotate
their knowledge bases with probabilistic information, and to
use that information in answering probabilistic queries.

1

Introduction

Frame representation systems (FRSs) are currently the pri-
mary technology used for large scale knowledge representa-
tion in AI [8, 3, 7]. Their modular organization according
to cognitively meaningful entities and their ability to capture
patterns common to many individuals provide a convenient
language for representing complex structured domain mod-
els. One of the most signicant gaps in the expressive power
of this type of framework is its inability to represent and
reason with uncertain and noisy information. Uncertainty is
unavoidable in the real world: our information is often inac-
curate and always incomplete, and only a few of the rules
that we use for reasoning are true in all possible cases.

In the propositional setting, this problem has largely
been resolved over the past decade by the development of

Copyright 1998, American Association for Articial Intelli-

gence (www.aaai.org). All rights reserved.

probabilistic reasoning systems, and particularly Bayesian
networks [10]. A Bayesian network (BN) is a representation
of a full joint distribution over a set of random variables; it
can be used to answer queries about any of its variables given
any evidence. A BN allows a complex distribution to be
represented compactly by using the locality of inuence in
our model of the world. But, like all propositional systems,
the applicability of BNs is largely limited to situations that
can be encoded, in advance, using a xed set of attributes.
Thus, they are inadequate for large-scale complex KR tasks.
Building on our recent work [6, 5], we propose a represen-
tation language that integrates frame-representation systems
and Bayesian networks, thereby providing the rst bridge
between these two very different threads of work in KR. The
key component in our representation is the annotation of a
frame with a probability model. This probability model is,
broadly speaking, a BN representing a distribution over the
possible values of the slots in the frame. That is, each sim-
ple slot in the frame is annotated with a local probability
model, representing the dependence of its value on the val-
ues of related slots. For example, in a frame representing a
PhD student, the value of the slot years-to-graduation may
depend on the slot year and the slot chain advisor.picky.

As we can see even from this simple example, by build-
ing on standard FRS functionality, our approach provides
signicantly more expressive power than traditional BNs.
For example, by allowing the probability model of a slot to
depend on a slot chain, we allow the properties of one in-
stance in the model to depend on properties of other related
instances. We can also use the standard class hierarchy of
the FRS to allow the probability model of a class to be used
by multiple instances of that class, and to allow inheritance
of probability models from classes to subclasses, using the
same mechanism in which slot values are currently inherited.
Finally, by making domain individuals rst-class citizens in
our framework we can also express a new and important type
of uncertainty called structural uncertainty. We can have a
probabilistic model expressing our uncertainty about the set
of entities in our model, e.g., the number of PhD students
in a department. We can also represent uncertainty about
relations between entities, e.g., which of several conferences
a paper appeared in.

We provide a probabilistic inference algorithm for our
language based on an approach known as knowledge-based

model construction. The algorithm takes a knowledge base
in our language, including a set of instances, and generates
a standard BN which can then be queried effectively for our
beliefs about the value of any slots.

Our probability model is expressed using standard frame
representation techniques such as facets and value restric-
tions. This property is important, since it allows our approach
to be used with virtually any frame system, and thereby to
annotate existing KBs with probabilistic information.
In
particular, we have implemented a system based on our ap-
proach, capable of interacting with most existing FRSs via
OKBC [2], an emerging standard for FRS interoperability.

Our work is a signcant improvement over previous ap-
proaches to combining rst-order logic and Bayesian net-
works. Most of the attempts in this direction (e.g., [12, 11, 9])
use probabilistic Horn clauses as the basic representation.
The choice of Horn clauses as an underlying language al-
ready dictates some of the properties of the representation,
e.g., its inability to encapsulate an object and its properties
within a cognitively meaningful frame. Moreover, the use
of structural uncertainty in this framework typically causes
combinatorial blowup of the resulting models, leading most
approaches to outlaw it entirely. Our framework also over-
comes some major limitations of our earlier proposals [6, 5],
by allowing both structural uncertainty (absent in the rst)
and probabilistic dependencies between instances (absent in
the second).
It also provides the crucial ability, absent in
both, to create complex models containing many instances
that are connected to each other in a variety of ways.

2 Basic representation

We begin with some basic terminology for frame systems.
The terminology varies widely from system to system.
In
this paper we adopt the language and basic knowledge model
of the OKBC protocol [2].

The basic unit of discourse in a frame system is a frame. A
frame has a set of slots, each of which may have slot values or
llers. Formally, a slot represents a binary relation on frames;
if the ller of slot A in frame X is frame Y , then the relation
AX; Y  holds.
In general slots may be single-valued or
multi-valued. In this section we assume that slots are single-
valued. This assumption will be relaxed in Section 4. A
slot-chain is a sequence of zero or more slots separated by
periods. A slot-chain represents a binary relation: the slot-
chain A: where A is a slot and  is a slot-chain denotes the
relation fX; Z j AX; Y  ^ Y; Zg. A slot in a frame
may have associated facets. A facet is a ternary relation: if
the facet value of facet F on slot A in frame X is Y , then
the relation F X; A; Y  holds. A standard facet is value-type,
which species a value restriction on the values of a slot. The
value-type of a slot will be called its type.

The two main types of frames are class frames, represent-
ing sets of entities, and instance frames. The class frames
are organized in an is-a hierarchy, where one class may be
a subclass of another (its superclass). The slots of a class
frame may be own slots, which describe a property of the
class itself, and template slots, which are slots inherited by
all instances and subclasses of the class. The facets asso-
ciated with template slots are template facets, and are also

inherited. An instance or subclass may override the values
of inherited slots or facets.

Probabilistic information is incorporated into a frame KB
by annotating class frames with local probabilisticmodels. A
class frame that has been so annotated is called a p-class. A
p-class has a set of template slots, each with a value-type facet.
Depending on the type, a slot is either simple or complex. The
type of a complex slot is another p-class. The type of a simple
slot is an explicitly enumerated list of possible values for the
slot. For example, the phd-student p-class may have a simple
slot year, whose type is f1st, 2nd, 3rd, 4th6th, tenuredg, and
a complex slot advisor whose type is the p-class professor. A
p-class may also have other slots that do not participate in the
probability model, whose type is neither of the above. For
example, phd-student may also have the slot name, which does
not have an associated probability model. This feature allows
existing KBs to be annotated with probabilistic information,
without requiring a complete redesign of the ontology.

A simple slot is very much like a node in a Bayes net.
It has a range of values, a set of parents, and a CPT. A p-
class species a probability model for its simple slots using
two special-purpose facets: parents and distribution. Facets
are a natural place to put a probability model, since such a
model can be viewed as a generalization of a value restric-
tion: not only does it specify a range of possible values, but
also a distribution over that range. The parents facet lists the
slots on which the value of this slot depends. Each parent
is specied by a slot-chain referring to some other simple
slot. More precisely, let X be a p-class and A a simple slot.
The parents facet of A is a list of slot chains 1; : : : ; n,
such that X:i refers to a simple slot. For example, in the
phd-student p-class, year may have the parent [age], while the
parents of years-to-graduation may be year; advisor.picky.
The distribution facet species the conditional probability dis-
tribution over values of the slot given values of its parents.
The conditional distribution is specied using a conditional
probability table (CPT) as in Bayesian networks. For each
combination of values of its parents, the CPT provides a
probability distribution over values of the slot. For the pur-
poses of this paper, we assume that the CPTs are represented
as fully specied functions of parent values. More compact
representations such as noisy-or can easily be accomodated
within our framework.

The probability model of a complex slot is simply de-
scribed by its p-class Y . However, each complex slot A also
has an additional facet called imports, whose value is a list of
slots in Y . This list, called the import list of A, is the list of
slots of Y that are visible within X. We require that if A:B:
(for a possibly empty slot chain ) is a slot chain appearing
within X, then B must be in the import list of A.

Once a probability model has been specied for a p-class,
the p-class can be used just like any other class frame. One
can create instances of the class, which will inherit all of
its template slots and facets.
In particular, the probability
distribution over values of slots of the instance will be as de-
scribed in the p-class. Similarly, the inheritance mechanism
of a frame system can be used to make one p-class a subclass
of another. A subclass can extend the denition of the super-
class as well as overwrite parts of it. In particular, a subclass

can redene the probability model of one or more of the slots.
For example, we can dene associate-professorto be a subclass
of professor, and overwrite the distribution over salary to one
that is appropriate to the more specic class. Another im-
portant aspect of subtyping is that an instance of a subclass
is also an instance of the superclass, so that it can ll a slot
whose type is the superclass. For example, in a particular
instance of phd-student, the value of the advisor slot may be
specied to be an instance whose class is associate-professor.
Values can be assigned to an own slot of an instance frame
either directly or by assignment to a template slot at the
class level. Both types of assignments are interpreted in the
same way. An assignment to a simple slot is interpreted
as observing the value of the slot, thereby conditioning the
probability distribution for the instance. This conditioning
process may result in a change in our beliefs for other related
slots. Consider, for example, a subclass graduating-phd-student
of phd-student which assigns 1 to the slot years-to-graduation.
Then the conditioning process will result in a new probability
model for any instance I of this subclass; in particular, our
beliefs about I:year and I:advisor.picky will both change, as
will our beliefs about other related slots.

An assignment to a complex slot species that the value of
that slot is another particular instance. Thus, complex net-
works of inter-related frames can be created, such as students
who share an advisor, and students of different advisors in
the same department. Such an assignment at the class level
results in all of the class instances having the same frame as
their value for that slot.

One of the features of a probabilistic frame system is that
related frames can inuence each other. We have already
seen one mechanism for such interactions: since a parent
of a slot is a slot-chain, the value of a simple slot may be
inuenced probabilistically by the value of a slot in another
frame. This mechanism, however, only allows a frame to
be inuenced by related frames, but not to inuence them in
turn. We resolve this difculty by utilizing a basic feature of
most FRSsinverse slots.

Let X and Y be two class frames, A a slot of X with
type Y , and B a slot of Y with type X. Then A and B are
inverse slots if, for every instance I of X, if I:A = J then
J:B = I, and vice versa. Thus, we view an assignment of a
specic instance frame J to a slot I:A as encompassing the
corresponding assignment of I to J:B. For that reason, we
do not allow assignments of values to slots such as A at the
class level; otherwise, for any given frame J of class Y , the
value of J:B would be the set consisting of every frame of
class X, a model which is too unwieldy to deal with.

Inverse slots allow either of the frames to refer to slots in
the other, thereby allowing probabilistic dependencies in both
directions. Allowing such intertwined dependencies without
restriction could lead to horribly complex interactions be-
tween two frames.
In particular, it could lead to a cyclic
chain of inuences that has no coherent probability model.
Therefore, one of the two inverse slotssay X:Ais desig-
nated to be the primary direction while the otherY:Bis
secondary. Similarly, X is called the primary frame, while
Y is the secondary frame. A primary inverse slot such as A
in X has a parents facet just like a simple slot, i.e., it is a list

of slot-chains in X. Intuitively, the parents of A are the only
slots of X that are exported to Y via B. More precisely, the
parent list of A in X must be identical to the import list of
B in Y . Thus, the ow of inuence between the two frames
is neatly regulated: The parents of A in X can inuence any
of the slots in Y ; some of those can, in turn, inuence other
slots in X that are downstream from A.

For example, suppose we decide that the thesis slot of
phd-student should be an inverse slot, with its inverse being
the author slot of phd-thesis. The slot thesis is designated
to be primary, and is given the parent eld in phd-student.
Then eld is visible within the phd-thesis class, so that for
example, jargon-content may depend on author.eld. Other
slots of phd-student may depend on slots of thesis; thus, for
example, job-prospects may depend on thesis.quality, which
would therefore have to be on the import list of thesis.

Inverse slots serve a dual purpose in our language. As
we said, they allow bidirectional dependencies between two
instances. But they also allow our probabilistic models to be
multicentered. If, as above, X:A and Y:B are inverses, and
we dene an instance from class X, it immediately implies
the existence of a corresponding instance from class Y . Al-
ternatively, we could start modeling with an object of class
Y , and guarantee that the corresponding X will exist. Thus,
we can dene a model centered around whatever entities are
of interest to us in our context.

3 Semantics

In this section we present a semantics for probabilistic frame
knowledge bases. For a given KB with a set of class and
instance frames, our semantics denes a probability distribu-
tion over the slot values of the instance frames (and of some
other related instance frames). In order to dene a coherent
probability distribution, our frame KB must satisfy several
conditions. The basic theme of the conditions is familiar from
the realm of Bayesian networks: our dependency model must
be acyclic. However, since there may be complicated chains
of dependencies both within a frame and between frames,
and on both the class and instance levels, we need to develop
some tools to reason about dependencies.
Denition 3.1: A dependency is a pair X:A  Y:B, where
X and Y are frames (not necessarily distinct), A is a slot of
X and B is a slot of Y . We say that X:A  Y:B holds if
 A is a simple slot of X, Y = X and B: is a parent of A;
 A is a complex slot of X, B is in the import list of A,
and Y is either an instance frame assigned to X:A or the
p-class frame which is the value type of X:A.
Intuitively, a dependency X:A  Y:B asserts that for
every instance frame I consistent with X there exists an
instance frame J consistent with Y such that I:A depends
on J:B.
(If X is itself an instance frame I, then only I
is consistent with X; if X is a class, then any instance of
that class is consistent with X.) Note however, that our
denition of dependencies only considers the rst slot in a
chain on which a slot depends; thus, it makes only a rst-level
partition of dependency. It is a conservative overestimate of
the true dependency model, since if X:A  Y:B, it is not
necessarily the case that X:A depends on every slot-chain

Y:B:. While it is fairly straightforward to rene our notion
of dependency, we have found our denition to be adequate
for most purposes.
Denition 3.2 : A dependency chain is a list X1:A1 
X2:A2     such that, for each i, Xi:Ai  Xi+1:Ai+1.
A dependency cycle is a dependency chain that begins and
ends with the same slot.

Dependency cycles reect potential problems with our
model.
(Although, as indicated by our discussion above,
some correct models may appear to be problematic sim-
ply because of our overestimate for probabilistic dependen-
cies.) A dependency cycle containing I:A, where I is an
instance frame, corresponds to a possible chain of depen-
dencies through which I:A depends on itself. Such a cyclic
dependency, if it exists, prevents us from dening a coherent
probability model. A dependency cycle containing X:A for
some class X means that for every instance I1 of X there
is some instance I2 of X such that I1:A depends on I2:A.
In some cases, I1 and I2 are necessarily the same instance;
such cases are called truly cyclic. In others, however, they
are distinct instances of the class X. These cases can also
be problematic, as they may represent an innite dependency
chain beginning with I1:A: I1:A depends on I2:A which de-
pends on some I3:A, etc. Such models also do not typically
have well-dened probabilistic semantics.

We conclude from this discussion that we want to disallow
all dependency cycles.1 Some types of dependency cycles
are easy to prevent using purely local considerations. Specif-
ically, we can build, for each class X, a dependency graph
for X. This graph contains all the slots of X, with an edge
from B to A if the dependency X:A  X:B holds. Clearly,
if we want to avoid dependency cycles, this graph should be
acyclic. Indeed, our care in designing the dependency model
for inverse slots implies that if we make all class dependency
graphs acyclic, we avoid any truly cyclic dependency chains
at the class level. Formally, if we dene a class-level depen-
dency chain to be one in which all the Xis are p-classes, we
obtain the following theorem:
Theorem 3.3: If we have a knowledge base in which all class
dependency graphs are acyclic, then there are no truly cyclic
class-level dependency chains.

However, as we discussed, even dependency chains that are
not truly cyclic can result in incoherent models. In addition,
we have not eliminated instance-level dependency chains that
are truly cyclic. Unfortunately, the general problem is not
so easy to prevent using purely local constraints. However,
we can detect whether or not the KB contains a dependency
cycle by building a more global directed graph G, called the
dependency graph of the KB. The nodes of G are all X:A
where X is a p-class or named individualframe and A is a slot
of X. There is an edge from Y:B to X:A if the dependency
X:A  Y:B holds. Clearly, the KB contains a dependency
cycle iff G is cyclic.

For a KB that contains no dependency cycles, our goal
now is to dene a probability distribution over instantiations

1Note that we are not disallowing innite reference chains
(chains of related instances), unless they imply innite dependency
chains.

to frames, i.e., assignments of values to the slots of the frames.
Several issues combine to make such a denition difcult.

The most obvious idea is to follow the approach taken in
the semantics of Bayesian networks: we determine a set of
random variables, and dene a distribution over their joint
value space. Unfortunately, our framework is too rich to
make this approach appropriate. As we mentioned, the set
of instance frames that we can potentially refer to may be
innite. While one might be able to circumvent this partic-
ular problem, a more serious one manifests when we enrich
our language with structural uncertainty in Sections 4 and 5.
Then, the set of instance frames can also vary probabilisti-
cally, in a way that both depends on and inuences the values
of other random variables in the model.

We therefore dene our semantics via a data generating
process, that randomly samples values for the various frames
in the model. The random sampling process implicitly de-
nes a distribution over the different possible value assign-
ments: the probability of a value assignment is the probability
with which it is generated by the process. Note that, although
a random sampling process can also be used as a stochastic
algorithm for approximate inference, we are not proposing
this approach; our sampling process is purely a thought ex-
periment for dening the distribution. In Section 6, we show
how a more standard process of exact inference can be used
to effectively answer queries relative to this distribution.

The sampling process builds value assignments to slots
of frames incrementally, as the different components are re-
quired. By allowing such partial assignments, we bypass
the problem of going off on innite sampling chains. The
assumption of nite dependency chains guarantees that the
sampling chains required to sample the value of any simple
slot will always terminate.
Denition 3.4: A partial value  for an instance frame is an
assignment of values (of the appropriate type) to some subset
of its simple slots, an assignment of instance frames (from
the appropriate p-class) to some subset of its complex slots,
and a partial value for each of these assigned instances.

One nal subtlety arises in the sampling construction.
Some instance frames may have specic values pre-assigned
to some of the their slots. Such an assignment can be done
via an explicit statement for a named instance frame, or via a
process of inheritance from a template slot of a class to which
the instance belongs. As we explained in Section 2, the se-
mantics of such assignments is to condition the distribution.
To obtain the right semantics, we make the obvious modi-
cation to our data generating process. If, during the sampling
process, a value is generated for a slot which is inconsistent
with the observed value, we simply discard the entire partial
value generated up to that point. It is easy to see [4] that
the relative probability with which a partial value is gener-
ated in this data generating process is exactly the same as its
probability conditioned on the observed slot values.

As we discussed, our sampling procedure builds up a par-
tial value  piece by piece, as the pieces are needed. Our
main procedure, shown in Figure 1, is Sample(I, A), which
samples the value of a single simple slot A of a single instance
frame I. In order to sample A from the correct distribution,
it must backward chain and sample other slots on which the

Sample(I, A)
If A has a value in  then return
Foreach parent  of A
If  is a slot B in I then
Sample(I, B)
Else /*  is of the form :C */
Let J := ComplexValue(I, )
Sample(J, C)

Choose(I, A)

Choose(I, A)
Choose a value v for A
according to P A j PaA
Extend  with I :A = v
If A has a pre-assigned value v then
If v = v then fail

ComplexValue(I, )
If  is empty then
Return I
/*  is of the form B: */
If I :B is assigned a value J in  then
Let K := J
Else if I :B is pre-assigned a value J then
Extend  with I :B = J
Let K := J
Else
Let Y be value-type(B)
Create a new instance K of p-class Y
Extend  with I :B = K
If B has an inverse B in K then
Extend  with K:B = I
Return ComplexValue(K, )

Figure 1: Data generating sampling model

If the dependency graph is acyclic,

value of A depends. ComplexValue(I, ) determines the
value of the complex slot-chain I:, if necessary creating
new instance frames to represent the values of complex slots.
When the procedure returns, the partial value  (a global
variable in the procedure) contains a value for I:A.
Lemma 3.5 :
then
Sample(I, A), executed from a partial value  denes a
probability distribution over extensions of  with a value as-
signed to I:A. Furthermore, the distributiondoes not depend
on the order in which the parents of A are examined.
Proof: The basic steps in the proof are as follows. To prove
the rst part of the theorem, it sufces to show that the sam-
pling algorithm terminates. This proof proceeds using a sim-
ple inductive argument over the length of dependency chains.
To prove that the distribution is independent of the order, we
observe that a simple slot is always generated from the same
conditional distribution, regardless of when it is sampled, and
that the failure conditions are also applied universally.

We can now dene a SampleKB procedure that uses
Sample(I, A) to sample values for the slots of all named
instances in the KB. If any call to Sample fails, the entire
sampling process needs to be restarted. Once a value has
been assigned to all simple slots of named instances, we have
accounted for all evidence in the model, and therefore further
sampling of other slots cannot possibly fail. Therefore the
distribution we have obtained over the slots we have sampled
is the nal one.
Theorem 3.6: If the dependency graph is acyclic then Sam-
pleKB denes a probability distribution over partial values
 which have values assigned to all simple slots I:A for all
named instances I.

4 Multivalued slots and number uncertainty
To this point, we have assumed that every slot is single-
valued. However, slots that take on multiple values are a
fundamental concept in frame systems. The ai-professor p-
class may have a multi-valued papers slot of type ai-paper.
To simplify our discussion, we require multi-valued slots to
be complex, and all values of the slot must be of the same
p-class, as specied in the slots value-type.

To allow other slots in a frame X to depend on the prop-
erties of a multi-valued slot A, we must present a way for a

slot to depend on a set of slots. As the elements in the set
cannot be referred to individually, we must refer instead to
the properties of the set.
Denition 4.1: A quantier slot for a multi-valued slot A
has the form A: : e, 	A: : e,  nA: : e
or  nA: : e, where  is a slot chain of A and e is
an element of the value type of A:. The value-type of a
quantier slot is the set ftrue; falseg.
Given a set of values for A, the value of a quantifer slot on
A:B has precisely the meaning that one would expect; for
example, if for at least 10 values I of the papers slot I:impact
has the value high, then the value of  10papers.impact
:
high is true. Note that the CPT of a quantier slot is well-
dened for any number of values of its multi-valued slot.
On the other hand, no other slot can depend directly on the
multi-valued slot, thereby avoiding the problem of dening
general CPTs with a variable number of parents. A quantier
slot, on the other hand, is treated in the same way as a simple
slot, so it may be used as a parent of another slot. Thus,
for example, the will-get-tenure slot of the assistant-professor
class may depend on the above quantier slot.

So far, we have not specied the number of values that
a given multi-valued slot can take. In many cases, e.g., the
number of papers, this number is not xed. Therefore, we
would like to be able to model situations in which different
numbers of papers are possible, and to represent our beliefs
in these various possibilities. In other words, we would like
to allow structural uncertaintyuncertainty over the set of
entities in the world and the relationships between them.
Uncertainty over the number of values of a multi-valued slot
is a type of structural uncertainty called number uncertainty.
We can extend our language to represent number uncer-
tainty by associating with each multivalued slot A of X a new
number slot numA, which ranges over some set of natural
numbers f0; 1; : : :; ng (we assume that the number of values
of every slot is bounded). The slot numA is treated just like
any other simple slot; it has parents and distribution facets that
describe its probability model. Thus, the number of values
of A in X can depend on values of other slots of X and
of related frames, and it can also be the parent of another
slot. For example, ai-professor will have a numpapers slot,
whose value ranges from 0 to 50; numpapers may depend
on productivity and in turn inuence tired.

As with the case of single-valued slots, a specic value I
may be asserted for a multi-valued slot A of both a p-class
and a named individual. We interpret such assignments as
asserting that one of As values is I. It does not prevent A
from having other values; in fact, multiple values may be
asserted for the slot. Such assignments do not eliminate our
number uncertainty for this slot, but any case where the slot
has fewer than the number of asserted llers is eliminated;
thus, we must condition numA to be at least the asserted
number. To assert that A has only the values mentioned, we
would need to explicitly assert a value for numA.

It is interesting to examine the interaction between multi-
valued slots and inverses. Assume that the advisees slot has
an inverse advisor within the phd-student frame. If we now
have a student instance frame I, then we automatically assert
at least one valuethe value Ifor the advisees slot in the

instance frame I:advisor. Thus, even if we have no other
information whatsoever about this instance frame, it will not
be a generic member of the professor class. The very fact that
the professor is someones advisor modies its distribution
by conditioning it on the fact that numadvisees  1. Note
that the inverse slot may also be multi-valued. Many-many
relations give rise to potentially innite reference chains.
For example, a paper may have several authors, who have
all written several papers, and so on. However, due to the
restrictions on the ow of inuence between primary and
secondary inverses, an innite reference chain of this sort
cannot lead to an innite dependency chain.

Number uncertainty can be incorporated into our semantics
quite easily. We need to add number and quantier slots into
the dependency graph. Number slots are treated just like
simple slots: there is an edge into X:numA for each of the
parents of numA in X.
If X:Q is a quantier slot over
X:A, there is an edge from X:A to X:Q. Finally, we must
make the value of X:A depend both on X:numA and the
properties it imports from each of its llers. If X:A imports
B, then we have X:A  Y:B, where Y is the type of A, and
X:A  I:B for every asserted value I of X:A.

The sampling process can easily be modied to account
for multi-valued slots. When a multi-valued slot A needs to
be sampled for the rst time, we sample rst a value n for
numA. Let m be the number of asserted values for A. If
n  m, the sample fails. Otherwise, n (cid:0) m new instances
of the type of A are created, and the set of values of A is
set to be the m asserted llers and the n (cid:0) m new instances.
Theorem 3.6 continues to hold.

5 Reference uncertainty

As we said, structural uncertainty allows us to represent dis-
tributions over models with different structures. Number un-
certainty allows us to vary the set of instances in our model.
In this section, we describe reference uncertainty, which al-
lows us to vary the relations between the instances. For
example, we may want the conference slot of the AI-paper
class to be AAAI with probability 0.3, and another generic
AI conference with probability 0.7; note that AAAI is not
the value of a simple slot, but an instance frame itself. We
extend our language to accomodate reference uncertainty by
allowing some complex slots to be indirect. Each indirect
slot A is associated with a reference slot ref A, a simple
slot whose value dictates the value of the indirect slot.
Denition 5.1: If A is an indirect slot of type Y in p-class
X, then ref A is a simple slot in X whose value type is an
enumerated set R, each of whose values  is either: a named
individual of type Y , a slot-chain of X whose type is Y , or
the class Y itself.

In any instance I of X, the value of A is dened in terms
of the value of ref A:
if the value of ref A is a named
individual J, I:A = J; if the value of ref A is a slot-chain
, then I:A = I:; if ref A is the p-class Y , then the value
of A is a new instance of Y .

A reference slot is treated just like any other simple slot,
so it has parents and a CPT, and can inuence other simple
slots. A value can be assigned to a reference slot from

within its value-type, and the value of the indirect slot will
be determined by it. An indirect slot is treated like any other
single-valued complex slot; it has an import list, and other
slots can depend on the slots it imports. For technical reasons,
we do not allow direct assignments to indirect slots, nor do
we allow them to have inverses.

As with number uncertainty, reference uncertainty can be
incorporated quite easily into our semantics. We need to
add reference and indirect slots to the dependency graph.
A reference slot is treated like any other simple slot; thus,
its only dependencies are on its parents. An indirect slot A
clearly depends on ref A, so we have A  ref A. Since A
is a complex slot, it also depends on the slots that it imports.
However, because of reference uncertainty, we do not know
the frame from which it imports those slots. Let B be some
slot on the import list of A. To be safe, we need to account
for every possible value  of ref A. Thus, for each   R,
if  is a named individual I, we have X:A  I:B; if  is a
slot chain C:, we have X:A  X:C; if  is the class Y , we
have X:A  Y:B, denoting the fact A may import B from
some instance of Y .

The sampling process requires a small change to Assign-
Complex to deal with indirect slots. When AssignComplex
is called with an indirect slot, we rst sample the value of
the corresponding reference slot in the usual manner, and
then assign the value of the indirect slot in the manner deter-
mined by the value of the reference slot. With this change,
Theorem 3.6 continues to hold.

6

Inference

In the preceding sections, we presented a representation lan-
guage and semantics for probabilistic frame systems. To
complete the story, we now present a simple inference al-
gorithm for answering probabilistic queries in such a sys-
tem. Our algorithmcan handle any instance-based query, i.e.,
queries about the values of slots of instances. For simplicity,
we restrict attention to simple slots of named instances, as
other queries can easily be reduced to these. The algorithm,
called ConstructBN, is based on knowledge-based model
construction [12], the process of taking a KB and deriving
a BN B representing the same probability model. Standard
BN inference can then be used to answer queries.

Nodes in the Bayes net B have the form I::A where I
is an instance frame (not necessarily named),  is a possibly
empty slot chain, and A is a simple slot. The algorithmworks
by backward chaining along dependency chains, constructing
the appropriate nodes in the BN if they do not already exist.
More specically, the algorithm maintains an open list L of
nodes to be processed. Initially, L contains only the simple
slots of named instances.
In each iteration, the algorithm
removes a node from L, and processes it. When a node is
removed from L, it is processed in one of three ways: as a
simple slot, as a slot chain, or as a quantier slot.

Simple slots I:A are processed as follows. For each par-
ent I:, an edge is added from I: to I:A by a call to
AddParent(I:A, I:); if I: is not already in B, this rou-
tine adds I: to B and L. (Note that the parent list of any
simple non-quantier slot is xed.) When all parents have
been added, the CPT is constructed from the distribution facet

of A.

A slot chain I:B: is processed as follows:

ProcessComplex(I:B:)
If B is indirect then
ProcessIndirect(I:B:)
Else
If B is assigned a value J in I then K = J
Else K = XI:B, where X is the type of B in I
AddParent(I:B:, K:)
Set CPT of I:B: to copy the value of K:
Essentially, if B is assigned a named individual J in I, then
I:B = J. Otherwise, I:B = XI:B, an instance of X that
does not appear anywhere else in the KB; roughly, XI:B
serves the role of a Skolem function. Either way, the value
of I:B is known to be some other frame K, so that I:B: is
equal to K: . We make K: a parent of I:B: , and dene
the CPT to enforce this equality. These intermediate nodes
along the slot chain are introduced to monitor the ow of
values through complex slots. They are needed because the
ow becomes complicated when the chain contains indirect
slots. Intermediate variables that are spurious can easily be
eliminated in a simple post-processing phase.

If B is indirect, then the value of I:B could be one of
several frames, depending on the value of the reference slot
I:ref B. For any value  of I:ref B, let K denote the
frame which is the value of I:B. The value of I:B: is equal
to the value of K:. In other words, I:ref B selects the
value of I:B: from a set of possibilities. Therefore, the
node I:B: is a multiplexer node [1]; it has as parents the
node I:ref B and all nodes K:B:, and it uses the value
of I:ref B to select, as its value, the value of one of its
appropriate parents.
ProcessIndirect(I:B:)
AddParent(I:B:, I:refB)
For each value  of I:refB
If  is a named individual J then K = J
If  is the slot chain  then K = I:
If  is the class Y then K = Y I:C
AddParent(I:B:, K:)
Set CPT for I:B: to select the value of KI:B:
It remains to deal with the cases introduced by number
uncertainty. Since a multi-valued slot A can only be used
by quantier slots, these are the only slots which we need to
consider. Consider a quantier slot I:Q over A:. The value
of I:Q is fully determined by I:numA and the value of 
in each of the possible values of A. Let n be the maximum
number of such values, and suppose that A is assigned m
values J1; : : : ; Jm in I. In addition to these, there can be
up to n (cid:0) m other instances that are values for A; we build a
frame for each of them, Jm + 1; : : : ; Jn. The node I:Q
depends on I:numA and on the appropriate subset of the
variables Ji:; i.e., if numA is k, then only J1; : : : ; Jk
will inuence I:Q. The exact form of the dependence will
depend on the form of the quantier. For example, a  quanti-
er slot will be a deterministic conjunction of the appropriate
subset of its parents.
ProcessQuantier(I:Q) /* a quantier over A: */
AddParent(I:Q, I:numA)
Let n be the maximum value of I:numA
Let J 1; : : : ; J m be the assigned values to I:A
For i = m + 1 to n
J i = XiI:A, where X is the type of I:A

Gump.productivity

AAAI.standard

Gump.num(papers)

Conf[1].standard

Paper[1].ref(conf)

Conf[1].prestige

Paper[1].conf.standard

Paper[1].quality

Gump.brilliance

Paper[1].accepted

Paper[1].conf.prestige

Paper[1].impact

AAAI.prestige

Gump.tired

Conf[50].standard

Paper[50].ref(conf)

Conf[50].prestige

Gump.>=10(papers.impact:high)

Paper[50].conf.standard

Paper[50].quality

Gump.will-get-tenure

Paper[50].accepted

Paper[50].conf.prestige

Paper[50].impact

Figure 2: Part of the constructed BN for Prof. Gumps tenure
case. Models for only two of the fty possible papers are
shown. Paper[i] is shorthand for paper[Gump.papers][i], and
Conf[i] is short for conf[paper[Gump.papers.conference][i]].

the BN constructed for

For i = 1 to n
AddParent(I:Q, J i:)
Set the CPT for I:Q to depend on J 1:; : : : ; J numA:
To illustrate the algorithm, Figure 2 shows part
a KB concerning the
of
tenure case of one Prof. F. Gump,
an instance of
ai-assistant-professor. The node Gump.will-get-tenure depends
on Gump. 10(papers.impact:high). The latter is a quantier
slot, so it has as parents Gump:numpapers and the impact
slot of each of the 50 possible papers. Now, assume that
the class paper has impact depending on conference.prestige.
This dependence is duplicated for each of the 50 possible pa-
pers. The slot conference is indirect, so that for each i (using
the shorthand of the gure) Paper[i].conf.prestige has the
parent Paper[i].ref (conf) and the prestige slot of the possible
values of that reference, which are AAAI and conf[i].

Looking over the overall structure of the algorithm, we
see that there is a correspondence between the structure of
the dependency graph G and that of the BN B. We can de-
ne a mapping  from nodes of B to nodes of G as follows:
I:A: is I:A if I is a named individual, and X:A other-
wise for X the p-class of I. Intuitively, the node X:A in G
is a representative for all the nodes I:A where I is a generic
instance of X.
Lemma 6.1: If there is an edge from node 1 to node 2 in
B, then there is an path from 1 to 2 in G.

In other words, G serves as a template for the dependencies
in B. Many edges in B may map to the same path in G, but
any cycle or innite dependency in B will necessarily map to
a cycle in G (because G is nite). This property is the basis
for the following theorem.
Theorem 6.2:
If the dependency graph is acyclic, Con-
structBN terminates and the constructed Bayes net is acyclic.
This construction also provides us with an alternative spec-
ication for the distribution dened by a probabilistic frame
KB. Intuitively, the BN corresponds to the prior distribution
dened by the KB. In particular, the CPT of a numA slot
can ascribe a positive probability to numA = 0, despite the
fact that one or more values have been asserted for A in the

KB. In order for B to represent the distribution dened by our
semantics, we must condition it on all of our observations.
Specically, we assert the value for any simple slot whose
value was assigned in the KB, and lower bounds on the value
of numA corresponding to the number of values asserted
for A (including indirectly via inverses).
Theorem 6.3: Let S be the set of simple slots of named
individuals, and E the evidence on simple slots and number
slots derived from the KB. Then PrBS j E is the same as
the distribution over S dened by our semantics.

We have implemented our approach within a system that
contains the following functionality: A graphical network-
based editor/browser can be used to annotate a frame KB
with the facets encoding its probability model. The edi-
tor/browser interacts with the underlying FRS using OKBC,
thus allowing its use with many of the existing frame systems
(e.g., [3, 8, 7]). Our inference component also connects to
the FRS via OKBC; it extracts the relevant information about
frames, instances, and their probabilistic models, constructs
the BN corresponding to the KB, and utilizes the BN to an-
swer probabilistic queries. The system has been integrated
successfully with the Ontolingua frame system [3] and was
used for representing simple models of vehicle movement
patterns in a military setting. Our experience with the sys-
tem showed that even very simple models with three or four
simple p-classes could be used to generate fairly complicated
BNs (with hundreds of nodes) involving several interacting
entities.

7 Discussion and conclusions

In this paper, we have described the rst integration between
two of the most dominant threads of work in KR. Our lan-
guage provides a clean synthesis between the probabilistic
reasoning component and standard frame reasoning capabil-
ities. From the perspective of frame systems, our system
allows existing frame KBs to be annotated with probabilistic
models, greatly increasing the ability of frame systems to ex-
press meaningful knowledge in real-world applications. We
have also provided an inference algorithm capable of answer-
ing probabilistic queries about instances, thereby providing a
signicant increase to the inferential ability of such systems.
From the perspective of probabilistic modeling, our language
provides the tools for the construction of probabilisticmodels
for very large complex domains, signicantly scaling up our
ability to do uncertain reasoning.

Our language has given us the exciting capability of cre-
ating highly expressive probabilistic models with structural
uncertainty. Clearly, we have only scratched the surface of
this idea. For example, it is easy to add uncertainty over
the type of an object, e.g., to dene a probability distribu-
tion with which a professor is an assistant, associate, and full
professor. It is also easy to combine number and reference
uncertainty, allowing, for example, the advisor of a student
to be selected from the set of faculty members in the CS
department. These are two of many possible extensions that
can now be considered.

Another important issue which we have partially resolved
is the inference problem for probabilistic frame-based mod-
els. We have shown how we can reduce the problem to that of

reasoning in a standard BN, but this approach does not make
full use of the structure encoded in our representation.
In
particular, it fails to exploit encapsulation of frames within
other frames and the reuse of class models among several
objects. These ideas are put to good use in [6, 5], and it is an
important research topic to apply them in our richer frame-
work. We believe that by exploiting these features in our
inference as well as in our representation, we will be able to
effectively represent and reason in large uncertain domains.
Acknowledgments This work was supported by ONR con-
tract N66001-97-C-8554 under DARPAs HPKB program, by
DARPA contract DACA76-93-C-0025 under subcontract to
Information Extraction and Transport, Inc., and through the
generosity of the Powell Foundation and the Sloan Founda-
tion.

