Abstract

In typical classication tasks, we seek a function which assigns a label to a sin-
gle object. Kernel-based approaches, such as support vector machines (SVMs),
which maximize the margin of condence of the classier, are the method of
choice for many such tasks. Their popularity stems both from the ability to
use high-dimensional feature spaces, and from their strong theoretical guaran-
tees. However, many real-world tasks involve sequential, spatial, or structured
data, where multiple labels must be assigned. Existing kernel-based methods ig-
nore structure in the problem, assigning labels independently to each object, los-
ing much useful information. Conversely, probabilistic graphical models, such
as Markov networks, can represent correlations between labels, by exploiting
problem structure, but cannot handle high-dimensional feature spaces, and lack
strong theoretical generalization guarantees.
In this paper, we present a new
framework that combines the advantages of both approaches: Maximum mar-
gin Markov (M3) networks incorporate both kernels, which efciently deal with
high-dimensional features, and the ability to capture correlations in structured
data. We present an efcient algorithm for learning M3 networks based on a
compact quadratic program formulation. We provide a new theoretical bound
for generalization in structured domains. Experiments on the task of handwrit-
ten character recognition and collective hypertext classication demonstrate very
signicant gains over previous approaches.

1 Introduction
In supervised classication, our goal is to classify instances into some set of discrete cat-
egories. Recently, support vector machines (SVMs) have demonstrated impressive suc-
cesses on a broad range of tasks, including document categorization, character recognition,
image classication, and many more. SVMs owe a great part of their success to their
ability to use kernels, allowing the classier to exploit a very high-dimensional (possibly
even innite-dimensional) feature space. In addition to their empirical success, SVMs are
also appealing due to the existence of strong generalization guarantees, derived from the
margin-maximizing properties of the learning algorithm.

However, many supervised learning tasks exhibit much richer structure than a simple cat-
egorization of instances into one of a small number of classes. In some cases, we might
need to label a set of inter-related instances. For example: optical character recognition
(OCR) or part-of-speech tagging both involve labeling an entire sequence of elements into
some number of classes; image segmentation involves labeling all of the pixels in an im-
age; and collective webpage classication involves labeling an entire set of interlinked
webpages. In other cases, we might want to label an instance (e.g., a news article) with
multiple non-exclusive labels. In both of these cases, we need to assign multiple labels si-
multaneously, leading to a classication problem that has an exponentially large set of joint

labels. A common solution is to treat such problems as a set of independent classication
tasks, dealing with each instance in isolation. However, it is well-known that this approach
fails to exploit signicant amounts of correlation information [7].

An alternative approach is offered by the probabilistic framework, and specically by
probabilistic graphical models. In this case, we can dene and learn a joint probabilistic
model over the set of label variables. For example, we can learn a hidden Markov model,
or a conditional random eld (CRF) [7] over the labels and features of a sequence, and
then use a probabilistic inference algorithm (such as the Viterbi algorithm) to classify these
instances collectively, nding the most likely joint assignment to all of the labels simultane-
ously. This approach has the advantage of exploiting the correlations between the different
labels, often resulting in signicant improvements in accuracy over approaches that classify
instances independently [7, 10]. The use of graphical models also allows problem structure
to be exploited very effectively. Unfortunately, even probabilistic graphical models that are
trained discriminatively do not usually achieve the same level of generalization accuracy
as SVMs, especially when kernel features are used. Moreover, they are not (yet) associated
with generalization bounds comparable to those of margin-based classiers.

Clearly, the frameworks of kernel-based and probabilistic classiers offer complemen-
tary strengths and weaknesses. In this paper, we present maximum margin Markov (M3)
networks, which unify the two frameworks, and combine the advantages of both. Our ap-
proach denes a log-linear Markov network over a set of label variables (e.g., the labels
of the letters in an OCR problem); this network allows us to represent the correlations be-
tween these label variables. We then dene a margin-based optimization problem for the
parameters of this model. For Markov networks that can be triangulated tractably, the re-
sulting quadratic program (QP) has an equivalent polynomial-size formulation (e.g., linear
for sequences) that allows a very effective solution. By contrast, previous margin-based
formulations for sequence labeling [3, 1] require an exponential number of constraints. For
non-triangulated networks, we provide an approximate reformulation based on the relax-
ation used by belief propagation algorithms [8, 12]. Importantly, the resulting QP supports
the same kernel trick as do SVMs, allowing probabilistic graphical models to inherit the
important benets of kernels. We also show a generalization bound for such margin-based
classiers. Unlike previous results [3], our bound grows logarithmically rather than lin-
early with the number of label variables. Our experimental results on character recognition
and on hypertext classication, demonstrate dramatic improvements in accuracy over both
kernel-based instance-by-instance classication and probabilistic models.

2 Structure in classication problems
In supervised classication, the task is to learn a function h : X 7! Y from a set of m i.i.d.
instances S = f(x(i); y(i) = t(x(i)))gm
i=1, drawn from a x ed distribution DX (cid:2)Y. The
classication function h is typically selected from some parametric family H. A common
choice is the linear family: Given n real-valued basis functions fj : X (cid:2) Y 7! IR, a
hypothesis hw 2 H is dened by a set of n coefcients wj such that:

hw(x) = arg max

y

n

X

i=1

wjfj(x; y) = arg max

y

w>f (x; y) ;

(1)

where the f (x; y) are features or basis functions.

The most common classication setting  single-label classication  takes Y =
fy1; : : : ; ykg.
In this paper, we consider the much more general setting of multi-label
classication, where Y = Y1 (cid:2) : : : (cid:2) Yl with Yi = fy1; : : : ; ykg. In an OCR task, for
example, each Yi is a character, while Y is a full word. In a webpage collective classica-
tion task [10], each Yi is a webpage label, whereas Y is a joint label for an entire website.
In these cases, the number of possible assignments to Y is exponential in the number of
labels l. Thus, both representing the basis functions fj(x; y) in (1) and computing the
maximization arg maxy are infeasible.

An alternative approach is based on the framework of probabilistic graphical models. In
this case, the model denes (directly or indirectly) a conditional distribution P (Y j X ). We
can then select the label arg maxy P (y j x). The advantage of the probabilistic framework
is that it can exploit sparseness in the correlations between labels Yi. For example, in the
OCR task, we might use a Markov model, where Yi is conditionally independent of the rest
of the labels given Yi(cid:0)1; Yi+1.

We can encode this structure using a Markov network.

In this paper, purely for sim-
plicity of presentation, we focus on the case of pairwise interactions between labels. We
emphasize that our results extend easily to the general case. A pairwise Markov network
is dened as a graph G = (Y; E), where each edge (i; j) is associated with a potential
function  ij(x; yi; yj). The network encodes a joint conditional probability distribution as
P (y j x) / Q(i;j)2E  ij(x; yi; yj). These networks exploit the interaction structure to
parameterize a classier very compactly. In many cases (e.g., tree-structured networks),
we can use effective dynamic programming algorithms (such as the Viterbi algorithm) to
nd the highest probability label y; in others, we can use approximate inference algorithms
that also exploit the structure [12].

The Markov network distribution is simply a log-linear model, with the pairwise potential
ij(x; yi; yj) representing (in log-space) a sum of basis functions over x; yi; yj. We can
therefore parameterize such a model using a set of pairwise basis functions f (x; yi; yj) for
(i; j) 2 E. We assume for simplicity of notation that all edges in the graph denote the same
type of interaction, so that we can dene a set of features

fk(x; y) = X

fk(x; yi; yj):

(i;j)2E

(2)

k=1 wkfk(x; yi; yj)] =

The network potentials are then  ij(x; yi; yj) = exp [Pn
exp (cid:2)w>f (x; yi; yj)(cid:3) :
The parameters w in a log-linear model can be trained to t the data, typically by maxi-
mizing the likelihood or conditional likelihood (e.g., [7, 10]). This paper presents an algo-
rithm for selecting w that maximize the margin, gaining all of the advantages of SVMs.
3 Margin-based structured classication
For a single-label binary classication problem, support vector machines (SVMs) [11] pro-
vide an effective method of learning a maximum-margin decision boundary. For single-
label multi-class classication, Crammer and Singer [5] provide a natural extension of this
framework by maximizing the margin (cid:13) subject to constraints:

maximize (cid:13) s:t:

jjwjj (cid:20) 1; w>(cid:1)fx(y) (cid:21) (cid:13); 8 x 2 S; 8y 6= t(x);

(3)
where (cid:1)fx(y) = f (x; t(x)) (cid:0) f (x; y). The constraints in this formulation ensure that
arg maxy w>f (x; y) = t(x). Maximizing (cid:13) magnies the difference between the value
of the true label and the best runner-up, increasing the condence of the classication.

In structured problems, where we are predicting multiple labels, the loss function is usu-
ally not simple 0-1 loss I(arg maxy w>fx(y) = t(x)), but per-label loss, such as the
proportion of incorrect labels predicted. In order to extend the margin-based framework
to the multi-label setting, we must generalize the notion of margin to take into account
the number of labels in y that are misclassied.
In particular, we would like the margin
between t(x) and y to scale linearly with the number of wrong labels in y, (cid:1)tx(y):
jjwjj (cid:20) 1; w>(cid:1)fx(y) (cid:21) (cid:13) (cid:1)tx(y); 8x 2 S; 8 y;

maximize (cid:13) s:t:

(4)

where (cid:1)tx(y) = P l
transformation to eliminate (cid:13), we get a quadratic program (QP):

i=1 (cid:1)tx(yi) and (cid:1)tx(yi) (cid:17) I(yi 6= (t(x))i). Now, using a standard

minimize

jjwjj2

s:t: w>(cid:1)fx(y) (cid:21) (cid:1)tx(y); 8x 2 S; 8 y:

(5)

1
2

Unfortunately, the data is often not separable by a hyperplane dened over the space of
the given set of features. In such cases, we need to introduce slack variables (cid:24)x to allow

some constraints to be violated. We can now present the complete form of our optimization
problem, as well as the equivalent dual problem [2]:

Primal formulation (6)

Dual formulation

min

1
2

jjwjj2 + CXx

(cid:24)x ;

s:t: w>(cid:1)fx(y) (cid:21) (cid:1)tx(y) (cid:0) (cid:24)x; 8x; y:

(cid:11)x(y)(cid:1)tx(y) (cid:0)

max Xx;y
s:t: Xy

(cid:11)x(y) = C; 8x; (cid:11)x(y) (cid:21) 0 ; 8x; y:

(7)
2

Xx;y

1

2 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

;

(cid:11)x(y)(cid:1)fx(y)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(Note: for each x, we add an extra dual variable (cid:11)x(t(x)), with no effect on the solution.)

4 Exploiting structure in M3 networks
Unfortunately, both the number of constraints in the primal QP in (6), and the number of
variables in the dual QP in (7) are exponential in the number of labels l. In this section, we
present an equivalent, polynomially-sized, formulation.

Our main insight is that the variables (cid:11)x(y) in the dual formulation (7) can be interpreted
as a density function over y conditional on x, as Py (cid:11)x(y) = C and (cid:11)x(y) (cid:21) 0. The dual
objective is a function of expectations of (cid:1)tx(y) and (cid:1)fx(y) with respect to (cid:11)x(y). Since
both (cid:1)tx(y) = Pi (cid:1)tx(yi) and (cid:1)fx(y) = P(i;j) (cid:1)fx(yi; yj) are sums of functions over
nodes and edges, we only need node and edge marginals of the measure (cid:11)x(y) to compute
their expectations. We dene the marginal dual variables as follows:

(cid:22)x(yi; yj) = Py(cid:24)[yi;yj ] (cid:11)x(y); 8 (i; j) 2 E; 8yi; yj; 8 x;
(cid:22)x(yi)

8 i; 8yi; 8 x;

= Py(cid:24)[yi] (cid:11)x(y);

(8)

where y (cid:24) [yi; yj] denotes a full assignment y consistent with partial assignment yi; yj.

Now we can reformulate our entire QP (7) in terms of these dual variables. Consider, for

example, the rst term in the objective function:

Xy

(cid:11)x(y)(cid:1)tx(y) =Xy Xi

(cid:11)x(y)(cid:1)tx(yi) =Xi;yi

(cid:1)tx(yi) Xy(cid:24)[yi]

(cid:11)x(y) =Xi;yi

(cid:22)x(yi)(cid:1)tx(yi):

The decomposition of the second term in the objective uses edge marginals (cid:22)x(yi; yj).
In order to produce an equivalent QP, however, we must also ensure that the dual variables
(cid:22)x(yi; yj); (cid:22)x(yi) are the marginals resulting from a legal density (cid:11)(y); that is, that they
belong to the marginal polytope [4]. In particular, we must enforce consistency between
the pairwise and singleton marginals (and hence between overlapping pairwise marginals):
(9)

(cid:22)x(yi; yj) = (cid:22)x(yj); 8yj; 8(i; j) 2 E; 8x:

X

yi

If the Markov network for our basis functions is a forest (singly connected), these con-
straints are equivalent to the requirement that the (cid:22) variables arise from a density. There-
fore, the following factored dual QP is equivalent to the original dual QP:

(cid:22)x(yi)(cid:1)tx(yi) (cid:0)

(cid:22)x(yi; yj)(cid:22)^x(yr; ys)fx(yi; yj)>f^x(yr; ys);

max Xx Xi;yi
s:t: Xyi

(cid:22)x(yi; yj) = (cid:22)x(yj); Xyi

1

2 Xx;^x X(i;j)

yi;yj X(r;s)

yr ;ys

Similarly, the original primal can be factored as follows:

(cid:22)x(yi) = C; (cid:22)x(yi; yj) (cid:21) 0:

(10)

min

1
2

jjwjj2 + CXx Xi

(cid:24)x;i + CXx X(i;j)

(cid:24)x;ij;

s:t: w>(cid:1)fx(yi; yj) + X(i0;j):i06=i

mx;i0 (yj) + X(j 0;i):j 06=j

mx;j 0 (yi) (cid:21) (cid:0)(cid:24)x;ij;

mx;j(yi) (cid:21) (cid:1)tx(yi) (cid:0) (cid:24)x;i;

(cid:24)x;ij (cid:21) 0; (cid:24)x;i (cid:21) 0:

(11)

X(i;j)

The solution to the factored dual gives us: w = Px P(i;j) Pyi;yj
Theorem 4.1 If for each x the edges E form a forest, then a set of weights w will be
optimal for the QP in (6) if and only if it is optimal for the factored QP in (11).

(cid:22)x(yi; yj)(cid:1)fx(yi; yj).

If the underlying Markov net is not a forest, then the constraints in (9) are not sufcient to
enforce the fact that the (cid:22)s are in the marginal polytope. We can address this problem by
triangulating the graph, and introducing new (cid:17) LP variables that now span larger subsets of
Yis. For example, if our graph is a 4-cycle Y1Y2Y3Y4Y1, we might triangulate
the graph by adding an arc Y1Y3, and introducing (cid:17) variables over joint instantiations of
the cliques Y1; Y2; Y3 and Y1; Y3; Y4. These new (cid:17) variables are used in linear equalities
that constrain the original (cid:22) variables to be consistent with a density. The (cid:17) variables appear
only in the constraints; they do not add any new basis functions nor change the objective
function. The number of constraints introduced is exponential in the number of variables
in the new cliques. Nevertheless, in many classication problems, such as sequences and
other graphs with low tree-width [4], the extended QP can be solved efciently .

Unfortunately, triangulation is not feasible in highly connected problems. However, we
can still solve the QP in (10) dened by an untriangulated graph with loops. Such a proce-
dure, which enforces only local consistency of marginals, optimizes our objective only over
a relaxation of the marginal polytope. In this way, our approximation is analogous to the
approximate belief propagation (BP) algorithm for inference in graphical models [8]. In
fact, BP makes an additional approximation, using not only the relaxed marginal polytope
but also an approximate objective (Bethe free-energy) [12]. Although the approximate QP
does not offer the theoretical guarantee in Theorem 4.1, the solutions are often very accu-
rate in practice, as we demonstrate below.

As with SVMs [11], the factored dual formulation in (10) uses only dot products between
basis functions. This allows us to use a kernel to dene very large (and even innite) set of
features. In particular, we dene our basis functions by fx(yi; yj) = (cid:26)(yi; yj)(cid:30)ij(x), i.e.,
the product of a selector function (cid:26)(yi; yj) with a possibly innite feature vector (cid:30)ij(x).
For example, in the OCR task, (cid:26)(yi; yj) could be an indicator function over the class of
two adjacent characters i and j, and (cid:30)ij(x) could be an RBF kernel on the images of these
two characters. The operation fx(yi; yj)>f^x(yr; ys) used in the objective function of the
factored dual QP is now (cid:26)(yi; yj)(cid:26)(yr; ys)K(cid:30)(x; i; j; ^x; r; s), where K(cid:30)(x; i; j; ^x; r; s) =
(cid:30)ij(x) (cid:1) (cid:30)rs(x) is the kernel function for the feature (cid:30). Even for some very complex
functions (cid:30), the dot-product required to compute K(cid:30) can be executed efciently [11].
5 SMO learning of M3 networks
Although the number of variables and constraints in the factored dual in (10) is polynomial
in the size of the data, the number of coefcients in the quadratic term (kernel matrix) in the
objective is quadratic in the number of examples and edges in the network. Unfortunately,
this matrix is often too large for standard QP solvers. Instead, we use a coordinate descent
method analogous to the sequential minimal optimization (SMO) used for SVMs [9].

Let us begin by considering the original dual problem (7). The SMO approach solves
this QP by analytically optimizing two-variable subproblems. Recall that Py (cid:11)x(y) = C.
We can therefore take any two variables (cid:11)x(y1), (cid:11)x(y2) and move weight from one to
the other, keeping the values of all other variables x ed. More precisely, we optimize for
(cid:11)0

x(y1); (cid:11)0
Clearly, however, we cannot perform this optimization in terms of the original dual, which
is exponentially large. Fortunately, we can perform precisely the same optimization in
x(y2).
terms of the marginal dual variables. Let (cid:21) = (cid:11)0
Consider a dual variable (cid:22)x(yi; yj). It is easy to see that a change from (cid:11)x(y1); (cid:11)x(y2) to
(cid:11)0

x(y1) (cid:0) (cid:11)x(y1) = (cid:11)x(y2) (cid:0) (cid:11)0

x(y2) = (cid:11)x(y1) + (cid:11)x(y2).

x(y2) such that (cid:11)0

x(y1) + (cid:11)0

x(y1); (cid:11)0

x(y2) has the following effect on (cid:22)x(yi; yj):
i ; yj = y1

x(yi; yj) = (cid:22)x(yi; yj) + (cid:21)I(yi = y1
(cid:22)0

j ) (cid:0) (cid:21)I(yi = y2

i ; yj = y2

j ):

(12)

We can solve the one-variable quadratic subproblem in (cid:21) analytically and update the ap-
propriate (cid:22) variables. We use inference in the network to test for optimality of the current
solution (the KKT conditions [2]) and use violations from optimality as a heuristic to select
the next pair y1; y2. We omit details for lack of space.
6 Generalization bound
In this section, we show a generalization bound for the task of multi-label classication
that allows us to relate the error rate on the training set to the generalization error. As we
shall see, this bound is signicantly stronger than previous bounds for this problem.

Our goal in multi-label classication is to maximize the number of correctly classi-
ed labels. Thus an appropriate error function is the average per-label loss L(w; x) =
1
l (cid:1)tx(arg maxy w>fx(y)). As in other generalization bounds for margin-based classi-
ers, we relate the generalization error to the margin of the classier . In Sec. 3, we dene
the notion of per-label margin, which grows with the number of mistakes between the cor-
rect assignment and the best runner-up. We can now dene a (cid:13)-margin per-label loss:

L(cid:13)(w; x) = supz: jz(y)(cid:0)w>fx(y)j(cid:20)(cid:13)(cid:1)tx(y); 8y

1
l (cid:1)tx(arg maxy z(y)):

This loss function measures the worst per-label loss on x made by any classier z which
is perturbed from w>fx by at most a (cid:13)-margin per-label. We can now prove that the gen-
eralization accuracy of any classier is bounded by its expected (cid:13)-margin per-label loss on
the training data, plus a term that grows inversely with the margin.Intuitively, the rst term
corresponds to the bias, as margin (cid:13) decreases the complexity of our hypothesis class by
considering a (cid:13)-per-label margin ball around w>fx and selecting one (the worst) classier
within this ball. As (cid:13) shrinks, our hypothesis class becomes more complex, and the rst
term becomes smaller, but at the cost of increasing the second term, which intuitively cor-
responds to the variance. Thus, the result provides a bound to the generalization error
that trades off the effective complexity of the hypothesis space with the training error.
Theorem 6.1 If the edge features have bounded 2-norm, max(i;j);yi;yj kfx(yi; yj)k2 (cid:20)
Redge, then for a family of hyperplanes parameterized by w, and any (cid:14) > 0, there exists a
constant K such that for any (cid:13) > 0 per-label margin, and m > 1 samples, the per-label
loss is bounded by:

[ln m + ln l + ln q + ln k] + ln

1

(cid:14)# ;

ExL(w; x) (cid:20) ESL(cid:13)(w; x) +vuut

edge kwk2

2 q2

K

m " R2

(cid:13)2

with probability at least 1 (cid:0) (cid:14), where q = maxi jf(i; j) 2 Egj is the maximum edge degree
in the network, k is the number of classes in a label, and l is the number of labels.
Unfortunately, we omit the proof due to lack of space. (See a longer version of the paper at
http://cs.stanford.edu/btaskar/.) The proof uses a covering number argument
analogous to previous results in SVMs [13]. However we propose a novel method for
covering structured problems by constructing a cover to the loss function from a cover
of the individual edge basis function differences (cid:1)fx(yi; yj). This new type of cover is
polynomial in the number of edges, yielding signicant improvements in the bound.

Specically , our bound has a logarithmic dependence on the number of labels (ln l) and
depends only on the 2-norm of the basis functions per-edge (Redge). This is a signicant
gain over the previous result of Collins [3] which has linear dependence on the number of
labels (l), and depends on the joint 2-norm of all of the features (which is (cid:24) lRedge, unless
each sequence is normalized separately, which is often ineffective in practice). Finally, note
m = O(1) (for example, in OCR, if the number of instances is at least a constant
that if l
times the length of a word), then our bound is independent of the number of labels l. Such
a result was, until now, an open problem for margin-based sequence classication [3].
7 Experiments
We evaluate our approach on two very different tasks: a sequence model for handwriting
recognition and an arbitrary topology Markov network for hypertext classication.

0.35

0.3

0.25

0.2

0.15

0.1

0.05

0

)
r
e
t
c
a
r
a
h
c
-
r
e
p
e
g
a
r
e
v
a
(

r
o
r
r
e




t
s
e
T

linear

quadratic

cubic

Log-Reg

CRF

mSVM

M^3N

)
l
o
o
h
c
s




r
e
p
s
e
g
a
p
(

r
o
r
r
e


t
s
e
T

0.25

0.2

0.15

0.1

0.05

0

mSVM

RMN

M^3N

Cor

Tex

Was

Wis

Ave

(c)

(a)

(b)

Figure 1: (a) 3 example words from the OCR data set; (b) OCR: Average per-character test error for
logistic regression, CRFs, multiclass SVMs, and M3Ns, using linear, quadratic, and cubic kernels;
(c) Hypertext: Test error for multiclass SVMs, RMNs and M3Ns, by school and average.

Handwriting Recognition. We selected a subset of (cid:24) 6100 handwritten words, with av-
erage length of (cid:24) 8 characters, from 150 human subjects, from the data set collected by
Kassel [6]. Each word was divided into characters, each character was rasterized into an
image of 16 by 8 binary pixels. (See Fig. 1(a).) In our framework, the image for each word
corresponds to x, a label of an individual character to Yi, and a labeling for a complete
word to Y. Each label Yi takes values from one of 26 classes fa; : : : ; zg.

The data set is divided into 10 folds of (cid:24) 600 training and (cid:24) 5500 testing examples.
The accuracy results, summarized in Fig. 1(b), are averages over the 10 folds. We im-
plemented a selection of state-of-the-art classication algorithms: independent label ap-
proaches, which do not consider the correlation between neighboring characters  logistic
regression, multi-class SVMs as described in (3), and one-against-all SVMs (whose perfor-
mance was slightly lower than multi-class SVMs); and sequence approaches  CRFs, and
our proposed M3 networks. Logistic regression and CRFs are both trained by maximiz-
ing the conditional likelihood of the labels given the features, using a zero-mean diagonal
Gaussian prior over the parameters, with a standard deviation between 0.1 and 1. The other
methods are trained by margin maximization. Our features for each label Yi are the corre-
sponding image of ith character. For the sequence approaches (CRFs and M3), we used an
indicator basis function to represent the correlation between Yi and Yi+1. For margin-based
methods (SVMs and M3), we were able to use kernels (both quadratic and cubic were eval-
uated) to increase the dimensionality of the feature space. Using these high-dimensional
feature spaces in CRFs is not feasible because of the enormous number of parameters.

Fig. 1(b) shows two types of gains in accuracy: First, by using kernels, margin-based
methods achieve a very signicant gain over the respective likelihood maximizing methods.
Second, by using sequences, we obtain another signicant gain in accuracy. Interestingly,
the error rate of our method using linear features is 16% lower than that of CRFs, and
about the same as multi-class SVMs with cubic kernels. Once we use cubic kernels our
error rate is 45% lower than CRFs and about 33% lower than the best previous approach.
For comparison, the previously published results, although using a different setup (e.g., a
larger training set), are about comparable to those of multiclass SVMs.
Hypertext. We also tested our approach on collective hypertext classication, using the
data set in [10], which contains web pages from four different Computer Science depart-
ments. Each page is labeled as one of course, faculty, student, project, other. In all of our
experiments, we learn a model from three schools, and test on the remaining school. The
text content of the web page and anchor text of incoming links is represented using a set
of binary attributes that indicate the presence of different words. The baseline model is a
simple linear multi-class SVM that uses only words to predict the category of the page. The
second model is a relational Markov network (RMN) of Taskar et al. [10], which in addi-
tion to word-label dependence, has an edge with a potential over the labels of two pages
that are hyper-linked to each other. This model denes a Markov network over each web
site that was trained to maximize the conditional probability of the labels given the words

and the links. The third model is a M3 net with the same features but trained by maximizing
the margin using the relaxed dual formulation and loopy BP for inference.

Fig. 1(c) shows a gain in accuracy from SVMs to RMNs by using the correlations between
labels of linked web pages, and a very signicant additional gain by using maximum margin
training. The error rate of M3Ns is 40% lower than that of RMNs, and 51% lower than
multi-class SVMs.
8 Discussion
We present a discriminative framework for labeling and segmentation of structured data
such as sequences, images, etc. Our approach seamlessly integrates state-of-the-art kernel
methods developed for classication of independent instances with the rich language of
graphical models that can exploit the structure of complex data. In our experiments with
the OCR task, for example, our sequence model signicantly outperforms other approaches
by incorporating high-dimensional decision boundaries of polynomial kernels over charac-
ter images while capturing correlations between consecutive characters. We construct our
models by solving a convex quadratic program that maximizes the per-label margin. Al-
though the number of variables and constraints of our QP formulation is polynomial in the
example size (e.g., sequence length), we also address its quadratic growth using an effec-
tive optimization procedure inspired by SMO. We provide theoretical guarantees on the
average per-label generalization error of our models in terms of the training set margin.
Our generalization bound signicantly tightens previous results of Collins [3] and suggests
possibilities for analyzing per-label generalization properties of graphical models.

For brevity, we simplied our presentation of graphical models to only pairwise Markov
networks. Our formulation and generalization bound easily extend to interaction patterns
involving more than two labels (e.g., higher-order Markov models). Overall, we believe
that M3 networks will signicantly further the applicability of high accuracy margin-based
methods to real-world structured data.
Acknowledgments.
P00002 under DARPAs EELD program.
