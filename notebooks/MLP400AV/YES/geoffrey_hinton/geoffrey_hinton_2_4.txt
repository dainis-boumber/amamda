Abstract

We describe a probabilistic approach to the task of placing objects, de-
scribed by high-dimensional vectors or by pairwise dissimilarities, in a
low-dimensional space in a way that preserves neighbor identities. A
Gaussian is centered on each object in the high-dimensional space and
the densities under this Gaussian (or the given dissimilarities) are used
to dene a probability distribution over all the potential neighbors of
the object. The aim of the embedding is to approximate this distribu-
tion as well as possible when the same operation is performed on the
low-dimensional images of the objects. A natural cost function is a
sum of Kullback-Leibler divergences, one per object, which leads to a
simple gradient for adjusting the positions of the low-dimensional im-
ages. Unlike other dimensionality reduction methods, this probabilistic
framework makes it easy to represent each object by a mixture of widely
separated low-dimensional images. This allows ambiguous objects, like
the document count vector for the word bank, to have versions close to
the images of both river and nance without forcing the images of
outdoor concepts to be located close to those of corporate concepts.

1 Introduction
Automatic dimensionality reduction is an important toolkit operation in machine learn-
ing, both as a preprocessing step for other algorithms (e.g. to reduce classier input size)
and as a goal in itself for visualization, interpolation, compression, etc. There are many
ways to embed objects, described by high-dimensional vectors or by pairwise dissim-
ilarities, into a lower-dimensional space. Multidimensional scaling methods[1] preserve
dissimilarities between items, as measured either by Euclidean distance, some nonlinear
squashing of distances, or shortest graph paths as with Isomap[2, 3]. Principal compo-
nents analysis (PCA) nds a linear projection of the original data which captures as much
variance as possible. Other methods attempt to preserve local geometry (e.g. LLE[4]) or
associate high-dimensional points with a xed grid of points in the low-dimensional space
(e.g. self-organizing maps[5] or their probabilistic extension GTM[6]). All of these meth-
ods, however, require each high-dimensional object to be associated with only a single
location in the low-dimensional space. This makes it difcult to unfold many-to-one
mappings in which a single ambiguous object really belongs in several disparate locations
in the low-dimensional space. In this paper we dene a new notion of embedding based on
probable neighbors. Our algorithm, Stochastic Neighbor Embedding (SNE) tries to place
the objects in a low-dimensional space so as to optimally preserve neighborhood identity,
and can be naturally extended to allow multiple different low-d images of each object.


2 The basic SNE algorithm

:

%&%

%&%

(1)



symmetric), or they may be computed using the scaled squared Euclidean distance (afn-

 , may be given as part of the problem denition (and need not be

For each object,  , and each potential neighbor, , we start by computing the asymmetric
probability, , that would pick as its neighbor:








The dissimilarities,  
ity) between two high-dimensional points,!"$#$!
()
where)
for the value of)
Here,-
(which we set without loss of generality to be /
 picks point  as its neighbor is a function of the low-dimensional images 23 of all the
%'%
4



4



that makes the entropy of the distribution over neighbors equal to*'+,.-

) so the induced probability 01
%'%

In the low-dimensional space we also use Gaussian neighborhoods but with a xed variance
that point

is either set by hand or (as in some of our experiments) found by a binary search
.

is the effective number of local neighbors or perplexity and is chosen by hand.

The aim of the embedding is to match these two distributions as well as possible. This is
achieved by minimizing a cost function which is a sum of Kullback-Leibler divergences

objects and is given by the expression:

0

2

(2)

(3)

%'%

%&%



%&%

@?

;8

:
0

space is chosen by hand (much less than the number of objects).

98
large when

between the original (5 ) and induced (06 ) distributions over neighbors for each object:
The dimensionality of the2
Notice that making0

*'+,

is small wastes some of the probability mass in the0

distribution so there is a cost for modeling a big distance in the high-dimensional space with
a small distance in the low-dimensional space, though it is less than the cost of modeling
a small distance with a big one. In this respect, SNE is an improvement over methods
like LLE [4] or SOM [5] in which widely separated data-points can be collapsed as near
neighbors in the low-dimensional space. The intuition is that while SNE emphasizes local
distances, its cost function cleanly enforces both keeping the images of nearby objects
nearby and keeping the images of widely separated objects relatively far apart.

9<>=

(4)





 affects0
2


the result is simple:

Differentiating C is tedious because2
2

 via the normalization term in Eq. 3, but
0DCEFG
which has the nice interpretation of a sum of forces pulling2" toward2 or pushing it away
depending on whether
Given the gradient, there are many possible ways to minimize7

and we have only just be-
gun the search for the best method. Steepest descent in which all of the points are adjusted
in parallel is inefcient and can get stuck in poor local optima. Adding random jitter that
decreases with time nds much better local optima and is the method we used for the exam-
ples in this paper, even though it is still quite slow. We initialize the embedding by putting
all the low-dimensional images in random locations very close to the origin. Several other
minimization methods, including annealing the perplexity, are discussed in sections 5&6.

is observed to be a neighbor more or less often than desired.

0HG

(5)











!


!








2


2







2



7

8



A


B
7
B
2


(
8








3 Application of SNE to image and document collections
As a graphic illustration of the ability of SNE to model high-dimensional, near-neighbor
relationships using only two dimensions, we ran the algorithm on a collection of bitmaps of
handwritten digits and on a set of word-author counts taken from the scanned proceedings
of NIPS conference papers. Both of these datasets are likely to have intrinsic structure in
many fewer dimensions than their raw dimensionalities: 256 for the handwritten digits and
13679 for the author-word counts.

from each of the ve classes 0,1,2,3,4. The variance of the Gaussian around each point

To begin, we used a set of digit bitmaps from the UPS database[7] with examples
in the (
 -dimensional raw pixel image space was set to achieve a perplexity of 15 in the
distribution over high-dimensional neighbors. SNE was initialized by putting all the 2"

in random locations very close to the origin and then was trained using gradient descent
with annealed noise. Although SNE was given no information about class labels, it quite
cleanly separates the digit groups as shown in gure 1. Furthermore, within each region of
the low-dimensional space, SNE has arranged the data so that properties like orientation,
skew and stroke-thickness tend to vary smoothly. For the embedding shown, the SNE

cost function in Eq. 4 has a value of 
nats; with a uniform distribution across low-
 nats. We also applied
dimensional neighbors, the cost is 

principal component analysis (PCA)[8] to the same data; the projection onto the rst two
principal components does not separate classes nearly as cleanly as SNE because PCA is
much more interested in getting the large separations right which causes it to jumble up
some of the boundaries between similar classes. In this experiment, we used digit classes
that do not have very similar pairs like 3 and 5 or 7 and 9. When there are more classes and
only two available dimensions, SNE does not as cleanly separate very similar pairs.

	


*'+,



We have also applied SNE to word-document and word-author matrices calculated from
the OCRed text of NIPS volume 0-12 papers[9]. Figure 2 shows a map locating NIPS au-
thors into two dimensions. Each of the 676 authors who published more than one paper

and corresponding last names are authors who published six or more papers in that period.

word counts, summed across all NIPS papers. Co-authored papers gave fractional counts
evenly to all authors. All words occurring in six or more documents were included, ex-
cept for stopwords giving a vocabulary size of 13649. (The bow toolkit[10] was used for

in NIPS vols. 0-12 is shown by a dot at the position 2
Distances 
part of the pre-processing of the data.) The )

 found by SNE; larger red dots
 were computed as the norm of the difference between log aggregate author
 were set to achieve a local perplexity of
( neighbors. SNE seems to have grouped authors by broad NIPS eld: generative

models, support vector machines, neuroscience, reinforcement learning and VLSI all have
distinguishable localized regions.

4 A full mixture version of SNE
The clean probabilistic formulation of SNE makes it easy to modify the cost function so
that instead of a single image, each high-dimensional object can have several different
versions of its low-dimensional image. These alternative versions have mixing proportions

is a mixture of the distributions induced

that sum to  . Image-version of object has location2
low-dimensional neighborhood distribution for 
by each of its image-versions across all image-versions of a potential neighbor :
%&%
4

$
2




4

$

 and mixing proportion5 . The
2

%&%
In this multiple-image model, the derivatives with respect to the image locations 23 are
straightforward; the derivatives w.r.t the mixing proportions 
 are most easily expressed

8



:

0

8

%'%

%'%

(6)


(




-





2



2


Touretzky
Pomerleau

Wiles

Maass

Kailath

Chauvin Munro
Sanger
Lewicki Schmidhuber

Shavlik

Baluja

Pearlmutter

Tenenbaum

Yang
AbuMostafa

Movellan

Baldi

Cottrell

Schraudolph

Lippmann

Hertz
Buhmann
Krogh
Omohundro
MacKay

Robinson

Smyth

Coolen

Cohn

Goodman

Pentland

Ahmad

Tesauro

Neuneier

Atkeson

Warmuth
Sollich
Thrun

Barber

Moore

Koch

Obermayer

Ruderman
Cowan

Bialek
Mel

Giles

Chen

Sun
Lee

Lee
Seung

Movellan
Baldi

Cottrell
Lippmann
Kawato
Bourlard

Waibel

Morgan

Nowlan

Viola

Pouget

Dayan

Bower

Ruppin
Meilijson Mead

Horn

Eeckman
Baird
Li

Brown
Doya

Spence
Touretzky

Bell
Pomerleau
Tenenbaum

MacKay
Smyth
WarmuthSollich
Thrun

Barber

Lazzaro
Harris

Murray

Andreou

Cauwenberghs

Jabri
Stork
Kailath

Principe
Maass
Amari
Yang
AbuMostafa
CohnKowalczyk
Atkeson
Moore

Sutton

Singh

Barto
Kearns

Saad

Sejnowski

Zemel

Mozer

Tishby

Singer
Saul

WolpertOpper
Moody
TrespLeen
Jaakkola

Bishop

Hinton

Jordan

Ghahramani
Williams
Bengio

LeCun

Graf

Simard

Denker

Guyon

Alspector Mjolsness
Meir
Rangarajan

Gold

Williamson
ShaweTaylor

Platt Bartlett

Solla

Vapnik

Smola

Scholkopf
Muller

cation 2

Figure 2: Embedding of NIPS authors into two dimensions. Each of the 676 authors
who published more than one paper in NIPS vols. 0-12 is show by a dot at the lo-
found by the SNE algorithm. Larger red dots and corresponding last names
are authors who published six or more papers in that period. The inset in upper left
shows a blowup of the crowded boxed central portion of the space. Dissimilarities be-
tween authors were computed based on squared Euclidean distance between vectors of
log aggregate author word counts. Co-authored papers gave fractional counts evenly
to all authors. All words occurring in six or more documents were included, except
for stopwords giving a vocabulary size of 13649. The NIPS text data is available at
http://www.cs.toronto.edu/

roweis/data.html.





(7)

8

$

8

is given by

%'%

2

@

9

%&%

%'%




0

%&%

4

$





(8)

(9)

:


in terms of
 , the probability that version  of picks version of :
The effect on06 of changing the mixing proportion for version of object
@
 on the cost, C, is
where
4




	
 and  otherwise. The effect of changing 

optimization on softmax weights dened by 

As a proof-of-concept, we recently implemented a simplied mixture version in which
every object is represented in the low-dimensional space by exactly two components that

Rather than optimizing the mixing proportions directly, it is easier to perform unconstrained


4



0

if

by a force which increases linearly up to a threshold separation. Beyond this threshold
the force remains constant.1 We ran two experiments with this simplied mixture version

the classes and taking each pixel at random from one of these two parents. After mini-
of the non-hybrids had signicantly different
locations for their two mixture components. Moreover, the mixture components of each
hybrid always lay in the regions of the space devoted to the classes of its two parents and

 . The two components are pulled together
of SNE. We took a dataset containing  pictures of each of the digits 2,3,4 and added

 hybrid digit-pictures that were each constructed by picking new examples of two of

are constrained to have mixing proportions of 
of the hybrids and only 

mization, 
in dening the local neighborhoods, a step size of for each position update of 
gradient, and used a constant jitter of 

makes it possible to map a circle onto a line without losing any near neighbor relationships
or introducing any new ones. Points near one cut point on the circle can mapped to a
mixture of two points, one near one end of the line and one near the other end. Obviously,
the location of the cut on the two-dimensional circle gets decided by which pairs of mixture
components split rst during the stochastic optimization. For certain optimization param-
eters that control the ease with which two mixture components can be pulled apart, only
a single cut in the circle is made. For other parameter settings, however, the circle may
fragment into two or more smaller line-segments, each of which is topologically correct
but which may not be linked to each other.

never in the region devoted to the third class. For this example we used a perplexity of 
 . Our very simple mixture version of SNE also

times the

	

 .

The example with hybrid digits demonstrates that even the most primitive mixture version
of SNE can deal with ambiguous high-dimensional objects that need to be mapped to two
widely separated regions of the low-dimensional space. More work needs to be done before
SNE is efcient enough to cope with large matrices of document-word counts, but it is
the only dimensionality reduction method we know of that promises to treat homonyms
sensibly without going back to the original documents to disambiguate each occurrence of
the homonym.

1We used a threshold of

. At threshold the force was

space has a natural scale because the variance of the Gaussian used to determine




nats per unit length. The low-d
is x ed at 0.5.







2



2










2




B
B




C
8

















B
7
B





8

8


B
0
B





















5 Practical optimization strategies
Our current method of reducing the SNE cost is to use steepest descent with added jitter
that is slowly reduced. This produces quite good embeddings, which demonstrates that the
SNE cost function is worth minimizing, but it takes several hours to nd a good embedding



The time per iteration could be reduced considerably by ignoring pairs of points for which
is xed during the learning, it is
natural to sparsify it by replacing all entries below a certain threshold with zero and renor-

for just  datapoints so we clearly need a better search algorithm.
all four of
G are small. Since the matrix
#G0
#G0
G

malizing. Then pairsH#
for which both5 andFG are zero can be ignored from gradient
 and 0
calculations if both 0
G are small. This can in turn be determined in logarithmic
 . Com-
as K-D trees, ball-trees and AD-trees, since the 0 depend only on

putational physics has attacked exactly this same complexity when performing multibody
gravitational or electrostatic simulations using, for example, the fast multipole method.

time in the size of the training set by using sophisticated geometric data structures such



42

2

In the mixture version of SNE there appears to be an interesting way of avoiding local
optima that does not involve annealing the jitter. Consider two components in the mixture
for an object that are far apart in the low-dimensional space. By raising the mixing propor-
tion of one and lowering the mixing proportion of the other, we can move probability mass
from one part of the space to another without it ever appearing at intermediate locations.
This type of probability wormhole seems like a good way to avoid local optima that arise
because a cluster of low-dimensional points must move through a bad region of the space
in order to reach a better one.

Yet another search method, which we have used with some success on toy problems, is
to provide extra dimensions in the low-dimensional space but to penalize non-zero values
on these dimensions. During the search, SNE will use the extra dimensions to go around
lower-dimensional barriers but as the penalty on using these dimensions is increased, they
will cease to be used, effectively constraining the embedding to the original dimensionality.

6 Discussion and Conclusions
Preliminary experiments show that we can nd good optima by rst annealing the perplex-
(using high jitter) and only reducing the jitter after the nal perplexity has been

sian centered on each high-dimensional point is very big so that the distribution across
neighbors is almost uniform. It is clear that in the high variance limit, the contribution of
to the SNE cost function is just as important for distant neighbors as for
is very large, it can be shown that SNE is equivalent to minimizing the
mismatch between squared distances in the two spaces, provided all the squared distances

ities )
reached. This raises the question of what SNE is doing when the variance,)
:*&+
close ones. When)
from an object are rst normalized by subtracting off their antigeometric mean,

 , of the Gaus-

 :

:

06

(10)

(11)

(12)




%&%
!

%'%

%&%

!
%'%





@



;8







4


4

$

*&+




*'+,

is the number of objects.

where

#






,




























)

#





,
8









#




2


2



)

#






8











This mismatch is very similar to stress functions used in nonmetric versions of MDS,
and enables us to understand the large-variance limit of SNE as a particular variant of such
procedures. We are still investigating the relationship to metric MDS and to PCA.

SNE can also be seen as an interesting special case of Linear Relational Embedding (LRE)
[11].
In LRE the data consists of triples (e.g. Colin has-mother Victoria) and the task
is to predict the third term from the other two. LRE learns an N-dimensional vector for
each object and an NxN-dimensional matrix for each relation. To predict the third term in
a triple, LRE multiplies the vector representing the rst term by the matrix representing
the relationship and uses the resulting vector as the mean of a Gaussian. Its predictive
distribution for the third term is then determined by the relative densities of all known
objects under this Gaussian. SNE is just a degenerate version of LRE in which the only
relationship is near and the matrix representing this relationship is the identity.

In summary, we have presented a new criterion, Stochastic Neighbor Embedding, for map-
ping high-dimensional points into a low-dimensional space based on stochastic selection
of similar neighbors. Unlike self-organizing maps, in which the low-dimensional coor-
dinates are xed to a grid and the high-dimensional ends are free to move, in SNE the
high-dimensional coordinates are xed to the data and the low-dimensional points move.
Our method can also be applied to arbitrary pairwise dissimilarities between objects if such
are available instead of (or in addition to) high-dimensional observations. The gradient of
the SNE cost function has an appealing push-pull property in which the forces acting on

2 to bring it closer to points it is under-selecting and further from points it is over-selecting

as its neighbor. We have shown results of applying this algorithm to image and document
collections for which it sensibly placed similar objects nearby in a low-dimensional space
while keeping dissimilar objects well separated.

Most importantly, because of its probabilistic formulation, SNE has the ability to be ex-
tended to mixtures in which ambiguous high-dimensional objects (such as the word bank)
can have several widely-separated images in the low-dimensional space.
Acknowledgments We thank the anonymous referees and several visitors to our poster for helpful
suggestions. Yann LeCun provided digit and NIPS text data. This research was funded by NSERC.

