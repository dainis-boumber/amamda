Abstract

We consider the use of language models
whose size and accuracy are intermedi-
ate between dierent order n-gram models.
Two types of models are studied in partic-
ular. Aggregate Markov models are class-
based bigram models in which the map-
ping from words to classes is probabilis-
tic. Mixed-order Markov models combine
bigram models whose predictions are con-
ditioned on dierent words. Both types
of models are trained by Expectation-
Maximization (EM) algorithms for maxi-
mum likelihood estimation. We examine
smoothing procedures in which these mod-
els are interposed between dierent order
n-grams. This is found to signicantly re-
duce the perplexity of unseen word combi-
nations.

1 Introduction

The purpose of a statistical language model is to as-
sign high probabilities to likely word sequences and
low probabilities to unlikely ones. The challenge
here arises from the combinatorially large number
of possibilities, only a fraction of which can ever be
observed.
In general, language models must learn
to recognize word sequences that are functionally
similar but lexically distinct. The learning problem,
one of generalizing from sparse data, is particularly
acute for large-sized vocabularies (Jelinek, Mercer,
and Roukos, 1992).

The simplest models of natural language are n-
gram Markov models.
In these models, the prob-
ability of each word depends on the n  1 words
that precede it. The problems in estimating ro-
bust models of this form are well-documented. The
number of parametersor transition probabilities
scales as V n, where V is the vocabulary size. For

typical models (e.g., n = 3, V = 104), this num-
ber exceeds by many orders of magnitude the total
number of words in any feasible training corpus.

The transition probabilities in n-gram models are
estimated from the counts of word combinations in
the training corpus. Maximum likelihood (ML) esti-
mation leads to zero-valued probabilities for unseen
n-grams. In practice, one adjusts or smoothes (Chen
and Goodman, 1996) the ML estimates so that
the language model can generalize to new phrases.
Smoothing can be done in many waysfor example,
by introducing articial counts, backing o to lower-
order models (Katz, 1987), or combining models by
interpolation (Jelinek and Mercer, 1980).

Often a great deal of information is lost in the
smoothing procedure. This is due to the great dis-
crepancy between n-gram models of dierent order.
The goal of this paper is to investigate models that
are intermediate, in both size and accuracy, between
dierent order n-gram models. We show that such
models can intervene between dierent order n-
grams in the smoothing procedure. Experimentally,
we nd that this signicantly reduces the perplexity
of unseen word combinations.

The language models in this paper were evalu-
ated on the ARPA North American Business News
(NAB) corpus. All our experiments used a vo-
cabulary of sixty-thousand words, including tokens
for punctuation, sentence boundaries, and an un-
known word token standing for all out-of-vocabulary
words. The training data consisted of approxi-
mately 78 million words (three million sentences);
the test data, 13 million words (one-half million
sentences). All sentences were drawn randomly
without replacement from the NAB corpus. All
perplexity gures given in the paper are com-
puted by combining sentence probabilities; the prob-
ability of sentence w0w1    wnwn+1 is given by
Qn+1
i=1 P (wi|w0    wi1), where w0 and wn+1 are
the start- and end-of-sentence markers, respectively.

Though not reported below, we also conrmed that
the results did not vary signicantly for dierent ran-
domly drawn test sets of the same size.

The organization of this paper is as follows.
In Section 2, we examine aggregate Markov mod-
els, or class-based bigram models (Brown et al.,
1992) in which the mapping from words to classes
is probabilistic. We describe an iterative algo-
rithm for discovering soft word classes, based on
the Expectation-Maximization (EM) procedure for
maximum likelihood estimation (Dempster, Laird,
and Rubin, 1977). Several features make this algo-
rithm attractive for large-vocabulary language mod-
eling: it has no tuning parameters, converges mono-
tonically in the log-likelihood, and handles proba-
bilistic constraints in a natural way. The number
of classes, C, can be small or large depending on
the constraints of the modeler. Varying the number
of classes leads to models that are intermediate be-
tween unigram (C = 1) and bigram (C = V ) models.
In Section 3, we examine another sort of inter-
mediate model, one that arises from combinations
of non-adjacent words. Language models using such
combinations have been proposed by Huang et al.
(1993), Ney, Essen, and Kneser (1994), and Rosen-
feld (1996), among others. We consider specically
the skip-k transition matrices, M (wtk, wt), whose
predictions are conditioned on the kth previous word
in the sentence.
(The value of k determines how
many words one skips back to make the predic-
tion.) These predictions, conditioned on only a sin-
gle previous word in the sentence, are inherently
weaker than those conditioned on all k previous
words. Nevertheless, by combining several predic-
tions of this form (for dierent values of k), we can
create a model that is intermediate in size and ac-
curacy between bigram and trigram models.

Mixed-order Markov models express the predic-
tions P (wt|wt1, wt2, . . . , wtm) as a convex com-
bination of skip-k transition matrices, M (wtk, wt).
We derive an EM algorithm to learn the mixing co-
ecients, as well as the elements of the transition
matrices. The number of transition probabilities in
these models scales as mV 2, as opposed to V m+1.
Mixed-order models are not as powerful as trigram
models, but they can make much stronger predic-
tions than bigram models. The reason is that quite
often the immediately preceding word has less pre-
dictive value than earlier words in the same sentence.
In Section 4, we use aggregate and mixed-order
models to improve the probability estimates from
n-grams. This is done by interposing these models
between dierent order n-grams in the smoothing
procedure. We compare our results to a baseline tri-

gram model that backs o to bigram and unigram
models. The use of intermediate models is found
to reduce the perplexity of unseen word combina-
tions by over 50%.

In Section 5, we discuss some extensions to these
models and some open problems for future research.
We conclude that aggregate and mixed-order models
provide a compelling alternative to language models
based exclusively on n-grams.

2 Aggregate Markov models

In this section we consider how to construct class-
based bigram models (Brown et al., 1992). The
problem is naturally formulated as one of hidden
variable density estimation. Let P (c|w1) denote the
probability that word w1 is mapped into class c.
Likewise, let P (w2|c) denote the probability that
words in class c are followed by the word w2. The
class-based bigram model predicts that word w1 is
followed by word w2 with probability

P (w2|w1) =

C
X
c=1

P (w2|c)P (c|w1),

(1)

where C is the total number of classes. The hidden
variable in this problem is the class label c, which
is unknown for each word w1. Note that eq. (1)
represents the V 2 elements of the transition matrix
P (w2|w1) in terms of the 2CV elements of P (w2|c)
and P (c|w1).

The Expectation-Maximization (EM) algorithm
(Dempster, Laird, and Rubin, 1977) is an iterative
procedure for estimating the parameters of hidden
variable models. Each iteration consists of two steps:
an E-step which computes statistics over the hidden
variables, and an M-step which updates the param-
eters to reect these statistics.

The EM algorithm for aggregate Markov models
is particularly simple. The E-step is to compute, for
each bigram w1w2 in the training set, the posterior
probability

P (c|w1, w2) =

P (w2|c)P (c|w1)

Pc P (w2|c)P (c|w1)

.

(2)

Eq. (2) gives the probability that word w1 was as-
signed to class c, based on the observation that it
was followed by word w2. The M-step uses these
posterior probabilities to re-estimate the model pa-
rameters. The updates for aggregate Markov models
are:

P (c|w1)  Pw N (w1, w)P (c|w1, w)
Pwc N (w1, w)P (c|w1, w)
P (w2|c)  Pw N (w, w2)P (c|w, w2)
Pww N (w, w)P (c|w, w)

,

,

(3)

(4)

C train
964.7
1
771.2
2
541.9
4
8
399.5
328.8
16
32
278.9
V 123.6 

test
964.9
772.2
543.6
401.5
331.8
283.2

Table 1: Perplexities of aggregate Markov models on
the training and test sets; C is the number of classes.
The case C = 1 corresponds to a ML unigram model;
C = V , to a ML bigram model.

0

0.2

0.4

0.6

winning assignment probability

0.8

1

Figure 2: Histogram of the winning assignment
probabilities, maxc P (c|w), for the three hundred
most commonly occurring words.

where N (w1, w2) denotes the number of counts of
w1w2 in the training set. These updates are guar-
anteed to increase the overall log-likelihood,

 = X
w1w2

N (w1, w2) ln P (w2|w1),

(5)

at each iteration. In general, they converge to a local
(though not global) maximum of the log-likelihood.
The perplexity V  is related to the log-likelihood by
V  = e/N , where N is the total number of words
processed.

Though several algorithms (Brown et al., 1992;
Pereira, Tishby, and Lee, 1993) have been proposed
for performing the decomposition in eq. (1), it is
worth noting that only the EM algorithm directly
optimizes the log-likelihood in eq. (5). This has ob-
vious advantages if the goal of nding word classes is
to improve the perplexity of a language model. The
EM algorithm also handles probabilistic constraints
in a natural way, allowing words to belong to more
than one class if this increases the overall likelihood.
Our approach diers in important ways from the
use of hidden Markov models (HMMs) for class-
based language modeling (Jelinek et al., 1992).

While HMMs also use hidden variables to represent
word classes, the dynamics are fundamentally dif-
ferent. In HMMs, the hidden state at time t + 1 is
predicted (via the state transition matrix) from the
hidden state at time t. On the other hand, in aggre-
gate Markov models, the hidden state at time t + 1
is predicted (via the matrix P (ct+1|wt)) from the
word at time t. The state-to-state versus word-to-
state dynamics lead to dierent learning algorithms.
For example, the Baum-Welch algorithm for HMMs
requires forward and backward passes through each
training sentence, while the EM algorithm we use
does not.

We trained aggregate Markov models with 2, 4,
8, 16, and 32 classes. Figure 1 shows typical plots
of the training and test set perplexities versus the
number of iterations of the EM algorithm. Clearly,
the two curves are very close, and the monotonic
decrease in test set perplexity strongly suggests lit-
tle if any overtting, at least when the number of
classes is small compared to the number of words in
the vocabulary. Table 1 shows the nal perplexities
(after thirty-two iterations of EM) for various ag-
gregate Markov models. These results conrm that
aggregate Markov models are intermediate in accu-
racy between unigram (C = 1) and bigram (C = V )
models.

The aggregate Markov models were also observed
to discover meaningful word classes. Table 2 shows,
for the aggregate model with C = 32 classes, the
most probable class assignments of the three hun-
dred most commonly occurring words. To be precise,
for each class c, we have listed the words for which
c = arg maxc P (c|w). Figure 2 shows a histogram of
the winning assignment probabilities, maxc P (c|w),
for these words. Note that the winning assignment
probabilities are distributed broadly over the inter-
val [ 1
C , 1]. This demonstrates the utility of allowing
soft membership classes: for most words, the max-
imum likelihood estimates of P (c|w) do not corre-
spond to a winner-take-all assignment, and therefore
any method that assigns each word to a single class
(hard clustering), such as those used by Brown et
al. (1992) or Ney, Essen, and Kneser (1994), would
lose information.

We conclude this section with some nal com-
ments on overtting. Our models were trained by
thirty-two iterations of EM, allowing for nearly com-
plete convergence in the log-likelihood. Moreover,
we did not implement any ooring constraints1 on

1It is worth noting, in this regard, that individual
zeros in the matrices P (w2|c) and P (c|w1) do not nec-
essarily give rise to zeros in the matrix P (w2|w1), as
computed from eq. (1).

1000

900

800

700

600

500

400

300

1000

900

800

700

600

500

400

300

)
t
s
e

t
(

y
t
i
x
e
p
r
e
p

l

i

)
n
a
r
t
(

y
t
i
x
e
p
r
e
p

l

200
0

5

10

15

20
iteration of EM
(a)

25

30

200
0

5

10

25

30

15

20
iteration of EM
(b)

Figure 1: Plots of (a) training and (b) test perplexity versus number of iterations of the EM algorithm, for
the aggregate Markov model with C = 32 classes.

1 as cents made make take

2

ago day earlier Friday Monday month quarter
reported said Thursday trading Tuesday
Wednesday h. . . i

3 even get to

4

based days down home months up work years
h%i

5 those h,i hi
6 h.i h?i

7

eighty fty forty ninety seventy sixty thirty
twenty h(i hi

8 can could may should to will would
9 about at just only or than h&i h;i
10 economic high interest much no such tax united

well

11 president
12 because do how if most say so then think very

what when where

13 according back expected going him plan used way
15 dont I people they we you

16

Bush company court department more ocials
police retort spokesman

17 former the

18

American big city federal general house military
national party political state union York

19 billion hundred million nineteen
20 did hi hi
21 but called San h:i hstart-of-sentencei

22

23

bank board chairman end group members
number oce out part percent price prices rate
sales shares use
a an another any dollar each rst good her his its
my old our their this

24 long Mr. year

25

business California case companies corporation
dollars incorporated industry law money
thousand time today war week h)i hunknowni

26 also government he it market she that there

which who

27 A. B. C. D. E. F. G. I. L. M. N. P. R. S. T. U.
28 both foreign international major many new oil

other some Soviet stock these west world
after all among and before between by during for
from in including into like of o on over since
through told under until while with
eight fteen ve four half last next nine oh one
second seven several six ten third three twelve
two zero h-i

29

30

31 are be been being had has have is its not still

was were

32 chief exchange news public service trade

Table 2: Most probable assignments for the 300 most frequent words in an aggregate Markov model with
C = 32 classes. Class 14 is absent because it is not the most probable class for any of the selected words.)

the probabilities P (c|w1) or P (w2|c). Nevertheless,
in all our experiments, the ML aggregate Markov
models assigned non-zero probability to all the bi-
grams in the test set. This suggests that for large
vocabularies there is a useful regime 1  C  V
in which aggregate models do not suer much from
overtting. In this regime, aggregate models can be
relied upon to compute the probabilities of unseen
word combinations. We will return to this point in

Section 4, when we consider how to smooth n-gram
language models.

3 Mixed-order Markov models

One of the drawbacks of n-gram models is that their
size grows rapidly with their order. In this section,
we consider how to make predictions based on a con-
vex combination of pairwise correlations. This leads

to language models whose size grows linearly in the
number of words used for each prediction.

For each k > 0, the skip-k transition matrix
M (wtk, wt) predicts the current word from the
kth previous word in the sentence. A mixed-order
Markov model combines the information in these
matrices for dierent values of k. Let m denote
the number of bigram models being combined. The
probability distribution for these models has the
form:

P (wt|wt1, . . . , wtm) =

(6)

m
X
k=1

k(wtk) Mk(wtk, wt)

k1
Y
j=1

[1  j (wtj)].

The terms in this equation have a simple interpreta-
tion. The V  V matrices Mk(w, w) in eq. (6) de-
ne the skip-k stochastic dependency of w at some
position t on w at position t  k; the parameters
k(w) are mixing coecients that weight the predic-
tions from these dierent dependencies. The value of
k(w) can be interpreted as the probability that the
model, upon seeing the word wtk, looks no further
back to make its prediction (Singer, 1996). Thus the
model predicts from wt1 with probability 1(wt1),
from wt2 with probability [1  1(wt1)]2(wt2),
and so on. Though included in eq. (6) for cosmetic
reasons, the parameters m(w) are actually xed to
unity so that the model never looks further than m
words back.

We can view eq. (6) as a hidden variable model.
Imagine that we adopt the following strategy to pre-
dict the word at time t. Starting with the previous
word, we toss a coin (with bias 1(wt1)) to see if
this word has high predictive value. If the answer
is yes, then we predict from the skip-1 transition
matrix, M1(wt1, wt). Otherwise, we shift our at-
tention one word to the left and repeat the process.
If after m  1 tosses we have not settled on a pre-
diction, then as a last resort, we make a prediction
using Mm(wtm, wt). The hidden variables in this
process are the outcomes of the coin tosses, which
are unknown for each word wtk.

Viewing the model in this way, we can derive an
EM algorithm to learn the mixing coecients k(w)
and the transition matrices2 Mk(w, w). The E-step
of the algorithm is to compute, for each word in the

2Note that the ML estimates of Mk(w, w) do not
depend only on the raw counts of k-separated bigrams;
they are also coupled to the values of the mixing coef-
cients, k(w). In particular, the EM algorithm adapts
the matrix elements to the weighting of word combina-
tions in eq. (6). The raw counts of k-separated bigrams,
however, do give good initial estimates.

i

)
n
a
r
t
(

y
t
i
x
e
p
r
e
p

l

110

105

100

95

90

85

80

75

70
0

1

2

iteration of EM

3

4

Figure 3: Plot of (training set) perplexity versus
number of iterations of the EM algorithm. The re-
sults are for the m = 4 mixed-order Markov model.

training set, the posterior probability that it was
generated by Mk(wtk, wt). Denoting these poste-
rior probabilities by k(t), we have:

k(t) =

k(wtk)Mk(wtk, wt) Qk1

j=1 [1j(wtj )]

P (wt|wt1, wt2, . . . , wtm)

(7)

,

where the denominator is given by eq. (6). The
M-step of the algorithm is to update the parame-
ters k(w) and Mk(w, w) to reect the statistics in
eq. (7). The updates for mixed-order Markov models
are given by:

k(w)  Pt (w, wtk)k(t)

Pt Pm

j=k (w, wtk)j (t)

,

(8)

Mk(w, w)  Pt (w, wtk)(w, wt)k(t)

Pt (w, wtk)k(t)

, (9)

where the sums are over all the sentences in the
training set, and (w, w) = 1 i w = w.

We trained mixed-order Markov models with 2 
m  4. Figure 3 shows a typical plot of the train-
ing set perplexity as a function of the number of
iterations of the EM algorithm. Table 3 shows the
nal perplexities on the training set (after four iter-
ations of EM). Mixed-order models cannot be used
directly on the test set because they predict zero
probability for unseen word combinations. Unlike
standard n-gram models, however, the number of
unseen word combinations actually decreases with
the order of the model. The reason for this is that
mixed-order models assign nite probability to all n-
grams w1w2 . . . wn for which any of the k-separated
bigrams wkwn are observed in the training set. To
illustrate this point, Table 3 shows the fraction of
words in the test set that were assigned zero proba-
bility by the mixed-order model. As expected, this
fraction decreases monotonically with the number of
bigrams that are mixed into each prediction.

m train missing
0.045
1
2
0.014
0.0063
3
4
0.0037

123.2
89.4
77.9
72.4

Table 3: Results for ML mixed-order models; m de-
notes the number of bigrams that were mixed into
each prediction. The rst column shows the per-
plexities on the training set. The second shows the
fraction of words in the test set that were assigned
zero probability. The case m = 1 corresponds to a
ML bigram model.

Clearly, the success of mixed-order models de-
pends on the ability to gauge the predictive value
of each word, relative to earlier words in the same
sentence. Let us see how this plays out for the
second-order (m = 2) model in Table 3.
In this
model, a small value for 1(w) indicates that the
word w typically carries less information that the
word that precedes it. On the other hand, a large
value for 1(w) indicates that the word w is highly
predictive. The ability to learn these relationships
is conrmed by the results in Table 4. Of the three-
hundred most common words, Table 4 shows the
fty with the lowest and highest values of 1(w).
Note how low values of 1(w) are associated with
prepositions, mid-sentence punctuation marks, and
conjunctions, while high values are associated with
contentful words and end-of-sentence markers. (A
particularly interesting dichotomy arises for the two
forms a and an of the indenite article; the lat-
ter, because it always precedes a word that begins
with a vowel, is inherently more predictive.) These
results underscore the importance of allowing the
coecients 1(w) to depend on the context w, as
opposed to being context-independent (Ney, Essen,
and Kneser, 1994).

4 Smoothing

Smoothing plays an essential role in language models
where ML predictions are unreliable for rare events.
In n-gram modeling, it is common to adopt a re-
cursive strategy, smoothing bigrams by unigrams,
trigrams by bigrams, and so on. Here we adopt a
similar strategy, using the (m  1)th mixed-order
model to smooth the mth one. At the root of
our smoothing procedure, however, lies not a uni-
gram model, but an aggregate Markov model with
C > 1 classes. As shown in Section 2, these models
assign nite probability to all word combinations,

0.1 < 1(w) < 0.7

h-i and of hi or h;i to h,i h&i by with S. from
nine were for that eight low seven the h(i h:i six
are not against was four between a their two
three its hunknowni B. on as is hi ve h)i into
C. M. her him over than A.

0.96 < 1(w)  1

ocials prices which go way he last they earlier
an Tuesday there foreign quarter she former
federal dont days Friday next Wednesday h%i
Thursday I Monday Mr. we half based part
United its years going nineteen thousand months
hi million very cents San ago U. percent billion
h?i according h.i

Table 4: Words with low and high values of 1(w)
in an m = 2 mixed order model.

C validation
163.615
1
162.982
2
161.513
4
8
161.327
160.034
16
32
159.247

test

167.112
166.193
164.363
164.104
162.686
161.683

unseen
293175
259360
200067
190178
164673
150958

Table 5: Perplexities of bigram models smoothed by
aggregate Markov models with dierent numbers of
classes (C).

even those that are not observed in the training set.
Hence, they can legitimately replace unigrams as the
base model in the smoothing procedure.

Let us rst examine the impact of replacing uni-
gram models by aggregate models at the root of the
smoothing procedure. To this end, a held-out inter-
polation algorithm (Jelinek and Mercer, 1980) was
used to smooth an ML bigram model with the aggre-
gate Markov models from Section 2. The smoothing
parameters, one for each row of the bigram transi-
tion matrix, were estimated from a validation set the
same size as the test set. Table 5 gives the nal per-
plexities on the validation set, the test set, and the
unseen bigrams in the test set. Note that smooth-
ing with the C = 32 aggregate Markov model has
nearly halved the perplexity of unseen bigrams, as
compared to smoothing with the unigram model.

Let us now examine the recursive use of mixed-
order models to obtain smoothed probability esti-
mates. Again, a held-out interpolation algorithm
was used to smooth the mixed-order Markov models
from Section 3. The mth mixed-order model had
mV smoothing parameters k(w), corresponding to
the V rows in each skip-k transition matrix. The

m validation
1
2
3
4

160.1
135.3
131.4
131.2

test
161.3
136.9
133.5
133.7

Table 6: Perplexities of smoothed mixed-order mod-
els on the validation and test sets.

mth mixed-order model was smoothed by discount-
ing the weight of each skip-k prediction, then ll-
ing in the leftover probability mass by a lower-order
model. In particular, the discounted weight of the
skip-k prediction was given by

[1  k(wtk)]k(wtk)

k1
Y
j=1

[1  j (wtj)]

, (10)

leaving a total mass of

m
X
k=1

k(wtk)k(wtk)

k1
Y
j=1

[1  j (wtj)]

(11)

for the (m  1)th mixed-order model.
(Note that
the m = 1 mixed-order model corresponds to a ML
bigram model.)

Table 6 shows the perplexities of the smoothed
mixed-order models on the validation and test sets.
An aggregate Markov model with C = 32 classes
was used as the base model in the smoothing proce-
dure. The rst row corresponds to a bigram model
smoothed by a aggregate Markov model; the second
row corresponds to an m = 2 mixed-order model,
smoothed by a ML bigram model, smoothed by an
aggregate Markov model; the third row corresponds
to an m = 3 mixed-order model, smoothed by a
m = 2 mixed-order model, smoothed by a ML bi-
gram model, etc. A signicant decrease in perplex-
ity occurs in moving to the smoothed m = 2 mixed-
order model. On the other hand, the dierence in
perplexity for higher values of m is not very dra-
matic.

Our last experiment looked at the smoothing of
a trigram model. Our baseline was a ML trigram
model that backed o 3 to bigrams (and when nec-
essary, unigrams) using the Katz backo procedure
(Katz, 1987). In this procedure, the predictions of
the ML trigram model are discounted by an amount
determined by the Good-Turing coecients; the left-
over probability mass is then lled in by the backo

3We used a backo procedure (instead of interpo-
lation) to avoid the estimation of trigram smoothing
parameters.

backo
baseline
mixed

test
95.2
79.8

unseen
2799
1363

Table 7: Perplexities of two smoothed trigram mod-
els on the test set and the subset of unseen word
combinations. The baseline model backed o to bi-
grams and unigrams; the other backed o to the
m = 2 model in Table 6.

model. We compared this to a trigram model that
backed o to the m = 2 model in Table 6. This was
handled by a slight variant of the Katz procedure
(Dagan, Pereira, and Lee, 1994) in which the mixed-
order model substituted for the backo model.

One advantage of this smoothing procedure is that
it is straightforward to assess the performance of dif-
ferent backo models. Because the backo models
are only consulted for unseen word combinations,
the perplexity on these word combinations serves as
a reasonable gure-of-merit.

Table 7 shows those perplexities for the two
smoothed trigram models (baseline and backo).
The mixed-order smoothing was found to reduce
the perplexity of unseen word combinations by 51%.
Also shown in the table are the perplexities on the
entire test set. The overall perplexity decreased
by 16%a signicant amount considering that only
24% of the predictions involved unseen word com-
binations and required backing o from the trigram
model.

The models in Table 7 were constructed from all
n-grams (1  n  3) observed in the training data.
Because many n-grams occur very infrequently, a
natural question is whether truncated models, which
omit low-frequency n-grams from the training set,
can perform as well as untruncated ones. The ad-
vantage of truncated models is that they do not need
to store nearly as many non-zero parameters as un-
truncated models. The results in Table 8 were ob-
tained by dropping trigrams that occurred less than
t times in the training corpus. The t = 1 row cor-
responds to the models in Table 7. The most in-
teresting observation from the table is that omitting
very low-frequency trigrams does not decrease the
quality of the mixed-order model, and may in fact
slightly improve it. This contrasts with the standard
backo model, in which truncation causes signicant
increases in perplexity.

t
1
2
3
4
5

baseline mixed
79.8
78.3
79.6
81.1
82.4

95.2
98.6
101.7
104.2
106.2

trigrams(106) missing

25.4
6.1
3.3
2.3
1.7

0.24
0.32
0.36
0.38
0.41

Table 8: Eect of truncating trigrams that occur
less than t times. The table shows the baseline and
mixed-order perplexities on the test set, the num-
ber of distinct trigrams with t or more counts, and
the fraction of trigrams in the test set that required
backing o.

5 Discussion

Our results demonstrate the utility of language mod-
els that are intermediate in size and accuracy be-
tween dierent order n-gram models. The two
models considered in this paper were hidden vari-
able Markov models trained by EM algorithms for
maximum likelihood estimation. Combinations of
intermediate-order models were also investigated by
Rosenfeld (1996). His experiments used the 20,000-
word vocabulary Wall Street Journal corpus, a pre-
decessor of the NAB corpus. He trained a maximum-
entropy model consisting of unigrams, bigrams, tri-
grams, skip-2 bigrams and trigrams; after selecting
long-distance bigrams (word triggers) on 38 million
words, the model was tested on a held-out 325 thou-
sand word sample. Rosenfeld reported a test-set
perplexity of 86, a 19% reduction from the 105 per-
plexity of a baseline trigram backo model. In our
experiments, the perplexity gain of the mixed-order
model ranged from 16% to 22%, depending on the
amount of truncation in the trigram model.

While Rosenfelds results and ours are not di-
rectly comparable, both demonstrate the utility of
mixed-order models.
It is worth discussing, how-
ever, the dierent approaches to combining infor-
mation from non-adjacent words. Unlike the max-
imum entropy approach, which allows one to com-
bine many non-independent features, ours calls for
a careful Markovian decomposition. Rosenfeld ar-
gues at length against nave linear combinations in
favor of maximum entropy methods. His arguments
do not apply to our work for several reasons. First,
we use a large number of context-dependent mixing
parameters to optimize the overall likelihood of the
combined model. Thus, the weighting in eq. (6) en-
sures that the skip-k predictions are only invoked
when the context is appropriate. Second, we adjust
the predictions of the skip-k transition matrices (by

EM) so that they match the contexts in which they
are invoked. Hence, the count-based models are in-
terpolated in a way that is consistent with their
eventual use.

Training eciency is another issue in evaluating
language models. The maximum entropy method
requires very long training times: e.g., 200 CPU-
days in Rosenfelds experiments. Our methods re-
quire signicantly less; for example, we trained the
smoothed m = 2 mixed-order model, from start to
nish,
in less than 12 CPU-hours (while using a
larger training corpus). Even accounting for dier-
ences in processor speed, this amounts to a signi-
cant mismatch in overall training time.

In conclusion, let us mention some open problems
for further research. Aggregate Markov models can
be viewed as approximating the full bigram tran-
sition matrix by a matrix of lower rank.
(From
eq. (1), it should be clear that the rank of the class-
based transition matrix is bounded by the num-
ber of classes, C.) As such, there are interesting
parallels between Expectation-Maximization (EM),
which minimizes the approximation error as mea-
sured by the KL divergence, and singular value de-
composition (SVD), which minimizes the approxi-
mation error as measured by the L2 norm (Press
et al., 1988; Schutze, 1992). Whereas SVD nds a
global minimum in its error measure, however, EM
only nds a local one. It would clearly be desirable
to improve our understanding of this fundamental
problem.

In this paper we have focused on bigram models,
but the ideas and algorithms generalize in a straight-
forward way to higher-order n-grams. Aggregate
models based on higher-order n-grams (Brown et al.,
1992) might be able to capture multi-word struc-
tures such as noun phrases. Likewise, trigram-based
mixed-order models would be useful complements to
4-gram and 5-gram models, which are not uncom-
mon in large-vocabulary language modeling.

A nal

issue that needs to be addressed is
scalingthat is, how the performance of these mod-
els depends on the vocabulary size and amount
of training data. Generally, one expects that the
sparser the data, the more helpful are models that
can intervene between dierent order n-grams. Nev-
ertheless, it would be interesting to see exactly how
this relationship plays out for aggregate and mixed-
order Markov models.

Acknowledgments

We thank Michael Kearns and Yoram Singer for use-
ful discussions, the anonymous reviewers for ques-
tions and suggestions that helped improve the paper,

and Don Hindle for help with his language modeling
tools, which we used to build the baseline models
considered in the paper.

R. Rosenfeld. 1996. A Maximum Entropy Approach
to Adaptive Statistical Language Modeling. Com-
puter Speech and Language, 10:187228.

H. Schutze. 1992. Dimensions of Meaning. In Pro-
ceedings of Supercomputing, 787796. Minneapolis
MN.

Y. Singer. 1996. Adaptive Mixtures of Probabilistic
Transducers.
In D. Touretzky, M. Mozer, and M.
Hasselmo (eds). Advances in Neural Information
Processing Systems 8:381387. MIT Press: Cam-
bridge, MA.

