By now I hope it’s clear that, even without the refinements to be discussed in later chapters, the TR model is certainly good for retrieval. (At least in principle! I’ll describe in more detail how retrievals are actually implemented in Chapter 10.) But what about updates?1 Conventional wisdom has always been that a given data structure can be good for either retrieval or update, but not both. In a direct-image implementation, for example, indexes are generally held to be good for retrieval but bad for update. So what about TR? How are updates done in TR? This chapter examines this question. 
To repeat from Chapter 5, then, the operators we need to consider are as follows (see Section 5.3): 
■■INSERT: Insert a new record. 
■■DELETE: Delete the record “passing through” cell [i,j] of the Record Reconstruction Table. 
■■UPDATE: Update the record “passing through” cell [i,j] of the Record Reconstruction Table. 
Note: The notion of a record “passing through” some cell of the Record Reconstruction Table was also explained in Section 5.3.
Section 6.2 immediately following discusses the three update operators in general terms; Sections 6.3 then presents a detailed example, and Section 6.4 discusses the swap algorithm. Section 6.5 briefly describes an alternative implementation technique that makes use of an overflow structure. Finally, Section 6.6 offers some observations regarding the performance aspects of TR update operations.
It’s convenient to begin by discussing the INSERT operator specifically. Consider the suppliers file shown in Fig. 6.1 (it’s the same as the one shown in Fig. 4.1 in Chapter 4, except that the last record, the one for supplier S3, has been omitted). Figs. 6.2 and 6.3 show the corresponding Field Values Table and a corresponding Record Reconstruction Table, respectively. Exercise 6: Check that these tables are correct.
Now suppose the user asks the system to insert the following tuple into the suppliers relation:
In terms of the file of Fig. 6.1, of course, we can imagine a new record corresponding to this tuple simply being appended at the end, in position 5 (since record ordering within files is arbitrary). If we now rebuild the Field Values Table, it’ll appear as shown on the left-hand side of Fig. 6.4 (a copy of the Field Values Table from Fig. 4.3 in Chapter 4). And if we then build a corresponding Record Reconstruction Table, it might appear as shown on the right-hand side of Fig. 6.4
As you can see by comparing Fig. 6.4 with Figs. 6.2 and 6.3, respectively, inserting supplier S3 has caused both the Field Values Table and the Record Reconstruction Table to change dramatically. It follows that INSERT operations have the potential to be quite disruptive, and hence (possibly) to display very poor performance. What can be done about this problem? 
Well, let me say right away that the effect on the Field Values Table is actually not as dramatic as it might appear. Although I’ve been calling it a table and showing it as a table in figures like Fig. 6.4, the Field Values Table doesn’t necessarily have to be physically stored as a table; in fact, it almost certainly won’t be. Much more likely, it’ll be stored “column-wise” as a set of vectors (one-dimensional arrays), or possibly as a set of chained lists, one such vector or list for each column. Indeed, such an implementation is virtually certain to be used in practice if the refinements to be discussed in Chapters 8 and 9 are adopted, as we’ll see.2 
For definiteness, let’s assume a vector implementation. Of course, those vectors will be kept in the sort orders associated with the corresponding columns of the Field Values Table. As a consequence, the insert point in each such vector for the pertinent field value from the new record is easily determined—for example, by binary search—and the vectors themselves, and hence the overall Field Values Table, are thus easily maintained.

The Record Reconstruction Table is another matter, however. Is there a way to avoid rebuilding the entire table every time a new record is inserted into the user file? The answer, of course, is yes. One possible approach is as follows (the details are a little complicated, but the fundamental idea is straightforward): When a record is deleted from the user file, we3 don’t physically remove the corresponding entries from the Field Values and Record Reconstruction Tables, we just flag those entries as “logically deleted.” Those flagged cells can then be regarded as free space in each of the two tables. Then, when we subsequently insert a new record, it might be possible to use such flagged cells for the record in question (removing the flags, of course), thereby avoiding the overhead of completely rebuilding the Record Reconstruction Table (and the overhead of completely rebuilding the Field Values Table also, as a matter of fact). Detailed examples illustrating this process are given in Sections 6.3 and 6.4 below. 
I should immediately add that the scheme just described in outline makes considerably more sense if the refinements to be discussed in Chapters 8 and 9 are adopted. If they are—and in practice it’s virtually certain they will be—then it becomes possible for distinct records at the file level to share entries in the Field Values Table. For example, the supplier records for suppliers S2 and S3 might share the entry in that table that contains the city name Paris. Thus, when a new record is inserted, it might well be the case that most if not all of the field values in that record already exist in the Field Values Table—perhaps logically deleted, perhaps not—and such values can simply be shared by that new record with previously existing records. In effect, the ability to share field values in this way means that INSERT operations work at the field level instead of the usual record level—yet another significant difference between the TR approach and conventional implementation technology. Of course, analogous remarks apply to DELETE and UPDATE operations also, as you’d surely expect.