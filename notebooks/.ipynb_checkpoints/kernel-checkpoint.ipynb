{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2bbc12eaec49c1263fbff0f71383d5db001a5c3"
   },
   "outputs": [],
   "source": [
    "# LDA and Document Similarity\n",
    "\n",
    "We are again working with the same fake news articles supplied by Kaggle.\n",
    "\n",
    "**I do not endorse and am not expressing any political affiliation or intent expressed in the articles in this dataset.**\n",
    "\n",
    "We will explain LDA and train an LDA model on this corpus of fake news to see what topics emerge.\n",
    "\n",
    "We will hold out some documents for testing to infer their topic distributions and compare them to the rest of the corpus to find the most similar documents.\n",
    "\n",
    "We use the [gensim](https://radimrehurek.com/gensim/models/ldamodel.html) package to do this, as it is highly optimised in C and has many features that make the implementation easy to use and very flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "253030c358d567a5b6a82aa4c12bb84355075b49",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim import models, corpora, similarities\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "from nltk import FreqDist\n",
    "from scipy.stats import entropy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.io.json import json_normalize #package for flattening json in pandas df\n",
    "#from fastai.imports import *\n",
    "#from pandas_summary import DataFrameSummary\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "import os\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"PAN14/pan14_train_english-essays/\"\n",
    "\n",
    "def train_lda(docs, num_topics=5):\n",
    "    \"\"\"\n",
    "    This function trains the lda model\n",
    "    We setup parameters like number of topics, the chunksize to use in Hoffman method\n",
    "    We also do 2 passes of the data since this is a small dataset, so we want the distributions to stabilize\n",
    "    \"\"\"\n",
    "    #num_topics = 100\n",
    "    #chunksize = 300\n",
    "    dictionary = corpora.Dictionary(docs)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    t1 = time.time()\n",
    "    # low alpha means each document is only represented by a small number of topics, and vice versa\n",
    "    # low eta means each topic is only represented by a small number of words, and vice versa\n",
    "    lda = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                   alpha=1e-2, eta=0.5e-2, minimum_probability=0.0, passes=2)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to train LDA model on \", len(docs), \"articles: \", (t2-t1)/60, \"min\")\n",
    "    return dictionary,corpus,lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_json('PAN14/pan14_train_english-essays/truth.json')\n",
    "train=json_normalize(train['problems'])\n",
    "train['known01']=None\n",
    "train['known02']=None\n",
    "train['known03']=None\n",
    "train['known04']=None\n",
    "train['known05']=None\n",
    "train['unknown']=None\n",
    "train.set_index('name', drop=True, inplace=True)\n",
    "train=train[['known01','known02','known03','known04','known05', 'unknown', 'answer']]\n",
    "\n",
    "dirs = []\n",
    "docs = []\n",
    "\n",
    "for i, x in enumerate(os.walk(DATAPATH)):\n",
    "    if i:\n",
    "        for fname in x[2]:\n",
    "            with open(DATAPATH+dirs[i-1]+'/'+fname, 'r') as f:\n",
    "                text = nltk.word_tokenize(f.read())\n",
    "                docs.append(text)\n",
    "                train.loc[dirs[i-1],fname[:-4]]=text\n",
    "    else:\n",
    "        dirs = x[1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff Origin of words in Canterbury Tales  1 Introduction and Aim  In this essay I have investigated lexical words from two pages in the Prologue of Canterbury Tales. \\nMy aim was to try and find out of what origin these words were. \\nI also wanted to see if there was any relation between the origin of the words and what kind of words they were, that is, what their meaning was. \\nI wanted to find out if, for example, all of the Old English loans came from a specific wordclass, and also, if these words were from some specific area, for example metallurgy, trade etc.  \\n2 Method and material  I studied two pages from the prologue of Canterbury Tales, lines 1-75, and picked out every noun, verb, adjective and adverb from it. \\nI have chosen to concentrate on lexical words only, since the function words are not loans. \\nTo investigate of which origin the words I found were, I consulted three different dictionaries, namely the Dictionary: Chaucer 1969,Collins English Dictionary, 1979 and The Oxford Dictionary of English Etymology. \\n1966. \\nThese dictionaries have supplemented each other, why I will not give any specific source references for each one of the words that I have worked with. \\nThis list of words can be found in the appendix. \\nI have tried to clarify my results by converting the numbers into diagrams. \\nThese show the percentage of words originating from a specific language, but at times there was not only one possible origin for a word, why other possible origins also have been listed. \\nThere are only brief comments to the diagrams and the results will be discussed under number 4, Conclusion and discussion.  \\t\\nI was unable to find what wordclass the Old English word [yit ]comes from, therefore my tables and diagrams could differ with maximum +/- 1%. \\nI believe that one word more or less does not affect my conclusion, since my material consists of only 196 words, it is already too limited in order to draw any sure and fully reliable conclusions from. \\nIn order to do that \\x03, it is necessary to study a far greater amount of words that I have have done. \\nMy conclusions will therefore be a bit uncertain, but perhaps they can indicate some sort of connection between wordclasses and loanwords.   \\n2.1 Previous research  ost of the words in the prologue to Canterbury Tales come from, as will be shown, old English and old French why I thought it suitable to include some of the history behind these languages.  \\n2.1.1 Old English  Around the year 450 AD Anglo-Saxon bands began to settle down in Britain and continued doing so through the sixth century. \\n(1993) Barber writes that \"The Anglo-Saxon conquest was not just the arrival of a ruling minority, but the settlement of a whole people\" (1993: p101) Barber also writes that their language became and remained the dominant one. \\nDifferent tribes ruled in small kingdoms all over the British islands, but when England was unified under the West Saxon kings in the late ninth century it led to a recognition of the West Saxon dialect as a literary standard. \\nOld English dropped a number of noun cases so that only four remained, a new tense system using auxiliaries began to develop as did forms for the perfect and passive. \\nOld English created new words using affixes and compounding and not by borrowing from other languages. \\n(Barber, 1993)  2.1.2 Old Norse  Barber writes that During the later part of the Old English period, two different groups of non-English speakers invaded the country. \\n[...] \\nBoth of their languages, Old Norse and Old French, had a considerable influence on English. (1993, p127) The Scandinavian influence can today be traced in English place names. \\nThe main reason why Old Norse has affected Old English to such a great extent as it has, is due to various bilingual situations in the later Old English period. \\nSince Old English and Old Norse were very similar, it was easy for people to pick up each others languages and then alter them by adding words from their own language or give them another meaning. \\nThis way the two languages got mixed up, so even if Old Norse eventually died out, it left its mark upon the English language. \\nScandinavian words can be detected because of their phonological form; neither the front-mutation nor the palatalization occurred in Old Norse, why a word, if borrowed in Late Old English it would develop in a clearly different way. \\n(Barber. \\n1993)   2.1.3 Old French  Barber states that The Norman Conquest of 1066 had a profound influence on the English language, but the French influence occurred, mainly at the higher levels of society, even before the Conquest. \\nThis led to the English language ceasing to be the language of the governing classes for about 200 years. \\n(Barber.1993) The borrowing from French continued toward the end of the Middle English Period. \\n(Baugh & Cable. \\n1951) During this time West Saxon lost its place as a standard literary language. \\n(Barber. \\n1993) Baugh and Cable adds that the year 1250 is an important dividing line concerning borrowings from Old French, since words borrowed before 1250 were such that lower classes would come into contact with only in association with a French speaking upperclass. \\nThese were words like: baron, noble, servant etc. \\nAfter 1250 the language changed primarily due to the nobility borrowing more and more words into the English language, perhaps because of a need to supply deficiencies in their own vocabulary. \\nThese words were borrowed from several different areas of the language, there were governmental and ecclesiastical words, borrowings in the field of law, army, social life an words from art and medicine, to give a few examples. \\nFrench words were assimilated by the adding of English suffixes and became so called hybrid forms. \\nWhen there was two words with the same meaning, one native and one French, often the Old English one died out, or, if they both survived they were differentiated in meaning.  \\n3 Presentation of results  I have divided the presentation into six parts. \\nIn the first part I will deal generally with the origin of all the lexical words that I have investigated. \\nThen, I will continue with separate tables for each wordclass, whereby I will see if it is possible to determine if the specific wordclass is borrowed mainly from one period or not. \\nThe results of my investigations is presented in tables and diagrams, which will speak for themselves, with only a brief textual clarification. \\nIn the sixth part I have made an attempt to determine if the words borrowed from the different languages have some specific area in common, for example terms relating to trade, nobility and so on.   \\n3.1 The origin of the words in general  When I investigated the 196 words I had picked out from the prologue, I found that they came from eleven different groups, most of them, 135 words, were of Old English origin. \\nThe largest group of words apart from the Old English come from Old French and consisted of 27 words, as shown in table 1.   \\nIn order to show more clearly to what extent the different origins are represented in, my material of 196 words, I have also put them in a diagram, labelled Origin of all lexical words, which can be found below table 1. \\nI have also added how many percent the words of each origin make of all the lexical words. \\nThe table shows that there out of 196 words, 135 are words of Old English origin. \\nIn the diagram it is shown more clearly how dominant this origin is, since words of Old English origin make 68,4% of the whole.  \\n3.2 The origin of nouns  When investigating the origin of nouns, I found that most of them, again, come from Old English. \\nThe table, number two, and the diagram Origin of nouns also show that the difference between the amount of words coming from Old English and Old French is not as great as when looking at the origin of all the lexical words. \\nThe amount of words coming from Old French when looking at all the words was 13,6%, the amount of the nouns was 23,8%.  \\n3.3 The origin of verbs  Old English continues to be the most common origin, but when looking at verbs, it is clearly shown that while Old English keeps its position as number one, French and Old French words take a step back, and though Old French still is the second largest language from which verbs in Canterbury Tales have originated, the difference between the amount of words coming from Old French compared with Old English is striking.   \\n3.4 The origin of adjectives  Adjectives seem to come from mainly six different languages, but since I have investigated only a limited number of words, it is not possible to draw the conclusion that there were not any adjectives at all from example Anglo-French. \\nOn the other hand I do believe it is possible to draw the conclusion that most of the adjectives come from Old English.   \\n3.5 The origin of adverbs When looking at adverbs there are only two languages left, namely Old English and Old French or Latin. \\nOld French or Latin stands for a word where it is not clear of which origin it is.   \\n3.6 Connection between meaning and origin.  \\nI looked at the 196 words in my list, trying to determine whether there was a connection between meaning of words and of which origin they were. \\nSince Old English dominated the origin of the words, I thought that words of many different meanings could be found there, why there was not much use to look for some specific feature. \\nInstead I checked words originating from Old French and found what could be a weak connection between the words. \\nMany had to do with chivalry and described how a knight should be and act, for example: adventure,war, siege, knightly conduct, honour, supreme, company, fellowship, heart; mood, spirit, courtesy, rank, virtue, power, ease, comfort perfect, behaviour, gentle, refined. \\nThe objection to this would be the source from which the words are gathered, the general prologue of the Canterbury Tales describes a knight and his actions, why many of the words would obviously have to do with that. \\nThe interesting thing is that they all are of French origin, so perhaps there is a weak connection between the Old French origin and the meaning of these words.  \\n4 Conclusion and discussion  When looking at all the diagrams and comparing them, it is clearly shown that the dominating language in all lexical wordclasses is Old English, followed by Old French. \\nThere are perhaps many reasons for this, but I believe that the amount of words from these two languages is as great as it is due to the two invasions. \\nThese invasions, the Anglo-Saxon around the year 450 AD and The Norman Conquest of 1066 have left a mark in history. \\nSince Old English created new words by compounding or using affixes, there have not been many borrowings during that period, why there are lot of Old English words. \\nOld French was for a couple of hundred years even the governmental language of Great Britain. \\nBefore this conquest the French language was already affecting the people when words like noble and servant entered the language. \\nWhen comparing all the diagrams, it is shown that the Old French words in this text were mainly nouns. \\nPerhaps that was the kind of words borrowed most frequently, at least before 1250. \\nIt was although mainly the upper-classes who used French, but it spread among the people and has also, as shown, left a mark in the English language. \\nFrench seems to have given many words to the English language concerning warfare and knightly conduct. \\nIf there is a connection between language and the meaning of the words gathered from that language cannot be said for sure, but I believe that it is quite possible, since war has always been an accurate topic of every time. \\nPeople could have used these new terms about war, especially since the language of the whole nation for a time was French and the language itself was introduced through a Conquest. \\t\\nThe only reliable conclusion I believe I can come to is that the main part of the words from lines 1-75 in the Prologue of Canterbury Tales originates from Old English.   \\nReferences  Barber, Charles. \\n1993 The English Language a Historical Introduction, pp100-151, Cambridge University Press Baugh, Albert C and Cable, Thomas, 1951 A History of the English Language, pp 163-187. 4:th edition. \\nTJ Press (Padstow) Ltd. \\nGreat Britain Chaucer, The Prologue & Three Tales .1969 Thomas Nelson Ltd, London and Edinburgh Collins English Dictionary, 1979, William Collins Sons & Co. \\nLtd. \\nThe Oxford Dictionary of English Etymology. \\n1966, Oxford University Press, Oxford\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d69ee4e9cdda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d1a22cc41cbe>\u001b[0m in \u001b[0;36mtrain_lda\u001b[0;34m(docs, num_topics)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#num_topics = 100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#chunksize = 300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# update Dictionary with the document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[0;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \"\"\"\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "dictionary,corpus,lda = train_lda(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a74d5171e0dd8e35f9f7c9f4014074a87981377b"
   },
   "source": [
    "Read in data; only keep essential columns and English language articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22ce8c7efc61758696a7769ae246c20f6e34200b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('fake.csv', usecols = ['uuid','author','title','text','language','site_url','country'])\n",
    "df = df[df.language == 'english']\n",
    "df = df[df['text'].map(type) == str]\n",
    "df['title'].fillna(value=\"\", inplace=True)\n",
    "df.dropna(axis=0, inplace=True, subset=['text'])\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1.0)\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "69c4f82c0203ac1e71f3ead813e93598ac20a538"
   },
   "source": [
    "Define some functions to clean and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ece863124a29e3f7902b5055411c8cc0d08f656f"
   },
   "outputs": [],
   "source": [
    "def initial_clean(text):\n",
    "    \"\"\"\n",
    "    Function to clean text of websites, email addresess and any punctuation\n",
    "    We also lower case the text\n",
    "    \"\"\"\n",
    "    text = re.sub(\"((\\S+)?(http(s)?)(\\S+))|((\\S+)?(www)(\\S+))|((\\S+)?(\\@)(\\S+)?)\", \" \", text)\n",
    "    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n",
    "    text = text.lower() # lower case the text\n",
    "    text = nltk.word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"\n",
    "    Function that removes all stopwords from text\n",
    "    \"\"\"\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    \"\"\"\n",
    "    Function to stem words, so plural and singular are treated the same\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = [stemmer.stem(word) for word in text]\n",
    "        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n",
    "    except IndexError: # the word \"oed\" broke this, so needed try except\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def apply_all(text):\n",
    "    \"\"\"\n",
    "    This function applies all the functions above into one\n",
    "    \"\"\"\n",
    "    return stem_words(remove_stop_words(initial_clean(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9666b08136234b6316be06f489e689770ec6f0d"
   },
   "outputs": [],
   "source": [
    "# clean text and title and create new column \"tokenized\"\n",
    "t1 = time.time()\n",
    "df['tokenized'] = df['text'].apply(apply_all) + df['title'].apply(apply_all)\n",
    "t2 = time.time()\n",
    "print(\"Time to clean and tokenize\", len(df), \"articles:\", (t2-t1)/60, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b754b906543c1bc88b1cbdcf28dc050f3c637ad"
   },
   "source": [
    "### Get word frequency\n",
    "\n",
    "We'll use nltk to get a word frequency (by count) here and only keep the top most used words to train the LDA model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b627fb9ca2db667b426dbd7994ffaf7f9e5b26a4"
   },
   "outputs": [],
   "source": [
    "# first get a list of all words\n",
    "all_words = [word for item in list(df['tokenized']) for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "len(fdist) # number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fe3266cb61ca8db7c62393a712d187d77a6f813e"
   },
   "outputs": [],
   "source": [
    "# choose k and visually inspect the bottom 10 words of the top k\n",
    "k = 50000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04c551e8082adb999ffb4fca12b2fa50aafdb7ae"
   },
   "outputs": [],
   "source": [
    "# choose k and visually inspect the bottom 10 words of the top k\n",
    "k = 15000\n",
    "top_k_words = fdist.most_common(k)\n",
    "top_k_words[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4ba31a2ce0a8936b08dec5f4771b0f2ca638fff5"
   },
   "source": [
    "k = 50,000 is too high, as the bottom words aren't even real words and are very rarely used (once in entire corpus)\n",
    "\n",
    "k = 15,000 is much more reasonable as these have been used at least 13 times in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1dbe335a3f12a5a8d8c42e8a30cadff3b85958d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function only to keep words in the top k words\n",
    "top_k_words,_ = zip(*fdist.most_common(k))\n",
    "top_k_words = set(top_k_words)\n",
    "def keep_top_k_words(text):\n",
    "    return [word for word in text if word in top_k_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cda255d76f921f5d2d02c9823c32f37197ba8625"
   },
   "outputs": [],
   "source": [
    "df['tokenized'] = df['tokenized'].apply(keep_top_k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1326353f15d316c9d90e0f3e41d236bc1eb1528d"
   },
   "outputs": [],
   "source": [
    "# document length\n",
    "df['doc_len'] = df['tokenized'].apply(lambda x: len(x))\n",
    "doc_lengths = list(df['doc_len'])\n",
    "df.drop(labels='doc_len', axis=1, inplace=True)\n",
    "\n",
    "print(\"length of list:\",len(doc_lengths),\n",
    "      \"\\naverage document length\", np.average(doc_lengths),\n",
    "      \"\\nminimum document length\", min(doc_lengths),\n",
    "      \"\\nmaximum document length\", max(doc_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4bed9a845aef27b25761580c068a7765f5c9286"
   },
   "outputs": [],
   "source": [
    "# plot a histogram of document length\n",
    "num_bins = 1000\n",
    "fig, ax = plt.subplots(figsize=(12,6));\n",
    "# the histogram of the data\n",
    "n, bins, patches = ax.hist(doc_lengths, num_bins, normed=1)\n",
    "ax.set_xlabel('Document Length (tokens)', fontsize=15)\n",
    "ax.set_ylabel('Normed Frequency', fontsize=15)\n",
    "ax.grid()\n",
    "ax.set_xticks(np.logspace(start=np.log10(50),stop=np.log10(2000),num=8, base=10.0))\n",
    "plt.xlim(0,2000)\n",
    "ax.plot([np.average(doc_lengths) for i in np.linspace(0.0,0.0035,100)], np.linspace(0.0,0.0035,100), '-',\n",
    "        label='average doc length')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da032fda3e67cc0e36eb5c3a4dbcee06884b25b5"
   },
   "source": [
    "We can see that, compared to our histogram in exploring_news notebook, the average document length is about half when all stop words are removed and only the top 15,000 words are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "573ae0666e70aaf250b48216bff3f6c9f6b72e0b"
   },
   "source": [
    "### Drop short articles\n",
    "\n",
    "LDA does not work very well on short documents, which we will explain later, so we will drop some of the shorter articles here before training the model.\n",
    "\n",
    "From the histogram above, droping all articles less than 40 tokens seems appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "601f67c747bb0565f8d7b111147141feaddffb71",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only keep articles with more than 30 tokens, otherwise too short\n",
    "df = df[df['tokenized'].map(len) >= 40]\n",
    "# make sure all tokenized items are lists\n",
    "df = df[df['tokenized'].map(type) == list]\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "print(\"After cleaning and excluding short aticles, the dataframe now has:\", len(df), \"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "000416eb7463df67145b2fe8daffa90076e8289b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "20d83a00fdbe689e433cd23a06a76017d8400fb3"
   },
   "source": [
    "### Split the corpus into training and testing\n",
    "Here we will split the corpus into training and testing sets.\n",
    "\n",
    "The training set will be used to train the LDA model on, while the testing set will be used to retrieve similar articles later in our recommendation algorithm.\n",
    "\n",
    "The dataframe is already shuffled from the begining, so no need to do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d299803d009575d311d833d34e1775368ab79fb1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a mask of binary values\n",
    "msk = np.random.rand(len(df)) < 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ba0b64ffd6e298e4c645fea877ae0699679d4b41"
   },
   "outputs": [],
   "source": [
    "train_df = df[msk]\n",
    "train_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "test_df = df[~msk]\n",
    "test_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66073273178e2f754a29c50784353a9570dcdc5e"
   },
   "outputs": [],
   "source": [
    "print(len(df),len(train_df),len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12b4cd7862b680387f32f1cd3a2d818e44d87137"
   },
   "source": [
    "# LDA\n",
    "\n",
    "Latent Dirichlet Allocation, is an unsupervised generative model that assigns topic distributions to documents.\n",
    "\n",
    "At a high level, the model assumes that each document will contain several topics, so that there is topic overlap within a document. The words in each document contribute to these topics. The topics may not be known a priori, and needn't even be specified, but the **number** of topics must be specified a priori. Finally, there can be words overlap between topics, so several topics may share the same words.\n",
    "\n",
    "The model generates to **latent** (hidden) variables\n",
    "1) A distribution over topics for each document\n",
    "2) A distribution over words for each topics\n",
    "\n",
    "After training, each document will have a discrete distribution over all topics, and each topic will have a discrete distribution over all words.\n",
    "\n",
    "It is best to demonstrate this with an example. Let's say a document about the presidential elections may have a high contribution from the topics \"presidential elections\", \"america\", \"voting\" but have very low contributions from topics \"himalayan mountain range\", \"video games\", \"machine learning\" (assuming the corpus is varied enough to contain such articles); the topics \"presidential elections\" may have top contributing words [\"vote\",\"election\",\"people\",\"usa\",\"clinton\",\"trump\",...] whereas the top contributing words in the topic \"himalayan mountain range\" may be [\"nepal\",\"everest\",\"china\",\"altitude\",\"river\",\"snow\",....]. This very rough example should give you an idea of what LDA aims to do.\n",
    "\n",
    "An important point to note: although I have named some topics in the example above, the model itself does not actually do any \"naming\" or classifying of topics. But by visually inspecting the top contributing words of a topic i.e. the discrete distribution over words for a topic, one can name the topics if necessary after training. We will show this more later.\n",
    "\n",
    "There a several ways to implement LDA, however I will speak about collapsed gibbs sampling as I usually find this to be the easiest way to understand it.\n",
    "\n",
    "The model initialises by assigning every word in every document to a **random** topic. Then, we iterate through each word, unassign it's current topic, decrement the topic count corpus wide and reassign the word to a new topic based on the local probability of topic assignemnts to the current document, and the global (corpus wide) probability of the word assignments to the current topic. This may be hard to understand in words, so the equations are below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "54cbacefcd1888fab2ed1214d5914bf005783890"
   },
   "source": [
    "### The mathematics of collapsed gibbs sampling (cut back version)\n",
    "\n",
    "Recall that when we iterate through each word in each document, we unassign its current topic assignment and reassign the word to a new topic. The topic we reassign the word to is based on the probabilities below.\n",
    "\n",
    "$$\n",
    "P\\left(\\text{document \"likes\" the topic}\\right) \\times P\\left(\\text{topic \"likes\" the word } w'\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Rightarrow \\frac{n_{i,k}+\\alpha}{N_i-1+K\\alpha} \\times \\frac{m_{w',k}+\\gamma}{\\sum_{w\\in V}m_{w,k} + V\\gamma}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$n_{i,k}$ - number of word assignments to topic $k$ in document $i$\n",
    "\n",
    "$n_{i,k}$ - number of assignments to topic $k$ in document $i$\n",
    "\n",
    "$\\alpha$ - smoothing parameter (hyper parameter - make sure probability is never 0)\n",
    "\n",
    "$N_i$ - number of words in document $i$\n",
    "\n",
    "$-1$ - don't count the current word you're on\n",
    "\n",
    "$K$ - total number of topics\n",
    "\n",
    "\n",
    "$m_{w',k}$ - number of assignments, corpus wide, of word $w'$ to topic $k$\n",
    "\n",
    "$m_{w',k}$ - number of assignments, corpus wide, of word $w'$ to topic $k$\n",
    "\n",
    "$\\gamma$ - smoothing parameter (hyper parameter - make sure probability is never 0)\n",
    "\n",
    "$\\sum_{w\\in V}m_{w,k}$ - sum over all words in vocabulary currently assigned to topic $k$\n",
    "\n",
    "$V$ size of vocabulary i.e. number of distinct words corpus wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acf75ae9276dbcdd9848b20e23c3ba48e863a63c"
   },
   "source": [
    "### Notes and Uses of LDA\n",
    "\n",
    "LDA has many uses; understanding the different varieties topics in a corpus (obviously), getting a better insight into the type of documents in a corpus (whether they are about news, wikipedia articles, business documents), quantifying the most used / most important words in a corpus, and even document similarity and recommendation.\n",
    "\n",
    "LDA does not work well with very short documents, like twitter feeds, as explained here [[1]](https://pdfs.semanticscholar.org/f499/5dc2a4eb901594578e3780a6f33dee02dad1.pdf) [[2]](https://stackoverflow.com/questions/29786985/whats-the-disadvantage-of-lda-for-short-texts), which is why we dropped articles under 40 tokens previously. Very briefly, this is because the model infers parameters from observations and if there are not enough observations (words) in a document, the model performs poorly. For short texts, although yet to be rigoursly tested, it may be best to use a [biterm model](https://pdfs.semanticscholar.org/f499/5dc2a4eb901594578e3780a6f33dee02dad1.pdf).\n",
    "\n",
    "Unlike the word2vec algorithm, which performs extremely well with full structured sentences, LDA is a bag of words model, meaning word order in a document doesnt count. This also means that stopwords and rare words should be excluded, so that the model doesnt overcompensate for very frequent words and very rare words, both of which do not contribute to general topics.\n",
    "\n",
    "#### Hyperparameters\n",
    "\n",
    "LDA has 2 hyperparameters: $\\alpha$ and $\\eta$\n",
    "\n",
    "$\\alpha$ - A low value for $\\alpha$ means that documents have only a low number of topics contributing to them. A high value of $\\alpha$ yields the inverse, meaning the documents appear more alike within a corpus.\n",
    "\n",
    "$\\eta$ - A low value for $\\eta$ means the topics have a low number of contributing words. A high value of $\\eta$ yields the inverse, meaning topics will have word overlap and appear more alike.\n",
    "\n",
    "The values of $\\alpha$ and $\\eta$ really depend on the application, and may need to be tweaked several times before the desired results are found... even then, LDA is non-deterministic since parameters are randomly initialised, so the outcome of any run of the model can never be known in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a80e9a5ebfde9b65425b7107afeb29ee9c1b1b5a"
   },
   "outputs": [],
   "source": [
    "dictionary,corpus,lda = train_lda(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d365bc72b57530fadc20816ef0b404482c041a60"
   },
   "source": [
    "### Let's inspect some topics!\n",
    "\n",
    "Bear in mind, when we see the words they may seem shortened. Recall this is because of our stemming function we previously implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc66c27d1a909fa6d90050f3310be4864082df36"
   },
   "outputs": [],
   "source": [
    "# show_topics method shows the the top num_words contributing to num_topics number of random topics\n",
    "lda.show_topics(num_topics=10, num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c274c3ce894e40d867f4ab3d1fa3d4304c63edb2"
   },
   "source": [
    "#### We can inspect individual topics as such\n",
    "\n",
    "Note that if you re run the model again, as it is non-deterministic, word contributions to topics and topic ID's will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f098f5a165a0c14c155167caedaec80c6e48f67"
   },
   "source": [
    "#### This topic is about court cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bbdc6b184807a0cf8d0a0832a150e702ce102e70"
   },
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=4, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3ce8492c016b819d751d69519302e848cbae950"
   },
   "source": [
    "#### This topic is about (supposedly) Illegal Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31400ff04debbfe4738f9406ced3f2f0de85d61d"
   },
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=85, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "355a1edb9f423920190d03d396d40c3b8fd90a89"
   },
   "source": [
    "#### This topic is about Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afb39e09e8e0bf4d84b6cabe55185922bbcdd3f3"
   },
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=75, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6f48593eb8ff3badcc80b770b7712d14446ac468"
   },
   "source": [
    "#### This topic is about Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af43d900ed8f66b680998a18cf3a4e0e03bba214"
   },
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=39, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bcacf7b0b1c937225f128aa140c29772cdb83ace"
   },
   "source": [
    "What the about above means, is that topic 4 has top contributing words [\"judge\",\"case\",\"court\",...], which indicates the topic is about court cases. Topic 75 has top contributing words [\"god\",\"christian\",\"love\",...], which indicates the topic is about religion.\n",
    "\n",
    "Now, not only can we see the word contribution for each topic, but we can also visualise the topic contribution for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f16119e7579d2e524c54c8aa0f1d4042d11766b"
   },
   "outputs": [],
   "source": [
    "# select and article at random from train_df\n",
    "random_article_index = np.random.randint(len(train_df))\n",
    "bow = dictionary.doc2bow(train_df.iloc[random_article_index,7])\n",
    "print(random_article_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5f9d41e31374bf6a4aa5c6401d46c5ea302ad19",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(train_df.iloc[random_article_index,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f719e07543cab10d54c02cedbd1a8ce517d1be37",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the topic contributions for the document chosen at random above\n",
    "doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=bow)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af080e1e63e695756157509ccf055ab17205fcc8"
   },
   "outputs": [],
   "source": [
    "# bar plot of topic distribution for this document\n",
    "fig, ax = plt.subplots(figsize=(12,6));\n",
    "# the histogram of the data\n",
    "patches = ax.bar(np.arange(len(doc_distribution)), doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=15)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=15)\n",
    "ax.set_title(\"Topic Distribution for Article \" + str(random_article_index), fontsize=20)\n",
    "ax.set_xticks(np.linspace(10,100,10))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b50427ce3cfe5de33b9999686d8fa8a2cd0c6c32"
   },
   "source": [
    "Ok, so clearly this document has various contributions from different topics. But what are these topics? Lets find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1096ddc05022da80eeea33b1ff68a19e3c774460",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print the top 5 contributing topics and their words\n",
    "for i in doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97aefd11b71bc7adfd06045f2b5e89a3be556213"
   },
   "source": [
    "Let's interpret this.\n",
    "\n",
    "Topic 9  - Protests\n",
    "\n",
    "Topic 72 - Middl Eastern Countries\n",
    "\n",
    "Topic 36 - Islam\n",
    "\n",
    "Topic 55 - Power (socio political sense)\n",
    "\n",
    "Topic 38 - Peoples actions\n",
    "\n",
    "These are rough interpretations for these topics, most of which make sense. Reading the article we see the it is about riots in the Middle East. So the model seems to have worked well, at least in this one case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5756e67cdf9af09d6dca43f78995aa2412a19a8"
   },
   "source": [
    "# Similarity Queries and Unseen Data\n",
    "\n",
    "We will now turn our attention to the test set of data which the model has not yet seen. Although the articles in *test_df* have been unseen by the model, gensim has a way of infering their topic distributions given the trained model. Of course, the correct approach to yield accurate results would be to retrain the model with these new articles part of the corpus, but this can be timely and infeasable in a real case scenario where results are needed quickly.\n",
    "\n",
    "First, lets show how we can infer document topics for a new unseen article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e7afecbccde4fb17bcdb0ce7539d3cc1c4d104f"
   },
   "outputs": [],
   "source": [
    "# select and article at random from test_df\n",
    "random_article_index = np.random.randint(len(test_df))\n",
    "print(random_article_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "460e5be2729ef3dd7e6ec8f893d32dbc992fe7d1"
   },
   "source": [
    "Here's the important bit. In obtaining the BOW representation for this unseen article, gensim cleverly only considers words in the existing dictionary we used to train the model. So if there are new words in this article, they will not be considered when infering the topic distribution. This is good in that no errors arise for unseen words, but bad in that some words may be cut out, and therefore we could miss out on an accurate topic distribution for this article.\n",
    "\n",
    "However, we mitigate this risk because the training set is very much representative of the entire corpus; 99.9% of the observations are in the training set, with only 0.01% of observations in the test set. So most, if not all, words from the test set should be in the training set's dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0e6e61f1cb84acedea35c0c6dd311abc53fdc0c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_bow = dictionary.doc2bow(test_df.iloc[random_article_index,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91f7b357007722477594b9087ca282ae5b9364c1"
   },
   "outputs": [],
   "source": [
    "print(test_df.iloc[random_article_index,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "754832dc01bb39d1cca21b7bae4ef93bb4d598d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc_distribution = np.array([tup[1] for tup in lda.get_document_topics(bow=new_bow)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a9ee3321e7fdbe094fa62551411777fcfeda4627"
   },
   "source": [
    "Let's do the same visual analysis as before on this new unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "caf868f7619d8574807445a39da8d94f4cf0495a"
   },
   "outputs": [],
   "source": [
    "# bar plot of topic distribution for this document\n",
    "fig, ax = plt.subplots(figsize=(12,6));\n",
    "# the histogram of the data\n",
    "patches = ax.bar(np.arange(len(new_doc_distribution)), new_doc_distribution)\n",
    "ax.set_xlabel('Topic ID', fontsize=15)\n",
    "ax.set_ylabel('Topic Contribution', fontsize=15)\n",
    "ax.set_title(\"Topic Distribution for an Unseen Article\", fontsize=20)\n",
    "ax.set_xticks(np.linspace(10,100,10))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "690b18b03635c8bb717bccd9ca97c566fb66e8c6"
   },
   "outputs": [],
   "source": [
    "# print the top 8 contributing topics and their words\n",
    "for i in new_doc_distribution.argsort()[-5:][::-1]:\n",
    "    print(i, lda.show_topic(topicid=i, topn=10), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5e2edaa8f89dfa2f6a7d6507d0ad814d30d23a30"
   },
   "source": [
    "And there we have it! An accurate topic distribution for an unseen document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1576aa73b17a4d7b250915640506d2db07f2a81f"
   },
   "source": [
    "### Similarity query\n",
    "\n",
    "Ok, now that we have a topic distribution for a new unseen document, let's say we wanted to find the most similar documents in the corpus. We can do this by comparing the topic distribution of the new document to all the topic distributions of the documents in the corpus. We use the [Jensen-Shannon distance](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence) metric to find the most similar documents.\n",
    "\n",
    "What the Jensen-Shannon distance tells us, is which documents are statisically \"closer\" (and therefore more similar), by comparing the divergence of their distributions. Jensen-Shannon is symmetric, unlike Kullback-Leibler on which the formula is based. This is good, because we want the similarity between documents A and B to be the same as the similarity between B and A.\n",
    "\n",
    "The formula is described below.\n",
    "\n",
    "For discrete distirbutions $P$ and $Q$, the Jensen-Shannon divergence, $JSD$ is defined as\n",
    "\n",
    "$$JSD\\left(P||Q\\right) = \\frac{1}{2}D\\left(P||M\\right)+\\frac{1}{2}D\\left(Q||M\\right)$$\n",
    "\n",
    "where $M = \\frac{1}{2}\\left(P+Q\\right)$\n",
    "\n",
    "and $D$ is the Kullback-Leibler divergence\n",
    "\n",
    "$$D\\left(P||Q\\right) = \\sum_iP(i)\\log\\left(\\frac{P(i)}{Q(i)}\\right)$$\n",
    "\n",
    "$$\\Rightarrow JSD\\left(P||Q\\right) = \\frac{1}{2}\\sum_i\n",
    "\\left[\n",
    "P(i)\\log\\left(\\frac{P(i)}{\\frac{1}{2}\\left(P(i)+Q(i)\\right)}\\right)\n",
    "+\n",
    "Q(i)\\log\\left(\\frac{Q(i)}{\\frac{1}{2}\\left(P(i)+Q(i)\\right)}\\right)\n",
    "\\right]$$\n",
    "\n",
    "The square root of the Jensen-Shannon divergence is the Jensen-Shannon Distance: $\\sqrt{JSD\\left ( P||Q\\right )}$\n",
    "\n",
    "**The smaller the Jensen-Shannon Distance, the more similar two distributions are (and in our case, the more similar any 2 documents are)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14d246d018b71919e51a1405b86d27ab4de2492a"
   },
   "source": [
    "We can use the scipy implementation of entropy to do this. Entropy calculates the KL divergence.\n",
    "\n",
    "But first, we need to get all our LDA topic distributions into a dense matrix. This will enable fast and efficient computation.\n",
    "\n",
    "We will create a dense matrix, **doc_topic_dist**, of size $M\\times K$ where $M$ is the number of documents and $K$ is the number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d5aa8bdf004b17efa3e3a1c6f512deed5ffd6e6e"
   },
   "outputs": [],
   "source": [
    "# we need to use nested list comprehension here\n",
    "# this may take 1-2 minutes...\n",
    "doc_topic_dist = np.array([[tup[1] for tup in lst] for lst in lda[corpus]])\n",
    "doc_topic_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49330d856acde01162ee1968d1100280e8600b08",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jensen_shannon(query, matrix):\n",
    "    \"\"\"\n",
    "    This function implements a Jensen-Shannon similarity\n",
    "    between the input query (an LDA topic distribution for a document)\n",
    "    and the entire corpus of topic distributions.\n",
    "    It returns an array of length M where M is the number of documents in the corpus\n",
    "    \"\"\"\n",
    "    # lets keep with the p,q notation above\n",
    "    p = query[None,:].T # take transpose\n",
    "    q = matrix.T # transpose matrix\n",
    "    m = 0.5*(p + q)\n",
    "    return np.sqrt(0.5*(entropy(p,m) + entropy(q,m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96d8a5efab892371fc220c8dc158e244afbac5ff"
   },
   "source": [
    "Let's compare the new unseen document, to the corpus, and see which articles are most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a722a8d097dd1821b68fed471116e4dc693ac406",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar_documents(query,matrix,k=10):\n",
    "    \"\"\"\n",
    "    This function implements the Jensen-Shannon distance above\n",
    "    and retruns the top k indices of the smallest jensen shannon distances\n",
    "    \"\"\"\n",
    "    sims = jensen_shannon(query,matrix) # list of jensen shannon distances\n",
    "    return sims.argsort()[:k] # the top k positional index of the smallest Jensen Shannon distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecd1f80f12a5ca49b426138c2e969d688e872acd"
   },
   "source": [
    "#### Query time + most similar documents... at last!\n",
    "\n",
    "Ok, let's be 100% clear about what we are doing here.\n",
    "\n",
    "We are comparing the new unseen document above to the entire corpus of ~10k documents to find which one is most similar to the new document.\n",
    "\n",
    "How are we doing that? Well, we have the new documents LDA topic distribution in stored as varibale **new_doc_distribution**, and we have the entire corpus of documents topic distributions stored in the dense matrix **doc_topic_dist**. So now, we pass each row of **doc_topic_dist** through the Jensen-Shannon function above as the Q distribution, while the P distribution remains static as **new_doc_distribution**. Then we get the smallest distances and their corresponding index in the array, which we can pass to the **train_df** dataframe to print out the most similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "abf60ca921774f22a50035be37a9bc99e23afc0b"
   },
   "outputs": [],
   "source": [
    "# this is surprisingly fast\n",
    "most_sim_ids = get_most_similar_documents(new_doc_distribution,doc_topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43b3f66c3e9c26e5b9f17504f21c8e2a08659e80",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_similar_df = train_df[train_df.index.isin(most_sim_ids)]\n",
    "most_similar_df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c2154320f890dfc60d697821033ef665458e5d5"
   },
   "source": [
    "I think we can see, the top most similar articles are quite similar indeed to the query article ;)\n",
    "\n",
    "Our query article is about Trump, Huffington Post and the election. The top 10 most similar documents in the corpus also contain these topics, as their title show above. The reader can print out the full articles, or visualise the topic distributions for the most similar document and compare them to the query document to check the overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eef6d26bad60a383994dd59869fda9205e4b341e"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- After cleaning the corpus and keeping only the top 15,000 words, we reduced the unique words in the corpus by 84%\n",
    "- The average document length is halved to 345 tokens after cleaning, compared to the raw version we saw in our explore notebook using word2vec\n",
    "- The LDA algorithm was explained in detail\n",
    "- The LDA model was able to accurately identify different topics in the fake news corpus. We visually inspected these topics to see that the top words were related\n",
    "- We were able to infer a topic distribution from a new unseen document\n",
    "- We quickly retrieved the most similar documents in the trained corpus when comparing to the new unseen document. These most similar documents were in fact closely related to the query document"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
